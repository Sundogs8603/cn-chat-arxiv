<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#30340;&#32463;&#39564;&#20248;&#21183;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren</title><link>http://arxiv.org/abs/2308.13985</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#65306;&#19968;&#20010;&#29702;&#35770;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective. (arXiv:2308.13985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#30340;&#32463;&#39564;&#20248;&#21183;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26631;&#37327;&#21270;&#65292;&#21363;&#36890;&#36807;&#21152;&#26435;&#24635;&#21644;&#26469;&#32452;&#21512;&#25152;&#26377;&#25439;&#22833;&#20989;&#25968;&#65292;&#33258;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#21019;&#31435;&#20197;&#26469;&#19968;&#30452;&#26159;&#25991;&#29486;&#20013;&#30340;&#40664;&#35748;&#36873;&#25321;&#12290;&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24320;&#21457;&#19987;&#38376;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#22120;&#65288;SMTOs&#65289;&#26469;&#22788;&#29702;MTL&#20316;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;SMTOs&#26159;&#21542;&#27604;&#26631;&#37327;&#21270;&#26377;&#26681;&#26412;&#19978;&#30340;&#20248;&#21183;&#12290;&#23454;&#38469;&#19978;&#65292;&#31038;&#21306;&#20013;&#23384;&#22312;&#23545;&#27604;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#28608;&#28872;&#35752;&#35770;&#65292;&#20027;&#35201;&#26159;&#20174;&#32463;&#39564;&#35282;&#24230;&#20986;&#21457;&#12290;&#20026;&#20102;&#22238;&#31572;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#26631;&#37327;&#21270;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;MTL&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#37027;&#20123;&#22768;&#31216;&#26631;&#37327;&#21270;&#20855;&#26377;&#32463;&#39564;&#20248;&#21183;&#30340;&#26368;&#36817;&#24037;&#20316;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren
&lt;/p&gt;
&lt;p&gt;
Linear scalarization, i.e., combining all loss functions by a weighted sum, has been the default choice in the literature of multi-task learning (MTL) since its inception. In recent years, there is a surge of interest in developing Specialized Multi-Task Optimizers (SMTOs) that treat MTL as a multi-objective optimization problem. However, it remains open whether there is a fundamental advantage of SMTOs over scalarization. In fact, heated debates exist in the community comparing these two types of algorithms, mostly from an empirical perspective. To approach the above question, in this paper, we revisit scalarization from a theoretical perspective. We focus on linear MTL models and study whether scalarization is capable of fully exploring the Pareto front. Our findings reveal that, in contrast to recent works that claimed empirical advantages of scalarization, scalarization is inherently incapable of full exploration, especially for those Pareto optimal solutions that strike the balanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#39044;&#35757;&#32451;&#30340;Segment Anything Model&#21644;&#24494;&#35843;&#30340;Segment Anything Model&#22312;&#21508;&#31181;&#22270;&#20687;&#32972;&#26223;&#19979;&#30340;&#34880;&#36857;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#28385;&#24847;&#24230;&#65292;&#24494;&#35843;&#30340;&#27169;&#22411;&#27604;&#39044;&#35757;&#32451;&#27169;&#22411;&#20934;&#30830;&#24615;&#25552;&#21319;&#20102;2.2&#65285;&#65292;&#22312;&#22270;&#20687;&#35782;&#21035;&#36895;&#24230;&#26041;&#38754;&#21152;&#24555;&#20102;4.70&#65285;&#12290;</title><link>http://arxiv.org/abs/2308.13979</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;AI&#30340;&#20998;&#21106;&#22686;&#24378;&#34880;&#36857;&#20998;&#26512;&#65306;&#21033;&#29992;Segment Anything Model&#36827;&#34892;&#29616;&#22330;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Enhancing Bloodstain Analysis Through AI-Based Segmentation: Leveraging Segment Anything Model for Crime Scene Investigation. (arXiv:2308.13979v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#39044;&#35757;&#32451;&#30340;Segment Anything Model&#21644;&#24494;&#35843;&#30340;Segment Anything Model&#22312;&#21508;&#31181;&#22270;&#20687;&#32972;&#26223;&#19979;&#30340;&#34880;&#36857;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#28385;&#24847;&#24230;&#65292;&#24494;&#35843;&#30340;&#27169;&#22411;&#27604;&#39044;&#35757;&#32451;&#27169;&#22411;&#20934;&#30830;&#24615;&#25552;&#21319;&#20102;2.2&#65285;&#65292;&#22312;&#22270;&#20687;&#35782;&#21035;&#36895;&#24230;&#26041;&#38754;&#21152;&#24555;&#20102;4.70&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34880;&#36857;&#27169;&#24335;&#20998;&#26512;&#36890;&#36807;&#30740;&#31350;&#29420;&#29305;&#30340;&#34880;&#36857;&#27169;&#24335;&#20026;&#29616;&#22330;&#35843;&#26597;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#23545;&#22270;&#20687;&#32972;&#26223;&#26377;&#20005;&#26684;&#35201;&#27714;&#65292;&#23545;&#20110;&#28082;&#28404;&#22270;&#20687;&#20998;&#21106;&#26469;&#35828;&#24037;&#20316;&#37327;&#20063;&#24456;&#22823;&#12290;Segment Anything Model&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#36817;&#26399;&#25552;&#20986;&#30340;&#24191;&#27867;&#22270;&#20687;&#35782;&#21035;&#26041;&#27861;&#65292;&#23578;&#26410;&#20805;&#20998;&#35780;&#20272;&#20854;&#22312;&#34880;&#36857;&#22270;&#20687;&#20998;&#21106;&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#30340;SAM&#21644;&#24494;&#35843;&#30340;SAM&#22312;&#20855;&#26377;&#19981;&#21516;&#22270;&#20687;&#32972;&#26223;&#30340;&#34880;&#36857;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;SAM&#21644;&#24494;&#35843;&#30340;SAM&#37117;&#33021;&#22815;&#20197;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#25191;&#34892;&#34880;&#36857;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#65292;&#32780;&#24494;&#35843;&#30340;SAM&#30456;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;SAM&#22312;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;2.2&#65285;&#65292;&#22312;&#22270;&#20687;&#35782;&#21035;&#36895;&#24230;&#26041;&#38754;&#21152;&#24555;&#20102;4.70&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bloodstain pattern analysis plays a crucial role in crime scene investigations by providing valuable information through the study of unique blood patterns. Conventional image analysis methods, like Thresholding and Contrast, impose stringent requirements on the image background and is labor-intensive in the context of droplet image segmentation. The Segment Anything Model (SAM), a recently proposed method for extensive image recognition, is yet to be adequately assessed for its accuracy and efficiency on bloodstain image segmentation. This paper explores the application of pre-trained SAM and fine-tuned SAM on bloodstain image segmentation with diverse image backgrounds. Experiment results indicate that both pre-trained and fine-tuned SAM perform the bloodstain image segmentation task with satisfactory accuracy and efficiency, while fine-tuned SAM achieves an overall 2.2\% accuracy improvement than pre-trained SAM and 4.70\% acceleration in terms of speed for image recognition. Analys
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22270;&#19978;&#22522;&#20110;QUBO&#20844;&#24335;&#30340;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#20844;&#24335;&#24418;&#24335;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;...</title><link>http://arxiv.org/abs/2308.13978</link><description>&lt;p&gt;
&#29702;&#35299;&#22522;&#20110;QUBO&#30340;&#21704;&#23494;&#39039;&#20989;&#25968;&#22312;&#22270;&#19978;&#30340;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#20351;&#29992;&#65306;&#20197;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20026;&#20363;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Understanding the Usage of QUBO-based Hamiltonian Function in Combinatorial Optimization over Graphs: A Discussion Using Max Cut (MC) Problem. (arXiv:2308.13978v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13978
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22270;&#19978;&#22522;&#20110;QUBO&#20844;&#24335;&#30340;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#20844;&#24335;&#24418;&#24335;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#65288;QUBO&#65289;&#26159;&#19968;&#31181;&#24191;&#20041;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#21508;&#31181;NP&#22256;&#38590;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#24314;&#27169;&#20026;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#24418;&#24335;&#12290;&#21704;&#23494;&#39039;&#20989;&#25968;&#32463;&#24120;&#29992;&#20110;&#24418;&#25104;QUBO&#38382;&#39064;&#65292;&#20854;&#20013;&#23427;&#22312;&#20248;&#21270;&#30340;&#19978;&#19979;&#25991;&#20013;&#34987;&#29992;&#20316;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#35299;&#20915;QUBO&#20844;&#24335;&#20013;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#22312;&#33410;&#28857;&#20043;&#38388;&#20256;&#36882;&#20449;&#24687;&#12290;&#25105;&#20204;&#20027;&#35201;&#30740;&#31350;&#20102;&#19977;&#31181;&#20844;&#24335;&#65292;Monty-Carlo Tree Search with GNN-based RL&#65288;MCTS-GNN&#65289;&#12289;DQN with GNN-based RL&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#30340;&#36890;&#29992;GNN&#65288;GRL&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to model various NP-hard combinatorial optimization problems in the form of binary variables. The Hamiltonian function is often used to formulate QUBO problems where it is used as the objective function in the context of optimization. In this study, we investigate how reinforcement learning-based (RL) paradigms with the presence of the Hamiltonian function can address combinatorial optimization problems over graphs in QUBO formulations. We use Graph Neural Network (GNN) as the message-passing architecture to convey the information among the nodes. We have centered our discussion on QUBO formulated Max-Cut problem but the intuitions can be extended to any QUBO supported canonical NP-Hard combinatorial optimization problems. We mainly investigate three formulations, Monty-Carlo Tree Search with GNN-based RL (MCTS-GNN), DQN with GNN-based RL, and a generic GNN with attention-based RL (GRL). Our findings state that i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;</title><link>http://arxiv.org/abs/2308.13976</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Label Denoising through Cross-Model Agreement. (arXiv:2308.13976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20174;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#23398;&#20064;&#26159;&#38750;&#24120;&#24120;&#35265;&#30340;&#12290;&#35760;&#24518;&#36825;&#20123;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#40065;&#26834;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#65288;DeCA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#26368;&#23567;&#21270;&#30001;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;DeCA&#26041;&#27861;&#24212;&#29992;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#21644;&#22810;&#26631;&#31614;&#24773;&#26223;&#12290;&#23545;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#65292;&#25105;&#20204;&#36873;&#25321;&#38544;&#24335;&#21453;&#39304;&#25512;&#33616;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from corrupted labels is very common in real-world machine-learning applications. Memorizing such noisy labels could affect the learning of the model, leading to sub-optimal performances. In this work, we propose a novel framework to learn robust machine-learning models from noisy labels. Through an empirical study, we find that different models make relatively similar predictions on clean examples, while the predictions on noisy examples vary much more across different models. Motivated by this observation, we propose \em denoising with cross-model agreement \em (DeCA) which aims to minimize the KL-divergence between the true label distributions parameterized by two machine learning models while maximizing the likelihood of data observation. We employ the proposed DeCA on both the binary label scenario and the multiple label scenario. For the binary label scenario, we select implicit feedback recommendation as the downstream task and conduct experiments with four state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#21644;&#38544;&#31169;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#39046;&#22495;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2308.13970</link><description>&lt;p&gt;
FAM&#65306;&#24555;&#36895;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FAM: fast adaptive meta-learning. (arXiv:2308.13970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#21644;&#38544;&#31169;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#39046;&#22495;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#20840;&#23616;&#27169;&#22411;&#65292;&#28982;&#21518;&#21487;&#20197;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#20010;&#24615;&#21270;&#12290;&#32852;&#37030;&#23398;&#20064;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#25968;&#25454;&#12290;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#31471;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#25110;&#25968;&#25454;&#22810;&#26679;&#24615;&#23548;&#33268;&#23398;&#20064;&#21463;&#21040;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#26102;&#65292;&#23398;&#20064;&#20250;&#21463;&#21040;&#22256;&#25200;&#12290;&#26377;&#24517;&#35201;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#20351;&#29992;&#23458;&#25143;&#31471;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#33258;&#36866;&#24212;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#23458;&#25143;&#31471;&#19978;&#21019;&#24314;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;MRI&#25968;&#25454;&#23384;&#22312;&#36825;&#20010;&#38382;&#39064;&#65292;&#31532;&#19968;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#25361;&#25112;&#65292;&#22312;&#26576;&#20010;&#22320;&#28857;&#30340;&#26412;&#22320;&#25968;&#25454;&#36275;&#20197;&#35757;&#32451;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#31532;&#20108;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#26377;&#25968;&#25454;&#20849;&#20139;&#38480;&#21046;&#65292;&#31532;&#19977;&#65292;&#30001;&#20110;&#23458;&#25143;&#31471;&#31449;&#28857;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#21464;&#65292;&#38656;&#35201;&#23545;&#23398;&#20064;&#30340;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a fast adaptive federated meta-learning (FAM) framework for collaboratively learning a single global model, which can then be personalized locally on individual clients. Federated learning enables multiple clients to collaborate to train a model without sharing data. Clients with insufficient data or data diversity participate in federated learning to learn a model with superior performance. Nonetheless, learning suffers when data distributions diverge. There is a need to learn a global model that can be adapted using client's specific information to create personalised models on clients is required. MRI data suffers from this problem, wherein, one, due to data acquisition challenges, local data at a site is sufficient for training an accurate model and two, there is a restriction of data sharing due to privacy concerns and three, there is a need for personalization of a learnt shared global model on account of domain shift across client sites. The global model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13969</link><description>&lt;p&gt;
&#27880;&#37325;&#27880;&#24847;&#21147;&#65306;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Fixating on Attention: Integrating Human Eye Tracking into Vision Transformers. (arXiv:2308.13969v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22522;&#20110;Transformer&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#22810;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20851;&#38190;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#22270;&#20687;&#35299;&#37322;&#21644;&#33258;&#21160;&#39550;&#39542;&#65292;&#20173;&#28982;&#38656;&#35201;&#20381;&#36182;&#20154;&#31867;&#21028;&#26029;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#31867;&#35270;&#35273;&#36755;&#20837;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#30524;&#21160;&#20202;&#25910;&#38598;&#21040;&#30340;&#27880;&#35270;&#28857;&#65292;&#38598;&#25104;&#21040;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#21644;Vision Transformer&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#27880;&#35270;&#21306;&#22495;&#22312;&#24038;&#21491;&#39550;&#39542;&#20915;&#31574;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#27880;&#35270;&#22270;&#21644;ViT&#27880;&#24847;&#21147;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21333;&#20010;&#22836;&#37096;&#21644;&#23618;&#20043;&#38388;&#30340;&#37325;&#21472;&#21160;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#37325;&#21472;&#21160;&#24577;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#30340;&#20462;&#21098;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#39550;&#39542;&#22330;&#26223;&#20449;&#24687;&#19982;&#27880;&#35270;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#37319;&#29992;&#8220;&#32852;&#21512;&#31354;&#38388;-&#27880;&#35270;&#8221;&#65288;JSF&#65289;&#30340;&#27880;&#24847;&#21147;&#35774;&#32622;&#12290;&#26368;&#21518;
&lt;/p&gt;
&lt;p&gt;
Modern transformer-based models designed for computer vision have outperformed humans across a spectrum of visual tasks. However, critical tasks, such as medical image interpretation or autonomous driving, still require reliance on human judgments. This work demonstrates how human visual input, specifically fixations collected from an eye-tracking device, can be integrated into transformer models to improve accuracy across multiple driving situations and datasets. First, we establish the significance of fixation regions in left-right driving decisions, as observed in both human subjects and a Vision Transformer (ViT). By comparing the similarity between human fixation maps and ViT attention weights, we reveal the dynamics of overlap across individual heads and layers. This overlap is exploited for model pruning without compromising accuracy. Thereafter, we incorporate information from the driving scene with fixation data, employing a "joint space-fixation" (JSF) attention setup. Lastly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25913;&#36827;BERT&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#12289;&#26144;&#23556;&#26041;&#27861;&#21644;&#26435;&#37325;&#35843;&#25972;&#65292;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#65292;&#20174;&#32780;&#21387;&#32553;&#22823;&#22411;Transformer&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.13958</link><description>&lt;p&gt;
&#25913;&#36827;BERT&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65306;&#25439;&#22833;&#20989;&#25968;&#12289;&#26144;&#23556;&#26041;&#27861;&#21644;&#26435;&#37325;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Improving Knowledge Distillation for BERT Models: Loss Functions, Mapping Methods, and Weight Tuning. (arXiv:2308.13958v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25913;&#36827;BERT&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#12289;&#26144;&#23556;&#26041;&#27861;&#21644;&#26435;&#37325;&#35843;&#25972;&#65292;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#65292;&#20174;&#32780;&#21387;&#32553;&#22823;&#22411;Transformer&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#22914;BERT&#12289;GPT&#21644;T5&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38656;&#35201;&#37319;&#29992;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26469;&#20943;&#23567;&#20854;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#26412;&#39033;&#30446;&#30740;&#31350;&#24182;&#24212;&#29992;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;BERT&#27169;&#22411;&#21387;&#32553;&#65292;&#29305;&#21035;&#20851;&#27880;TinyBERT&#23398;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#31181;&#25216;&#26415;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#65292;&#21253;&#25324;&#23454;&#39564;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#12289;Transformer&#23618;&#26144;&#23556;&#26041;&#27861;&#21644;&#35843;&#25972;&#27880;&#24847;&#21147;&#21644;&#34920;&#31034;&#25439;&#22833;&#30340;&#26435;&#37325;&#65292;&#24182;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26412;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24320;&#21457;&#25552;&#20379;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of large transformer-based models such as BERT, GPT, and T5 has led to significant advancements in natural language processing. However, these models are computationally expensive, necessitating model compression techniques that reduce their size and complexity while maintaining accuracy. This project investigates and applies knowledge distillation for BERT model compression, specifically focusing on the TinyBERT student model. We explore various techniques to improve knowledge distillation, including experimentation with loss functions, transformer layer mapping methods, and tuning the weights of attention and representation loss and evaluate our proposed techniques on a selection of downstream tasks from the GLUE benchmark. The goal of this work is to improve the efficiency and effectiveness of knowledge distillation, enabling the development of more efficient and accurate models for a range of natural language processing tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#27169;&#22359;&#21270;&#26435;&#37325;&#21644;&#39046;&#22495;&#36801;&#31227;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#20445;&#25345;&#28304;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#20801;&#35768;&#39640;&#25928;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13957</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#26435;&#37325;&#25513;&#30721;&#29992;&#20110;&#39046;&#22495;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Differentiable Weight Masks for Domain Transfer. (arXiv:2308.13957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#27169;&#22359;&#21270;&#26435;&#37325;&#21644;&#39046;&#22495;&#36801;&#31227;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#20445;&#25345;&#28304;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#20801;&#35768;&#39640;&#25928;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#26080;&#27861;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#20445;&#30041;&#22810;&#20010;&#20449;&#24687;&#28304;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#20010;&#22312;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#36807;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#20445;&#25345;&#20854;&#22312;&#28304;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23558;&#20854;&#37325;&#26032;&#35757;&#32451;&#21040;&#19968;&#20010;&#30456;&#20284;&#20294;&#19981;&#21516;&#30340;&#30446;&#26631;&#20219;&#21153;&#19978;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#32593;&#32476;&#26435;&#37325;&#30340;&#27169;&#22359;&#21270;&#65292;&#20197;&#23450;&#20301;&#21644;&#30830;&#23450;&#23545;&#20110;&#35302;&#21457;&#32473;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#26435;&#37325;&#38598;&#21512;&#12290;&#19968;&#20123;&#24037;&#20316;&#30740;&#31350;&#20102;&#36890;&#36807;&#23398;&#20064;&#21644;&#20998;&#26512;&#26435;&#37325;&#25513;&#30721;&#24341;&#20837;&#30340;&#32593;&#32476;&#26435;&#37325;&#30340;&#27169;&#22359;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#39046;&#22495;&#32467;&#21512;&#36215;&#26469;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#32531;&#35299;&#28304;&#20219;&#21153;&#30340;&#8220;&#36951;&#24536;&#8221;&#21516;&#26102;&#20801;&#35768;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#25513;&#30721;&#25216;&#26415;&#22312;&#20445;&#30041;&#28304;&#20219;&#21153;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major drawbacks of deep learning models for computer vision has been their inability to retain multiple sources of information in a modular fashion. For instance, given a network that has been trained on a source task, we would like to re-train this network on a similar, yet different, target task while maintaining its performance on the source task. Simultaneously, researchers have extensively studied modularization of network weights to localize and identify the set of weights culpable for eliciting the observed performance on a given task. One set of works studies the modularization induced in the weights of a neural network by learning and analysing weight masks. In this work, we combine these fields to study three such weight masking methods and analyse their ability to mitigate "forgetting'' on the source task while also allowing for efficient finetuning on the target task. We find that different masking techniques have trade-offs in retaining knowledge in the source t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13916</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#20247;&#22810;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#19981;&#23436;&#25972;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#19977;&#20803;&#32452;&#35270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30693;&#35782;&#22270;&#35889;LLM&#65288;KG-LLM&#65289;&#65292;&#26469;&#23545;&#36825;&#20123;&#19977;&#20803;&#32452;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;&#19977;&#20803;&#32452;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#31561;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;LLaMA-7B&#65292;ChatGLM-6B&#65289;&#20248;&#20110;&#26368;&#26032;&#30340;ChatGPT&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24191;&#27867;&#30740;&#31350;&#20102;ChatGPT&#27169;&#22411;&#22312;13&#20010;&#24773;&#24863;&#35745;&#31639;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35780;&#20272;ChatGPT&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.13911</link><description>&lt;p&gt;
ChatGPT&#22312;&#24773;&#24863;&#35745;&#31639;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Wide Evaluation of ChatGPT on Affective Computing Tasks. (arXiv:2308.13911v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24191;&#27867;&#30740;&#31350;&#20102;ChatGPT&#27169;&#22411;&#22312;13&#20010;&#24773;&#24863;&#35745;&#31639;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35780;&#20272;ChatGPT&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#19968;&#20010;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#20986;&#29616;&#20102;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#30446;&#30340;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#31034;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#38382;&#39064;&#35757;&#32451;&#21333;&#29420;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#26174;&#31034;&#20986;&#35299;&#20915;&#19968;&#20123;&#26368;&#21021;&#26410;&#32463;&#35757;&#32451;&#30340;&#38382;&#39064;&#30340;&#26032;&#24615;&#36136;&#12290;&#23545;&#20110;&#36825;&#31867;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#30340;&#30740;&#31350;&#36824;&#30456;&#24403;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24191;&#27867;&#30740;&#31350;&#20102;ChatGPT&#27169;&#22411;&#65288;&#21363;GPT-4&#21644;GPT-3.5&#65289;&#22312;13&#20010;&#24773;&#24863;&#35745;&#31639;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#26041;&#38754;&#25552;&#21462;&#12289;&#26041;&#38754;&#26497;&#24615;&#20998;&#31867;&#12289;&#24847;&#35265;&#25552;&#21462;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#24863;&#24378;&#24230;&#25490;&#24207;&#12289;&#24773;&#32490;&#24378;&#24230;&#25490;&#24207;&#12289;&#33258;&#26432;&#20542;&#21521;&#26816;&#27979;&#12289;&#27602;&#24615;&#26816;&#27979;&#12289;&#31119;&#31049;&#35780;&#20272;&#12289;&#21442;&#19982;&#24230;&#27979;&#37327;&#12289;&#20154;&#26684;&#35780;&#20272;&#12289;&#35773;&#21050;&#26816;&#27979;&#21644;&#20027;&#35266;&#24615;&#26816;&#27979;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#35780;&#20272;ChatGPT&#27169;&#22411;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30340;&#26694;&#26550;&#65292;&#27604;&#22914;&#24378;&#24230;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of foundation models, a new artificial intelligence paradigm has emerged, by simply using general purpose foundation models with prompting to solve problems instead of training a separate machine learning model for each problem. Such models have been shown to have emergent properties of solving problems that they were not initially trained on. The studies for the effectiveness of such models are still quite limited. In this work, we widely study the capabilities of the ChatGPT models, namely GPT-4 and GPT-3.5, on 13 affective computing problems, namely aspect extraction, aspect polarity classification, opinion extraction, sentiment analysis, sentiment intensity ranking, emotions intensity ranking, suicide tendency detection, toxicity detection, well-being assessment, engagement measurement, personality assessment, sarcasm detection, and subjectivity detection. We introduce a framework to evaluate the ChatGPT models on regression-based problems, such as intensity ranking p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25506;&#32034;&#20102;&#22312;&#35270;&#39057;&#20013;&#36861;&#36394;&#20154;&#31867;&#32676;&#20307;&#27169;&#24335;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36827;&#32780;&#22686;&#24378;&#23433;&#20840;&#21644;&#30417;&#35270;&#12290;&#36890;&#36807;&#23558;&#36816;&#21160;&#20998;&#20026;&#19981;&#21516;&#30340;&#24359;&#24418;&#12289;&#36710;&#36947;&#12289;&#27719;&#32858;/&#20998;&#25955;&#21644;&#38543;&#26426;/&#38459;&#22622;&#36816;&#21160;&#65292;&#24182;&#20351;&#29992;&#20809;&#27969;&#25216;&#26415;&#12289;CNN&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30446;&#26631;&#65292;&#24182;&#33719;&#24471;&#20102;&#20855;&#26377;&#21069;&#26223;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12289;&#22522;&#20110;&#36816;&#21160;&#25552;&#20379;&#34892;&#20026;&#27934;&#23519;&#65292;&#24182;&#22686;&#24378;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13910</link><description>&lt;p&gt;
&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25506;&#32034;&#35270;&#39057;&#20013;&#20154;&#31867;&#32676;&#20307;&#27169;&#24335;&#21644;&#20998;&#31867;&#65292;&#20197;&#22686;&#24378;&#23433;&#20840;&#21644;&#30417;&#35270;
&lt;/p&gt;
&lt;p&gt;
Exploring Human Crowd Patterns and Categorization in Video Footage for Enhanced Security and Surveillance using Computer Vision and Machine Learning. (arXiv:2308.13910v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25506;&#32034;&#20102;&#22312;&#35270;&#39057;&#20013;&#36861;&#36394;&#20154;&#31867;&#32676;&#20307;&#27169;&#24335;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36827;&#32780;&#22686;&#24378;&#23433;&#20840;&#21644;&#30417;&#35270;&#12290;&#36890;&#36807;&#23558;&#36816;&#21160;&#20998;&#20026;&#19981;&#21516;&#30340;&#24359;&#24418;&#12289;&#36710;&#36947;&#12289;&#27719;&#32858;/&#20998;&#25955;&#21644;&#38543;&#26426;/&#38459;&#22622;&#36816;&#21160;&#65292;&#24182;&#20351;&#29992;&#20809;&#27969;&#25216;&#26415;&#12289;CNN&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30446;&#26631;&#65292;&#24182;&#33719;&#24471;&#20102;&#20855;&#26377;&#21069;&#26223;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12289;&#22522;&#20110;&#36816;&#21160;&#25552;&#20379;&#34892;&#20026;&#27934;&#23519;&#65292;&#24182;&#22686;&#24378;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#31185;&#23398;&#23478;&#21644;&#26222;&#36890;&#27665;&#20247;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#24863;&#30693;&#21464;&#38761;&#12290;&#36825;&#20123;&#25216;&#26415;&#26366;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#20225;&#21450;&#30340;&#65292;&#20294;&#22914;&#20170;&#21364;&#21462;&#24471;&#20102;&#30475;&#20284;&#19981;&#21487;&#33021;&#30340;&#25104;&#23601;&#12290;&#23427;&#20204;&#22312;&#23433;&#20840;&#12289;&#20892;&#19994;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#21331;&#36234;&#24212;&#29992;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#24433;&#21709;&#21147;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#20840;&#37096;&#28508;&#21147;&#23578;&#26410;&#34987;&#20805;&#20998;&#21457;&#25496;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#23433;&#20840;&#21644;&#30417;&#35270;&#20013;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#39057;&#36816;&#21160;&#36861;&#36394;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#36816;&#21160;&#20449;&#24687;&#22270;&#20687;&#21644;&#20998;&#22359;&#20027;&#23548;&#36816;&#21160;&#25968;&#25454;&#23558;&#36816;&#21160;&#20998;&#20026;&#24359;&#24418;&#12289;&#36710;&#36947;&#12289;&#27719;&#32858;/&#20998;&#25955;&#21644;&#38543;&#26426;/&#38459;&#22622;&#36816;&#21160;&#65292;&#26412;&#25991;&#32771;&#23519;&#20102;&#19981;&#21516;&#30340;&#20809;&#27969;&#25216;&#26415;&#12289;CNN&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20197;&#24456;&#39640;&#31934;&#24230;&#36798;&#25104;&#30446;&#26631;&#65292;&#26412;&#25991;&#30340;&#32467;&#26524;&#21487;&#20197;&#35757;&#32451;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#26681;&#25454;&#36816;&#21160;&#25552;&#20379;&#34892;&#20026;&#27934;&#23519;&#65292;&#24182;&#22686;&#24378;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision and machine learning have brought revolutionary shifts in perception for researchers, scientists, and the general populace. Once thought to be unattainable, these technologies have achieved the seemingly impossible. Their exceptional applications in diverse fields like security, agriculture, and education are a testament to their impact. However, the full potential of computer vision remains untapped. This paper explores computer vision's potential in security and surveillance, presenting a novel approach to track motion in videos. By categorizing motion into Arcs, Lanes, Converging/Diverging, and Random/Block motions using Motion Information Images and Blockwise dominant motion data, the paper examines different optical flow techniques, CNN models, and machine learning models. Successfully achieving its objectives with promising accuracy, the results can train anomaly-detection models, provide behavioral insights based on motion, and enhance scene comprehension.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;&#65288;FedLLM&#65289;&#30340;&#25928;&#29575;&#12290;FwdLLM&#36890;&#36807;&#20351;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#26469;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13894</link><description>&lt;p&gt;
&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Federated Fine-tuning of Billion-Sized Language Models across Mobile Devices. (arXiv:2308.13894v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13894
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;&#65288;FedLLM&#65289;&#30340;&#25928;&#29575;&#12290;FwdLLM&#36890;&#36807;&#20351;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#26469;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#31227;&#21160;&#26234;&#33021;&#30340;&#26684;&#23616;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#29992;&#20110;&#23545;&#19979;&#28216;&#31227;&#21160;&#20219;&#21153;&#36827;&#34892;LLM&#30340;&#24494;&#35843;&#65292;&#36825;&#34987;&#31216;&#20026;FedLLM&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#30001;&#24222;&#22823;&#27169;&#22411;&#22823;&#23567;&#24341;&#36215;&#30340;&#32593;&#32476;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22312;&#19982;&#31227;&#21160;&#35774;&#22791;&#30340;&#25972;&#21512;&#26041;&#38754;&#24182;&#27809;&#26377;&#23454;&#38469;&#32531;&#35299;&#35832;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#26174;&#33879;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#32531;&#24930;&#30340;&#27169;&#22411;&#25910;&#25947;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;FedLLM&#30340;&#25928;&#29575;&#12290;FwdLLM&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#37319;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#35774;&#22791;&#25191;&#34892;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#12290;&#22240;&#27492;&#65292;FwdLLM&#20855;&#26377;&#26356;&#22909;&#30340;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#65288;&#36890;&#36807;&#31227;&#21160;NPUs&#21644;&#25193;&#22823;&#30340;&#21442;&#19982;&#35774;&#22791;&#25968;&#32452;&#65289;&#12290;FwdLLM&#22260;&#32469;&#19977;&#20010;&#20851;&#38190;&#35774;&#35745;&#23637;&#24320;&#65306;&#65288;1&#65289;&#23558;&#26080;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#19982;p
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are transforming the landscape of mobile intelligence. Federated Learning (FL), a method to preserve user data privacy, is often employed in fine-tuning LLMs to downstream mobile tasks, an approach known as FedLLM. Though recent efforts have addressed the network issue induced by the vast model size, they have not practically mitigated vital challenges concerning integration with mobile devices, such as significant memory consumption and sluggish model convergence.  In response to these challenges, this work introduces FwdLLM, an innovative FL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLM to employ backpropagation (BP)-free training methods, requiring devices only to execute ``perturbed inferences''. Consequently, FwdLLM delivers way better memory efficiency and time efficiency (expedited by mobile NPUs and an expanded array of participant devices). FwdLLM centers around three key designs: (1) it combines BP-free training with p
&lt;/p&gt;</description></item><item><title>DiffuseStyleGesture+&#26159;&#23545;GENEA&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#20250;&#35805;&#25163;&#21183;&#65292;&#24182;&#22312;&#20154;&#31867;&#31867;&#20284;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#19982;&#39030;&#32423;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13879</link><description>&lt;p&gt;
&#12298;GENEA Challenge 2023&#12299;&#20013;&#30340;DiffuseStyleGesture+&#20837;&#38376;
&lt;/p&gt;
&lt;p&gt;
The DiffuseStyleGesture+ entry to the GENEA Challenge 2023. (arXiv:2308.13879v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13879
&lt;/p&gt;
&lt;p&gt;
DiffuseStyleGesture+&#26159;&#23545;GENEA&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#20250;&#35805;&#25163;&#21183;&#65292;&#24182;&#22312;&#20154;&#31867;&#31867;&#20284;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#19982;&#39030;&#32423;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffuseStyleGesture+&#65292;&#36825;&#26159;&#25105;&#20204;&#23545;&#20110;&#12298;GENEA Challenge 2023&#12299;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#25361;&#25112;&#26088;&#22312;&#20419;&#36827;&#29983;&#25104;&#20250;&#35805;&#25163;&#21183;&#30340;&#29616;&#23454;&#12289;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#21442;&#19982;&#32773;&#23558;&#33719;&#24471;&#19968;&#20010;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20247;&#21253;&#35780;&#20998;&#26469;&#35780;&#20272;&#20854;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;DiffuseStyleGesture+&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#25163;&#21183;&#12290;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#38899;&#39057;&#12289;&#25991;&#26412;&#12289;&#35828;&#35805;&#32773;&#36523;&#20221;&#21644;&#31181;&#23376;&#25163;&#21183;&#12290;&#36825;&#20123;&#22810;&#26679;&#30340;&#27169;&#24577;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#38544;&#34255;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#38024;&#23545;&#32473;&#23450;&#30340;&#35821;&#38899;&#36755;&#20837;&#29983;&#25104;&#30456;&#24212;&#30340;&#25163;&#21183;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;DiffuseStyleGesture+&#23637;&#31034;&#20102;&#19982;&#25361;&#25112;&#20013;&#39030;&#32423;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#34920;&#29616;&#20986;&#19982;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#31867;&#31867;&#20284;&#24615;&#12289;&#36866;&#24212;&#23545;&#35805;&#32773;&#30340;&#21512;&#36866;&#24615;&#21644;&#23454;&#29616;&#36798;&#21040;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the DiffuseStyleGesture+, our solution for the Generation and Evaluation of Non-verbal Behavior for Embodied Agents (GENEA) Challenge 2023, which aims to foster the development of realistic, automated systems for generating conversational gestures. Participants are provided with a pre-processed dataset and their systems are evaluated through crowdsourced scoring. Our proposed model, DiffuseStyleGesture+, leverages a diffusion model to generate gestures automatically. It incorporates a variety of modalities, including audio, text, speaker ID, and seed gestures. These diverse modalities are mapped to a hidden space and processed by a modified diffusion model to produce the corresponding gesture for a given speech input. Upon evaluation, the DiffuseStyleGesture+ demonstrated performance on par with the top-tier models in the challenge, showing no significant differences with those models in human-likeness, appropriateness for the interlocutor, and achieving com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21516;&#30340;&#20851;&#27880;&#26041;&#24335;&#23398;&#20064;&#22270;&#32534;&#36753;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22270;&#32423;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#33410;&#28857;&#32423;&#34701;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13871</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21516;&#30340;&#20851;&#27880;&#26041;&#24335;&#23398;&#20064;&#22270;&#32534;&#36753;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Graph Edit Distance Learning via Different Attention. (arXiv:2308.13871v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21516;&#30340;&#20851;&#27880;&#26041;&#24335;&#23398;&#20064;&#22270;&#32534;&#36753;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22270;&#32423;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#33410;&#28857;&#32423;&#34701;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#35299;&#20915;&#22270;&#30456;&#20284;&#24615;&#35745;&#31639;&#38382;&#39064;&#65288;GSC&#65289;&#65292;&#21363;&#35745;&#31639;&#20004;&#20010;&#22270;&#20043;&#38388;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#65288;GED&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#25226;GSC&#35270;&#20026;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#65292;&#20854;&#26680;&#24515;&#26159;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#19982;&#20004;&#20010;&#22270;&#30340;&#29305;&#24449;&#20132;&#20114;&#12290;&#29616;&#26377;&#26041;&#27861;&#35748;&#20026;&#65292;&#22270;&#32423;&#23884;&#20837;&#24456;&#38590;&#25429;&#25417;&#20004;&#20010;&#22270;&#20043;&#38388;&#23616;&#37096;&#23567;&#32467;&#26500;&#30340;&#24046;&#24322;&#65292;&#22240;&#27492;&#23545;&#33410;&#28857;&#32423;&#23884;&#20837;&#36827;&#34892;&#32454;&#31890;&#24230;&#29305;&#24449;&#34701;&#21512;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#32423;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;Different Attention&#65288;DiffAtt&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#22270;&#32423;&#34701;&#21512;&#23884;&#20837;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#36825;&#20123;&#22797;&#26434;&#30340;&#33410;&#28857;&#32423;&#34701;&#21512;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, more and more research has focused on using Graph Neural Networks (GNN) to solve the Graph Similarity Computation problem (GSC), i.e., computing the Graph Edit Distance (GED) between two graphs. These methods treat GSC as an end-to-end learnable task, and the core of their architecture is the feature fusion modules to interact with the features of two graphs. Existing methods consider that graph-level embedding is difficult to capture the differences in local small structures between two graphs, and thus perform fine-grained feature fusion on node-level embedding can improve the accuracy, but leads to greater time and memory consumption in the training and inference phases. However, this paper proposes a novel graph-level fusion module Different Attention (DiffAtt), and demonstrates that graph-level fusion embeddings can substantially outperform these complex node-level fusion embeddings. We posit that the relative difference structure of the two graphs plays an important rol
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#21644;&#20998;&#35010;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24322;&#26500;&#24615;&#24102;&#26469;&#30340;&#35757;&#32451;&#36895;&#24230;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13849</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65306;&#22522;&#20110;&#37197;&#23545;&#21644;&#20998;&#35010;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Effectively Heterogeneous Federated Learning: A Pairing and Split Learning Based Approach. (arXiv:2308.13849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#21644;&#20998;&#35010;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24322;&#26500;&#24615;&#24102;&#26469;&#30340;&#35757;&#32451;&#36895;&#24230;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#20998;&#24067;&#24335;&#35774;&#22791;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#26102;&#36991;&#20813;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#31471;&#30340;&#24322;&#26500;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064;&#23384;&#22312;&#35757;&#32451;&#36895;&#24230;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#24310;&#36831;&#21644;&#26381;&#21153;&#22120;&#32858;&#21512;&#30340;&#24310;&#36831;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#35745;&#31639;&#36164;&#28304;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36890;&#20449;&#36895;&#29575;&#23558;&#23458;&#25143;&#31471;&#36827;&#34892;&#37197;&#23545;&#65292;&#24182;&#22312;&#36923;&#36753;&#23618;&#23558;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#25104;&#20004;&#37096;&#20998;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#21482;&#35745;&#31639;&#20854;&#20998;&#37197;&#30340;&#37096;&#20998;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#35010;&#23398;&#20064;&#23454;&#29616;&#21069;&#21521;&#25512;&#29702;&#21644;&#21518;&#21521;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#23458;&#25143;&#31471;&#37197;&#23545;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#23146;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26500;&#35757;&#32451;&#24310;&#36831;&#20248;&#21270;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a promising paradigm federated Learning (FL) is widely used in privacy-preserving machine learning, which allows distributed devices to collaboratively train a model while avoiding data transmission among clients. Despite its immense potential, the FL suffers from bottlenecks in training speed due to client heterogeneity, leading to escalated training latency and straggling server aggregation. To deal with this challenge, a novel split federated learning (SFL) framework that pairs clients with different computational resources is proposed, where clients are paired based on computing resources and communication rates among clients, meanwhile the neural network model is split into two parts at the logical level, and each client only computes the part assigned to it by using the SL to achieve forward inference and backward training. Moreover, to effectively deal with the client pairing problem, a heuristic greedy algorithm is proposed by reconstructing the optimization of training late
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20026;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#21457;&#29616;&#20102;&#20445;&#25345;&#32467;&#26500;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#23884;&#20837;&#65292;&#36890;&#36807;&#36763;&#21464;&#25442;&#33719;&#24471;&#32039;&#20945;&#36763;&#23884;&#20837;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20854;&#21160;&#21147;&#23398;&#30340;&#26377;&#30028;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13835</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#20445;&#25345;&#32467;&#26500;&#30340;&#38750;&#32447;&#24615;&#26631;&#20934;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#31283;&#23450;&#24211;&#26222;&#26364;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Structure-Preserving Universal Stable Koopman-Inspired Embeddings for Nonlinear Canonical Hamiltonian Dynamics. (arXiv:2308.13835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20026;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#21457;&#29616;&#20102;&#20445;&#25345;&#32467;&#26500;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#23884;&#20837;&#65292;&#36890;&#36807;&#36763;&#21464;&#25442;&#33719;&#24471;&#32039;&#20945;&#36763;&#23884;&#20837;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20854;&#21160;&#21147;&#23398;&#30340;&#26377;&#30028;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#36866;&#24403;&#22352;&#26631;&#21464;&#25442;&#21487;&#20197;&#26500;&#24314;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#20415;&#20110;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#39044;&#27979;&#12289;&#25511;&#21046;&#21644;&#20248;&#21270;&#12290;&#20026;&#27492;&#65292;&#24211;&#26222;&#26364;&#31639;&#23376;&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#30340;&#26694;&#26550;&#65292;&#20174;&#32780;&#20801;&#35768;&#20351;&#29992;&#32447;&#24615;&#24037;&#20855;&#36827;&#34892;&#35774;&#35745;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36890;&#36807;&#36763;&#21464;&#25442;&#35782;&#21035;&#26631;&#20934;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#23884;&#20837;&#12290;&#23613;&#31649;&#36825;&#20010;&#20219;&#21153;&#36890;&#24120;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#26469;&#21457;&#29616;&#25152;&#38656;&#30340;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20811;&#26381;&#24211;&#26222;&#26364;&#31639;&#23376;&#22312;&#20855;&#26377;&#36830;&#32493;&#39057;&#35889;&#30340;&#31995;&#32479;&#20013;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#25260;&#21319;&#21407;&#29702;&#24182;&#23398;&#20064;&#20840;&#23616;&#30340;&#31435;&#26041;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30528;&#37325;&#24378;&#35843;&#20102;&#25152;&#21457;&#29616;&#23884;&#20837;&#30340;&#21160;&#21147;&#23398;&#30340;&#26377;&#30028;&#31283;&#23450;&#24615;&#30340;&#24378;&#21046;&#24615;&#35201;&#27714;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33719;&#21462;&#32039;&#20945;&#36763;&#23884;&#20837;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering a suitable coordinate transformation for nonlinear systems enables the construction of simpler models, facilitating prediction, control, and optimization for complex nonlinear systems. To that end, Koopman operator theory offers a framework for global linearization for nonlinear systems, thereby allowing the usage of linear tools for design studies. In this work, we focus on the identification of global linearized embeddings for canonical nonlinear Hamiltonian systems through a symplectic transformation. While this task is often challenging, we leverage the power of deep learning to discover the desired embeddings. Furthermore, to overcome the shortcomings of Koopman operators for systems with continuous spectra, we apply the lifting principle and learn global cubicized embeddings. Additionally, a key emphasis is paid to enforce the bounded stability for the dynamics of the discovered embeddings. We demonstrate the capabilities of deep learning in acquiring compact symplect
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#65292;&#26088;&#22312;&#32416;&#27491;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13821</link><description>&lt;p&gt;
&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#32508;&#36848;&#65306;&#38382;&#39064;&#12289;&#25216;&#26415;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions. (arXiv:2308.13821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#65292;&#26088;&#22312;&#32416;&#27491;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#19982;&#19990;&#30028;&#21508;&#31181;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#30456;&#20114;&#36830;&#25509;&#30340;&#32467;&#26500;&#12290;&#26377;&#25928;&#30340;&#22270;&#20998;&#26512;&#25216;&#26415;&#65292;&#22914;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#20174;&#22270;&#25968;&#25454;&#20013;&#33719;&#24471;&#28145;&#21051;&#30340;&#27934;&#23519;&#21147;&#65292;&#20026;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#36335;&#39044;&#27979;&#31561;&#21508;&#31181;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#22270;&#25968;&#25454;&#20013;&#26576;&#20123;&#29255;&#27573;&#25317;&#26377;&#22823;&#37327;&#25968;&#25454;&#32780;&#20854;&#20182;&#25968;&#25454;&#31232;&#32570;&#65292;&#20174;&#32780;&#23548;&#33268;&#20559;&#20506;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#36825;&#23601;&#38656;&#35201;&#20986;&#29616;&#20102;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#26088;&#22312;&#32416;&#27491;&#36825;&#20123;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#23545;&#35813;&#27010;&#24565;&#21644;&#30456;&#20851;&#26415;&#35821;&#30340;&#26126;&#30830;&#29702;&#35299;&#65292;&#20026;&#35835;&#32773;&#24314;&#31435;&#20102;&#25166;&#23454;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65306;&#65288;1&#65289;&#38382;&#39064;&#20998;&#31867;&#27861;&#65288;Problem Taxonomy&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs represent interconnected structures prevalent in a myriad of real-world scenarios. Effective graph analytics, such as graph learning methods, enables users to gain profound insights from graph data, underpinning various tasks including node classification and link prediction. However, these methods often suffer from data imbalance, a common issue in graph data where certain segments possess abundant data while others are scarce, thereby leading to biased learning outcomes. This necessitates the emerging field of imbalanced learning on graphs, which aims to correct these data distribution skews for more accurate and representative learning outcomes. In this survey, we embark on a comprehensive review of the literature on imbalanced learning on graphs. We begin by providing a definitive understanding of the concept and related terminologies, establishing a strong foundational understanding for readers. Following this, we propose two comprehensive taxonomies: (1) the problem taxono
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#29615;&#35856;&#25391;&#22120;&#30340;&#20840;&#20809;&#20648;&#22791;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26368;&#22823;&#21270;&#24310;&#36831;-&#24102;&#23485;&#31215;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#20809;&#20998;&#32452;&#22836;&#35782;&#21035;&#12290;&#20248;&#21270;&#21518;&#30340;&#32423;&#32852;&#29615;&#21487;&#29992;&#20110;&#23454;&#29616;&#23567;&#23610;&#23544;&#12289;&#24179;&#39030;&#24310;&#36831;&#35889;&#30340;&#20809;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2308.13818</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30340;&#21452;&#29615;&#35856;&#25391;&#22120;&#30340;&#20840;&#20809;&#20648;&#22791;&#30340;&#20998;&#32452;&#22836;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Packet Header Recognition Utilizing an All-Optical Reservoir Based on Reinforcement-Learning-Optimized Double-Ring Resonator. (arXiv:2308.13818v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#29615;&#35856;&#25391;&#22120;&#30340;&#20840;&#20809;&#20648;&#22791;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26368;&#22823;&#21270;&#24310;&#36831;-&#24102;&#23485;&#31215;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#20809;&#20998;&#32452;&#22836;&#35782;&#21035;&#12290;&#20248;&#21270;&#21518;&#30340;&#32423;&#32852;&#29615;&#21487;&#29992;&#20110;&#23454;&#29616;&#23567;&#23610;&#23544;&#12289;&#24179;&#39030;&#24310;&#36831;&#35889;&#30340;&#20809;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20998;&#32452;&#22836;&#35782;&#21035;&#26159;&#20809;&#36890;&#20449;&#32593;&#32476;&#20013;&#37325;&#35201;&#30340;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#20809;&#20648;&#22791;&#31995;&#32479;&#65292;&#20854;&#20013;&#38598;&#25104;&#20102;&#21452;&#29615;&#35856;&#25391;&#22120;&#20316;&#20026;&#33410;&#28857;&#65292;&#29992;&#20110;&#24555;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#20809;&#20998;&#32452;&#22836;&#12290;&#30001;&#20110;&#33410;&#28857;&#30340;&#24310;&#36831;-&#24102;&#23485;&#31215;&#65288;DBP&#65289;&#26159;&#20648;&#22791;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#21442;&#25968;&#65292;&#25105;&#20204;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#21452;&#29615;&#35856;&#25391;&#22120;&#36827;&#34892;DBP&#30340;&#26368;&#22823;&#21270;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#20840;&#21442;&#25968;&#31354;&#38388;&#20248;&#21270;&#21644;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#30340;&#20248;&#21183;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#32423;&#32852;&#12289;&#24182;&#32852;&#21644;&#23884;&#20837;&#24335;&#37197;&#32622;&#30340;&#20248;&#21270;DBP&#36798;&#21040;&#20102;&#30456;&#21516;&#30340;&#26368;&#22823;&#20540;&#65292;&#34987;&#35748;&#20026;&#26159;&#20840;&#23616;&#26368;&#22823;&#20540;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#20248;&#21270;&#30340;&#32423;&#32852;&#29615;&#32452;&#25104;&#30340;&#20840;&#20809;&#20648;&#22791;&#31995;&#32479;&#36827;&#34892;&#20102;3&#20301;&#21644;6&#20301;&#20998;&#32452;&#22836;&#35782;&#21035;&#20219;&#21153;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#22823;&#22823;&#20943;&#23567;&#30340;&#33455;&#29255;&#23610;&#23544;&#21644;&#25152;&#38656;&#30340;&#8220;&#24179;&#39030;&#8221;&#24310;&#36831;&#35889;&#12290;&#21033;&#29992;&#36825;&#31181;&#20809;&#35745;&#31639;&#26041;&#26696;&#65292;&#23383;&#38169;&#35823;&#29575;
&lt;/p&gt;
&lt;p&gt;
Optical packet header recognition is an important signal processing task of optical communication networks. In this work, we propose an all-optical reservoir, consisting of integrated double-ring resonators (DRRs) as nodes, for fast and accurate optical packet header recognition. As the delay-bandwidth product (DBP) of the node is a key figure-of-merit in the reservoir, we adopt a deep reinforcement learning algorithm to maximize the DBPs for various types of DRRs, which has the advantage of full parameter space optimization and fast convergence speed. Intriguingly, the optimized DBPs of the DRRs in cascaded, parallel, and embedded configurations reach the same maximum value, which is believed to be the global maximum. Finally, 3-bit and 6-bit packet header recognition tasks are performed with the all-optical reservoir consisting of the optimized cascaded rings, which have greatly reduced chip size and the desired "flat-top" delay spectra. Using this optical computing scheme, word-erro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13816</link><description>&lt;p&gt;
&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Homological Convolutional Neural Networks. (arXiv:2308.13816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13816
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#65292;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#27604;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#19988;&#21516;&#26679;&#26377;&#25928;&#12290;&#36825;&#26159;&#22240;&#20026;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#38388;&#30340;&#30456;&#20851;&#24615;&#27604;&#22270;&#20687;&#25110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31354;&#38388;&#25110;&#35821;&#20041;&#20851;&#31995;&#35201;&#24369;&#65292;&#32780;&#19988;&#38656;&#35201;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#23545;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#25152;&#24471;&#27169;&#22411;&#21033;&#29992;&#20102;&#21367;&#31215;&#30340;&#33021;&#21147;&#65292;&#24182;&#22260;&#32469;&#32593;&#32476;&#25299;&#25169;&#20013;&#30340;&#26377;&#38480;&#27010;&#24565;&#26469;&#20445;&#35777;&#65288;i&#65289;&#25968;&#25454;&#19968;&#33268;&#24615;&#12289;&#65288;ii&#65289;&#34920;&#31034;&#26377;&#25928;&#24615;&#12289;&#21644;&#65288;iii&#65289;&#23481;&#37327;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have demonstrated outstanding performances on classification and regression tasks on homogeneous data types (e.g., image, audio, and text data). However, tabular data still poses a challenge with classic machine learning approaches being often computationally cheaper and equally effective than increasingly complex deep learning architectures. The challenge arises from the fact that, in tabular data, the correlation among features is weaker than the one from spatial or semantic relationships in images or natural languages, and the dependency structures need to be modeled without any prior information. In this work, we propose a novel deep learning architecture that exploits the data structural organization through topologically constrained network representations to gain spatial information from sparse tabular data. The resulting model leverages the power of convolutions and is centered on a limited number of concepts from network topology to guarantee (i) a data-c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;Dysen&#65289;&#26469;&#22686;&#24378;&#38754;&#21521;&#21160;&#24577;&#24863;&#30693;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#65292;&#36890;&#36807;&#25552;&#21462;&#20851;&#38190;&#21160;&#20316;&#12289;&#24314;&#31435;&#21160;&#24577;&#22330;&#26223;&#22270;&#21644;&#20016;&#23500;&#32454;&#33410;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;T2V&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.13812</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#38754;&#21521;&#21160;&#24577;&#24863;&#30693;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models. (arXiv:2308.13812v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;Dysen&#65289;&#26469;&#22686;&#24378;&#38754;&#21521;&#21160;&#24577;&#24863;&#30693;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#65292;&#36890;&#36807;&#25552;&#21462;&#20851;&#38190;&#21160;&#20316;&#12289;&#24314;&#31435;&#21160;&#24577;&#22330;&#26223;&#22270;&#21644;&#20016;&#23500;&#32454;&#33410;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;T2V&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#21512;&#25104;&#22312;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#26368;&#36817;&#20986;&#29616;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#27604;&#36807;&#21435;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#24378;&#22823;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;DMs&#33021;&#22815;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#30340;&#35270;&#39057;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#24314;&#27169;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#20027;&#35201;&#38480;&#21046;&#65288;&#20363;&#22914;&#65292;&#21160;&#20316;&#20986;&#29616;&#38556;&#30861;&#65292;&#31895;&#31961;&#30340;&#35270;&#39057;&#36816;&#21160;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21152;&#24378;DMs&#23545;&#35270;&#39057;&#21160;&#24577;&#30340;&#24863;&#30693;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;T2V&#29983;&#25104;&#12290;&#21463;&#21040;&#20154;&#31867;&#30452;&#35273;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#21160;&#24577;&#22330;&#26223;&#31649;&#29702;&#22120;&#65288;&#31216;&#20026;Dysen&#65289;&#27169;&#22359;&#65292;&#20854;&#20013;&#21253;&#25324;&#65288;&#27493;&#39588;1&#65289;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21462;&#20855;&#26377;&#36866;&#24403;&#26102;&#38388;&#39034;&#24207;&#23433;&#25490;&#30340;&#20851;&#38190;&#21160;&#20316;&#65292;&#65288;&#27493;&#39588;2&#65289;&#23558;&#21160;&#20316;&#26102;&#38388;&#34920;&#36716;&#21270;&#20026;&#21160;&#24577;&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#34920;&#31034;&#65292;&#20197;&#21450;&#65288;&#27493;&#39588;3&#65289;&#20016;&#23500;DSG&#20013;&#30340;&#22330;&#26223;&#24182;&#25552;&#20379;&#20805;&#20998;&#21644;&#21512;&#29702;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-video (T2V) synthesis has gained increasing attention in the community, in which the recently emerged diffusion models (DMs) have promisingly shown stronger performance than the past approaches. While existing state-of-the-art DMs are competent to achieve high-resolution video generation, they may largely suffer from key limitations (e.g., action occurrence disorders, crude video motions) with respect to the intricate temporal dynamics modeling, one of the crux of video synthesis. In this work, we investigate strengthening the awareness of video dynamics for DMs, for high-quality T2V generation. Inspired by human intuition, we design an innovative dynamic scene manager (dubbed as Dysen) module, which includes (step-1) extracting from input text the key actions with proper time-order arrangement, (step-2) transforming the action schedules into the dynamic scene graph (DSG) representations, and (step-3) enriching the scenes in the DSG with sufficient and reasonable details. Takin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#38598;&#39046;&#22495;&#20013;&#30340;&#26032;&#31867;&#21035;&#21457;&#29616;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#37319;&#29992;&#25104;&#21592;-&#39046;&#23548;&#32773;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#20174;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#25552;&#21462;&#21644;&#34701;&#21512;&#29305;&#24449;&#65292;&#36827;&#19968;&#27493;&#34701;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20197;&#22686;&#24378;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#26041;&#27861;&#26469;&#23454;&#29616;&#23545;&#28508;&#22312;&#26032;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.13801</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#26032;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Based Multi-modal Feature Fusion Network for Novel Class Discovery. (arXiv:2308.13801v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#38598;&#39046;&#22495;&#20013;&#30340;&#26032;&#31867;&#21035;&#21457;&#29616;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#37319;&#29992;&#25104;&#21592;-&#39046;&#23548;&#32773;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#20174;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#25552;&#21462;&#21644;&#34701;&#21512;&#29305;&#24449;&#65292;&#36827;&#19968;&#27493;&#34701;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20197;&#22686;&#24378;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#26041;&#27861;&#26469;&#23454;&#29616;&#23545;&#28508;&#22312;&#26032;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#20154;&#21592;&#20026;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#35774;&#35745;&#20102;&#35768;&#22810;&#30456;&#24212;&#30340;&#27169;&#22411;&#65292;&#22312;&#30417;&#30563;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#21508;&#20010;&#39046;&#22495;&#25968;&#25454;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#36880;&#28176;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#24320;&#25918;&#38598;&#39046;&#22495;&#20013;&#30340;&#26032;&#31867;&#21035;&#21457;&#29616;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#25104;&#21592;-&#39046;&#23548;&#32773;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#20174;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#25552;&#21462;&#21644;&#34701;&#21512;&#29305;&#24449;&#65292;&#26088;&#22312;&#33719;&#24471;&#23545;&#29305;&#24449;&#31354;&#38388;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#20419;&#36827;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#34701;&#20837;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#32858;&#31867;&#26041;&#27861;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#20005;&#26684;&#21040;&#26494;&#25955;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25506;&#32034;&#21644;&#35782;&#21035;&#28508;&#22312;&#30340;&#26032;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of deep learning techniques, supervised learning has achieved performances surpassing those of humans. Researchers have designed numerous corresponding models for different data modalities, achieving excellent results in supervised tasks. However, with the exponential increase of data in multiple fields, the recognition and classification of unlabeled data have gradually become a hot topic. In this paper, we employed a Reinforcement Learning framework to simulate the cognitive processes of humans for effectively addressing novel class discovery in the Open-set domain. We deployed a Member-to-Leader Multi-Agent framework to extract and fuse features from multi-modal information, aiming to acquire a more comprehensive understanding of the feature space. Furthermore, this approach facilitated the incorporation of self-supervised learning to enhance model training. We employed a clustering method with varying constraint conditions, ranging from strict to loose, allowin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;Logical-GLM&#65292;&#29992;&#20110;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#27491;&#30830;&#36923;&#36753;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Logical-GLM&#22312;&#20351;&#29992;&#36739;&#23569;&#25968;&#25454;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.13782</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#36923;&#36753;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#29983;&#25104;&#30340;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning with Logical Graph-based Language Model for Instruction Generation. (arXiv:2308.13782v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;Logical-GLM&#65292;&#29992;&#20110;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#27491;&#30830;&#36923;&#36753;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;&#25552;&#39640;&#25991;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Logical-GLM&#22312;&#20351;&#29992;&#36739;&#23569;&#25968;&#25454;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#31070;&#32463;&#27169;&#22411;&#38590;&#20197;&#20174;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#20013;&#25429;&#25417;&#21040;&#38544;&#21547;&#30340;&#35268;&#21017;&#65292;&#22240;&#27492;&#24456;&#38590;&#29983;&#25104;&#20855;&#26377;&#27491;&#30830;&#36923;&#36753;&#30340;&#25991;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;Logical-GLM&#65292;&#23558;&#36923;&#36753;&#27880;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#26356;&#26377;&#25928;&#30340;&#25991;&#26412;&#29983;&#25104;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#26500;&#24314;&#36890;&#24120;&#25551;&#36848;&#39046;&#22495;&#30340;&#36923;&#36753;&#36125;&#21494;&#26031;&#22270;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#29983;&#25104;&#36923;&#36753;&#39592;&#26550;&#20197;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20132;&#26367;&#20248;&#21270;&#22270;&#30340;&#25628;&#32034;&#31574;&#30053;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#30452;&#33267;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Logical-GLM&#19982;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#23613;&#31649;&#20351;&#29992;&#35268;&#27169;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36739;&#23569;&#30340;&#21442;&#25968;&#65292;&#20173;&#28982;&#20855;&#26377;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26377;&#25928;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the superior performance of large language models to generate natural language texts, it is hard to generate texts with correct logic according to a given task, due to the difficulties for neural models to capture implied rules from free-form texts. In this paper, we propose a novel graph-based language model, Logical-GLM, to infuse logic into language models for more valid text generation and interpretability. Specifically, we first capture information from natural language instructions and construct logical bayes graphs that generally describe domains. Next, we generate logical skeletons to guide language model training, infusing domain knowledge into language models. Finally, we alternately optimize the searching policy of graphs and language models until convergence. The experimental results show that Logical-GLM is both effective and efficient compared with traditional language models, despite using smaller-scale training data and fewer parameters. Our approach can generat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21333;&#32423;Transformer RGB-T&#36319;&#36394;&#32593;&#32476;USTrack&#65292;&#23427;&#36890;&#36807;&#21452;&#23884;&#20837;&#23618;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#23454;&#29616;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#20114;&#21161;&#25351;&#23548;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32593;&#32476;&#36866;&#24212;&#30446;&#26631;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#19977;&#38454;&#27573;&#34701;&#21512;&#36319;&#36394;&#33539;&#20363;&#21512;&#24182;&#20026;&#19968;&#20010;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#36319;&#36394;&#36895;&#24230;&#21644;&#30446;&#26631;-&#32972;&#26223;&#21487;&#21306;&#20998;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13764</link><description>&lt;p&gt;
&#39640;&#25928;RGB-T&#36319;&#36394;&#30340;&#32479;&#19968;&#21333;&#32423;Transformer&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Unified Single-Stage Transformer Network for Efficient RGB-T Tracking. (arXiv:2308.13764v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13764
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21333;&#32423;Transformer RGB-T&#36319;&#36394;&#32593;&#32476;USTrack&#65292;&#23427;&#36890;&#36807;&#21452;&#23884;&#20837;&#23618;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#23454;&#29616;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#20114;&#21161;&#25351;&#23548;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32593;&#32476;&#36866;&#24212;&#30446;&#26631;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#19977;&#38454;&#27573;&#34701;&#21512;&#36319;&#36394;&#33539;&#20363;&#21512;&#24182;&#20026;&#19968;&#20010;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#36319;&#36394;&#36895;&#24230;&#21644;&#30446;&#26631;-&#32972;&#26223;&#21487;&#21306;&#20998;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#29616;&#26377;&#30340;RGB-T&#36319;&#36394;&#32593;&#32476;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#25552;&#21462;&#27169;&#24577;&#29305;&#24449;&#65292;&#32570;&#20047;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#20114;&#21161;&#25351;&#23548;&#12290;&#36825;&#38480;&#21046;&#20102;&#32593;&#32476;&#36866;&#24212;&#30446;&#26631;&#30340;&#22810;&#26679;&#21452;&#27169;&#24577;&#22806;&#35266;&#21644;&#27169;&#24577;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#21478;&#22806;&#65292;&#36825;&#20123;&#32593;&#32476;&#36981;&#24490;&#30340;&#19977;&#38454;&#27573;&#34701;&#21512;&#36319;&#36394;&#33539;&#20363;&#20005;&#37325;&#38480;&#21046;&#20102;&#36319;&#36394;&#36895;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21333;&#32423;Transformer RGB-T&#36319;&#36394;&#32593;&#32476;&#65292;&#21363;USTrack&#65292;&#23558;&#19978;&#36848;&#19977;&#20010;&#38454;&#27573;&#21512;&#24182;&#20026;&#19968;&#20010;&#20855;&#22791;&#21452;&#23884;&#20837;&#23618;&#30340;&#21333;&#19968;ViT&#65288;&#35270;&#35273;Transformer&#65289;&#39592;&#24178;&#65292;&#24182;&#36890;&#36807;&#33258;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#27169;&#24577;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#36825;&#31181;&#32467;&#26500;&#65292;&#32593;&#32476;&#21487;&#20197;&#22312;&#27169;&#24577;&#30340;&#30456;&#20114;&#20316;&#29992;&#19979;&#25552;&#21462;&#27169;&#26495;&#21644;&#25628;&#32034;&#21306;&#22495;&#30340;&#34701;&#21512;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#20851;&#31995;&#24314;&#27169;&#65292;&#39640;&#25928;&#22320;&#33719;&#24471;&#20855;&#26377;&#26356;&#22909;&#30340;&#30446;&#26631;-&#32972;&#26223;&#21487;&#21306;&#20998;&#24615;&#30340;&#25628;&#32034;&#21306;&#22495;&#34701;&#21512;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing RGB-T tracking networks extract modality features in a separate manner, which lacks interaction and mutual guidance between modalities. This limits the network's ability to adapt to the diverse dual-modality appearances of targets and the dynamic relationships between the modalities. Additionally, the three-stage fusion tracking paradigm followed by these networks significantly restricts the tracking speed. To overcome these problems, we propose a unified single-stage Transformer RGB-T tracking network, namely USTrack, which unifies the above three stages into a single ViT (Vision Transformer) backbone with a dual embedding layer through self-attention mechanism. With this structure, the network can extract fusion features of the template and search region under the mutual interaction of modalities. Simultaneously, relation modeling is performed between these features, efficiently obtaining the search region fusion features with better target-background discriminability f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27573;&#33853;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26032;&#39062;&#26041;&#27861;PCAS&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;PCAS&#19981;&#20165;&#22312;&#26816;&#32034;&#26368;&#30456;&#20851;&#30340;&#27573;&#33853;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#65292;&#32780;&#19988;&#22312;&#30830;&#23450;&#30456;&#20851;&#19978;&#19979;&#25991;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#23558;&#28608;&#21457;&#26410;&#26469;&#30740;&#31350;&#30340;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2308.13760</link><description>&lt;p&gt;
&#22914;&#20309;&#21033;&#29992;&#19978;&#19979;&#25991;&#24110;&#21161;&#65311;&#25506;&#32034;&#27573;&#33853;&#21644;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#30340;&#32852;&#21512;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
How Can Context Help? Exploring Joint Retrieval of Passage and Personalized Context. (arXiv:2308.13760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27573;&#33853;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26032;&#39062;&#26041;&#27861;PCAS&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;PCAS&#19981;&#20165;&#22312;&#26816;&#32034;&#26368;&#30456;&#20851;&#30340;&#27573;&#33853;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#65292;&#32780;&#19988;&#22312;&#30830;&#23450;&#30456;&#20851;&#19978;&#19979;&#25991;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#23558;&#28608;&#21457;&#26410;&#26469;&#30740;&#31350;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22806;&#37096;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#20449;&#24687;&#25972;&#21512;&#21040;&#20197;&#25991;&#26723;&#20026;&#22522;&#30784;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21830;&#19994;&#20215;&#20540;&#65292;&#20294;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#21463;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#24863;&#30693;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#30340;&#27010;&#24565;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27573;&#33853;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#27492;&#30446;&#30340;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22810;&#20010;&#22522;&#20934;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#24863;&#30693;&#25628;&#32034;(Personalized Context-Aware Search&#65292;PCAS)&#65292;&#23427;&#22312;&#27573;&#33853;&#26816;&#32034;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#27969;&#34892;&#30340;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#26816;&#32034;&#26368;&#30456;&#20851;&#30340;&#27573;&#33853;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#65292;&#32780;&#19988;&#22312;&#30830;&#23450;&#25152;&#26377;&#21487;&#29992;&#19978;&#19979;&#25991;&#20013;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#36129;&#29486;&#23558;&#25104;&#20026;&#28608;&#21169;&#26410;&#26469;&#30740;&#31350;&#30340;&#20652;&#21270;&#21058;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of external personalized context information into document-grounded conversational systems has significant potential business value, but has not been well-studied. Motivated by the concept of personalized context-aware document-grounded conversational systems, we introduce the task of context-aware passage retrieval. We also construct a dataset specifically curated for this purpose. We describe multiple baseline systems to address this task, and propose a novel approach, Personalized Context-Aware Search (PCAS), that effectively harnesses contextual information during passage retrieval. Experimental evaluations conducted on multiple popular dense retrieval systems demonstrate that our proposed approach not only outperforms the baselines in retrieving the most relevant passage but also excels at identifying the pertinent context among all the available contexts. We envision that our contributions will serve as a catalyst for inspiring future research endeavors in this pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;SamDSK&#65292;&#32467;&#21512;&#20998;&#21106;&#20219;&#24847;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#26041;&#27861;&#21253;&#25324;&#36845;&#20195;&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#20998;&#21106;&#27169;&#22411;&#35757;&#32451;&#21644;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25193;&#23637;&#26377;&#26631;&#31614;&#38598;&#12290;&#36890;&#36807;&#23558;&#20998;&#21106;&#24314;&#35758;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#32467;&#21512;&#65292;&#26500;&#24314;&#26080;&#26631;&#31614;&#22270;&#20687;&#30340;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.13759</link><description>&lt;p&gt;
SamDSK: &#32467;&#21512;&#20998;&#21106;&#20219;&#24847;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SamDSK: Combining Segment Anything Model with Domain-Specific Knowledge for Semi-Supervised Learning in Medical Image Segmentation. (arXiv:2308.13759v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;SamDSK&#65292;&#32467;&#21512;&#20998;&#21106;&#20219;&#24847;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#26041;&#27861;&#21253;&#25324;&#36845;&#20195;&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#20998;&#21106;&#27169;&#22411;&#35757;&#32451;&#21644;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25193;&#23637;&#26377;&#26631;&#31614;&#38598;&#12290;&#36890;&#36807;&#23558;&#20998;&#21106;&#24314;&#35758;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#32467;&#21512;&#65292;&#26500;&#24314;&#26080;&#26631;&#31614;&#22270;&#20687;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#20998;&#21106;&#21508;&#31181;&#23545;&#35937;&#30340;&#33021;&#21147;&#65292;&#26159;&#21508;&#31181;&#19979;&#28216;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#22810;&#21151;&#33021;&#24863;&#30693;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#36890;&#24120;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65288;DSK&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65288;&#21363;SAM&#65289;&#19982;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21487;&#38752;&#22320;&#21033;&#29992;&#26080;&#26631;&#31614;&#22270;&#20687;&#26500;&#24314;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#26159;&#36845;&#20195;&#30340;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;&#65288;1&#65289;&#20998;&#21106;&#27169;&#22411;&#35757;&#32451;&#65307;&#65288;2&#65289;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#12289;&#26080;&#26631;&#31614;&#38598;&#12289;SAM&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25193;&#23637;&#26377;&#26631;&#31614;&#38598;&#12290;&#36825;&#20004;&#20010;&#38454;&#27573;&#37325;&#22797;&#36827;&#34892;&#65292;&#30452;&#21040;&#26080;&#27861;&#20877;&#28155;&#21152;&#26679;&#26412;&#21040;&#26377;&#26631;&#31614;&#38598;&#20026;&#27490;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#23558;SAM&#29983;&#25104;&#30340;&#20998;&#21106;&#24314;&#35758;&#19982;&#20687;&#32032;&#32423;&#21644;&#22270;&#20687;&#32423;&#30340;DSK&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#26080;&#26631;&#31614;&#22270;&#20687;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) exhibits a capability to segment a wide array of objects in natural images, serving as a versatile perceptual tool for various downstream image segmentation tasks. In contrast, medical image segmentation tasks often rely on domain-specific knowledge (DSK). In this paper, we propose a novel method that combines the segmentation foundation model (i.e., SAM) with domain-specific knowledge for reliable utilization of unlabeled images in building a medical image segmentation model. Our new method is iterative and consists of two main stages: (1) segmentation model training; (2) expanding the labeled set by using the trained segmentation model, an unlabeled set, SAM, and domain-specific knowledge. These two stages are repeated until no more samples are added to the labeled set. A novel optimal-matching-based method is developed for combining the SAM-generated segmentation proposals and pixel-level and image-level DSK for constructing annotations of unlabeled 
&lt;/p&gt;</description></item><item><title>i-Align&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#23545;&#40784;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#23545;&#27599;&#20010;&#23545;&#40784;&#39044;&#27979;&#30340;&#35299;&#37322;&#65292;&#24182;&#20445;&#25345;&#39640;&#23545;&#40784;&#24615;&#33021;&#65292;&#32500;&#25252;&#30693;&#35782;&#22270;&#30340;&#39640;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.13755</link><description>&lt;p&gt;
i-Align: &#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#23545;&#40784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
i-Align: an interpretable knowledge graph alignment model. (arXiv:2308.13755v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13755
&lt;/p&gt;
&lt;p&gt;
i-Align&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#23545;&#40784;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#23545;&#27599;&#20010;&#23545;&#40784;&#39044;&#27979;&#30340;&#35299;&#37322;&#65292;&#24182;&#20445;&#25345;&#39640;&#23545;&#40784;&#24615;&#33021;&#65292;&#32500;&#25252;&#30693;&#35782;&#22270;&#30340;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#26469;&#35828;&#24050;&#32463;&#25104;&#20026;&#24517;&#19981;&#21487;&#23569;&#30340;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#19981;&#23436;&#25972;&#24615;&#21487;&#33021;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#36827;&#34892;&#36830;&#32493;&#32500;&#25252;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#31574;&#30053;&#20043;&#19968;&#26159;KG&#23545;&#40784;&#65292;&#21363;&#36890;&#36807;&#21512;&#24182;&#20004;&#20010;&#25110;&#22810;&#20010;KG&#26469;&#24418;&#25104;&#19968;&#20010;&#26356;&#23436;&#25972;&#30340;KG&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;KG&#23545;&#40784;&#27169;&#22411;i-Align&#12290;&#19982;&#29616;&#26377;&#30340;KG&#23545;&#40784;&#27169;&#22411;&#19981;&#21516;&#65292;i-Align&#22312;&#20445;&#25345;&#39640;&#23545;&#40784;&#24615;&#33021;&#30340;&#21516;&#26102;&#20026;&#27599;&#20010;&#23545;&#40784;&#39044;&#27979;&#25552;&#20379;&#35299;&#37322;&#12290;&#19987;&#23478;&#21487;&#20197;&#20351;&#29992;&#35299;&#37322;&#26469;&#26816;&#26597;&#23545;&#40784;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#32500;&#25252;&#36807;&#31243;&#20013;&#65288;&#20363;&#22914;&#20004;&#20010;KG&#30340;&#21512;&#24182;&#36807;&#31243;&#65289;&#21487;&#20197;&#20445;&#25345;KG&#30340;&#39640;&#36136;&#37327;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#22270;&#32534;&#30721;&#22120;&#65288;Trans-GE&#65289;&#65292;&#20316;&#20026;i-Align&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#29992;&#20110;&#32858;&#21512;&#23454;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#65288;&#32467;&#26500;&#65289;&#20449;&#24687;&#12290;Trans-GE&#20351;&#29992;&#36793;&#32536;&#38376;&#25511;&#27880;&#24847;&#21147;&#26469;&#32467;&#21512;&#37051;&#25509;&#30697;&#38453;&#21644;&#8230;&#65288;&#21407;&#25991;&#26410;&#23436;&#65289;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are becoming essential resources for many downstream applications. However, their incompleteness may limit their potential. Thus, continuous curation is needed to mitigate this problem. One of the strategies to address this problem is KG alignment, i.e., forming a more complete KG by merging two or more KGs. This paper proposes i-Align, an interpretable KG alignment model. Unlike the existing KG alignment models, i-Align provides an explanation for each alignment prediction while maintaining high alignment performance. Experts can use the explanation to check the correctness of the alignment prediction. Thus, the high quality of a KG can be maintained during the curation process (e.g., the merging process of two KGs). To this end, a novel Transformer-based Graph Encoder (Trans-GE) is proposed as a key component of i-Align for aggregating information from entities' neighbors (structures). Trans-GE uses Edge-gated Attention that combines the adjacency matrix and th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PE-MED&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#20132;&#20114;&#24335;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#25104;&#21151;&#29575;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#33258;&#24490;&#29615;&#31574;&#30053;&#21644;&#25552;&#31034;&#27880;&#24847;&#21147;&#23398;&#20064;&#27169;&#22359;&#26469;&#25913;&#21892;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#20449;&#24687;&#30340;&#21033;&#29992;&#65292;&#20174;&#32780;&#38450;&#27490;&#19981;&#21033;&#24773;&#20917;&#30340;&#21457;&#29983;&#65292;&#24182;&#25552;&#39640;&#20998;&#21106;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13746</link><description>&lt;p&gt;
PE-MED: &#25552;&#39640;&#20132;&#20114;&#24335;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#25552;&#31034;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PE-MED: Prompt Enhancement for Interactive Medical Image Segmentation. (arXiv:2308.13746v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PE-MED&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#20132;&#20114;&#24335;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#25104;&#21151;&#29575;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#33258;&#24490;&#29615;&#31574;&#30053;&#21644;&#25552;&#31034;&#27880;&#24847;&#21147;&#23398;&#20064;&#27169;&#22359;&#26469;&#25913;&#21892;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#20449;&#24687;&#30340;&#21033;&#29992;&#65292;&#20174;&#32780;&#38450;&#27490;&#19981;&#21033;&#24773;&#20917;&#30340;&#21457;&#29983;&#65292;&#24182;&#25552;&#39640;&#20998;&#21106;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26159;&#25351;&#36890;&#36807;&#29992;&#25143;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#65288;&#20363;&#22914;&#65292;&#28857;&#20987;&#65289;&#20934;&#30830;&#22320;&#20998;&#21106;&#20986;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;&#36817;&#24180;&#26469;&#65292;&#23427;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#19981;&#22826;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#27604;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#20998;&#21106;&#26356;&#21152;&#28789;&#27963;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#36824;&#27809;&#26377;&#20805;&#20998;&#25506;&#32034;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#28857;&#65289;&#65292;&#21253;&#25324;&#22312;&#19968;&#20010;&#20132;&#20114;&#20013;&#25366;&#25496;&#30340;&#30693;&#35782;&#20197;&#21450;&#22810;&#20010;&#20132;&#20114;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35013;&#22791;&#26377;&#25552;&#31034;&#22686;&#24378;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21517;&#20026;PE-MED&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#24490;&#29615;&#31574;&#30053;&#65292;&#22522;&#20110;&#31532;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#28201;&#26262;&#30340;&#21021;&#22987;&#20998;&#21106;&#32467;&#26524;&#12290;&#23427;&#21487;&#20197;&#38450;&#27490;&#20986;&#29616;&#39640;&#24230;&#19981;&#21033;&#30340;&#24773;&#20917;&#65292;&#27604;&#22914;&#22312;&#31532;&#19968;&#27425;&#20132;&#20114;&#21518;&#36935;&#21040;&#31354;&#30333;&#36974;&#32617;&#20316;&#20026;&#21021;&#22987;&#36755;&#20837;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#27880;&#24847;&#21147;&#23398;&#20064;&#27169;&#22359;&#65288;PALM&#65289;&#65292;&#29992;&#20110;&#25366;&#25496;&#26377;&#29992;&#30340;&#25552;&#31034;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive medical image segmentation refers to the accurate segmentation of the target of interest through interaction (e.g., click) between the user and the image. It has been widely studied in recent years as it is less dependent on abundant annotated data and more flexible than fully automated segmentation. However, current studies have not fully explored user-provided prompt information (e.g., points), including the knowledge mined in one interaction, and the relationship between multiple interactions. Thus, in this paper, we introduce a novel framework equipped with prompt enhancement, called PE-MED, for interactive medical image segmentation. First, we introduce a Self-Loop strategy to generate warm initial segmentation results based on the first prompt. It can prevent the highly unfavorable scenarios, such as encountering a blank mask as the initial input after the first interaction. Second, we propose a novel Prompt Attention Learning Module (PALM) to mine useful prompt infor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#19982;&#25968;&#23398;&#30456;&#32467;&#21512;&#30340;Philomatics&#21644;Psychomatics&#27010;&#24565;&#65292;&#24182;&#35299;&#37322;&#20102;&#22235;&#20010;&#21160;&#26426;&#65306;&#28385;&#36275;&#20998;&#26512;&#21746;&#23398;&#30340;&#38656;&#27714;&#12289;&#25552;&#20986;&#21746;&#23398;&#31185;&#23398;&#12289;&#29992;&#21746;&#23398;&#26469;&#35777;&#26126;&#25968;&#23398;&#31639;&#27861;&#20197;&#21450;&#21746;&#23398;&#21644;&#25968;&#23398;&#30340;&#25277;&#35937;&#12290;&#24182;&#21015;&#20030;&#20102;&#22810;&#20010;&#31034;&#20363;&#65292;&#21253;&#25324;&#25968;&#23398;&#20013;&#27880;&#24847;&#26426;&#21046;&#21644;&#19978;&#19979;&#25991;&#21407;&#21017;&#12289;&#24418;&#24335;&#29702;&#35770;&#19982;&#20840;&#24687;&#21407;&#29702;&#30340;&#20851;&#31995;&#31561;&#12290;&#26412;&#25991;&#20026;&#23558;&#21746;&#23398;&#21644;&#24515;&#29702;&#23398;&#19982;&#25968;&#23398;&#30456;&#32467;&#21512;&#30340;&#30740;&#31350;&#24320;&#36767;&#20102;&#30740;&#31350;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.13738</link><description>&lt;p&gt;
&#20851;&#20110;&#23558;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#19982;&#25968;&#23398;&#30456;&#32467;&#21512;&#30340;Philomatics&#21644;Psychomatics&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Philomatics and Psychomatics for Combining Philosophy and Psychology with Mathematics. (arXiv:2308.13738v1 [math.HO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#19982;&#25968;&#23398;&#30456;&#32467;&#21512;&#30340;Philomatics&#21644;Psychomatics&#27010;&#24565;&#65292;&#24182;&#35299;&#37322;&#20102;&#22235;&#20010;&#21160;&#26426;&#65306;&#28385;&#36275;&#20998;&#26512;&#21746;&#23398;&#30340;&#38656;&#27714;&#12289;&#25552;&#20986;&#21746;&#23398;&#31185;&#23398;&#12289;&#29992;&#21746;&#23398;&#26469;&#35777;&#26126;&#25968;&#23398;&#31639;&#27861;&#20197;&#21450;&#21746;&#23398;&#21644;&#25968;&#23398;&#30340;&#25277;&#35937;&#12290;&#24182;&#21015;&#20030;&#20102;&#22810;&#20010;&#31034;&#20363;&#65292;&#21253;&#25324;&#25968;&#23398;&#20013;&#27880;&#24847;&#26426;&#21046;&#21644;&#19978;&#19979;&#25991;&#21407;&#21017;&#12289;&#24418;&#24335;&#29702;&#35770;&#19982;&#20840;&#24687;&#21407;&#29702;&#30340;&#20851;&#31995;&#31561;&#12290;&#26412;&#25991;&#20026;&#23558;&#21746;&#23398;&#21644;&#24515;&#29702;&#23398;&#19982;&#25968;&#23398;&#30456;&#32467;&#21512;&#30340;&#30740;&#31350;&#24320;&#36767;&#20102;&#30740;&#31350;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#19982;&#25968;&#23398;&#30456;&#32467;&#21512;&#30340;Philomatics&#21644;Psychomatics&#27010;&#24565;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#31181;&#32467;&#21512;&#30340;&#22235;&#20010;&#21160;&#26426;&#65292;&#21253;&#25324;&#28385;&#36275;&#20998;&#26512;&#21746;&#23398;&#30340;&#38656;&#27714;&#12289;&#25552;&#20986;&#21746;&#23398;&#31185;&#23398;&#12289;&#29992;&#21746;&#23398;&#26469;&#35777;&#26126;&#25968;&#23398;&#31639;&#27861;&#20197;&#21450;&#21746;&#23398;&#21644;&#25968;&#23398;&#30340;&#25277;&#35937;&#12290;&#25105;&#20204;&#21015;&#20030;&#20102;&#21508;&#31181;Philomatics&#21644;Psychomatics&#30340;&#31034;&#20363;&#65292;&#20854;&#20013;&#19968;&#20123;&#22312;&#26356;&#28145;&#20837;&#22320;&#35299;&#37322;&#12290;&#31532;&#19968;&#20010;&#31034;&#20363;&#26159;&#20851;&#20110;&#25968;&#23398;&#20013;&#27880;&#24847;&#26426;&#21046;&#19982;&#19978;&#19979;&#25991;&#21407;&#21017;&#12289;&#35821;&#20041;&#25972;&#20307;&#20027;&#20041;&#21644;&#20351;&#29992;&#29702;&#35770;&#30340;&#20851;&#31995;&#20998;&#26512;&#12290;&#21478;&#19968;&#20010;&#31034;&#20363;&#26159;&#20851;&#20110;&#21746;&#23398;&#20013;&#26575;&#25289;&#22270;&#30340;&#24418;&#24335;&#29702;&#35770;&#19982;&#24358;&#29702;&#35770;&#20013;&#30340;&#20840;&#24687;&#21407;&#29702;&#12289;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#32500;&#29305;&#26681;&#26031;&#22374;&#30340;&#23478;&#26063;&#30456;&#20284;&#24615;&#19982;&#25968;&#23398;&#20013;&#30340;&#32858;&#31867;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#20026;&#23558;&#21746;&#23398;&#21644;&#24515;&#29702;&#23398;&#19982;&#25968;&#23398;&#30456;&#32467;&#21512;&#30340;&#30740;&#31350;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the concepts of philomatics and psychomatics as hybrid combinations of philosophy and psychology with mathematics. We explain four motivations for this combination which are fulfilling the desire of analytical philosophy, proposing science of philosophy, justifying mathematical algorithms by philosophy, and abstraction in both philosophy and mathematics. We enumerate various examples for philomatics and psychomatics, some of which are explained in more depth. The first example is the analysis of relation between the context principle, semantic holism, and the usage theory of meaning with the attention mechanism in mathematics. The other example is on the relations of Plato's theory of forms in philosophy with the holographic principle in string theory, object-oriented programming, and machine learning. Finally, the relation between Wittgenstein's family resemblance and clustering in mathematics is explained. This paper opens the door of research for combining philosophy and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#20102;&#20027;&#35266;&#12289;&#23458;&#35266;&#21644;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#29983;&#25104;&#38899;&#20048;&#65292;&#31361;&#20986;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20026;&#32479;&#19968;&#38899;&#20048;&#35780;&#20272;&#39046;&#22495;&#30340;&#29983;&#25104;AI&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2308.13736</link><description>&lt;p&gt;
AI&#29983;&#25104;&#38899;&#20048;&#35780;&#20272;&#26041;&#27861;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey for Evaluation Methodologies of AI-Generated Music. (arXiv:2308.13736v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13736
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#20102;&#20027;&#35266;&#12289;&#23458;&#35266;&#21644;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#29983;&#25104;&#38899;&#20048;&#65292;&#31361;&#20986;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20026;&#32479;&#19968;&#38899;&#20048;&#35780;&#20272;&#39046;&#22495;&#30340;&#29983;&#25104;AI&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;AI&#29983;&#25104;&#30340;&#38899;&#20048;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#22810;&#27169;&#24577;&#21644;&#22797;&#26434;&#30340;&#38899;&#20048;&#27969;&#27966;&#21644;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#21487;&#20197;&#20351;&#29992;&#23458;&#35266;&#24230;&#37327;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#38899;&#20048;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#38899;&#20048;&#35780;&#20272;&#26041;&#38754;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24120;&#24120;&#20381;&#36182;&#20027;&#35266;&#29992;&#25143;&#30740;&#31350;&#26469;&#35780;&#20272;&#29983;&#25104;&#20316;&#21697;&#30340;&#36136;&#37327;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#32791;&#36153;&#36164;&#28304;&#65292;&#24182;&#19988;&#27604;&#23458;&#35266;&#24230;&#37327;&#25351;&#26631;&#26356;&#38590;&#22797;&#29616;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#20027;&#35266;&#12289;&#23458;&#35266;&#21644;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#29983;&#25104;&#38899;&#20048;&#65292;&#24182;&#31361;&#20986;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#26368;&#32456;&#65292;&#26412;&#30740;&#31350;&#20026;&#22312;&#38899;&#20048;&#35780;&#20272;&#39046;&#22495;&#32479;&#19968;&#29983;&#25104;AI&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, AI-generated music has made significant progress, with several models performing well in multimodal and complex musical genres and scenes. While objective metrics can be used to evaluate generative music, they often lack interpretability for musical evaluation. Therefore, researchers often resort to subjective user studies to assess the quality of the generated works, which can be resource-intensive and less reproducible than objective metrics. This study aims to comprehensively evaluate the subjective, objective, and combined methodologies for assessing AI-generated music, highlighting the advantages and disadvantages of each approach. Ultimately, this study provides a valuable reference for unifying generative AI in the field of music evaluation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ISR-LLM&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#33258;&#25105;&#23436;&#21892;&#36807;&#31243;&#25913;&#36827;&#20102;LLM-based&#35268;&#21010;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39044;&#22788;&#29702;&#12289;&#35268;&#21010;&#21644;&#36845;&#20195;&#33258;&#25105;&#23436;&#21892;&#19977;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#24341;&#20837;LLM&#32763;&#35793;&#22120;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;PDDL&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#20219;&#21153;&#35745;&#21010;&#30340;&#21487;&#34892;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13724</link><description>&lt;p&gt;
ISR-LLM: &#36845;&#20195;&#33258;&#25105;&#23436;&#21892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38271;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning. (arXiv:2308.13724v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ISR-LLM&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#33258;&#25105;&#23436;&#21892;&#36807;&#31243;&#25913;&#36827;&#20102;LLM-based&#35268;&#21010;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39044;&#22788;&#29702;&#12289;&#35268;&#21010;&#21644;&#36845;&#20195;&#33258;&#25105;&#23436;&#21892;&#19977;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#24341;&#20837;LLM&#32763;&#35793;&#22120;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;PDDL&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#20219;&#21153;&#35745;&#21010;&#30340;&#21487;&#34892;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#30340;&#37325;&#22823;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#25506;&#32034;&#23558;LLMs&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#22797;&#26434;&#38271;&#26102;&#24207;&#21015;&#20219;&#21153;&#35268;&#21010;&#25361;&#25112;&#12290;LLMs&#20855;&#26377;&#20248;&#21183;&#65292;&#21487;&#20197;&#25552;&#21319;&#36890;&#29992;&#24615;&#20316;&#20026;&#20219;&#21153;&#26080;&#20851;&#30340;&#35268;&#21010;&#32773;&#65292;&#24182;&#20419;&#36827;&#20154;&#31867;&#25945;&#24072;&#21644;&#35268;&#21010;&#31995;&#32479;&#20043;&#38388;&#30340;&#28789;&#27963;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;LLMs&#29983;&#25104;&#30340;&#20219;&#21153;&#35745;&#21010;&#32463;&#24120;&#32570;&#20047;&#21487;&#34892;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ISR-LLM&#65292;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#33258;&#25105;&#23436;&#21892;&#36807;&#31243;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19977;&#20010;&#36830;&#32493;&#30340;&#27493;&#39588;&#36827;&#34892;&#25805;&#20316;&#65306;&#39044;&#22788;&#29702;&#12289;&#35268;&#21010;&#21644;&#36845;&#20195;&#33258;&#25105;&#23436;&#21892;&#12290;&#22312;&#39044;&#22788;&#29702;&#38454;&#27573;&#65292;&#20351;&#29992;LLM&#32763;&#35793;&#22120;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#35268;&#21010;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#24418;&#24335;&#12290;&#22312;&#35268;&#21010;&#38454;&#27573;&#65292;LLM&#35268;&#21010;&#22120;&#21046;&#23450;&#20102;&#20219;&#21153;&#35745;&#21010;&#30340;&#21021;&#27493;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the substantial achievements observed in Large Language Models (LLMs) in the field of natural language processing, recent research has commenced investigations into the application of LLMs for complex, long-horizon sequential task planning challenges in robotics. LLMs are advantageous in offering the potential to enhance the generalizability as task-agnostic planners and facilitate flexible interaction between human instructors and planning systems. However, task plans generated by LLMs often lack feasibility and correctness. To address this challenge, we introduce ISR-LLM, a novel framework that improves LLM-based planning through an iterative self-refinement process. The framework operates through three sequential steps: preprocessing, planning, and iterative self-refinement. During preprocessing, an LLM translator is employed to convert natural language input into a Planning Domain Definition Language (PDDL) formulation. In the planning phase, an LLM planner formulates 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;Reddit&#20869;&#23481;&#20013;&#35782;&#21035;&#20581;&#24247;&#32500;&#24230;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#22797;&#26434;&#30340;&#31934;&#31070;&#20581;&#24247;&#20998;&#26512;&#12290;&#20182;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;WELLXPLAIN&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#27880;&#37322;&#26694;&#26550;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#28508;&#22312;&#30340;&#31934;&#31070;&#38382;&#39064;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.13710</link><description>&lt;p&gt;
WellXplain: Reddit&#24086;&#23376;&#20013;&#30340;&#20581;&#24247;&#27010;&#24565;&#25552;&#21462;&#21644;&#20998;&#31867;&#65292;&#29992;&#20110;&#31934;&#31070;&#20581;&#24247;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
WellXplain: Wellness Concept Extraction and Classification in Reddit Posts for Mental Health Analysis. (arXiv:2308.13710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;Reddit&#20869;&#23481;&#20013;&#35782;&#21035;&#20581;&#24247;&#32500;&#24230;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#22797;&#26434;&#30340;&#31934;&#31070;&#20581;&#24247;&#20998;&#26512;&#12290;&#20182;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;WELLXPLAIN&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#27880;&#37322;&#26694;&#26550;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#28508;&#22312;&#30340;&#31934;&#31070;&#38382;&#39064;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#31934;&#31070;&#20581;&#24247;&#21361;&#26426;&#20013;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#20013;&#35782;&#21035;&#28508;&#22312;&#30340;&#31934;&#31070;&#38382;&#39064;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#26377;&#25152;&#22686;&#21152;&#12290;&#24573;&#35270;&#31934;&#31070;&#21644;&#31038;&#20250;&#24184;&#31119;&#30340;&#22810;&#38754;&#24615;&#21487;&#33021;&#23545;&#19968;&#20010;&#20154;&#30340;&#31934;&#31070;&#29366;&#24577;&#20135;&#29983;&#26377;&#23475;&#24433;&#21709;&#12290;&#22312;&#20256;&#32479;&#30340;&#27835;&#30103;&#36807;&#31243;&#20013;&#65292;&#19987;&#19994;&#20154;&#21592;&#38656;&#35201;&#25163;&#21160;&#30830;&#23450;&#28508;&#22312;&#31934;&#31070;&#25361;&#25112;&#30340;&#36215;&#28304;&#21644;&#32467;&#26524;&#65292;&#36825;&#26159;&#19968;&#20010;&#35814;&#32454;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;Reddit&#20869;&#23481;&#20013;&#30340;&#20581;&#24247;&#32500;&#24230;&#35782;&#21035;&#20026;&#20581;&#24247;&#27010;&#24565;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#31934;&#31070;&#20581;&#24247;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;WELLXPLAIN&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;3,092&#20010;&#26465;&#30446;&#65292;&#24635;&#35745;72,813&#20010;&#21333;&#35789;&#12290;&#22522;&#20110;&#21704;&#23572;&#20271;&#29305;&#183;L&#183;&#37011;&#24681;&#30340;&#33879;&#21517;&#20581;&#24247;&#29702;&#35770;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#21046;&#23450;&#20102;&#19968;&#20010;&#27880;&#37322;&#26694;&#26550;&#21644;&#25351;&#21335;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#25324;&#20154;&#24037;&#26631;&#35760;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#28165;&#26970;&#35299;&#37322;&#20102;&#20581;&#24247;&#27010;&#24565;&#20998;&#31867;&#20915;&#31574;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the current mental health crisis, the importance of identifying potential indicators of mental issues from social media content has surged. Overlooking the multifaceted nature of mental and social well-being can have detrimental effects on one's mental state. In traditional therapy sessions, professionals manually pinpoint the origins and outcomes of underlying mental challenges, a process both detailed and time-intensive. We introduce an approach to this intricate mental health analysis by framing the identification of wellness dimensions in Reddit content as a wellness concept extraction and categorization challenge. We've curated a unique dataset named WELLXPLAIN, comprising 3,092 entries and totaling 72,813 words. Drawing from Halbert L. Dunn's well-regarded wellness theory, our team formulated an annotation framework along with guidelines. This dataset also includes human-marked textual segments, offering clear reasoning for decisions made in the wellness concept categoriza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;Dynamic PicoProbe&#30340;&#25968;&#25454;&#20256;&#36755;&#19982;Argonne&#39046;&#23548;&#35745;&#31639;&#35774;&#26045;&#30456;&#37051;&#36229;&#32423;&#35745;&#31639;&#26426;&#30456;&#36830;&#25509;&#30340;&#36719;&#20214;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#25903;&#25345;&#22823;&#35268;&#27169;&#25968;&#25454;&#20256;&#36755;&#21644;&#31163;&#32447;&#25968;&#25454;&#20998;&#26512;&#65292;&#20026;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#26597;&#35810;&#21644;&#37325;&#26032;&#20998;&#26512;&#36807;&#21435;&#23454;&#39564;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13701</link><description>&lt;p&gt;
&#23558;&#21160;&#24577;PicoProbe&#20998;&#26512;&#30005;&#23376;&#20809;&#23398;&#26463;&#32447;/&#26174;&#24494;&#38236;&#19982;&#36229;&#32423;&#35745;&#31639;&#26426;&#30456;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Linking the Dynamic PicoProbe Analytical Electron-Optical Beam Line / Microscope to Supercomputers. (arXiv:2308.13701v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;Dynamic PicoProbe&#30340;&#25968;&#25454;&#20256;&#36755;&#19982;Argonne&#39046;&#23548;&#35745;&#31639;&#35774;&#26045;&#30456;&#37051;&#36229;&#32423;&#35745;&#31639;&#26426;&#30456;&#36830;&#25509;&#30340;&#36719;&#20214;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#25903;&#25345;&#22823;&#35268;&#27169;&#25968;&#25454;&#20256;&#36755;&#21644;&#31163;&#32447;&#25968;&#25454;&#20998;&#26512;&#65292;&#20026;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#26597;&#35810;&#21644;&#37325;&#26032;&#20998;&#26512;&#36807;&#21435;&#23454;&#39564;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;PicoProbe&#27491;&#22312;Argonne&#22269;&#23478;&#23454;&#39564;&#23460;&#36827;&#34892;&#21319;&#32423;&#65292;&#23558;&#33021;&#22815;&#27599;&#22825;&#20135;&#29983;&#39640;&#36798;&#25968;&#30334;GB&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#23545;&#22522;&#30784;&#31185;&#23398;&#21644;&#24037;&#19994;&#24212;&#29992;&#37117;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#22312;&#22330;&#20869;&#30340;&#22522;&#30784;&#35774;&#26045;&#26080;&#27861;&#22788;&#29702;&#36825;&#20123;&#39640;&#23481;&#37327;&#30340;&#25968;&#25454;&#27969;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#22815;&#25903;&#25345;&#22823;&#35268;&#27169;&#25968;&#25454;&#20256;&#36755;&#21040;Argonne&#39046;&#23548;&#35745;&#31639;&#35774;&#26045;&#30456;&#37051;&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#36719;&#20214;&#26550;&#26500;&#12290;&#20026;&#20102;&#20934;&#22791;&#26410;&#26469;&#30340;&#31185;&#23398;&#24037;&#20316;&#27969;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#20004;&#20010;&#31034;&#20363;&#29992;&#20363;&#65292;&#21253;&#25324;&#39640;&#20809;&#35889;&#21644;&#26102;&#31354;&#25968;&#25454;&#38598;&#30340;&#65288;i&#65289;&#31163;&#22330;&#25968;&#25454;&#20256;&#36755;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;/&#20154;&#24037;&#26234;&#33021;&#21644;&#20256;&#32479;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#23454;&#39564;&#32467;&#26524;&#30340;&#33258;&#21160;&#20803;&#25968;&#25454;&#25552;&#21462;&#21644;&#32534;&#30446;&#12290;&#36825;&#31181;&#22522;&#30784;&#35774;&#26045;&#19981;&#20165;&#25903;&#25345;&#39044;&#26399;&#24037;&#20316;&#36127;&#36733;&#65292;&#36824;&#20351;&#39046;&#22495;&#31185;&#23398;&#23478;&#33021;&#22815;&#37325;&#26032;&#26597;&#35810;&#36807;&#21435;&#23454;&#39564;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Dynamic PicoProbe at Argonne National Laboratory is undergoing upgrades that will enable it to produce up to 100s of GB of data per day. While this data is highly important for both fundamental science and industrial applications, there is currently limited on-site infrastructure to handle these high-volume data streams. We address this problem by providing a software architecture capable of supporting large-scale data transfers to the neighboring supercomputers at the Argonne Leadership Computing Facility. To prepare for future scientific workflows, we implement two instructive use cases for hyperspectral and spatiotemporal datasets, which include: (i) off-site data transfer, (ii) machine learning/artificial intelligence and traditional data analysis approaches, and (iii) automatic metadata extraction and cataloging of experimental results. This infrastructure supports expected workloads and also provides domain scientists the ability to reinterrogate data from past experiments to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26463;&#25628;&#32034;&#21644;&#31351;&#20030;&#25628;&#32034;&#20043;&#38388;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#25628;&#32034;&#28145;&#24230;&#65292;&#25552;&#20986;&#20102;&#21069;&#30651;&#26463;&#25628;&#32034;&#65288;LBS&#65289;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#26463;&#25628;&#32034;&#30340;&#25628;&#32034;&#35823;&#24046;&#36739;&#39640;&#65292;&#20294;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#31351;&#20030;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2308.13696</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#26463;&#25628;&#32034;&#21644;&#31351;&#20030;&#25628;&#32034;&#30340;&#28145;&#24230;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Depth between Beam Search and Exhaustive Search for Text Generation. (arXiv:2308.13696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26463;&#25628;&#32034;&#21644;&#31351;&#20030;&#25628;&#32034;&#20043;&#38388;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#25628;&#32034;&#28145;&#24230;&#65292;&#25552;&#20986;&#20102;&#21069;&#30651;&#26463;&#25628;&#32034;&#65288;LBS&#65289;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#26463;&#25628;&#32034;&#30340;&#25628;&#32034;&#35823;&#24046;&#36739;&#39640;&#65292;&#20294;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#31351;&#20030;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26463;&#25628;&#32034;&#21644;&#31351;&#20030;&#25628;&#32034;&#26159;&#25991;&#26412;&#35299;&#30721;&#31639;&#27861;&#20013;&#28145;&#24230;&#25628;&#32034;&#30340;&#20004;&#20010;&#26497;&#31471;&#12290;&#26463;&#25628;&#32034;&#22312;&#25628;&#32034;&#23485;&#24230;&#21644;&#28145;&#24230;&#19978;&#37117;&#26377;&#38480;&#21046;&#65292;&#32780;&#31351;&#20030;&#25628;&#32034;&#26159;&#20840;&#23616;&#25628;&#32034;&#65292;&#27809;&#26377;&#36825;&#20123;&#38480;&#21046;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;&#26463;&#25628;&#32034;&#30340;&#25628;&#32034;&#35823;&#24046;&#36739;&#39640;&#65292;&#20294;&#23427;&#19981;&#20165;&#35745;&#31639;&#25104;&#26412;&#26356;&#20302;&#65292;&#32780;&#19988;&#34920;&#29616;&#26356;&#22909;&#12290;&#35768;&#22810;&#30740;&#31350;&#23545;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#26463;&#23485;&#24230;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#25253;&#21578;&#31216;&#26082;&#19981;&#22826;&#22823;&#20063;&#19981;&#22826;&#23567;&#30340;&#26463;&#23485;&#24230;&#26159;&#29702;&#24819;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#25628;&#32034;&#28145;&#24230;&#26041;&#38754;&#65292;&#21482;&#26377;&#26463;&#25628;&#32034;&#21644;&#31351;&#20030;&#25628;&#32034;&#36825;&#20004;&#20010;&#26497;&#31471;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20171;&#20110;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#30340;&#19968;&#31995;&#21015;&#25628;&#32034;&#28145;&#24230;&#65292;&#20197;&#21457;&#29616;&#29702;&#24819;&#30340;&#25628;&#32034;&#28145;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21069;&#30651;&#26463;&#25628;&#32034;&#65288;LBS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27493;&#21069;&#30651;&#25628;&#32034;&#65292;&#36890;&#36807;&#32771;&#34385;&#26410;&#26469;&#22266;&#23450;&#27493;&#25968;&#26469;&#20248;&#21270;&#30446;&#26631;&#12290;&#26463;&#25628;&#32034;&#21644;&#31351;&#20030;&#25628;&#32034;&#26159;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beam search and exhaustive search are two extreme ends of text decoding algorithms with respect to the search depth. Beam search is limited in both search width and depth, whereas exhaustive search is a global search that has no such limitations. Surprisingly, beam search is not only computationally cheaper but also performs better than exhaustive search despite its higher search error. Plenty of research has investigated a range of beam widths, from small to large, and reported that a beam width that is neither too large nor too small is desirable. However, in terms of search depth, only the two extreme ends, beam search and exhaustive search are studied intensively. In this paper, we examine a range of search depths between the two extremes to discover the desirable search depth. To this end, we introduce Lookahead Beam Search (LBS), a multi-step lookahead search that optimizes the objective considering a fixed number of future steps. Beam search and exhaustive search are special cas
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#21355;&#26143;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;HYPSO-1&#21355;&#26143;&#30340;&#28023;&#38470;&#20113;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#19988;&#36890;&#36807;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13679</link><description>&lt;p&gt;
&#19968;&#39063;&#24320;&#25918;&#30340;&#21355;&#26143;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#19982;&#26469;&#33258;HYPSO-1&#21355;&#26143;&#30340;&#28023;&#38470;&#20113;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
An Open Hyperspectral Dataset with Sea-Land-Cloud Ground-Truth from the HYPSO-1 Satellite. (arXiv:2308.13679v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13679
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#21355;&#26143;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;HYPSO-1&#21355;&#26143;&#30340;&#28023;&#38470;&#20113;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#19988;&#36890;&#36807;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#25104;&#20687;&#34987;&#21355;&#26143;&#29992;&#20110;&#31354;&#38388;&#36965;&#24863;&#65292;&#22312;&#20687;HYPSO-1&#36825;&#26679;&#30340;&#21355;&#26143;&#19978;&#65292;&#38754;&#20020;&#30528;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#24433;&#21709;&#20102;&#23545;&#38656;&#27714;&#36825;&#20123;&#22320;&#38754;&#30495;&#23454;&#26631;&#27880;&#30340;AI&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HYPSO-1&#28023;&#38470;&#20113;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;200&#20010;&#19981;&#21516;&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;&#20110;HYPSO-1&#20219;&#21153;&#65292;&#21487;&#25552;&#20379;&#26410;&#32463;&#26657;&#20934;&#21644;&#32463;&#36807;&#26657;&#20934;&#30340;&#24418;&#24335;&#65292;&#20379;&#22320;&#29699;&#35266;&#27979;&#30340;&#31185;&#23398;&#30740;&#31350;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;38&#20010;&#22270;&#20687;&#65292;&#22312;&#20687;&#32032;&#32423;&#21035;&#19978;&#21253;&#21547;&#24635;&#35745;&#32422;2500&#19975;&#20010;&#20026;&#28023;&#27915;/&#38470;&#22320;/&#20113;&#20998;&#31867;&#26631;&#35760;&#30340;&#20809;&#35889;&#29305;&#24449;&#12290;&#20026;&#20102;&#23637;&#31034;&#25968;&#25454;&#38598;&#21450;&#20854;&#26631;&#35760;&#23376;&#38598;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;1D&#23436;&#20840;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#12290;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#65292;&#22320;&#38754;&#30495;&#23454;&#26631;&#27880;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#36719;&#20214;&#20195;&#30721;&#21487;&#22312;&#32593;&#31449;https://ntnu-sm&#19978;&#20813;&#36153;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral Imaging, employed in satellites for space remote sensing, like HYPSO-1, faces constraints due to few labeled data sets, affecting the training of AI models demanding these ground-truth annotations. In this work, we introduce The HYPSO-1 Sea-Land-Cloud-Labeled Dataset, an open dataset with 200 diverse hyperspectral images from the HYPSO-1 mission, available in both raw and calibrated forms for scientific research in Earth observation. Moreover, 38 of these images from different countries include ground-truth labels at pixel-level totaling about 25 million spectral signatures labeled for sea/land/cloud categories. To demonstrate the potential of the dataset and its labeled subset, we have additionally optimized a deep learning model (1D Fully Convolutional Network), achieving superior performance to the current state of the art. The complete dataset, ground-truth labels, deep learning model, and software code are openly accessible for download at the website https://ntnu-sm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22823;&#23567;&#21644;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#21542;&#28085;&#30422;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#25299;&#25169;&#21644;&#35821;&#20041;&#23646;&#24615;&#65292;&#36825;&#23545;&#20110;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.13676</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31526;&#21495;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Rethinking Language Models as Symbolic Knowledge Graphs. (arXiv:2308.13676v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22823;&#23567;&#21644;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#21542;&#28085;&#30422;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#25299;&#25169;&#21644;&#35821;&#20041;&#23646;&#24615;&#65292;&#36825;&#23545;&#20110;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#30693;&#35782;&#22270;&#35889;&#22312;&#25628;&#32034;&#12289;&#38382;&#31572;&#21644;&#25512;&#33616;&#31561;&#20197;&#30693;&#35782;&#20026;&#20013;&#24515;&#30340;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#38543;&#30528;&#24403;&#20195;&#22522;&#20110;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#26159;&#21542;&#33021;&#22815;&#19982;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#30693;&#35782;&#30456;&#21305;&#37197;&#12290;&#21508;&#31181;&#26041;&#27861;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#25110;&#35757;&#32451;&#25968;&#25454;&#37327;&#21487;&#20197;&#22686;&#24378;&#20854;&#26816;&#32034;&#31526;&#21495;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#36890;&#24120;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#28085;&#30422;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#25299;&#25169;&#21644;&#35821;&#20041;&#23646;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#20123;&#23646;&#24615;&#23545;&#20110;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#22823;&#23567;&#21644;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20061;&#20010;&#23450;&#24615;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#23646;&#24615;&#65292;&#21253;&#25324;&#23545;&#31216;&#24615;&#12289;&#19981;&#23545;&#31216;&#24615;&#12289;
&lt;/p&gt;
&lt;p&gt;
Symbolic knowledge graphs (KGs) play a pivotal role in knowledge-centric applications such as search, question answering and recommendation. As contemporary language models (LMs) trained on extensive textual data have gained prominence, researchers have extensively explored whether the parametric knowledge within these models can match up to that present in knowledge graphs. Various methodologies have indicated that enhancing the size of the model or the volume of training data enhances its capacity to retrieve symbolic knowledge, often with minimal or no human supervision. Despite these advancements, there is a void in comprehensively evaluating whether LMs can encompass the intricate topological and semantic attributes of KGs, attributes crucial for reasoning processes. In this work, we provide an exhaustive evaluation of language models of varying sizes and capabilities. We construct nine qualitative benchmarks that encompass a spectrum of attributes including symmetry, asymmetry, h
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#27010;&#29575;&#36710;&#36947;&#22270;&#29983;&#25104;&#21644;&#35299;&#37322;&#35282;&#33853;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;&#35282;&#33853;&#24773;&#20917;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13658</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#27010;&#29575;&#36710;&#36947;&#22270;&#26469;&#29983;&#25104;&#21644;&#35299;&#37322;&#35282;&#33853;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Generating and Explaining Corner Cases Using Learnt Probabilistic Lane Graphs. (arXiv:2308.13658v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#27010;&#29575;&#36710;&#36947;&#22270;&#29983;&#25104;&#21644;&#35299;&#37322;&#35282;&#33853;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;&#35282;&#33853;&#24773;&#20917;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#21160;&#24577;&#29615;&#22659;&#20013;&#39564;&#35777;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#30340;&#23433;&#20840;&#24615;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36710;&#36742;&#26368;&#32456;&#20250;&#36935;&#21040;&#27809;&#26377;&#20195;&#34920;&#24615;&#35757;&#32451;&#25968;&#25454;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#12290;&#36890;&#36807;&#22312;&#22522;&#20110;&#27169;&#25311;&#30340;&#22330;&#26223;&#27979;&#35797;&#20013;&#22686;&#21152;&#19981;&#21516;&#30340;&#36947;&#36335;&#21644;&#20132;&#36890;&#26465;&#20214;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#21253;&#25324;&#35282;&#33853;&#24773;&#20917;&#65292;&#21487;&#20197;&#25552;&#39640;AV&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#21253;&#21547;&#22810;&#20010;&#20195;&#29702;&#30340;&#35282;&#33853;&#24773;&#20917;&#22330;&#26223;&#30340;&#21019;&#24314;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35753;&#24037;&#31243;&#24072;&#22522;&#20110;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;&#35282;&#33853;&#24773;&#20917;&#65292;&#24182;&#35299;&#37322;&#20026;&#20160;&#20040;&#36825;&#20123;&#24773;&#20917;&#26159;&#23433;&#20840;&#20851;&#38190;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27010;&#29575;&#36710;&#36947;&#22270;&#65288;PLGs&#65289;&#26469;&#25551;&#36848;&#36710;&#36742;&#21487;&#33021;&#34892;&#39542;&#30340;&#26377;&#38480;&#19968;&#32452;&#36710;&#36947;&#20301;&#32622;&#21644;&#26041;&#21521;&#12290;PLGs&#30340;&#32467;&#26500;&#26159;&#30452;&#25509;&#20174;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#12290;&#22270;&#27169;&#22411;&#20197;&#27010;&#29575;&#31574;&#30053;&#30340;&#24418;&#24335;&#34920;&#31034;&#39550;&#39542;&#21592;&#23545;&#32473;&#23450;&#29366;&#24577;&#30340;&#21709;&#24212;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Validating the safety of Autonomous Vehicles (AVs) operating in open-ended, dynamic environments is challenging as vehicles will eventually encounter safety-critical situations for which there is not representative training data. By increasing the coverage of different road and traffic conditions and by including corner cases in simulation-based scenario testing, the safety of AVs can be improved. However, the creation of corner case scenarios including multiple agents is non-trivial. Our approach allows engineers to generate novel, realistic corner cases based on historic traffic data and to explain why situations were safety-critical. In this paper, we introduce Probabilistic Lane Graphs (PLGs) to describe a finite set of lane positions and directions in which vehicles might travel. The structure of PLGs is learnt directly from spatio-temporal traffic data. The graph model represents the actions of the drivers in response to a given state in the form of a probabilistic policy. We use
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#20195;&#20113;&#26381;&#21153;&#20013;&#33258;&#21160;&#32034;&#24341;&#35843;&#20248;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24037;&#20316;&#36127;&#36733;&#36873;&#25321;&#12289;&#20505;&#36873;&#32034;&#24341;&#36807;&#28388;&#12289;&#21152;&#36895;&#32034;&#24341;&#37197;&#32622;&#25628;&#32034;&#12289;&#20943;&#23569;&#26597;&#35810;&#20248;&#21270;&#22120;&#35843;&#29992;&#30340;&#25968;&#37327;&#21644;&#38477;&#20302;&#24615;&#33021;&#22238;&#24402;&#26426;&#20250;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.13641</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#32034;&#24341;&#35843;&#20248;&#65306;&#26368;&#26032;&#36827;&#23637;&#19982;&#24320;&#25918;&#25361;&#25112;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
ML-Powered Index Tuning: An Overview of Recent Progress and Open Challenges. (arXiv:2308.13641v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#20195;&#20113;&#26381;&#21153;&#20013;&#33258;&#21160;&#32034;&#24341;&#35843;&#20248;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24037;&#20316;&#36127;&#36733;&#36873;&#25321;&#12289;&#20505;&#36873;&#32034;&#24341;&#36807;&#28388;&#12289;&#21152;&#36895;&#32034;&#24341;&#37197;&#32622;&#25628;&#32034;&#12289;&#20943;&#23569;&#26597;&#35810;&#20248;&#21270;&#22120;&#35843;&#29992;&#30340;&#25968;&#37327;&#21644;&#38477;&#20302;&#24615;&#33021;&#22238;&#24402;&#26426;&#20250;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20113;&#26381;&#21153;&#20013;&#24037;&#20316;&#36127;&#36733;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#33258;&#21160;&#32034;&#24341;&#35843;&#20248;&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#8212;&#8212;&#22312;&#20445;&#25345;&#32034;&#24341;&#35843;&#20248;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#25512;&#33616;&#39640;&#36136;&#37327;&#30340;&#32034;&#24341;&#12290;&#36825;&#19968;&#25361;&#25112;&#36827;&#19968;&#27493;&#21463;&#21040;&#33258;&#21160;&#32034;&#24341;&#23454;&#29616;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#24341;&#20837;&#26368;&#23567;&#26597;&#35810;&#24615;&#33021;&#22238;&#24402;&#30340;&#35201;&#27714;&#30340;&#24433;&#21709;&#65292;&#36825;&#26500;&#25104;&#20102;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#21644;&#20840;&#33258;&#21160;&#21270;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#20851;&#27880;&#33258;&#21160;&#32034;&#24341;&#35843;&#20248;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#25552;&#20379;&#30340;&#26032;&#26426;&#36935;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22312;&#24037;&#20316;&#36127;&#36733;&#36873;&#25321;&#12289;&#20505;&#36873;&#32034;&#24341;&#36807;&#28388;&#12289;&#21152;&#36895;&#32034;&#24341;&#37197;&#32622;&#25628;&#32034;&#12289;&#20943;&#23569;&#26597;&#35810;&#20248;&#21270;&#22120;&#35843;&#29992;&#30340;&#25968;&#37327;&#20197;&#21450;&#38477;&#20302;&#24615;&#33021;&#22238;&#24402;&#26426;&#20250;&#26041;&#38754;&#24320;&#23637;&#30340;&#26368;&#26032;&#24037;&#20316;&#12290;&#25105;&#20204;&#24378;&#35843;&#36825;&#20123;&#24037;&#20316;&#30340;&#20851;&#38190;&#35201;&#28857;&#65292;&#24182;&#24378;&#35843;&#20854;&#21019;&#26032;&#19982;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scale and complexity of workloads in modern cloud services have brought into sharper focus a critical challenge in automated index tuning -- the need to recommend high-quality indexes while maintaining index tuning scalability. This challenge is further compounded by the requirement for automated index implementations to introduce minimal query performance regressions in production deployments, representing a significant barrier to achieving scalability and full automation. This paper directs attention to these challenges within automated index tuning and explores ways in which machine learning (ML) techniques provide new opportunities in their mitigation. In particular, we reflect on recent efforts in developing ML techniques for workload selection, candidate index filtering, speeding up index configuration search, reducing the amount of query optimizer calls, and lowering the chances of performance regressions. We highlight the key takeaways from these efforts and underline the g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#20351;&#29992;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.13633</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30333;&#21270;&#65306;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adaptive whitening with fast gain modulation and slow synaptic plasticity. (arXiv:2308.13633v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#20351;&#29992;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#24863;&#35273;&#21306;&#30340;&#31070;&#32463;&#20803;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#20854;&#20010;&#20307;&#21709;&#24212;&#30340;&#26041;&#24046;&#36827;&#34892;&#24402;&#19968;&#21270;&#20197;&#21450;&#20943;&#23569;&#21709;&#24212;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#36716;&#25442;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#30333;&#21270;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#30340;&#26426;&#21046;&#27169;&#22411;&#21482;&#20351;&#29992;&#31361;&#35302;&#21487;&#22609;&#24615;&#25110;&#22686;&#30410;&#35843;&#21046;&#20316;&#20026;&#36866;&#24212;&#30340;&#29983;&#29289;&#22522;&#36136;&#65292;&#28982;&#32780;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#26377;&#26174;&#33879;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#33539;&#24615;&#30340;&#22810;&#26102;&#38388;&#23610;&#24230;&#26426;&#21046;&#27169;&#22411;&#65292;&#36890;&#36807;&#31361;&#35302;&#21487;&#22609;&#24615;&#21644;&#22686;&#30410;&#35843;&#21046;&#30340;&#35745;&#31639;&#35282;&#33394;&#26469;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#30333;&#21270;&#12290;&#22686;&#30410;&#22312;&#24555;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#26681;&#25454;&#24403;&#21069;&#30340;&#32479;&#35745;&#24773;&#20917;&#36827;&#34892;&#35843;&#25972;&#65292;&#32780;&#31361;&#35302;&#22312;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#35843;&#25972;&#65292;&#23398;&#20064;&#36755;&#20837;&#32479;&#35745;&#20013;&#19982;&#24773;&#22659;&#26080;&#20851;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26469;&#33258;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26102;&#38388;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to learn structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale
&lt;/p&gt;</description></item><item><title>HiFiHR&#26159;&#19968;&#31181;&#36890;&#36807;&#39640;&#20445;&#30495;&#32441;&#29702;&#22686;&#24378;&#26469;&#33258;&#21333;&#24352;&#22270;&#20687;&#30340;3D&#25163;&#37096;&#37325;&#24314;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#20934;&#30830;&#30340;&#25163;&#37096;&#32593;&#26684;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#31243;&#24230;&#30340;&#30417;&#30563;&#26041;&#24335;&#25913;&#21892;&#25163;&#37096;&#23039;&#24577;&#21644;&#24418;&#29366;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.13628</link><description>&lt;p&gt;
HiFiHR&#65306;&#36890;&#36807;&#39640;&#20445;&#30495;&#32441;&#29702;&#22686;&#24378;&#26469;&#33258;&#21333;&#24352;&#22270;&#20687;&#30340;3D&#25163;&#37096;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
HiFiHR: Enhancing 3D Hand Reconstruction from a Single Image via High-Fidelity Texture. (arXiv:2308.13628v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13628
&lt;/p&gt;
&lt;p&gt;
HiFiHR&#26159;&#19968;&#31181;&#36890;&#36807;&#39640;&#20445;&#30495;&#32441;&#29702;&#22686;&#24378;&#26469;&#33258;&#21333;&#24352;&#22270;&#20687;&#30340;3D&#25163;&#37096;&#37325;&#24314;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#20934;&#30830;&#30340;&#25163;&#37096;&#32593;&#26684;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#31243;&#24230;&#30340;&#30417;&#30563;&#26041;&#24335;&#25913;&#21892;&#25163;&#37096;&#23039;&#24577;&#21644;&#24418;&#29366;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HiFiHR&#65292;&#19968;&#31181;&#39640;&#20445;&#30495;&#25163;&#37096;&#37325;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#28210;&#26579;&#21644;&#27604;&#36739;&#33021;&#21147;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#20934;&#30830;&#30340;3D&#25163;&#37096;&#32593;&#26684;&#65292;&#24182;&#24674;&#22797;&#30495;&#23454;&#30340;&#32441;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#39044;&#23450;&#20041;&#32441;&#29702;&#36164;&#28304;&#30340;&#21442;&#25968;&#21270;&#25163;&#37096;&#27169;&#22411;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22312;&#28210;&#26579;&#21644;&#36755;&#20837;&#22270;&#20687;&#20043;&#38388;&#24314;&#31435;&#32441;&#29702;&#37325;&#24314;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#21331;&#36234;&#30340;&#32441;&#29702;&#37325;&#24314;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#23545;&#32593;&#32476;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#31649;&#36947;&#20197;&#19981;&#21516;&#31243;&#24230;&#30340;&#30417;&#30563;&#26041;&#24335;&#65288;&#33258;&#30417;&#30563;&#12289;&#24369;&#30417;&#30563;&#21644;&#20840;&#30417;&#30563;&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#35752;&#35770;&#20102;&#23398;&#20064;&#21040;&#30340;&#39640;&#20445;&#30495;&#32441;&#29702;&#22312;&#25913;&#21892;&#25163;&#37096;&#23039;&#24577;&#21644;&#24418;&#29366;&#20272;&#35745;&#20013;&#30340;&#19981;&#21516;&#36129;&#29486;&#27700;&#24179;&#12290;&#22312;&#21253;&#25324;FreiHAND&#21644;HO-3D&#22312;&#20869;&#30340;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32441;&#29702;&#37325;&#24314;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25163;&#37096;&#37325;&#24314;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present HiFiHR, a high-fidelity hand reconstruction approach that utilizes render-and-compare in the learning-based framework from a single image, capable of generating visually plausible and accurate 3D hand meshes while recovering realistic textures. Our method achieves superior texture reconstruction by employing a parametric hand model with predefined texture assets, and by establishing a texture reconstruction consistency between the rendered and input images during training. Moreover, based on pretraining the network on an annotated dataset, we apply varying degrees of supervision using our pipeline, i.e., self-supervision, weak supervision, and full supervision, and discuss the various levels of contributions of the learned high-fidelity textures in enhancing hand pose and shape estimation. Experimental results on public benchmarks including FreiHAND and HO-3D demonstrate that our method outperforms the state-of-the-art hand reconstruction methods in texture reconstruction qu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30340;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#24182;&#27604;&#36739;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65292;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#22914;&#20309;&#36890;&#36807;AI&#24037;&#20855;&#25903;&#25345;&#30002;&#29366;&#33146;&#30284;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;</title><link>http://arxiv.org/abs/2308.13592</link><description>&lt;p&gt;
&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;: &#25216;&#26415;&#12289;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
AI in Thyroid Cancer Diagnosis: Techniques, Trends, and Future Directions. (arXiv:2308.13592v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30340;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#24182;&#27604;&#36739;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65292;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#22914;&#20309;&#36890;&#36807;AI&#24037;&#20855;&#25903;&#25345;&#30002;&#29366;&#33146;&#30284;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21019;&#24314;&#26234;&#33021;&#35786;&#26029;&#31995;&#32479;&#20197;&#21327;&#21161;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#20998;&#26512;&#21644;&#22788;&#29702;&#27835;&#30103;&#26080;&#27861;&#27835;&#24840;&#30142;&#30149;&#30340;&#22823;&#25968;&#25454;&#26041;&#38754;&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#26816;&#27979;&#30002;&#29366;&#33146;&#30284;&#65292;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#36827;&#34892;&#30002;&#29366;&#33146;&#30284;&#39044;&#21518;&#35780;&#20272;&#21644;&#30830;&#23450;&#24739;&#32773;&#24694;&#24615;&#39118;&#38505;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#24635;&#32467;&#20102;&#19982;&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30456;&#20851;&#30340;&#22823;&#37327;&#25991;&#31456;&#12290;&#30456;&#24212;&#22320;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#26681;&#25454;&#25152;&#20351;&#29992;&#30340;AI&#31639;&#27861;&#12289;&#26694;&#26550;&#30340;&#30446;&#30340;&#21644;&#35745;&#31639;&#24179;&#21488;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#26681;&#25454;&#20854;&#29305;&#24449;&#27604;&#36739;&#20102;&#29616;&#26377;&#30340;&#30002;&#29366;&#33146;&#30284;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#26377;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#25110;&#28151;&#21512;&#30340;AI&#24037;&#20855;&#22914;&#20309;&#25903;&#25345;&#30002;&#29366;&#33146;&#30284;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a growing interest in creating intelligent diagnostic systems to assist medical professionals in analyzing and processing big data for the treatment of incurable diseases. One of the key challenges in this field is detecting thyroid cancer, where advancements have been made using machine learning (ML) and big data analytics to evaluate thyroid cancer prognosis and determine a patient's risk of malignancy. This review paper summarizes a large collection of articles related to artificial intelligence (AI)-based techniques used in the diagnosis of thyroid cancer. Accordingly, a new classification was introduced to classify these techniques based on the AI algorithms used, the purpose of the framework, and the computing platforms used. Additionally, this study compares existing thyroid cancer datasets based on their features. The focus of this study is on how AI-based tools can support the diagnosis and treatment of thyroid cancer, through supervised, unsupervised, or hybrid
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;FrFT&#21644;Transformer&#30340;&#32852;&#21512;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#34920;&#31034;&#21644;&#35782;&#21035;&#20809;&#32420;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22833;&#30495;&#12290;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13575</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;FrFT&#31639;&#27861;&#20272;&#35745;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22833;&#30495;
&lt;/p&gt;
&lt;p&gt;
FrFT based estimation of linear and nonlinear impairments using Vision Transformer. (arXiv:2308.13575v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;FrFT&#21644;Transformer&#30340;&#32852;&#21512;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#34920;&#31034;&#21644;&#35782;&#21035;&#20809;&#32420;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22833;&#30495;&#12290;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#20809;&#32420;&#36890;&#20449;&#31995;&#32479;&#30340;&#24773;&#20917;&#65292;&#24517;&#39035;&#23454;&#26045;&#20197;&#19979;&#22235;&#20010;&#20851;&#38190;&#22833;&#30495;&#30340;&#32852;&#21512;&#20272;&#35745;&#65306;&#38750;&#32447;&#24615;&#20449;&#22122;&#27604;&#65288;SNRNL&#65289;&#65292;&#20809;&#20449;&#22122;&#27604;&#65288;OSNR&#65289;&#65292;&#33394;&#25955;&#65288;CD&#65289;&#21644;&#24046;&#20998;&#32452;&#24310;&#36831;&#65288;DGD&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#33021;&#21147;&#30340;&#38480;&#21046;&#21644;&#22833;&#30495;&#32479;&#19968;&#34920;&#31034;&#30340;&#32570;&#20047;&#65292;&#24403;&#21069;&#30740;&#31350;&#21482;&#33021;&#22312;&#26377;&#38480;&#30340;&#33539;&#22260;&#20869;&#35782;&#21035;&#23569;&#25968;&#22833;&#30495;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20998;&#25968;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FrFT&#65289;&#30340;&#26102;&#39057;&#20449;&#21495;&#22788;&#29702;&#26469;&#23454;&#29616;&#22833;&#30495;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21516;&#26102;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26469;&#31361;&#30772;&#32593;&#32476;&#24615;&#33021;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;5&#36890;&#36947;&#20559;&#25391;&#20998;&#22797;&#29992;&#22235;&#30456;&#31227;&#38190;&#25511;&#65288;PDM-QPSK&#65289;&#38271;&#36317;&#31163;&#20809;&#20256;&#36755;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comprehensively assess optical fiber communication system conditions, it is essential to implement joint estimation of the following four critical impairments: nonlinear signal-to-noise ratio (SNRNL), optical signal-to-noise ratio (OSNR), chromatic dispersion (CD) and differential group delay (DGD). However, current studies only achieve identifying a limited number of impairments within a narrow range, due to limitations in network capabilities and lack of unified representation of impairments. To address these challenges, we adopt time-frequency signal processing based on fractional Fourier transform (FrFT) to achieve the unified representation of impairments, while employing a Transformer based neural networks (NN) to break through network performance limitations. To verify the effectiveness of the proposed estimation method, the numerical simulation is carried on a 5-channel polarization-division-multiplexed quadrature phase shift keying (PDM-QPSK) long haul optical transmission 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLLM-DataEngine&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#24369;&#28857;&#65292;&#29983;&#25104;&#36866;&#24403;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.13566</link><description>&lt;p&gt;
MLLM-DataEngine&#65306;&#19968;&#31181;MLLM&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MLLM-DataEngine: An Iterative Refinement Approach for MLLM. (arXiv:2308.13566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLLM-DataEngine&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#24369;&#28857;&#65292;&#29983;&#25104;&#36866;&#24403;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25351;&#23548;&#25968;&#25454;&#38598;&#26500;&#24314;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#29420;&#31435;&#24615;&#20351;&#24471;&#24403;&#21069;&#30340;MLLM&#24456;&#38590;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#20154;&#21147;&#25104;&#26412;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23553;&#38381;&#24490;&#29615;&#31995;&#32479;MLLM-DataEngine&#65292;&#23427;&#36830;&#25509;&#20102;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#22312;&#27599;&#20010;&#24490;&#29615;&#36845;&#20195;&#20013;&#65292;MLLM-DataEngine&#39318;&#20808;&#26681;&#25454;&#35780;&#20272;&#32467;&#26524;&#20998;&#26512;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#28982;&#21518;&#29983;&#25104;&#21512;&#36866;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#29992;&#20110;&#19979;&#19968;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#19982;&#22522;&#20934;&#27979;&#35797;&#20998;&#31163;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#37117;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental datas
&lt;/p&gt;</description></item><item><title>&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#21475;(ChatGPT, BARD&#21644;GPT4)&#22312;&#20998;&#26512;&#20107;&#25925;&#21465;&#36848;&#20013;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#25552;&#21462;&#20107;&#25925;&#30456;&#20851;&#20449;&#24687;&#21644;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#37117;&#20855;&#26377;&#19968;&#23450;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.13563</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#20107;&#25925;&#21465;&#36848;&#20013;&#30340;&#24212;&#29992;&#8212;&#8212;ChatGPT&#12289;BARD&#21644;GPT-4&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Analyzing Crash Narratives -- A Comparative Study of ChatGPT, BARD and GPT-4. (arXiv:2308.13563v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13563
&lt;/p&gt;
&lt;p&gt;
&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#21475;(ChatGPT, BARD&#21644;GPT4)&#22312;&#20998;&#26512;&#20107;&#25925;&#21465;&#36848;&#20013;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#25552;&#21462;&#20107;&#25925;&#30456;&#20851;&#20449;&#24687;&#21644;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#37117;&#20855;&#26377;&#19968;&#23450;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#36890;&#23433;&#20840;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#25991;&#26412;&#20998;&#26512;&#20174;&#20107;&#25925;&#21465;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20102;&#35299;&#27969;&#34892;&#30340;LLM&#25509;&#21475;&#22312;&#20998;&#31867;&#25110;&#20174;&#20107;&#25925;&#21465;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#34920;&#29616;&#23558;&#38750;&#24120;&#26377;&#29992;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#30446;&#21069;&#26368;&#27969;&#34892;&#30340;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25509;&#21475;&#8212;&#8212;ChatGPT&#12289;BARD&#21644;GPT4&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23427;&#20204;&#22312;&#25552;&#21462;&#20449;&#24687;&#21644;&#22238;&#31572;&#19982;&#20107;&#25925;&#26377;&#20851;&#30340;&#26597;&#35810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#20174;&#29233;&#33655;&#21326;&#24030;&#21644;&#22570;&#33832;&#26031;&#24030;&#30340;100&#20010;&#20107;&#25925;&#21465;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#23545;&#26597;&#35810;&#30340;&#21709;&#24212;&#12290;&#20116;&#20010;&#19982;&#21465;&#36848;&#30456;&#20851;&#30340;&#38382;&#39064;&#34987;&#25552;&#20986;&#65306;1&#65289;&#35841;&#26159;&#36131;&#20219;&#26041;&#65311;2&#65289;&#30896;&#25758;&#26041;&#24335;&#26159;&#20160;&#20040;&#65311;3&#65289;&#20107;&#25925;&#21457;&#29983;&#22312;&#24037;&#20316;&#21306;&#21527;&#65311;4&#65289;&#20107;&#25925;&#28041;&#21450;&#34892;&#20154;&#21527;&#65311;5&#65289;&#20107;&#25925;&#20013;&#26377;&#23475;&#20107;&#20214;&#30340;&#39034;&#24207;&#26159;&#20160;&#20040;&#65311;&#23545;&#20110;&#31532;1&#21040;&#31532;4&#20010;&#38382;&#39064;&#65292;&#19977;&#20010;LLM&#25509;&#21475;&#30340;&#22238;&#31572;&#37117;&#32463;&#36807;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traffic safety research, extracting information from crash narratives using text analysis is a common practice. With recent advancements of large language models (LLM), it would be useful to know how the popular LLM interfaces perform in classifying or extracting information from crash narratives. To explore this, our study has used the three most popular publicly available LLM interfaces- ChatGPT, BARD and GPT4. This study investigated their usefulness and boundaries in extracting information and answering queries related to accidents from 100 crash narratives from Iowa and Kansas. During the investigation, their capabilities and limitations were assessed and their responses to the queries were compared. Five questions were asked related to the narratives: 1) Who is at-fault? 2) What is the manner of collision? 3) Has the crash occurred in a work-zone? 4) Did the crash involve pedestrians? and 5) What are the sequence of harmful events in the crash? For questions 1 through 4, the o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20114;&#21160;&#24335;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#25968;&#25454;&#25193;&#20805;&#20013;&#24341;&#20837;&#30340;&#20559;&#35265;&#65292;&#20197;&#21450;&#29992;&#20110;&#24230;&#37327;&#20559;&#35265;&#21152;&#37325;&#31243;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#26377;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#21487;&#29992;&#65292;&#20294;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#24230;&#37327;&#19981;&#21516;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#20559;&#35265;&#21152;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.13554</link><description>&lt;p&gt;
&#12298;&#23545;GAN&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#36827;&#34892;&#23450;&#37327;&#21270;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#12299;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study on Quantifying Bias in GAN-Augmented Data. (arXiv:2308.13554v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13554
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20114;&#21160;&#24335;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#25968;&#25454;&#25193;&#20805;&#20013;&#24341;&#20837;&#30340;&#20559;&#35265;&#65292;&#20197;&#21450;&#29992;&#20110;&#24230;&#37327;&#20559;&#35265;&#21152;&#37325;&#31243;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#26377;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#21487;&#29992;&#65292;&#20294;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#24230;&#37327;&#19981;&#21516;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#20559;&#35265;&#21152;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26368;&#36817;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#32773;&#20351;&#29992;&#30340;&#27969;&#34892;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#23384;&#22312;&#25152;&#35859;&#30340;&#27169;&#24335;&#23849;&#28291;&#25925;&#38556;&#27169;&#24335;&#65292;&#20351;&#20854;&#23481;&#26131;&#21152;&#21095;&#24050;&#32463;&#20559;&#26012;&#25968;&#25454;&#38598;&#19978;&#30340;&#20559;&#35265;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#27604;&#35757;&#32451;&#20998;&#24067;&#26356;&#19981;&#22810;&#26679;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23450;&#37327;&#21270;&#27169;&#24335;&#23849;&#28291;&#31243;&#24230;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#19968;&#39033;&#31995;&#32479;&#24615;&#30340;&#24037;&#20316;&#65292;&#37325;&#28857;&#35780;&#20272;&#21487;&#33021;&#23545;GAN&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#23450;&#37327;&#21270;&#30340;&#26368;&#26032;&#24230;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#26377;&#20960;&#31181;&#26041;&#27861;&#21487;&#29992;&#65292;&#20294;&#27809;&#26377;&#21333;&#19968;&#30340;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#23450;&#37327;&#21270;&#19981;&#21516;&#22270;&#20687;&#39046;&#22495;&#30340;&#20559;&#35265;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) have recently become a popular data augmentation technique used by machine learning practitioners. However, they have been shown to suffer from the so-called mode collapse failure mode, which makes them vulnerable to exacerbating biases on already skewed datasets, resulting in the generated data distribution being less diverse than the training distribution. To this end, we address the problem of quantifying the extent to which mode collapse occurs. This study is a systematic effort focused on the evaluation of state-of-the-art metrics that can potentially quantify biases in GAN-augmented data. We show that, while several such methods are available, there is no single metric that quantifies bias exacerbation reliably over the span of different image domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20249;&#20276;&#33310;&#32773;&#29983;&#25104;&#30340;&#22810;&#33310;&#32773;&#21512;&#25104;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#20445;&#25345;&#19982;&#20027;&#23548;&#33310;&#32773;&#26102;&#38388;&#21327;&#35843;&#30340;&#21516;&#26102;&#30830;&#20445;&#20249;&#20276;&#33310;&#32773;&#30340;&#21487;&#25511;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#19982;&#20320;&#20849;&#33310;&#8221;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;&#65288;DanY&#65289;&#65292;&#23427;&#33021;&#33258;&#21160;&#35774;&#35745;&#20249;&#20276;&#33310;&#32773;&#30340;&#23039;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.13551</link><description>&lt;p&gt;
&#19982;&#20320;&#20849;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#22810;&#26679;&#24615;&#21487;&#25511;&#30340;&#33310;&#32773;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dance with You: The Diversity Controllable Dancer Generation via Diffusion Models. (arXiv:2308.13551v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20249;&#20276;&#33310;&#32773;&#29983;&#25104;&#30340;&#22810;&#33310;&#32773;&#21512;&#25104;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#20445;&#25345;&#19982;&#20027;&#23548;&#33310;&#32773;&#26102;&#38388;&#21327;&#35843;&#30340;&#21516;&#26102;&#30830;&#20445;&#20249;&#20276;&#33310;&#32773;&#30340;&#21487;&#25511;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#19982;&#20320;&#20849;&#33310;&#8221;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;&#65288;DanY&#65289;&#65292;&#23427;&#33021;&#33258;&#21160;&#35774;&#35745;&#20249;&#20276;&#33310;&#32773;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#34394;&#25311;&#29615;&#22659;&#20013;&#29992;&#20110;&#20154;&#38469;&#20132;&#20114;&#30340;&#25968;&#23383;&#20154;&#31867;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#22810;&#33310;&#32773;&#21512;&#25104;&#20219;&#21153;&#65292;&#31216;&#20026;&#20249;&#20276;&#33310;&#32773;&#29983;&#25104;&#65292;&#20854;&#28041;&#21450;&#21512;&#25104;&#33021;&#22815;&#19982;&#29992;&#25143;&#19968;&#36215;&#36339;&#33310;&#30340;&#34394;&#25311;&#20154;&#31867;&#33310;&#32773;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#25511;&#21046;&#20027;&#23548;&#33310;&#32773;&#21644;&#20249;&#20276;&#33310;&#32773;&#20043;&#38388;&#30340;&#23039;&#21183;&#22810;&#26679;&#24615;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#26680;&#24515;&#26159;&#30830;&#20445;&#29983;&#25104;&#30340;&#20249;&#20276;&#33310;&#32773;&#20855;&#26377;&#21487;&#25511;&#30340;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#19982;&#20027;&#23548;&#33310;&#32773;&#20445;&#25345;&#26102;&#38388;&#19978;&#30340;&#21327;&#35843;&#12290;&#19982;&#20197;&#24448;&#36890;&#36807;&#38899;&#20048;&#39537;&#21160;&#29983;&#25104;&#33310;&#36424;&#21160;&#20316;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#22810;&#26679;&#24615;&#12289;&#20027;&#23548;&#33310;&#32773;&#30340;&#23039;&#21183;&#20197;&#21450;&#20276;&#22863;&#38899;&#20048;&#33258;&#21160;&#35774;&#35745;&#20249;&#20276;&#33310;&#32773;&#30340;&#23039;&#21183;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#19982;&#20320;&#20849;&#33310;&#8221;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;&#65288;DanY&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#32500;&#23039;&#21183;&#25910;&#38598;&#38454;&#27573;&#26469;&#25910;&#38598;&#21508;&#31181;&#22522;&#26412;&#33310;&#36424;&#23039;&#21183;&#20316;&#20026;&#21442;&#32771;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, digital humans for interpersonal interaction in virtual environments have gained significant attention. In this paper, we introduce a novel multi-dancer synthesis task called partner dancer generation, which involves synthesizing virtual human dancers capable of performing dance with users. The task aims to control the pose diversity between the lead dancer and the partner dancer. The core of this task is to ensure the controllable diversity of the generated partner dancer while maintaining temporal coordination with the lead dancer. This scenario varies from earlier research in generating dance motions driven by music, as our emphasis is on automatically designing partner dancer postures according to pre-defined diversity, the pose of lead dancer, as well as the accompanying tunes. To achieve this objective, we propose a three-stage framework called Dance-with-You (DanY). Initially, we employ a 3D Pose Collection stage to collect a wide range of basic dance poses as referenc
&lt;/p&gt;</description></item><item><title>Infinitia&#26159;&#19968;&#20010;&#27169;&#25311;&#28216;&#25103;&#31995;&#32479;&#65292;&#20351;&#29992;&#29983;&#25104;&#22270;&#20687;&#21644;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#29609;&#23478;&#30340;&#25551;&#36848;&#22609;&#36896;&#28216;&#25103;&#22330;&#26223;&#21644;NPC&#65292;&#31867;&#20284;&#20110;&#20840;&#24687;&#33329;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38480;&#29983;&#25104;&#30340;&#24187;&#24819;&#19990;&#30028;&#12289;&#21487;&#25511;&#30340;NPC&#34892;&#20026;&#12289;&#24189;&#40664;&#23545;&#35805;&#12289;&#25104;&#26412;&#21644;&#26102;&#38388;&#25928;&#29575;&#12289;&#29609;&#23478;&#21512;&#20316;&#20197;&#21450;&#28216;&#25103;&#20869;&#20107;&#20214;&#30340;&#38750;&#30830;&#23450;&#24615;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.13548</link><description>&lt;p&gt;
&#26397;&#30528;&#20840;&#24687;&#33329;&#24335;&#27169;&#25311;&#28216;&#25103;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards a Holodeck-style Simulation Game. (arXiv:2308.13548v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13548
&lt;/p&gt;
&lt;p&gt;
Infinitia&#26159;&#19968;&#20010;&#27169;&#25311;&#28216;&#25103;&#31995;&#32479;&#65292;&#20351;&#29992;&#29983;&#25104;&#22270;&#20687;&#21644;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#29609;&#23478;&#30340;&#25551;&#36848;&#22609;&#36896;&#28216;&#25103;&#22330;&#26223;&#21644;NPC&#65292;&#31867;&#20284;&#20110;&#20840;&#24687;&#33329;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38480;&#29983;&#25104;&#30340;&#24187;&#24819;&#19990;&#30028;&#12289;&#21487;&#25511;&#30340;NPC&#34892;&#20026;&#12289;&#24189;&#40664;&#23545;&#35805;&#12289;&#25104;&#26412;&#21644;&#26102;&#38388;&#25928;&#29575;&#12289;&#29609;&#23478;&#21512;&#20316;&#20197;&#21450;&#28216;&#25103;&#20869;&#20107;&#20214;&#30340;&#38750;&#30830;&#23450;&#24615;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Infinitia&#65292;&#19968;&#20010;&#27169;&#25311;&#28216;&#25103;&#31995;&#32479;&#65292;&#22312;&#28216;&#25103;&#26102;&#38388;&#20869;&#20351;&#29992;&#29983;&#25104;&#22270;&#20687;&#21644;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#29609;&#23478;&#30340;&#31616;&#30701;&#25551;&#36848;&#37325;&#26032;&#22609;&#36896;&#28216;&#25103;&#22330;&#26223;&#21644;NPC&#65292;&#31867;&#20284;&#20110;&#34394;&#26500;&#30340;&#20840;&#24687;&#33329;&#20013;&#21019;&#24314;&#35774;&#32622;&#30340;&#26041;&#24335;&#12290;&#22522;&#20110;&#12298;&#29983;&#25104;&#20195;&#29702;&#12299;&#35770;&#25991;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#24341;&#20837;&#20102;&#28216;&#25103;&#24615;&#20803;&#32032;&#65292;&#22914;&#26080;&#38480;&#29983;&#25104;&#30340;&#24187;&#24819;&#19990;&#30028;&#65292;NPC&#34892;&#20026;&#30340;&#21487;&#25511;&#24615;&#65292;&#24189;&#40664;&#23545;&#35805;&#65292;&#25104;&#26412;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#29609;&#23478;&#20043;&#38388;&#30340;&#21512;&#20316;&#20197;&#21450;&#28216;&#25103;&#20869;&#20107;&#20214;&#30340;&#38750;&#30830;&#23450;&#24615;&#20803;&#32032;&#12290;Infinitia&#20351;&#29992;Unity&#24341;&#25806;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#26550;&#26500;&#65292;&#26041;&#20415;&#26410;&#26469;&#31038;&#21306;&#24320;&#21457;&#32773;&#21152;&#20837;&#20196;&#20154;&#20852;&#22859;&#30340;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#29992;&#20102;&#22810;&#20154;&#26694;&#26550;&#65292;&#20801;&#35768;&#29609;&#23478;&#22312;&#27169;&#25311;&#20013;&#23384;&#22312;&#24182;&#36827;&#34892;&#20132;&#20114;&#12290;&#27169;&#25311;&#23558;&#24456;&#24555;&#22312;https://infinitia.ai/&#19978;&#25552;&#20379;&#24320;&#25918;&#24335;&#27979;&#35797;&#29256;&#65292;&#24182;&#19988;&#25105;&#20204;&#26399;&#24453;&#19982;&#31038;&#21306;&#20849;&#21516;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Infinitia, a simulation game system that uses generative image and language models at play time to reshape all aspects of the setting and NPCs based on a short description from the player, in a way similar to how settings are created on the fictional Holodeck. Building off the ideas of the Generative Agents paper, our system introduces gameplay elements, such as infinite generated fantasy worlds, controllability of NPC behavior, humorous dialogue, cost &amp; time efficiency, collaboration between players and elements of non-determinism among in-game events. Infinitia is implemented in the Unity engine with a server-client architecture, facilitating the addition of exciting features by community developers in the future. Furthermore, it uses a multiplayer framework to allow humans to be present and interact in the simulation. The simulation will be available in open-alpha shortly at https://infinitia.ai/ and we are looking forward to building upon it with the community.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#24341;&#20837;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25506;&#31350;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.13546</link><description>&lt;p&gt;
Hyperscanning EEG&#30340;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#25581;&#31034;&#20102;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;
&lt;/p&gt;
&lt;p&gt;
Functional Graph Contrastive Learning of Hyperscanning EEG Reveals Emotional Contagion Evoked by Stereotype-Based Stressors. (arXiv:2308.13546v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#24341;&#20837;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25506;&#31350;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#24773;&#32490;&#20256;&#26579;&#30340;&#32454;&#24494;&#24046;&#24322;&#21450;&#20854;&#23545;&#21452;&#20154;&#20114;&#21160;&#20013;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#32858;&#28966;&#20110;&#22899;&#24615;&#23545;&#30340;&#21512;&#20316;&#35299;&#20915;&#38382;&#39064;&#20219;&#21153;&#20013;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#32972;&#26223;&#12290;&#36890;&#36807;&#23545;&#24773;&#32490;&#20256;&#26579;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#25581;&#31034;&#20854;&#28508;&#22312;&#26426;&#21046;&#21644;&#24433;&#21709;&#12290;&#21033;&#29992;&#22522;&#20110;EEG&#30340;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;fGCL&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#25552;&#21462;&#20027;&#20307;&#19981;&#21464;&#30340;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#34920;&#31034;&#12290;&#36825;&#20123;&#34920;&#31034;&#36827;&#19968;&#27493;&#24212;&#29992;&#21160;&#24577;&#22270;&#20998;&#31867;&#65288;DGC&#65289;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#26088;&#22312;&#21078;&#26512;&#24773;&#32490;&#20256;&#26579;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#33041;&#37096;&#21516;&#27493;&#21644;&#36830;&#25509;&#24615;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#32467;&#26524;&#24378;&#35843;&#24773;&#32490;&#20256;&#26579;&#22312;&#22609;&#36896;&#36712;&#36857;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study delves into the intricacies of emotional contagion and its impact on performance within dyadic interactions. Specifically, it focuses on the context of stereotype-based stress (SBS) during collaborative problem-solving tasks among female pairs. Through an exploration of emotional contagion, the research seeks to unveil its underlying mechanisms and effects. Leveraging EEG-based hyperscanning technology, the study introduces an innovative approach known as functional Graph Contrastive Learning (fGCL), which extracts subject-invariant representations of neural activity patterns. These representations are further subjected to analysis using the Dynamic Graph Classification (DGC) model, aimed at dissecting the process of emotional contagion. By scrutinizing brain synchronization and connectivity, the study reveals the intricate interplay between emotional contagion and cognitive functioning. The results underscore the substantial role of emotional contagion in shaping the trajec
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;LaGR&#65288;&#35821;&#35328;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#25552;&#20986;&#35299;&#20915;&#37096;&#20998;&#23436;&#25104;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2308.13542</link><description>&lt;p&gt;
LaGR-SEQ: &#35821;&#35328;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#19982;&#39640;&#25928;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
LaGR-SEQ: Language-Guided Reinforcement Learning with Sample-Efficient Querying. (arXiv:2308.13542v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;LaGR&#65288;&#35821;&#35328;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#25552;&#20986;&#35299;&#20915;&#37096;&#20998;&#23436;&#25104;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20986;&#20102;&#20182;&#20204;&#36890;&#36807;&#25991;&#26412;&#25552;&#20379;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#28508;&#22312;&#22320;&#29992;&#20110;&#22312;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#39044;&#27979;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#20219;&#21153;&#19982;&#27169;&#24335;&#23436;&#25104;&#26377;&#20851;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#35266;&#23519;&#37096;&#20998;&#22534;&#21472;&#30340;&#31435;&#26041;&#20307;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25512;&#26029;&#35266;&#23519;&#21040;&#30340;&#27169;&#24335;&#65288;&#22914;&#31435;&#26041;&#20307;&#30340;&#22823;&#23567;&#12289;&#39068;&#33394;&#25110;&#20854;&#20182;&#23646;&#24615;&#65289;&#26469;&#39044;&#27979;&#21097;&#20313;&#31435;&#26041;&#20307;&#24212;&#35813;&#22534;&#21472;&#30340;&#27491;&#30830;&#39034;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LaGR&#65288;&#35821;&#35328;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#36825;&#31181;&#39044;&#27979;&#33021;&#21147;&#65292;&#25552;&#20986;&#30001;&#20027;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#37096;&#20998;&#23436;&#25104;&#30340;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#38543;&#21518;&#24341;&#23548;&#21518;&#32773;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;RL&#35757;&#32451;&#36890;&#24120;&#19981;&#20855;&#22791;&#39640;&#25928;&#26679;&#26412;&#21033;&#29992;&#29575;&#65292;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#23558;&#22266;&#26377;&#22320;&#24847;&#21619;&#30528;&#38656;&#35201;&#21453;&#22797;&#26597;&#35810;LLMs&#26469;&#33719;&#21462;&#35299;&#20915;&#26041;&#26696;&#65307;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#21644;&#19981;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently demonstrated their impressive ability to provide context-aware responses via text. This ability could potentially be used to predict plausible solutions in sequential decision making tasks pertaining to pattern completion. For example, by observing a partial stack of cubes, LLMs can predict the correct sequence in which the remaining cubes should be stacked by extrapolating the observed patterns (e.g., cube sizes, colors or other attributes) in the partial stack. In this work, we introduce LaGR (Language-Guided Reinforcement learning), which uses this predictive ability of LLMs to propose solutions to tasks that have been partially completed by a primary reinforcement learning (RL) agent, in order to subsequently guide the latter's training. However, as RL training is generally not sample-efficient, deploying this approach would inherently imply that the LLM be repeatedly queried for solutions; a process that can be expensive and infeasible. T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#28216;&#25103;&#29305;&#24449;&#24314;&#35758;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#65292;&#25552;&#21462;&#20027;&#39064;&#30456;&#20284;&#30340;&#28216;&#25103;&#29305;&#24449;&#24182;&#29983;&#25104;&#26032;&#29305;&#24449;&#12290;&#32463;&#36807;&#29992;&#25143;&#30740;&#31350;&#27604;&#36739;&#65292;&#35813;&#31995;&#32479;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#20154;&#24037;&#24314;&#35758;&#12290;&#35813;&#31995;&#32479;&#26159;&#19968;&#20010;&#19982;&#29992;&#25143;&#22312;&#27010;&#24565;&#23618;&#38754;&#19978;&#36827;&#34892;&#21327;&#20316;&#30340;&#28216;&#25103;&#35774;&#35745;&#21161;&#25163;&#24037;&#20855;&#30340;&#19968;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.13538</link><description>&lt;p&gt;
&#19968;&#20010;&#27010;&#24565;&#28216;&#25103;&#29305;&#24449;&#29983;&#25104;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Preliminary Study on a Conceptual Game Feature Generation and Recommendation System. (arXiv:2308.13538v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#28216;&#25103;&#29305;&#24449;&#24314;&#35758;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#65292;&#25552;&#21462;&#20027;&#39064;&#30456;&#20284;&#30340;&#28216;&#25103;&#29305;&#24449;&#24182;&#29983;&#25104;&#26032;&#29305;&#24449;&#12290;&#32463;&#36807;&#29992;&#25143;&#30740;&#31350;&#27604;&#36739;&#65292;&#35813;&#31995;&#32479;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#20154;&#24037;&#24314;&#35758;&#12290;&#35813;&#31995;&#32479;&#26159;&#19968;&#20010;&#19982;&#29992;&#25143;&#22312;&#27010;&#24565;&#23618;&#38754;&#19978;&#36827;&#34892;&#21327;&#20316;&#30340;&#28216;&#25103;&#35774;&#35745;&#21161;&#25163;&#24037;&#20855;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#28216;&#25103;&#29305;&#24449;&#24314;&#35758;&#30340;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#30340;GLoVe&#27169;&#22411;&#30340;&#35789;&#23884;&#20837;&#26469;&#25552;&#21462;&#20027;&#39064;&#30456;&#20284;&#30340;&#28216;&#25103;&#20013;&#30340;&#29305;&#24449;&#21644;&#23454;&#20307;&#65292;&#24182;&#23558;&#20854;&#36890;&#36807;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#20256;&#36882;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#25143;&#25552;&#31034;&#30340;&#26032;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30701;&#26399;&#29992;&#25143;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#26469;&#33258;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-2&#27169;&#22411;&#12289;&#20351;&#29992;ConceptNet&#30340;&#27169;&#22411;&#20197;&#21450;&#20154;&#24037;&#32534;&#20889;&#30340;&#28216;&#25103;&#29305;&#24449;&#29983;&#25104;&#30340;&#29305;&#24449;&#12290;&#34429;&#28982;&#20154;&#24037;&#24314;&#35758;&#33719;&#24471;&#20102;&#32477;&#22823;&#22810;&#25968;&#30340;&#25237;&#31080;&#65292;&#20294;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#65292;GPT-2&#27169;&#22411;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#20154;&#24037;&#24314;&#35758;&#12290;&#35813;&#31995;&#32479;&#26159;&#19968;&#20010;&#26356;&#22823;&#30340;&#28216;&#25103;&#35774;&#35745;&#21161;&#25163;&#24037;&#20855;&#30340;&#19968;&#37096;&#20998;&#65292;&#33021;&#22815;&#22312;&#27010;&#24565;&#23618;&#38754;&#19978;&#19982;&#29992;&#25143;&#36827;&#34892;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a system used to generate game feature suggestions based on a text prompt. Trained on the game descriptions of almost 60k games, it uses the word embeddings of a small GLoVe model to extract features and entities found in thematically similar games which are then passed through a generator model to generate new features for a user's prompt. We perform a short user study comparing the features generated from a fine-tuned GPT-2 model, a model using the ConceptNet, and human-authored game features. Although human suggestions won the overall majority of votes, the GPT-2 model outperformed the human suggestions in certain games. This system is part of a larger game design assistant tool that is able to collaborate with users at a conceptual level.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#36848;&#21644;&#35299;&#20915;&#26041;&#26696;&#26550;&#26500;&#65292;&#29992;&#20110;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#12289;&#38544;&#31169;&#24863;&#30693;&#30340;&#23545;&#35805;&#22411;AI&#31995;&#32479;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;LLM&#27169;&#22411;&#30340;&#32508;&#21512;&#24037;&#20855;LLMXplorer&#65292;&#24182;&#38416;&#26126;&#20102;&#20854;&#23545;&#31038;&#20250;&#12289;&#20262;&#29702;&#21644;&#30417;&#31649;&#31561;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21160;&#24577;&#19982;LLM&#30340;&#35821;&#35328;&#33021;&#21147;&#26080;&#32541;&#38598;&#25104;&#30340;&#26550;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;AI&#26032;&#38395;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#26550;&#26500;&#25104;&#21151;&#22320;&#34701;&#21512;&#20102;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#19982;&#20107;&#23454;&#30340;&#20005;&#35880;&#24615;&#65292;&#24182;&#22686;&#24378;&#20102;&#25968;&#25454;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13534</link><description>&lt;p&gt;
&#24314;&#31435;&#23545;&#35805;&#22411;AI&#20013;&#30340;&#20449;&#20219;&#65306;&#20351;&#29992;LLMs&#21644;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#12289;&#38544;&#31169;&#24863;&#30693;&#30340;&#31995;&#32479;&#30340;&#32508;&#36848;&#21644;&#35299;&#20915;&#26041;&#26696;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Building Trust in Conversational AI: A Comprehensive Review and Solution Architecture for Explainable, Privacy-Aware Systems using LLMs and Knowledge Graph. (arXiv:2308.13534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#36848;&#21644;&#35299;&#20915;&#26041;&#26696;&#26550;&#26500;&#65292;&#29992;&#20110;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#12289;&#38544;&#31169;&#24863;&#30693;&#30340;&#23545;&#35805;&#22411;AI&#31995;&#32479;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;LLM&#27169;&#22411;&#30340;&#32508;&#21512;&#24037;&#20855;LLMXplorer&#65292;&#24182;&#38416;&#26126;&#20102;&#20854;&#23545;&#31038;&#20250;&#12289;&#20262;&#29702;&#21644;&#30417;&#31649;&#31561;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21160;&#24577;&#19982;LLM&#30340;&#35821;&#35328;&#33021;&#21147;&#26080;&#32541;&#38598;&#25104;&#30340;&#26550;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;AI&#26032;&#38395;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#26550;&#26500;&#25104;&#21151;&#22320;&#34701;&#21512;&#20102;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#19982;&#20107;&#23454;&#30340;&#20005;&#35880;&#24615;&#65292;&#24182;&#22686;&#24378;&#20102;&#25968;&#25454;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#22411;AI&#31995;&#32479;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#23454;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#20132;&#20114;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#38590;&#20197;&#25226;&#25569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;LLMXplorer&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#35814;&#32454;&#23457;&#35270;&#20102;150&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#38416;&#26126;&#20102;&#23427;&#20204;&#20174;&#31038;&#20250;&#12289;&#20262;&#29702;&#21040;&#30417;&#31649;&#30340;&#21508;&#31181;&#24433;&#21709;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#21508;&#34892;&#21508;&#19994;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21151;&#33021;&#26550;&#26500;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21160;&#24577;&#19982;LLMs&#30340;&#35821;&#35328;&#33021;&#21147;&#26080;&#32541;&#38598;&#25104;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;AI&#26032;&#38395;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#24039;&#22937;&#22320;&#34701;&#21512;&#20102;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#19982;&#20107;&#23454;&#30340;&#20005;&#35880;&#24615;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#35282;&#33394;&#30340;&#35775;&#38382;&#25511;&#21046;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;&#25968;&#25454;&#23433;&#20840;&#24615;&#12290;&#26412;&#30740;&#31350;&#20026;&#23545;&#35805;&#22411;AI&#21457;&#23637;&#30340;&#21464;&#21270;&#26223;&#35266;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#65292;&#24378;&#35843;&#20102;&#39640;&#25928;&#12289;&#36879;&#26126;&#30340;&#31995;&#32479;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational AI systems have emerged as key enablers of human-like interactions across diverse sectors. Nevertheless, the balance between linguistic nuance and factual accuracy has proven elusive. In this paper, we first introduce LLMXplorer, a comprehensive tool that provides an in-depth review of over 150 Large Language Models (LLMs), elucidating their myriad implications ranging from social and ethical to regulatory, as well as their applicability across industries. Building on this foundation, we propose a novel functional architecture that seamlessly integrates the structured dynamics of Knowledge Graphs with the linguistic capabilities of LLMs. Validated using real-world AI news data, our architecture adeptly blends linguistic sophistication with factual rigour and further strengthens data security through Role-Based Access Control. This research provides insights into the evolving landscape of conversational AI, emphasizing the imperative for systems that are efficient, transp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12902</link><description>&lt;p&gt;
CDAN: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#20197;&#19981;&#36275;&#30340;&#29031;&#26126;&#20026;&#29305;&#24449;&#65292;&#38754;&#20020;&#28165;&#26224;&#24230;&#20943;&#24369;&#12289;&#39068;&#33394;&#26263;&#28129;&#21644;&#32454;&#33410;&#20943;&#23569;&#30340;&#25361;&#25112;&#12290;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#25913;&#21892;&#20142;&#24230;&#12289;&#23545;&#27604;&#24230;&#21644;&#25972;&#20307;&#24863;&#30693;&#36136;&#37327;&#26469;&#32416;&#27491;&#36825;&#20123;&#38382;&#39064;&#65292;&#20174;&#32780;&#20419;&#36827;&#20934;&#30830;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65288;CDAN&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20302;&#20809;&#22270;&#20687;&#12290;CDAN&#23558;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#19982;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#30456;&#32467;&#21512;&#65292;&#37197;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#12290;&#35813;&#26550;&#26500;&#30830;&#20445;&#20102;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36882;&#21644;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#29615;&#22659;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32597;&#35265;&#30149;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12890</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25237;&#31080;&#65306;&#29992;&#20110;&#32597;&#35265;&#30149;&#35782;&#21035;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#29615;&#22659;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32597;&#35265;&#30149;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24378;&#35843;&#20102;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;LLMs&#32463;&#24120;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#37324;&#20219;&#21153;&#21482;&#20351;&#29992;&#24456;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#25191;&#34892;&#12290;FSL&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;(AI)&#23376;&#39046;&#22495;&#20013;&#21464;&#24471;&#27969;&#34892;&#65292;&#21253;&#25324;&#29992;&#20110;&#20581;&#24247;&#30340;AI&#12290;&#32597;&#35265;&#30149;&#24433;&#21709;&#20154;&#21475;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#22312;&#25968;&#25454;&#21487;&#29992;&#24615;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979; inherently &#38656;&#35201;FSL&#25216;&#26415;&#65292;&#23613;&#31649;&#20154;&#24037;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#36153;&#26102;&#36153;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;FSL&#29615;&#22659;&#20013;LLM&#26597;&#35810;&#24615;&#33021;&#30340;&#28789;&#27963;&#25552;&#31034;&#26041;&#27861;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#22312;&#21333;&#27425;&#32597;&#35265;&#30149;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#20219;&#20309;&#21333;&#20010;&#27169;&#22411;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32597;&#35265;&#30149;&#25968;&#25454;&#38598;&#29992;&#20110;FSL&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases, affecting a small fraction of the population, inherently require FSL techniques due to limited data availability, though manual data collection and annotation is costly and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FS
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CGMI&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#20154;&#38469;&#20132;&#24448;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#26641;&#29366;&#32467;&#26500;&#26041;&#27861;&#26469;&#31649;&#29702;&#26234;&#33021;&#20307;&#30340;&#20010;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;ACT*&#27169;&#22411;&#30340;&#35748;&#30693;&#26550;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;CGMI&#26694;&#26550;&#65292;&#25105;&#20204;&#25104;&#21151;&#27169;&#25311;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#35838;&#22530;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.12503</link><description>&lt;p&gt;
CGMI: &#21487;&#37197;&#32622;&#30340;&#36890;&#29992;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
CGMI: Configurable General Multi-Agent Interaction Framework. (arXiv:2308.12503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CGMI&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#20154;&#38469;&#20132;&#24448;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#26641;&#29366;&#32467;&#26500;&#26041;&#27861;&#26469;&#31649;&#29702;&#26234;&#33021;&#20307;&#30340;&#20010;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;ACT*&#27169;&#22411;&#30340;&#35748;&#30693;&#26550;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;CGMI&#26694;&#26550;&#65292;&#25105;&#20204;&#25104;&#21151;&#27169;&#25311;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#35838;&#22530;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#24050;&#32463;&#23637;&#29616;&#20986;&#35299;&#20915;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#20197;&#21450;&#32570;&#20047;&#26377;&#25928;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#29983;&#25104;&#30340;&#20869;&#23481;&#20173;&#28982;&#30456;&#23545;&#34920;&#38754;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#36890;&#29992;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#65288;CGMI&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#20154;&#38469;&#20132;&#24448;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#26234;&#33021;&#20307;&#30340;&#20010;&#24615;&#20998;&#37197;&#12289;&#26816;&#27979;&#21644;&#32500;&#25252;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;ACT*&#27169;&#22411;&#30340;&#25216;&#33021;&#24211;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#35760;&#24518;&#12289;&#21453;&#24605;&#21644;&#35268;&#21010;&#27169;&#22359;&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#20102;&#36890;&#29992;&#26234;&#33021;&#20307;&#26469;&#22686;&#24378;&#34394;&#25311;&#29615;&#22659;&#30340;&#30495;&#23454;&#24863;&#12290;&#21033;&#29992;CGMI&#26694;&#26550;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#22810;&#20010;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#35838;&#22530;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benefiting from the powerful capabilities of large language models (LLMs), agents based on LLMs have shown the potential to address domain-specific tasks and emulate human behaviors. However, the content generated by these agents remains somewhat superficial, owing to their limited domain expertise and the absence of an effective cognitive architecture. To address this, we present the Configurable General Multi-Agent Interaction (CGMI) framework, designed to replicate human interactions in real-world scenarios. Specifically, we propose a tree-structured methodology for the assignment, detection, and maintenance of agent personality. Additionally, we designed a cognitive architecture equipped with a skill library based on the ACT* model, which contains memory, reflection, and planning modules. We have also integrated general agents to augment the virtual environment's realism. Using the CGMI framework, we simulated numerous classroom interactions between teacher and students. The experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2308.12143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIA)&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35760;&#24405;&#12290;&#23545;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#30340;MIA&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#23558;MIA&#24212;&#29992;&#21040;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;&#29983;&#25104;&#27169;&#22411;&#30340;MIA&#20027;&#35201;&#20381;&#36182;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#36991;&#20813;&#65292;&#32780;&#29616;&#26377;&#30340;MIA&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#19982;&#36807;&#25311;&#21512;&#19981;&#21516;&#65292;&#35760;&#24518;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20026;&#26222;&#36941;&#30340;&#29616;&#35937;&#12290;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#23548;&#33268;&#29983;&#25104;&#35760;&#24405;&#30340;&#27010;&#29575;&#20998;&#24067;&#21576;&#29616;&#20986;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#23427;&#26159;&#19968;&#31181;&#40657;&#30418;MIA&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#27874;&#21160;&#26469;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#29615;&#22659;&#65292;&#20316;&#20026;&#25915;&#20987;&#20195;&#29702;&#20154;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#35774;&#35745;&#34920;&#26126;LLMs&#22312;&#39640;&#25928;&#24212;&#23545;&#22797;&#26434;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12086</link><description>&lt;p&gt;
&#36208;&#20986;&#31548;&#23376;&#65306;&#38543;&#26426;&#40550;&#40521;&#22312;&#32593;&#32476;&#23433;&#20840;&#29615;&#22659;&#20013;&#30340;&#32988;&#21033;
&lt;/p&gt;
&lt;p&gt;
Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments. (arXiv:2308.12086v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#29615;&#22659;&#65292;&#20316;&#20026;&#25915;&#20987;&#20195;&#29702;&#20154;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#35774;&#35745;&#34920;&#26126;LLMs&#22312;&#39640;&#25928;&#24212;&#23545;&#22797;&#26434;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#28041;&#21450;&#25991;&#26412;&#29983;&#25104;&#12289;&#25688;&#35201;&#21644;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#19981;&#21516;&#39046;&#22495;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#22522;&#20110;LLM&#30340;&#35774;&#35745;&#22312;&#35268;&#21010;&#21644;&#23548;&#33322;&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#23558;&#39044;&#35757;&#32451;&#30340;LLMs&#29992;&#20316;&#32593;&#32476;&#23433;&#20840;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#20154;&#30340;&#26032;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#22312;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#20316;&#20026;&#20004;&#20010;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#25915;&#20987;&#20195;&#29702;&#12290;&#22312;&#22823;&#22810;&#25968;&#22330;&#26223;&#21644;&#37197;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20195;&#29702;&#22312;&#34920;&#29616;&#19978;&#19982;&#32463;&#36807;&#25968;&#21315;&#27425;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#20195;&#29702;&#30456;&#20284;&#25110;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#26368;&#20339;LLM&#20195;&#29702;&#22312;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#29615;&#22659;&#30340;&#20154;&#31867;&#27979;&#35797;&#32773;&#31867;&#20284;&#12290;&#36825;&#31181;&#35774;&#35745;&#31361;&#26174;&#20102;LLMs&#22312;&#39640;&#25928;&#24212;&#23545;&#22797;&#26434;&#20915;&#31574;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. Despite their inherent limitations, LLM-based designs have shown promising capabilities in planning and navigating open-world scenarios. This paper introduces a novel application of pre-trained LLMs as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.  We present an approach wherein pre-trained LLMs are leveraged as attacking agents in two reinforcement learning environments. Our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. In addition, the best LLM agents perform similarly to human testers of the environment without any additional training process. This design highlights the potential of LLMs to efficiently address complex decision
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#21644;&#31232;&#30095;&#30340;&#25668;&#20687;&#26426;&#32452;&#26469;&#37325;&#24314;&#21160;&#24577;&#20154;&#20307;&#36816;&#21160;&#21644;&#24418;&#29366;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#35282;&#33394;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39057;&#22495;&#20013;&#26159;&#33258;&#36866;&#24212;&#21644;&#26174;&#24335;&#30340;&#65292;&#24182;&#36890;&#36807;&#24314;&#27169;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#34915;&#29289;&#21644;&#30382;&#32932;&#30340;&#21464;&#24418;&#24314;&#27169;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11951</link><description>&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#29983;&#25104;&#23039;&#21183;&#35843;&#33410;&#30340;&#34394;&#25311;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Pose Modulated Avatars from Video. (arXiv:2308.11951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#21644;&#31232;&#30095;&#30340;&#25668;&#20687;&#26426;&#32452;&#26469;&#37325;&#24314;&#21160;&#24577;&#20154;&#20307;&#36816;&#21160;&#21644;&#24418;&#29366;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#35282;&#33394;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39057;&#22495;&#20013;&#26159;&#33258;&#36866;&#24212;&#21644;&#26174;&#24335;&#30340;&#65292;&#24182;&#36890;&#36807;&#24314;&#27169;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#34915;&#29289;&#21644;&#30382;&#32932;&#30340;&#21464;&#24418;&#24314;&#27169;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30001;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#31232;&#30095;&#30340;&#25668;&#20687;&#26426;&#32452;&#21487;&#20197;&#37325;&#24314;&#21160;&#24577;&#20154;&#20307;&#36816;&#21160;&#21644;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#34915;&#29289;&#21644;&#30382;&#32932;&#30340;&#21464;&#24418;&#19982;&#39592;&#26550;&#23039;&#21183;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#19982;&#38544;&#24335;&#23398;&#20064;&#25110;&#20381;&#36182;&#20195;&#29702;&#34920;&#38754;&#30340;&#29616;&#26377;&#35282;&#33394;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28304;&#20110;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#23039;&#21183;&#38656;&#35201;&#19981;&#21516;&#30340;&#39057;&#29575;&#20998;&#37197;&#12290;&#24573;&#35270;&#36825;&#31181;&#21306;&#21035;&#20250;&#22312;&#24179;&#28369;&#21306;&#22495;&#20135;&#29983;&#22122;&#28857;&#20266;&#24433;&#65292;&#25110;&#20351;&#38160;&#21033;&#21306;&#22495;&#30340;&#32454;&#31890;&#24230;&#32441;&#29702;&#21644;&#24418;&#29366;&#32454;&#33410;&#27169;&#31946;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#39057;&#22495;&#20013;&#26159;&#33258;&#36866;&#24212;&#21644;&#26174;&#24335;&#30340;&#21452;&#20998;&#25903;&#31070;&#32463;&#32593;&#32476;&#12290;&#31532;&#19968;&#20010;&#20998;&#25903;&#26159;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#22320;&#24314;&#27169;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#39592;&#26550;&#23039;&#21183;&#20316;&#20026;&#36755;&#20837;&#12290;&#31532;&#20108;&#20010;&#20998;&#25903;&#23558;&#36825;&#20123;&#30456;&#20851;&#29305;&#24449;&#32452;&#21512;&#21040;&#19968;&#32452;&#20840;&#23616;&#39057;&#29575;&#20013;&#65292;&#28982;&#21518;&#35843;&#33410;&#29305;&#24449;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#34920;&#26684;&#24207;&#21015;&#21270;&#27169;&#22359;&#21644;&#32416;&#27491;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#20173;&#26377;&#24046;&#36317;&#65292;&#20294;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.11891</link><description>&lt;p&gt;
&#32447;&#24615;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20998;&#26512;&#34920;&#26684;&#25968;&#25454;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Deciphering Tabular Data Using Large Language Model. (arXiv:2308.11891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#34920;&#26684;&#24207;&#21015;&#21270;&#27169;&#22359;&#21644;&#32416;&#27491;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#20173;&#26377;&#24046;&#36317;&#65292;&#20294;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#19968;&#30452;&#26159;&#23398;&#26415;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#38543;&#30528;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#24222;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#22788;&#29702;&#19982;&#34920;&#26684;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34920;&#26684;&#32467;&#26500;&#21644;&#20869;&#23481;&#19978;&#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#23558;&#34920;&#26684;&#24207;&#21015;&#21270;&#30340;&#27169;&#22359;&#65292;&#24182;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#32416;&#27491;&#26426;&#21046;&#26469;&#20462;&#27491;&#28508;&#22312;&#30340;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#20173;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGPT, has ushered in a wave of endeavors wherein researchers aim to harness these models for tasks related to table-based question answering. Central to our investigative pursuits is the elucidation of methodologies that amplify the aptitude of such large language models in discerning both the structural intricacies and inherent content of tables, ultimately facilitating their capacity to provide informed responses to pertinent queries. To this end, we have architected a distinctive module dedicated to the serialization of tables for seamless integration with expansive language models. Additionally, we've instituted a corrective mechanism within the model to rectify potential inaccuracies. Experimental results indicate that, although our proposed method trails the SOTA by ap
&lt;/p&gt;</description></item><item><title>ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11339</link><description>&lt;p&gt;
ProAgent&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20027;&#21160;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
ProAgent: Building Proactive Cooperative AI with Large Language Models. (arXiv:2308.11339v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11339
&lt;/p&gt;
&lt;p&gt;
ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AGI&#30740;&#31350;&#20013;&#65292;&#26500;&#24314;&#20855;&#26377;&#33258;&#36866;&#24212;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#20197;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#20851;&#27880;&#28857;&#12290;&#30446;&#21069;&#65292;&#24320;&#21457;&#21512;&#20316;&#20195;&#29702;&#20154;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25919;&#31574;&#27867;&#21270;&#20005;&#37325;&#20381;&#36182;&#20110;&#19982;&#29305;&#23450;&#38431;&#21451;&#30340;&#36807;&#21435;&#20114;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#38480;&#21046;&#20102;&#20195;&#29702;&#20154;&#22312;&#38754;&#23545;&#26032;&#30340;&#38431;&#21451;&#26102;&#37325;&#26032;&#26657;&#20934;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#39044;&#27979;&#38431;&#21451;&#26410;&#26469;&#20915;&#31574;&#33021;&#21147;&#21644;&#20026;&#33258;&#36523;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#33021;&#21147;&#30340;&#20027;&#21160;&#20195;&#29702;&#12290;ProAgent&#22312;&#21512;&#20316;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#21160;&#24577;&#35843;&#25972;&#34892;&#20026;&#20197;&#22686;&#24378;&#19982;&#38431;&#21451;&#30340;&#21327;&#20316;&#21162;&#21147;&#12290;&#27492;&#22806;&#65292;ProAgent&#26694;&#26550;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20415;&#20110;&#26080;&#32541;&#38598;&#25104;&#65292;&#20197;&#24212;&#23545;&#21508;&#31181;&#21327;&#35843;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#26234;&#33021;&#20307;&#65292;&#20854;&#33021;&#22815;&#21516;&#26102;&#20316;&#20026;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#65292;&#36890;&#36807;&#34892;&#21160;&#28436;&#31034;&#21644;&#35821;&#35328;&#25351;&#20196;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.10842</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#25552;&#21319;&#26234;&#33021;&#20307;&#30340;&#27807;&#36890;&#21644;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Agent Communication and Learning through Action and Language. (arXiv:2308.10842v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10842
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#26234;&#33021;&#20307;&#65292;&#20854;&#33021;&#22815;&#21516;&#26102;&#20316;&#20026;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#65292;&#36890;&#36807;&#34892;&#21160;&#28436;&#31034;&#21644;&#35821;&#35328;&#25351;&#20196;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;GC&#26234;&#33021;&#20307;&#31867;&#21035;&#65292;&#33021;&#22815;&#21516;&#26102;&#20805;&#24403;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#30340;&#35282;&#33394;&#12290;&#20511;&#21161;&#22522;&#20110;&#34892;&#21160;&#30340;&#28436;&#31034;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20154;&#31867;&#27807;&#36890;&#21644;&#30446;&#26631;&#23454;&#29616;&#20013;&#30340;&#37325;&#35201;&#20803;&#32032;&#8212;&#8212;&#25945;&#32946;&#23398;&#21644;&#23454;&#29992;&#20027;&#20041;&#30340;&#34701;&#20837;&#65292;&#25552;&#21319;&#20102;&#26234;&#33021;&#20307;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22810;&#27169;&#24335;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel category of GC-agents capable of functioning as both teachers and learners. Leveraging action-based demonstrations and language-based instructions, these agents enhance communication efficiency. We investigate the incorporation of pedagogy and pragmatism, essential elements in human communication and goal achievement, enhancing the agents' teaching and learning capabilities. Furthermore, we explore the impact of combining communication modes (action and language) on learning outcomes, highlighting the benefits of a multi-modal approach.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#38598;&#20013;&#20110;&#20102;&#35299;&#22914;&#20309;&#21033;&#29992;LLM&#26469;&#20248;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#25991;&#31456;&#24635;&#32467;&#20102;&#19981;&#21516;LLM&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#20197;&#21450;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10620</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Software Engineering: A Systematic Literature Review. (arXiv:2308.10620v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10620
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#38598;&#20013;&#20110;&#20102;&#35299;&#22914;&#20309;&#21033;&#29992;LLM&#26469;&#20248;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#25991;&#31456;&#24635;&#32467;&#20102;&#19981;&#21516;LLM&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#20197;&#21450;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21253;&#25324;&#36719;&#20214;&#24037;&#31243;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#25991;&#29486;&#25506;&#35752;&#20102;LLM&#22312;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;LLM&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#12289;&#24433;&#21709;&#21644;&#21487;&#33021;&#30340;&#38480;&#21046;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;LLM&#19982;&#36719;&#20214;&#24037;&#31243;&#30340;&#20132;&#21449;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65292;&#29305;&#21035;&#20851;&#27880;LLM&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#22914;&#20309;&#34987;&#21033;&#29992;&#26469;&#20248;&#21270;&#36807;&#31243;&#21644;&#32467;&#26524;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#20998;&#26512;&#20102;2017&#24180;&#33267;2023&#24180;&#30340;229&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#20197;&#22238;&#31572;&#22235;&#20010;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65288;RQs&#65289;&#12290;&#22312;RQ1&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;LLM&#36827;&#34892;&#20998;&#31867;&#21644;&#27604;&#36739;&#20998;&#26512;&#65292;&#25551;&#32472;&#20854;&#29420;&#29305;&#30340;&#29305;&#28857;&#21644;&#29992;&#36884;&#12290;&#22312;RQ2&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#24378;&#22823;&#12289;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#25104;&#21151;&#21033;&#29992;LLM&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks and applications. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review on the intersection of LLMs and SE, with a particular focus on understanding how LLMs can be exploited in SE to optimize processes and outcomes. We collect and analyze a total of 229 research papers from 2017 to 2023 to answer four key research questions (RQs). In RQ1, we categorize and provide a comparative analysis of different LLMs that have been employed in SE tasks, characterising their distinctive features and uses. In RQ2, we analyse the methods used in data collection, preprocessing, and application highlighting the role of robust, well-curated datasets for successful LLM for S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.10335</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#30721;&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21644;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#38750;&#20961;&#33021;&#21147;&#12290;&#24403;&#36935;&#21040;&#32534;&#30721;&#38382;&#39064;&#26102;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#24120;&#24120;&#20250;&#21672;&#35810;LLMs&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#21162;&#21147;&#26469;&#36991;&#20813;&#35821;&#27861;&#38169;&#35823;&#24182;&#20351;&#20195;&#30721;&#19982;&#39044;&#26399;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#20294;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#23578;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#29615;&#22659;&#20013;&#65292;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#31561;&#21516;&#20110;&#21487;&#38752;&#21644;&#40065;&#26834;&#30340;&#20195;&#30721;&#12290;&#22312;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#28389;&#29992;API&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#22914;&#36164;&#28304;&#27844;&#28431;&#12289;&#31243;&#24207;&#23849;&#28291;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;LLM&#20195;&#30721;&#29983;&#25104;&#26381;&#21153;&#30340;&#29992;&#25143;&#23454;&#38469;&#19978;&#26159;&#26368;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#30475;&#20284;&#27491;&#30830;&#30340;&#20195;&#30721;&#24433;&#21709;&#30340;&#24320;&#21457;&#32773;&#8212;&#8212;&#20182;&#20204;&#36890;&#24120;&#26159;&#19981;&#29087;&#24713;LLMs&#20026;&#20182;&#20204;&#29983;&#25104;&#20195;&#30721;&#30340;API&#30340;&#21021;&#32423;&#24320;&#21457;&#32773;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;API&#30340;&#28389;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;Electra Transformer&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#21019;&#26032;&#38382;&#39064;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#34701;&#21512;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#33719;&#24471;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06828</link><description>&lt;p&gt;
&#38382;&#39064;&#20998;&#31867;&#30340;&#38598;&#25104;&#26041;&#27861;&#65306;&#34701;&#21512;Electra Transformer&#12289;GloVe&#21644;LSTM
&lt;/p&gt;
&lt;p&gt;
An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM. (arXiv:2308.06828v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;Electra Transformer&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#21019;&#26032;&#38382;&#39064;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#34701;&#21512;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#33719;&#24471;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24050;&#32463;&#25104;&#20026;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#23427;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#20998;&#31867;&#26041;&#38754;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#39046;&#22495;&#65292;&#38382;&#39064;&#20998;&#31867;&#19987;&#27880;&#20110;&#30830;&#23450;&#25152;&#38656;&#20449;&#24687;&#30340;&#31867;&#22411;&#65292;&#36825;&#26159;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#31561;&#19979;&#28216;&#24212;&#29992;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38382;&#39064;&#20998;&#31867;&#38598;&#25104;&#26041;&#27861;&#65292;&#23558;Electra&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;&#35813;&#27169;&#22411;&#22312;&#33879;&#21517;&#30340;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#25972;&#21512;&#36825;&#20123;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;Electra&#25552;&#20379;&#20102;&#22522;&#20110;transformer&#30340;&#22797;&#26434;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;GloVe&#25552;&#20379;&#20102;&#20840;&#23616;&#21521;&#37327;&#34920;&#31034;&#20197;&#25429;&#25417;&#35789;&#32423;&#35821;&#20041;&#65292;LSTM&#21017;&#36129;&#29486;&#20102;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#20197;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) has emerged as a crucial technology for understanding and generating human language, playing an essential role in tasks such as machine translation, sentiment analysis, and more pertinently, question classification. As a subfield within NLP, question classification focuses on determining the type of information being sought, a fundamental step for downstream applications like question answering systems. This study presents an innovative ensemble approach for question classification, combining the strengths of Electra, GloVe, and LSTM models. Rigorously tested on the well-regarded TREC dataset, the model demonstrates how the integration of these disparate technologies can lead to superior results. Electra brings in its transformer-based capabilities for complex language understanding, GloVe offers global vector representations for capturing word-level semantics, and LSTM contributes its sequence learning abilities to model long-term dependencies. By fus
&lt;/p&gt;</description></item><item><title>CLE&#25193;&#25955;&#26159;&#19968;&#31181;&#21487;&#25511;&#30340;&#20809;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#21644;&#20809;&#29031;&#23884;&#20837;&#65292;&#35753;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#20142;&#24230;&#27700;&#24179;&#65292;&#24182;&#32467;&#21512;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#23454;&#29616;&#29992;&#25143;&#21451;&#22909;&#30340;&#21306;&#22495;&#21487;&#25511;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;CLE Diffusion&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06725</link><description>&lt;p&gt;
CLE&#25193;&#25955;&#65306;&#21487;&#25511;&#24615;&#20809;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLE Diffusion: Controllable Light Enhancement Diffusion Model. (arXiv:2308.06725v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06725
&lt;/p&gt;
&lt;p&gt;
CLE&#25193;&#25955;&#26159;&#19968;&#31181;&#21487;&#25511;&#30340;&#20809;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#21644;&#20809;&#29031;&#23884;&#20837;&#65292;&#35753;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#20142;&#24230;&#27700;&#24179;&#65292;&#24182;&#32467;&#21512;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#23454;&#29616;&#29992;&#25143;&#21451;&#22909;&#30340;&#21306;&#22495;&#21487;&#25511;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;CLE Diffusion&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35270;&#35273;&#21019;&#20316;&#21644;&#32534;&#36753;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20302;&#20809;&#22686;&#24378;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#22686;&#24378;&#31639;&#27861;&#37117;&#35774;&#35745;&#20026;&#22343;&#21248;&#22686;&#21152;&#22270;&#20687;&#30340;&#20142;&#24230;&#21040;&#39044;&#23450;&#20041;&#30340;&#31243;&#24230;&#65292;&#38480;&#21046;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26694;&#26550;&#8212;&#8212;&#21487;&#25511;&#20809;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;(CLE Diffusion)&#65292;&#20197;&#25552;&#20379;&#20016;&#23500;&#30340;&#21487;&#25511;&#24615;&#32473;&#29992;&#25143;&#12290;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20809;&#29031;&#23884;&#20837;&#65292;&#35753;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#25152;&#38656;&#30340;&#20142;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#8220;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#8221;(Segment-Anything Model, SAM)&#65292;&#23454;&#29616;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#21306;&#22495;&#21487;&#25511;&#24615;&#65292;&#29992;&#25143;&#21487;&#20197;&#28857;&#20987;&#23545;&#35937;&#26469;&#25351;&#23450;&#20182;&#20204;&#24076;&#26395;&#22686;&#24378;&#30340;&#21306;&#22495;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CLE Diffusion&#22312;&#23450;&#37327;&#25351;&#26631;&#12289;&#23450;&#24615;&#32467;&#26524;&#21644;&#22810;&#26679;&#21270;&#21487;&#25511;&#24615;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low light enhancement has gained increasing importance with the rapid development of visual creation and editing. However, most existing enhancement algorithms are designed to homogeneously increase the brightness of images to a pre-defined extent, limiting the user experience. To address this issue, we propose Controllable Light Enhancement Diffusion Model, dubbed CLE Diffusion, a novel diffusion framework to provide users with rich controllability. Built with a conditional diffusion model, we introduce an illumination embedding to let users control their desired brightness level. Additionally, we incorporate the Segment-Anything Model (SAM) to enable user-friendly region controllability, where users can click on objects to specify the regions they wish to enhance. Extensive experiments demonstrate that CLE Diffusion achieves competitive performance regarding quantitative metrics, qualitative results, and versatile controllability. Project page: https://yuyangyin.github.io/CLEDiffusio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36317;&#31163;&#24230;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#20174;&#20505;&#36873;&#26550;&#26500;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;/&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#35757;&#32451;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25552;&#20379;&#27169;&#22411;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.03580</link><description>&lt;p&gt;
&#25581;&#31034;&#28508;&#22312;&#27169;&#24335;&#65306;&#30740;&#31350;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#12289;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Revealing the Underlying Patterns: Investigating Dataset Similarity, Performance, and Generalization. (arXiv:2308.03580v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03580
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36317;&#31163;&#24230;&#37327;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#20174;&#20505;&#36873;&#26550;&#26500;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;/&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#35757;&#32451;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25552;&#20379;&#27169;&#22411;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#25165;&#33021;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#21487;&#25509;&#21463;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#29992;&#39069;&#22806;&#21644;&#22810;&#26679;&#21270;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#12289;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22270;&#20687;-&#22270;&#20687;&#12289;&#25968;&#25454;&#38598;-&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;-&#25968;&#25454;&#38598;&#36317;&#31163;&#65292;&#20197;&#27934;&#23519;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#32467;&#21512;&#27169;&#22411;&#24615;&#33021;&#21487;&#20197;&#24110;&#21161;&#20174;&#20505;&#36873;&#26550;&#26500;&#20013;&#36873;&#25321;&#19968;&#20010;&#21512;&#36866;&#30340;&#27169;&#22411;/&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#38656;&#23558;&#23569;&#37327;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#65288;&#22914;1&#12289;3&#25110;7&#20010;&#65289;&#28155;&#21152;&#21040;&#35757;&#32451;&#38598;&#20013;&#21363;&#21487;&#25913;&#21892;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20943;&#23569;&#35757;&#32451;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#25552;&#20379;&#27169;&#22411;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised deep learning models require significant amount of labelled data to achieve an acceptable performance on a specific task. However, when tested on unseen data, the models may not perform well. Therefore, the models need to be trained with additional and varying labelled data to improve the generalization. In this work, our goal is to understand the models, their performance and generalization. We establish image-image, dataset-dataset, and image-dataset distances to gain insights into the model's behavior. Our proposed distance metric when combined with model performance can help in selecting an appropriate model/architecture from a pool of candidate architectures. We have shown that the generalization of these models can be improved by only adding a small number of unseen images (say 1, 3 or 7) into the training set. Our proposed approach reduces training and annotation costs while providing an estimate of model performance on unseen data in dynamic environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;</title><link>http://arxiv.org/abs/2307.12057</link><description>&lt;p&gt;
&#22806;&#37096;&#25512;&#29702;&#65306;&#26397;&#30528;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20114;&#25442;&#36741;&#21161;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback. (arXiv:2307.12057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#34987;&#35748;&#20026;&#26159;&#20351;&#28023;&#39532;&#20307;&#21644;&#33041;&#31070;&#32463;&#20803;&#20869;&#20445;&#25345;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#12289;&#38543;&#21518;&#29992;&#20110;&#35299;&#20915;&#36890;&#36807;&#23398;&#20064;&#19968;&#29983;&#20013;&#36935;&#21040;&#30340;&#29616;&#23454;&#25361;&#25112;&#30340;&#20851;&#38190;&#20154;&#31867;&#33021;&#21147;&#12290;&#36890;&#36807;&#24212;&#29992;&#24050;&#33719;&#24471;&#30340;&#30693;&#35782;&#35299;&#20915;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19968;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20687;GPT-3.5&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#20132;&#20114;&#21644;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#24191;&#27867;&#12289;&#19981;&#26029;&#28436;&#21464;&#30340;&#30693;&#35782;&#24211;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory is identified as a crucial human faculty that allows for the retention of visual and linguistic information within the hippocampus and neurons in the brain, which can subsequently be retrieved to address real-world challenges that arise through a lifetime of learning. The resolution of complex AI tasks through the application of acquired knowledge represents a stride toward the realization of artificial general intelligence. However, despite the prevalence of Large Language Models (LLMs) like GPT-3.5 and GPT-4 , which have displayed remarkable capabilities in language comprehension, generation, interaction, and reasoning, they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases. This paper proposes that LLMs could be augmented through the selective integration of knowledge from external repositories, and in doing so, introduces a novel methodology for External Reasoning, exemplified by ChatPDF. Central to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#34892;&#20026;&#32463;&#27982;&#23398;&#35282;&#24230;&#65292;&#23545;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;AI&#23545;&#40784;&#20013;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;AI&#23433;&#20840;&#38382;&#39064;&#19981;&#20165;&#28041;&#21450;&#35774;&#35745;&#32773;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#36824;&#28041;&#21450;&#21040;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#19982;&#25928;&#29992;&#20989;&#25968;&#20043;&#38388;&#30340;&#38169;&#20301;&#12290;</title><link>http://arxiv.org/abs/2307.11137</link><description>&lt;p&gt;
&#27169;&#22411;&#19982;&#38177;&#20154;&#20043;&#38388;&#8212;&#8212;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;AI&#23545;&#40784;&#20013;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#30340;&#34892;&#20026;&#32463;&#27982;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Of Models and Tin Men -- a behavioural economics study of principal-agent problems in AI alignment using large-language models. (arXiv:2307.11137v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#34892;&#20026;&#32463;&#27982;&#23398;&#35282;&#24230;&#65292;&#23545;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;AI&#23545;&#40784;&#20013;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;AI&#23433;&#20840;&#38382;&#39064;&#19981;&#20165;&#28041;&#21450;&#35774;&#35745;&#32773;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#36824;&#28041;&#21450;&#21040;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#19982;&#25928;&#29992;&#20989;&#25968;&#20043;&#38388;&#30340;&#38169;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#19968;&#20010;&#35774;&#35745;&#32773;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35774;&#35745;&#32773;&#35797;&#22270;&#30830;&#20445;&#20195;&#29702;&#30340;&#34892;&#20026;&#19982;&#20854;&#30446;&#30340;&#19968;&#33268;&#65292;&#24182;&#19988;&#39118;&#38505;&#20165;&#20165;&#26159;&#30001;&#20110;&#35774;&#35745;&#32773;&#24847;&#22270;&#20013;&#30340;&#25928;&#29992;&#20989;&#25968;&#19982;&#20195;&#29702;&#30340;&#20869;&#37096;&#25928;&#29992;&#20989;&#25968;&#20043;&#38388;&#30340;&#24847;&#22806;&#38169;&#20301;&#32780;&#23548;&#33268;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23454;&#20363;&#21270;&#30340;&#20195;&#29702;&#30340;&#20986;&#29616;&#65292;&#36825;&#31181;&#25551;&#36848;&#19981;&#33021;&#25429;&#25417;&#21040;AI&#23433;&#20840;&#30340;&#26680;&#24515;&#26041;&#38754;&#65292;&#22240;&#20026;&#29616;&#23454;&#19990;&#30028;&#20013;&#35774;&#35745;&#32773;&#19982;&#20195;&#29702;&#20043;&#38388;&#24182;&#27809;&#26377;&#19968;&#23545;&#19968;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#32780;&#19988;&#35768;&#22810;&#20195;&#29702;&#65292;&#26080;&#35770;&#26159;&#20154;&#24037;&#26234;&#33021;&#36824;&#26159;&#20154;&#31867;&#65292;&#37117;&#20855;&#26377;&#22810;&#26679;&#30340;&#20215;&#20540;&#35266;&#12290;&#22240;&#27492;&#65292;AI&#23433;&#20840;&#20855;&#26377;&#32463;&#27982;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#21487;&#33021;&#20250;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI Alignment is often presented as an interaction between a single designer and an artificial agent in which the designer attempts to ensure the agent's behavior is consistent with its purpose, and risks arise solely because of conflicts caused by inadvertent misalignment between the utility function intended by the designer and the resulting internal utility function of the agent. With the advent of agents instantiated with large-language models (LLMs), which are typically pre-trained, we argue this does not capture the essential aspects of AI safety because in the real world there is not a one-to-one correspondence between designer and agent, and the many agents, both artificial and human, have heterogeneous values. Therefore, there is an economic aspect to AI safety and the principal-agent problem is likely to arise. In a principal-agent problem conflict arises because of information asymmetry together with inherent misalignment between the utility of the agent and its principal, an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#26041;&#27861;&#35843;&#26597;&#20102;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24182;&#37319;&#29992;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#27169;&#22411;&#36825;&#19968;&#26415;&#35821;&#30340;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.09673</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#21487;&#35299;&#37322;&#27169;&#22411;&#65306;&#19968;&#39033;&#33539;&#22260;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
What's meant by explainable model: A Scoping Review. (arXiv:2307.09673v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09673
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#26041;&#27861;&#35843;&#26597;&#20102;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24182;&#37319;&#29992;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#27169;&#22411;&#36825;&#19968;&#26415;&#35821;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32463;&#24120;&#22312;&#25551;&#36848;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24212;&#29992;&#30340;&#35770;&#25991;&#26631;&#39064;&#20013;&#30475;&#21040;&#21487;&#35299;&#37322;&#36825;&#20010;&#26415;&#35821;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;XAI&#20013;&#30340;&#35299;&#37322;&#26159;&#29305;&#23450;&#24212;&#29992;&#21644;&#39046;&#22495;&#30340;&#65292;&#22240;&#27492;&#22312;&#29992;&#20110;&#35299;&#37322;&#29305;&#23450;&#24212;&#29992;&#38382;&#39064;&#30340;&#27169;&#22411;&#26102;&#38656;&#35201;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#25581;&#31034;&#20102;&#20107;&#21518;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#24615;&#33021;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#65292;&#26263;&#31034;&#23427;&#20204;&#24182;&#19981;&#33021;&#25104;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22312;&#20351;&#29992;XAI&#26041;&#27861;&#26102;&#65292;&#24212;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#35780;&#20272;&#20854;&#20449;&#24687;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#22240;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#33539;&#22260;&#23457;&#26597;&#26041;&#27861;&#26469;&#30740;&#31350;&#24212;&#29992;AI&#27169;&#22411;&#21644;&#37319;&#29992;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#21516;&#26102;&#23558;&#36825;&#20123;&#27169;&#22411;&#31216;&#20026;&#21487;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We often see the term explainable in the titles of papers that describe applications based on artificial intelligence (AI). However, the literature in explainable artificial intelligence (XAI) indicates that explanations in XAI are application- and domain-specific, hence requiring evaluation whenever they are employed to explain a model that makes decisions for a specific application problem. Additionally, the literature reveals that the performance of post-hoc methods, particularly feature attribution methods, varies substantially hinting that they do not represent a solution to AI explainability. Therefore, when using XAI methods, the quality and suitability of their information outputs should be evaluated within the specific application. For these reasons, we used a scoping review methodology to investigate papers that apply AI models and adopt methods to generate post-hoc explanations while referring to said models as explainable. This paper investigates whether the term explainabl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.07887</link><description>&lt;p&gt;
&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#65306;&#19968;&#20010;&#31614;&#21517;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#26512;&#25195;&#25551;&#25991;&#26723;&#26102;&#65292;&#25163;&#20889;&#25991;&#26412;&#21487;&#33021;&#35206;&#30422;&#25171;&#21360;&#25991;&#26412;&#12290;&#36825;&#22312;&#25991;&#26723;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#21644;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#36896;&#25104;&#22256;&#38590;&#65292;&#24182;&#19988;&#36827;&#32780;&#24433;&#21709;&#21040;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#20165;&#20851;&#27880;&#25163;&#20889;&#25991;&#26412;&#30340;&#20108;&#20998;&#31867;&#65292;&#35201;&#20040;&#36827;&#34892;&#19977;&#31867;&#25991;&#26723;&#30340;&#20998;&#21106;&#65292;&#21363;&#25163;&#20889;&#12289;&#25171;&#21360;&#21644;&#32972;&#26223;&#20687;&#32032;&#30340;&#35782;&#21035;&#12290;&#36825;&#23548;&#33268;&#25163;&#20889;&#21644;&#25171;&#21360;&#37325;&#21472;&#30340;&#20687;&#32032;&#21482;&#34987;&#20998;&#37197;&#21040;&#19968;&#20010;&#31867;&#21035;&#20013;&#65292;&#22240;&#27492;&#22312;&#21478;&#19968;&#20010;&#31867;&#21035;&#20013;&#19981;&#34987;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#30446;&#26631;&#26159;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#25552;&#39640;&#37325;&#21472;&#37096;&#20998;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#33258;&#30495;&#23454;&#30340;&#27861;&#24459;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as
&lt;/p&gt;</description></item><item><title>AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.07851</link><description>&lt;p&gt;
AspectCSE: &#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07851
&lt;/p&gt;
&lt;p&gt;
AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#31895;&#30053;&#36817;&#20284;&#65292;&#20294;&#24573;&#30053;&#20102;&#20351;&#25991;&#26412;&#30456;&#20284;&#30340;&#29305;&#23450;&#26041;&#38754;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#26041;&#38754;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#39044;&#27979;&#26356;&#21152;&#38024;&#23545;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AspectCSE&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#26368;&#22909;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;AspectCSE&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#25913;&#21892;3.97%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;Wikidata&#30693;&#35782;&#22270;&#23646;&#24615;&#26469;&#35757;&#32451;&#22810;&#26041;&#38754;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#20854;&#20013;&#22312;&#30456;&#20284;&#24615;&#39044;&#27979;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#26041;&#38754;&#23884;&#20837;&#22312;&#29305;&#23450;&#26041;&#38754;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23884;&#20837;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#36827;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generic sentence embeddings provide a coarse-grained approximation of semantic textual similarity but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose using Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks. Finally, w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03380</link><description>&lt;p&gt;
&#20851;&#20110;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;ML&#27169;&#22411;&#33030;&#24369;&#24615;&#65292;&#20844;&#24179;&#24615;&#20197;&#21450;&#35299;&#37322;&#24615;&#30340;&#32570;&#20047;&#31561;&#37325;&#35201;&#38382;&#39064;&#38656;&#35201;&#31215;&#26497;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#24418;&#24335;&#21270;&#30340;ML&#27169;&#22411;&#39564;&#35777;&#12290;XAI&#30340;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;Anchors&#65289;&#21644;&#29305;&#24449;&#24402;&#22240;&#25216;&#26415;&#65288;&#20363;&#22914;&#65292;LIME&#21644;SHAP&#65289;&#12290;&#23613;&#31649;&#26377;&#24076;&#26395;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#37117;&#23481;&#26131;&#20986;&#29616;&#19968;&#31995;&#21015;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#35299;&#37322;&#19981;&#27491;&#30830;&#21644;&#36229;&#20986;&#20998;&#24067;&#37319;&#26679;&#12290;&#36817;&#26399;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#65288;FXAI&#65289;&#34429;&#28982;&#20316;&#20026;&#20197;&#19978;&#26041;&#27861;&#30340;&#26367;&#20195;&#21697;&#24182;&#36991;&#20813;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20363;&#22914;&#65292;&#38500;&#20102;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#22806;&#65292;&#36825;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#29305;&#24449;&#24402;&#22240;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03104</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22823;&#22810;&#25968;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#26159;&#38024;&#23545;&#19968;&#33324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#35201;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#65292;&#24517;&#39035;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35813;&#39046;&#22495;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#23545;&#24863;&#20852;&#36259;&#30340;&#22495;&#23545;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26356;&#26032;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20351;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20026;&#27599;&#20010;&#30446;&#26631;&#39046;&#22495;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#19981;&#38656;&#35201;&#24494;&#35843;&#25152;&#26377;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21482;&#35757;&#32451;&#23569;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#22987;&#32456;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;BERT&#27169;&#22411;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#30340;&#34920;&#24773;&#31526;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.02054</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#27169;&#22411;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;
&lt;/p&gt;
&lt;p&gt;
Emoji Prediction using Transformer Models. (arXiv:2307.02054v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02054
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;BERT&#27169;&#22411;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#30340;&#34920;&#24773;&#31526;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31038;&#20132;&#23186;&#20307;&#20013;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#30340;&#39057;&#29575;&#22823;&#24133;&#22686;&#21152;&#65292;&#20351;&#24471;&#23427;&#20204;&#25104;&#20026;&#20102;&#29702;&#35299;&#22312;&#32447;&#27807;&#36890;&#30340;&#37325;&#35201;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#21547;&#31946;&#30340;&#29305;&#24615;&#65292;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#20013;&#34920;&#24773;&#31526;&#21495;&#30340;&#21547;&#20041;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;BERT&#36827;&#34892;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;&#65292;BERT&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;&#25991;&#26412;&#21644;&#34920;&#24773;&#31526;&#21495;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#30340;&#26368;&#21512;&#36866;&#30340;&#34920;&#24773;&#31526;&#21495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;75&#65285;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the use of emojis in social media has increased dramatically, making them an important element in understanding online communication. However, predicting the meaning of emojis in a given text is a challenging task due to their ambiguous nature. In this study, we propose a transformer-based approach for emoji prediction using BERT, a widely-used pre-trained language model. We fine-tuned BERT on a large corpus of text containing both text and emojis to predict the most appropriate emoji for a given text. Our experimental results demonstrate that our approach outperforms several state-of-the-art models in predicting emojis with an accuracy of over 75 percent. This work has potential applications in natural language processing, sentiment analysis, and social media marketing.
&lt;/p&gt;</description></item><item><title>Tensorformer&#26159;&#19968;&#31181;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#30340;&#28857;&#20113;&#37325;&#24314;&#12290;&#23427;&#36890;&#36807;&#30697;&#38453;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#36880;&#28857;&#21644;&#36880;&#36890;&#36947;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#23616;&#37096;&#20960;&#20309;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15989</link><description>&lt;p&gt;
Tensorformer: &#39640;&#36136;&#37327;&#28857;&#20113;&#37325;&#24314;&#30340;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Tensorformer: Normalized Matrix Attention Transformer for High-quality Point Cloud Reconstruction. (arXiv:2306.15989v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15989
&lt;/p&gt;
&lt;p&gt;
Tensorformer&#26159;&#19968;&#31181;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#30340;&#28857;&#20113;&#37325;&#24314;&#12290;&#23427;&#36890;&#36807;&#30697;&#38453;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#36880;&#28857;&#21644;&#36880;&#36890;&#36947;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#23616;&#37096;&#20960;&#20309;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#30028;&#65292;&#20174;&#21407;&#22987;&#28857;&#20113;&#36827;&#34892;&#34920;&#38754;&#37325;&#24314;&#30340;&#30740;&#31350;&#24050;&#32463;&#36827;&#34892;&#20102;&#20960;&#21313;&#24180;&#65292;&#36825;&#22312;&#29616;&#20170;&#30340;&#24314;&#27169;&#21644;&#28210;&#26579;&#24212;&#29992;&#20013;&#38656;&#27714;&#38750;&#24120;&#39640;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;Poisson&#34920;&#38754;&#37325;&#24314;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#28857;&#27861;&#32447;&#36755;&#20837;&#20197;&#20135;&#29983;&#21512;&#29702;&#30340;&#32467;&#26524;&#12290;&#29616;&#20195;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#27861;&#32447;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#20294;&#30001;&#20110;&#31163;&#25955;&#28857;&#30340;&#23616;&#37096;&#34701;&#21512;&#32534;&#30721;&#24615;&#33021;&#26377;&#38480;&#65292;&#32467;&#26524;&#36739;&#20026;&#31895;&#31961;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;Tensorformer&#65289;&#26469;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#25152;&#25552;&#20986;&#30340;&#30697;&#38453;&#27880;&#24847;&#21147;&#20801;&#35768;&#21516;&#26102;&#36827;&#34892;&#36880;&#28857;&#21644;&#36880;&#36890;&#36947;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#32780;&#20043;&#21069;&#30340;&#21521;&#37327;&#27880;&#24847;&#21147;&#22312;&#19981;&#21516;&#36890;&#36947;&#20043;&#38388;&#20002;&#22833;&#20102;&#30456;&#37051;&#28857;&#30340;&#20449;&#24687;&#12290;&#23427;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#24102;&#26469;&#26356;&#22810;&#33258;&#30001;&#24230;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;ShapeNetCore&#21644;ABC&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#24182;&#19988;
&lt;/p&gt;
&lt;p&gt;
Surface reconstruction from raw point clouds has been studied for decades in the computer graphics community, which is highly demanded by modeling and rendering applications nowadays. Classic solutions, such as Poisson surface reconstruction, require point normals as extra input to perform reasonable results. Modern transformer-based methods can work without normals, while the results are less fine-grained due to limited encoding performance in local fusion from discrete points. We introduce a novel normalized matrix attention transformer (Tensorformer) to perform high-quality reconstruction. The proposed matrix attention allows for simultaneous point-wise and channel-wise message passing, while the previous vector attention loses neighbor point information across different channels. It brings more degree of freedom in feature learning and thus facilitates better modeling of local geometries. Our method achieves state-of-the-art on two commonly used datasets, ShapeNetCore and ABC, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#20840;&#23616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21327;&#35843;&#34892;&#21160;&#65292;&#23454;&#29616;&#26356;&#24555;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12926</link><description>&lt;p&gt;
&#20855;&#26377;&#20840;&#23616;&#29366;&#24577;&#39044;&#27979;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Agent Reinforcement Learning with Global State Prediction. (arXiv:2306.12926v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#20840;&#23616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21327;&#35843;&#34892;&#21160;&#65292;&#23454;&#29616;&#26356;&#24555;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#25511;&#21046;&#21333;&#20010;&#26426;&#22120;&#20154;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23558;DRL&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#32676;&#20307;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38750;&#38745;&#24577;&#24615;&#65292;&#21363;&#24403;&#20004;&#20010;&#25110;&#26356;&#22810;&#26426;&#22120;&#20154;&#21516;&#26102;&#26356;&#26032;&#20010;&#20307;&#25110;&#20849;&#20139;&#25919;&#31574;&#26102;&#65292;&#20250;&#36827;&#20837;&#19968;&#20010;&#30456;&#20114;&#20381;&#23384;&#30340;&#22521;&#35757;&#36807;&#31243;&#65292;&#24182;&#19988;&#19981;&#20445;&#35777;&#25910;&#25947;&#12290;&#20811;&#26381;&#38750;&#38745;&#24577;&#24615;&#36890;&#24120;&#28041;&#21450;&#20351;&#29992;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#65292;&#20363;&#22914;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#21644;/&#25110;&#34892;&#21160;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#28040;&#38500;&#20840;&#23616;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;&#30001;&#20110;&#32570;&#20047;&#20854;&#20182;&#20449;&#24687;&#20307;&#30340;&#20840;&#23616;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#25551;&#36848;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#20197;&#38598;&#20307;&#36816;&#36755;&#20026;&#27979;&#35797;&#22330;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#22810;&#26234;&#33021;&#20307;&#22521;&#35757;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#31181;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#20154;&#19981;&#20132;&#25442;&#20449;&#24687;&#65292;&#24182;&#19988;&#34987;&#35757;&#32451;&#20381;&#38752;&#36890;&#36807;&#25512;&#65288;push&#65289;&#21644;&#25289;&#65288;pull&#65289;&#29289;&#20307;&#36827;&#34892;&#38544;&#24335;&#36890;&#20449;&#12290;&#22312;&#31532;&#20108;&#31181;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#20154;&#24444;&#27492;&#20849;&#20139;&#29366;&#24577;&#39044;&#27979;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#21327;&#35843;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20849;&#20139;&#39044;&#27979;&#21487;&#20197;&#20351;&#26234;&#33021;&#20307;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#38656;&#35201;&#26356;&#23569;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has seen remarkable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second appro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.11167</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35823;&#23548;&#65306;&#20351;&#29992;Only Connect Wall&#25968;&#25454;&#38598;&#25506;&#32034;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#21644;Einstellung&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20154;&#24037;&#26234;&#33021;&#35806;&#29983;&#20197;&#26469;&#65292;&#23545;&#20154;&#31867;&#20223;&#30495;&#26234;&#33021;&#30340;&#36861;&#27714;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25345;&#20037;&#35805;&#39064;&#12290;&#26368;&#26032;&#19968;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#28436;&#36827;&#21644;&#26032;&#20852;&#33021;&#21147;&#23558;&#36825;&#20010;&#20027;&#39064;&#20174;&#23398;&#26415;&#30028;&#24102;&#21040;&#20102;&#25991;&#21270;&#26102;&#20195;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;NLP&#35780;&#20272;&#22522;&#20934;&#20219;&#21153;&#27979;&#35797;&#20102;&#20154;&#31867;&#20223;&#30495;&#34892;&#20026;&#30340;&#19968;&#20123;&#26041;&#38754;&#65288;&#20363;&#22914;BIG-bench&#30340;&#8220;&#31867;&#20154;&#34892;&#20026;&#8221;&#20219;&#21153;&#65289;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#19968;&#20010;&#20219;&#21153;&#32771;&#23519;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20154;&#31867;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#26159;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#30740;&#31350;&#36739;&#20026;&#28145;&#20837;&#30340;&#20027;&#39064;&#65292;&#26631;&#20934;&#21270;&#27979;&#35797;&#20027;&#35201;&#20351;&#29992;&#23558;&#32447;&#32034;&#35789;&#20043;&#38388;&#30340;&#65288;&#24322;&#26500;&#65289;&#36830;&#25509;&#33021;&#21147;&#20316;&#20026;&#21019;&#36896;&#24615;&#30340;&#24230;&#37327;&#12290;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#65292;&#26263;&#31034;&#24615;&#30340;&#35823;&#23548;&#24615;&#21050;&#28608;-&#34987;&#31216;&#20026;&#8220;&#35825;&#23548;&#35823;&#35299;&#8221;&#30340;&#24178;&#25200;&#22240;&#32032;-&#36890;&#36807;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#38459;&#30861;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20107;&#20808;&#35753;&#21442;&#19982;&#32773;&#25509;&#35302;&#21040;&#26377;&#30456;&#20284;&#25340;&#20889;&#30340;&#38169;&#35823;&#22240;&#32032;&#26469;&#23454;&#39564;&#24615;&#22320;&#35825;&#23548;&#36825;&#26679;&#30340;&#22266;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#20855;&#22791;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#32570;&#20047;&#21608;&#22260;&#19990;&#30028;&#30340;&#20855;&#20307;&#23884;&#20837;&#24335;&#20449;&#24687;&#65292;&#19988;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#26080;&#27861;&#20570;&#21040;&#23384;&#22312;&#30340;&#20381;&#36182;&#20110;&#20854;&#34892;&#20026;&#65292;&#36825;&#24847;&#21619;&#30528;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;&#23384;&#22312;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2306.00915</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#30340;&#35270;&#35282;&#25506;&#31350;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
The feasibility of artificial consciousness through the lens of neuroscience. (arXiv:2306.00915v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00915
&lt;/p&gt;
&lt;p&gt;
&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#20855;&#22791;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#32570;&#20047;&#21608;&#22260;&#19990;&#30028;&#30340;&#20855;&#20307;&#23884;&#20837;&#24335;&#20449;&#24687;&#65292;&#19988;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#26080;&#27861;&#20570;&#21040;&#23384;&#22312;&#30340;&#20381;&#36182;&#20110;&#20854;&#34892;&#20026;&#65292;&#36825;&#24847;&#21619;&#30528;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;&#23384;&#22312;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24341;&#21457;&#20102;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#30340;&#29468;&#27979;&#12290;&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#35266;&#28857;&#24456;&#38590;&#34987;&#35777;&#23454;&#12290;&#39318;&#20808;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#32570;&#23569;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#32570;&#20047;&#25105;&#20204;&#19982;&#21608;&#22260;&#19990;&#30028;&#30340;&#24863;&#23448;&#25509;&#35302;&#30340;&#20855;&#26377;&#20307;&#39564;&#12289;&#23884;&#20837;&#24335;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#21069;&#20004;&#20010;&#35770;&#28857;&#22312;&#26410;&#26469;&#30340;AI&#31995;&#32479;&#20013;&#21487;&#20197;&#34987;&#20811;&#26381;&#65292;&#20294;&#31532;&#19977;&#20010;&#21487;&#33021;&#26356;&#38590;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#36328;&#36234;&#12290;&#25442;&#35328;&#20043;&#65292;&#25105;&#20204;&#35748;&#20026;&#24847;&#35782;&#21487;&#33021;&#21462;&#20915;&#20110;&#26159;&#21542;&#22312;&#8220;&#28216;&#25103;&#20013;&#26377;&#30382;&#32932;&#8221;&#65292;&#21363;&#31995;&#32479;&#30340;&#23384;&#22312;&#26159;&#21542;&#21462;&#20915;&#20110;&#20854;&#34892;&#20026;&#65292;&#32780;&#36825;&#22312;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#24182;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions with large language models have led to the suggestion that these models may be conscious. From the perspective of neuroscience, this position is difficult to defend. For one, the architecture of large language models is missing key features of the thalamocortical system that have been linked to conscious awareness in mammals. Secondly, the inputs to large language models lack the embodied, embedded information content characteristic of our sensory contact with the world around us. Finally, while the previous two arguments can be overcome in future AI systems, the third one might be harder to bridge in the near future. Namely, we argue that consciousness might depend on having 'skin in the game', in that the existence of the system depends on its actions, which is not true for present-day artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#30340;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;Fusemate&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#24341;&#23548;&#30340;&#30456;&#20851;&#24615;&#27979;&#35797;&#20462;&#21098;&#35268;&#21017;&#65292;&#35299;&#20915;&#20102;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#38590;&#20197;&#25511;&#21046;ground clauses&#29983;&#25104;&#25968;&#37327;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21253;&#21547;&#8220;&#26102;&#38388;&#8221;&#30340;&#31034;&#20363;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18924</link><description>&lt;p&gt;
Fusemate&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#20013;&#30340;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bottom-Up Grounding in the Probabilistic Logic Programming System Fusemate. (arXiv:2305.18924v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18924
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#30340;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;Fusemate&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#24341;&#23548;&#30340;&#30456;&#20851;&#24615;&#27979;&#35797;&#20462;&#21098;&#35268;&#21017;&#65292;&#35299;&#20915;&#20102;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#38590;&#20197;&#25511;&#21046;ground clauses&#29983;&#25104;&#25968;&#37327;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21253;&#21547;&#8220;&#26102;&#38388;&#8221;&#30340;&#31034;&#20363;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fusemate&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#30340;&#25512;&#29702;&#24341;&#25806;&#21253;&#25324;&#19968;&#20010;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#30340;grounding&#32452;&#20214;&#21644;&#19968;&#20010;&#21464;&#37327;&#28040;&#38500;&#26041;&#27861;&#29992;&#20110;&#27010;&#29575;&#25512;&#29702;&#12290;Fusemate&#19981;&#21516;&#20110;&#22823;&#22810;&#25968;&#31995;&#32479;&#65292;&#23427;&#37319;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#24335;&#23545;&#31243;&#24207;&#36827;&#34892;grounding&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#38590;&#20197;&#25511;&#21046;&#29983;&#25104;&#30340;ground clauses&#25968;&#37327;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#38169;grounding&#21644;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#24341;&#23548;&#30340;&#30456;&#20851;&#24615;&#27979;&#35797;&#26469;&#20462;&#21098;&#19982;&#26597;&#35810;&#19981;&#19968;&#33268;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21253;&#21547;&#8220;&#26102;&#38388;&#8221;&#30340;&#31034;&#20363;&#65288;&#22914;&#65288;&#38544;&#34255;&#65289;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65289;&#36827;&#34892;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39640;&#20998;&#25903;&#38382;&#39064;&#19978;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#65292;&#25105;&#20204;&#30340;&#24615;&#33021;&#26356;&#20855;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Fusemate probabilistic logic programming system. Fusemate's inference engine comprises a grounding component and a variable elimination method for probabilistic inference. Fusemate differs from most other systems by grounding the program in a bottom-up way instead of the common top-down way. While bottom-up grounding is attractive for a number of reasons, e.g., for dynamically creating distributions of varying support sizes, it makes it harder to control the amount of ground clauses generated. We address this problem by interleaving grounding with a query-guided relevance test which prunes rules whose bodies are inconsistent with the query. We present our method in detail and demonstrate it with examples that involve "time", such as (hidden) Markov models. Our experiments demonstrate competitive or better performance compared to a state-of-the art probabilistic logic programming system, in particular for high branching problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.18703</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#20102;&#39640;&#24230;&#23454;&#29992;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#22522;&#30784;&#12290;LLMs &#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20419;&#20351;&#20154;&#20204;&#23558;&#20854;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21161;&#25163;&#29978;&#33267;&#26367;&#20195;&#29305;&#23450;&#39046;&#22495;&#30340;&#19987;&#23478;&#21644;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23558;LLMs&#30452;&#25509;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#20250;&#36935;&#21040;&#35768;&#22810;&#22256;&#38590;&#65292;&#21253;&#25324;&#39046;&#22495;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#39046;&#22495;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12289;&#39046;&#22495;&#30446;&#26631;&#30340;&#29420;&#29305;&#24615;&#20197;&#21450;&#32422;&#26463;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#24046;&#36317;&#65292;&#26368;&#36817;&#20960;&#24180;&#36827;&#34892;&#20102;&#24613;&#21095;&#22686;&#21152;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#33268;&#21147;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#28982;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#24635;&#32467;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;QA-Eval&#21644;&#25968;&#25454;&#38598;EVOUNA&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#21457;&#23637;&#21644;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.12421</link><description>&lt;p&gt;
&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Open-QA Evaluation. (arXiv:2305.12421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;QA-Eval&#21644;&#25968;&#25454;&#38598;EVOUNA&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#21457;&#23637;&#21644;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#23545;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35813;&#20219;&#21153;&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20107;&#23454;&#24615;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#24050;&#26174;&#31034;&#20986;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#34920;&#26126;&#20154;&#24037;&#35780;&#20272;&#20173;&#28982;&#26159;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35780;&#20272;QA&#35780;&#20272;&#65288;QA-Eval&#65289;&#20197;&#21450;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;EVOUNA&#65292;&#26088;&#22312;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;Open-QA&#20013;&#30340;&#26631;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#32467;&#26524;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#37027;&#20123;&#19982;&#20154;&#24037;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#35748;&#20026;&#23427;&#20204;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#20197;&#21450;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#20010;&#26032;&#30340;QA-Eval&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;EVOUNA&#23558;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#30340;&#24320;&#21457;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;&#65292;&#20998;&#31867;&#23427;&#20204;&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#20854;&#20998;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#20197;&#25552;&#20379;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2305.11391</link><description>&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation. (arXiv:2305.11391v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;&#65292;&#20998;&#31867;&#23427;&#20204;&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#20854;&#20998;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#20197;&#25552;&#20379;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#22312;&#35768;&#22810;&#30693;&#35782;&#39046;&#22495;&#20013;&#20026;&#32456;&#31471;&#29992;&#25143;&#25552;&#20379;&#35814;&#32454;&#21644;&#26377;&#26465;&#29702;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#36827;&#34892;&#20154;&#31867;&#32423;&#21035;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;AI&#30340;&#19968;&#27874;&#26032;&#28909;&#28526;&#12290;&#20026;&#20102;&#24212;&#23545;&#23427;&#20204;&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24555;&#36895;&#37319;&#29992;&#65292;&#26412;&#27425;&#35843;&#26597;&#20851;&#27880;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22238;&#39038;LLM&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#23427;&#20204;&#20998;&#31867;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#23558;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#20256;&#32479;&#36719;&#20214;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#39564;&#35777;&#21644;&#39564;&#35777;&#65288;V&#65286;V&#65289;&#25216;&#26415;&#65292;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;LLM&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#65292;&#20197;&#25552;&#20379;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#30830;&#20445;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#65306;&#34394;&#20551;&#24615;&#21644;&#35780;&#20272;&#12289;&#39564;&#35777;&#12289;&#36816;&#34892;&#26102;&#30417;&#35270;&#21644;&#36947;&#24503;&#20351;&#29992;&#12290;&#32771;&#34385;&#21040;LLM&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exploded a new heatwave of AI, for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities of the LLMs, categorising them into inherent issues, intended attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V&amp;V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and ethical use. Considering the fast development of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07011</link><description>&lt;p&gt;
&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#65306;&#35270;&#35273;&#21464;&#21387;&#22120;&#19979;&#30340;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21306;&#22495;&#24863;&#30693;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;RO-ViT&#65289;&#65292;&#19968;&#31181;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24314;&#35758;&#38543;&#26426;&#35009;&#21098;&#24182;&#35843;&#25972;&#20301;&#32622;&#23884;&#20837;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#25972;&#20010;&#22270;&#20687;&#20301;&#32622;&#23884;&#20837;&#12290;&#36825;&#26356;&#22909;&#22320;&#21305;&#37197;&#20102;&#26816;&#27979;&#24494;&#35843;&#38454;&#27573;&#20013;&#21306;&#22495;&#32423;&#21035;&#19978;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29992;&#32858;&#28966;&#25439;&#22833;&#26367;&#25442;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;softmax&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#37027;&#20123;&#26377;&#20449;&#24687;&#37327;&#20294;&#38590;&#20197;&#25429;&#25417;&#30340;&#20363;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20197;&#25913;&#36827;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;LVIS&#21644;COCO&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#23436;&#25972;&#27169;&#22411;&#21644;&#38646;-shot&#36716;&#31227;&#24615;&#33021;&#12290;RO-ViT&#22312;LVIS&#19978;&#23454;&#29616;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#36229;&#36807;&#29616;&#26377;&#26368;&#20339;&#26041;&#27861;5.8&#20010;&#30334;&#20998;&#28857;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38646;-shot&#36716;&#31227;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a contrastive image-text pretraining recipe to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of positional embeddings at region-level in the detection finetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to better learn the informative yet difficult examples. Finally, we leverage recent advances in novel object proposals to improve open-vocabulary detection finetuning. We evaluate our full model on the LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best existing approach by +5.8 points in addition to competitive zero-shot transfer detec
&lt;/p&gt;</description></item><item><title>TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.05105</link><description>&lt;p&gt;
&#38754;&#21521;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection. (arXiv:2305.05105v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05105
&lt;/p&gt;
&lt;p&gt;
TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#23626;ACM/IEEE&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;&#65288;TDC&#65289;&#20110;2022&#24180;&#22312;&#31532;41&#23626;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#22269;&#38469;&#20250;&#35758;&#65288;ICCAD&#65289;&#19978;&#20030;&#34892;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26376;&#30740;&#21457;&#31454;&#36187;&#12290;TDC'22&#19987;&#27880;&#20110;&#38656;&#35201;&#22312;&#21487;&#26893;&#20837;&#35774;&#22791;&#19978;&#21019;&#26032;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#30340;&#30495;&#23454;&#21307;&#30103;&#38382;&#39064;&#12290;TDC'22&#30340;&#25361;&#25112;&#38382;&#39064;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#24515;&#33039;&#38500;&#39076;&#22120;&#65288;ICDs&#65289;&#19978;&#20351;&#29992;&#30340;&#20302;&#21151;&#29575;&#24494;&#25511;&#21046;&#22120;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;90&#20010;&#21463;&#35797;&#32773;&#30340;8&#31181;&#19981;&#21516;&#24515;&#24459;&#31867;&#22411;&#30340;&#36229;&#36807;38,000&#20010;5&#31186;&#24515;&#20869;&#30005;&#22270;&#65288;IEGM&#65289;&#29255;&#27573;&#12290;&#19987;&#29992;&#30828;&#20214;&#24179;&#21488;&#26159;STMicroelectronics&#21046;&#36896;&#30340;NUCLEO-L432KC&#12290;TDC'22&#38754;&#21521;&#20840;&#29699;&#22810;&#20154;&#22242;&#38431;&#65292;&#21560;&#24341;&#20102;&#26469;&#33258;50&#22810;&#20010;&#32452;&#32455;&#30340;150&#22810;&#25903;&#38431;&#20237;&#21442;&#36187;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#36825;&#19968;&#21307;&#30103;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The first ACM/IEEE TinyML Design Contest (TDC) held at the 41st International Conference on Computer-Aided Design (ICCAD) in 2022 is a challenging, multi-month, research and development competition. TDC'22 focuses on real-world medical problems that require the innovation and implementation of artificial intelligence/machine learning (AI/ML) algorithms on implantable devices. The challenge problem of TDC'22 is to develop a novel AI/ML-based real-time detection algorithm for life-threatening ventricular arrhythmia over low-power microcontrollers utilized in Implantable Cardioverter-Defibrillators (ICDs). The dataset contains more than 38,000 5-second intracardiac electrograms (IEGMs) segments over 8 different types of rhythm from 90 subjects. The dedicated hardware platform is NUCLEO-L432KC manufactured by STMicroelectronics. TDC'22, which is open to multi-person teams world-wide, attracted more than 150 teams from over 50 organizations. This paper first presents the medical problem, da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02527</link><description>&lt;p&gt;
&#12298;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward. (arXiv:2305.02527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#12290;&#22870;&#21169;&#30340;&#24310;&#36831;&#21644;&#22797;&#26434;&#24615;&#24847;&#21619;&#30528;&#22312;&#32473;&#23450;&#29366;&#24577;&#19979;&#37319;&#21462;&#34892;&#21160;&#29983;&#25104;&#30340;&#22870;&#21169;&#34987;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#24310;&#36831;&#30340;&#26102;&#38388;&#23454;&#20363;&#20013;&#34987;&#39034;&#24207;&#23454;&#29616;&#12290;&#37096;&#20998;&#21311;&#21517;&#23646;&#24615;&#24847;&#21619;&#30528;&#23545;&#20110;&#27599;&#20010;&#29366;&#24577;&#65292;&#23398;&#20064;&#32773;&#21482;&#35266;&#23519;&#21040;&#22312;&#35813;&#29366;&#24577;&#19979;&#37319;&#21462;&#19981;&#21516;&#34892;&#21160;&#20135;&#29983;&#30340;&#36807;&#21435;&#22870;&#21169;&#32452;&#25104;&#37096;&#20998;&#30340;&#24635;&#21644;&#65292;&#20294;&#26159;&#22312;&#35266;&#23519;&#23454;&#20363;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\mathrm{DUCRL2}$&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33719;&#24471;&#27492;&#35774;&#32622;&#30340;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#34920;&#26126;&#23427;&#23454;&#29616;&#20102;$\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ &#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$S$&#21644;$A$&#20998;&#21035;&#26159;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;$D$&#26159;MDP&#30340;&#30452;&#24452;&#65292;$d$&#26159;&#19968;&#20010;&#30001;&#26368;&#22823;&#22870;&#21169;&#24310;&#36831;&#38480;&#21046;&#30340;&#21442;&#25968;&#65292;$T$&#34920;&#31034;&#26102;&#38388;&#30340;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate an infinite-horizon average reward Markov Decision Process (MDP) with delayed, composite, and partially anonymous reward feedback. The delay and compositeness of rewards mean that rewards generated as a result of taking an action at a given state are fragmented into different components, and they are sequentially realized at delayed time instances. The partial anonymity attribute implies that a learner, for each state, only observes the aggregate of past reward components generated as a result of different actions taken at that state, but realized at the observation instance. We propose an algorithm named $\mathrm{DUCRL2}$ to obtain a near-optimal policy for this setting and show that it achieves a regret bound of $\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ where $S$ and $A$ are the sizes of the state and action spaces, respectively, $D$ is the diameter of the MDP, $d$ is a parameter upper bounded by the maximum reward delay, and $T$ denotes the time horizon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#31181;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#20197;&#22312;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#30340;&#21516;&#26102;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02054</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#65306;&#24378;&#21270;&#23398;&#20064;&#20013;&#36951;&#24536;&#29616;&#35937;&#30340;&#20869;&#23384;&#33410;&#32422;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Map-based Experience Replay: A Memory-Efficient Solution to Catastrophic Forgetting in Reinforcement Learning. (arXiv:2305.02054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#31181;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#20197;&#22312;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#30340;&#21516;&#26102;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#35757;&#32451;&#26032;&#25968;&#25454;&#26102;&#24120;&#24120;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#65292;&#36951;&#24536;&#20808;&#21069;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22238;&#25918;&#35760;&#24518;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#23427;&#20250;&#23545;&#26087;&#21644;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#21435;&#20851;&#32852;&#21644;&#28151;&#27927;&#12290;&#20182;&#20204;&#22825;&#30495;&#22320;&#25353;&#29031;&#29366;&#24577;&#36807;&#28193;&#30340;&#39034;&#24207;&#23384;&#20648;&#29366;&#24577;&#36716;&#21464;&#65292;&#32780;&#19981;&#32771;&#34385;&#20887;&#20313;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Grow-When-Required&#65288;GWR&#65289;&#33258;&#32452;&#32455;&#32593;&#32476;&#30340;&#26032;&#22411;&#35748;&#30693;&#21551;&#21457;&#24335;&#22238;&#25918;&#20869;&#23384;&#26041;&#27861;&#65292;&#23427;&#31867;&#20284;&#20110;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#19990;&#30028;&#35748;&#30693;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#20010;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#23558;&#30456;&#20284;&#30340;&#26679;&#26412;&#21512;&#24182;&#20197;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#24182;&#22686;&#21152;&#26679;&#26412;&#20043;&#38388;&#30340;&#20004;&#20004;&#36317;&#31163;&#65292;&#20174;&#32780;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#34920;&#26126;&#65292;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#20801;&#35768;&#26174;&#30528;&#20943;&#23569;&#20869;&#23384;&#65292;&#21482;&#20250;&#20135;&#29983;&#36731;&#24494;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning agents often suffer from catastrophic forgetting, forgetting previously found solutions in parts of the input space when training on new data. Replay Memories are a common solution to the problem, decorrelating and shuffling old and new training samples. They naively store state transitions as they come in, without regard for redundancy. We introduce a novel cognitive-inspired replay memory approach based on the Grow-When-Required (GWR) self-organizing network, which resembles a map-based mental model of the world. Our approach organizes stored transitions into a concise environment-model-like network of state-nodes and transition-edges, merging similar samples to reduce the memory size and increase pair-wise distance among samples, which increases the relevancy of each sample. Overall, our paper shows that map-based experience replay allows for significant memory reduction with only small performance decreases.
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#21333;&#24103;&#28145;&#24230;&#21644;&#22810;&#24103;&#28145;&#24230;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20687;&#32032;&#36880;&#20687;&#32032;&#33258;&#36866;&#24212;&#28145;&#24230;&#37319;&#26679;&#27169;&#22359;&#65292;&#20197;&#21333;&#24103;&#28145;&#24230;&#20026;&#25351;&#23548;&#26469;&#35757;&#32451;&#22810;&#24103;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#37325;&#25237;&#24433;&#20026;&#22522;&#30784;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#20248;&#21270;&#21333;&#24103;&#28145;&#24230;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.12685</link><description>&lt;p&gt;
&#25506;&#32034;&#33258;&#30417;&#30563;&#21333;&#24103;&#21644;&#22810;&#24103;&#28145;&#24230;&#20272;&#35745;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Mutual Influence between Self-Supervised Single-Frame and Multi-Frame Depth Estimation. (arXiv:2304.12685v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12685
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#21333;&#24103;&#28145;&#24230;&#21644;&#22810;&#24103;&#28145;&#24230;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20687;&#32032;&#36880;&#20687;&#32032;&#33258;&#36866;&#24212;&#28145;&#24230;&#37319;&#26679;&#27169;&#22359;&#65292;&#20197;&#21333;&#24103;&#28145;&#24230;&#20026;&#25351;&#23548;&#26469;&#35757;&#32451;&#22810;&#24103;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#37325;&#25237;&#24433;&#20026;&#22522;&#30784;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#20248;&#21270;&#21333;&#24103;&#28145;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#30417;&#30563;&#21333;&#24103;&#21644;&#22810;&#24103;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#37117;&#21482;&#38656;&#35201;&#26080;&#26631;&#31614;&#21333;&#30446;&#35270;&#39057;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#20204;&#25152;&#21033;&#29992;&#30340;&#20449;&#24687;&#19981;&#21516;&#65292;&#21333;&#24103;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#22806;&#35980;&#30340;&#29305;&#24449;&#65292;&#32780;&#22810;&#24103;&#26041;&#27861;&#21017;&#19987;&#27880;&#20110;&#20960;&#20309;&#32447;&#32034;&#12290;&#32771;&#34385;&#21040;&#21333;&#24103;&#21644;&#22810;&#24103;&#26041;&#27861;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#21033;&#29992;&#21333;&#24103;&#28145;&#24230;&#26469;&#25913;&#36827;&#22810;&#24103;&#28145;&#24230;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#26082;&#19981;&#33021;&#21033;&#29992;&#21333;&#24103;&#28145;&#24230;&#21644;&#22810;&#24103;&#28145;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25913;&#36827;&#22810;&#24103;&#28145;&#24230;&#65292;&#20063;&#19981;&#33021;&#21033;&#29992;&#22810;&#24103;&#28145;&#24230;&#26469;&#20248;&#21270;&#21333;&#24103;&#28145;&#24230;&#27169;&#22411;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#21333;&#24103;&#21644;&#22810;&#24103;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#20010;&#30001;&#21333;&#24103;&#28145;&#24230;&#25351;&#23548;&#30340;&#20687;&#32032;&#36880;&#20687;&#32032;&#33258;&#36866;&#24212;&#28145;&#24230;&#37319;&#26679;&#27169;&#22359;&#26469;&#35757;&#32451;&#22810;&#24103;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#23567;&#37325;&#25237;&#24433;&#20026;&#22522;&#30784;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#21033;&#29992;&#22810;&#24103;&#28145;&#24230;&#20248;&#21270;&#21333;&#24103;&#28145;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although both self-supervised single-frame and multi-frame depth estimation methods only require unlabeled monocular videos for training, the information they leverage varies because single-frame methods mainly rely on appearance-based features while multi-frame methods focus on geometric cues. Considering the complementary information of single-frame and multi-frame methods, some works attempt to leverage single-frame depth to improve multi-frame depth. However, these methods can neither exploit the difference between single-frame depth and multi-frame depth to improve multi-frame depth nor leverage multi-frame depth to optimize single-frame depth models. To fully utilize the mutual influence between single-frame and multi-frame methods, we propose a novel self-supervised training framework. Specifically, we first introduce a pixel-wise adaptive depth sampling module guided by single-frame depth to train the multi-frame model. Then, we leverage the minimum reprojection based distillat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#35273;&#36741;&#21161;&#35745;&#21010;&#65288;VPA&#65289;&#30340;&#20219;&#21153;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#35270;&#39057;&#34892;&#21160;&#20998;&#21106;&#21644;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;AI&#21161;&#25163;&#25351;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#22810;&#27493;&#39588;&#30446;&#26631;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.09179</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20154;&#31867;&#36741;&#21161;&#35270;&#35273;&#35745;&#21010;&#32773;
&lt;/p&gt;
&lt;p&gt;
Pretrained Language Models as Visual Planners for Human Assistance. (arXiv:2304.09179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#35273;&#36741;&#21161;&#35745;&#21010;&#65288;VPA&#65289;&#30340;&#20219;&#21153;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#35270;&#39057;&#34892;&#21160;&#20998;&#21106;&#21644;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;AI&#21161;&#25163;&#25351;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#22810;&#27493;&#39588;&#30446;&#26631;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;AI&#21161;&#25163;&#25351;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#22810;&#27493;&#39588;&#30446;&#26631;&#30340;&#36827;&#23637;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#35273;&#36741;&#21161;&#35745;&#21010;&#65288;VPA&#65289;&#30340;&#20219;&#21153;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#31616;&#35201;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;&#20363;&#22914;&#8220;&#21046;&#20316;&#20070;&#26550;&#8221;&#65292;&#20197;&#21450;&#29992;&#25143;&#36804;&#20170;&#20026;&#27490;&#30340;&#35270;&#39057;&#36827;&#23637;&#65292;VPA&#30340;&#30446;&#26631;&#26159;&#33719;&#24471;&#19968;&#20010;&#35745;&#21010;&#65292;&#21363;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#22914;&#8220;&#30722;&#20809;&#20070;&#26550;&#8221;&#12289;&#8220;&#28034;&#28422;&#20070;&#26550;&#8221;&#31561;&#65292;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#36825;&#38656;&#35201;&#35780;&#20272;&#29992;&#25143;&#22312;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#30340;&#36827;&#23637;&#65292;&#24182;&#19982;&#24213;&#23618;&#30446;&#26631;&#30340;&#35201;&#27714;&#30456;&#20851;&#32852;&#65292;&#21363;&#34892;&#21160;&#30340;&#30456;&#20851;&#24615;&#21644;&#20854;&#20013;&#30340;&#25490;&#24207;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#36825;&#38656;&#35201;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#35270;&#39057;&#21382;&#21490;&#35760;&#24405;&#21644;&#20219;&#24847;&#22797;&#26434;&#30340;&#34892;&#21160;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;VPA&#20998;&#35299;&#20026;&#35270;&#39057;&#34892;&#21160;&#20998;&#21106;&#21644;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#39044;&#27979;&#27493;&#39588;&#20844;&#24335;&#21270;&#20026;&#22810;&#27169;&#24577;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#21010;&#32773;&#65288;VLaMP&#65289;&#65292;&#20854;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LMs&#20316;&#20026;&#24207;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;Epic Kitchen&#21644;Charades-Ego&#65289;&#19978;&#23637;&#31034;&#20102;VLaMP&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VLaMP&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make progress towards multi-modal AI assistants which can guide users to achieve complex multi-step goals, we propose the task of Visual Planning for Assistance (VPA). Given a goal briefly described in natural language, e.g., "make a shelf", and a video of the user's progress so far, the aim of VPA is to obtain a plan, i.e., a sequence of actions such as "sand shelf", "paint shelf", etc., to achieve the goal. This requires assessing the user's progress from the untrimmed video, and relating it to the requirements of underlying goal, i.e., relevance of actions and ordering dependencies amongst them. Consequently, this requires handling long video history, and arbitrarily complex action dependencies. To address these challenges, we decompose VPA into video action segmentation and forecasting. We formulate the forecasting step as a multi-modal sequence modeling problem and present Visual Language Model based Planner (VLaMP), which leverages pre-trained LMs as the sequence model. We dem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20915;&#31574;Transformer&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#25506;&#32034;&#29992;&#25143;&#34892;&#20026;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25351;&#23548;&#20195;&#29702;&#25429;&#25417;&#21160;&#24577;&#20852;&#36259;&#65292;&#24182;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#24773;&#22659;&#19979;&#25968;&#25454;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07920</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22240;&#26524;&#20915;&#31574;Transformer&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Causal Decision Transformer for Recommender Systems via Offline Reinforcement Learning. (arXiv:2304.07920v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20915;&#31574;Transformer&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#25506;&#32034;&#29992;&#25143;&#34892;&#20026;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25351;&#23548;&#20195;&#29702;&#25429;&#25417;&#21160;&#24577;&#20852;&#36259;&#65292;&#24182;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#24773;&#22659;&#19979;&#25968;&#25454;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#25512;&#33616;&#31574;&#30053;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#35774;&#35745;&#24448;&#24448;&#24182;&#19981;&#31616;&#21333;&#12290;&#25506;&#32034;&#29992;&#25143;&#34892;&#20026;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#65292;&#25351;&#23548;&#20195;&#29702;&#25429;&#25417;&#29992;&#25143;&#30340;&#21160;&#24577;&#20852;&#36259;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20223;&#30495;&#29615;&#22659;&#30340;&#20856;&#22411;&#38480;&#21046;&#65288;&#22914;&#25968;&#25454;&#25928;&#29575;&#65289;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#26080;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#24773;&#22659;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#20223;&#30495;&#22120;&#65292;&#20294;&#25968;&#25454;&#25928;&#29575;&#20351;&#23398;&#20064;&#36807;&#31243;&#21464;&#24471;&#26356;&#21152;&#32531;&#24930;&#12290;&#30001;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#36136;&#26159;&#36890;&#36807;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#23398;&#20064;&#65292;&#23427;&#26080;&#27861;&#22312;&#21333;&#27425;&#20132;&#20114;&#36807;&#31243;&#20013;&#25910;&#38598;&#36275;&#22815;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19981;&#20687;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37027;&#26679;&#20855;&#22791;&#30452;&#25509;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#30340;&#29282;&#22266;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-based recommender systems have recently gained popularity. However, the design of the reward function, on which the agent relies to optimize its recommendation policy, is often not straightforward. Exploring the causality underlying users' behavior can take the place of the reward function in guiding the agent to capture the dynamic interests of users. Moreover, due to the typical limitations of simulation environments (e.g., data inefficiency), most of the work cannot be broadly applied in large-scale situations. Although some works attempt to convert the offline dataset into a simulator, data inefficiency makes the learning process even slower. Because of the nature of reinforcement learning (i.e., learning by interaction), it cannot collect enough data to train during a single interaction. Furthermore, traditional reinforcement learning algorithms do not have a solid capability like supervised learning methods to learn from offline datasets directly. In this p
&lt;/p&gt;</description></item><item><title>&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#38656;&#35201;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#65292;&#32780;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#26159;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.06634</link><description>&lt;p&gt;
PGTask&#65306;&#20171;&#32461;&#20174;&#23545;&#35805;&#20013;&#29983;&#25104;&#26723;&#26696;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
PGTask: Introducing the Task of Profile Generation from Dialogues. (arXiv:2304.06634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06634
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#38656;&#35201;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#65292;&#32780;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#26159;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23558;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#26469;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#20449;&#24687;&#31232;&#23569;&#19988;&#38590;&#20197;&#33719;&#21462;&#65292;&#36825;&#20351;&#24471;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#25104;&#20026;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#12290;&#25105;&#20204;&#20026;&#27492;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#30456;&#20851;&#35805;&#35821;&#23545;&#40784;&#30340;&#26723;&#26696;&#21477;&#23376;&#65292;&#20174;&#23545;&#35805;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26723;&#26696;&#29983;&#25104;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26723;&#26696;&#29983;&#25104;&#30340;&#25361;&#25112;&#65292;&#24182;&#24076;&#26395;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches have attempted to personalize dialogue systems by leveraging profile information into models. However, this knowledge is scarce and difficult to obtain, which makes the extraction/generation of profile information from dialogues a fundamental asset. To surpass this limitation, we introduce the Profile Generation Task (PGTask). We contribute with a new dataset for this problem, comprising profile sentences aligned with related utterances, extracted from a corpus of dialogues. Furthermore, using state-of-the-art methods, we provide a benchmark for profile generation on this novel dataset. Our experiments disclose the challenges of profile generation, and we hope that this introduces a new research direction.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;ASR&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#21644;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#30456;&#27604;&#65292;ASR&#21487;&#20197;&#25104;&#21151;&#32771;&#34385;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#23454;&#29616;&#25512;&#29702;&#26399;&#38388;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06345</link><description>&lt;p&gt;
ASR: &#20687;&#27880;&#24847;&#21147;&#19968;&#26679;&#30340;&#32467;&#26500;&#20877;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
ASR: Attention-alike Structural Re-parameterization. (arXiv:2304.06345v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06345
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;ASR&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#21644;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#30456;&#27604;&#65292;ASR&#21487;&#20197;&#25104;&#21151;&#32771;&#34385;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#23454;&#29616;&#25512;&#29702;&#26399;&#38388;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20877;&#21442;&#25968;&#21270;&#65288;SRP&#65289;&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#35813;&#25216;&#26415;&#20351;&#24471;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#36825;&#20123;&#36716;&#25442;&#20943;&#23569;&#24615;&#33021;&#25552;&#21319;&#30340;&#26032;&#22686;&#20195;&#20215;&#65292;&#20363;&#22914;&#21442;&#25968;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#22240;&#27492;SRP&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#24050;&#25104;&#21151;&#32771;&#34385;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#26550;&#26500;&#65292;&#20363;&#22914;&#24402;&#19968;&#21270;&#12289;&#27744;&#21270;&#26041;&#27861;&#12289;&#22810;&#20998;&#25903;&#21367;&#31215;&#31561;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#33258;&#27880;&#24847;&#27169;&#22359;&#30001;&#20110;&#22312;&#25512;&#29702;&#26399;&#38388;&#36890;&#24120;&#20197;&#20056;&#27861;&#26041;&#24335;&#20316;&#29992;&#20110;&#39592;&#24178;&#32593;&#32476;&#24182;&#19988;&#27169;&#22359;&#30340;&#36755;&#20986;&#22312;&#25512;&#29702;&#26102;&#20381;&#36182;&#20110;&#36755;&#20837;&#65292;&#25152;&#20197;&#26080;&#27861;&#30452;&#25509;&#23454;&#29616;SRP&#65292;&#32780;&#36825;&#38480;&#21046;&#20102;SRP&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32479;&#35745;&#35282;&#24230;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
The structural re-parameterization (SRP) technique is a novel deep learning technique that achieves interconversion between different network architectures through equivalent parameter transformations. This technique enables the mitigation of the extra costs for performance improvement during training, such as parameter size and inference time, through these transformations during inference, and therefore SRP has great potential for industrial and practical applications. The existing SRP methods have successfully considered many commonly used architectures, such as normalizations, pooling methods, multi-branch convolution. However, the widely used self-attention modules cannot be directly implemented by SRP due to these modules usually act on the backbone network in a multiplicative manner and the modules' output is input-dependent during inference, which limits the application scenarios of SRP. In this paper, we conduct extensive experiments from a statistical perspective and discover
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#20132;&#20114;&#27169;&#22359;&#36827;&#34892;&#20132;&#20114;&#24863;&#30693;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#26102;&#31354;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04688</link><description>&lt;p&gt;
&#20132;&#20114;&#24863;&#30693;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#26102;&#31354;&#21160;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection. (arXiv:2304.04688v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04688
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#20132;&#20114;&#27169;&#22359;&#36827;&#34892;&#20132;&#20114;&#24863;&#30693;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#26102;&#31354;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#27599;&#20010;&#20154;&#22312;&#35270;&#39057;&#20013;&#21160;&#20316;&#21457;&#29983;&#30340;&#26102;&#38388;&#21644;&#20301;&#32622;&#65292;&#24182;&#23545;&#30456;&#24212;&#30340;&#21160;&#20316;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#20840;&#30417;&#30563;&#23398;&#20064;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#38590;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20195;&#34920;&#24615;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#20132;&#20114;&#27169;&#22359;&#24314;&#27169;&#36825;&#20123;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#24471;&#21040;&#20132;&#20114;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#36825;&#20010;&#29305;&#24449;&#25552;&#31034;&#27599;&#20010;&#26631;&#31614;&#20197;&#33719;&#21462;&#26356;&#21512;&#36866;&#30340;&#25991;&#26412;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35745;&#31639;&#27599;&#20010;&#26631;&#31614;&#30340;&#20132;&#20114;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#30830;&#23450;&#21160;&#20316;&#31867;&#21035;&#12290;&#22312;J-HMDB&#21644;UCF101-24&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20132;&#20114;&#27169;&#22359;&#21644;&#25552;&#31034;&#20351;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#26356;&#21152;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of spatial-temporal action detection is to determine the time and place where each person's action occurs in a video and classify the corresponding action category. Most of the existing methods adopt fully-supervised learning, which requires a large amount of training data, making it very difficult to achieve zero-shot learning. In this paper, we propose to utilize a pre-trained visual-language model to extract the representative image and text features, and model the relationship between these features through different interaction modules to obtain the interaction feature. In addition, we use this feature to prompt each label to obtain more appropriate text features. Finally, we calculate the similarity between the interaction feature and the text feature for each label to determine the action category. Our experiments on J-HMDB and UCF101-24 datasets demonstrate that the proposed interaction module and prompting make the visual-language features better aligned, thus achievi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;HR-DSS&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#24110;&#21161;&#20154;&#21147;&#36164;&#28304;&#37096;&#38376;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#30340;&#21592;&#24037;&#27969;&#22833;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#25552;&#20379;&#8220;What-if-analysis&#8221;&#26469;&#35266;&#23519;&#20010;&#20307;&#21592;&#24037;&#21487;&#33021;&#23548;&#33268;&#31163;&#32844;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.03103</link><description>&lt;p&gt;
&#30041;&#20303;&#20154;&#25165;&#26159;&#26368;&#37325;&#35201;&#30340;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#26469;&#35299;&#20915;&#21592;&#24037;&#31163;&#32844;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Retention Is All You Need. (arXiv:2304.03103v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;HR-DSS&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#24110;&#21161;&#20154;&#21147;&#36164;&#28304;&#37096;&#38376;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#30340;&#21592;&#24037;&#27969;&#22833;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#25552;&#20379;&#8220;What-if-analysis&#8221;&#26469;&#35266;&#23519;&#20010;&#20307;&#21592;&#24037;&#21487;&#33021;&#23548;&#33268;&#31163;&#32844;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29087;&#32451;&#30340;&#21592;&#24037;&#36890;&#24120;&#34987;&#35270;&#20026;&#32452;&#32455;&#30340;&#26368;&#37325;&#35201;&#25903;&#26609;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#32452;&#32455;&#37117;&#38754;&#20020;&#30528;&#39640;&#31163;&#32844;&#29575;&#21644;&#27969;&#22833;&#29575;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#20998;&#26512;&#31163;&#32844;&#21450;&#20854;&#21407;&#22240;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#35299;&#37322;&#20173;&#28982;&#19981;&#36879;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HR-DSS&#26041;&#27861;&#65292;&#21363;&#20154;&#21147;&#36164;&#28304;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#35299;&#20915;&#21592;&#24037;&#27969;&#22833;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#26088;&#22312;&#24110;&#21161;&#20154;&#21147;&#36164;&#28304;&#37096;&#38376;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#20102;&#20843;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#19988;&#26368;&#20339;&#34920;&#29616;&#30340;&#27169;&#22411;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#32463;&#36807;&#20102;SHAP&#35299;&#37322;&#24615;&#36807;&#31243;&#30340;&#22788;&#29702;&#12290;&#25105;&#20204;&#20248;&#21270;&#20102;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#8220;What-if-analysis&#8221;&#26469;&#35266;&#23519;&#20010;&#20307;&#21592;&#24037;&#21487;&#33021;&#23548;&#33268;&#31163;&#32844;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skilled employees are usually seen as the most important pillar of an organization. Despite this, most organizations face high attrition and turnover rates. While several machine learning models have been developed for analyzing attrition and its causal factors, the interpretations of those models remain opaque. In this paper, we propose the HR-DSS approach, which stands for Human Resource Decision Support System, and uses explainable AI for employee attrition problems. The system is designed to assist human resource departments in interpreting the predictions provided by machine learning models. In our experiments, eight machine learning models are employed to provide predictions, and the results achieved by the best-performing model are further processed by the SHAP explainability process. We optimize both the correctness and explanation of the results. Furthermore, using "What-if-analysis", we aim to observe plausible causes for attrition of an individual employee. The results show 
&lt;/p&gt;</description></item><item><title>GINA-3D&#26159;&#19968;&#31181;&#23398;&#20064;&#20174;&#30495;&#23454;&#22330;&#26223;&#20013;&#29983;&#25104;3D&#38544;&#24335;&#31070;&#32463;&#36164;&#20135;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#33258;&#20027;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#23398;&#20064;&#38382;&#39064;&#30340;&#27979;&#35797;&#21644;&#39564;&#35777;&#29615;&#22659;&#26159;&#37325;&#35201;&#30340;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2304.02163</link><description>&lt;p&gt;
GINA-3D&#65306;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23398;&#20064;&#29983;&#25104;&#38544;&#24335;&#31070;&#32463;&#36164;&#20135;
&lt;/p&gt;
&lt;p&gt;
GINA-3D: Learning to Generate Implicit Neural Assets in the Wild. (arXiv:2304.02163v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02163
&lt;/p&gt;
&lt;p&gt;
GINA-3D&#26159;&#19968;&#31181;&#23398;&#20064;&#20174;&#30495;&#23454;&#22330;&#26223;&#20013;&#29983;&#25104;3D&#38544;&#24335;&#31070;&#32463;&#36164;&#20135;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#33258;&#20027;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#23398;&#20064;&#38382;&#39064;&#30340;&#27979;&#35797;&#21644;&#39564;&#35777;&#29615;&#22659;&#26159;&#37325;&#35201;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#24314;&#27169;3D&#19990;&#30028;&#20197;&#36827;&#34892;&#20223;&#30495;&#26159;&#24320;&#21457;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#23398;&#20064;&#38382;&#39064;&#30340;&#27979;&#35797;&#21644;&#39564;&#35777;&#29615;&#22659;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#21019;&#24314;&#25110;&#37325;&#26032;&#21019;&#24314;&#31867;&#20284;&#30495;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#26159;&#22256;&#38590;&#65292;&#26114;&#36149;&#19988;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#26368;&#36817;&#30340;&#29983;&#25104;&#27169;&#22411;&#25216;&#26415;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#20016;&#23500;&#30340;2D&#22270;&#20687;&#26469;&#23398;&#20064;3D&#36164;&#20135;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637; -- &#20294;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21033;&#29992;&#20154;&#31867;&#31574;&#21010;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#25110;&#25163;&#21160;&#21019;&#24314;&#30340;&#21512;&#25104;3D&#29615;&#22659;&#30340;&#28210;&#26579;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;GINA-3D&#65292;&#36825;&#26159;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#26469;&#33258;&#30456;&#26426;&#21644;LiDAR&#20256;&#24863;&#22120;&#30340;&#30495;&#23454;&#39550;&#39542;&#25968;&#25454;&#21019;&#24314;&#30495;&#23454;3D&#38544;&#24335;&#31070;&#32463;&#36164;&#20135;&#65292;&#21253;&#25324;&#21508;&#31181;&#36710;&#36742;&#21644;&#34892;&#20154;&#12290;&#19982;&#29616;&#26377;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#30495;&#23454;&#39550;&#39542;&#29615;&#22659;&#30001;&#20110;&#36974;&#25377;&#65292;&#20809;&#29031;&#21464;&#21270;&#21644;&#38271;&#23614;&#20998;&#24067;&#32780;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#12290;GINA-3D&#36890;&#36807;&#35299;&#32806;&#20174;&#21333;&#20010;&#35270;&#35282;&#29983;&#25104;3D&#36798;&#21040;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#30446;&#30340;&#65292;&#36827;&#32780;&#20351;3D&#22330;&#26223;&#30340;&#33258;&#21160;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the 3D world from sensor data for simulation is a scalable way of developing testing and validation environments for robotic learning problems such as autonomous driving. However, manually creating or re-creating real-world-like environments is difficult, expensive, and not scalable. Recent generative model techniques have shown promising progress to address such challenges by learning 3D assets using only plentiful 2D images -- but still suffer limitations as they leverage either human-curated image datasets or renderings from manually-created synthetic 3D environments. In this paper, we introduce GINA-3D, a generative model that uses real-world driving data from camera and LiDAR sensors to create realistic 3D implicit neural assets of diverse vehicles and pedestrians. Compared to the existing image datasets, the real-world driving setting poses new challenges due to occlusions, lighting-variations and long-tail distributions. GINA-3D tackles these challenges by decoupling re
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#31867;&#21512;&#20316;&#26159;&#21542;&#22686;&#24378;&#20102;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#20004;&#32452;&#20154;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01002</link><description>&lt;p&gt;
&#20154;&#31867;&#21512;&#20316;&#26159;&#21542;&#22686;&#24378;&#20102;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?. (arXiv:2304.01002v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01002
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#31867;&#21512;&#20316;&#26159;&#21542;&#22686;&#24378;&#20102;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#20316;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#20004;&#32452;&#20154;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#12289;LLaMA&#65289;&#30340;&#36827;&#23637;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#20889;&#20316;&#30340;&#36830;&#36143;&#21477;&#23376;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#25152;&#35859;&#30340;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#24341;&#21457;&#20102;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#25285;&#24551;&#65292;&#38656;&#35201;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#26469;&#21306;&#20998;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#21644;&#20154;&#31867;&#20070;&#20889;&#25991;&#26412;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#26816;&#27979;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#8220;&#21512;&#20316;&#8221;&#26159;&#21542;&#33021;&#25552;&#39640;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22635;&#34917;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#29702;&#35299;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#20004;&#32452;&#20154;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;&#65288;1&#65289;&#26469;&#33258;AMT&#24179;&#21488;&#30340;&#38750;&#19987;&#23478;&#32676;&#20307;&#21644;&#65288;2&#65289;&#26469;&#33258;Upwork&#24179;&#21488;&#30340;&#20889;&#20316;&#19987;&#23478;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#20043;&#38388;&#30340;&#21512;&#20316;&#21487;&#33021;&#20250;&#25552;&#39640;&#20004;&#32452;&#23545;&#28145;&#24230;&#20266;&#36896;&#25991;&#26412;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#38750;&#19987;&#23478;&#32452;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;6.36%&#65292;&#19987;&#23478;&#32452;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;12.76%&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the generation of coherent sentences resembling human writing on a large scale, resulting in the creation of so-called deepfake texts. However, this progress poses security and privacy concerns, necessitating effective solutions for distinguishing deepfake texts from human-written ones. Although prior works studied humans' ability to detect deepfake texts, none has examined whether "collaboration" among humans improves the detection of deepfake texts. In this study, to address this gap of understanding on deepfake texts, we conducted experiments with two groups: (1) nonexpert individuals from the AMT platform and (2) writing experts from the Upwork platform. The results demonstrate that collaboration among humans can potentially improve the detection of deepfake texts for both groups, increasing detection accuracies by 6.36% for non-experts and 12.76% for experts, respectively, compared to individuals' detection accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.12091</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning. (arXiv:2303.12091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#26631;&#35760;&#25968;&#25454;&#12289;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#32771;&#34385;&#21040;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#21253;&#21547;&#26631;&#35760;&#25968;&#25454;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#31867;&#21035;&#65288;&#24322;&#24120;&#20540;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;ANEDL&#65289;&#65292;&#20197;&#24212;&#23545;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#22914;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#21644;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#20316;&#20026;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#22120;&#26469;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#20351;EDL&#26356;&#21152;&#36866;&#21512;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;ANEDL&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GeoMIM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#30340;&#26041;&#24335;&#26469;&#25913;&#36827;&#22810;&#35270;&#35282;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#28608;&#20809;&#38647;&#36798;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#24182;&#21033;&#29992;&#28608;&#20809;&#38647;&#36798;BEV&#29305;&#24449;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GeoMIM&#22312;&#22810;&#35270;&#35282;&#19977;&#32500;&#29702;&#35299;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11325</link><description>&lt;p&gt;
GeoMIM&#65306;&#36890;&#36807;&#22522;&#20110;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#30340;&#19977;&#32500;&#30693;&#35782;&#36716;&#31227;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#35270;&#35282;&#19977;&#32500;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
GeoMIM: Towards Better 3D Knowledge Transfer via Masked Image Modeling for Multi-view 3D Understanding. (arXiv:2303.11325v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11325
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GeoMIM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#30340;&#26041;&#24335;&#26469;&#25913;&#36827;&#22810;&#35270;&#35282;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#28608;&#20809;&#38647;&#36798;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#24182;&#21033;&#29992;&#28608;&#20809;&#38647;&#36798;BEV&#29305;&#24449;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GeoMIM&#22312;&#22810;&#35270;&#35282;&#19977;&#32500;&#29702;&#35299;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#22810;&#35270;&#35282;&#22522;&#20110;&#30456;&#26426;&#30340;&#19977;&#32500;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28608;&#20809;&#38647;&#36798;&#26816;&#27979;&#27169;&#22411;&#23558;&#30693;&#35782;&#36716;&#31227;&#21040;&#22522;&#20110;&#30456;&#26426;&#30340;&#23398;&#29983;&#32593;&#32476;&#20013;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#28608;&#20809;&#38647;&#36798;BEV&#29305;&#24449;&#21644;&#22522;&#20110;&#30456;&#26426;&#30340;BEV&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#30340;&#39046;&#22495;&#24046;&#24322;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#21644;&#26469;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20309;&#22686;&#24378;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#65288;GeoMIM&#65289;&#65292;&#20197;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#36890;&#36807;&#28608;&#20809;&#38647;&#36798;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#25913;&#36827;&#22522;&#20110;&#30456;&#26426;&#30340;&#22810;&#35270;&#35282;&#19977;&#32500;&#26816;&#27979;&#12290;GeoMIM&#26159;&#19968;&#20010;&#22810;&#30456;&#26426;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#20855;&#26377;&#36328;&#35270;&#22270;&#27880;&#24847;&#21147;&#65288;CVA&#65289;&#22359;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BEV&#27169;&#22411;&#32534;&#30721;&#30340;&#28608;&#20809;&#38647;&#36798;BEV&#29305;&#24449;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#12290;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;GeoMIM&#30340;&#35299;&#30721;&#22120;&#20855;&#26377;&#19968;&#20010;&#35821;&#20041;&#20998;&#25903;&#65292;&#23436;&#25104;&#23494;&#38598;&#30340;&#36879;&#35270;&#22270;&#29305;&#24449;&#65292;&#24182;&#19988;&#21478;&#19968;&#20010;&#20960;&#20309;&#20998;&#25903;&#37325;&#26500;&#23494;&#38598;&#30340;&#36879;&#35270;&#35270;&#22270;&#28145;&#24230;&#22270;&#12290;&#28145;&#24230;&#20998;&#25903;&#35774;&#35745;&#20026;&#30456;&#26426;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view camera-based 3D detection is a challenging problem in computer vision. Recent works leverage a pretrained LiDAR detection model to transfer knowledge to a camera-based student network. However, we argue that there is a major domain gap between the LiDAR BEV features and the camera-based BEV features, as they have different characteristics and are derived from different sources. In this paper, we propose Geometry Enhanced Masked Image Modeling (GeoMIM) to transfer the knowledge of the LiDAR model in a pretrain-finetune paradigm for improving the multi-view camera-based 3D detection. GeoMIM is a multi-camera vision transformer with Cross-View Attention (CVA) blocks that uses LiDAR BEV features encoded by the pretrained BEV model as learning targets. During pretraining, GeoMIM's decoder has a semantic branch completing dense perspective-view features and the other geometry branch reconstructing dense perspective-view depth maps. The depth branch is designed to be camera-aware b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;&#36827;&#34892;&#20102;&#23454;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#30001;&#20110;&#21442;&#32771;&#27169;&#22411;&#25991;&#26723;&#19981;&#20840;&#12289;&#38656;&#27714;&#21464;&#21270;&#20197;&#21450;&#23454;&#29616;&#21644;&#27979;&#35797;&#25104;&#26412;&#31561;&#21407;&#22240;&#65292;&#35813;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20010;&#21035;&#24037;&#31243;&#24072;&#21487;&#33021;&#32570;&#20047;&#36719;&#20214;&#24037;&#31243;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20294;&#22242;&#38431;&#24517;&#39035;&#24212;&#29992;&#36719;&#20214;&#24037;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25165;&#33021;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2303.07476</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Challenges and Practices of Deep Learning Model Reengineering: A Case Study on Computer Vision. (arXiv:2303.07476v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;&#36827;&#34892;&#20102;&#23454;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#30001;&#20110;&#21442;&#32771;&#27169;&#22411;&#25991;&#26723;&#19981;&#20840;&#12289;&#38656;&#27714;&#21464;&#21270;&#20197;&#21450;&#23454;&#29616;&#21644;&#27979;&#35797;&#25104;&#26412;&#31561;&#21407;&#22240;&#65292;&#35813;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20010;&#21035;&#24037;&#31243;&#24072;&#21487;&#33021;&#32570;&#20047;&#36719;&#20214;&#24037;&#31243;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20294;&#22242;&#38431;&#24517;&#39035;&#24212;&#29992;&#36719;&#20214;&#24037;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25165;&#33021;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24037;&#31243;&#32452;&#32455;&#27491;&#22312;&#37325;&#26032;&#23454;&#29616;&#21644;&#25193;&#23637;&#30740;&#31350;&#30028;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#25551;&#36848;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;-&#37325;&#29992;&#65292;&#20877;&#29616;&#65292;&#35843;&#25972;&#21644;&#22686;&#24378;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;-&#30001;&#20110;&#21442;&#32771;&#27169;&#22411;&#25991;&#26723;&#19981;&#20840;&#12289;&#38656;&#27714;&#21464;&#21270;&#20197;&#21450;&#23454;&#29616;&#21644;&#27979;&#35797;&#25104;&#26412;&#31561;&#21407;&#22240;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#20010;&#21035;&#24037;&#31243;&#24072;&#21487;&#33021;&#32570;&#20047;&#36719;&#20214;&#24037;&#31243;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20294;&#22242;&#38431;&#24517;&#39035;&#24212;&#29992;&#36719;&#20214;&#24037;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25165;&#33021;&#25104;&#21151;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20174;&#8220;&#20135;&#21697;&#8221;&#35270;&#35282;&#30740;&#31350;DL&#31995;&#32479;&#65292;&#26080;&#35770;&#24037;&#31243;&#24072;&#30340;&#30446;&#30340;&#22914;&#20309;&#65292;&#37117;&#20250;&#30740;&#31350;&#39033;&#30446;&#20013;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#8220;&#36807;&#31243;&#8221;&#35270;&#35282;&#30340;&#37325;&#26500;&#27963;&#21160;&#19978;&#65292;&#19987;&#27880;&#20110;&#21442;&#19982;&#37325;&#26500;&#36807;&#31243;&#30340;&#24037;&#31243;&#24072;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#26500;&#30340;&#29305;&#28857;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Many engineering organizations are reimplementing and extending deep neural networks from the research community. We describe this process as deep learning model reengineering. Deep learning model reengineering - reusing, reproducing, adapting, and enhancing state-of-the-art deep learning approaches - is challenging for reasons including under-documented reference models, changing requirements, and the cost of implementation and testing. In addition, individual engineers may lack expertise in software engineering, yet teams must apply knowledge of software engineering and deep learning to succeed. Prior work has examined on DL systems from a "product" view, examining defects from projects regardless of the engineers' purpose. Our study is focused on reengineering activities from a "process" view, and focuses on engineers specifically engaged in the reengineering process.  Our goal is to understand the characteristics and challenges of deep learning model reengineering. We conducted a c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;</title><link>http://arxiv.org/abs/2303.00396</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning. (arXiv:2303.00396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20219;&#21153;&#65292;&#23398;&#20064;&#29305;&#23450;&#20110;&#24207;&#25968;&#20998;&#31867;&#30340;&#33391;&#22909;&#32467;&#26500;&#21270;&#29305;&#24449;&#31354;&#38388;&#26377;&#21161;&#20110;&#24688;&#24403;&#22320;&#25429;&#25417;&#31867;&#20043;&#38388;&#30340;&#24207;&#25968;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20026;&#27599;&#20010;&#24207;&#25968;&#31867;&#23398;&#20064;&#19968;&#20010;&#20195;&#29702;&#65292;&#28982;&#21518;&#36890;&#36807;&#38480;&#21046;&#36825;&#20123;&#20195;&#29702;&#26469;&#35843;&#25972;&#31867;&#30340;&#20840;&#23616;&#24067;&#23616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#30828;&#24067;&#23616;&#32422;&#26463;&#21644;&#36719;&#24067;&#23616;&#32422;&#26463;&#12290;&#30828;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#30452;&#25509;&#25511;&#21046;&#20195;&#29702;&#30340;&#29983;&#25104;&#26469;&#23454;&#29616;&#65292;&#20197;&#24378;&#21046;&#23558;&#20854;&#25918;&#32622;&#22312;&#20005;&#26684;&#30340;&#32447;&#24615;&#24067;&#23616;&#25110;&#21322;&#22278;&#24418;&#24067;&#23616;&#65288;&#21363;&#20005;&#26684;&#24207;&#25968;&#24067;&#23616;&#30340;&#20004;&#31181;&#23454;&#20363;&#65289;&#20013;&#12290;&#36719;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#26469;&#23454;&#29616;&#65292;&#35813;&#39033;&#24809;&#32602;&#20559;&#31163;&#29702;&#24819;&#24207;&#25968;&#24067;&#23616;&#30340;&#24773;&#20917;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;CPL&#26041;&#27861;&#22312;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordi
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;</title><link>http://arxiv.org/abs/2302.13268</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#38761;&#26032;&#22522;&#22240;&#32452;&#23398;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Genomics with Reinforcement Learning Techniques. (arXiv:2302.13268v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13268
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20915;&#31574;&#21644;&#22522;&#22240;&#32452;&#23398;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#30340;&#21407;&#22987;&#22522;&#22240;&#32452;&#25968;&#25454;&#25351;&#25968;&#22686;&#38271;&#24050;&#32463;&#36229;&#20986;&#20102;&#25163;&#21160;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;&#36825;&#23548;&#33268;&#23545;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;RL&#31639;&#27861;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#30417;&#30563;&#19979;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;RL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#38477;&#20302;&#20102;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36825;&#26159;&#30417;&#30563;&#23398;&#20064;&#25152;&#38656;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#26412;&#35843;&#26597;&#20165;&#19987;&#27880;&#20110;&#22312;&#21508;&#31181;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#65288;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65292;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#65289;&#20013;&#20351;&#29992;RL&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for solving a wide range of problems, including decision-making and genomics. The exponential growth of raw genomic data over the past two decades has exceeded the capacity of manual analysis, leading to a growing interest in automatic data analysis and processing. RL algorithms are capable of learning from experience with minimal human supervision, making them well-suited for genomic data analysis and interpretation. One of the key benefits of using RL is the reduced cost associated with collecting labeled training data, which is required for supervised learning. While there have been numerous studies examining the applications of Machine Learning (ML) in genomics, this survey focuses exclusively on the use of RL in various genomics research fields, including gene regulatory networks (GRNs), genome assembly, and sequence alignment. We present a comprehensive technical overview of existing studies on the applic
&lt;/p&gt;</description></item><item><title>CEMA&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#22240;&#26524;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#37319;&#26679;&#21453;&#20107;&#23454;&#19990;&#30028;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#21644;&#25490;&#21517;&#20915;&#31574;&#32972;&#21518;&#30340;&#26174;&#33879;&#21407;&#22240;&#12290;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#29983;&#25104;&#22522;&#20110;&#25152;&#36873;&#21407;&#22240;&#30340;&#23545;&#27604;&#35299;&#37322;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#24490;&#29615;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10809</link><description>&lt;p&gt;
&#38543;&#26426;&#24207;&#21015;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Causal Explanations for Stochastic Sequential Multi-Agent Decision-Making. (arXiv:2302.10809v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10809
&lt;/p&gt;
&lt;p&gt;
CEMA&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#22240;&#26524;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#37319;&#26679;&#21453;&#20107;&#23454;&#19990;&#30028;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#21644;&#25490;&#21517;&#20915;&#31574;&#32972;&#21518;&#30340;&#26174;&#33879;&#21407;&#22240;&#12290;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#29983;&#25104;&#22522;&#20110;&#25152;&#36873;&#21407;&#22240;&#30340;&#23545;&#27604;&#35299;&#37322;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#24490;&#29615;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CEMA&#65306;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#22240;&#26524;&#35299;&#37322;&#31995;&#32479;&#65307;&#29992;&#20110;&#22312;&#38543;&#26426;&#24207;&#21015;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#29983;&#25104;&#20851;&#20110;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;CEMA&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#36873;&#25321;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20043;&#21069;&#20551;&#35774;&#29305;&#23450;&#22240;&#26524;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#39044;&#27979;&#29615;&#22659;&#26410;&#26469;&#29366;&#24577;&#30340;&#27010;&#29575;&#27169;&#22411;&#21363;&#21487;&#24212;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#27169;&#22411;&#37319;&#26679;&#21453;&#20107;&#23454;&#19990;&#30028;&#65292;&#20197;&#35782;&#21035;&#21644;&#25490;&#21517;&#20915;&#31574;&#32972;&#21518;&#30340;&#26174;&#33879;&#21407;&#22240;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;CEMA&#20197;&#28385;&#36275;&#31038;&#20250;&#21487;&#35299;&#37322;AI&#30340;&#35201;&#27714;&#12290;&#23427;&#21487;&#20197;&#22522;&#20110;&#25152;&#36873;&#21407;&#22240;&#29983;&#25104;&#23545;&#27604;&#35299;&#37322;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#24490;&#29615;&#26469;&#30830;&#20445;&#23545;&#29992;&#25143;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;&#25105;&#20204;&#23558;CEMA&#23454;&#29616;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#36816;&#21160;&#35268;&#21010;&#20013;&#65292;&#24182;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#27169;&#25311;&#22330;&#26223;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;CEMA&#33021;&#22815;&#27491;&#30830;&#32780;&#19988;&#40065;&#26834;&#22320;&#35782;&#21035;&#20915;&#31574;&#32972;&#21518;&#30340;&#30456;&#20851;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#30456;&#20851;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CEMA: Causal Explanations for Multi-Agent decision-making; a system to generate causal explanations for agents' decisions in stochastic sequential multi-agent environments. The core of CEMA is a novel causal selection method which, unlike prior work that assumes a specific causal structure, is applicable whenever a probabilistic model for predicting future states of the environment is available. We sample counterfactual worlds with this model which are used to identify and rank the salient causes behind decisions. We also designed CEMA to meet the requirements of social explainable AI. It can generate contrastive explanations based on selected causes and it works as an interaction loop with users to assure relevance and intelligibility for them. We implement CEMA for motion planning for autonomous driving and test it in four diverse simulated scenarios. We show that CEMA correctly and robustly identifies the relevant causes behind decisions and delivers relevant explanations
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#21363;&#20351;&#22312;&#20002;&#24323;&#25935;&#24863;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20173;&#23384;&#22312;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#26816;&#27979;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2302.08204</link><description>&lt;p&gt;
&#22312;&#26080;&#24847;&#35782;&#20844;&#24179;&#24615;&#35774;&#32622;&#20013;&#65292;&#23545;&#20559;&#35265;&#35780;&#20272;&#21644;&#26816;&#27979;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Reasoning for Bias Evaluation and Detection in a Fairness under Unawareness setting. (arXiv:2302.08204v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#21363;&#20351;&#22312;&#20002;&#24323;&#25935;&#24863;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20173;&#23384;&#22312;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#26816;&#27979;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;AI&#27861;&#35268;&#35201;&#27714;&#22312;&#31639;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#20002;&#24323;&#25935;&#24863;&#29305;&#24449;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#23447;&#25945;&#65289;&#65292;&#20197;&#38450;&#27490;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#38598;&#20013;&#27809;&#26377;&#25935;&#24863;&#29305;&#24449;&#65292;&#31639;&#27861;&#20173;&#21487;&#33021;&#25345;&#32493;&#36827;&#34892;&#27495;&#35270;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#25935;&#24863;&#29305;&#24449;&#34987;&#24573;&#30053;&#30340;&#24773;&#20917;&#19979;&#65288;&#26080;&#24847;&#35782;&#20844;&#24179;&#24615;&#65289;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21487;&#20197;&#36890;&#36807;&#25152;&#35859;&#30340;&#20195;&#29702;&#29305;&#24449;&#25512;&#27979;&#20986;&#36825;&#20123;&#25935;&#24863;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25581;&#31034;&#21363;&#20351;&#22312;&#20002;&#24323;&#25935;&#24863;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20173;&#23384;&#22312;&#30340;&#28508;&#22312;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#21487;&#20197;&#25581;&#31034;&#20986;&#40657;&#30418;&#39044;&#27979;&#22120;&#26159;&#21542;&#20173;&#23384;&#22312;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#39044;&#27979;&#22120;&#25552;&#20379;&#36127;&#38754;&#20998;&#31867;&#32467;&#26524;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20026;&#34987;&#27495;&#35270;&#30340;&#29992;&#25143;&#31867;&#21035;&#26500;&#24314;&#21453;&#20107;&#23454;&#31034;&#20363;&#65292;&#20197;&#33719;&#24471;&#27491;&#38754;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#30456;&#21516;&#30340;&#21453;&#20107;&#23454;&#26679;&#26412;&#34987;&#29992;&#20110;&#22806;&#37096;&#20998;&#31867;&#22120;&#65288;&#38024;&#23545;&#25935;&#24863;&#29305;&#24449;&#65289;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current AI regulations require discarding sensitive features (e.g., gender, race, religion) in the algorithm's decision-making process to prevent unfair outcomes. However, even without sensitive features in the training set, algorithms can persist in discrimination. Indeed, when sensitive features are omitted (fairness under unawareness), they could be inferred through non-linear relations with the so called proxy features. In this work, we propose a way to reveal the potential hidden bias of a machine learning model that can persist even when sensitive features are discarded. This study shows that it is possible to unveil whether the black-box predictor is still biased by exploiting counterfactual reasoning. In detail, when the predictor provides a negative classification outcome, our approach first builds counterfactual examples for a discriminated user category to obtain a positive outcome. Then, the same counterfactual samples feed an external classifier (that targets a sensitive f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#30340;&#31354;&#38388;&#30456;&#20851;&#34928;&#20943;&#24615;&#36136;&#23454;&#29616;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.07938</link><description>&lt;p&gt;
&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-Agent Reinforcement Learning with General Utilities. (arXiv:2302.07938v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#30340;&#31354;&#38388;&#30456;&#20851;&#34928;&#20943;&#24615;&#36136;&#23454;&#29616;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#20854;&#20013;&#36890;&#29992;&#25928;&#29992;&#34987;&#23450;&#20041;&#20026;&#22242;&#38431;&#38271;&#26399;&#29366;&#24577;-&#21160;&#20316;&#21344;&#26377;&#29575;&#27979;&#24230;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#23616;&#37096;&#31574;&#30053;&#65292;&#26368;&#22823;&#21270;&#22242;&#38431;&#23616;&#37096;&#25928;&#29992;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#32780;&#19981;&#38656;&#35201;&#23436;&#20840;&#35266;&#27979;&#22242;&#38431;&#20013;&#30340;&#27599;&#20010;&#26234;&#33021;&#20307;&#12290;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#30340;&#31354;&#38388;&#30456;&#20851;&#34928;&#20943;&#24615;&#36136;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#38452;&#24433;&#22870;&#21169;&#20272;&#35745;&#65292;&#65288;2&#65289;&#25130;&#26029;&#38452;&#24433;Q&#20989;&#25968;&#20272;&#35745;&#65292;&#20197;&#21450;&#65288;3&#65289;&#25130;&#26029;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#21644;&#31574;&#30053;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#25910;&#25947;&#20110;$\epsilon$-&#31283;&#23450;&#24615;&#65292;&#39640;&#27010;&#29575;&#19979;&#38656;&#35201;$\widetilde{\mathcal{O}}(\epsilon^{-2})$&#20010;&#26679;&#26412;&#65292;&#30452;&#21040;&#19968;&#23450;&#31243;&#24230;&#19978;&#30340;&#36817;&#20284;&#35823;&#24046;&#20197;&#25351;&#25968;&#36895;&#24230;&#20943;&#23567;&#21040;&#36890;&#20449;&#21322;&#24452;&#20869;&#12290;&#36825;&#26159;&#20851;&#20110;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#30340;&#39318;&#20010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the scalable multi-agent reinforcement learning (MARL) with general utilities, defined as nonlinear functions of the team's long-term state-action occupancy measure. The objective is to find a localized policy that maximizes the average of the team's local utility functions without the full observability of each agent in the team. By exploiting the spatial correlation decay property of the network structure, we propose a scalable distributed policy gradient algorithm with shadow reward and localized policy that consists of three steps: (1) shadow reward estimation, (2) truncated shadow Q-function estimation, and (3) truncated policy gradient estimation and policy update. Our algorithm converges, with high probability, to $\epsilon$-stationarity with $\widetilde{\mathcal{O}}(\epsilon^{-2})$ samples up to some approximation error that decreases exponentially in the communication radius. This is the first result in the literature on multi-agent RL with general utilities that does
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;"&#27169;&#22359;&#21270;&#32487;&#20219;&#29305;&#24449;&#36924;&#36817;&#22120;"&#65288;MSFA&#65289;&#65292;&#36890;&#36807;&#35753;&#27169;&#22359;&#21457;&#29616;&#26377;&#29992;&#20110;&#39044;&#27979;&#30340;&#29305;&#24449;&#24182;&#23398;&#20064;&#33258;&#24049;&#30340;&#39044;&#27979;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.12305</link><description>&lt;p&gt;
&#29992;&#27169;&#22359;&#21270;&#30340;&#32487;&#20219;&#29305;&#24449;&#36924;&#36817;&#22120;&#32452;&#21512;&#20219;&#21153;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Composing Task Knowledge with Modular Successor Feature Approximators. (arXiv:2301.12305v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;"&#27169;&#22359;&#21270;&#32487;&#20219;&#29305;&#24449;&#36924;&#36817;&#22120;"&#65288;MSFA&#65289;&#65292;&#36890;&#36807;&#35753;&#27169;&#22359;&#21457;&#29616;&#26377;&#29992;&#20110;&#39044;&#27979;&#30340;&#29305;&#24449;&#24182;&#23398;&#20064;&#33258;&#24049;&#30340;&#39044;&#27979;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#32487;&#20219;&#29305;&#24449;&#21644;&#24191;&#20041;&#31574;&#30053;&#25913;&#36827;&#65288;SF&amp;GPI&#65289;&#26694;&#26550;&#20316;&#20026;&#23398;&#20064;&#12289;&#32452;&#21512;&#21644;&#36716;&#31227;&#39044;&#27979;&#30693;&#35782;&#21644;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;SF&amp;GPI&#36890;&#36807;&#35753;&#20195;&#29702;&#23398;&#20064;&#33021;&#22815;&#32467;&#21512;GPI&#36827;&#34892;&#20219;&#21153;&#36716;&#31227;&#30340;&#39044;&#27979;&#34920;&#31034;&#65288;SFs&#65289;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26377;&#25928;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#26377;&#29992;&#20110;&#39044;&#27979;&#30340;&#29366;&#24577;&#29305;&#24449;&#65292;&#32780;&#36825;&#20123;&#29366;&#24577;&#29305;&#24449;&#36890;&#24120;&#26159;&#25163;&#24037;&#35774;&#35745;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;"&#27169;&#22359;&#21270;&#32487;&#20219;&#29305;&#24449;&#36924;&#36817;&#22120;"&#65288;MSFA&#65289;&#65292;&#20854;&#20013;&#27169;&#22359;&#26082;&#21487;&#20197;&#21457;&#29616;&#26377;&#29992;&#20110;&#39044;&#27979;&#30340;&#29305;&#24449;&#65292;&#20063;&#21487;&#20197;&#23398;&#20064;&#33258;&#24049;&#30340;&#39044;&#27979;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MSFA&#30456;&#27604;&#22522;&#20934;&#26550;&#26500;&#26469;&#23398;&#20064;SFs&#21644;&#27169;&#22359;&#21270;&#26550;&#26500;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the Successor Features and Generalized Policy Improvement (SF&amp;GPI) framework has been proposed as a method for learning, composing, and transferring predictive knowledge and behavior. SF&amp;GPI works by having an agent learn predictive representations (SFs) that can be combined for transfer to new tasks with GPI. However, to be effective this approach requires state features that are useful to predict, and these state-features are typically hand-designed. In this work, we present a novel neural network architecture, "Modular Successor Feature Approximators" (MSFA), where modules both discover what is useful to predict, and learn their own predictive representations. We show that MSFA is able to better generalize compared to baseline architectures for learning SFs and modular architectures
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25317;&#26377;&#20016;&#23500;&#30340;&#20107;&#20214;&#30693;&#35782;&#65292;&#20960;&#20046;&#24635;&#26159;&#23558;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#27604;&#19981;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#36171;&#20104;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.01488</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#20214;&#30693;&#35782;&#65306;&#19981;&#21487;&#33021;&#24615;&#21644;&#19981;&#22826;&#21487;&#33021;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Event knowledge in large language models: the gap between the impossible and the unlikely. (arXiv:2212.01488v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01488
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25317;&#26377;&#20016;&#23500;&#30340;&#20107;&#20214;&#30693;&#35782;&#65292;&#20960;&#20046;&#24635;&#26159;&#23558;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#27604;&#19981;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#36171;&#20104;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35821;&#26009;&#24211;&#20013;&#30340;&#35789;&#20849;&#29616;&#27169;&#24335;&#21253;&#21547;&#30528;&#24847;&#24819;&#19981;&#21040;&#30340;&#27010;&#24565;&#30693;&#35782;&#12290;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#35789;&#35821;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#36825;&#20123;&#27169;&#24335;&#65292;&#22312;&#38656;&#35201;&#19990;&#30028;&#30693;&#35782;&#30340;&#21508;&#31181;&#35821;&#20041;&#20219;&#21153;&#19978;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20851;&#20110;LLMs&#30340;&#35821;&#20041;&#33021;&#21147;&#30340;&#37325;&#35201;&#20294;&#40092;&#20026;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#23427;&#20204;&#26159;&#21542;&#33719;&#24471;&#20102;&#24120;&#35265;&#20107;&#20214;&#30340;&#19968;&#33324;&#21270;&#30693;&#35782;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20116;&#20010;&#39044;&#35757;&#32451;&#30340;LLMs&#65288;&#20174;2018&#24180;&#30340;BERT&#21040;2023&#24180;&#30340;MPT&#65289;&#26159;&#21542;&#27604;&#21516;&#19968;&#20107;&#20214;&#30340;&#19981;&#22826;&#21487;&#33021;&#30340;&#29256;&#26412;&#26356;&#21487;&#33021;&#22320;&#20998;&#37197;&#32473;&#21512;&#29702;&#30340;&#20195;&#29702;-&#24739;&#32773;&#30456;&#20114;&#20316;&#29992;&#12290;&#20351;&#29992;&#19977;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#26368;&#23567;&#21477;&#23545;&#38598;&#21512;&#65288;&#24635;&#25968;n=1,215&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;LLMs&#25317;&#26377;&#30456;&#24403;&#22823;&#30340;&#20107;&#20214;&#30693;&#35782;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#20204;&#20960;&#20046;&#24635;&#26159;&#23558;&#21487;&#33021;&#20107;&#20214;&#19982;&#19981;&#21487;&#33021;&#20107;&#20214;&#30456;&#27604;&#36171;&#20104;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#65288;&#25945;&#24072;&#20080;&#20102;&#31508;&#35760;&#26412;&#30005;&#33041;&#30456;&#23545;&#20110;&#31508;&#35760;&#26412;&#30005;&#33041;&#20080;&#20102;&#25945;&#24072;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word co-occurrence patterns in language corpora contain a surprising amount of conceptual knowledge. Large language models (LLMs), trained to predict words in context, leverage these patterns to achieve impressive performance on diverse semantic tasks requiring world knowledge. An important but understudied question about LLMs' semantic abilities is whether they acquire generalized knowledge of common events. Here, we test whether five pre-trained LLMs (from 2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions of agent-patient interactions than to minimally different implausible versions of the same event. Using three curated sets of minimal sentence pairs (total n=1,215), we found that pre-trained LLMs possess substantial event knowledge, outperforming other distributional language models. In particular, they almost always assign higher likelihood to possible vs. impossible events (The teacher bought the laptop vs. The laptop bought the teacher). However, LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20110;&#21407;&#22987;&#22522;&#31449;LTE&#25968;&#25454;&#36827;&#34892;5G&#22522;&#31449;&#27969;&#37327;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.15220</link><description>&lt;p&gt;
&#38754;&#21521;5G&#22522;&#31449;&#27969;&#37327;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for 5G Base Station Traffic Forecasting. (arXiv:2211.15220v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20110;&#21407;&#22987;&#22522;&#31449;LTE&#25968;&#25454;&#36827;&#34892;5G&#22522;&#31449;&#27969;&#37327;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#21160;5G&#31227;&#21160;&#32593;&#32476;&#23454;&#29616;&#26234;&#33021;&#39640;&#25928;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#21644;&#31649;&#29702;&#30340;&#36807;&#31243;&#20013;&#65292;&#32454;&#32990;&#27969;&#37327;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#21487;&#29992;&#30340;&#25968;&#25454;&#20165;&#38480;&#20110;&#22522;&#31449;&#26085;&#24535;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#35757;&#32451;&#26041;&#27861;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#21442;&#19982;&#26041;&#30340;&#26032;&#35266;&#27979;&#20013;&#12290;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#20174;&#22810;&#20010;&#22522;&#31449;&#25910;&#38598;&#27979;&#37327;&#25968;&#25454;&#65292;&#23558;&#20854;&#20256;&#36755;&#21040;&#20013;&#22830;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#33719;&#21462;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25805;&#20316;&#12290;&#26412;&#22320;&#35266;&#27979;&#32467;&#26524;&#30340;&#20256;&#25773;&#24341;&#21457;&#20102;&#20851;&#20110;&#20445;&#23494;&#24615;&#21644;&#24615;&#33021;&#30340;&#25285;&#24551;&#65292;&#36825;&#24433;&#21709;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36866;&#29992;&#24615;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#21508;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22312;&#27969;&#37327;&#39044;&#27979;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#28982;&#24456;&#23569;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20110;&#21407;&#22987;&#22522;&#31449;LTE&#25968;&#25454;&#36827;&#34892;&#26102;&#24207;&#27969;&#37327;&#39044;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular traffic prediction is of great importance on the path of enabling 5G mobile networks to perform intelligent and efficient infrastructure planning and management. However, available data are limited to base station logging information. Hence, training methods for generating high-quality predictions that can generalize to new observations across diverse parties are in demand. Traditional approaches require collecting measurements from multiple base stations, transmitting them to a central entity and conducting machine learning operations using the acquire data. The dissemination of local observations raises concerns regarding confidentiality and performance, which impede the applicability of machine learning techniques. Although various distributed learning methods have been proposed to address this issue, their application to traffic prediction remains highly unexplored. In this work, we investigate the efficacy of federated learning applied to raw base station LTE data for tim
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#20219;&#21153;&#20013;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#29109;&#65292;&#24182;&#21457;&#29616;&#31574;&#30053;&#20248;&#21270;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20855;&#26377;&#20302;&#29109;&#31574;&#30053;&#65292;&#28982;&#32780;Q&#23398;&#20064;&#26234;&#33021;&#20307;&#23545;&#27492;&#24433;&#21709;&#36739;&#23567;&#65292;&#36890;&#24120;&#20445;&#25345;&#39640;&#29109;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.11869</link><description>&lt;p&gt;
&#30740;&#31350;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#20010;&#24615;&#21270;&#20219;&#21153;&#20013;&#30340;&#31574;&#30053;&#29109;
&lt;/p&gt;
&lt;p&gt;
Examining Policy Entropy of Reinforcement Learning Agents for Personalization Tasks. (arXiv:2211.11869v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11869
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#20219;&#21153;&#20013;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#29109;&#65292;&#24182;&#21457;&#29616;&#31574;&#30053;&#20248;&#21270;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20855;&#26377;&#20302;&#29109;&#31574;&#30053;&#65292;&#28982;&#32780;Q&#23398;&#20064;&#26234;&#33021;&#20307;&#23545;&#27492;&#24433;&#21709;&#36739;&#23567;&#65292;&#36890;&#24120;&#20445;&#25345;&#39640;&#29109;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#22312;&#20010;&#24615;&#21270;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#19981;&#21516;&#23398;&#20064;&#31639;&#27861;&#25152;&#20851;&#32852;&#30340;&#31574;&#30053;&#29109;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#31574;&#30053;&#20248;&#21270;&#26234;&#33021;&#20307;&#24448;&#24448;&#20855;&#26377;&#20302;&#29109;&#31574;&#30053;&#65292;&#23454;&#38469;&#19978;&#23548;&#33268;&#26234;&#33021;&#20307;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#21160;&#20316;&#32780;&#36991;&#20813;&#20854;&#20182;&#21160;&#20316;&#12290;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#20063;&#34920;&#26126;&#20102;Q&#23398;&#20064;&#26234;&#33021;&#20307;&#23545;&#36825;&#31181;&#34892;&#20026;&#30340;&#24433;&#21709;&#35201;&#23567;&#24471;&#22810;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#39640;&#29109;&#31574;&#30053;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#26356;&#21487;&#21462;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21508;&#31181;&#25968;&#20540;&#23454;&#39564;&#20197;&#21450;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20197;&#34920;&#26126;&#36825;&#20123;&#29109;&#24046;&#24322;&#26159;&#30001;&#25152;&#37319;&#29992;&#30340;&#23398;&#20064;&#31867;&#22411;&#25152;&#23548;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This effort is focused on examining the behavior of reinforcement learning systems in personalization environments and detailing the differences in policy entropy associated with the type of learning algorithm utilized. We demonstrate that Policy Optimization agents often possess low-entropy policies during training, which in practice results in agents prioritizing certain actions and avoiding others. Conversely, we also show that Q-Learning agents are far less susceptible to such behavior and generally maintain high-entropy policies throughout training, which is often preferable in real-world applications. We provide a wide range of numerical experiments as well as theoretical justification to show that these differences in entropy are due to the type of learning being employed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#32534;&#30721;&#22120;&#28789;&#27963;&#24615;&#12289;&#24310;&#36831;&#21644;&#33021;&#37327;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#21644;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2211.11760</link><description>&lt;p&gt;
&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Low Latency Adaptive Coding Spiking Framework for Deep Reinforcement Learning. (arXiv:2211.11760v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#32534;&#30721;&#22120;&#28789;&#27963;&#24615;&#12289;&#24310;&#36831;&#21644;&#33021;&#37327;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#21644;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20302;&#21151;&#32791;&#21644;&#20107;&#20214;&#39537;&#21160;&#29305;&#24615;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#34987;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#22266;&#23450;&#32534;&#30721;&#26041;&#27861;&#23548;&#33268;&#30340;&#33033;&#20914;&#24378;&#21270;&#23398;&#20064;&#65288;SRL&#65289;&#20173;&#28982;&#38754;&#20020;&#39640;&#24310;&#36831;&#21644;&#36739;&#24046;&#30340;&#28789;&#27963;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#30697;&#38453;&#20056;&#27861;&#23545;&#33033;&#20914;&#36827;&#34892;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#25552;&#39640;&#32534;&#30721;&#22120;&#30340;&#28789;&#27963;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#30452;&#25509;&#35757;&#32451;&#26041;&#27861;&#35757;&#32451;SNNs&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#32467;&#26500;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#25317;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24310;&#36831;&#26497;&#20302;&#65288;&#20165;&#20026;&#20854;&#20182;SRL&#26041;&#27861;&#30340;0.8%&#65289;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#33021;&#37327;&#25928;&#29575;&#65288;&#39640;&#36798;DNNs&#30340;5&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, spiking neural networks (SNNs) have been used in reinforcement learning (RL) due to their low power consumption and event-driven features. However, spiking reinforcement learning (SRL), which suffers from fixed coding methods, still faces the problems of high latency and poor versatility. In this paper, we use learnable matrix multiplication to encode and decode spikes, improving the flexibility of the coders and thus reducing latency. Meanwhile, we train the SNNs using the direct training method and use two different structures for online and offline RL algorithms, which gives our model a wider range of applications. Extensive experiments have revealed that our method achieves optimal performance with ultra-low latency (as low as 0.8% of other SRL methods) and excellent energy efficiency (up to 5X the DNNs) in different algorithms and different environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#23618;QuadConv&#65292;&#36890;&#36807;&#23450;&#31215;&#20998;&#23545;&#36830;&#32493;&#21367;&#31215;&#36827;&#34892;&#36817;&#20284;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#21387;&#32553;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#27169;&#25311;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#19982;&#26631;&#20934;&#31163;&#25955;&#21367;&#31215;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#33021;&#22815;&#22312;&#38750;&#22343;&#21248;&#25968;&#25454;&#19978;&#20445;&#25345;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;QuadConv&#36824;&#20248;&#20110;&#20854;&#20182;&#38750;&#32467;&#26500;&#21270;&#21367;&#31215;&#26041;&#27861;&#22914;&#22270;&#21367;&#31215;&#12290;</title><link>http://arxiv.org/abs/2211.05151</link><description>&lt;p&gt;
QuadConv&#65306;&#22522;&#20110;&#23450;&#31215;&#20998;&#30340;&#21367;&#31215;&#19982;&#38750;&#22343;&#21248;PDE&#25968;&#25454;&#21387;&#32553;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
QuadConv: Quadrature-Based Convolutions with Applications to Non-Uniform PDE Data Compression. (arXiv:2211.05151v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#23618;QuadConv&#65292;&#36890;&#36807;&#23450;&#31215;&#20998;&#23545;&#36830;&#32493;&#21367;&#31215;&#36827;&#34892;&#36817;&#20284;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#21387;&#32553;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#27169;&#25311;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#19982;&#26631;&#20934;&#31163;&#25955;&#21367;&#31215;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#33021;&#22815;&#22312;&#38750;&#22343;&#21248;&#25968;&#25454;&#19978;&#20445;&#25345;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;QuadConv&#36824;&#20248;&#20110;&#20854;&#20182;&#38750;&#32467;&#26500;&#21270;&#21367;&#31215;&#26041;&#27861;&#22914;&#22270;&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#21367;&#31215;&#23618;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;QuadConv&#8212;&#8212;&#36890;&#36807;&#23450;&#31215;&#20998;&#23545;&#36830;&#32493;&#21367;&#31215;&#36827;&#34892;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#25805;&#20316;&#31526;&#19987;&#38376;&#38024;&#23545;&#38750;&#22343;&#21248;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#25454;&#36827;&#34892;&#24320;&#21457;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#22312;&#20219;&#24847;&#20301;&#32622;&#36827;&#34892;&#37319;&#26679;&#30340;&#36830;&#32493;&#26680;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25805;&#20316;&#31526;&#30340;&#26500;&#24314;&#36824;&#20801;&#35768;&#39640;&#25928;&#30340;&#23454;&#29616;&#65292;&#25105;&#20204;&#22312;&#25991;&#20013;&#35814;&#32454;&#20171;&#32461;&#24182;&#26500;&#36896;&#20102;&#27492;&#23454;&#29616;&#12290;&#20316;&#20026;&#23545;&#25105;&#20204;&#30340;&#25805;&#20316;&#31526;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20174;&#22266;&#23450;&#32593;&#26684;&#20013;&#21387;&#32553;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#27169;&#25311;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;QuadConv&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;QCAE&#65289;&#19982;&#26631;&#20934;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CAE&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;QuadConv&#33021;&#22815;&#19982;&#26631;&#20934;&#31163;&#25955;&#21367;&#31215;&#22312;&#22343;&#21248;&#32593;&#26684;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;QCAE&#21363;&#20351;&#22312;&#38750;&#22343;&#21248;&#25968;&#25454;&#19978;&#20063;&#33021;&#22815;&#20445;&#25345;&#36825;&#31181;&#20934;&#30830;&#24615;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;QuadConv&#36824;&#20248;&#20110;&#22270;&#21367;&#31215;&#31561;&#20854;&#20182;&#38750;&#32467;&#26500;&#21270;&#21367;&#31215;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new convolution layer for deep learning architectures which we call QuadConv -- an approximation to continuous convolution via quadrature. Our operator is developed explicitly for use on non-uniform, mesh-based data, and accomplishes this by learning a continuous kernel that can be sampled at arbitrary locations. Moreover, the construction of our operator admits an efficient implementation which we detail and construct. As an experimental validation of our operator, we consider the task of compressing partial differential equation (PDE) simulation data from fixed meshes. We show that QuadConv can match the performance of standard discrete convolutions on uniform grid data by comparing a QuadConv autoencoder (QCAE) to a standard convolutional autoencoder (CAE). Further, we show that the QCAE can maintain this accuracy even on non-uniform data. In both cases, QuadConv also outperforms alternative unstructured convolution methods such as graph convolution.
&lt;/p&gt;</description></item><item><title>DynamicISP&#26159;&#19968;&#20010;&#21160;&#24577;&#25511;&#21046;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;&#65292;&#33021;&#22815;&#26681;&#25454;&#21069;&#19968;&#24103;&#30340;&#35782;&#21035;&#32467;&#26524;&#33258;&#21160;&#35843;&#25972;&#27599;&#24103;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#21333;&#31867;&#21035;&#21644;&#22810;&#31867;&#21035;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#21516;&#26102;&#35745;&#31639;&#25104;&#26412;&#20302;&#12290;</title><link>http://arxiv.org/abs/2211.01146</link><description>&lt;p&gt;
DynamicISP&#65306;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#30340;&#21160;&#24577;&#25511;&#21046;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
DynamicISP: Dynamically Controlled Image Signal Processor for Image Recognition. (arXiv:2211.01146v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01146
&lt;/p&gt;
&lt;p&gt;
DynamicISP&#26159;&#19968;&#20010;&#21160;&#24577;&#25511;&#21046;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;&#65292;&#33021;&#22815;&#26681;&#25454;&#21069;&#19968;&#24103;&#30340;&#35782;&#21035;&#32467;&#26524;&#33258;&#21160;&#35843;&#25972;&#27599;&#24103;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#21333;&#31867;&#21035;&#21644;&#22810;&#31867;&#21035;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#21516;&#26102;&#35745;&#31639;&#25104;&#26412;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;&#65288;ISPs&#65289;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#21644;&#25429;&#33719;&#22270;&#20687;&#30340;&#24863;&#30693;&#36136;&#37327;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#19987;&#23478;&#20204;&#20250;&#33457;&#36153;&#22823;&#37327;&#31934;&#21147;&#25163;&#21160;&#35843;&#25972;ISPs&#30340;&#35768;&#22810;&#21442;&#25968;&#65292;&#20294;&#36825;&#20123;&#21442;&#25968;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#32463;&#31215;&#26497;&#30740;&#31350;&#20102;&#20004;&#31181;&#25216;&#26415;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21442;&#25968;&#35843;&#25972;&#25216;&#26415;&#21644;&#22522;&#20110;DNN&#30340;ISP&#25216;&#26415;&#12290;&#21069;&#32773;&#36731;&#37327;&#32423;&#20294;&#32570;&#20047;&#34920;&#29616;&#21147;&#65292;&#21518;&#32773;&#20855;&#26377;&#34920;&#29616;&#21147;&#65292;&#20294;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#25104;&#26412;&#22826;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;DynamicISP&#8221;&#65292;&#23427;&#30001;&#22810;&#20010;&#20256;&#32479;&#30340;ISP&#20989;&#25968;&#32452;&#25104;&#65292;&#24182;&#26681;&#25454;&#21069;&#19968;&#24103;&#30340;&#35782;&#21035;&#32467;&#26524;&#21160;&#24577;&#25511;&#21046;&#27599;&#20010;&#24103;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25511;&#21046;&#20102;&#22810;&#20010;ISP&#20989;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#22312;&#21333;&#31867;&#21035;&#21644;&#22810;&#31867;&#21035;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#20013;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Signal Processors (ISPs) play important roles in image recognition tasks as well as in the perceptual quality of captured images. In most cases, experts make a lot of effort to manually tune many parameters of ISPs, but the parameters are sub-optimal. In the literature, two types of techniques have been actively studied: a machine learning-based parameter tuning technique and a DNN-based ISP technique. The former is lightweight but lacks expressive power. The latter has expressive power, but the computational cost is too heavy on edge devices. To solve these problems, we propose "DynamicISP," which consists of multiple classical ISP functions and dynamically controls the parameters of each frame according to the recognition result of the previous frame. We show our method successfully controls the parameters of multiple ISP functions and achieves state-of-the-art accuracy with low computational cost in single and multi-category object detection tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2211.00646</link><description>&lt;p&gt;
&#20174;&#30456;&#37051;&#30340;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#23398;&#20064;&#40657;&#33394;&#32032;&#32454;&#32990;&#25513;&#33180;
&lt;/p&gt;
&lt;p&gt;
Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#33394;&#32032;&#30244;&#26159;&#26368;&#20855;&#20405;&#34989;&#24615;&#30340;&#30382;&#32932;&#30284;&#20043;&#19968;&#65292;&#23548;&#33268;&#22823;&#37096;&#20998;&#30382;&#32932;&#30284;&#27515;&#20129;&#12290;&#28982;&#32780;&#65292;&#30149;&#29702;&#23398;&#23478;&#23545;&#40657;&#33394;&#32032;&#30244;&#30340;&#35786;&#26029;&#21487;&#38752;&#24615;&#36739;&#20302;&#12290;&#30001;&#20110;&#40657;&#33394;&#32032;&#30244;&#26159;&#40657;&#33394;&#32032;&#32454;&#32990;&#30340;&#32959;&#30244;&#65292;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#19982;&#30149;&#29702;&#23398;&#23478;&#30340;&#24046;&#24322;&#26080;&#20851;&#24182;&#33021;&#33258;&#21160;&#36827;&#34892;&#20687;&#32032;&#32423;&#27880;&#37322;&#30340;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#30149;&#29702;&#23398;&#23478;&#26631;&#27880;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#37051;&#36817;&#32452;&#32455;&#20999;&#29255;&#19978;&#30340;&#20598;&#32852;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#26579;&#33394;&#29255;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#65292;&#34429;&#28982;&#24456;&#38590;&#26377;&#23436;&#32654;&#30340;&#26631;&#31614;&#65292;&#20294;&#36798;&#21040;&#20102;0.64&#30340;&#24179;&#22343;IOU&#12290;
&lt;/p&gt;
&lt;p&gt;
Melanoma is one of the most aggressive forms of skin cancer, causing a large proportion of skin cancer deaths. However, melanoma diagnoses by pathologists shows low interrater reliability. As melanoma is a cancer of the melanocyte, there is a clear need to develop a melanocytic cell segmentation tool that is agnostic to pathologist variability and automates pixel-level annotation. Gigapixel-level pathologist labeling, however, is impractical. Herein, we propose a means to train deep neural networks for melanocytic cell segmentation from hematoxylin and eosin (H&amp;E) stained slides using paired immunohistochemical (IHC) slides of adjacent tissue sections, achieving a mean IOU of 0.64 despite imperfect ground-truth labels.
&lt;/p&gt;</description></item><item><title>TuneUp&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#23614;&#33410;&#28857;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.14843</link><description>&lt;p&gt;
TuneUp:&#19968;&#31181;&#31616;&#21333;&#30340;&#25913;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
TuneUp: A Simple Improved Training Strategy for Graph Neural Networks. (arXiv:2210.14843v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14843
&lt;/p&gt;
&lt;p&gt;
TuneUp&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#23614;&#33410;&#28857;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#36817;&#26399;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#31574;&#30053;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20256;&#32479;&#30340;&#35757;&#32451;&#31574;&#30053;&#23545;&#21407;&#22987;&#22270;&#20013;&#30340;&#25152;&#26377;&#33410;&#28857;&#36827;&#34892;&#24179;&#31561;&#23398;&#20064;&#65292;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#26576;&#20123;&#33410;&#28857;&#24448;&#24448;&#27604;&#20854;&#20182;&#33410;&#28857;&#26356;&#38590;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TuneUp&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#25552;&#39640;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;TuneUp&#23558;GNN&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;TuneUp&#24212;&#29992;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#33719;&#24471;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#30784;GNN&#12290;&#22522;&#30784;GNN&#22312;&#22836;&#33410;&#28857;&#65288;&#20855;&#26377;&#22823;&#24230;&#25968;&#30340;&#33410;&#28857;&#65289;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#23614;&#33410;&#28857;&#65288;&#20855;&#26377;&#23567;&#24230;&#25968;&#30340;&#33410;&#28857;&#65289;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;TuneUp&#30340;&#31532;&#20108;&#38454;&#27573;&#20391;&#37325;&#20110;&#36890;&#36807;&#36827;&#19968;&#27493;&#35757;&#32451;&#22522;&#30784;GNN&#20197;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#23614;&#33410;&#28857;&#19978;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;TuneUp&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#25913;&#21892;&#23614;&#33410;&#28857;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;TuneUp&#23454;&#29616;&#31616;&#21333;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances in Graph Neural Networks (GNNs), their training strategies remain largely under-explored. The conventional training strategy learns over all nodes in the original graph(s) equally, which can be sub-optimal as certain nodes are often more difficult to learn than others. Here we present TuneUp, a simple curriculum-based training strategy for improving the predictive performance of GNNs. TuneUp trains a GNN in two stages. In the first stage, TuneUp applies conventional training to obtain a strong base GNN. The base GNN tends to perform well on head nodes (nodes with large degrees) but less so on tail nodes (nodes with small degrees). Therefore, the second stage of TuneUp focuses on improving prediction on the difficult tail nodes by further training the base GNN on synthetically generated tail node data. We theoretically analyze TuneUp and show it provably improves generalization performance on tail nodes. TuneUp is simple to implement and applicable to a broad ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.13533</link><description>&lt;p&gt;
&#20998;&#24067;&#36716;&#31227;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sufficient Invariant Learning for Distribution Shift. (arXiv:2210.13533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#35777;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36328;&#32452;&#25110;&#39046;&#22495;&#30340;&#19981;&#21464;&#29305;&#24449;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#37096;&#20998;&#22320;&#23398;&#20064;&#20102;&#19981;&#21464;&#29305;&#24449;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#26377;&#38480;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#20294;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#21482;&#26377;&#35757;&#32451;&#38598;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#23384;&#22312;&#20110;&#20998;&#24067;&#36716;&#31227;&#26102;&#30340;&#27979;&#35797;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#25552;&#39640;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35748;&#20026;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#20805;&#20998;&#30340;&#19981;&#21464;&#29305;&#24449;&#23545;&#20110;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have shown remarkable performance in diverse applications. However, it is still challenging to guarantee performance in distribution shifts when distributions of training and test datasets are different. There have been several approaches to improve the performance in distribution shift cases by learning invariant features across groups or domains. However, we observe that the previous works only learn invariant features partially. While the prior works focus on the limited invariant features, we first raise the importance of the sufficient invariant features. Since only training sets are given empirically, the learned partial invariant features from training sets might not be present in the test sets under distribution shift. Therefore, the performance improvement on distribution shifts might be limited. In this paper, we argue that learning sufficient invariant features from the training set is crucial for the distribution shift case. Concretely, we newly 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35270;&#35282;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#33258;&#19978;&#32780;&#19979;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#25512;&#29702;&#35270;&#35282;&#65292;&#20197;&#21450;&#22810;&#31181;&#31561;&#20215;&#30340;&#26041;&#31243;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#26356;&#23436;&#25972;&#30340;&#35821;&#20041;&#21040;&#26041;&#31243;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2210.11694</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#25512;&#29702;&#65306;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25968;&#23398;&#24212;&#29992;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem. (arXiv:2210.11694v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35270;&#35282;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#33258;&#19978;&#32780;&#19979;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#25512;&#29702;&#35270;&#35282;&#65292;&#20197;&#21450;&#22810;&#31181;&#31561;&#20215;&#30340;&#26041;&#31243;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#26356;&#23436;&#25972;&#30340;&#35821;&#20041;&#21040;&#26041;&#31243;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#24212;&#29992;&#38382;&#39064;&#27714;&#35299;&#22120;&#38656;&#35201;&#23545;&#25991;&#26412;&#20013;&#30340;&#25968;&#37327;&#36827;&#34892;&#31934;&#30830;&#30340;&#20851;&#31995;&#25512;&#29702;&#21644;&#21487;&#38752;&#30340;&#26041;&#31243;&#29983;&#25104;&#12290;&#24403;&#21069;&#30340;&#24207;&#21015;&#21040;&#26641;&#25110;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#21482;&#20174;&#19968;&#20010;&#22266;&#23450;&#35270;&#35282;&#30475;&#24453;&#36825;&#20010;&#38382;&#39064;&#65292;&#24456;&#38590;&#21516;&#26102;&#22788;&#29702;&#22797;&#26434;&#30340;&#35821;&#20041;&#21644;&#22810;&#26679;&#30340;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#35299;&#39064;&#33258;&#28982;&#22320;&#28041;&#21450;&#20004;&#31181;&#19968;&#33268;&#30340;&#25512;&#29702;&#35270;&#35282;&#65306;&#33258;&#19978;&#32780;&#19979;&#21644;&#33258;&#19979;&#32780;&#19978;&#65292;&#23601;&#20687;&#25968;&#23398;&#26041;&#31243;&#20063;&#21487;&#20197;&#29992;&#22810;&#31181;&#31561;&#20215;&#24418;&#24335;&#34920;&#31034;&#65306;&#21069;&#24207;&#21644;&#21518;&#24207;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#29992;&#20110;&#26356;&#23436;&#25972;&#30340;&#35821;&#20041;&#21040;&#26041;&#31243;&#30340;&#26144;&#23556;&#12290;&#25972;&#20010;&#36807;&#31243;&#34987;&#20998;&#35299;&#20026;&#20004;&#20010;&#29420;&#31435;&#20294;&#19968;&#33268;&#30340;&#35270;&#35282;&#65306;&#33258;&#19978;&#32780;&#19979;&#30340;&#20998;&#35299;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#26500;&#24314;&#65292;&#24182;&#19988;&#20004;&#31181;&#25512;&#29702;&#35270;&#35282;&#22312;&#22810;&#31890;&#24230;&#19978;&#23545;&#40784;&#20197;&#20445;&#25345;&#19968;&#33268;&#24615;&#65292;&#22686;&#24378;&#20840;&#23616;&#29983;&#25104;&#21644;&#31934;&#30830;&#25512;&#29702;&#12290;&#22312;&#20004;&#31181;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Math word problem solver requires both precise relation reasoning about quantities in the text and reliable generation for the diverse equation. Current sequence-to-tree or relation extraction methods regard this only from a fixed view, struggling to simultaneously handle complex semantics and diverse equations. However, human solving naturally involves two consistent reasoning views: top-down and bottom-up, just as math equations also can be expressed in multiple equivalent forms: pre-order and post-order. We propose a multi-view consistent contrastive learning for a more complete semantics-to-equation mapping. The entire process is decoupled into two independent but consistent views: top-down decomposition and bottom-up construction, and the two reasoning views are aligned in multi-granularity for consistency, enhancing global generation and precise reasoning. Experiments on multiple datasets across two languages show our approach significantly outperforms the existing baselines, esp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27491;&#24335;&#21270;&#20102;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#26368;&#20339;&#36873;&#25321;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;BAE&#65289;&#65292;BAE&#33021;&#22815;&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#26368;&#20248;&#30340;&#27969;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#21387;&#32553;&#21644;&#26356;&#21152;&#31283;&#23450;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2210.00637</link><description>&lt;p&gt;
&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Benign Autoencoders. (arXiv:2210.00637v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27491;&#24335;&#21270;&#20102;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#26368;&#20339;&#36873;&#25321;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;BAE&#65289;&#65292;BAE&#33021;&#22815;&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#26368;&#20248;&#30340;&#27969;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#21387;&#32553;&#21644;&#26356;&#21152;&#31283;&#23450;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20854;&#20013;&#24120;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#23454;&#29616;&#25968;&#25454;&#30340;&#39640;&#25928;&#34920;&#31034;&#12290;&#26412;&#35770;&#25991;&#27491;&#24335;&#21270;&#20102;&#23547;&#25214;&#26368;&#20339;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#25968;&#23398;&#38382;&#39064;&#24182;&#34920;&#24449;&#20854;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#8220;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#8221;&#65288;BAE&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;BAE&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#19968;&#20010;&#27969;&#22411;&#19978;&#65292;&#20854;&#32500;&#25968;&#20026;&#29983;&#25104;&#38382;&#39064;&#30340;&#26368;&#20339;&#21487;&#21387;&#32553;&#32500;&#24230;&#12290;&#25105;&#20204;&#24378;&#35843;BAE&#19982;&#20154;&#24037;&#26234;&#33021;&#20013;&#20960;&#20010;&#26368;&#36817;&#21457;&#23637;&#30340;&#26041;&#21521;&#20043;&#38388;&#30340;&#24778;&#20154;&#32852;&#31995;&#65292;&#22914;&#26377;&#26465;&#20214;&#30340;GAN&#65292;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#65292;&#31283;&#23450;&#25193;&#25955;&#65292;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BAE&#22914;&#20309;&#25214;&#21040;&#26368;&#20248;&#30340;&#20302;&#32500;&#28508;&#22312;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#25552;&#39640;&#37492;&#21035;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21387;&#32553;&#8220;&#24694;&#24615;&#8221;&#25968;&#25454;&#32500;&#24230;&#65292;BAE&#23548;&#33268;&#26799;&#24230;&#26356;&#21152;&#24179;&#28369;&#21644;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Generative Artificial Intelligence (AI) relies on efficient data representations, often featuring encoder-decoder architectures. We formalize the mathematical problem of finding the optimal encoder-decoder pair and characterize its solution, which we name the "benign autoencoder" (BAE). We prove that BAE projects data onto a manifold whose dimension is the optimal compressibility dimension of the generative problem. We highlight surprising connections between BAE and several recent developments in AI, such as conditional GANs, context encoders, stable diffusion, stacked autoencoders, and the learning capabilities of generative models. As an illustration, we show how BAE can find optimal, low-dimensional latent representations that improve the performance of a discriminator under a distribution shift. By compressing "malignant" data dimensions, BAE leads to smoother and more stable gradients.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#34701;&#20837;&#21040;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#35774;&#35745;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#32452;&#20214;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#25193;&#23637;XAI&#38382;&#39064;&#24211;&#24182;&#25552;&#20379;&#35299;&#37322;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30495;&#27491;&#33258;&#28982;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2209.02552</link><description>&lt;p&gt;
&#33258;&#28982;&#23545;&#35805;&#20013;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#36208;&#21521;&#23545;&#35805;&#24335;XAI&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explaining Machine Learning Models in Natural Conversations: Towards a Conversational XAI Agent. (arXiv:2209.02552v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#34701;&#20837;&#21040;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#35774;&#35745;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#32452;&#20214;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#25193;&#23637;XAI&#38382;&#39064;&#24211;&#24182;&#25552;&#20379;&#35299;&#37322;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30495;&#27491;&#33258;&#28982;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#26041;&#27861;&#26469;&#25581;&#31034;&#40657;&#30418;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20197;&#20415;&#21521;&#20154;&#31867;&#35299;&#37322;&#12290;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#25351;&#20986;&#65292;&#36825;&#26679;&#30340;&#35299;&#37322;&#24212;&#35813;&#26159;&#23545;&#35805;&#24335;&#30340;&#65292;&#31867;&#20284;&#20110;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;XAI&#34701;&#20837;&#21040;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#32452;&#20214;&#30340;&#26631;&#20934;&#35774;&#35745;&#12290;&#25105;&#20204;&#26681;&#25454;&#36136;&#25511;&#30340;&#37322;&#20041;&#37325;&#36848;&#25193;&#23637;&#20102;&#19968;&#20010;XAI&#38382;&#39064;&#24211;&#65292;&#20197;&#29702;&#35299;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36866;&#21512;&#25552;&#20379;&#31572;&#26696;&#20449;&#24687;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#25991;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24314;&#35758;&#21015;&#34920;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#23454;&#29616;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30495;&#27491;&#33258;&#28982;&#23545;&#35805;&#30340;&#31532;&#19968;&#27493;&#65292;&#19982;&#19968;&#20010;&#35299;&#37322;&#20195;&#29702;&#26377;&#20851;&#30340;&#20840;&#38754;&#30340;XAI&#38382;&#39064;&#21015;&#34920;&#21644;&#30456;&#24212;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Explainable AI (XAI) is to design methods to provide insights into the reasoning process of black-box models, such as deep neural networks, in order to explain them to humans. Social science research states that such explanations should be conversational, similar to human-to-human explanations. In this work, we show how to incorporate XAI in a conversational agent, using a standard design for the agent comprising natural language understanding and generation components. We build upon an XAI question bank which we extend by quality-controlled paraphrases to understand the user's information needs. We further systematically survey the literature for suitable explanation methods that provide the information to answer those questions, and present a comprehensive list of suggestions. Our work is the first step towards truly natural conversations about machine learning models with an explanation agent. The comprehensive list of XAI questions and the corresponding explanation meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#23569;&#25968;&#31867;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TE2Rules&#26041;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#20934;&#30830;&#24615;&#36739;&#39640;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2206.14359</link><description>&lt;p&gt;
TE2Rules: &#20351;&#29992;&#35268;&#21017;&#35299;&#37322;&#26641;&#38598;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TE2Rules: Explaining Tree Ensembles using Rules. (arXiv:2206.14359v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#23569;&#25968;&#31867;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TE2Rules&#26041;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#20934;&#30830;&#24615;&#36739;&#39640;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#38598;&#21512;&#65288;&#22914;&#26799;&#24230;&#25552;&#21319;&#26641;&#65289;&#36890;&#24120;&#30456;&#27604;&#21333;&#26869;&#20915;&#31574;&#26641;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#26641;&#38598;&#21512;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#38590;&#20197;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#36923;&#36753;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#25509;&#36817;&#26641;&#38598;&#21512;&#30340;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#23569;&#25968;&#31867;&#21035;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TE2Rules&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;TE2Rules&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#31867;&#20284;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#65292;TE2Rules&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#21487;&#20197;&#20197;&#31245;&#24494;&#38477;&#20302;&#30340;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tree Ensemble (TE) models (like Gradient Boosted Trees) often provide higher prediction performance compared to single decision trees. However, TE models generally lack transparency and interpretability, as humans have difficulty understanding their decision logic. This paper presents a novel approach to convert a TE trained for a binary classification task, to a rule list (RL) that closely approximates the TE and is interpretable for a human. This RL can effectively explain the model even on the minority class predicted by the model. Experiments on benchmark datasets demonstrate that, (i) predictions from the RL generated by TE2Rules have higher fidelity (with respect to the original TE) compared to state-of-the-art methods, (ii) the run-time of TE2Rules is comparable to that of some other similar baselines and (iii) the run-time of TE2Rules algorithm can be traded off at the cost of a slightly lower fidelity.
&lt;/p&gt;</description></item><item><title>ReCo&#26159;&#19968;&#20010;&#29992;&#20110;&#20303;&#23429;&#31038;&#21306;&#24067;&#23616;&#35268;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.04678</link><description>&lt;p&gt;
ReCo: &#19968;&#31181;&#29992;&#20110;&#20303;&#23429;&#31038;&#21306;&#24067;&#23616;&#35268;&#21010;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ReCo: A Dataset for Residential Community Layout Planning. (arXiv:2206.04678v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04678
&lt;/p&gt;
&lt;p&gt;
ReCo&#26159;&#19968;&#20010;&#29992;&#20110;&#20303;&#23429;&#31038;&#21306;&#24067;&#23616;&#35268;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23616;&#35268;&#21010;&#22312;&#24314;&#31569;&#21644;&#22478;&#24066;&#35774;&#35745;&#39046;&#22495;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#25215;&#36733;&#22478;&#24066;&#21151;&#33021;&#30340;&#21508;&#31181;&#22522;&#26412;&#21333;&#20301;&#20013;&#65292;&#20303;&#23429;&#31038;&#21306;&#23545;&#25903;&#25345;&#20154;&#31867;&#29983;&#27963;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#20303;&#23429;&#31038;&#21306;&#30340;&#24067;&#23616;&#35268;&#21010;&#19968;&#30452;&#21463;&#21040;&#20851;&#27880;&#65292;&#24182;&#33258;&#28145;&#24230;&#23398;&#20064;&#38382;&#19990;&#20197;&#26469;&#65292;&#23588;&#20854;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#28145;&#24230;&#23398;&#20064;&#26377;&#21161;&#20110;&#33258;&#21160;&#24067;&#23616;&#29983;&#25104;&#21644;&#31354;&#38388;&#27169;&#24335;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#39046;&#22495;&#26222;&#36941;&#38754;&#20020;&#20303;&#23429;&#31038;&#21306;&#24067;&#23616;&#22522;&#20934;&#25110;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#36825;&#38459;&#30861;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20303;&#23429;&#31038;&#21306;&#24067;&#23616;&#35268;&#21010;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#20027;&#35201;&#26159;&#30001;&#20110;&#22823;&#35268;&#27169;&#23454;&#38469;&#20303;&#23429;&#25968;&#25454;&#37319;&#38598;&#21644;&#38271;&#26399;&#19987;&#23478;&#31579;&#36873;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25512;&#36827;&#26234;&#24935;&#22478;&#24066;&#21457;&#23637;&#20013;&#21508;&#31181;&#26234;&#33021;&#31354;&#38388;&#35774;&#35745;&#21644;&#20998;&#26512;&#24212;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ReCo&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layout planning is centrally important in the field of architecture and urban design. Among the various basic units carrying urban functions, residential community plays a vital part for supporting human life. Therefore, the layout planning of residential community has always been of concern, and has attracted particular attention since the advent of deep learning that facilitates the automated layout generation and spatial pattern recognition. However, the research circles generally suffer from the insufficiency of residential community layout benchmark or high-quality datasets, which hampers the future exploration of data-driven methods for residential community layout planning. The lack of datasets is largely due to the difficulties of large-scale real-world residential data acquisition and long-term expert screening. In order to address the issues and advance a benchmark dataset for various intelligent spatial design and analysis applications in the development of smart city, we in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Res2Net&#30340;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#34701;&#21512;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.14540</link><description>&lt;p&gt;
Res2NetFuse&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#30340;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Res2NetFuse: A Fusion Method for Infrared and Visible Images. (arXiv:2112.14540v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Res2Net&#30340;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#34701;&#21512;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Res2Net&#30340;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#12290;&#25552;&#20986;&#30340;&#34701;&#21512;&#27169;&#22411;&#21253;&#25324;&#32534;&#30721;&#22120;&#12289;&#34701;&#21512;&#23618;&#21644;&#35299;&#30721;&#22120;&#19977;&#20010;&#37096;&#20998;&#12290;&#21033;&#29992;&#22522;&#20110;Res2Net&#30340;&#32534;&#30721;&#22120;&#25552;&#21462;&#28304;&#22270;&#20687;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20165;&#20351;&#29992;&#21333;&#20010;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#34701;&#21512;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35299;&#30721;&#22120;&#37325;&#26500;&#34701;&#21512;&#22270;&#20687;&#12290;&#26412;&#25991;&#36824;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#37117;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel Res2Net-based fusion framework for infrared and visible images. The proposed fusion model has three parts: an encoder, a fusion layer and a decoder, respectively. The Res2Net-based encoder is used to extract multi-scale features of source images, the paper introducing a new training strategy for training a Res2Net-based encoder that uses only a single image. Then, a new fusion strategy is developed based on the attention model. Finally, the fused image is reconstructed by the decoder. The proposed approach is also analyzed in detail. Experiments show that our method achieves state-of-the-art fusion performance in objective and subjective assessment by comparing with the existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#22495;&#23383;&#31526;&#36317;&#31163;&#24863;&#30693;&#65288;MDCDP&#65289;&#30340;&#26032;&#39062;&#27169;&#22359;&#65292;&#29992;&#20110;&#35299;&#20915;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#20013;&#29305;&#24449;&#21644;&#23383;&#31526;&#23545;&#40784;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22359;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#20869;&#23481;&#24863;&#30693;&#23884;&#20837;&#26469;&#24863;&#30693;&#23383;&#31526;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2111.11011</link><description>&lt;p&gt;
CDistNet&#65306;&#24863;&#30693;&#22810;&#22495;&#23383;&#31526;&#36317;&#31163;&#30340;&#40065;&#26834;&#25991;&#26412;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition. (arXiv:2111.11011v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.11011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#22495;&#23383;&#31526;&#36317;&#31163;&#24863;&#30693;&#65288;MDCDP&#65289;&#30340;&#26032;&#39062;&#27169;&#22359;&#65292;&#29992;&#20110;&#35299;&#20915;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#20013;&#29305;&#24449;&#21644;&#23383;&#31526;&#23545;&#40784;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22359;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#20869;&#23481;&#24863;&#30693;&#23884;&#20837;&#26469;&#24863;&#30693;&#23383;&#31526;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#32534;&#30721;-&#35299;&#30721;&#26694;&#26550;&#22312;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#33021;&#22815;&#33258;&#28982;&#22320;&#25972;&#21512;&#26469;&#33258;&#35270;&#35273;&#21644;&#35821;&#20041;&#39046;&#22495;&#30340;&#35782;&#21035;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20004;&#31181;&#32447;&#32034;&#24182;&#19981;&#24635;&#26159;&#24456;&#22909;&#22320;&#27880;&#20876;&#65292;&#22240;&#27492;&#22312;&#22256;&#38590;&#25991;&#26412;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#32597;&#35265;&#24418;&#29366;&#30340;&#25991;&#26412;&#65289;&#20013;&#65292;&#29305;&#24449;&#21644;&#23383;&#31526;&#21487;&#33021;&#19981;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#20102;&#23383;&#31526;&#20301;&#32622;&#31561;&#32422;&#26463;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#23613;&#31649;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#35270;&#35273;&#21644;&#35821;&#20041;&#20173;&#28982;&#26159;&#20998;&#21035;&#24314;&#27169;&#30340;&#65292;&#23427;&#20204;&#20043;&#38388;&#21482;&#26159;&#26494;&#25955;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22359;&#65292;&#31216;&#20026;&#22810;&#22495;&#23383;&#31526;&#36317;&#31163;&#24863;&#30693;&#65288;MDCDP&#65289;&#65292;&#29992;&#20110;&#24314;&#31435;&#19968;&#20010;&#35270;&#35273;&#19978;&#21644;&#35821;&#20041;&#19978;&#30456;&#20851;&#30340;&#20301;&#32622;&#23884;&#20837;&#12290;MDCDP&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#26597;&#35810;&#35270;&#35273;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#36825;&#20004;&#31181;&#32447;&#32034;&#34987;&#34701;&#21512;&#21040;&#20301;&#32622;&#20998;&#25903;&#20013;&#65292;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#24456;&#22909;&#22320;&#24863;&#30693;&#23383;&#31526;&#30340;&#20869;&#23481;&#24863;&#30693;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Transformer-based encoder-decoder framework is becoming popular in scene text recognition, largely because it naturally integrates recognition clues from both visual and semantic domains. However, recent studies show that the two kinds of clues are not always well registered and therefore, feature and character might be misaligned in difficult text (e.g., with a rare shape). As a result, constraints such as character position are introduced to alleviate this problem. Despite certain success, visual and semantic are still separately modeled and they are merely loosely associated. In this paper, we propose a novel module called Multi-Domain Character Distance Perception (MDCDP) to establish a visually and semantically related position embedding. MDCDP uses the position embedding to query both visual and semantic features following the cross-attention mechanism. The two kinds of clues are fused into the position branch, generating a content-aware embedding that well perceives characte
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#32593;&#32476;&#12289;&#28216;&#25103;&#21644;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#29702;&#35299;&#32593;&#32476;&#19978;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#36873;&#25321;&#24615;&#30340;&#21338;&#24328;&#29702;&#35770;&#23398;&#20064;&#31639;&#27861;&#27010;&#36848;&#21644;&#22312;&#29616;&#20195;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2105.08158</link><description>&lt;p&gt;
&#32593;&#32476;&#12289;&#28216;&#25103;&#21644;&#23398;&#20064;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
The Confluence of Networks, Games and Learning. (arXiv:2105.08158v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.08158
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#32593;&#32476;&#12289;&#28216;&#25103;&#21644;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#29702;&#35299;&#32593;&#32476;&#19978;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#36873;&#25321;&#24615;&#30340;&#21338;&#24328;&#29702;&#35770;&#23398;&#20064;&#31639;&#27861;&#27010;&#36848;&#21644;&#22312;&#29616;&#20195;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29616;&#20195;&#32593;&#32476;&#24212;&#29992;&#30340;&#25216;&#26415;&#21644;&#26381;&#21153;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#26234;&#33021;&#30005;&#32593;&#31649;&#29702;&#12289;&#26080;&#32447;&#36890;&#20449;&#12289;&#32593;&#32476;&#23433;&#20840;&#20197;&#21450;&#22810;&#26234;&#33021;&#20307;&#33258;&#20027;&#31995;&#32479;&#12290;&#32771;&#34385;&#21040;&#32593;&#32476;&#23454;&#20307;&#30340;&#24322;&#26500;&#24615;&#65292;&#26032;&#20852;&#32593;&#32476;&#24212;&#29992;&#38656;&#35201;&#21338;&#24328;&#29702;&#35770;&#27169;&#22411;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21019;&#24314;&#23545;&#21160;&#24577;&#25110;&#23545;&#25239;&#29615;&#22659;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#24178;&#25200;&#20316;&#20986;&#21709;&#24212;&#30340;&#20998;&#24067;&#24335;&#32593;&#32476;&#26234;&#33021;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#32593;&#32476;&#12289;&#28216;&#25103;&#21644;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#29702;&#35299;&#32593;&#32476;&#19978;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#22880;&#23450;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#36924;&#36817;&#29702;&#35770;&#30340;&#26694;&#26550;&#20869;&#25552;&#20379;&#20102;&#21338;&#24328;&#29702;&#35770;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#24615;&#27010;&#36848;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#29616;&#20195;&#32593;&#32476;&#31995;&#32479;&#30340;&#19968;&#20123;&#20195;&#34920;&#24615;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#19979;&#19968;&#20195;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#12289;&#26234;&#33021;&#30005;&#32593;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed significant advances in technologies and services in modern network applications, including smart grid management, wireless communication, cybersecurity as well as multi-agent autonomous systems. Considering the heterogeneous nature of networked entities, emerging network applications call for game-theoretic models and learning-based approaches in order to create distributed network intelligence that responds to uncertainties and disruptions in a dynamic or an adversarial environment. This paper articulates the confluence of networks, games and learning, which establishes a theoretical underpinning for understanding multi-agent decision-making over networks. We provide an selective overview of game-theoretic learning algorithms within the framework of stochastic approximation theory, and associated applications in some representative contexts of modern network systems, such as the next generation wireless communication networks, the smart grid and distribute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#30340;&#29616;&#26377;&#20316;&#21697;&#65292;&#20171;&#32461;&#20102;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#30340;&#32972;&#26223;&#12289;&#22522;&#20934;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#27969;&#34892;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/1901.07474</link><description>&lt;p&gt;
&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;: &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Pedestrian Attribute Recognition: A Survey. (arXiv:1901.07474v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.07474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#30340;&#29616;&#26377;&#20316;&#21697;&#65292;&#20171;&#32461;&#20102;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#30340;&#32972;&#26223;&#12289;&#22522;&#20934;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#27969;&#34892;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#35270;&#39057;&#30417;&#25511;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#35782;&#21035;&#34892;&#20154;&#23646;&#24615;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#35768;&#22810;&#31639;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#22238;&#39038;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#25110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#29616;&#26377;&#20316;&#21697;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#30340;&#32972;&#26223;&#65292;&#21253;&#25324;&#34892;&#20154;&#23646;&#24615;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#30456;&#24212;&#30340;&#25361;&#25112;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#29616;&#26377;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#35299;&#37322;&#20102;&#36825;&#20004;&#31181;&#23398;&#20064;&#31639;&#27861;&#19982;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#20123;&#27969;&#34892;&#32593;&#32476;&#26550;&#26500;&#12290;&#31532;&#22235;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27492;&#20219;&#21153;&#30340;&#27969;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#23646;&#24615;&#32452;&#21644;&#22522;&#20110;&#37096;&#20998;&#30340;&#26041;&#27861;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing pedestrian attributes is an important task in the computer vision community due to it plays an important role in video surveillance. Many algorithms have been proposed to handle this task. The goal of this paper is to review existing works using traditional methods or based on deep learning networks. Firstly, we introduce the background of pedestrian attribute recognition (PAR, for short), including the fundamental concepts of pedestrian attributes and corresponding challenges. Secondly, we introduce existing benchmarks, including popular datasets and evaluation criteria. Thirdly, we analyze the concept of multi-task learning and multi-label learning and also explain the relations between these two learning algorithms and pedestrian attribute recognition. We also review some popular network architectures which have been widely applied in the deep learning community. Fourthly, we analyze popular solutions for this task, such as attributes group, part-based, etc. Fifthly, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/1710.05468</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Generalization in Deep Learning. (arXiv:1710.05468v8 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1710.05468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#22238;&#24212;&#20102;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25552;&#20379;&#28145;&#24230;&#23398;&#20064;&#38750;&#34394;&#31354;&#27867;&#21270;&#20445;&#35777;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#29702;&#35770;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.
&lt;/p&gt;</description></item></channel></rss>