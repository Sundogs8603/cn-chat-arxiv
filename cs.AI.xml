<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24494;&#22609;&#26009;&#24223;&#24323;&#29289;&#30340;&#36793;&#32536;&#26816;&#27979;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#37319;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#38468;&#20214;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#25163;&#21160;&#20998;&#25315;&#25805;&#20316;&#21644;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.16289</link><description>&lt;p&gt;
&#22312;&#24494;&#22609;&#26009;&#24223;&#24323;&#29289;&#30340;&#36793;&#32536;&#26816;&#27979;&#23454;&#29616;&#20013;
&lt;/p&gt;
&lt;p&gt;
Implementing Edge Based Object Detection For Microplastic Debris. (arXiv:2307.16289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24494;&#22609;&#26009;&#24223;&#24323;&#29289;&#30340;&#36793;&#32536;&#26816;&#27979;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#37319;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#38468;&#20214;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#25163;&#21160;&#20998;&#25315;&#25805;&#20316;&#21644;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22609;&#26009;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#27963;&#21160;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#30001;&#20110;&#20854;&#19981;&#21487;&#20998;&#35299;&#30340;&#24615;&#36136;&#21644;&#26356;&#20415;&#23452;&#30340;&#29983;&#20135;&#20215;&#26684;&#65292;&#23427;&#25104;&#20026;&#20102;&#38382;&#39064;&#30340;&#26681;&#28304;&#12290;&#38543;&#20043;&#32780;&#26469;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#24212;&#23545;&#25110;&#20943;&#36731;&#20854;&#22788;&#32622;&#25110;&#32570;&#20047;&#36866;&#24403;&#22788;&#29702;&#21518;&#30340;&#21518;&#26524;&#65292;&#36825;&#20250;&#23548;&#33268;&#24223;&#29289;&#22312;&#26576;&#20123;&#22320;&#26041;&#31215;&#32858;&#65292;&#24182;&#25200;&#20081;&#26893;&#29289;&#21644;&#21160;&#29289;&#30340;&#29983;&#24577;&#31995;&#32479;&#12290;&#38543;&#30528;&#22609;&#26009;&#24223;&#24323;&#29289;&#27700;&#24179;&#30340;&#19981;&#26029;&#19978;&#21319;&#65292;&#24182;&#19988;&#24223;&#29289;&#22312;&#22403;&#22334;&#22635;&#22475;&#22330;&#21644;&#26356;&#21361;&#38505;&#30340;&#22825;&#28982;&#27700;&#20307;&#20013;&#31215;&#32047;&#65292;&#38656;&#35201;&#36805;&#36895;&#37319;&#21462;&#34892;&#21160;&#26469;&#38459;&#27490;&#25110;&#20572;&#27490;&#36825;&#31181;&#27969;&#21160;&#12290;&#34429;&#28982;&#25163;&#21160;&#20998;&#25315;&#25805;&#20316;&#21644;&#26816;&#27979;&#21487;&#20197;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21487;&#20197;&#20511;&#21161;&#19982;&#26426;&#22120;&#20154;&#38468;&#20214;&#20851;&#32852;&#30340;&#39640;&#24230;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#22270;&#20687;&#26469;&#22686;&#24378;&#23427;&#20204;&#12290;&#26412;&#25253;&#21578;&#30340;&#20027;&#35201;&#20851;&#27880;&#24212;&#29992;&#26159;&#22791;&#21463;&#20851;&#27880;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#24320;&#25918;&#35270;&#35273;&#65292;&#23427;&#20204;&#22240;&#20854;&#23545;&#20114;&#32852;&#32593;&#30340;&#36731;&#24230;&#20381;&#36182;&#24615;&#21644;&#20256;&#36755;&#20449;&#24687;&#30340;&#33021;&#21147;&#32780;&#33719;&#24471;&#20102;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plastic has imbibed itself as an indispensable part of our day to day activities, becoming a source of problems due to its non-biodegradable nature and cheaper production prices. With these problems, comes the challenge of mitigating and responding to the aftereffects of disposal or the lack of proper disposal which leads to waste concentrating in locations and disturbing ecosystems for both plants and animals. As plastic debris levels continue to rise with the accumulation of waste in garbage patches in landfills and more hazardously in natural water bodies, swift action is necessary to plug or cease this flow. While manual sorting operations and detection can offer a solution, they can be augmented using highly advanced computer imagery linked with robotic appendages for removing wastes. The primary application of focus in this report are the much-discussed Computer Vision and Open Vision which have gained novelty for their light dependence on internet and ability to relay informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23454;&#29616;&#23384;&#20648;&#31995;&#32479;&#23398;&#20064;&#39044;&#27979;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21069;&#26816;&#27979;&#32531;&#24930;&#30340;IO&#35831;&#27714;&#21644;&#39044;&#27979;&#35774;&#22791;&#25925;&#38556;&#65292;&#20197;&#26500;&#24314;&#20855;&#26377;&#20302;&#23614;&#24310;&#36831;&#21644;&#39640;&#21487;&#29992;&#24615;&#30340;&#23384;&#20648;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.16288</link><description>&lt;p&gt;
&#23454;&#29616;&#23384;&#20648;&#31995;&#32479;&#23398;&#20064;&#39044;&#27979;&#24615;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Learned Predictability of Storage Systems. (arXiv:2307.16288v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23454;&#29616;&#23384;&#20648;&#31995;&#32479;&#23398;&#20064;&#39044;&#27979;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21069;&#26816;&#27979;&#32531;&#24930;&#30340;IO&#35831;&#27714;&#21644;&#39044;&#27979;&#35774;&#22791;&#25925;&#38556;&#65292;&#20197;&#26500;&#24314;&#20855;&#26377;&#20302;&#23614;&#24310;&#36831;&#21644;&#39640;&#21487;&#29992;&#24615;&#30340;&#23384;&#20648;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20113;&#35745;&#31639;&#21644;&#22823;&#25968;&#25454;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23384;&#20648;&#31995;&#32479;&#24050;&#25104;&#20026;&#25968;&#25454;&#20013;&#24515;&#30340;&#22522;&#26412;&#26500;&#24314;&#21333;&#20803;&#65292;&#32467;&#21512;&#20102;&#38378;&#23384;&#22266;&#24577;&#30828;&#30424;&#21644;&#38750;&#26131;&#22833;&#24615;&#20869;&#23384;&#31561;&#30828;&#20214;&#21019;&#26032;&#65292;&#20197;&#21450;RAID&#21644;&#20998;&#24067;&#24335;&#25991;&#20214;&#31995;&#32479;&#31561;&#36719;&#20214;&#22522;&#30784;&#35774;&#26045;&#12290;&#23613;&#31649;&#23384;&#20648;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#21644;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;&#20294;&#30001;&#20110;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#21644;&#30828;&#20214;&#25925;&#38556;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#35774;&#35745;&#21644;&#23454;&#29616;&#21487;&#38752;&#30340;&#23384;&#20648;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#31215;&#26497;&#39044;&#27979;&#33021;&#26497;&#22823;&#22686;&#24378;&#23384;&#20648;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#39044;&#27979;&#26377;&#20004;&#20010;&#32500;&#24230;&#65306;&#24615;&#33021;&#21644;&#25925;&#38556;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25552;&#21069;&#26816;&#27979;&#32531;&#24930;&#30340;IO&#35831;&#27714;&#65292;&#24182;&#22312;&#35774;&#22791;&#30495;&#27491;&#21457;&#29983;&#25925;&#38556;&#20043;&#21069;&#39044;&#27979;&#25925;&#38556;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#20855;&#26377;&#29305;&#21035;&#20302;&#23614;&#24310;&#36831;&#21644;&#39640;&#21487;&#29992;&#24615;&#30340;&#23384;&#20648;&#31995;&#32479;&#12290;&#23613;&#31649;&#23427;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#20102;&#24191;&#27867;&#35748;&#21487;&#65292;&#20294;&#23384;&#20648;&#31995;&#32479;&#20013;&#30340;&#36825;&#31181;&#31215;&#26497;&#39044;&#27979;&#21364;&#29305;&#21035;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of cloud computing and big data technologies, storage systems have become a fundamental building block of datacenters, incorporating hardware innovations such as flash solid state drives and non-volatile memories, as well as software infrastructures such as RAID and distributed file systems. Despite the growing popularity and interests in storage, designing and implementing reliable storage systems remains challenging, due to their performance instability and prevailing hardware failures.  Proactive prediction greatly strengthens the reliability of storage systems. There are two dimensions of prediction: performance and failure. Ideally, through detecting in advance the slow IO requests, and predicting device failures before they really happen, we can build storage systems with especially low tail latency and high availability. While its importance is well recognized, such proactive prediction in storage systems, on the other hand, is particularly difficult. 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20998;&#23618;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16265</link><description>&lt;p&gt;
&#20998;&#23618;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Hierarchical Multi-label Text Classification: A Survey. (arXiv:2307.16265v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16265
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20998;&#23618;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#36755;&#20837;&#30340;&#25991;&#26412;&#20998;&#20026;&#22810;&#20010;&#26631;&#31614;&#65292;&#20854;&#20013;&#26631;&#31614;&#20043;&#38388;&#20855;&#26377;&#32467;&#26500;&#21644;&#23618;&#27425;&#20851;&#31995;&#12290;&#23427;&#26159;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#31185;&#23398;&#25991;&#29486;&#24402;&#26723;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#20998;&#23618;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#24320;&#28304;&#25968;&#25454;&#38598;&#12289;&#20027;&#35201;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#24403;&#21069;&#30340;&#25361;&#25112;&#12290;&#36824;&#21015;&#20030;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20379;&#30740;&#31350;&#32773;&#36827;&#19968;&#27493;&#25913;&#36827;&#36825;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical multi-label text classification aims to classify the input text into multiple labels, among which the labels are structured and hierarchical. It is a vital task in many real world applications, e.g. scientific literature archiving. In this paper, we survey the recent progress of hierarchical multi-label text classification, including the open sourced data sets, the main methods, evaluation metrics, learning strategies and the current challenges. A few future research directions are also listed for community to further improve this field.
&lt;/p&gt;</description></item><item><title>DRL4Route&#26159;&#19968;&#31181;&#29992;&#20110;&#25509;&#36865;&#36335;&#32447;&#39044;&#27979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#23398;&#20064;&#33021;&#21147;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#21487;&#24494;&#20998;&#30446;&#26631;&#20248;&#21270;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#26631;&#20934;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16246</link><description>&lt;p&gt;
DRL4Route:&#19968;&#31181;&#29992;&#20110;&#25509;&#36865;&#36335;&#32447;&#39044;&#27979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction. (arXiv:2307.16246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16246
&lt;/p&gt;
&lt;p&gt;
DRL4Route&#26159;&#19968;&#31181;&#29992;&#20110;&#25509;&#36865;&#36335;&#32447;&#39044;&#27979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#23398;&#20064;&#33021;&#21147;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#21487;&#24494;&#20998;&#30446;&#26631;&#20248;&#21270;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#26631;&#20934;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25509;&#36865;&#36335;&#32447;&#39044;&#27979;(PDRP)&#22312;&#39044;&#27979;&#24037;&#20154;&#30340;&#26410;&#26469;&#26381;&#21153;&#36335;&#32447;&#26041;&#38754;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#33021;&#22815;&#20174;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#24037;&#20154;&#34892;&#20026;&#27169;&#24335;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#25104;&#20026;&#35813;&#20219;&#21153;&#30340;&#20027;&#23548;&#27169;&#22411;&#12290;&#34429;&#28982;&#26377;&#30528;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#23558;&#19981;&#21487;&#24494;&#20998;&#30340;&#27979;&#35797;&#26631;&#20934;&#24341;&#20837;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#27979;&#35797;&#26631;&#20934;&#19981;&#21305;&#37197;&#12290;&#36825;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#20351;&#29992;&#26102;&#26497;&#22823;&#22320;&#21066;&#20943;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#23558;&#24378;&#21270;&#23398;&#20064;(RL)&#25512;&#24191;&#21040;&#36335;&#32447;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31181;&#21517;&#20026;DRL4Route&#30340;&#26032;&#22411;RL&#26694;&#26550;&#12290;&#23427;&#32467;&#21512;&#20102;&#20808;&#21069;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#23398;&#20064;&#33021;&#21147;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#21487;&#24494;&#20998;&#30446;&#26631;&#20248;&#21270;&#33021;&#21147;&#12290;DRL4Route&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pick-up and Delivery Route Prediction (PDRP), which aims to estimate the future service route of a worker given his current task pool, has received rising attention in recent years. Deep neural networks based on supervised learning have emerged as the dominant model for the task because of their powerful ability to capture workers' behavior patterns from massive historical data. Though promising, they fail to introduce the non-differentiable test criteria into the training process, leading to a mismatch in training and test criteria. Which considerably trims down their performance when applied in practical systems. To tackle the above issue, we present the first attempt to generalize Reinforcement Learning (RL) to the route prediction task, leading to a novel RL-based framework called DRL4Route. It combines the behavior-learning abilities of previous deep learning models with the non-differentiable objective optimization ability of reinforcement learning. DRL4Route can serve as a plug-
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31995;&#21015;&#29983;&#29289;&#21551;&#21457;&#30340;&#31361;&#35302;&#21487;&#22609;&#24615;&#27169;&#22411;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;&#29983;&#29289;&#21551;&#21457;&#28145;&#24230;&#23398;&#20064;&#20195;&#34920;&#20102;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#25512;&#21160;&#24403;&#21069;&#25216;&#26415;&#30340;&#21457;&#23637;&#20197;&#21450;&#23545;&#26234;&#33021;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.16236</link><description>&lt;p&gt;
&#31361;&#35302;&#21487;&#22609;&#24615;&#27169;&#22411;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey. (arXiv:2307.16236v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31995;&#21015;&#29983;&#29289;&#21551;&#21457;&#30340;&#31361;&#35302;&#21487;&#22609;&#24615;&#27169;&#22411;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;&#29983;&#29289;&#21551;&#21457;&#28145;&#24230;&#23398;&#20064;&#20195;&#34920;&#20102;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#25512;&#21160;&#24403;&#21069;&#25216;&#26415;&#30340;&#21457;&#23637;&#20197;&#21450;&#23545;&#26234;&#33021;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#38754;&#20020;&#30528;&#23545;&#25239;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#12289;&#29983;&#24577;&#24433;&#21709;&#20197;&#21450;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#20851;&#27880;&#29983;&#29289;&#23398;&#22522;&#30784;&#26426;&#21046;&#65292;&#36825;&#26159;&#30001;&#20110;&#29983;&#29289;&#22823;&#33041;&#25152;&#23637;&#31034;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#25152;&#21560;&#24341;&#12290;&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#36825;&#20123;&#29983;&#29289;&#21551;&#21457;&#27169;&#22411;&#30340;&#31361;&#35302;&#21487;&#22609;&#24615;&#27169;&#22411;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#32852;&#31995;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#29983;&#29289;&#21551;&#21457;&#28145;&#24230;&#23398;&#20064;&#20195;&#34920;&#20102;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#25512;&#21160;&#19981;&#20165;&#24403;&#21069;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#32780;&#19988;&#36824;&#26377;&#23545;&#26234;&#33021;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged technologies based on Deep Learning (DL) achieved outstanding results on a variety of tasks in the field of Artificial Intelligence (AI). However, these encounter several challenges related to robustness to adversarial inputs, ecological impact, and the necessity of huge amounts of training data. In response, researchers are focusing more and more interest on biologically grounded mechanisms, which are appealing due to the impressive capabilities exhibited by biological brains. This survey explores a range of these biologically inspired models of synaptic plasticity, their application in DL scenarios, and the connections with models of plasticity in Spiking Neural Networks (SNNs). Overall, Bio-Inspired Deep Learning (BIDL) represents an exciting research direction, aiming at advancing not only our current technologies but also our understanding of intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30005;&#21160;&#36710;&#22312;&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#20013;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#36710;&#36742;&#20379;&#24212;&#21644;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16228</link><description>&lt;p&gt;
&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#20013;&#30005;&#21160;&#36710;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#65306;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach. (arXiv:2307.16228v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30005;&#21160;&#36710;&#22312;&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#31995;&#32479;&#20013;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#36710;&#36742;&#20379;&#24212;&#21644;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#32463;&#27982;&#21644;&#31038;&#20250;&#25928;&#30410;&#65292;&#30005;&#21160;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;EAV&#65289;&#27491;&#22312;&#26410;&#26469;&#30340;&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#65288;AMoD&#65289;&#31995;&#32479;&#20013;&#24341;&#36215;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;EAV&#30340;&#29420;&#29305;&#20805;&#30005;&#27169;&#24335;&#65288;&#38271;&#26102;&#38388;&#20805;&#30005;&#12289;&#39640;&#20805;&#30005;&#39057;&#29575;&#12289;&#26080;&#27861;&#39044;&#27979;&#30340;&#20805;&#30005;&#34892;&#20026;&#31561;&#65289;&#20351;&#24471;&#20934;&#30830;&#39044;&#27979;EAV&#20379;&#24212;&#22312;E-AMoD&#31995;&#32479;&#20013;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#31227;&#21160;&#38656;&#27714;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20351;&#24471;&#22312;&#20379;&#24212;&#21644;&#38656;&#27714;&#19981;&#30830;&#23450;&#24615;&#19979;&#35774;&#35745;&#19968;&#20010;&#38598;&#25104;&#30340;&#36710;&#36742;&#24179;&#34913;&#35299;&#20915;&#26041;&#26696;&#25104;&#20026;&#32039;&#36843;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;E-AMoD&#24179;&#34913;&#31639;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;EV&#20379;&#24212;&#25110;&#31227;&#21160;&#38656;&#27714;&#19979;&#30340;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;E-AMoD&#31995;&#32479;&#20013;&#30340;EAV&#24179;&#34913;&#65292;&#20854;&#20013;&#21253;&#21547;&#23545;&#25239;&#24615;&#26234;&#33021;&#20307;&#65292;&#29992;&#20110;&#27169;&#25311;&#21487;&#33021;&#30772;&#22351;&#36710;&#36742;&#24179;&#34913;&#35299;&#20915;&#26041;&#26696;&#30340;EAV&#20379;&#24212;&#21644;&#31227;&#21160;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;E-AMoD&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electric autonomous vehicles (EAVs) are getting attention in future autonomous mobility-on-demand (AMoD) systems due to their economic and societal benefits. However, EAVs' unique charging patterns (long charging time, high charging frequency, unpredictable charging behaviors, etc.) make it challenging to accurately predict the EAVs supply in E-AMoD systems. Furthermore, the mobility demand's prediction uncertainty makes it an urgent and challenging task to design an integrated vehicle balancing solution under supply and demand uncertainties. Despite the success of reinforcement learning-based E-AMoD balancing algorithms, state uncertainties under the EV supply or mobility demand remain unexplored. In this work, we design a multi-agent reinforcement learning (MARL)-based framework for EAVs balancing in E-AMoD systems, with adversarial agents to model both the EAVs supply and mobility demand uncertainties that may undermine the vehicle balancing solutions. We then propose a robust E-AMo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#23478;&#26063;&#35889;&#38382;&#31572;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#23478;&#26063;&#35889;&#25968;&#25454;&#34920;&#31034;&#20026;&#30693;&#35782;&#22270;&#24182;&#19982;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#32467;&#21512;&#65292;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#22270;&#32467;&#26500;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16214</link><description>&lt;p&gt;
&#29992;&#20110;&#21322;&#32467;&#26500;&#24322;&#26500;&#23478;&#26063;&#35889;&#30693;&#35782;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs. (arXiv:2307.16214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#23478;&#26063;&#35889;&#38382;&#31572;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#23478;&#26063;&#35889;&#25968;&#25454;&#34920;&#31034;&#20026;&#30693;&#35782;&#22270;&#24182;&#19982;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#32467;&#21512;&#65292;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#22270;&#32467;&#26500;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29992;&#25143;&#29983;&#25104;&#30340;&#23478;&#26063;&#35889;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#26032;&#30340;&#23478;&#26063;&#35889;&#20449;&#24687;&#31995;&#32479;&#24471;&#21040;&#20102;&#24320;&#21457;&#12290;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#31639;&#27861;&#20351;&#29992;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;&#24207;&#21015;&#30340;&#36755;&#20837;&#65292;&#19981;&#36866;&#21512;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#32467;&#26500;&#65292;&#32780;&#22522;&#20110;&#22270;&#30340;DNN&#27169;&#22411;&#21017;&#20381;&#36182;&#20110;&#22312;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#19981;&#23384;&#22312;&#30340;&#39640;&#24230;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26377;&#30417;&#30563;&#30340;DNN&#27169;&#22411;&#38656;&#35201;&#22312;&#23478;&#26063;&#35889;&#39046;&#22495;&#20013;&#32570;&#20047;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23478;&#26063;&#35889;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65306;1&#65289;&#23558;&#23478;&#26063;&#35889;&#25968;&#25454;&#34920;&#31034;&#20026;&#30693;&#35782;&#22270;&#65292;2&#65289;&#23558;&#20854;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;3&#65289;&#19982;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#32467;&#21512;&#65292;4&#65289;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#38382;&#31572;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#38656;&#35201;&#19987;&#38376;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#65292;&#23545;&#27604;&#20102;&#24494;&#35843;&#27169;&#24335;&#19979;&#30340;&#27169;&#22411;&#19982;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising popularity of user-generated genealogical family trees, new genealogical information systems have been developed. State-of-the-art natural question answering algorithms use deep neural network (DNN) architecture based on self-attention networks. However, some of these models use sequence-based inputs and are not suitable to work with graph-based structure, while graph-based DNN models rely on high levels of comprehensiveness of knowledge graphs that is nonexistent in the genealogical domain. Moreover, these supervised DNN models require training datasets that are absent in the genealogical domain. This study proposes an end-to-end approach for question answering using genealogical family trees by: 1) representing genealogical data as knowledge graphs, 2) converting them to texts, 3) combining them with unstructured texts, and 4) training a trans-former-based question answering model. To evaluate the need for a dedicated approach, a comparison between the fine-tuned mode
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#35270;&#35273;&#22270;&#20687;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#27169;&#31946;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#23436;&#25972;&#24615;&#26102;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16210</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#32570;&#22833;&#21644;&#27169;&#31946;&#30340;&#35270;&#35273;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment. (arXiv:2307.16210v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16210
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#35270;&#35273;&#22270;&#20687;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#27169;&#31946;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#23436;&#25972;&#24615;&#26102;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#37325;&#35201;&#25193;&#23637;&#65292;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#26469;&#35782;&#21035;&#36328;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20043;&#38388;&#30340;&#30456;&#21516;&#23454;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MMEA&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#29305;&#24449;&#30340;&#34701;&#21512;&#33539;&#24335;&#19978;&#65292;&#32780;&#24573;&#35270;&#20102;&#32570;&#22833;&#21644;&#20869;&#22312;&#27169;&#31946;&#24615;&#30340;&#35270;&#35273;&#22270;&#20687;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#35270;&#35273;&#27169;&#24577;&#19981;&#23436;&#25972;&#24615;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#20998;&#26512;&#65292;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;MMEA-UMVM&#25968;&#25454;&#38598;&#19978;&#23545;&#26368;&#26032;&#30340;MMEA&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#28085;&#30422;&#21452;&#35821;&#21644;&#21333;&#35821;&#23545;&#40784;KGs&#30340;&#31867;&#22411;&#65292;&#24182;&#37319;&#29992;&#26631;&#20934;&#65288;&#38750;&#36845;&#20195;&#65289;&#21644;&#36845;&#20195;&#35757;&#32451;&#33539;&#24335;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38754;&#23545;&#27169;&#24577;&#19981;&#23436;&#25972;&#24615;&#26102;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#27169;&#24577;&#22122;&#22768;&#65292;&#24182;&#22312;&#39640;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#24615;&#33021;&#25391;&#33633;&#25110;&#19979;&#38477;&#12290;&#36825;&#35777;&#26126;&#20102;&#22686;&#21152;&#35270;&#35273;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#22312;&#22522;&#22240;&#35889;&#39046;&#22495;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24182;&#33719;&#24471;&#20934;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;&#36824;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.16208</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;GLOBE&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks. (arXiv:2307.16208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#22312;&#22522;&#22240;&#35889;&#39046;&#22495;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24182;&#33719;&#24471;&#20934;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;&#36824;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#23478;&#35889;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25968;&#23383;&#32858;&#21512;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19982;&#31934;&#30830;&#31572;&#26696;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#30446;&#21069;&#65292;&#22312;&#22522;&#22240;&#35889;&#39046;&#22495;&#65292;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24182;&#33719;&#24471;&#20934;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;&#36824;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#65292;&#32780;&#30740;&#31350;&#32773;&#22312;&#20154;&#25991;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#21487;&#20197;&#20174;&#36825;&#31181;&#33021;&#21147;&#20013;&#21463;&#30410;&#21290;&#27973;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key AI tools for textual corpora exploration is natural language question-answering (QA). Unlike keyword-based search engines, QA algorithms receive and process natural language questions and produce precise answers to these questions, rather than long lists of documents that need to be manually scanned by the users. State-of-the-art QA algorithms based on DNNs were successfully employed in various domains. However, QA in the genealogical domain is still underexplored, while researchers in this field (and other fields in humanities and social sciences) can highly benefit from the ability to ask questions in natural language, receive concrete answers and gain insights hidden within large corpora. While some research has been recently conducted for factual QA in the genealogical domain, to the best of our knowledge, there is no previous research on the more challenging task of numerical aggregation QA (i.e., answering questions combining aggregation functions, e.g., count, ave
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#34394;&#25311;&#31354;&#38388;&#21512;&#25104;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#26085;&#24120;&#27963;&#21160;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#29702;&#35299;&#26085;&#24120;&#29983;&#27963;&#30340;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#22256;&#38590;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.16206</link><description>&lt;p&gt;
&#20351;&#29992;&#34394;&#25311;&#31354;&#38388;&#21512;&#25104;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#26085;&#24120;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Event-centric Knowledge Graphs of Daily Activities Using Virtual Space. (arXiv:2307.16206v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16206
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#34394;&#25311;&#31354;&#38388;&#21512;&#25104;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#26085;&#24120;&#27963;&#21160;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#29702;&#35299;&#26085;&#24120;&#29983;&#27963;&#30340;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#22256;&#38590;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39044;&#35745;&#23558;&#20307;&#29616;&#22312;&#33021;&#22815;&#29702;&#35299;&#23478;&#24237;&#29615;&#22659;&#20013;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#31181;&#24773;&#22659;&#20449;&#24687;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#20154;&#31867;&#34892;&#20026;&#21644;&#20915;&#31574;&#30340;&#36719;&#20214;&#20195;&#29702;&#12289;&#26426;&#22120;&#20154;&#21644;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#12290;&#22330;&#26223;&#22270;&#21644;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#26500;&#24314;&#25216;&#26415;&#23545;&#20110;&#28385;&#36275;&#36825;&#19968;&#26399;&#26395;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#23454;&#20307;&#38382;&#31572;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#29289;&#29702;&#31354;&#38388;&#20013;&#25910;&#38598;&#21644;&#31649;&#29702;&#21508;&#31181;&#23454;&#39564;&#26465;&#20214;&#19979;&#30340;&#26085;&#24120;&#27963;&#21160;&#30340;&#30495;&#23454;&#25968;&#25454;&#26159;&#30456;&#24403;&#26114;&#36149;&#30340;&#65292;&#32780;&#19988;&#24320;&#21457;&#20986;&#29702;&#35299;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#30340;&#20154;&#24037;&#26234;&#33021;&#20063;&#24456;&#22256;&#38590;&#12290;&#26410;&#26469;&#65292;&#39044;&#35745;&#23558;&#32467;&#21512;&#26469;&#33258;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#26465;&#20214;&#30340;&#34394;&#25311;&#31354;&#38388;&#21644;&#26465;&#20214;&#38590;&#20197;&#25913;&#21464;&#30340;&#29289;&#29702;&#31354;&#38388;&#30340;&#25968;&#25454;&#26469;&#20998;&#26512;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20351;&#29992;&#34394;&#25311;&#31354;&#38388;&#26500;&#24314;&#26085;&#24120;&#27963;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;&#24212;&#29992;&#30340;&#30740;&#31350;&#23578;&#26410;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is expected to be embodied in software agents, robots, and cyber-physical systems that can understand the various contextual information of daily life in the home environment to support human behavior and decision making in various situations. Scene graph and knowledge graph (KG) construction technologies have attracted much attention for knowledge-based embodied question answering meeting this expectation. However, collecting and managing real data on daily activities under various experimental conditions in a physical space are quite costly, and developing AI that understands the intentions and contexts is difficult. In the future, data from both virtual spaces, where conditions can be easily modified, and physical spaces, where conditions are difficult to change, are expected to be combined to analyze daily living activities. However, studies on the KG construction of daily activities using virtual space and their application have yet to progress. The po
&lt;/p&gt;</description></item><item><title>UnIVAL&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.16184</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#20219;&#21153;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unified Model for Image, Video, Audio and Language Tasks. (arXiv:2307.16184v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16184
&lt;/p&gt;
&lt;p&gt;
UnIVAL&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#24471;&#24314;&#31435;&#36890;&#29992;&#20195;&#29702;&#21464;&#24471;&#19981;&#20877;&#26159;&#24187;&#24819;&#12290;&#26500;&#24314;&#36825;&#31181;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38590;&#39064;&#26159;&#20219;&#21153;&#21644;&#27169;&#24577;&#30340;&#22810;&#26679;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#25903;&#25345;&#22810;&#26679;&#30340;&#20219;&#21153;&#21644;&#27169;&#24577;&#12290;&#34429;&#28982;&#19968;&#20123;&#22823;&#22411;&#27169;&#22411;&#65288;&#20363;&#22914;Flameigno&#65289;&#32463;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;&#20004;&#20010;&#27169;&#24577;&#65292;&#20294;&#30446;&#21069;&#23567;&#21040;&#20013;&#22411;&#30340;&#32479;&#19968;&#27169;&#22411;&#20173;&#28982;&#23616;&#38480;&#20110;&#20004;&#20010;&#27169;&#24577;&#65292;&#36890;&#24120;&#26159;&#22270;&#20687;-&#25991;&#26412;&#25110;&#35270;&#39057;-&#25991;&#26412;&#12290;&#25105;&#20204;&#35201;&#25552;&#20986;&#30340;&#38382;&#39064;&#26159;&#65306;&#26159;&#21542;&#21487;&#33021;&#26500;&#24314;&#19968;&#20010;&#39640;&#25928;&#25903;&#25345;&#25152;&#26377;&#27169;&#24577;&#30340;&#32479;&#19968;&#27169;&#22411;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UnIVAL&#65292;&#36825;&#26159;&#23545;&#36825;&#20010;&#38596;&#24515;&#21187;&#21187;&#30446;&#26631;&#36808;&#20986;&#30340;&#19968;&#27493;&#12290;UnIVAL&#27169;&#22411;&#25317;&#26377;&#32422;0.25&#20159;&#20010;&#21442;&#25968;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#32479;&#19968;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#39564;&#22686;&#24378;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20223;&#30495;&#25968;&#25454;&#21644;&#23454;&#39564;&#25968;&#25454;&#26469;&#20943;&#36731;&#27169;&#22411;&#24046;&#24322;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16173</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#39564;&#22686;&#24378;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#29992;&#20110;&#21452;&#26377;&#28304;&#26725;&#21464;&#25442;&#22120;&#30340;&#35843;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Modeling with Experimental Augmentation for the Modulation Strategy of the Dual-Active-Bridge Converter. (arXiv:2307.16173v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#39564;&#22686;&#24378;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20223;&#30495;&#25968;&#25454;&#21644;&#23454;&#39564;&#25968;&#25454;&#26469;&#20943;&#36731;&#27169;&#22411;&#24046;&#24322;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30005;&#21147;&#36716;&#25442;&#22120;&#30340;&#24615;&#33021;&#24314;&#27169;&#65292;&#20027;&#27969;&#26041;&#27861;&#22522;&#26412;&#19978;&#26159;&#22522;&#20110;&#30693;&#35782;&#30340;&#65292;&#30001;&#20110;&#20154;&#21147;&#36127;&#25285;&#37325;&#21644;&#24314;&#27169;&#31934;&#24230;&#20302;&#32780;&#21463;&#22256;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#36890;&#36807;&#20174;&#20223;&#30495;&#25968;&#25454;&#20013;&#33258;&#21160;&#24314;&#27169;&#26497;&#22823;&#22320;&#20943;&#36731;&#20102;&#23545;&#20154;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26410;&#24314;&#27169;&#30340;&#23492;&#29983;&#20803;&#20214;&#12289;&#19981;&#36275;&#30340;&#28909;&#30913;&#27169;&#22411;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#29615;&#22659;&#26465;&#20214;&#31561;&#21407;&#22240;&#65292;&#27169;&#22411;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#21457;&#29983;&#12290;&#36825;&#20123;&#20165;&#22522;&#20110;&#20223;&#30495;&#30340;&#19981;&#20934;&#30830;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26080;&#27861;&#20195;&#34920;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#24615;&#33021;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#30005;&#21147;&#36716;&#25442;&#22120;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#20943;&#36731;&#27169;&#22411;&#24046;&#24322;&#24182;&#22312;&#23454;&#36341;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#39564;&#22686;&#24378;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;(D2EA)&#65292;&#32467;&#21512;&#20102;&#20223;&#30495;&#25968;&#25454;&#21644;&#23454;&#39564;&#25968;&#25454;&#12290;&#22312;D2EA&#20013;&#65292;&#20223;&#30495;&#25968;&#25454;&#26088;&#22312;&#24314;&#31435;&#22522;&#26412;&#30340;&#21151;&#33021;&#26223;&#35266;&#65292;&#23454;&#39564;&#25968;&#25454;&#21017;&#19987;&#27880;&#20110;&#21305;&#37197;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;&#23454;&#20363;&#21270;&#20102;D2EA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
For the performance modeling of power converters, the mainstream approaches are essentially knowledge-based, suffering from heavy manpower burden and low modeling accuracy. Recent emerging data-driven techniques greatly relieve human reliance by automatic modeling from simulation data. However, model discrepancy may occur due to unmodeled parasitics, deficient thermal and magnetic models, unpredictable ambient conditions, etc. These inaccurate data-driven models based on pure simulation cannot represent the practical performance in physical world, hindering their applications in power converter modeling. To alleviate model discrepancy and improve accuracy in practice, this paper proposes a novel data-driven modeling with experimental augmentation (D2EA), leveraging both simulation data and experimental data. In D2EA, simulation data aims to establish basic functional landscape, and experimental data focuses on matching actual performance in real world. The D2EA approach is instantiated
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.16149</link><description>&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#30340;LSTM-DDPM&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16149
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#21644;DDPM&#30456;&#32467;&#21512;&#30340;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#21644;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#30423;&#31363;&#26816;&#27979;&#65288;ETD&#65289;&#21644;&#33021;&#37327;&#28040;&#32791;&#39044;&#27979;&#65288;ECF&#65289;&#26159;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#20849;&#21516;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#31995;&#32479;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;&#30340;ETD&#21644;ECF&#30340;&#30456;&#20114;&#20851;&#32852;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#36755;&#20837;&#37325;&#26500;&#21644;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#37325;&#26500;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#31995;&#32479;&#33021;&#22815;&#35782;&#21035;&#33021;&#37327;&#30423;&#31363;&#30340;&#23454;&#20363;&#65292;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#21644;&#39044;&#27979;&#35823;&#24046;&#30340;&#26041;&#27861;&#30456;&#20114;&#34917;&#20805;&#65292;&#21487;&#20197;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;ETD&#21644;ECF&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#38598;&#25104;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;ETD&#24615;&#33021;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#21040;&#22522;&#20934;&#26041;&#27861;&#26410;&#33021;&#26816;&#27979;&#21040;&#30340;&#33021;&#37327;&#30423;&#31363;&#25915;&#20987;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#20013;ETD&#21644;ECF&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#21367;&#31215;&#36731;&#37327;&#32423;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;&#65292;&#36890;&#36807;&#34701;&#21512;$3\times3$&#21644;$1\times1$&#21367;&#31215;&#26680;&#30340;&#20248;&#28857;&#65292;&#20197;&#21450;&#24341;&#20837;&#26080;&#21442;&#25968;&#30340;&#31354;&#38388;&#24179;&#31227;&#25805;&#20316;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.16140</link><description>&lt;p&gt;
&#20840;&#21367;&#31215;&#36731;&#37327;&#32423;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fully $1\times1$ Convolutional Network for Lightweight Image Super-Resolution. (arXiv:2307.16140v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#21367;&#31215;&#36731;&#37327;&#32423;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#32593;&#32476;&#65292;&#36890;&#36807;&#34701;&#21512;$3\times3$&#21644;$1\times1$&#21367;&#31215;&#26680;&#30340;&#20248;&#28857;&#65292;&#20197;&#21450;&#24341;&#20837;&#26080;&#21442;&#25968;&#30340;&#31354;&#38388;&#24179;&#31227;&#25805;&#20316;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#22312;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#20102;&#22823;&#21367;&#31215;&#26680;&#65288;$3\times3$&#25110;&#26356;&#22823;&#65289;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#26102;&#12289;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#12290;&#30456;&#21453;&#65292;$1\times1$&#21367;&#31215;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20294;&#22312;&#32858;&#21512;&#23616;&#37096;&#31354;&#38388;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;SISR&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#23545;&#31435;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34701;&#21512;$3\times3$&#21644;$1\times1$&#21367;&#31215;&#26680;&#20248;&#28857;&#30340;&#36731;&#37327;&#32423;SISR&#32593;&#32476;&#65292;&#21517;&#20026;Shift-Conv-based Network (SCNet)&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26080;&#21442;&#25968;&#30340;&#31354;&#38388;&#24179;&#31227;&#25805;&#20316;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36171;&#20104;&#20102;&#20840;&#21367;&#31215;&#36731;&#37327;&#32423;&#32593;&#32476;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep models have achieved significant process on single image super-resolution (SISR) tasks, in particular large models with large kernel ($3\times3$ or more). However, the heavy computational footprint of such models prevents their deployment in real-time, resource-constrained environments. Conversely, $1\times1$ convolutions bring substantial computational efficiency, but struggle with aggregating local spatial representations, an essential capability to SISR models. In response to this dichotomy, we propose to harmonize the merits of both $3\times3$ and $1\times1$ kernels, and exploit a great potential for lightweight SISR tasks. Specifically, we propose a simple yet effective fully $1\times1$ convolutional network, named Shift-Conv-based Network (SCNet). By incorporating a parameter-free spatial-shift operation, it equips the fully $1\times1$ convolutional network with powerful representation capability while impressive computational efficiency. Extensive experiments demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#21487;&#25511;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#35843;&#33410;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22238;&#24212;&#26102;&#21019;&#36896;&#21147;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#24544;&#35802;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36825;&#31181;&#26426;&#21046;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#25968;&#20540;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#36807;&#31243;&#35745;&#31639;&#26631;&#35760;&#30340;&#31243;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#20381;&#36182;&#31243;&#24230;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.16139</link><description>&lt;p&gt;
&#29992;&#25143;&#21487;&#25511;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#34701;&#21512;&#65306;&#24179;&#34913;&#21019;&#36896;&#21147;&#21644;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination. (arXiv:2307.16139v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#21487;&#25511;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#35843;&#33410;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22238;&#24212;&#26102;&#21019;&#36896;&#21147;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#24544;&#35802;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36825;&#31181;&#26426;&#21046;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#25968;&#20540;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#36807;&#31243;&#35745;&#31639;&#26631;&#35760;&#30340;&#31243;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#20381;&#36182;&#31243;&#24230;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#29983;&#25104;&#22810;&#26679;&#12289;&#30456;&#20851;&#19988;&#26377;&#21019;&#36896;&#24615;&#30340;&#22238;&#24212;&#33021;&#21147;&#32780;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#23613;&#31649;LLMs&#20855;&#26377;&#36825;&#20123;&#20248;&#28857;&#65292;&#20294;&#22312;&#21019;&#36896;&#21147;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#24544;&#35802;&#24230;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#29992;&#25143;&#21487;&#25511;&#26426;&#21046;&#65292;&#29992;&#20110;&#35843;&#33410;LLM&#22312;&#24819;&#35937;&#33021;&#21147;&#21644;&#19982;&#20107;&#23454;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;LLM&#30340;&#35757;&#32451;&#30340;&#24494;&#35843;&#38454;&#27573;&#20013;&#24341;&#20837;&#19968;&#20010;&#34920;&#31034;&#29983;&#25104;&#22238;&#24212;&#20013;&#23545;&#21442;&#32771;&#30693;&#35782;&#24544;&#35802;&#24230;&#31243;&#24230;&#30340;&#25968;&#20540;&#26631;&#35760;&#12290;&#36825;&#20010;&#31243;&#24230;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#35745;&#31639;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29992;ROUGE&#20998;&#25968;&#34913;&#37327;&#35789;&#27719;&#37325;&#21472;&#65292;&#20351;&#29992;Sentence-BERT&#23884;&#20837;&#34913;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;LLM&#30340;&#33258;&#25105;&#35780;&#20272;&#20998;&#25968;&#12290;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#25805;&#20316;&#36825;&#20010;&#25968;&#20540;&#26631;&#35760;&#65292;&#20174;&#32780;&#25511;&#21046;LLM&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#20381;&#36182;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2307.16104</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#20840;&#29699;&#21487;&#38752;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27946;&#27700;&#26159;&#26368;&#24120;&#35265;&#21644;&#24433;&#21709;&#26368;&#22823;&#30340;&#33258;&#28982;&#28798;&#23475;&#20043;&#19968;&#65292;&#23545;&#21457;&#23637;&#20013;&#22269;&#23478;&#23588;&#20854;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22269;&#23478;&#24448;&#24448;&#32570;&#20047;&#23494;&#38598;&#30340;&#27700;&#27969;&#30417;&#27979;&#32593;&#32476;&#12290;&#20934;&#30830;&#21450;&#26102;&#30340;&#39044;&#35686;&#23545;&#20110;&#20943;&#36731;&#27946;&#27700;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20934;&#30830;&#30340;&#27700;&#25991;&#27169;&#25311;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26681;&#25454;&#27599;&#20010;&#24212;&#29992;&#30340;&#27969;&#22495;&#20013;&#30340;&#38271;&#26102;&#38388;&#25968;&#25454;&#35760;&#24405;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;7&#22825;&#20869;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#22823;&#27954;&#12289;&#21069;&#23548;&#26102;&#38388;&#21644;&#37325;&#29616;&#26399;&#20013;&#22343;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20840;&#29699;&#27700;&#25991;&#27169;&#22411;&#65288;Copernicus&#24212;&#24613;&#31649;&#29702;&#26381;&#21153;&#20840;&#29699;&#27946;&#27700;&#24847;&#35782;&#31995;&#32479;&#65289;&#12290;AI&#22312;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#20013;&#30340;&#39044;&#27979;&#23588;&#20854;&#26377;&#25928;&#65292;&#36825;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20840;&#29699;&#21482;&#26377;&#30334;&#20998;&#20043;&#20960;&#30340;&#27969;&#22495;&#20855;&#26377;&#27969;&#37327;&#35266;&#27979;&#31449;&#65292;&#32780;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#25968;&#37327;&#21344;&#27604;&#24456;&#39640;&#65292;&#23545;&#20154;&#31867;&#29305;&#21035;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20998;&#21106;&#32593;&#32476;&#21644;&#21355;&#26143;&#22270;&#20687;&#65292;PD-SEG&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20154;&#21475;&#32479;&#35745;&#65292;&#25913;&#36827;&#20102;&#20197;&#24448;&#22312;&#21457;&#23637;&#20013;&#22269;&#23478;&#20934;&#30830;&#24230;&#19981;&#39640;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.16084</link><description>&lt;p&gt;
PD-SEG: &#20351;&#29992;&#28145;&#24230;&#20998;&#21106;&#32593;&#32476;&#36827;&#34892;&#20154;&#21475;&#20998;&#35299;&#65292;&#20197;&#25913;&#36827;&#24314;&#31569;&#20303;&#21306;&#25513;&#27169;
&lt;/p&gt;
&lt;p&gt;
PD-SEG: Population Disaggregation Using Deep Segmentation Networks For Improved Built Settlement Mask. (arXiv:2307.16084v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16084
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20998;&#21106;&#32593;&#32476;&#21644;&#21355;&#26143;&#22270;&#20687;&#65292;PD-SEG&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20154;&#21475;&#32479;&#35745;&#65292;&#25913;&#36827;&#20102;&#20197;&#24448;&#22312;&#21457;&#23637;&#20013;&#22269;&#23478;&#20934;&#30830;&#24230;&#19981;&#39640;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#28041;&#21450;&#36164;&#28304;&#24320;&#21457;&#21644;&#35268;&#21010;&#20513;&#35758;&#30340;&#25919;&#31574;&#23618;&#20915;&#31574;&#36807;&#31243;&#21644;&#23398;&#26415;&#30740;&#31350;&#37117;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#20154;&#21475;&#23494;&#24230;&#32479;&#35745;&#25968;&#25454;&#12290;WorldPop&#21644;Meta&#25552;&#20379;&#30340;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;&#23545;&#20110;&#20687;&#24052;&#22522;&#26031;&#22374;&#36825;&#26679;&#30340;&#21457;&#23637;&#20013;&#22269;&#23478;&#24182;&#19981;&#33021;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65307;&#23427;&#20204;&#31639;&#27861;&#30340;&#36755;&#20837;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20272;&#35745;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#31354;&#38388;&#21644;&#22303;&#22320;&#21033;&#29992;&#21160;&#24577;&#12290;&#20026;&#20102;&#31934;&#30830;&#20272;&#35745;30&#31859;&#215;30&#31859;&#20998;&#36776;&#29575;&#19979;&#30340;&#20154;&#21475;&#25968;&#37327;&#65292;&#25105;&#20204;&#20351;&#29992;&#20934;&#30830;&#30340;&#24314;&#31569;&#20303;&#21306;&#25513;&#27169;&#65292;&#35813;&#25513;&#27169;&#26159;&#20351;&#29992;&#28145;&#24230;&#20998;&#21106;&#32593;&#32476;&#21644;&#21355;&#26143;&#22270;&#20687;&#33719;&#24471;&#30340;&#65292;&#24182;&#19988;&#36824;&#20351;&#29992;&#20102;&#20852;&#36259;&#28857;(POI)&#25968;&#25454;&#26469;&#25490;&#38500;&#38750;&#23621;&#27665;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any policy-level decision-making procedure and academic research involving the optimum use of resources for development and planning initiatives depends on accurate population density statistics. The current cutting-edge datasets offered by WorldPop and Meta do not succeed in achieving this aim for developing nations like Pakistan; the inputs to their algorithms provide flawed estimates that fail to capture the spatial and land-use dynamics. In order to precisely estimate population counts at a resolution of 30 meters by 30 meters, we use an accurate built settlement mask obtained using deep segmentation networks and satellite imagery. The Points of Interest (POI) data is also used to exclude non-residential areas.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16082</link><description>&lt;p&gt;
EnrichEvent: &#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#20026;&#26032;&#20986;&#29616;&#30340;&#20107;&#20214;&#25552;&#20379;&#20016;&#23500;&#30340;&#31038;&#20132;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#24179;&#21488;&#24050;&#25104;&#20026;&#20256;&#25773;&#21644;&#35752;&#35770;&#30495;&#23454;&#20107;&#20214;&#20449;&#24687;&#30340;&#20851;&#38190;&#24179;&#21488;&#65292;&#20026;&#21450;&#26089;&#21457;&#29616;&#26377;&#26032;&#38395;&#20215;&#20540;&#30340;&#20107;&#20214;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#20165;&#21033;&#29992;&#20851;&#38190;&#35789;&#31361;&#21457;&#24615;&#25110;&#32593;&#32476;&#32467;&#26500;&#26469;&#26816;&#27979;&#28909;&#28857;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20107;&#20214;&#21644;&#31038;&#20132;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35328;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#22312;&#36798;&#21040;&#36235;&#21183;&#29366;&#24577;&#20043;&#21069;&#35782;&#21035;&#20986;&#26032;&#20986;&#29616;&#30340;&#31038;&#20132;&#20107;&#20214;&#12290;&#31038;&#20132;&#25968;&#25454;&#65292;&#20363;&#22914;&#25512;&#25991;&#65292;&#20855;&#26377;&#25340;&#20889;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#12289;&#27495;&#20041;&#24615;&#21644;&#35821;&#35328;&#19981;&#35268;&#33539;&#24615;&#65292;&#20197;&#21450;&#24847;&#35265;&#26041;&#38754;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#23398;&#20064;&#20107;&#20214;&#30340;&#28436;&#21464;&#29305;&#24449;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20960;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#27969;&#24335;&#31038;&#20132;&#25968;&#25454;&#30340;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#23545;&#35805;&#31995;&#32479;&#19982;&#25968;&#25454;&#21487;&#35270;&#21270;&#65292;&#36890;&#36807;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#29983;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;&#30740;&#31350;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#37325;&#26041;&#27861;&#26469;&#23454;&#29616;&#27492;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.16013</link><description>&lt;p&gt;
&#23558;&#23545;&#35805;&#31995;&#32479;&#19982;&#25968;&#25454;&#21487;&#35270;&#21270;&#32467;&#21512;&#65306;&#20174;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#20013;&#29983;&#25104;&#20132;&#20114;&#24335;&#25968;&#25454;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Marrying Dialogue Systems with Data Visualization: Interactive Data Visualization Generation from Natural Language Conversations. (arXiv:2307.16013v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#23545;&#35805;&#31995;&#32479;&#19982;&#25968;&#25454;&#21487;&#35270;&#21270;&#65292;&#36890;&#36807;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#29983;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;&#30740;&#31350;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#37325;&#26041;&#27861;&#26469;&#23454;&#29616;&#27492;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#24050;&#32463;&#25104;&#20026;&#24066;&#22330;&#19978;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#33021;&#26377;&#25928;&#22320;&#23637;&#31034;&#22823;&#37327;&#30340;&#25968;&#25454;&#35265;&#35299;&#12290;&#20026;&#20102;&#38477;&#20302;&#20351;&#29992;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#38376;&#27099;&#65292;&#30740;&#31350;&#30028;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#33258;&#21160;&#25968;&#25454;&#21487;&#35270;&#21270;&#20219;&#21153;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#21040;&#21487;&#35270;&#21270;&#36716;&#25442;&#65288;&#27491;&#24335;&#31216;&#20026;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#65289;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#20551;&#35774;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24050;&#32463;&#26377;&#19968;&#20010;&#33391;&#22909;&#30340;&#32452;&#32455;&#24182;&#19988;&#29992;&#19968;&#20010;&#21477;&#23376;&#26469;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22797;&#26434;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#38656;&#35201;&#36890;&#36807;&#21487;&#35270;&#21270;&#31995;&#32479;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#36830;&#32493;&#20132;&#20114;&#26469;&#23436;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;CoVis&#65292;&#21363;&#23545;&#35805;&#24335;&#25991;&#26412;&#21040;&#21487;&#35270;&#21270;&#65292;&#26088;&#22312;&#36890;&#36807;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#22810;&#20010;&#20132;&#20114;&#26469;&#26500;&#24314;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;&#22240;&#20026;&#36825;&#20010;&#20219;&#21153;&#22312;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#34987;&#30740;&#31350;&#65292;&#25152;&#20197;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;Dial-NVBench&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#29992;&#25143;&#30340;&#19968;&#31995;&#21015;&#26597;&#35810;&#21644;&#31995;&#32479;&#30340;&#22238;&#24212;&#30340;&#23545;&#35805;&#20250;&#35805;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#37325;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data visualization (DV) has become the prevailing tool in the market due to its effectiveness into illustrating insights in vast amounts of data. To lower the barrier of using DVs, automatic DV tasks, such as natural language question (NLQ) to visualization translation (formally called text-to-vis), have been investigated in the research community. However, text-to-vis assumes the NLQ to be well-organized and expressed in a single sentence. However, in real-world settings, complex DV is needed through consecutive exchanges between the DV system and the users. In this paper, we propose a new task named CoVis, short for Conversational text-to-Visualization, aiming at constructing DVs through a series of interactions between users and the system. Since it is the task which has not been studied in the literature, we first build a benchmark dataset named Dial-NVBench, including dialogue sessions with a sequence of queries from a user and responses from the system. Then, we propose a multi-m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;UPFL&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#26032;&#23458;&#25143;&#21152;&#20837;&#26102;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15994</link><description>&lt;p&gt;
UPFL&#65306;&#38754;&#21521;&#26032;&#23458;&#25143;&#30340;&#26080;&#30417;&#30563;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
UPFL: Unsupervised Personalized Federated Learning towards New Clients. (arXiv:2307.15994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;UPFL&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#26032;&#23458;&#25143;&#21152;&#20837;&#26102;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#19968;&#20010;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#30340;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#24403;&#32852;&#37030;&#27169;&#22411;&#34987;&#35757;&#32451;&#21644;&#37096;&#32626;&#21518;&#65292;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#26032;&#23458;&#25143;&#21152;&#20837;&#26102;&#65292;&#20026;&#26032;&#23458;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#27169;&#22411;&#25104;&#20026;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#33258;&#36866;&#24212;&#39118;&#38505;&#26368;&#23567;&#21270;&#25216;&#26415;&#25193;&#23637;&#21040;&#26080;&#30417;&#30563;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;FedTTA&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20004;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#25913;&#36827;&#20102;FedTTA&#65306;&#20351;&#29992;&#20195;&#29702;&#27491;&#21017;&#21270;&#22686;&#24378;&#33258;&#36866;&#24212;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#29109;&#25552;&#21069;&#20572;&#27490;&#33258;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;FedTTA&#35774;&#35745;&#30340;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#65292;&#20197;&#35299;&#20915;&#35774;&#22791;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#23545;&#27604;11&#20010;&#22522;&#20934;&#26041;&#27861;&#22312;5&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning has gained significant attention as a promising approach to address the challenge of data heterogeneity. In this paper, we address a relatively unexplored problem in federated learning. When a federated model has been trained and deployed, and an unlabeled new client joins, providing a personalized model for the new client becomes a highly challenging task. To address this challenge, we extend the adaptive risk minimization technique into the unsupervised personalized federated learning setting and propose our method, FedTTA. We further improve FedTTA with two simple yet effective optimization strategies: enhancing the training of the adaptation model with proxy regularization and early-stopping the adaptation through entropy. Moreover, we propose a knowledge distillation loss specifically designed for FedTTA to address the device heterogeneity. Extensive experiments on five datasets against eleven baselines demonstrate the effectiveness of our proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30896;&#25758;&#33258;&#30001;&#31354;&#38388;&#20013;&#24314;&#27169;&#20809;&#27969;&#30340;&#31574;&#30053;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#19977;&#32500;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#20809;&#27969;&#30340;&#26126;&#30830;&#34920;&#31034;&#21644;&#20809;&#27969;&#20998;&#37327;&#19982;&#22402;&#30452;&#22352;&#26631;&#20043;&#38388;&#30340;&#20108;&#27425;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.15989</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#30340;&#33258;&#30001;&#31354;&#38388;&#20809;&#27969;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Freespace Optical Flow Modeling for Automated Driving. (arXiv:2307.15989v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30896;&#25758;&#33258;&#30001;&#31354;&#38388;&#20013;&#24314;&#27169;&#20809;&#27969;&#30340;&#31574;&#30053;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#19977;&#32500;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#20809;&#27969;&#30340;&#26126;&#30830;&#34920;&#31034;&#21644;&#20809;&#27969;&#20998;&#37327;&#19982;&#22402;&#30452;&#22352;&#26631;&#20043;&#38388;&#30340;&#20108;&#27425;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#27969;&#21644;&#35270;&#24046;&#26159;&#33258;&#20027;&#39550;&#39542;&#24863;&#30693;&#20013;&#20004;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#23427;&#20204;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#22914;&#38556;&#30861;&#29289;&#21644;&#36710;&#36947;&#26816;&#27979;&#12290;"U-V-Disparity"&#30340;&#27010;&#24565;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#35752;&#65292;&#32780;&#20809;&#27969;&#30340;&#23545;&#24212;&#27010;&#24565;&#21364;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#36816;&#21160;&#20998;&#26512;&#31639;&#27861;&#36890;&#36807;&#21305;&#37197;&#20004;&#20010;&#36830;&#32493;&#35270;&#39057;&#24103;&#20043;&#38388;&#30340;&#23545;&#24212;&#28857;&#26469;&#20272;&#35745;&#20809;&#27969;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29615;&#22659;&#20449;&#24687;&#21644;&#20960;&#20309;&#32422;&#26463;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#22312;&#26234;&#33021;&#36710;&#36742;&#30340;&#30896;&#25758;&#33258;&#30001;&#31354;&#38388;&#65288;&#20063;&#31216;&#20026;&#21487;&#36890;&#34892;&#21306;&#22495;&#25110;&#31616;&#31216;&#20026;&#33258;&#30001;&#31354;&#38388;&#65289;&#20013;&#24314;&#27169;&#20809;&#27969;&#65292;&#20805;&#20998;&#21033;&#29992;&#19977;&#32500;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20809;&#27969;&#30340;&#26126;&#30830;&#34920;&#31034;&#65292;&#24182;&#25512;&#23548;&#20986;&#20809;&#27969;&#20998;&#37327;&#19982;&#22402;&#30452;&#22352;&#26631;&#20043;&#38388;&#30340;&#20108;&#27425;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical flow and disparity are two informative visual features for autonomous driving perception. They have been used for a variety of applications, such as obstacle and lane detection. The concept of "U-V-Disparity" has been widely explored in the literature, while its counterpart in optical flow has received relatively little attention. Traditional motion analysis algorithms estimate optical flow by matching correspondences between two successive video frames, which limits the full utilization of environmental information and geometric constraints. Therefore, we propose a novel strategy to model optical flow in the collision-free space (also referred to as drivable area or simply freespace) for intelligent vehicles, with the full utilization of geometry information in a 3D driving environment. We provide explicit representations of optical flow and deduce the quadratic relationship between the optical flow component and the vertical coordinate. Through extensive experiments on severa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#38376;&#25915;&#20987;&#23545;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#20849;&#20139;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19977;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15971</link><description>&lt;p&gt;
&#20320;&#21487;&#20197;&#36890;&#36807;&#21518;&#38376;&#25915;&#20987;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
You Can Backdoor Personalized Federated Learning. (arXiv:2307.15971v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#38376;&#25915;&#20987;&#23545;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#20849;&#20139;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19977;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#36890;&#29992;FL&#22330;&#26223;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#21363;&#25152;&#26377;&#23458;&#25143;&#31471;&#21512;&#20316;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#20849;&#20139;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65306;BapFL&#65292;BapFL+&#21644;Gen-BapFL&#65292;&#24182;&#32463;&#39564;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25915;&#20987;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks pose a significant threat to the security of federated learning systems. However, existing research primarily focuses on backdoor attacks and defenses within the generic FL scenario, where all clients collaborate to train a single global model. \citet{qin2023revisiting} conduct the first study of backdoor attacks in the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In this paper, we whistleblow that pFL methods with partial model-sharing are still vulnerable to backdoor attacks in the absence of any defense. We propose three backdoor attack methods: BapFL, BapFL+, and Gen-BapFL, and we empirically demonstrate that they can effectively attack the pFL methods. Specifically, the key principle of BapFL lies in maintaining clean local parameters while implanting t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#65288;MCond&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#21512;&#25104;&#22270;&#20013;&#39640;&#25928;&#22320;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15967</link><description>&lt;p&gt;
&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#29992;&#20110;&#24402;&#32435;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Condensation for Inductive Node Representation Learning. (arXiv:2307.15967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#65288;MCond&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#21512;&#25104;&#22270;&#20013;&#39640;&#25928;&#22320;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#24341;&#23548;&#32593;&#32476;&#38754;&#20020;&#30528;&#35745;&#31639;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22270;&#24418;&#21387;&#32553;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#20986;&#29616;&#20102;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#30340;&#21512;&#25104;&#22270;&#26469;&#39640;&#25928;&#22320;&#35757;&#32451;&#22270;&#24341;&#23548;&#32593;&#32476;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33410;&#28857;&#20043;&#38388;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#22270;&#24418;&#21387;&#32553;&#20165;&#38480;&#20110;&#21387;&#32553;&#35266;&#23519;&#21040;&#30340;&#35757;&#32451;&#33410;&#28857;&#21450;&#20854;&#23545;&#24212;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#32570;&#20047;&#26377;&#25928;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#20173;&#38656;&#35201;&#21407;&#22987;&#22823;&#22270;&#26469;&#23545;&#24402;&#32435;&#33410;&#28857;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#65292;&#23548;&#33268;&#35745;&#31639;&#38656;&#27714;&#24040;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#65288;MCond&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#23398;&#20064;&#20174;&#21407;&#22987;&#33410;&#28857;&#21040;&#21512;&#25104;&#33410;&#28857;&#30340;&#19968;&#23545;&#22810;&#33410;&#28857;&#26144;&#23556;&#65292;&#20197;&#26080;&#32541;&#22320;&#23558;&#26032;&#33410;&#28857;&#25972;&#21512;&#21040;&#21512;&#25104;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for induc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#23545;&#20998;&#24067;&#24335;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#33410;&#28857;&#36830;&#25509;&#24615;&#21644;&#32593;&#32476;&#23646;&#24615;&#22312;&#30693;&#35782;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#36830;&#25509;&#24615;&#21487;&#33021;&#23548;&#33268;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15947</link><description>&lt;p&gt;
&#32593;&#32476;&#25299;&#25169;&#23545;&#23436;&#20840;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#21021;&#27493;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The effect of network topologies on fully decentralized learning: a preliminary investigation. (arXiv:2307.15947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#23545;&#20998;&#24067;&#24335;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#33410;&#28857;&#36830;&#25509;&#24615;&#21644;&#32593;&#32476;&#23646;&#24615;&#22312;&#30693;&#35782;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#36830;&#25509;&#24615;&#21487;&#33021;&#23548;&#33268;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#25968;&#25454;&#36890;&#24120;&#34987;&#20998;&#21106;&#22312;&#22810;&#20010;&#35774;&#22791;&#25110;&#33410;&#28857;&#20043;&#38388;&#65292;&#27599;&#20010;&#33410;&#28857;&#20351;&#29992;&#33258;&#24049;&#30340;&#25968;&#25454;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#26412;&#22320;&#27169;&#22411;&#34987;&#20849;&#20139;&#21644;&#21512;&#24182;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#26032;&#25968;&#25454;&#19978;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#26412;&#25991;&#24320;&#22987;&#25506;&#32034;&#36830;&#25509;&#33410;&#28857;&#30340;&#32593;&#32476;&#25299;&#25169;&#23545;&#36890;&#36807;&#33410;&#28857;&#20043;&#38388;&#30340;&#30452;&#25509;&#21327;&#20316;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#31867;&#22411;&#30340;&#25299;&#25169;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#8220;&#30693;&#35782;&#20256;&#25773;&#8221;&#65292;&#21363;&#33410;&#28857;&#33021;&#22815;&#23558;&#20854;&#20182;&#33410;&#28857;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#27169;&#24335;&#30693;&#35782;&#34701;&#20837;&#21040;&#26412;&#22320;&#27169;&#22411;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#26356;&#25110;&#32773;&#26356;&#23569;&#36830;&#25509;&#30340;&#33410;&#28857;&#65288;&#20013;&#24515;&#33410;&#28857;&#21644;&#21494;&#33410;&#28857;&#65289;&#20197;&#21450;&#23439;&#35266;&#32593;&#32476;&#23646;&#24615;&#65288;&#20027;&#35201;&#26159;&#24230;&#20998;&#24067;&#21644;&#27169;&#22359;&#21270;&#65289;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#34429;&#28982;&#24050;&#30693;&#24369;&#36830;&#25509;&#33021;&#22815;&#20445;&#35777;&#30693;&#35782;&#20256;&#25773;&#65292;&#20294;&#20855;&#26377;&#26356;&#39640;&#30340;&#36830;&#25509;&#24615;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a decentralized machine learning system, data is typically partitioned among multiple devices or nodes, each of which trains a local model using its own data. These local models are then shared and combined to create a global model that can make accurate predictions on new data. In this paper, we start exploring the role of the network topology connecting nodes on the performance of a Machine Learning model trained through direct collaboration between nodes. We investigate how different types of topologies impact the "spreading of knowledge", i.e., the ability of nodes to incorporate in their local model the knowledge derived by learning patterns in data available in other nodes across the networks. Specifically, we highlight the different roles in this process of more or less connected nodes (hubs and leaves), as well as that of macroscopic network properties (primarily, degree distribution and modularity). Among others, we show that, while it is known that even weak connectivity a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15936</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#38598;&#21512;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#25193;&#22823;&#26102;&#65292;&#26032;&#30340;&#25216;&#33021;&#23558;&#22312; AI &#20135;&#21697;&#20013;&#20986;&#29616;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#36825;&#31181;&#29616;&#35937;&#23578;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#22522;&#20110;&#26799;&#24230;&#35757;&#32451;&#30340;&#25968;&#23398;&#20998;&#26512;&#25552;&#20379;&#26426;&#26800;&#35299;&#37322;&#20284;&#20046;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#37319;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33879;&#21517;&#30340;&#65288;&#21644;&#32463;&#39564;&#24615;&#30340;&#65289;LLM&#25193;&#23637;&#23450;&#24459;&#21644;&#31616;&#21333;&#30340;&#32479;&#35745;&#26694;&#26550;&#26469;&#20998;&#26512;&#20986;&#29616;&#12290;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;a&#65289;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#23558;LLM&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#35821;&#35328;&#20219;&#21153;&#22522;&#26412;&#25216;&#33021;&#30340;&#33021;&#21147;&#30456;&#20851;&#32852;&#12290;&#65288;b&#65289;&#25968;&#23398;&#20998;&#26512;&#34920;&#26126;&#65292;&#25193;&#23637;&#23450;&#24459;&#24847;&#21619;&#30528;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#24471;&#38750;&#24120;&#39640;&#25928;&#12290;&#25105;&#20204;&#38750;&#27491;&#24335;&#22320;&#31216;&#20043;&#20026;&#8220;&#24377;&#24339;&#27867;&#21270;&#8221;&#65292;&#22240;&#20026;&#34920;&#38754;&#19978;&#30475;&#65292;&#23427;&#20284;&#20046;&#25552;&#20379;&#20102;&#22312;&#25216;&#33021;&#27700;&#24179;&#19978;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;&#65288;c&#65289;&#24377;&#24339;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#20363;&#23376;&#65292;&#21363;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing task
&lt;/p&gt;</description></item><item><title>ATESA-B{\AE}RT&#26159;&#19968;&#20010;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24322;&#26500;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#20026;&#26041;&#38754;&#35789;&#25552;&#21462;&#21644;&#26041;&#38754;&#35789;&#24773;&#24863;&#20998;&#26512;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;\textit{argmax}&#22810;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22312;&#26041;&#38754;&#32423;&#21035;&#19978;&#30340;&#31890;&#24230;&#65292;&#25913;&#36827;&#20102;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#22312;&#22810;&#26041;&#38754;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15920</link><description>&lt;p&gt;
ATESA-B{\AE}RT: &#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24322;&#26500;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ATESA-B{\AE}RT: A Heterogeneous Ensemble Learning Model for Aspect-Based Sentiment Analysis. (arXiv:2307.15920v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15920
&lt;/p&gt;
&lt;p&gt;
ATESA-B{\AE}RT&#26159;&#19968;&#20010;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24322;&#26500;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#20026;&#26041;&#38754;&#35789;&#25552;&#21462;&#21644;&#26041;&#38754;&#35789;&#24773;&#24863;&#20998;&#26512;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;\textit{argmax}&#22810;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22312;&#26041;&#38754;&#32423;&#21035;&#19978;&#30340;&#31890;&#24230;&#65292;&#25913;&#36827;&#20102;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#22312;&#22810;&#26041;&#38754;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#35780;&#35770;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#21457;&#23637;&#25104;&#20026;&#20102;&#30830;&#23450;&#28040;&#36153;&#32773;&#23545;&#19981;&#21516;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#24847;&#35265;&#30340;&#21487;&#33021;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24773;&#24863;&#20998;&#26512;&#24050;&#34987;&#35777;&#26126;&#26159;&#30830;&#23450;&#35780;&#35770;&#25972;&#20307;&#26497;&#24615;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#20026;&#20102;&#25552;&#39640;&#22312;&#26041;&#38754;&#32423;&#21035;&#19978;&#30340;&#31890;&#24230;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#26381;&#21153;&#25110;&#20135;&#21697;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#39318;&#20808;&#26088;&#22312;&#35782;&#21035;&#26041;&#38754;&#65292;&#28982;&#21518;&#30830;&#23450;&#29992;&#25143;&#23545;&#23427;&#20204;&#30340;&#24847;&#35265;&#12290;&#36825;&#39033;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#22312;&#20110;&#21516;&#19968;&#35780;&#35770;&#21487;&#33021;&#20250;&#21576;&#29616;&#22810;&#20010;&#26041;&#38754;&#65292;&#27599;&#20010;&#26041;&#38754;&#20855;&#26377;&#33258;&#24049;&#30340;&#26497;&#24615;&#12290;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#36825;&#31181;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;ATESA-B{\AE}RT&#65292;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24322;&#26500;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#26041;&#38754;&#35789;&#25552;&#21462;&#21644;&#26041;&#38754;&#35789;&#24773;&#24863;&#20998;&#26512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;\textit{argmax}&#22810;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#23545;&#26041;&#38754;&#35789;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing volume of online reviews has made possible the development of sentiment analysis models for determining the opinion of customers regarding different products and services. Until now, sentiment analysis has proven to be an effective tool for determining the overall polarity of reviews. To improve the granularity at the aspect level for a better understanding of the service or product, the task of aspect-based sentiment analysis aims to first identify aspects and then determine the user's opinion about them. The complexity of this task lies in the fact that the same review can present multiple aspects, each with its own polarity. Current solutions have poor performance on such data. We address this problem by proposing ATESA-B{\AE}RT, a heterogeneous ensemble learning model for Aspect-Based Sentiment Analysis. Firstly, we divide our problem into two sub-tasks, i.e., Aspect Term Extraction and Aspect Term Sentiment Analysis. Secondly, we use the \textit{argmax} multi-class 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31354;&#38388;&#32467;&#26500;&#30340;&#24050;&#26377;&#21644;&#26032;&#22686;&#22522;&#30784;&#35774;&#26045;&#25910;&#38598;&#30340;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#25968;&#25454;&#65292;&#24182;&#23884;&#20837;&#21040;&#20219;&#20309;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#20013;&#24212;&#29992;&#20110;&#20855;&#26377;&#28436;&#21464;&#31354;&#38388;&#32467;&#26500;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2307.15916</link><description>&lt;p&gt;
&#26426;&#20250;&#20027;&#20041;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#30417;&#27979;&#21644;&#39044;&#27979;&#19982;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Opportunistic Air Quality Monitoring and Forecasting with Expandable Graph Neural Networks. (arXiv:2307.15916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15916
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31354;&#38388;&#32467;&#26500;&#30340;&#24050;&#26377;&#21644;&#26032;&#22686;&#22522;&#30784;&#35774;&#26045;&#25910;&#38598;&#30340;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#25968;&#25454;&#65292;&#24182;&#23884;&#20837;&#21040;&#20219;&#20309;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#20013;&#24212;&#29992;&#20110;&#20855;&#26377;&#28436;&#21464;&#31354;&#38388;&#32467;&#26500;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#30417;&#27979;&#21644;&#39044;&#27979;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#30001;&#20110;&#22478;&#24066;&#22320;&#21306;&#24050;&#24314;&#31435;&#20102;&#20581;&#20840;&#30340;&#25968;&#25454;&#25910;&#38598;&#35774;&#26045;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36890;&#24120;&#30001;&#22269;&#23478;&#30740;&#31350;&#26426;&#26500;&#25110;&#31185;&#25216;&#24040;&#22836;&#37096;&#32626;&#30340;&#22266;&#23450;&#22522;&#30784;&#35774;&#26045;&#24448;&#24448;&#26080;&#27861;&#28385;&#36275;&#19981;&#21516;&#20010;&#24615;&#21270;&#22330;&#26223;&#30340;&#38656;&#27714;&#65292;&#20363;&#22914;&#22312;&#27809;&#26377;&#20219;&#20309;&#29616;&#26377;&#22522;&#30784;&#35774;&#26045;&#30340;&#21306;&#22495;&#36827;&#34892;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#35268;&#27169;&#36739;&#23567;&#30340;&#30740;&#31350;&#26426;&#26500;&#25110;&#26377;&#38480;&#39044;&#31639;&#30340;&#20844;&#21496;&#19981;&#24471;&#19981;&#23547;&#27714;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#26356;&#28789;&#27963;&#30340;&#25968;&#25454;&#25910;&#38598;&#22522;&#30784;&#35774;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476; (EGAT) &#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#24050;&#26377;&#21644;&#26032;&#22686;&#22522;&#30784;&#35774;&#26045;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#30340;&#31354;&#38388;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#21487;&#20197;&#23884;&#20837;&#20219;&#20309;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#28436;&#21464;&#31354;&#38388;&#32467;&#26500;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air Quality Monitoring and Forecasting has been a popular research topic in recent years. Recently, data-driven approaches for air quality forecasting have garnered significant attention, owing to the availability of well-established data collection facilities in urban areas. Fixed infrastructures, typically deployed by national institutes or tech giants, often fall short in meeting the requirements of diverse personalized scenarios, e.g., forecasting in areas without any existing infrastructure. Consequently, smaller institutes or companies with limited budgets are compelled to seek tailored solutions by introducing more flexible infrastructures for data collection. In this paper, we propose an expandable graph attention network (EGAT) model, which digests data collected from existing and newly-added infrastructures, with different spatial structures. Additionally, our proposal can be embedded into any air quality forecasting models, to apply to the scenarios with evolving spatial str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MoisesDB&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;&#38899;&#20048;&#28304;&#20998;&#31163;&#12290;&#23427;&#21253;&#21547;240&#39318;&#26469;&#33258;45&#20301;&#33402;&#26415;&#23478;&#30340;&#26354;&#30446;&#65292;&#28085;&#30422;&#20102;&#21313;&#20108;&#31181;&#38899;&#20048;&#27969;&#27966;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#20004;&#32423;&#23618;&#27425;&#20998;&#31867;&#27861;&#65292;&#36229;&#36234;&#20102;&#20351;&#29992;&#22235;&#20010;&#38899;&#39057;&#28304;&#30340;&#38480;&#21046;&#65292;&#26377;&#21161;&#20110;&#26500;&#24314;&#21644;&#35780;&#20272;&#32454;&#31890;&#24230;&#30340;&#28304;&#20998;&#31163;&#31995;&#32479;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#22522;&#32447;&#32467;&#26524;&#21644;&#20998;&#26512;&#65292;&#23545;&#20110;&#19981;&#21516;&#20998;&#31163;&#32454;&#31890;&#24230;&#30340;&#24320;&#28304;&#20998;&#31163;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.15913</link><description>&lt;p&gt;
Moisesdb: &#19968;&#20221;&#36229;&#36234;&#22235;&#20010;&#38899;&#39057;&#28304;&#30340;&#20998;&#31163;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Moisesdb: A dataset for source separation beyond 4-stems. (arXiv:2307.15913v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MoisesDB&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;&#38899;&#20048;&#28304;&#20998;&#31163;&#12290;&#23427;&#21253;&#21547;240&#39318;&#26469;&#33258;45&#20301;&#33402;&#26415;&#23478;&#30340;&#26354;&#30446;&#65292;&#28085;&#30422;&#20102;&#21313;&#20108;&#31181;&#38899;&#20048;&#27969;&#27966;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#20004;&#32423;&#23618;&#27425;&#20998;&#31867;&#27861;&#65292;&#36229;&#36234;&#20102;&#20351;&#29992;&#22235;&#20010;&#38899;&#39057;&#28304;&#30340;&#38480;&#21046;&#65292;&#26377;&#21161;&#20110;&#26500;&#24314;&#21644;&#35780;&#20272;&#32454;&#31890;&#24230;&#30340;&#28304;&#20998;&#31163;&#31995;&#32479;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#22522;&#32447;&#32467;&#26524;&#21644;&#20998;&#26512;&#65292;&#23545;&#20110;&#19981;&#21516;&#20998;&#31163;&#32454;&#31890;&#24230;&#30340;&#24320;&#28304;&#20998;&#31163;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#38899;&#20048;&#28304;&#20998;&#31163;&#30340;MoisesDB&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;45&#20301;&#33402;&#26415;&#23478;&#30340;240&#39318;&#26354;&#30446;&#65292;&#28085;&#30422;&#20102;&#21313;&#20108;&#31181;&#38899;&#20048;&#27969;&#27966;&#12290;&#23545;&#20110;&#27599;&#39318;&#27468;&#26354;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;&#20010;&#21035;&#38899;&#39057;&#28304;&#65292;&#20197;&#20004;&#32423;&#23618;&#27425;&#20998;&#31867;&#27861;&#36827;&#34892;&#32452;&#32455;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#26500;&#24314;&#21644;&#35780;&#20272;&#32454;&#31890;&#24230;&#30340;&#28304;&#20998;&#31163;&#31995;&#32479;&#65292;&#36229;&#36234;&#20351;&#29992;&#22235;&#20010;&#38899;&#39057;&#28304;&#65288;&#40723;&#12289;&#36125;&#26031;&#12289;&#20854;&#20182;&#21644;&#20154;&#22768;&#65289;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#32780;&#23548;&#33268;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20415;&#20110;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#24211;&#65292;&#20197;&#19979;&#36733;&#12289;&#22788;&#29702;&#21644;&#20351;&#29992;MoisesDB&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20379;&#20102;&#38024;&#23545;&#19981;&#21516;&#20998;&#31163;&#32454;&#31890;&#24230;&#65288;&#22235;&#20010;&#12289;&#20116;&#20010;&#21644;&#20845;&#20010;&#38899;&#39057;&#28304;&#65289;&#30340;&#24320;&#28304;&#20998;&#31163;&#27169;&#22411;&#30340;&#22522;&#32447;&#32467;&#26524;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the MoisesDB dataset for musical source separation. It consists of 240 tracks from 45 artists, covering twelve musical genres. For each song, we provide its individual audio sources, organized in a two-level hierarchical taxonomy of stems. This will facilitate building and evaluating fine-grained source separation systems that go beyond the limitation of using four stems (drums, bass, other, and vocals) due to lack of data. To facilitate the adoption of this dataset, we publish an easy-to-use Python library to download, process and use MoisesDB. Alongside a thorough documentation and analysis of the dataset contents, this work provides baseline results for open-source separation models for varying separation granularities (four, five, and six stems), and discuss their results.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26426;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#22797;&#26434;&#26102;&#31354;&#32422;&#26463;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#22987;&#32456;&#20445;&#25345;&#25152;&#38656;&#32422;&#26463;&#28385;&#36275;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15910</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#27010;&#29575;&#26102;&#31354;&#32422;&#26463;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Under Probabilistic Spatio-Temporal Constraints with Time Windows. (arXiv:2307.15910v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15910
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26426;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#22797;&#26434;&#26102;&#31354;&#32422;&#26463;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#22987;&#32456;&#20445;&#25345;&#25152;&#38656;&#32422;&#26463;&#28385;&#36275;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26426;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#22797;&#26434;&#26102;&#31354;&#32422;&#26463;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#20351;&#29992;&#26377;&#30028;&#26102;&#24577;&#36923;&#36753;&#32422;&#26463;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#12290;&#19982;&#29616;&#26377;&#30340;RL&#26041;&#27861;&#21487;&#20197;&#26368;&#32456;&#23398;&#20064;&#21040;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#30340;&#26368;&#20248;&#31574;&#30053;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#22987;&#32456;&#20445;&#25345;&#25152;&#38656;&#32422;&#26463;&#28385;&#36275;&#30340;&#27010;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#26377;&#30028;&#26102;&#24577;&#36923;&#36753;&#32422;&#26463;&#36716;&#21270;&#20026;&#24635;&#33258;&#21160;&#26426;&#65292;&#24182;&#22522;&#20110;&#20851;&#20110;&#36716;&#31227;&#27010;&#29575;&#30340;&#20808;&#21069;&#21487;&#29992;&#20449;&#24687;&#65288;&#21363;&#27599;&#20010;&#36716;&#31227;&#27010;&#29575;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#65289;&#36991;&#20813;&#8220;&#19981;&#23433;&#20840;&#8221;&#21160;&#20316;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#22312;&#32467;&#26524;&#20013;&#25552;&#20379;&#20102;&#32422;&#26463;&#28385;&#36275;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#22330;&#26223;&#20013;&#25552;&#20379;&#20102;&#25968;&#20540;&#32467;&#26524;&#65292;&#35813;&#22330;&#26223;&#20013;&#65292;&#26426;&#22120;&#20154;&#22312;&#25506;&#32034;&#29615;&#22659;&#65292;&#21457;&#29616;&#39640;&#22238;&#25253;&#21306;&#22495;&#30340;&#21516;&#26102;&#65292;&#36824;&#38656;&#35201;&#25191;&#34892;&#19968;&#20123;&#21608;&#26399;&#24615;&#30340;&#25342;&#21462;&#21644;&#20132;&#20184;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
We propose an automata-theoretic approach for reinforcement learning (RL) under complex spatio-temporal constraints with time windows. The problem is formulated using a Markov decision process under a bounded temporal logic constraint. Different from existing RL methods that can eventually learn optimal policies satisfying such constraints, our proposed approach enforces a desired probability of constraint satisfaction throughout learning. This is achieved by translating the bounded temporal logic constraint into a total automaton and avoiding "unsafe" actions based on the available prior information regarding the transition probabilities, i.e., a pair of upper and lower bounds for each transition probability. We provide theoretical guarantees on the resulting probability of constraint satisfaction. We also provide numerical results in a scenario where a robot explores the environment to discover high-reward regions while fulfilling some periodic pick-up and delivery tasks that are enc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#20026;$O(1/t)$&#12290;</title><link>http://arxiv.org/abs/2307.15892</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#30340;&#26032;&#22411;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65306;&#36890;&#36807;$L$-$\lambda$&#24179;&#28369;&#24615;&#36827;&#34892;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$\lambda$ Smoothness. (arXiv:2307.15892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#20026;$O(1/t)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#65288;GTD&#65289;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026;$O(d)$&#65288;$d$&#26159;&#29305;&#24449;&#25968;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Impression GTD&#30340;&#20840;&#26032;&#21333;&#26102;&#38388;&#23610;&#24230;GTD&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#26399;&#26395;td&#26356;&#26032;&#65288;NEU&#65289;&#30446;&#26631;&#65292;&#24182;&#21482;&#26377;&#19968;&#20010;&#27493;&#38271;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26032;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#19982;$O(1/t)$&#19968;&#26679;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient Temporal Difference (GTD) algorithms (Sutton et al., 2008, 2009) are the first $O(d)$ ($d$ is the number features) algorithms that have convergence guarantees for off-policy learning with linear function approximation. Liu et al. (2015) and Dalal et. al. (2018) proved the convergence rates of GTD, GTD2 and TDC are $O(t^{-\alpha/2})$ for some $\alpha \in (0,1)$. This bound is tight (Dalal et al., 2020), and slower than $O(1/\sqrt{t})$. GTD algorithms also have two step-size parameters, which are difficult to tune. In literature, there is a "single-time-scale" formulation of GTD. However, this formulation still has two step-size parameters.  This paper presents a truly single-time-scale GTD algorithm for minimizing the Norm of Expected td Update (NEU) objective, and it has only one step-size parameter. We prove that the new algorithm, called Impression GTD, converges at least as fast as $O(1/t)$. Furthermore, based on a generalization of the expected smoothness (Gower et al. 201
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26080;&#37327;&#32434;&#21464;&#37327;&#21644;&#21046;&#24230;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#25968;&#20540;&#29983;&#25104;&#30340;&#26368;&#20248;&#25511;&#21046;&#27861;&#21017;&#25512;&#24191;&#21040;&#37327;&#32434;&#30456;&#20284;&#30340;&#31995;&#32479;&#65292;&#36825;&#23545;&#20110;&#25512;&#24191;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#39640;&#32500;&#38382;&#39064;&#30340;&#31574;&#30053;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.15852</link><description>&lt;p&gt;
&#22522;&#20110;&#24052;&#20811;&#27721;&#22982;&#960;&#23450;&#29702;&#30340;&#26080;&#37327;&#32434;&#31574;&#30053;&#65306;&#26159;&#25512;&#24191;&#25968;&#20540;&#32467;&#26524;&#30340;&#22909;&#26041;&#27861;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Dimensionless Policies based on the Buckingham $\pi$ Theorem: Is it a good way to Generalize Numerical Results?. (arXiv:2307.15852v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15852
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26080;&#37327;&#32434;&#21464;&#37327;&#21644;&#21046;&#24230;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#25968;&#20540;&#29983;&#25104;&#30340;&#26368;&#20248;&#25511;&#21046;&#27861;&#21017;&#25512;&#24191;&#21040;&#37327;&#32434;&#30456;&#20284;&#30340;&#31995;&#32479;&#65292;&#36825;&#23545;&#20110;&#25512;&#24191;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#39640;&#32500;&#38382;&#39064;&#30340;&#31574;&#30053;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#19978;&#19979;&#25991;&#29615;&#22659;&#21644;&#23450;&#20041;&#36816;&#21160;&#25511;&#21046;&#38382;&#39064;&#30340;&#21464;&#37327;&#21015;&#34920;&#26159;&#22312;&#37327;&#32434;&#19978;&#30456;&#20284;&#30340;&#35805;&#65292;&#26159;&#21487;&#20197;&#30340;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#26080;&#37327;&#32434;&#21464;&#37327;&#20462;&#25913;&#38382;&#39064;&#30340;&#24418;&#24335;&#65292;&#21487;&#20197;&#23558;&#25968;&#20540;&#29983;&#25104;&#30340;&#26368;&#20248;&#25511;&#21046;&#27861;&#21017;&#37325;&#26032;&#24212;&#29992;&#20110;&#22312;&#37327;&#32434;&#19978;&#30456;&#20284;&#30340;&#23376;&#31354;&#38388;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21463;&#25197;&#30697;&#38480;&#21046;&#30340;&#20498;&#31435;&#25670;&#32463;&#20856;&#36816;&#21160;&#25511;&#21046;&#38382;&#39064;&#36827;&#34892;&#25968;&#20540;&#29983;&#25104;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#30340;&#23637;&#31034;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21046;&#24230;&#30340;&#27010;&#24565;&#65292;&#21363;&#19978;&#19979;&#25991;&#21464;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#21306;&#22495;&#65292;&#21487;&#20197;&#24110;&#21161;&#25918;&#23485;&#37327;&#32434;&#30456;&#20284;&#24615;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23558;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#36827;&#34892;&#37327;&#32434;&#32553;&#25918;&#19982;&#22312;&#20998;&#26512;&#26041;&#31243;&#20013;&#29992;&#26032;&#30340;&#31995;&#32479;&#21442;&#25968;&#26367;&#20195;&#37327;&#32434;&#30456;&#20284;&#31995;&#32479;&#30340;&#31561;&#20215;&#24615;&#12290;&#23578;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#27492;&#26041;&#27861;&#26159;&#21542;&#20063;&#36866;&#29992;&#20110;&#25512;&#24191;&#26356;&#22797;&#26434;&#30340;&#39640;&#32500;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yes if the context, the list of variables defining the motion control problem, is dimensionally similar. Here we show that by modifying the problem formulation using dimensionless variables, we can re-use the optimal control law generated numerically for a specific system to a sub-space of dimensionally similar systems. This is demonstrated, with numerically generated optimal controllers, for the classic motion control problem of swinging-up a torque-limited inverted pendulum. We also discuss the concept of regime, a region in the space of context variables, that can help relax the condition on dimensional similarity. Futhermore, we discuss how applying dimensionnal scaling of the input and output of a context-specific policy is equivalent to substituing the new systems parameters in an analytical equation for dimentionnaly similar systems. It remains to be seen if this approach can also help generalizing policies for more complex high-dimensional problems.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20154;&#31867;&#35780;&#20215;&#21453;&#39304;&#30340;&#21407;&#22987;&#25216;&#33021;&#23548;&#21521;&#26426;&#22120;&#20154;&#23398;&#20064;&#26694;&#26550;&#65288;SEED&#65289;&#32467;&#21512;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21644;&#22522;&#20110;&#21407;&#22987;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31232;&#30095;&#22870;&#21169;&#21644;&#38271;&#26102;&#31243;&#20219;&#21153;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15801</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35780;&#20215;&#21453;&#39304;&#30340;&#21407;&#22987;&#25216;&#33021;&#23548;&#21521;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Primitive Skill-based Robot Learning from Human Evaluative Feedback. (arXiv:2307.15801v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15801
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35780;&#20215;&#21453;&#39304;&#30340;&#21407;&#22987;&#25216;&#33021;&#23548;&#21521;&#26426;&#22120;&#20154;&#23398;&#20064;&#26694;&#26550;&#65288;SEED&#65289;&#32467;&#21512;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21644;&#22522;&#20110;&#21407;&#22987;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31232;&#30095;&#22870;&#21169;&#21644;&#38271;&#26102;&#31243;&#20219;&#21153;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#38271;&#26102;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#26102;&#38754;&#20020;&#30528;&#26679;&#26412;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;SEED&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21644;&#22522;&#20110;&#21407;&#22987;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#38382;&#39064;&#21644;&#38271;&#26102;&#31243;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#12290;&#36890;&#36807;&#32467;&#21512;&#23427;&#20204;&#65292;SEED&#20943;&#23569;&#20102;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26426;&#22120;&#20154;&#25805;&#20316;&#25152;&#38656;&#30340;&#20154;&#21147;&#25237;&#20837;&#65292;&#24182;&#25552;&#39640;&#20102;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#21442;&#25968;&#21270;&#25216;&#33021;&#25552;&#20379;&#20102;&#23545;&#20195;&#29702;&#30340;&#39640;&#32423;&#24847;&#22270;&#30340;&#28165;&#26224;&#35270;&#22270;&#65292;&#20801;&#35768;&#20154;&#31867;&#22312;&#25191;&#34892;&#20043;&#21069;&#35780;&#20272;&#25216;&#33021;&#36873;&#25321;&#12290;&#36825;&#20010;&#29305;&#24615;&#20351;&#24471;&#35757;&#32451;&#36807;&#31243;&#26356;&#23433;&#20840;&#12289;&#26356;&#39640;&#25928;&#12290;&#20026;&#20102;&#35780;&#20272;SEED&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) algorithms face significant challenges when dealing with long-horizon robot manipulation tasks in real-world environments due to sample inefficiency and safety issues. To overcome these challenges, we propose a novel framework, SEED, which leverages two approaches: reinforcement learning from human feedback (RLHF) and primitive skill-based reinforcement learning. Both approaches are particularly effective in addressing sparse reward issues and the complexities involved in long-horizon tasks. By combining them, SEED reduces the human effort required in RLHF and increases safety in training robot manipulation with RL in real-world settings. Additionally, parameterized skills provide a clear view of the agent's high-level intentions, allowing humans to evaluate skill choices before they are executed. This feature makes the training process even safer and more efficient. To evaluate the performance of SEED, we conducted extensive experiments on five manipulation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;&#65292;&#36890;&#36807;&#20943;&#23569;&#20010;&#20154;&#20250;&#35758;&#36127;&#25285;&#21644;&#22686;&#21152;&#20250;&#35758;&#36755;&#20986;&#30340;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#20250;&#35758;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.15793</link><description>&lt;p&gt;
&#27010;&#35201;&#12289;&#20142;&#28857;&#21644;&#34892;&#21160;&#39033;&#30446;&#65306;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system. (arXiv:2307.15793v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15793
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;&#65292;&#36890;&#36807;&#20943;&#23569;&#20010;&#20154;&#20250;&#35758;&#36127;&#25285;&#21644;&#22686;&#21152;&#20250;&#35758;&#36755;&#20986;&#30340;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#20250;&#35758;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35758;&#22312;&#24037;&#20316;&#21327;&#35843;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#30340;&#22522;&#30784;&#35774;&#26045;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#21521;&#28151;&#21512;&#21644;&#36828;&#31243;&#24037;&#20316;&#30340;&#36716;&#21464;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20250;&#35758;&#27491;&#22312;&#36716;&#31227;&#21040;&#22312;&#32447;&#35745;&#31639;&#26426;&#23186;&#20307;&#31354;&#38388;&#12290;&#36825;&#23548;&#33268;&#20102;&#26032;&#30340;&#38382;&#39064;&#65288;&#20363;&#22914;&#22312;&#26356;&#19981;&#21560;&#24341;&#20154;&#30340;&#20250;&#35758;&#19978;&#33457;&#36153;&#26356;&#22810;&#30340;&#26102;&#38388;&#65289;&#21644;&#26032;&#30340;&#26426;&#20250;&#65288;&#20363;&#22914;&#33258;&#21160;&#36716;&#24405;/&#23383;&#24149;&#21644;&#24635;&#32467;&#25903;&#25345;&#65289;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24635;&#32467;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36890;&#36807;&#20943;&#23569;&#20010;&#20154;&#30340;&#20250;&#35758;&#36127;&#25285;&#21644;&#22686;&#21152;&#20250;&#35758;&#36755;&#20986;&#30340;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#24615;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#20250;&#35758;&#20307;&#39564;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#31181;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#38271;&#31687;&#36716;&#24405;&#21644;&#26080;&#27861;&#26681;&#25454;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#25429;&#25417;&#21040;&#22810;&#26679;&#30340;&#24635;&#32467;&#38656;&#27714;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#25216;&#26415;&#38480;&#21046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#35774;&#35745;&#12289;&#23454;&#29616;&#24182;&#22312;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20102;&#19968;&#31181;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24605;&#20102;&#20004;&#20010;&#26126;&#26174;&#30340;&#24635;&#32467;&#34920;&#31034;&#26041;&#24335;&#8212;&#8212;&#37325;&#35201;&#20142;&#28857;&#21644;&#32467;&#26500;&#21270;&#30340;&#20998;&#32423;&#20250;&#35758;&#32426;&#35201;&#35270;&#22270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#20123;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meetings play a critical infrastructural role in the coordination of work. In recent years, due to shift to hybrid and remote work, more meetings are moving to online Computer Mediated Spaces. This has led to new problems (e.g. more time spent in less engaging meetings) and new opportunities (e.g. automated transcription/captioning and recap support). Recent advances in large language models (LLMs) for dialog summarization have the potential to improve the experience of meetings by reducing individuals' meeting load and increasing the clarity and alignment of meeting outputs. Despite this potential, they face technological limitation due to long transcripts and inability to capture diverse recap needs based on user's context. To address these gaps, we design, implement and evaluate in-context a meeting recap system. We first conceptualize two salient recap representations -- important highlights, and a structured, hierarchical minutes view. We develop a system to operationalize the rep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15780</link><description>&lt;p&gt;
LLM-Rec: &#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;LLM-Rec&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65306;&#65288;1&#65289;&#22522;&#30784;&#24341;&#23548;&#65292;&#65288;2&#65289;&#25512;&#33616;&#39537;&#21160;&#24341;&#23548;&#65292;&#65288;3&#65289;&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#65292;&#21644;&#65288;4&#65289;&#25512;&#33616;&#39537;&#21160;+&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#21407;&#22987;&#20869;&#23481;&#25551;&#36848;&#19982;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#36755;&#20837;&#25991;&#26412;&#32467;&#21512;&#36215;&#26469;&#65292;&#37319;&#29992;&#36825;&#20123;&#24341;&#23548;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that combining the original content description with the augmented input text generated by LLM using these prompting strategies leads to improved recommendation performance. This finding highlights the importance of incorporating diverse prompts and input augmentation techniques to enhance the recommendation capabilities with large language models for personalized content recommendation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#25506;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#21457;&#29616;&#20102;Hydra&#25928;&#24212;&#21644;&#26202;&#26399;MLP&#23618;&#30340;&#24179;&#34913;&#21151;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.15771</link><description>&lt;p&gt;
Hydra&#25928;&#24212;&#65306;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#20013;&#30340;&#33258;&#36866;&#24212;&#33258;&#20462;&#22797;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Hydra Effect: Emergent Self-repair in Language Model Computations. (arXiv:2307.15771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#25506;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#21457;&#29616;&#20102;Hydra&#25928;&#24212;&#21644;&#26202;&#26399;MLP&#23618;&#30340;&#24179;&#34913;&#21151;&#33021;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22240;&#26524;&#20998;&#26512;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#31181;&#27169;&#24335;&#65306;&#65288;1&#65289;&#19968;&#31181;&#33258;&#36866;&#24212;&#35745;&#31639;&#24418;&#24335;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26576;&#19968;&#33258;&#27880;&#24847;&#23618;&#34987;&#21024;&#20943;&#21518;&#21478;&#19968;&#23618;&#36827;&#34892;&#34917;&#20607;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;Hydra&#25928;&#24212;&#65289;&#65307;&#65288;2&#65289;&#22312;&#21518;&#26399;&#22810;&#23618;&#24863;&#30693;&#26426;&#23618;&#20013;&#23384;&#22312;&#30340;&#24179;&#34913;&#21151;&#33021;&#65292;&#29992;&#20110;&#35843;&#33410;&#26368;&#22823;&#20284;&#28982;&#20196;&#29260;&#12290;&#25105;&#20204;&#30340;&#21024;&#20943;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#23618;&#20043;&#38388;&#36890;&#24120;&#30456;&#23545;&#26494;&#25955;&#32806;&#21512;&#65288;&#23545;&#19968;&#23618;&#30340;&#21024;&#20943;&#21482;&#20250;&#24433;&#21709;&#19968;&#23567;&#37096;&#20998;&#19979;&#28216;&#23618;&#65289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#24418;&#24335;&#30340;&#38543;&#26426;&#22833;&#27963;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36825;&#20123;&#25928;&#24212;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#22312;&#20107;&#23454;&#22238;&#24518;&#30340;&#32972;&#26223;&#19979;&#20998;&#26512;&#20102;&#36825;&#20123;&#25928;&#24212;&#65292;&#24182;&#32771;&#34385;&#20102;&#23427;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#30005;&#36335;&#23618;&#38754;&#24402;&#22240;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the internal structure of language model computations using causal analysis and demonstrate two motifs: (1) a form of adaptive computation where ablations of one attention layer of a language model cause another layer to compensate (which we term the Hydra effect) and (2) a counterbalancing function of late MLP layers that act to downregulate the maximum-likelihood token. Our ablation studies demonstrate that language model layers are typically relatively loosely coupled (ablations to one layer only affect a small number of downstream layers). Surprisingly, these effects occur even in language models trained without any form of dropout. We analyse these effects in the context of factual recall and consider their implications for circuit-level attribution in language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ChatReport&#30340;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#21487;&#36861;&#28335;&#30340;&#31572;&#26696;&#21644;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#20302;&#25928;&#24615;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.15770</link><description>&lt;p&gt;
CHATREPORT&#65306;&#36890;&#36807;&#22522;&#20110;LLM&#24037;&#20855;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools. (arXiv:2307.15770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ChatReport&#30340;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#21487;&#36861;&#28335;&#30340;&#31572;&#26696;&#21644;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#20302;&#25928;&#24615;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25259;&#38706;&#20998;&#26512;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#20844;&#21496;&#30495;&#30340;&#22312;&#26397;&#30528;&#26356;&#21487;&#25345;&#32493;&#32463;&#33829;&#36808;&#20986;&#23454;&#36136;&#24615;&#30340;&#27493;&#20240;&#21527;&#65311;&#19968;&#20010;&#20840;&#38754;&#30340;&#31572;&#26696;&#21487;&#20197;&#22312;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#30340;&#23494;&#38598;&#20449;&#24687;&#20013;&#25214;&#21040;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25253;&#21578;&#30340;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#20351;&#20154;&#24037;&#20998;&#26512;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#21482;&#26377;&#23569;&#25968;&#30340;&#26426;&#26500;&#25317;&#26377;&#36164;&#28304;&#33021;&#22815;&#22823;&#35268;&#27169;&#20998;&#26512;&#36825;&#20123;&#25253;&#21578;&#65292;&#36825;&#23548;&#33268;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#36890;&#36807;&#22522;&#20110;LLM&#33258;&#21160;&#20998;&#26512;&#24037;&#20855;&#36171;&#33021;&#21033;&#30410;&#30456;&#20851;&#32773;&#21487;&#33021;&#26159;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20998;&#26512;&#27665;&#20027;&#21270;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#26679;&#30340;&#24037;&#20855;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;LLM&#30340;&#24187;&#35273;&#38382;&#39064;&#21644;&#23558;&#39046;&#22495;&#19987;&#23478;&#24341;&#20837;AI&#24320;&#21457;&#36807;&#31243;&#30340;&#20302;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatReport&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26032;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#36890;&#36807;&#20351;&#31572;&#26696;&#21487;&#36861;&#28335;&#26469;&#20943;&#23569;&#24187;&#35273;&#30340;&#21361;&#23475;&#65292;&#24182;&#35299;&#20915;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;AI&#24320;&#21457;&#36807;&#31243;&#30340;&#20302;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) a
&lt;/p&gt;</description></item><item><title>AI&#29992;&#20110;&#39044;&#27979;&#34892;&#21160;&#20174;&#27668;&#20505;&#39044;&#27979;&#21521;&#39044;&#27979;&#34892;&#21160;&#36716;&#21464;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#22312;&#35780;&#20272;&#27668;&#20505;&#23545;&#29305;&#23450;&#20154;&#21475;&#30340;&#24433;&#21709;&#26041;&#38754;&#22635;&#34917;&#20102;&#26041;&#27861;&#35770;&#19978;&#30340;&#24046;&#36317;&#65292;&#20197;&#25512;&#36827;&#23545;&#27668;&#20505;&#21464;&#21270;&#26368;&#33030;&#24369;&#20154;&#21475;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.15727</link><description>&lt;p&gt;
AI&#29992;&#20110;&#39044;&#27979;&#34892;&#21160;&#65306;&#36229;&#36234;&#27668;&#20505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI for Anticipatory Action: Moving Beyond Climate Forecasting. (arXiv:2307.15727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15727
&lt;/p&gt;
&lt;p&gt;
AI&#29992;&#20110;&#39044;&#27979;&#34892;&#21160;&#20174;&#27668;&#20505;&#39044;&#27979;&#21521;&#39044;&#27979;&#34892;&#21160;&#36716;&#21464;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#22312;&#35780;&#20272;&#27668;&#20505;&#23545;&#29305;&#23450;&#20154;&#21475;&#30340;&#24433;&#21709;&#26041;&#38754;&#22635;&#34917;&#20102;&#26041;&#27861;&#35770;&#19978;&#30340;&#24046;&#36317;&#65292;&#20197;&#25512;&#36827;&#23545;&#27668;&#20505;&#21464;&#21270;&#26368;&#33030;&#24369;&#20154;&#21475;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28798;&#23475;&#24212;&#23545;&#26426;&#26500;&#27491;&#20174;&#27668;&#20505;&#39044;&#27979;&#33539;&#24335;&#36716;&#21521;&#39044;&#27979;&#34892;&#21160;&#33539;&#24335;&#65306;&#19981;&#20165;&#35780;&#20272;&#27668;&#20505;&#23558;&#22914;&#20309;&#65292;&#32780;&#19988;&#35780;&#20272;&#27668;&#20505;&#23545;&#29305;&#23450;&#20154;&#21475;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#20027;&#21160;&#21709;&#24212;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#27668;&#20505;&#39044;&#27979;&#26041;&#38754;&#21464;&#24471;&#24322;&#24120;&#24378;&#22823;&#65292;&#20294;&#22312;&#20419;&#36827;&#39044;&#27979;&#34892;&#21160;&#26041;&#38754;&#20173;&#23384;&#22312;&#26041;&#27861;&#35770;&#19978;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#39044;&#27979;&#34892;&#21160;&#30340;&#27010;&#36848;&#65292;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#30456;&#20851;&#24212;&#29992;&#65292;&#35782;&#21035;&#20102;&#20849;&#21516;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#25512;&#36827;&#23545;&#27668;&#20505;&#21464;&#21270;&#26368;&#33030;&#24369;&#20154;&#21475;&#30340;&#28798;&#23475;&#21709;&#24212;&#26041;&#38754;&#30340;&#29420;&#29305;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disaster response agencies have been shifting from a paradigm of climate forecasting towards one of anticipatory action: assessing not just what the climate will be, but how it will impact specific populations, thereby enabling proactive response and resource allocation. Machine learning models are becoming exceptionally powerful at climate forecasting, but methodological gaps remain in terms of facilitating anticipatory action. Here we provide an overview of anticipatory action, review relevant applications of machine learning, identify common challenges, and highlight areas where machine learning can uniquely contribute to advancing disaster response for populations most vulnerable to climate change.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#37324;&#31243;&#25968;&#25454;&#29983;&#25104;&#36866;&#24403;&#30340;&#30005;&#26426;&#36895;&#24230;&#26469;&#23454;&#29616;&#33258;&#20027;&#23398;&#20064;&#20197;&#25511;&#21046;&#39134;&#34892;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#65292;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#21487;&#20197;&#22312;&#25511;&#21046;&#20559;&#33322;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#20301;&#32622;&#30340;&#21516;&#26102;&#36890;&#36807;&#38556;&#30861;&#29289;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#26032;&#30340;&#22909;&#22855;&#24515;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15724</link><description>&lt;p&gt;
&#22522;&#20110;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20302;&#32423;&#39134;&#34892;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Curiosity-Driven Reinforcement Learning based Low-Level Flight Control. (arXiv:2307.15724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#37324;&#31243;&#25968;&#25454;&#29983;&#25104;&#36866;&#24403;&#30340;&#30005;&#26426;&#36895;&#24230;&#26469;&#23454;&#29616;&#33258;&#20027;&#23398;&#20064;&#20197;&#25511;&#21046;&#39134;&#34892;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#65292;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#21487;&#20197;&#22312;&#25511;&#21046;&#20559;&#33322;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#20301;&#32622;&#30340;&#21516;&#26102;&#36890;&#36807;&#38556;&#30861;&#29289;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#26032;&#30340;&#22909;&#22855;&#24515;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22909;&#22855;&#24515;&#26159;&#35768;&#22810;&#26377;&#26234;&#33021;&#30340;&#33258;&#28982;&#29983;&#29289;&#30340;&#20027;&#35201;&#21160;&#26426;&#20043;&#19968;&#65292;&#36890;&#36807;&#20855;&#26377;&#21487;&#34913;&#37327;&#26234;&#33021;&#27700;&#24179;&#30340;&#25506;&#32034;&#26469;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;&#23427;&#20351;&#20154;&#31867;&#21644;&#35768;&#22810;&#21160;&#29289;&#33021;&#22815;&#36890;&#36807;&#23547;&#25214;&#35753;&#20182;&#20204;&#24863;&#21040;&#24778;&#35766;&#30340;&#29366;&#24577;&#26469;&#39640;&#25928;&#22320;&#25506;&#32034;&#65292;&#20197;&#20415;&#23398;&#20064;&#26356;&#22810;&#20182;&#20204;&#25152;&#19981;&#30693;&#36947;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#22312;&#20445;&#25345;&#22909;&#22855;&#24515;&#30340;&#21516;&#26102;&#65292;&#20182;&#20204;&#33021;&#22815;&#23398;&#24471;&#26356;&#22909;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#22909;&#22855;&#24515;&#36890;&#24120;&#34987;&#32467;&#21512;&#21040;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#20013;&#65292;&#20316;&#20026;&#19968;&#31181;&#20869;&#22312;&#30340;&#22870;&#21169;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#37324;&#31243;&#25968;&#25454;&#29983;&#25104;&#36866;&#24403;&#30340;&#30005;&#26426;&#36895;&#24230;&#26469;&#23454;&#29616;&#33258;&#20027;&#23398;&#20064;&#20197;&#25511;&#21046;&#39134;&#34892;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#21487;&#20197;&#22312;&#25511;&#21046;&#20559;&#33322;&#26041;&#21521;&#26397;&#21521;&#25152;&#38656;&#20301;&#32622;&#30340;&#21516;&#26102;&#36890;&#36807;&#38556;&#30861;&#29289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#30340;&#26032;&#30340;&#22909;&#22855;&#24515;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22312;&#32447;&#31574;&#30053;&#12289;&#31163;&#32447;&#31574;&#30053;&#21644;&#22312;&#32447;&#31574;&#30053;&#21152;&#22122;&#38899;&#30340;&#27979;&#35797;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curiosity is one of the main motives in many of the natural creatures with measurable levels of intelligence for exploration and, as a result, more efficient learning. It makes it possible for humans and many animals to explore efficiently by searching for being in states that make them surprised with the goal of learning more about what they do not know. As a result, while being curious, they learn better. In the machine learning literature, curiosity is mostly combined with reinforcement learning-based algorithms as an intrinsic reward. This work proposes an algorithm based on the drive of curiosity for autonomous learning to control by generating proper motor speeds from odometry data. The quadcopter controlled by our proposed algorithm can pass through obstacles while controlling the Yaw direction of the quad-copter toward the desired location. To achieve that, we also propose a new curiosity approach based on prediction error. We ran tests using on-policy, off-policy, on-policy pl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#27169;&#25311;&#30149;&#27602;&#25193;&#23637;&#65292;&#24182;&#32467;&#21512;&#20010;&#20307;&#29305;&#24449;&#21644;&#20154;&#21475;&#34892;&#20026;&#65292;&#20197;&#35780;&#20272;&#36943;&#21046;&#25514;&#26045;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.15723</link><description>&lt;p&gt;
&#22522;&#20110;&#25509;&#21463;&#36943;&#21046;&#25514;&#26045;&#30340;&#30149;&#27602;&#25193;&#23637;&#30340;&#20195;&#29702;&#27169;&#22411;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Agent-Based Model: Simulating a Virus Expansion Based on the Acceptance of Containment Measures. (arXiv:2307.15723v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#27169;&#25311;&#30149;&#27602;&#25193;&#23637;&#65292;&#24182;&#32467;&#21512;&#20010;&#20307;&#29305;&#24449;&#21644;&#20154;&#21475;&#34892;&#20026;&#65292;&#20197;&#35780;&#20272;&#36943;&#21046;&#25514;&#26045;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38548;&#31163;&#24615;&#27969;&#34892;&#30149;&#27169;&#22411;&#26681;&#25454;&#20010;&#20307;&#30340;&#30142;&#30149;&#29366;&#24577;&#23558;&#20854;&#20998;&#31867;&#65292;&#22914;SEIRD&#27169;&#22411;&#65288;&#26131;&#24863;-&#26292;&#38706;-&#24863;&#26579;-&#24247;&#22797;-&#27515;&#20129;&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#30830;&#23450;&#20102;&#24433;&#21709;&#29190;&#21457;&#35268;&#27169;&#30340;&#21442;&#25968;&#65292;&#22914;&#20256;&#26579;&#21644;&#24247;&#22797;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#20010;&#20307;&#29305;&#24449;&#25110;&#20154;&#21475;&#34892;&#20026;&#65292;&#36825;&#23545;&#35780;&#20272;&#32531;&#35299;&#31574;&#30053;&#65288;&#22914;COVID-19&#20013;&#30340;&#21475;&#32617;&#20351;&#29992;&#25110;HIV&#20013;&#30340;&#36991;&#23381;&#22871;&#20998;&#21457;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#24378;&#35843;&#20844;&#27665;&#22242;&#32467;&#12289;&#20154;&#38469;&#20449;&#20219;&#21644;&#25919;&#24220;&#20449;&#35465;&#22312;&#35299;&#37322;&#22269;&#23478;&#20043;&#38388;&#20256;&#26579;&#29575;&#24046;&#24322;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#20195;&#29702;&#24314;&#27169;&#65288;ABM&#65289;&#36890;&#36807;&#27169;&#25311;&#20010;&#20307;&#32452;&#25104;&#37096;&#20998;&#12289;&#20854;&#34892;&#20026;&#21644;&#29615;&#22659;&#20869;&#30340;&#30456;&#20114;&#20316;&#29992;&#25552;&#20379;&#20102;&#30740;&#31350;&#22797;&#26434;&#31995;&#32479;&#30340;&#26377;&#20215;&#20540;&#26041;&#27861;&#12290;ABM&#20026;&#20998;&#26512;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36866;&#24212;&#30340;SEIRD&#27169;&#22411;&#19982;&#20915;&#31574;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;ABM&#26550;&#26500;&#65292;&#20197;&#27169;&#25311;&#30149;&#27602;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compartmental epidemiological models categorize individuals based on their disease status, such as the SEIRD model (Susceptible-Exposed-Infected-Recovered-Dead). These models determine the parameters that influence the magnitude of an outbreak, such as contagion and recovery rates. However, they don't account for individual characteristics or population actions, which are crucial for assessing mitigation strategies like mask usage in COVID-19 or condom distribution in HIV. Additionally, studies highlight the role of citizen solidarity, interpersonal trust, and government credibility in explaining differences in contagion rates between countries. Agent-Based Modeling (ABM) offers a valuable approach to study complex systems by simulating individual components, their actions, and interactions within an environment. ABM provides a useful tool for analyzing social phenomena. In this study, we propose an ABM architecture that combines an adapted SEIRD model with a decision-making model for 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#65292;&#26088;&#22312;&#19982;&#33647;&#29702;&#23398;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.15717</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33647;&#29702;&#23398;&#25968;&#25454;&#24211;&#30340;&#33258;&#28982;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Utilizing Large Language Models for Natural Interface to Pharmacology Databases. (arXiv:2307.15717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#65292;&#26088;&#22312;&#19982;&#33647;&#29702;&#23398;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#24320;&#21457;&#36807;&#31243;&#38656;&#35201;&#33647;&#29702;&#23398;&#23478;&#36827;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#22914;&#26597;&#38405;&#25991;&#29486;&#65292;&#25552;&#20986;&#20551;&#35774;&#65292;&#35774;&#35745;&#23454;&#39564;&#21644;&#35299;&#37322;&#32467;&#26524;&#12290;&#27599;&#20010;&#38454;&#27573;&#37117;&#38656;&#35201;&#35775;&#38382;&#21644;&#26597;&#35810;&#22823;&#37327;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#65292;&#26088;&#22312;&#19982;&#23384;&#20648;&#22312;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#25512;&#24191;&#21040;&#26597;&#35810;&#21508;&#31181;&#33647;&#29289;&#25968;&#25454;&#21644;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The drug development process necessitates that pharmacologists undertake various tasks, such as reviewing literature, formulating hypotheses, designing experiments, and interpreting results. Each stage requires accessing and querying vast amounts of information. In this abstract, we introduce a Large Language Model (LLM)-based Natural Language Interface designed to interact with structured information stored in databases. Our experiments demonstrate the feasibility and effectiveness of the proposed framework. This framework can generalize to query a wide range of pharmaceutical data and knowledge bases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#25688;&#35201;&#36827;&#34892;&#26497;&#31471;&#25688;&#35201;&#21270;&#65292;&#26088;&#22312;&#24110;&#21161;&#21021;&#32423;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20943;&#36731;&#35748;&#30693;&#36127;&#33655;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24037;&#20316;&#25928;&#29575;&#21644;&#20943;&#23569;&#24515;&#29702;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2307.15715</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26497;&#31471;&#25688;&#35201;&#21270;&#31185;&#23398;&#25991;&#29486;&#26469;&#25913;&#36827;&#21021;&#32423;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Improving Primary Healthcare Workflow Using Extreme Summarization of Scientific Literature Based on Generative AI. (arXiv:2307.15715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#25688;&#35201;&#36827;&#34892;&#26497;&#31471;&#25688;&#35201;&#21270;&#65292;&#26088;&#22312;&#24110;&#21161;&#21021;&#32423;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20943;&#36731;&#35748;&#30693;&#36127;&#33655;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24037;&#20316;&#25928;&#29575;&#21644;&#20943;&#23569;&#24515;&#29702;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21021;&#32423;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#22312;&#25351;&#23548;&#20197;&#35777;&#25454;&#20026;&#22522;&#30784;&#30340;&#23454;&#36341;&#26041;&#38754;&#65292;&#22914;&#20309;&#36319;&#19978;&#26368;&#26032;&#30340;&#31185;&#23398;&#25991;&#29486;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#23545;&#31185;&#23398;&#35770;&#25991;&#30340;&#25688;&#35201;&#36827;&#34892;&#24635;&#32467;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20943;&#23569;&#20174;&#19994;&#32773;&#35748;&#30693;&#36127;&#33655;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#25506;&#32034;&#20854;&#20943;&#36731;&#24515;&#29702;&#21162;&#21147;&#21644;&#36127;&#25285;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21442;&#19982;&#32773;&#25509;&#21463;&#19982;&#39044;&#38450;&#20445;&#20581;&#21644;&#34892;&#20026;&#25913;&#21464;&#30456;&#20851;&#30340;&#20004;&#20010;&#26696;&#20363;&#65292;&#27169;&#25311;&#23545;&#26032;&#30340;&#31185;&#23398;&#25991;&#29486;&#36827;&#34892;&#25628;&#32034;&#12290;&#30740;&#31350;&#21253;&#25324;&#26469;&#33258;&#26031;&#27931;&#25991;&#23612;&#20122;&#21644;&#32654;&#22269;&#30340;113&#21517;&#22823;&#23398;&#29983;&#65292;&#34987;&#38543;&#26426;&#20998;&#25104;&#19977;&#20010;&#19981;&#21516;&#30340;&#30740;&#31350;&#32452;&#12290;&#31532;&#19968;&#32452;&#34987;&#20998;&#37197;&#38405;&#35835;&#23436;&#25972;&#30340;&#25688;&#35201;&#65292;&#31532;&#20108;&#32452;&#34987;&#20998;&#37197;&#38405;&#35835;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#30701;&#25688;&#35201;&#65292;&#31532;&#19977;&#32452;&#26377;&#36873;&#25321;&#24615;&#22320;&#38405;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;
Primary care professionals struggle to keep up to date with the latest scientific literature critical in guiding evidence-based practice related to their daily work. To help solve the above-mentioned problem, we employed generative artificial intelligence techniques based on large-scale language models to summarize abstracts of scientific papers. Our objective is to investigate the potential of generative artificial intelligence in diminishing the cognitive load experienced by practitioners, thus exploring its ability to alleviate mental effort and burden. The study participants were provided with two use cases related to preventive care and behavior change, simulating a search for new scientific literature. The study included 113 university students from Slovenia and the United States randomized into three distinct study groups. The first group was assigned to the full abstracts. The second group was assigned to the short abstracts generated by AI. The third group had the option to se
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#25513;&#30721;&#22256;&#38590;&#23454;&#20363;&#25366;&#25496;&#30340;&#22810;&#31034;&#20363;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#23398;&#20064;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#32422;&#26463;&#26469;&#25506;&#32034;&#28508;&#22312;&#30340;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#21160;&#37327;&#25945;&#24072;&#38544;&#24335;&#25366;&#25496;&#36825;&#20123;&#23454;&#20363;&#26469;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15254</link><description>&lt;p&gt;
&#24102;&#26377;&#25513;&#30721;&#22256;&#38590;&#23454;&#20363;&#25366;&#25496;&#30340;&#22810;&#31034;&#20363;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification. (arXiv:2307.15254v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15254
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#25513;&#30721;&#22256;&#38590;&#23454;&#20363;&#25366;&#25496;&#30340;&#22810;&#31034;&#20363;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#23398;&#20064;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#32422;&#26463;&#26469;&#25506;&#32034;&#28508;&#22312;&#30340;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#21160;&#37327;&#25945;&#24072;&#38544;&#24335;&#25366;&#25496;&#36825;&#20123;&#23454;&#20363;&#26469;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#20998;&#31867;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#38382;&#39064;&#12290;&#30001;&#20110;&#38451;&#24615;&#32452;&#32455;&#20165;&#21344;&#20102;&#21513;&#27604;&#20687;&#32032;WSI&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#29616;&#26377;&#30340;MIL&#26041;&#27861;&#30452;&#35266;&#22320;&#20391;&#37325;&#20110;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#35782;&#21035;&#26174;&#33879;&#23454;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#23548;&#33268;&#20559;&#21521;&#26131;&#20110;&#20998;&#31867;&#30340;&#23454;&#20363;&#65292;&#24573;&#35270;&#20102;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#12290;&#19968;&#20123;&#25991;&#29486;&#25581;&#31034;&#20102;&#22256;&#38590;&#31034;&#20363;&#23545;&#20110;&#20934;&#30830;&#24314;&#27169;&#36793;&#30028;&#26159;&#26377;&#30410;&#30340;&#12290;&#36890;&#36807;&#23558;&#36825;&#19968;&#24605;&#24819;&#24212;&#29992;&#21040;&#23454;&#20363;&#32423;&#21035;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;MIL&#26694;&#26550;&#65292;&#21363;&#24102;&#26377;&#25513;&#30721;&#22256;&#38590;&#23454;&#20363;&#25366;&#25496;&#30340;MIL&#65288;MHIM-MIL&#65289;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#20849;&#20139;&#23398;&#20064;&#32467;&#26500;&#65288;&#25945;&#24072;-&#23398;&#29983;&#65289;&#21644;&#19968;&#33268;&#24615;&#32422;&#26463;&#26469;&#25506;&#32034;&#28508;&#22312;&#30340;&#22256;&#38590;&#23454;&#20363;&#12290;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#20998;&#25968;&#30340;&#22810;&#20010;&#23454;&#20363;&#25513;&#30721;&#31574;&#30053;&#65292;MHIM-MIL&#37319;&#29992;&#21160;&#37327;&#25945;&#24072;&#26469;&#38544;&#24335;&#25366;&#25496;&#29992;&#20110;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#30340;&#22256;&#38590;&#23454;&#20363;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#26159;&#20219;&#20309;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;MIL&#27169;&#22411;&#12290;&#36825;&#20010;&#21453;&#30452;&#35273;&#30340;&#31574;&#30053;&#23545;&#20110;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The whole slide image (WSI) classification is often formulated as a multiple instance learning (MIL) problem. Since the positive tissue is only a small fraction of the gigapixel WSI,existing MIL methods intuitively focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting hard-to-classify instances.Some literature has revealed that hard examples are beneficial for modeling a discriminative boundary accurately.By applying such an idea at the instance level,we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which uses a Siamese structure (Teacher-Student) with a consistency constraint to explore the potential hard instances. With several instance masking strategies based on attention scores, MHIM-MIL employs a momentum teacher to implicitly mine hard instances for training the student model, which can be any attention-based MIL model.This counter-intuitive strategy essent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#20449;&#24687;&#25552;&#21319;&#30340;&#26041;&#27861;&#36827;&#34892;&#23376;&#32676;&#21457;&#29616;&#12290;&#20855;&#20307;&#38024;&#23545;&#32954;&#30284;&#27835;&#30103;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#21103;&#20316;&#29992;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#38750;&#24120;&#37325;&#35201;&#65292;&#20020;&#24202;&#25351;&#21335;&#34429;&#28982;&#25552;&#20379;&#20102;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#20173;&#26410;&#23558;&#27835;&#30103;&#32467;&#26524;&#32435;&#20837;&#32771;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15089</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#25552;&#21319;&#23376;&#32676;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Information Gained Subgroup Discovery in Datasets. (arXiv:2307.15089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#20449;&#24687;&#25552;&#21319;&#30340;&#26041;&#27861;&#36827;&#34892;&#23376;&#32676;&#21457;&#29616;&#12290;&#20855;&#20307;&#38024;&#23545;&#32954;&#30284;&#27835;&#30103;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#21103;&#20316;&#29992;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#38750;&#24120;&#37325;&#35201;&#65292;&#20020;&#24202;&#25351;&#21335;&#34429;&#28982;&#25552;&#20379;&#20102;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#20173;&#26410;&#23558;&#27835;&#30103;&#32467;&#26524;&#32435;&#20837;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#30284;&#30151;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#39044;&#35745;2023&#24180;&#23558;&#26377;&#36229;&#36807;238,340&#20363;&#26032;&#30340;&#32954;&#30284;&#24739;&#32773;&#65292;&#20854;&#20013;&#26377;&#36229;&#36807;127,070&#20363;&#27515;&#20129;&#12290;&#36873;&#25321;&#27491;&#30830;&#30340;&#27835;&#30103;&#26041;&#26696;&#26159;&#25552;&#39640;&#23384;&#27963;&#29575;&#21644;&#25913;&#21892;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#30284;&#30151;&#27835;&#30103;&#21487;&#33021;&#24341;&#21457;&#21103;&#20316;&#29992;&#65292;&#36825;&#20123;&#27602;&#21103;&#21453;&#24212;&#20250;&#24341;&#36215;&#19981;&#21516;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24433;&#21709;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;&#27835;&#30103;&#25928;&#26524;&#30340;&#21516;&#26102;&#20943;&#23569;&#27835;&#30103;&#21103;&#20316;&#29992;&#26159;&#20020;&#24202;&#35282;&#24230;&#35201;&#36861;&#27714;&#30340;&#37325;&#35201;&#30446;&#26631;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20020;&#24202;&#25351;&#21335;&#21253;&#25324;&#30284;&#30151;&#27835;&#30103;&#24314;&#35758;&#30340;&#19968;&#33324;&#30693;&#35782;&#65292;&#20197;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#12290;&#23613;&#31649;&#20182;&#20204;&#26681;&#25454;&#30284;&#30151;&#30142;&#30149;&#26041;&#38754;&#21644;&#20010;&#20307;&#24739;&#32773;&#29305;&#24449;&#25552;&#20379;&#27835;&#30103;&#24314;&#35758;&#65292;&#20294;&#24182;&#26410;&#25552;&#20379;&#22522;&#20110;&#27835;&#30103;&#32467;&#26524;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#23545;&#20020;&#24202;&#25351;&#21335;&#19982;&#27835;&#30103;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is the leading cause of cancer death. More than 238,340 new cases of lung cancer patients are expected in 2023, with an estimation of more than 127,070 deaths. Choosing the correct treatment is an important element to enhance the probability of survival and to improve patient's quality of life. Cancer treatments might provoke secondary effects. These toxicities cause different health problems that impact the patient's quality of life. Hence, reducing treatments toxicities while maintaining or improving their effectivenes is an important goal that aims to be pursued from the clinical perspective. On the other hand, clinical guidelines include general knowledge about cancer treatment recommendations to assist clinicians. Although they provide treatment recommendations based on cancer disease aspects and individual patient features, a statistical analysis taking into account treatment outcomes is not provided here. Therefore, the comparison between clinical guidelines with tre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;&#65288;AAEs&#65289;&#8221;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#35770;&#35777;&#23545;&#8220;&#20027;&#39064;&#35770;&#35777;&#8221;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#22312;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#65288;QBAFs&#65289;&#20013;&#22635;&#34917;&#20102;&#35299;&#37322;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.13582</link><description>&lt;p&gt;
&#22312;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#20013;&#30340;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Argument Attribution Explanations in Quantitative Bipolar Argumentation Frameworks. (arXiv:2307.13582v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;&#65288;AAEs&#65289;&#8221;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#35770;&#35777;&#23545;&#8220;&#20027;&#39064;&#35770;&#35777;&#8221;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#22312;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#65288;QBAFs&#65289;&#20013;&#22635;&#34917;&#20102;&#35299;&#37322;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26377;&#20960;&#20010;&#20154;&#25552;&#20513;&#35770;&#35777;&#24615;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#35770;&#35777;&#26694;&#26550;&#65288;AFs&#65289;&#30340;&#25512;&#29702;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#34429;&#28982;&#20851;&#20110;&#29992;&#36777;&#35770;/&#20105;&#35770;/&#23545;&#35805;&#30340;&#25193;&#23637;&#35821;&#20041;&#31934;&#31070;&#23450;&#24615;&#22320;&#35299;&#37322;AFs&#30340;&#25512;&#29702;&#32467;&#26524;&#30340;&#30740;&#31350;&#25104;&#26524;&#24456;&#22810;&#65292;&#20294;&#26159;&#22312;&#28176;&#36827;&#35821;&#20041;&#19979;&#35299;&#37322;AFs&#30340;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#21364;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#65292;&#23613;&#31649;&#22312;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#24402;&#22240;&#31934;&#31070;&#24341;&#20837;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#65288;QBAFs&#65289;&#30340;&#32972;&#26223;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;&#65288;AAEs&#65289;&#8221;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#35770;&#35777;&#23545;&#8220;&#20027;&#39064;&#35770;&#35777;&#8221;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#32780;&#29305;&#24449;&#24402;&#22240;&#21017;&#29992;&#20110;&#30830;&#23450;&#29305;&#24449;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Argumentative explainable AI has been advocated by several in recent years, with an increasing interest on explaining the reasoning outcomes of Argumentation Frameworks (AFs). While there is a considerable body of research on qualitatively explaining the reasoning outcomes of AFs with debates/disputes/dialogues in the spirit of \emph{extension-based semantics}, explaining the quantitative reasoning outcomes of AFs under \emph{gradual semantics} has not received much attention, despite widespread use in applications. In this paper, we contribute to filling this gap by proposing a novel theory of \emph{Argument Attribution Explanations (AAEs)} by incorporating the spirit of feature attribution from machine learning in the context of Quantitative Bipolar Argumentation Frameworks (QBAFs): whereas feature attribution is used to determine the influence of features towards outputs of machine learning models, AAEs are used to determine the influence of arguments towards \emph{topic argument}s 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#65292;&#20197;&#35299;&#20915;&#31243;&#24207;&#27604;&#36739;&#12289;&#20998;&#26512;&#12289;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#22312;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21464;&#37327;&#26144;&#23556;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13014</link><description>&lt;p&gt;
&#29992;&#20110;&#31243;&#24207;&#20043;&#38388;&#21464;&#37327;&#26144;&#23556;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#8212;&#8212;&#25193;&#23637;&#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks For Mapping Variables Between Programs -- Extended Version. (arXiv:2307.13014v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#65292;&#20197;&#35299;&#20915;&#31243;&#24207;&#27604;&#36739;&#12289;&#20998;&#26512;&#12289;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#12290;&#22312;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21464;&#37327;&#26144;&#23556;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20998;&#26512;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#35768;&#22810;&#39046;&#22495;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#24418;&#24335;&#26041;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#12290;&#30001;&#20110;&#31243;&#24207;&#31561;&#20215;&#38382;&#39064;&#30340;&#19981;&#21487;&#21028;&#23450;&#24615;&#65292;&#27604;&#36739;&#20004;&#20010;&#31243;&#24207;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#24120;&#65292;&#20026;&#20102;&#27604;&#36739;&#20004;&#20010;&#31243;&#24207;&#65292;&#38656;&#35201;&#23545;&#20004;&#20010;&#31243;&#24207;&#30340;&#21464;&#37327;&#38598;&#20043;&#38388;&#24314;&#31435;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#22312;&#35832;&#22914;&#31243;&#24207;&#31561;&#20215;&#24615;&#12289;&#31243;&#24207;&#20998;&#26512;&#12289;&#31243;&#24207;&#20462;&#22797;&#21644;&#20811;&#38534;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#65292;&#26144;&#23556;&#20004;&#20010;&#31243;&#24207;&#20043;&#38388;&#30340;&#21464;&#37327;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22522;&#20110;&#20004;&#20010;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;(ASTs)&#26469;&#26144;&#23556;&#21464;&#37327;&#38598;&#12290;&#20026;&#20102;&#23637;&#31034;&#21464;&#37327;&#26144;&#23556;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#22312;&#31243;&#24207;&#20462;&#22797;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#36825;&#20123;&#26144;&#23556;&#30340;&#19977;&#20010;&#29992;&#20363;&#65292;&#20197;&#20462;&#22797;&#21021;&#23398;&#32773;&#32534;&#31243;&#20316;&#19994;&#20013;&#24120;&#35265;&#30340;&#21644;&#32463;&#24120;&#21457;&#29983;&#30340;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19968;&#20010;&#21253;&#21547;4166&#23545;&#38169;&#35823;/&#20462;&#27491;&#31243;&#24207;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/corr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11788</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Applying QNLP to sentiment analysis in finance. (arXiv:2307.11788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#21363;&#20351;&#26159;&#26368;&#24494;&#23567;&#30340;&#36136;&#37327;&#25913;&#36827;&#20063;&#33021;&#20135;&#29983;&#24040;&#22823;&#20215;&#20540;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#37329;&#34701;&#26159;&#26089;&#26399;&#37327;&#23376;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#32773;&#12290;&#22312;&#36805;&#36895;&#21457;&#23637;&#30340;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;DisCoCat&#21644;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QNLP)&#36825;&#20004;&#31181;&#20013;&#24515;&#26041;&#27861;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ChatGPT&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21253;&#21547;1000&#22810;&#20010;&#30495;&#23454;&#21477;&#23376;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;QLSTM&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;DisCoCat&#24555;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#21487;&#29992;&#30340;&#36719;&#20214;&#23454;&#29616;&#20013;&#20063;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an application domain where the slightest qualitative improvements can yield immense value, finance is a promising candidate for early quantum advantage. Focusing on the rapidly advancing field of Quantum Natural Language Processing (QNLP), we explore the practical applicability of the two central approaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the problem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data generation approach, we conduct a case study with more than 1000 realistic sentences and find that QLSTMs can be trained substantially faster than DisCoCat while also achieving close to classical results for their available software implementations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ESMC&#27169;&#22411;&#65292;&#36890;&#36807;&#21442;&#25968;&#32422;&#26463;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;&#20272;&#35745;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25193;&#23637;&#20915;&#31574;&#36335;&#24452;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#20915;&#31574;&#24847;&#22270;&#65292;&#24182;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09193</link><description>&lt;p&gt;
ESMC:&#25972;&#20010;&#31354;&#38388;&#22810;&#20219;&#21153;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#32422;&#26463;&#29992;&#20110;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;
&lt;/p&gt;
&lt;p&gt;
ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint. (arXiv:2307.09193v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09193
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ESMC&#27169;&#22411;&#65292;&#36890;&#36807;&#21442;&#25968;&#32422;&#26463;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;&#20272;&#35745;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25193;&#23637;&#20915;&#31574;&#36335;&#24452;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#20915;&#31574;&#24847;&#22270;&#65292;&#24182;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#22312;&#20114;&#32852;&#32593;&#19978;&#24191;&#27867;&#20351;&#29992;&#65292;&#36127;&#36131;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#21644;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;CVR&#20272;&#35745;&#22120;&#23384;&#22312;&#30528;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#36861;&#36394;&#8220;&#26333;&#20809;_&#28857;&#20987;_&#36141;&#20080;&#8221;&#36825;&#20010;&#20915;&#31574;&#36335;&#24452;&#65292;&#25552;&#20986;&#20102;&#25972;&#20010;&#31354;&#38388;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#30740;&#31350;&#32773;&#35266;&#23519;&#21040;&#22312;&#28857;&#20987;&#21644;&#36141;&#20080;&#20043;&#38388;&#23384;&#22312;&#36141;&#20080;&#30456;&#20851;&#34892;&#20026;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#30340;&#20915;&#31574;&#24847;&#22270;&#24182;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#20915;&#31574;&#36335;&#24452;&#24050;&#25193;&#23637;&#20026;&#8220;&#26333;&#20809;_&#28857;&#20987;_&#24215;&#20869;&#21160;&#20316;_&#36141;&#20080;&#8221;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#27010;&#29575;&#26041;&#27861;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26465;&#20214;&#27010;&#29575;&#30340;&#38142;&#24335;&#27861;&#21017;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#27010;&#29575;&#31354;&#38388;&#28151;&#28102;&#65288;PSC&#65289;&#38382;&#39064;&#65292;&#24182;&#25512;&#23548;&#20102;&#22320;&#38754;&#23454;&#20917;&#19982;&#20272;&#35745;&#25968;&#23398;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale online recommender system spreads all over the Internet being in charge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion Rate (CVR) estimations. However, traditional CVR estimators suffer from well-known Sample Selection Bias and Data Sparsity issues. Entire space models were proposed to address the two issues via tracing the decision-making path of "exposure_click_purchase". Further, some researchers observed that there are purchase-related behaviors between click and purchase, which can better draw the user's decision-making intention and improve the recommendation performance. Thus, the decision-making path has been extended to "exposure_click_in-shop action_purchase" and can be modeled with conditional probability approach. Nevertheless, we observe that the chain rule of conditional probability does not always hold. We report Probability Space Confusion (PSC) issue and give a derivation of difference between ground-truth and estimation mathematical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#38454;&#27573;&#30005;&#32518;&#24067;&#32447;&#20219;&#21153;&#20013;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#21487;&#21464;&#24418;&#29289;&#20307;&#12289;&#35270;&#35273;&#24863;&#30693;&#38381;&#29615;&#21644;&#25193;&#23637;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;&#25104;&#21151;&#25511;&#21046;&#22120;&#38656;&#35201;&#33021;&#22815;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#32416;&#27491;&#20302;&#32423;&#25511;&#21046;&#22120;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2307.08927</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#30005;&#32518;&#24067;&#32447;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Stage Cable Routing through Hierarchical Imitation Learning. (arXiv:2307.08927v3 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#38454;&#27573;&#30005;&#32518;&#24067;&#32447;&#20219;&#21153;&#20013;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#21487;&#21464;&#24418;&#29289;&#20307;&#12289;&#35270;&#35273;&#24863;&#30693;&#38381;&#29615;&#21644;&#25193;&#23637;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;&#25104;&#21151;&#25511;&#21046;&#22120;&#38656;&#35201;&#33021;&#22815;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#32416;&#27491;&#20302;&#32423;&#25511;&#21046;&#22120;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#22914;&#20309;&#25191;&#34892;&#22810;&#38454;&#27573;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#20110;&#30005;&#32518;&#24067;&#32447;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#24517;&#39035;&#36890;&#36807;&#19968;&#31995;&#21015;&#22841;&#23376;&#26469;&#24067;&#32447;&#12290;&#36825;&#20010;&#35774;&#32622;&#20195;&#34920;&#20102;&#22797;&#26434;&#22810;&#38454;&#27573;&#26426;&#22120;&#20154;&#25805;&#20316;&#22330;&#26223;&#30340;&#25361;&#25112;&#65306;&#22788;&#29702;&#21487;&#21464;&#24418;&#29289;&#20307;&#65292;&#23545;&#35270;&#35273;&#24863;&#30693;&#38381;&#29615;&#65292;&#22788;&#29702;&#30001;&#22810;&#20010;&#27493;&#39588;&#32452;&#25104;&#30340;&#25193;&#23637;&#34892;&#20026;&#65292;&#24517;&#39035;&#25104;&#21151;&#25191;&#34892;&#25165;&#33021;&#23436;&#25104;&#25972;&#20010;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#20026;&#27599;&#20010;&#38454;&#27573;&#23398;&#20064;&#25104;&#21151;&#29575;&#36275;&#22815;&#39640;&#30340;&#21333;&#20010;&#22522;&#20803;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65306;&#22914;&#26524;&#27599;&#20010;&#38454;&#27573;&#24517;&#39035;&#25104;&#21151;&#23436;&#25104;&#24182;&#19988;&#26377;&#36739;&#22823;&#30340;&#22833;&#36133;&#27010;&#29575;&#65292;&#25972;&#20010;&#20219;&#21153;&#25104;&#21151;&#23436;&#25104;&#30340;&#27010;&#29575;&#21464;&#24471;&#24494;&#19981;&#36275;&#36947;&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#22810;&#38454;&#27573;&#20219;&#21153;&#30340;&#25104;&#21151;&#25511;&#21046;&#22120;&#24517;&#39035;&#33021;&#22815;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#65292;&#24182;&#36890;&#36807;&#32874;&#26126;&#22320;&#36873;&#25321;&#20174;&#32780;&#32416;&#27491;&#20302;&#32423;&#25511;&#21046;&#22120;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning to perform multi-stage robotic manipulation tasks, with applications to cable routing, where the robot must route a cable through a series of clips. This setting presents challenges representative of complex multi-stage robotic manipulation scenarios: handling deformable objects, closing the loop on visual perception, and handling extended behaviors consisting of multiple steps that must be executed successfully to complete the entire task. In such settings, learning individual primitives for each stage that succeed with a high enough rate to perform a complete temporally extended task is impractical: if each stage must be completed successfully and has a non-negligible probability of failure, the likelihood of successful completion of the entire task becomes negligible. Therefore, successful controllers for such multi-stage tasks must be able to recover from failure and compensate for imperfections in low-level controllers by smartly choosing which con
&lt;/p&gt;</description></item><item><title>DeepIPCv2&#26159;&#19968;&#31181;&#21033;&#29992;LiDAR&#20256;&#24863;&#22120;&#24863;&#30693;&#29615;&#22659;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#28857;&#20113;&#20316;&#20026;&#24863;&#30693;&#36755;&#20837;&#65292;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06647</link><description>&lt;p&gt;
DeepIPCv2&#65306;&#21033;&#29992;LiDAR&#24378;&#21270;&#33258;&#21160;&#39550;&#39542;&#29615;&#22659;&#24863;&#30693;&#19982;&#23548;&#33322;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DeepIPCv2: LiDAR-powered Robust Environmental Perception and Navigational Control for Autonomous Vehicle. (arXiv:2307.06647v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06647
&lt;/p&gt;
&lt;p&gt;
DeepIPCv2&#26159;&#19968;&#31181;&#21033;&#29992;LiDAR&#20256;&#24863;&#22120;&#24863;&#30693;&#29615;&#22659;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#28857;&#20113;&#20316;&#20026;&#24863;&#30693;&#36755;&#20837;&#65292;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DeepIPCv2&#65292;&#19968;&#31181;&#21033;&#29992;LiDAR&#20256;&#24863;&#22120;&#24863;&#30693;&#29615;&#22659;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20809;&#29031;&#26465;&#20214;&#36739;&#24046;&#30340;&#24773;&#20917;&#19979;&#12290;DeepIPCv2&#20351;&#29992;&#19968;&#32452;LiDAR&#28857;&#20113;&#20316;&#20026;&#20854;&#20027;&#35201;&#24863;&#30693;&#36755;&#20837;&#12290;&#30001;&#20110;&#28857;&#20113;&#19981;&#21463;&#20809;&#29031;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#28165;&#26224;&#30340;&#29615;&#22659;&#35266;&#23519;&#65292;&#26080;&#35770;&#26465;&#20214;&#22914;&#20309;&#12290;&#36825;&#20351;&#24471;&#24863;&#30693;&#27169;&#22359;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#22330;&#26223;&#29702;&#35299;&#21644;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25903;&#25345;&#25511;&#21046;&#27169;&#22359;&#20934;&#30830;&#20272;&#35745;&#23548;&#33322;&#25511;&#21046;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#35813;&#27169;&#22411;&#26469;&#39044;&#27979;&#19968;&#32452;&#39550;&#39542;&#35760;&#24405;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#26465;&#20214;&#19979;&#36827;&#34892;&#30495;&#23454;&#33258;&#21160;&#39550;&#39542;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28040;&#34701;&#21644;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#35777;&#26126;&#20854;&#24615;&#33021;&#12290;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#65292;DeepIPCv2&#22312;&#25152;&#26377;&#26465;&#20214;&#19979;&#22343;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DeepIPCv2, an autonomous driving model that perceives the environment using a LiDAR sensor for more robust drivability, especially when driving under poor illumination conditions. DeepIPCv2 takes a set of LiDAR point clouds for its main perception input. As point clouds are not affected by illumination changes, they can provide a clear observation of the surroundings no matter what the condition is. This results in a better scene understanding and stable features provided by the perception module to support the controller module in estimating navigational control properly. To evaluate its performance, we conduct several tests by deploying the model to predict a set of driving records and perform real automated driving under three different conditions. We also conduct ablation and comparative studies with some recent models to justify its performance. Based on the experimental results, DeepIPCv2 shows a robust performance by achieving the best drivability in all conditions. C
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;GP&#24341;&#23548;&#30340;MPPI&#26041;&#27861;&#29992;&#20110;&#22312;&#22797;&#26434;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23616;&#37096;&#24863;&#30693;&#27169;&#22411;&#21644;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#34920;&#38754;&#65292;&#35782;&#21035;&#24182;&#25512;&#33616;&#26368;&#20248;&#23376;&#30446;&#26631;&#32473;&#23616;&#37096;&#30340;MPPI&#35268;&#21010;&#22120;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26368;&#20248;&#25511;&#21046;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2307.04019</link><description>&lt;p&gt;
GP&#24341;&#23548;&#30340;MPPI&#22312;&#22797;&#26434;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#30340;&#39640;&#25928;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
GP-guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments. (arXiv:2307.04019v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;GP&#24341;&#23548;&#30340;MPPI&#26041;&#27861;&#29992;&#20110;&#22312;&#22797;&#26434;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23616;&#37096;&#24863;&#30693;&#27169;&#22411;&#21644;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#34920;&#38754;&#65292;&#35782;&#21035;&#24182;&#25512;&#33616;&#26368;&#20248;&#23376;&#30446;&#26631;&#32473;&#23616;&#37096;&#30340;MPPI&#35268;&#21010;&#22120;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26368;&#20248;&#25511;&#21046;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#26377;&#38480;&#24863;&#30693;&#33021;&#21147;&#30340;&#26410;&#30693;&#26434;&#20081;&#29615;&#22659;&#20013;&#36827;&#34892;&#26426;&#22120;&#20154;&#23548;&#33322;&#23545;&#26426;&#22120;&#20154;&#23398;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#23616;&#37096;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#27169;&#22411;&#39044;&#27979;&#36335;&#24452;&#31215;&#20998;&#65288;MPPI&#65289;&#65292;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#36935;&#21040;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#26465;&#20214;&#25110;&#22312;&#35745;&#21010;&#33539;&#22260;&#20043;&#22806;&#23548;&#33322;&#26102;&#65292;&#38656;&#35201;&#20840;&#23616;&#24341;&#23548;&#26469;&#30830;&#20445;&#26377;&#25928;&#30340;&#23548;&#33322;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GP-MPPI&#65292;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#23427;&#23558;MPPI&#19982;&#22522;&#20110;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;SGP&#65289;&#30340;&#23616;&#37096;&#24863;&#30693;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;SGP&#30340;&#23398;&#20064;&#33021;&#21147;&#26500;&#24314;&#19968;&#20010;&#26041;&#24046;&#65288;&#19981;&#30830;&#23450;&#24615;&#65289;&#34920;&#38754;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20102;&#35299;&#21608;&#22260;&#30340;&#21487;&#23548;&#33322;&#31354;&#38388;&#65292;&#35782;&#21035;&#19968;&#32452;&#24314;&#35758;&#30340;&#23376;&#30446;&#26631;&#65292;&#24182;&#26368;&#32456;&#25512;&#33616;&#26368;&#23567;&#21270;&#39044;&#23450;&#20041;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#23376;&#30446;&#26631;&#32473;&#23616;&#37096;&#30340;MPPI&#35268;&#21010;&#22120;&#12290;&#20043;&#21518;&#65292;MPPI&#35745;&#31639;&#20986;&#28385;&#36275;&#35201;&#27714;&#30340;&#26368;&#20248;&#25511;&#21046;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic navigation in unknown, cluttered environments with limited sensing capabilities poses significant challenges in robotics. Local trajectory optimization methods, such as Model Predictive Path Intergal (MPPI), are a promising solution to this challenge. However, global guidance is required to ensure effective navigation, especially when encountering challenging environmental conditions or navigating beyond the planning horizon. This study presents the GP-MPPI, an online learning-based control strategy that integrates MPPI with a local perception model based on Sparse Gaussian Process (SGP). The key idea is to leverage the learning capability of SGP to construct a variance (uncertainty) surface, which enables the robot to learn about the navigable space surrounding it, identify a set of suggested subgoals, and ultimately recommend the optimal subgoal that minimizes a predefined cost function to the local MPPI planner. Afterward, MPPI computes the optimal control sequence that sati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02752</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2307.02752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#20013;&#23545;&#22522;&#20934;&#30340;&#26222;&#36941;&#20351;&#29992;&#23548;&#33268;&#20102;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#24573;&#35270;&#12290;&#30001;&#20110;&#25506;&#32034;&#25110;&#23433;&#20840;&#32771;&#34385;&#30340;&#25361;&#25112;&#65292;&#23454;&#38469;&#31163;&#32447;RL&#25968;&#25454;&#38598;&#22312;&#29366;&#24577;&#31354;&#38388;&#19978;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20855;&#20307;&#35828;&#26126;&#20102;&#31163;&#32447;RL&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#29366;&#24577;&#35206;&#30422;&#29575;&#36981;&#24490;&#19968;&#20010;&#30001;&#20559;&#24577;&#31574;&#30053;&#25152;&#29305;&#24449;&#21270;&#30340;&#24130;&#24459;&#20998;&#24067;&#12290;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#20998;&#24067;&#32422;&#26463;&#30340;&#20856;&#22411;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#22914;&#20445;&#23432;Q&#23398;&#20064;&#65288;CQL&#65289;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#25552;&#21462;&#31574;&#30053;&#26159;&#26080;&#25928;&#30340;&#12290;&#21463;&#33258;&#28982;&#26234;&#33021;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;CQL&#30340;&#22686;&#24378;&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#20197;&#22238;&#24518;&#20197;&#24448;&#30456;&#20851;&#32463;&#39564;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent use of benchmarks in current offline reinforcement learning (RL) research has led to a neglect of the imbalance of real-world dataset distributions in the development of models. The real-world offline RL dataset is often imbalanced over the state space due to the challenge of exploration or safety considerations. In this paper, we specify properties of imbalanced datasets in offline RL, where the state coverage follows a power law distribution characterized by skewed policies. Theoretically and empirically, we show that typically offline RL methods based on distributional constraints, such as conservative Q-learning (CQL), are ineffective in extracting policies under the imbalanced dataset. Inspired by natural intelligence, we propose a novel offline RL method that utilizes the augmentation of CQL with a retrieval process to recall past related experiences, effectively alleviating the challenges posed by imbalanced datasets. We evaluate our method on several tasks in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;Epistemic Neural Recommendation (ENR)&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#30340;Thompson&#25277;&#26679;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#28857;&#20987;&#29575;&#21644;&#29992;&#25143;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2306.14834</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;Bandit&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Neural Contextual Bandit for Recommender Systems. (arXiv:2306.14834v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;Epistemic Neural Recommendation (ENR)&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#30340;Thompson&#25277;&#26679;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#28857;&#20987;&#29575;&#21644;&#29992;&#25143;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#31995;&#32479;&#24212;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#26377;&#25928;&#21644;&#25506;&#32034;&#24615;&#20114;&#21160;&#25552;&#20379;&#21019;&#26032;&#21644;&#30456;&#20851;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#25512;&#33616;&#31995;&#32479;&#20013;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#20165;&#21033;&#29992;&#24050;&#35782;&#21035;&#30340;&#29992;&#25143;&#20852;&#36259;&#65292;&#23545;&#20110;&#26377;&#25928;&#21457;&#29616;&#26410;&#30693;&#29992;&#25143;&#20559;&#22909;&#23384;&#22312;&#19981;&#36275;&#12290;&#23613;&#31649;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#22312;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#22312;&#32447;&#25506;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20182;&#20204;&#23545;&#35745;&#31639;&#30340;&#35201;&#27714;&#36739;&#39640;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26679;&#26412;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#19978;&#19979;&#25991;Bandit&#31639;&#27861;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35748;&#30693;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;Epistemic Neural Recommendation (ENR)&#65292;&#23427;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#19978;&#23454;&#29616;Thompson&#25277;&#26679;&#12290;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;ENR&#26174;&#33879;&#25552;&#39640;&#20102;&#28857;&#20987;&#29575;&#21644;&#29992;&#25143;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality recommender systems ought to deliver both innovative and relevant content through effective and exploratory interactions with users. Yet, supervised learning-based neural networks, which form the backbone of many existing recommender systems, only leverage recognized user interests, falling short when it comes to efficiently uncovering unknown user preferences. While there has been some progress with neural contextual bandit algorithms towards enabling online exploration through neural networks, their onerous computational demands hinder widespread adoption in real-world recommender systems. In this work, we propose a scalable sample-efficient neural contextual bandit algorithm for recommender systems. To do this, we design an epistemic neural network architecture, Epistemic Neural Recommendation (ENR), that enables Thompson sampling at a large scale. In two distinct large-scale experiments with real-world tasks, ENR significantly boosts click-through rates and user rating
&lt;/p&gt;</description></item><item><title>G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11667</link><description>&lt;p&gt;
G-NM&#65306;&#19968;&#32452;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11667
&lt;/p&gt;
&lt;p&gt;
G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24320;&#21457;&#21644;&#23454;&#26045;&#19968;&#20010;&#32508;&#21512;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#21512;&#65292;&#32479;&#31216;&#20026;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#32452;&#65288;G-NM&#65289;&#12290;&#35813;&#38598;&#21512;&#21253;&#25324;&#20256;&#32479;&#27169;&#22411;&#22914;&#33258;&#22238;&#24402;&#32508;&#21512;&#31227;&#21160;&#24179;&#22343;&#65288;ARIMA&#65289;&#12289;Holt-Winters&#26041;&#27861;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#65292;&#20197;&#21450;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;G-NM&#26126;&#30830;&#26500;&#24314;&#20197;&#22686;&#24378;&#25105;&#20204;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#22266;&#26377;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#19982;&#36825;&#20123;&#20107;&#20214;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;G-NM&#20415;&#20110;&#23545;&#27492;&#31867;&#29616;&#35937;&#22312;&#24310;&#38271;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25512;&#36827;&#25105;&#20204;&#23545;&#27492;&#31867;&#20107;&#20214;&#30340;&#29702;&#35299;&#65292;&#24182;&#22823;&#24133;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;G-NM&#21253;&#25324;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#23395;&#33410;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on the development and implementation of a comprehensive ensemble of numerical time series forecasting models, collectively referred to as the Group of Numerical Time Series Prediction Model (G-NM). This inclusive set comprises traditional models such as Autoregressive Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector Regression (SVR), in addition to modern neural network models including Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is explicitly constructed to augment our predictive capabilities related to patterns and trends inherent in complex natural phenomena. By utilizing time series data relevant to these events, G-NM facilitates the prediction of such phenomena over extended periods. The primary objective of this research is to both advance our understanding of such occurrences and to significantly enhance the accuracy of our forecasts. G-NM encapsulates both linear and non-linear dependencies, seasonal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Enlighten-anything&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#20013;&#23558;&#20998;&#27573;&#27169;&#22411;&#19982;SAM&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#33391;&#22909;&#35270;&#35273;&#24863;&#30693;&#30340;&#34701;&#21512;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.10286</link><description>&lt;p&gt;
Enlighten-anything: &#24403;&#20998;&#27573;&#27169;&#22411;&#36935;&#35265;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enlighten-anything:When Segment Anything Model Meets Low-light Image Enhancement. (arXiv:2306.10286v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Enlighten-anything&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#20013;&#23558;&#20998;&#27573;&#27169;&#22411;&#19982;SAM&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#33391;&#22909;&#35270;&#35273;&#24863;&#30693;&#30340;&#34701;&#21512;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#26159;&#19968;&#39033;&#20302;&#32423;&#21035;&#35270;&#35273;&#20219;&#21153;&#65292;&#22823;&#22810;&#25968;CNN&#26041;&#27861;&#37117;&#26159;&#20316;&#20026;&#40657;&#30418;&#23376;&#35774;&#35745;&#30340;&#65292;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#22266;&#26377;&#32654;&#23398;&#12290;&#35768;&#22810;&#26080;&#30417;&#30563;&#26041;&#27861;&#24573;&#30053;&#20102;&#20302;&#20809;&#22330;&#26223;&#20013;&#21487;&#35265;&#20449;&#24687;&#30340;&#36864;&#21270;&#65292;&#36825;&#20250;&#20005;&#37325;&#24433;&#21709;&#34917;&#20805;&#20449;&#24687;&#30340;&#32858;&#21512;&#65292;&#24182;&#20351;&#34701;&#21512;&#31639;&#27861;&#26080;&#27861;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#34701;&#21512;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Enlighten-anything&#65292;&#33021;&#22815;&#23558;SAM&#20998;&#27573;&#30340;&#35821;&#20041;&#24847;&#22270;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#35270;&#35273;&#24863;&#30693;&#30340;&#34701;&#21512;&#22270;&#20687;&#12290;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#24471;&#21040;&#20102;&#26497;&#22823;&#25552;&#39640;&#65292;&#23545;LOL&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PSNR&#19978;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;3dB&#65292;&#22312;SSIM&#19978;&#25552;&#39640;&#20102;8&#12290; SAM&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#20026;&#26080;&#30417;&#30563;&#20302;&#20809;&#22686;&#24378;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24110;&#21161;&#12290;Enlighten-anything&#30340;&#28304;&#20195;&#30721;&#21487;&#20197;&#20174; https://github.com/zhangbaijin/enlighten-any &#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image restoration is a low-level visual task, and most CNN methods are designed as black boxes, lacking transparency and intrinsic aesthetics. Many unsupervised approaches ignore the degradation of visible information in low-light scenes, which will seriously affect the aggregation of complementary information and also make the fusion algorithm unable to produce satisfactory fusion results under extreme conditions. In this paper, we propose Enlighten-anything, which is able to enhance and fuse the semantic intent of SAM segmentation with low-light images to obtain fused images with good visual perception. The generalization ability of unsupervised learning is greatly improved, and experiments on LOL dataset are conducted to show that our method improves 3db in PSNR over baseline and 8 in SSIM. zero-shot learning of SAM introduces a powerful aid for unsupervised low-light enhancement. The source code of Enlighten-anything can be obtained from https://github.com/zhangbaijin/enlighten-any
&lt;/p&gt;</description></item><item><title>&#20845;&#36793;&#24418;&#26631;&#27880;&#22120;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#23384;&#20998;&#26512;&#22120;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#26102;&#23454;&#29616;&#23436;&#20840;&#24182;&#34892;&#21270;&#65292;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312; Penn Treebank &#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05477</link><description>&lt;p&gt;
&#20845;&#36793;&#24418;&#26631;&#27880;&#65306;&#23558;&#25237;&#24433;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#20316;&#20026;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Hexatagging: Projective Dependency Parsing as Tagging. (arXiv:2306.05477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05477
&lt;/p&gt;
&lt;p&gt;
&#20845;&#36793;&#24418;&#26631;&#27880;&#22120;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#23384;&#20998;&#26512;&#22120;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#26102;&#23454;&#29616;&#23436;&#20840;&#24182;&#34892;&#21270;&#65292;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312; Penn Treebank &#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#23384;&#20998;&#26512;&#22120;&#8212;&#8212;&#20845;&#36793;&#24418;&#26631;&#27880;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#21477;&#23376;&#20013;&#30340;&#21333;&#35789;&#26631;&#35760;&#20026;&#26469;&#33258;&#21487;&#33021;&#26631;&#35760;&#26377;&#38480;&#38598;&#21512;&#20013;&#30340;&#20803;&#32032;&#26469;&#26500;&#24314;&#20381;&#23384;&#26641;&#12290;&#19982;&#35768;&#22810;&#22788;&#29702;&#20381;&#23384;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#26159;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#65292;&#21363;&#29992;&#20110;&#26500;&#24314;&#20381;&#23384;&#20998;&#26512;&#25152;&#38656;&#30340;&#32467;&#26500;&#26500;&#24314;&#25805;&#20316;&#21487;&#20197;&#30456;&#20114;&#24182;&#34892;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#30830;&#20999;&#35299;&#30721;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#37117;&#26159;&#32447;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#20381;&#23384;&#20998;&#26512;&#22120;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#26469;&#39044;&#27979;&#20845;&#36793;&#26631;&#35760;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#20026;&#27492;&#20219;&#21153;&#26126;&#30830;&#35774;&#35745;&#30340;&#23450;&#21046;&#20307;&#31995;&#32467;&#26500;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#31616;&#21333;&#24615;&#65292;&#20294;&#22312; Penn Treebank &#27979;&#35797;&#38598;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102; 96.4 LAS &#21644; 97.4 UAS &#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#22120;&#30340;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#24615;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#22823;&#32422;&#21313;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a probabilistic dependency parser that predicts hexatags using no more than a linear model with features from a pretrained language model, i.e., we forsake a bespoke architecture explicitly designed for the task. Despite the generality and simplicity of our approach, we achieve state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test set. Additionally, our parser's linear time complexity and parallelism significantly improve computational efficiency, with a roughly 10-times speed-u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#20351;&#29992;&#21487;&#36873;&#38899;&#26631;&#26469;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#36890;&#36807;&#24341;&#20837;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#38543;&#26426;&#25513;&#34109;&#21644;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#25552;&#21319;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03557</link><description>&lt;p&gt;
&#21033;&#29992;&#37096;&#20998;&#26631;&#27880;&#30340;&#25991;&#26412;&#25552;&#21319;&#38463;&#25289;&#20271;&#35821;&#38899;&#26631;&#27880;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text. (arXiv:2306.03557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#20351;&#29992;&#21487;&#36873;&#38899;&#26631;&#26469;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#36890;&#36807;&#24341;&#20837;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#38543;&#26426;&#25513;&#34109;&#21644;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#25552;&#21319;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#38463;&#25289;&#20271;&#35821;&#38899;&#26631;&#27880;&#22312;&#24456;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#37117;&#38750;&#24120;&#26377;&#29992;&#65292;&#27604;&#22914;&#23545;&#20110;&#35821;&#35328;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#26631;&#27880;&#21487;&#20197;&#25552;&#20379;&#38405;&#35835;&#25903;&#25345;&#65292;&#32780;&#23545;&#20110;&#35821;&#38899;&#21512;&#25104;&#36825;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#26631;&#27880;&#20934;&#30830;&#24615;&#23545;&#20110;&#21457;&#38899;&#39044;&#27979;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#25968;&#19987;&#27880;&#20110;&#22788;&#29702;&#27809;&#26377;&#38899;&#26631;&#30340;&#21407;&#22987;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#36890;&#36807;&#32473;&#20154;&#31867;&#25552;&#20379;&#36873;&#23450;&#30340;&#25110;&#37096;&#20998;&#26631;&#27880;&#30340;&#25935;&#24863;&#35789;&#27719;&#65292;&#21487;&#20197;&#20351;&#24471;&#29983;&#20135;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25903;&#25345;&#36755;&#20837;&#20013;&#30340;&#21487;&#36873;&#38899;&#26631;&#20197;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#21033;&#29992;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#21644;&#19981;&#21516;&#31561;&#32423;&#30340;&#38543;&#26426;&#25513;&#34109;&#26469;&#25552;&#21319;&#26631;&#27880;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27979;&#35797;&#26399;&#38388;&#25552;&#20379;&#30340;&#26631;&#27880;&#33021;&#22815;&#24433;&#21709;&#26356;&#22810;&#30340;&#36755;&#20986;&#20301;&#32622;&#65292;&#23454;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20943;&#23569;60%&#30340;&#21442;&#25968;&#25968;&#30446;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Arabic diacritization is useful in many applications, ranging from reading support for language learners to accurate pronunciation predictor for downstream tasks like speech synthesis. While most of the previous works focused on models that operate on raw non-diacritized text, production systems can gain accuracy by first letting humans partly annotate ambiguous words. In this paper, we propose 2SDiac, a multi-source model that can effectively support optional diacritics in input to inform all predictions. We also introduce Guided Learning, a training scheme to leverage given diacritics in input with different levels of random masking. We show that the provided hints during test affect more output positions than those annotated. Moreover, experiments on two common benchmarks show that our approach i) greatly outperforms the baseline also when evaluated on non-diacritized text; and ii) achieves state-of-the-art results while reducing the parameter count by over 60%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#24418;&#24335;&#35299;&#37322;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#22238;&#31572;&#40065;&#26834;&#24615;&#26597;&#35810;&#26469;&#35745;&#31639;&#35299;&#37322;&#65292;&#24182;&#24314;&#31435;&#20102;&#24418;&#24335;&#35299;&#37322;&#22797;&#26434;&#24615;&#21644;&#40065;&#26834;&#24615;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.03048</link><description>&lt;p&gt;
&#20174;&#40065;&#26834;&#24615;&#21040;&#21487;&#35299;&#37322;&#24615;&#20877;&#21040;&#40065;&#26834;&#24615;&#12290;(arXiv:2306.03048v2 [cs.AI] &#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
From Robustness to Explainability and Back Again. (arXiv:2306.03048v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#24418;&#24335;&#35299;&#37322;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#22238;&#31572;&#40065;&#26834;&#24615;&#26597;&#35810;&#26469;&#35745;&#31639;&#35299;&#37322;&#65292;&#24182;&#24314;&#31435;&#20102;&#24418;&#24335;&#35299;&#37322;&#22797;&#26434;&#24615;&#21644;&#40065;&#26834;&#24615;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#20020;&#26102;&#26041;&#27861;&#30456;&#27604;&#65292;&#24418;&#24335;&#35299;&#37322;&#25552;&#20379;&#20102;&#20005;&#23494;&#24615;&#30340;&#37325;&#35201;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#24418;&#24335;&#35299;&#37322;&#22312;&#26576;&#20123;&#20998;&#31867;&#22120;&#23478;&#26063;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#31070;&#32463;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#25285;&#24515;&#24418;&#24335;&#35299;&#37322;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#20854;&#20182;&#26041;&#27861;&#30340;&#34917;&#20805;&#65292;&#20197;&#25552;&#20379;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#24418;&#24335;&#35299;&#37322;&#21487;&#25193;&#23637;&#24615;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#24418;&#24335;&#35299;&#37322;&#30340;&#26032;&#31639;&#27861;&#12290;&#36825;&#31181;&#26032;&#31639;&#27861;&#36890;&#36807;&#22238;&#31572;&#19968;&#31995;&#21015;&#40065;&#26834;&#24615;&#26597;&#35810;&#26469;&#35745;&#31639;&#35299;&#37322;&#65292;&#24182;&#19988;&#36825;&#20123;&#26597;&#35810;&#30340;&#25968;&#37327;&#26368;&#22810;&#19982;&#29305;&#24449;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#23454;&#38469;&#22797;&#26434;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#22797;&#26434;&#24615;&#20043;&#38388;&#24314;&#31435;&#20102;&#30452;&#25509;&#20851;&#31995;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#26412;&#25991;&#25512;&#24191;&#20102;&#24418;&#24335;&#35299;&#37322;&#30340;&#23450;&#20041;&#65292;&#20174;&#32780;&#20801;&#35768;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#26469;&#25913;&#36827;&#35299;&#37322;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast with ad-hoc methods for eXplainable Artificial Intelligence (XAI), formal explainability offers important guarantees of rigor. However, formal explainability is hindered by poor scalability for some families of classifiers, the most significant being neural networks. As a result, there are concerns as to whether formal explainability might serve to complement other approaches in delivering trustworthy AI. This paper addresses the limitation of scalability of formal explainability, and proposes novel algorithms for computing formal explanations. The novel algorithm computes explanations by answering instead a number of robustness queries, and such that the number of such queries is at most linear on the number of features. Consequently, the proposed algorithm establishes a direct relationship between the practical complexity of formal explainability and that of robustness. More importantly, the paper generalizes the definition of formal explanation, thereby allowing the use 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#19982;&#38544;&#31169;&#23041;&#32961;&#12289;&#29616;&#29366;&#21644;&#26410;&#26469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.18339</link><description>&lt;p&gt;
ChatGPT&#65306;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#25361;&#25112;&#19982;&#35299;&#20915;&#26041;&#26696;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions. (arXiv:2305.18339v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#19982;&#38544;&#31169;&#23041;&#32961;&#12289;&#29616;&#29366;&#21644;&#26410;&#26469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#27604;&#22914;ChatGPT&#30340;&#26222;&#21450;&#20351;&#29992;&#65292;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#27491;&#22312;&#24341;&#39046;&#20869;&#23481;&#21019;&#20316;&#21644;&#30693;&#35782;&#34920;&#31034;&#26041;&#24335;&#23454;&#29616;&#33539;&#24335;&#36716;&#21464;&#12290;AIGC&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;AI&#31639;&#27861;&#26469;&#36741;&#21161;&#25110;&#26367;&#20195;&#20154;&#31867;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#20302;&#30340;&#25104;&#26412;&#21019;&#24314;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#21644;&#31867;&#20154;&#30340;&#20869;&#23481;&#12290;&#23613;&#31649;&#22312;AIGC&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#25361;&#25112;&#20173;&#38656;&#35299;&#20915;&#12290;&#26412;&#25991;&#28145;&#20837;&#35843;&#26597;&#20102;AIGC&#33539;&#24335;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#12289;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#20102;AIGC&#30340;&#25216;&#26415;&#23454;&#29616;&#12289;&#24635;&#20307;&#26550;&#26500;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24037;&#20316;&#27169;&#24335;&#21644;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#38024;&#23545;AIGC&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#20998;&#31867;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;GPT&#21644;AIGC&#25216;&#26415;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#24050;&#30830;&#23450;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;AIGC&#39046;&#22495;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#21457;&#23637;AIGC&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of large artificial intelligence (AI) models such as ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. Despite the recent significant progress in AIGC, security, privacy, ethical, and legal challenges still need to be addressed. This paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the AIGC paradigm. Specifically, we first explore the enabling technologies, general architecture of AIGC, and discuss its working modes and key characteristics. Then, we investigate the taxonomy of security and privacy threats to AIGC and highlight the ethical and societal implications of GPT and AIGC tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#20248;&#21270;&#30340;&#35268;&#33539;&#36873;&#25321;&#21644;&#20551;&#35774;&#65292;&#24182;&#25351;&#20986;&#20102;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#21253;&#25324;&#20219;&#21153;&#36951;&#28431;&#12289;&#20915;&#31574;&#36793;&#30028;&#38382;&#39064;&#20197;&#21450;&#22810;&#20010;&#20195;&#29702;&#30340;&#30446;&#26631;&#20914;&#31361;&#22312;&#20869;&#30340;&#20845;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17465</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#24573;&#35270;&#35268;&#33539;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Optimization's Neglected Normative Commitments. (arXiv:2305.17465v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#20248;&#21270;&#30340;&#35268;&#33539;&#36873;&#25321;&#21644;&#20551;&#35774;&#65292;&#24182;&#25351;&#20986;&#20102;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#21253;&#25324;&#20219;&#21153;&#36951;&#28431;&#12289;&#20915;&#31574;&#36793;&#30028;&#38382;&#39064;&#20197;&#21450;&#22810;&#20010;&#20195;&#29702;&#30340;&#30446;&#26631;&#20914;&#31361;&#22312;&#20869;&#30340;&#20845;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#34987;&#25552;&#20379;&#20316;&#20026;&#35299;&#20915;&#28041;&#21450;&#19981;&#30830;&#23450;&#24615;&#21644;&#21033;&#30410;&#20914;&#31361;&#30340;&#22797;&#26434;&#23454;&#38469;&#20915;&#31574;&#30340;&#23458;&#35266;&#26041;&#27861;&#12290;&#23427;&#39537;&#21160;&#30528;&#21830;&#19994;&#31574;&#30053;&#21644;&#20844;&#20849;&#25919;&#31574;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#25104;&#20026;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#26680;&#24515;&#12290;&#20248;&#21270;&#20381;&#36182;&#20110;&#23558;&#29616;&#23454;&#19990;&#30028;&#25277;&#35937;&#20026;&#19968;&#31995;&#21015;&#20915;&#31574;&#12289;&#30446;&#26631;&#21644;&#32422;&#26463;&#26469;&#22788;&#29702;&#21487;&#33021;&#28041;&#21450;&#39640;&#39118;&#38505;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#20174;&#24314;&#27169;&#36807;&#31243;&#21644;&#19968;&#31995;&#21015;&#23454;&#38469;&#26696;&#20363;&#20013;&#25552;&#21462;&#65292;&#25551;&#36848;&#20102;&#20351;&#29992;&#20248;&#21270;&#26102;&#24517;&#28982;&#28041;&#21450;&#30340;&#35268;&#33539;&#36873;&#25321;&#21644;&#20551;&#35774;&#12290;&#28982;&#21518;&#65292;&#23427;&#35782;&#21035;&#20986;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#20845;&#20010;&#26032;&#38382;&#39064;&#65306;1&#65289;&#32473;&#23450;&#30340;&#20540;&#35774;&#32622;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#20248;&#21270;&#24573;&#30053;&#26576;&#20123;&#20219;&#21153;&#65292;&#25110;&#32773;&#38169;&#35823;&#23558;&#20854;&#32435;&#20837;&#32422;&#26463;&#25110;&#30446;&#26631;&#30340;&#19968;&#37096;&#20998;&#65307;2&#65289;&#38382;&#39064;&#30340;&#20915;&#31574;&#36793;&#30028;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#27169;&#22359;&#21270;&#20551;&#35774;&#21644;&#21453;&#39304;&#24490;&#29615;&#65307;3&#65289;&#26410;&#33021;&#32771;&#34385;&#21040;&#22810;&#20010;&#20195;&#29702;&#30340;&#20998;&#27495;&#30446;&#26631;&#21487;&#33021;&#23548;&#33268;&#20914;&#31361;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is offered as an objective approach to resolving complex, real-world decisions involving uncertainty and conflicting interests. It drives business strategies as well as public policies and, increasingly, lies at the heart of sophisticated machine learning systems. A paradigm used to approach potentially high-stakes decisions, optimization relies on abstracting the real world to a set of decision(s), objective(s) and constraint(s). Drawing from the modeling process and a range of actual cases, this paper describes the normative choices and assumptions that are necessarily part of using optimization. It then identifies six emergent problems that may be neglected: 1) Misspecified values can yield optimizations that omit certain imperatives altogether or incorporate them incorrectly as a constraint or as part of the objective, 2) Problematic decision boundaries can lead to faulty modularity assumptions and feedback loops, 3) Failing to account for multiple agents' divergent go
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.15299</link><description>&lt;p&gt;
&#22312;ChatGPT&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#31185;&#23398;&#65306;&#30740;&#31350;&#20262;&#29702;&#30340;&#25361;&#25112;&#21450;&#24212;&#23545;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond. (arXiv:2305.15299v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20855;&#26377;&#26174;&#33879;&#20294;&#26377;&#20105;&#35758;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#26102;&#20195;&#31185;&#23398;&#30740;&#31350;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#26088;&#22312;&#20026;&#39640;&#36136;&#37327;&#30340;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#22880;&#23450;&#26032;&#30340;&#21450;&#26102;&#22522;&#30784;&#12290;&#23545;AI&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30740;&#31350;&#24037;&#20855;&#21644;&#30740;&#31350;&#23545;&#35937;&#30340;&#35282;&#33394;&#36827;&#34892;&#20102;&#35814;&#32454;&#23457;&#26597;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31185;&#23398;&#23478;&#12289;&#21442;&#19982;&#32773;&#21644;&#35780;&#23457;&#20154;&#21592;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;&#35752;&#35770;&#20102;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#30340;&#26032;&#20852;&#23454;&#36341;&#65292;&#24182;&#32473;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20026;&#22312;AI&#26102;&#20195;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models of artificial intelligence (AI), such as ChatGPT, find remarkable but controversial applicability in science and research. This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI. This is with the aim to lay new timely foundations for a high-quality research ethics review. The role of AI language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. New emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of AI.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24369;&#28857;&#65292;&#36890;&#36807;&#22312;&#39134;&#34892;&#20013;&#25658;&#24102;&#30340;&#22270;&#20687;&#26469;&#25805;&#32437;&#33258;&#20027;&#22810;&#26059;&#32764;&#30340;&#34892;&#20026;&#65292;&#23454;&#29616;&#23545;&#21463;&#23475;&#22810;&#26059;&#32764;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.12859</link><description>&lt;p&gt;
&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#65306;&#25805;&#32437;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#20027;&#22810;&#26059;&#32764;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Flying Adversarial Patches: Manipulating the Behavior of Deep Learning-based Autonomous Multirotors. (arXiv:2305.12859v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12859
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24369;&#28857;&#65292;&#36890;&#36807;&#22312;&#39134;&#34892;&#20013;&#25658;&#24102;&#30340;&#22270;&#20687;&#26469;&#25805;&#32437;&#33258;&#20027;&#22810;&#26059;&#32764;&#30340;&#34892;&#20026;&#65292;&#23454;&#29616;&#23545;&#21463;&#23475;&#22810;&#26059;&#32764;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39134;&#34892;&#26426;&#22120;&#20154;&#65292;&#22914;&#22810;&#26059;&#32764;&#65292;&#36890;&#24120;&#20381;&#38752;&#31070;&#32463;&#32593;&#32476;&#26681;&#25454;&#25668;&#20687;&#22836;&#22270;&#20687;&#36827;&#34892;&#39044;&#27979;&#12290;&#22914;&#26524;&#23558;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#21487;&#33021;&#20250;&#24471;&#21040;&#20986;&#20154;&#24847;&#26009;&#30340;&#32467;&#26524;&#12290;&#23545;&#25239;&#25915;&#20987;&#21033;&#29992;&#20102;&#36825;&#20010;&#32570;&#38519;&#65292;&#20363;&#22914;&#36890;&#36807;&#35745;&#31639;&#23567;&#22270;&#20687;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#36148;&#29255;&#65292;&#23558;&#20854;&#25918;&#32622;&#22312;&#29615;&#22659;&#20013;&#65292;&#20197;&#25805;&#32437;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#65292;&#20854;&#20013;&#19968;&#20010;&#22270;&#20687;&#34987;&#23433;&#35013;&#22312;&#21478;&#19968;&#20010;&#39134;&#34892;&#26426;&#22120;&#20154;&#19978;&#65292;&#22240;&#27492;&#21487;&#20197;&#25918;&#32622;&#22312;&#21463;&#23475;&#22810;&#26059;&#32764;&#30340;&#20219;&#20309;&#35270;&#37326;&#20013;&#12290;&#20026;&#20102;&#26377;&#25928;&#25915;&#20987;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21516;&#26102;&#20248;&#21270;&#23545;&#25239;&#36148;&#29255;&#21450;&#20854;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#20301;&#32622;&#30340;&#19977;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#33258;&#20027;&#22810;&#26059;&#32764;&#30340;DL&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#22810;&#26059;&#32764;&#33021;&#22815;&#23436;&#20840;&#25511;&#21046;&#21463;&#23475;&#22810;&#26059;&#32764;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous flying robots, e.g. multirotors, often rely on a neural network that makes predictions based on a camera image. These deep learning (DL) models can compute surprising results if applied to input images outside the training domain. Adversarial attacks exploit this fault, for example, by computing small images, so-called adversarial patches, that can be placed in the environment to manipulate the neural network's prediction. We introduce flying adversarial patches, where an image is mounted on another flying robot and therefore can be placed anywhere in the field of view of a victim multirotor. For an effective attack, we compare three methods that simultaneously optimize the adversarial patch and its position in the input image. We perform an empirical validation on a publicly available DL model and dataset for autonomous multirotors. Ultimately, our attacking multirotor would be able to gain full control over the motions of the victim multirotor.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#22312;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11461</link><description>&lt;p&gt;
&#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#20174;&#35821;&#20041;&#32423;&#21035;&#21040;&#20195;&#30721;&#32423;&#21035;&#30340; SelfzCoT&#65292;&#26356;&#22909;&#22320;&#21033;&#29992;LLMs
&lt;/p&gt;
&lt;p&gt;
SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs. (arXiv:2305.11461v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#22312;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;LLMs&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558; SelfzCoT &#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#65292;&#20854;&#20934;&#30830;&#24615;&#20174;GSM8K&#30340;40.50%&#25552;&#39640;&#33267;82.34%&#65292;MultiArith&#20174;79.3%&#25552;&#39640;&#33267;94.7%&#65292;ADDSUB&#20174;74.70%&#25552;&#39640;&#33267;94.10%&#65292;SingleEq&#20174;78.70%&#25552;&#39640;&#33267;91.30%&#65292;AQUA&#20174;31.90%&#25552;&#39640;&#33267;82.33%&#65292;SVAMP&#20174;63.70%&#25552;&#39640;&#33267;79.70%&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#20351;&#29992;&#21069;&#20004;&#20010;&#25345;&#20037;&#36335;&#24452;&#28608;&#27963;&#21040;LLM&#65292;&#29305;&#21035;&#26159;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#20351; SelfzCoT &#22312;&#25152;&#26377;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#22312;GSM8K&#20013;&#65292;MzCoT&#30340;&#20934;&#30830;&#24615;&#20174;40.50%&#25552;&#39640;&#33267;76.32%&#65292;MultiArith&#20174;79.3%&#25552;&#39640;&#33267;96.97%&#65292;ADDSUB&#20174;74.70%&#25552;&#39640;&#33267;92.39%&#65292;SingleEq&#20174;78.70%&#25552;&#39640;&#33267;94.60%&#65292;AQUA&#20174;31.90%&#25552;&#39640;&#33267;79.90%&#65292;SVAMP&#20174;63.70%&#25552;&#39640;&#33267;81.50%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper show a work on better use of LLMs with SelfzCoT a self-prompt zero-shot CoT. Specifically, on the zero-shot arithmetic reasoning tasks, the accuracy of the proposed SelfzCoT is improved with GSM8K from 40.50% to 82.34%, with MultiArith from 79.3% to 94.7%, with ADDSUB from 74.70% to 94.10%, with SingleEq from 78.70% to 91.30%, with AQUA from 31.90% to 82.33%, and with SVAMP from 63.70% to 79.70%. Totally, using the first two lasting path activations to LLM and particularly, the code-level self-prompt, the SelfzCoT has a huge improvement on all six zero-shot arithmetic reasoning tasks. Additionally, our modified zero-shot CoT (MzCoT) also achieves remarkable performance in the reasoning tasks. The accuracy of the proposed MzCoT is enhanced with GSM8K from 40.50% to 76.32%, with MultiArith from 79.3% to 96.97%, with ADDSUB from 74.70% to 92.39%, with SingleEq from 78.70% to 94.60%, with AQUA from 31.90% to 79.90%, and with SVAMP from 63.70% to 81.50%. Notably, SelfzCoT has the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#32456;&#27490;&#33258;&#21160;&#26426;&#65292;&#22312;LTLf&#27169;&#22411;&#26816;&#26597;&#20013;&#20351;&#29992;&#38750;&#32456;&#27490;&#33258;&#21160;&#26426;&#20250;&#26356;&#21152;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.08319</link><description>&lt;p&gt;
&#20851;&#20110;&#26377;&#38480;&#36712;&#36857;&#19978;&#32508;&#21512;&#31574;&#30053;&#30340;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Strategies in Synthesis Over Finite Traces. (arXiv:2305.08319v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#32456;&#27490;&#33258;&#21160;&#26426;&#65292;&#22312;LTLf&#27169;&#22411;&#26816;&#26597;&#20013;&#20351;&#29992;&#38750;&#32456;&#27490;&#33258;&#21160;&#26426;&#20250;&#26356;&#21152;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LTLf&#65288;&#32447;&#24615;&#26102;&#24207;&#36923;&#36753;&#20013;&#30340;&#26377;&#38480;&#36712;&#36857;&#65289;&#30340;&#32508;&#21512;&#26041;&#27861;&#36816;&#29992;&#20110;&#39564;&#35777;&#31574;&#30053;&#30340;&#27491;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;LTLf&#27169;&#22411;&#26816;&#26597;&#24182;&#19981;&#31616;&#21333;&#65292;&#30001;LTLf&#32508;&#21512;&#29983;&#25104;&#30340;&#31574;&#30053;&#21487;&#20197;&#29992;&#21040;&#26377;&#38480;&#20294;&#26080;&#30028;&#38271;&#24230;&#25110;&#26080;&#38480;&#38271;&#24230;&#30340;&#32456;&#27490;&#21644;&#38750;&#32456;&#27490;&#33258;&#21160;&#26426;&#20013;&#12290;&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#38750;&#32456;&#27490;&#33258;&#21160;&#26426;&#19982;&#32456;&#27490;&#33258;&#21160;&#26426;&#22312;&#27169;&#22411;&#26816;&#26597;&#20013;&#30340;&#21306;&#21035;&#65292;&#35770;&#25991;&#20027;&#35201;&#36129;&#29486;&#26159;&#26174;&#31034;&#20986;LTLf&#27169;&#22411;&#26816;&#26597;&#20013;&#20351;&#29992;&#38750;&#32456;&#27490;&#33258;&#21160;&#26426;&#27604;&#32456;&#27490;&#33258;&#21160;&#26426;&#25351;&#25968;&#32423;&#26356;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The innovations in reactive synthesis from {\em Linear Temporal Logics over finite traces} (LTLf) will be amplified by the ability to verify the correctness of the strategies generated by LTLf synthesis tools. This motivates our work on {\em LTLf model checking}. LTLf model checking, however, is not straightforward. The strategies generated by LTLf synthesis may be represented using {\em terminating} transducers or {\em non-terminating} transducers where executions are of finite-but-unbounded length or infinite length, respectively. For synthesis, there is no evidence that one type of transducer is better than the other since they both demonstrate the same complexity and similar algorithms.  In this work, we show that for model checking, the two types of transducers are fundamentally different. Our central result is that LTLf model checking of non-terminating transducers is \emph{exponentially harder} than that of terminating transducers. We show that the problems are EXPSPACE-complete
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05368</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20934;&#22791;&#23601;&#32490;&#20102;&#21527;&#65311;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#19987;&#19994;&#24615;&#36136;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#8212;&#8212;GPT-3.5&#12289;GPT-4&#21644;Bard&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35813;&#35780;&#20272;&#33539;&#22260;&#28085;&#30422;&#20102;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#12289;&#25991;&#26723;&#20998;&#31867;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#8212;&#8212;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#65288;SQP&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#21457;&#19982;&#30456;&#20851;&#20020;&#24202;&#22330;&#26223;&#30456;&#20851;&#30340;&#20449;&#24687;&#24615;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#23450;&#21046;&#21270;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24378;&#35843;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#31574;&#30053;&#21644;&#25552;&#31034;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#22312;&#29273;&#31185;&#20020;&#24202;&#39046;&#22495;&#23454;&#29616;&#33258;&#21160;&#21270;&#21644;&#36328;&#27169;&#24577;&#35786;&#26029;&#30340;&#21487;&#33021;&#24615;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;LLM AI&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29273;&#31185;&#20020;&#24202;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03086</link><description>&lt;p&gt;
ChatGPT&#22609;&#36896;&#29273;&#31185;&#26410;&#26469;&#65306;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model. (arXiv:2304.03086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#22312;&#29273;&#31185;&#20020;&#24202;&#39046;&#22495;&#23454;&#29616;&#33258;&#21160;&#21270;&#21644;&#36328;&#27169;&#24577;&#35786;&#26029;&#30340;&#21487;&#33021;&#24615;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;LLM AI&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29273;&#31185;&#20020;&#24202;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;Generative Pretrained Transformer 4&#65288;GPT-4&#65289;&#30340;&#31934;&#31616;&#21644;&#23545;&#35805;&#21464;&#20307;&#65292;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#37324;&#31243;&#30865;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#12290;&#20107;&#23454;&#19978;&#65292;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#30340;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#23545;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20027;&#35201;&#35752;&#35770;LLMs&#22312;&#29273;&#31185;&#39046;&#22495;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;LLM&#37096;&#32626;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#21160;&#29273;&#31185;&#35786;&#26029;&#21644;&#36328;&#27169;&#24577;&#29273;&#31185;&#35786;&#26029;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#37197;&#22791;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#21333;&#20010;LLM&#21487;&#20197;&#31649;&#29702;&#22810;&#28304;&#25968;&#25454;&#24182;&#36827;&#34892;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#20020;&#24202;&#25805;&#20316;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#26469;&#23637;&#31034;&#38024;&#23545;&#29273;&#31185;&#20020;&#24202;&#24212;&#29992;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#22810;&#27169;&#24577;LLM AI&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;LLMs&#22312;&#25552;&#20379;&#24040;&#22823;&#30340;&#28508;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;
&lt;/p&gt;
&lt;p&gt;
The ChatGPT, as a lite and conversational variant of Generative Pretrained Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large Language Models (LLMs) with billions of parameters. LLMs, in fact, have stirred up a lot of interest among researchers and practitioners by their impressive skills in natural language processing tasks, which have a profound impact on a wide range of fields. This paper mainly discusses the future applications of LLMs in dentistry. We introduce two primary LLM deployment methods in dentistry, including automated dental diagnosis and cross-modal dental diagnosis, and examine their potential applications. Especially, equipped with a cross-modal encoder, a single LLM can manage multi-source data and conduct advanced natural language reasoning to perform complex clinical operations. A use case is presented to demonstrate the potential of a fully automatic Multi-Modal LLM AI system for dentistry clinical application. While LLMs offer significant p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#39640;&#23618;&#34892;&#20026;&#35268;&#21010;&#21644;&#20302;&#23618;&#36712;&#36857;&#35268;&#21010;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#39550;&#39542;&#20013;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13986</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#30340;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#21487;&#35299;&#37322;&#24335;&#36816;&#21160;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpretable Motion Planner for Urban Driving via Hierarchical Imitation Learning. (arXiv:2303.13986v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#39640;&#23618;&#34892;&#20026;&#35268;&#21010;&#21644;&#20302;&#23618;&#36712;&#36857;&#35268;&#21010;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#39550;&#39542;&#20013;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#20915;&#31574;&#21644;&#35268;&#21010;&#27169;&#22359;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#23450;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#39640;&#23618;&#27425;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#34892;&#20026;&#35268;&#21010;&#22120;&#21644;&#20302;&#23618;&#27425;&#30340;&#36712;&#36857;&#35268;&#21010;&#22120;&#65292;&#19981;&#20165;&#26159;&#19968;&#31181;&#20010;&#20307;&#30340;&#25968;&#25454;&#39537;&#21160;&#39550;&#39542;&#31574;&#30053;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36731;&#26494;&#22320;&#23884;&#20837;&#22522;&#20110;&#35268;&#21017;&#30340;&#26550;&#26500;&#20013;&#12290;&#25105;&#20204;&#22312;&#38381;&#29615;&#20223;&#30495;&#21644;&#23454;&#38469;&#39550;&#39542;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#35268;&#21010;&#22120;&#22312;&#22797;&#26434;&#30340;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based approaches have achieved impressive performance for autonomous driving and an increasing number of data-driven works are being studied in the decision-making and planning module. However, the reliability and the stability of the neural network is still full of challenges. In this paper, we introduce a hierarchical imitation method including a high-level grid-based behavior planner and a low-level trajectory planner, which is not only an individual data-driven driving policy and can also be easily embedded into the rule-based architecture. We evaluate our method both in closed-loop simulation and real world driving, and demonstrate the neural network planner has outstanding performance in complex urban autonomous driving scenarios.
&lt;/p&gt;</description></item><item><title>ICICLE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07811</link><description>&lt;p&gt;
ICICLE: &#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICICLE: Interpretable Class Incremental Continual Learning. (arXiv:2303.07811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07811
&lt;/p&gt;
&lt;p&gt;
ICICLE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#33021;&#22815;&#22686;&#37327;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#24536;&#35760;&#20043;&#21069;&#23398;&#20064;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#20419;&#36827;&#26032;&#26087;&#20219;&#21153;&#20043;&#38388;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#23398;&#20064;&#23545;&#35299;&#37322;&#24615;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#39044;&#27979;&#32972;&#21518;&#30340;&#21407;&#29702;&#21487;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#32780;&#25913;&#21464;&#65292;&#23548;&#33268;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#26679;&#26412;&#30340; Interpretable Class-InCremental LEarning (ICICLE) &#26041;&#27861;&#65292;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#30340;&#21019;&#26032;&#28857;&#65306;&#35299;&#37322;&#24615;&#27491;&#21017;&#21270;&#12289;&#20197;&#24494;&#31890;&#31890;&#24230;&#20026;&#22522;&#30784;&#30340;&#21407;&#22411;&#21021;&#22987;&#21270;&#31574;&#30053;&#20197;&#21450;&#38024;&#23545;&#21407;&#22411;&#37096;&#20998;&#30340;&#20219;&#21153;&#26102;&#25928;&#20559;&#24046;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ICICLE&#20943;&#23569;&#20102;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#65292;&#24182;&#19988;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning enables incremental learning of new tasks without forgetting those previously learned, resulting in positive knowledge transfer that can enhance performance on both new and old tasks. However, continual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental LEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach. It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the fine-grained setting; and task-recency bias compensation devoted to prototypical parts. Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the existing exemplar-free me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#26469;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;&#21644;&#24847;&#22270;&#21457;&#29616;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#21644;&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#24847;&#22270;&#65292;&#24182;&#21457;&#29616;&#28508;&#34255;&#22312;&#39046;&#22495;&#22806;&#36755;&#20837;&#20013;&#30340;&#19981;&#21516;&#26410;&#30693;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2303.04134</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;&#21644;&#24847;&#22270;&#21457;&#29616;&#30340;&#28151;&#21512;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Architecture for Out of Domain Intent Detection and Intent Discovery. (arXiv:2303.04134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#26469;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;&#21644;&#24847;&#22270;&#21457;&#29616;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#21644;&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#24847;&#22270;&#65292;&#24182;&#21457;&#29616;&#28508;&#34255;&#22312;&#39046;&#22495;&#22806;&#36755;&#20837;&#20013;&#30340;&#19981;&#21516;&#26410;&#30693;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#26816;&#27979;&#26159;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22359;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#39046;&#22495;&#22806;&#65288;OOS&#65289;&#21644;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#30340;&#36755;&#20837;&#21487;&#33021;&#20250;&#32473;&#36825;&#20123;&#31995;&#32479;&#24102;&#26469;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35757;&#32451;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#24847;&#22270;&#26816;&#27979;&#27169;&#22411;&#38656;&#35201;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#21019;&#24314;&#26631;&#35760;&#25968;&#25454;&#38598;&#32791;&#26102;&#19988;&#38656;&#35201;&#20154;&#21147;&#36164;&#28304;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#23558;&#35782;&#21035;OOD/OOS&#36755;&#20837;&#30340;&#20219;&#21153;&#21629;&#21517;&#20026;OOD/OOS&#24847;&#22270;&#26816;&#27979;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#26032;&#30340;&#24847;&#22270;&#24182;&#23545;OOD&#36755;&#20837;&#36827;&#34892;&#20266;&#26631;&#35760;&#65292;&#21017;&#34987;&#31216;&#20026;&#24847;&#22270;&#21457;&#29616;&#12290;&#22312;OOD&#24847;&#22270;&#26816;&#27979;&#37096;&#20998;&#65292;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#21306;&#20998;&#24050;&#30693;&#24847;&#22270;&#21644;&#26410;&#30693;&#24847;&#22270;&#65292;&#29420;&#31435;&#20110;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#12290;&#20043;&#21518;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;OOD/OOS&#36755;&#20837;&#20013;&#19981;&#21516;&#30340;&#26410;&#30693;&#24847;&#22270;&#12290;&#25105;&#20204;&#36824;&#23545;OOD/OOS&#34920;&#31034;&#24212;&#29992;&#38750;&#32447;&#24615;&#38477;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent Detection is one of the tasks of the Natural Language Understanding (NLU) unit in task-oriented dialogue systems. Out of Scope (OOS) and Out of Domain (OOD) inputs may run these systems into a problem. On the other side, a labeled dataset is needed to train a model for Intent Detection in task-oriented dialogue systems. The creation of a labeled dataset is time-consuming and needs human resources. The purpose of this article is to address mentioned problems. The task of identifying OOD/OOS inputs is named OOD/OOS Intent Detection. Also, discovering new intents and pseudo-labeling of OOD inputs is well known by Intent Discovery. In OOD intent detection part, we make use of a Variational Autoencoder to distinguish between known and unknown intents independent of input data distribution. After that, an unsupervised clustering method is used to discover different unknown intents underlying OOD/OOS inputs. We also apply a non-linear dimensionality reduction on OOD/OOS representations
&lt;/p&gt;</description></item><item><title>TopSpark&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#27493;&#38271;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#19978;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#22266;&#23450;&#26102;&#38388;&#27493;&#38271;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;SNNs&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#37117;&#23454;&#29616;&#26356;&#39640;&#30340;&#33021;&#37327;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.01826</link><description>&lt;p&gt;
TopSpark:&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#19978;&#30340;&#33410;&#33021;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#27493;&#38271;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TopSpark: A Timestep Optimization Methodology for Energy-Efficient Spiking Neural Networks on Autonomous Mobile Agents. (arXiv:2303.01826v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01826
&lt;/p&gt;
&lt;p&gt;
TopSpark&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#27493;&#38271;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#19978;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#22266;&#23450;&#26102;&#38388;&#27493;&#38271;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;SNNs&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#37117;&#23454;&#29616;&#26356;&#39640;&#30340;&#33021;&#37327;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#38656;&#35201;&#20302;&#21151;&#32791;/&#39640;&#25928;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#23436;&#25104;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#22810;&#26679;&#21270;&#30340;&#29615;&#22659;&#65292;&#22240;&#20026;&#31227;&#21160;&#26426;&#22120;&#20154;&#36890;&#24120;&#30001;&#30005;&#27744;&#20379;&#30005;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#20197;&#23454;&#29616;&#36825;&#20123;&#35201;&#27714;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#31232;&#30095;&#35745;&#31639;&#21644;&#20855;&#26377;&#29983;&#29289;&#21551;&#21457;&#24335;&#23398;&#20064;&#26426;&#21046;&#30340;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20379;&#20302;&#21151;&#32791;/&#39640;&#25928;&#33021;&#30340;&#22788;&#29702;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;&#31070;&#32463;&#20803;&#22788;&#29702;&#19968;&#20010;&#33033;&#20914;&#24207;&#21015;&#65288;&#26102;&#38388;&#27493;&#38271;&#65289;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#21487;&#20197;&#20248;&#21270;SNNs&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35774;&#35745;&#25628;&#32034;&#26469;&#30830;&#23450;&#21482;&#33021;&#36827;&#34892;&#25512;&#26029;&#30340;&#22266;&#23450;&#26102;&#38388;&#27493;&#38271;&#35774;&#32622;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;SNNs&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#33021;&#37327;&#25928;&#29575;&#25552;&#39640;&#12290;&#36825;&#20123;&#25216;&#26415;&#36824;&#38480;&#21046;&#20102;SNNs&#22312;&#36816;&#34892;&#26102;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TopSpark&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Autonomous mobile agents require low-power/energy-efficient machine learning (ML) algorithms to complete their ML-based tasks while adapting to diverse environments, as mobile agents are usually powered by batteries. These requirements can be fulfilled by Spiking Neural Networks (SNNs) as they offer low power/energy processing due to their sparse computations and efficient online learning with bio-inspired learning mechanisms for adapting to different environments. Recent works studied that the energy consumption of SNNs can be optimized by reducing the computation time of each neuron for processing a sequence of spikes (timestep). However, state-of-the-art techniques rely on intensive design searches to determine fixed timestep settings for only inference, thereby hindering the SNNs from achieving further energy efficiency gains in both training and inference. These techniques also restrict the SNNs from performing efficient online learning at run time. Toward this, we propose TopSpar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#27431;&#30431;&#25552;&#20986;&#30340;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#21644;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#23545;&#20110;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#24615;&#30340;&#22522;&#26412;&#23450;&#20041;&#65292;&#24378;&#35843;&#20102;&#23558;&#36825;&#20123;&#23450;&#20041;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#25216;&#26415;&#23454;&#36341;&#31526;&#21512;&#27861;&#35268;&#12290;</title><link>http://arxiv.org/abs/2302.10766</link><description>&lt;p&gt;
&#35753;&#20320;&#30340;&#34892;&#20026;&#20117;&#28982;&#26377;&#24207;&#65306;AI&#27861;&#26696;&#21644;&#25216;&#26415;&#36879;&#26126;&#24230;&#30340;&#27604;&#36739;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Get Your Act Together: A Comparative View on Transparency in the AI Act and Technology. (arXiv:2302.10766v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#27431;&#30431;&#25552;&#20986;&#30340;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#21644;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#23545;&#20110;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#24615;&#30340;&#22522;&#26412;&#23450;&#20041;&#65292;&#24378;&#35843;&#20102;&#23558;&#36825;&#20123;&#23450;&#20041;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#25216;&#26415;&#23454;&#36341;&#31526;&#21512;&#27861;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;&#25552;&#20986;&#20102;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#39118;&#38505;&#30340;&#27604;&#20363;&#26041;&#27861;&#26469;&#35268;&#33539;&#20154;&#24037;&#26234;&#33021;&#65292;&#20854;&#20013;&#35814;&#32454;&#35201;&#27714;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#34429;&#28982;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#39046;&#22495;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#35201;&#27714;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#65292;&#20294;&#22312;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20855;&#20307;&#23450;&#20041;&#19978;&#65292;XAI&#19982;&#35813;&#27861;&#26696;&#23384;&#22312;&#22522;&#26412;&#24046;&#24322;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;XAI&#21644;&#27431;&#27954;&#27861;&#35268;&#26159;&#22914;&#20309;&#30475;&#24453;&#36879;&#26126;&#24230;&#30340;&#22522;&#26412;&#23450;&#20041;&#30340;&#65292;&#29305;&#21035;&#26159;AI&#27861;&#26696;&#21644;&#30456;&#20851;&#30340;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26088;&#22312;&#30830;&#23450;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#23545;&#40784;&#30340;&#20027;&#35201;&#35201;&#28857;&#65306;&#28548;&#28165;&#36879;&#26126;&#24230;&#30340;&#33539;&#22260;&#65292;XAI&#30340;&#27861;&#24459;&#22320;&#20301;&#65292;&#30417;&#31649;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The European Union has proposed the Artificial Intelligence Act which introduces a proportional risk-based approach to AI regulation including detailed requirements for transparency and explainability. Many of these requirements may be addressed in practice by the field of explainable AI (XAI), however, there are fundamental differences between XAI and the Act regarding what transparency and explainability are. These basic definitions should be aligned to assure that regulation continually translates into appropriate technical practices. To facilitate this alignment, we first give an overview of how XAI and European regulation view basic definitions of transparency with a particular focus on the AI Act and the related General Data Protection Regulation (GDPR). We then present a comparison of XAI and regulatory approaches to identify the main points that would improve alignment between the fields: clarification of the scope of transparency, the legal status of XAI, oversight issues in c
&lt;/p&gt;</description></item><item><title>Arena-Rosnav 2.0&#26159;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#22522;&#20934;&#27979;&#35797;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#30340;&#24179;&#21488;&#65292;&#23427;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;API&#21644;&#26356;&#30495;&#23454;&#30340;&#27169;&#25311;&#65292;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21644;&#38598;&#25104;&#26368;&#26032;&#30340;&#23548;&#33322;&#26041;&#27861;&#39564;&#35777;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10023</link><description>&lt;p&gt;
Arena-Rosnav 2.0&#65306;&#29992;&#20110;&#22312;&#39640;&#24230;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#24320;&#21457;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Arena-Rosnav 2.0: A Development and Benchmarking Platform for Robot Navigation in Highly Dynamic Environments. (arXiv:2302.10023v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10023
&lt;/p&gt;
&lt;p&gt;
Arena-Rosnav 2.0&#26159;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#22522;&#20934;&#27979;&#35797;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#30340;&#24179;&#21488;&#65292;&#23427;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;API&#21644;&#26356;&#30495;&#23454;&#30340;&#27169;&#25311;&#65292;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21644;&#38598;&#25104;&#26368;&#26032;&#30340;&#23548;&#33322;&#26041;&#27861;&#39564;&#35777;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#25512;&#20986;&#20102;Arena-Rosnav 2.0&#65292;&#36825;&#26159;&#25105;&#20204;&#20043;&#21069;&#30340;&#20316;&#21697;Arena-Bench&#21644;Arena-Rosnav&#30340;&#25193;&#23637;&#29256;&#65292;&#22686;&#21152;&#20102;&#22810;&#31181;&#38468;&#21152;&#27169;&#22359;&#29992;&#20110;&#24320;&#21457;&#21644;&#22522;&#20934;&#27979;&#35797;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#12290;&#35813;&#24179;&#21488;&#36827;&#34892;&#20102;&#26681;&#26412;&#37325;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;API&#65292;&#29992;&#20110;&#28155;&#21152;&#39069;&#22806;&#30340;&#21151;&#33021;&#65292;&#22914;&#35268;&#21010;&#31639;&#27861;&#12289;&#27169;&#25311;&#22120;&#25110;&#35780;&#20272;&#21151;&#33021;&#12290;&#25105;&#20204;&#22686;&#21152;&#20102;&#26356;&#21152;&#30495;&#23454;&#30340;&#27169;&#25311;&#21644;&#34892;&#20154;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#25991;&#26723;&#20197;&#38477;&#20302;&#20837;&#38376;&#38376;&#27099;&#12290;&#25105;&#20204;&#36890;&#36807;&#39318;&#20808;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35201;&#27714;&#26377;&#32463;&#39564;&#30340;&#30740;&#31350;&#32773;&#21644;&#26032;&#30340;&#20174;&#19994;&#32773;&#21644;&#23398;&#29983;&#27979;&#35797;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;&#21453;&#39304;&#22823;&#22810;&#25968;&#26159;&#31215;&#26497;&#30340;&#65292;&#24456;&#22810;&#21442;&#19982;&#32773;&#27491;&#22312;&#23558;&#25105;&#20204;&#30340;&#31995;&#32479;&#29992;&#20110;&#20854;&#20182;&#30740;&#31350;&#39033;&#30446;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25972;&#21512;&#20004;&#20010;&#26032;&#30340;&#27169;&#25311;&#22120;&#21644;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#23548;&#33322;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#39564;&#35777;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following up on our previous works, in this paper, we present Arena-Rosnav 2.0 an extension to our previous works Arena-Bench and Arena-Rosnav, which adds a variety of additional modules for developing and benchmarking robotic navigation approaches. The platform is fundamentally restructured and provides unified APIs to add additional functionalities such as planning algorithms, simulators, or evaluation functionalities. We have included more realistic simulation and pedestrian behavior and provide a profound documentation to lower the entry barrier. We evaluated our system by first, conducting a user study in which we asked experienced researchers as well as new practitioners and students to test our system. The feedback was mostly positive and a high number of participants are utilizing our system for other research endeavors. Finally, we demonstrate the feasibility of our system by integrating two new simulators and a variety of state of the art navigation approaches and benchmark t
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.08913</link><description>&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#25351;&#20195;&#24615;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Referential communication in heterogeneous communities of pre-trained visual deep networks. (arXiv:2302.08913v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08913
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#34987;&#23884;&#20837;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25110;&#26426;&#22120;&#20154;&#31561;&#33258;&#20027;&#20195;&#29702;&#20013;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22312;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#30456;&#20114;&#20043;&#38388;&#36827;&#34892;&#27807;&#36890;&#20197;&#20102;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#12290;&#20316;&#20026;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22312;&#19968;&#32452;&#24322;&#26500;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#32593;&#32476;&#31038;&#21306;&#20013;&#36827;&#34892;"&#25351;&#20195;&#24615;&#27807;&#36890;"&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#21457;&#23637;&#19968;&#31181;&#20849;&#20139;&#21327;&#35758;&#26469;&#25351;&#20195;&#19968;&#32452;&#20505;&#36873;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#12290;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#65292;&#36825;&#31181;&#20849;&#20139;&#21327;&#35758;&#20063;&#21487;&#20197;&#29992;&#26469;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#26368;&#21021;&#19981;&#23646;&#20110;&#29616;&#26377;&#31038;&#21306;&#30340;&#35270;&#35273;&#32593;&#32476;&#21487;&#20197;&#36731;&#26494;&#22320;&#23398;&#20064;&#21040;&#31038;&#21306;&#30340;&#21327;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#20135;&#29983;&#30340;&#21327;&#35758;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of \textit{referential communication} in a community of heterogeneous state-of-the-art pre-trained visual networks, showing that they can develop, in a self-supervised way, a shared protocol to refer to a target object among a set of candidates. This shared protocol can also be used, to some extent, to communicate about previously unseen object categories of different granularity. Moreover, a visual network that was not initially part of an existing community can learn the community's protocol with remarkable ease. Finally, we study, both qualitatively and quantitatively, the properties of the emergent protocol, providing some evi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35774;&#35745;&#25968;&#25454;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#25968;&#25454;&#25910;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#23548;&#33268;&#30340;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#25910;&#38598;&#35745;&#21010;&#12289;&#25910;&#38598;&#30417;&#25511;&#21644;&#25968;&#25454;&#29087;&#24713;&#24230;&#31561;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#22312;&#36328;&#32452;&#20132;&#21449;&#32676;&#20307;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.10319</link><description>&lt;p&gt;
&#35774;&#35745;&#25968;&#25454;&#65306;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#21160;&#25968;&#25454;&#25910;&#38598;&#21644;&#36845;&#20195;
&lt;/p&gt;
&lt;p&gt;
Designing Data: Proactive Data Collection and Iteration for Machine Learning. (arXiv:2301.10319v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35774;&#35745;&#25968;&#25454;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#25968;&#25454;&#25910;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#23548;&#33268;&#30340;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#25910;&#38598;&#35745;&#21010;&#12289;&#25910;&#38598;&#30417;&#25511;&#21644;&#25968;&#25454;&#29087;&#24713;&#24230;&#31561;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#22312;&#36328;&#32452;&#20132;&#21449;&#32676;&#20307;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25910;&#38598;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#20005;&#37325;&#22833;&#36133;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36827;&#34892;&#25968;&#25454;&#21518;&#22788;&#29702;&#65292;&#20294;&#36825;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#24182;&#24456;&#23569;&#20840;&#38754;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#26469;&#36319;&#36394;&#21644;&#31649;&#29702;&#25968;&#25454;&#25910;&#38598;&#12289;&#36845;&#20195;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#20197;&#35780;&#20272;&#25968;&#25454;&#38598;&#26159;&#21542;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#21464;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35774;&#35745;&#25968;&#25454;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#21253;&#25324;&#65288;1&#65289;&#39044;&#25910;&#38598;&#35745;&#21010;&#65292;&#20027;&#21160;&#20419;&#20351;&#24182;&#35760;&#24405;&#39044;&#26399;&#30340;&#25968;&#25454;&#20998;&#24067;&#65307;&#65288;2&#65289;&#25910;&#38598;&#30417;&#25511;&#65292;&#31995;&#32479;&#24615;&#22320;&#40723;&#21169;&#37319;&#26679;&#22810;&#26679;&#24615;&#65307;&#65288;3&#65289;&#25968;&#25454;&#29087;&#24713;&#24230;&#65292;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#35782;&#21035;&#27169;&#22411;&#23545;&#20854;&#19981;&#29087;&#24713;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#23558;&#35774;&#35745;&#25968;&#25454;&#24212;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#24314;&#27169;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36328;&#32452;&#20132;&#21449;&#32676;&#20307;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#65292;&#8220;&#35774;&#35745;&#8221;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#20248;&#20110;&#22823;&#23567;&#30456;&#20284;&#20294;&#38024;&#23545;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lack of diversity in data collection has caused significant failures in machine learning (ML) applications. While ML developers perform post-collection interventions, these are time intensive and rarely comprehensive. Thus, new methods to track &amp; manage data collection, iteration, and model training are necessary for evaluating whether datasets reflect real world variability. We present designing data, an iterative approach to data collection connecting HCI concepts with ML techniques. Our process includes (1) Pre-Collection Planning, to reflexively prompt and document expected data distributions; (2) Collection Monitoring, to systematically encourage sampling diversity; and (3) Data Familiarity, to identify samples that are unfamiliar to a model using density estimation. We apply designing data to a data collection and modeling task. We find models trained on ''designed'' datasets generalize better across intersectional groups than those trained on similarly sized but less targeted da
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;&#26041;&#27861;SpArX&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26356;&#24544;&#23454;&#21644;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2301.09559</link><description>&lt;p&gt;
SpArX: &#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SpArX: Sparse Argumentative Explanations for Neural Networks. (arXiv:2301.09559v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#35770;&#35777;&#35299;&#37322;&#26041;&#27861;SpArX&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26356;&#24544;&#23454;&#21644;&#28145;&#20837;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20294;&#35299;&#37322;&#23427;&#20204;&#30340;&#20915;&#31574;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#35299;&#37322;&#25913;&#21464;&#21333;&#20010;&#36755;&#20837;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#19968;&#33268;&#30340;&#35299;&#37322;&#26410;&#24517;&#24544;&#23454;&#20110;&#20854;&#23454;&#38469;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#26426;&#21046;&#21019;&#24314;&#20102;&#35770;&#35777;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;SpArX&#26041;&#27861;&#39318;&#20808;&#23558;&#22810;&#23618;&#24863;&#30693;&#22120;&#31232;&#30095;&#21270;&#65292;&#21516;&#26102;&#20445;&#25345;&#23613;&#21487;&#33021;&#22810;&#30340;&#21407;&#22987;&#32467;&#26500;&#12290;&#28982;&#21518;&#23558;&#31232;&#30095;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#23450;&#37327;&#35770;&#35777;&#26694;&#26550;&#65292;&#20197;&#25581;&#31034;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#65292;&#20135;&#29983;&#20840;&#23616;&#21644;/&#25110;&#23616;&#37096;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SpArX&#27604;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#32473;&#20986;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#23454;&#38469;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) have various applications in AI, but explaining their decisions remains challenging. Existing approaches often focus on explaining how changing individual inputs affects NNs' outputs. However, an explanation that is consistent with the input-output behaviour of an NN is not necessarily faithful to the actual mechanics thereof. In this paper, we exploit relationships between multi-layer perceptrons (MLPs) and quantitative argumentation frameworks (QAFs) to create argumentative explanations for the mechanics of MLPs. Our SpArX method first sparsifies the MLP while maintaining as much of the original structure as possible. It then translates the sparse MLP into an equivalent QAF to shed light on the underlying decision process of the MLP, producing global and/or local explanations. We demonstrate experimentally that SpArX can give more faithful explanations than existing approaches, while simultaneously providing deeper insights into the actual reasoning process of M
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ACQ&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#27491;&#21512;&#25104;&#26679;&#26412;&#30340;&#27880;&#24847;&#21147;&#65292;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;&#26080;&#25968;&#25454;&#37327;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#24314;&#31435;&#27880;&#24847;&#21147;&#20013;&#24515;&#20301;&#32622;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#65292;&#23454;&#29616;&#20102;&#31867;&#20869;&#20851;&#27880;&#24230;&#30340;&#22343;&#19968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2301.07266</link><description>&lt;p&gt;
ACQ: &#20511;&#21161;&#27880;&#24847;&#21147;&#20462;&#27491;&#25913;&#36827;&#29983;&#25104;&#24335;&#26080;&#25968;&#25454;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ACQ: Improving Generative Data-free Quantization Via Attention Correction. (arXiv:2301.07266v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ACQ&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#27491;&#21512;&#25104;&#26679;&#26412;&#30340;&#27880;&#24847;&#21147;&#65292;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;&#26080;&#25968;&#25454;&#37327;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#24314;&#31435;&#27880;&#24847;&#21147;&#20013;&#24515;&#20301;&#32622;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#65292;&#23454;&#29616;&#20102;&#31867;&#20869;&#20851;&#27880;&#24230;&#30340;&#22343;&#19968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#37327;&#21270;&#26088;&#22312;&#23454;&#29616;&#27169;&#22411;&#37327;&#21270;&#32780;&#19981;&#35775;&#38382;&#20219;&#20309;&#30495;&#23454;&#26679;&#26412;&#12290;&#22312;&#28041;&#21450;&#25968;&#25454;&#38544;&#31169;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#29983;&#25104;&#22120;&#23558;&#22122;&#22768;&#21521;&#37327;&#36716;&#21270;&#20026;&#21512;&#25104;&#26679;&#26412;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26080;&#25968;&#25454;&#37327;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#29983;&#25104;&#24335;&#26080;&#25968;&#25454;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#26679;&#26412;&#21644;&#30495;&#23454;&#26679;&#26412;&#20043;&#38388;&#23384;&#22312;&#27880;&#24847;&#21147;&#24046;&#24322;&#65292;&#36825;&#19968;&#28857;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#38480;&#21046;&#20102;&#37327;&#21270;&#24615;&#33021;&#12290;ACQ&#26159;&#26412;&#25991;&#25552;&#20986;&#30340;&#20462;&#27491;&#21512;&#25104;&#26679;&#26412;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#20013;&#24515;&#20301;&#32622;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#65292;&#26088;&#22312;&#23454;&#29616;&#31867;&#20869;&#20851;&#27880;&#24230;&#30340;&#22343;&#19968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-free quantization aims to achieve model quantization without accessing any authentic sample. It is significant in an application-oriented context involving data privacy. Converting noise vectors into synthetic samples through a generator is a popular data-free quantization method, which is called generative data-free quantization. However, there is a difference in attention between synthetic samples and authentic samples. This is always ignored and restricts the quantization performance. First, since synthetic samples of the same class are prone to have homogenous attention, the quantized network can only learn limited modes of attention. Second, synthetic samples in eval mode and training mode exhibit different attention. Hence, the batch-normalization statistics matching tends to be inaccurate. ACQ is proposed in this paper to fix the attention of synthetic samples. An attention center position-condition generator is established regarding the homogenization of intra-class attent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14454</link><description>&lt;p&gt;
MEAformer: &#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid. (arXiv:2212.14454v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14454
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#21464;&#20307;&#65292;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#26088;&#22312;&#21457;&#29616;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#20855;&#26377;&#30456;&#20851;&#22270;&#20687;&#30340;&#30456;&#21516;&#23454;&#20307;&#12290; &#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#24403;&#21069;&#30340;MMEA&#31639;&#27861;&#37117;&#20840;&#23616;&#37319;&#29992;KG&#32423;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24335;&#23454;&#20307;&#34920;&#31034;&#65292;&#20294;&#24573;&#30053;&#20102;&#20010;&#20307;&#23454;&#20307;&#30340;&#27169;&#24577;&#20559;&#22909;&#21464;&#21270;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#23545;&#27169;&#24577;&#65288;&#20363;&#22914;&#27169;&#31946;&#22270;&#20687;&#21644;&#20851;&#31995;&#65289;&#20013;&#28508;&#22312;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MEAformer&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#20803;&#27169;&#24577;&#28151;&#21512;&#30340;&#22810;&#27169;&#24335;&#23454;&#20307;&#23545;&#40784;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21160;&#24577;&#39044;&#27979;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#31995;&#25968;&#20197;&#36827;&#34892;&#23454;&#20307;&#32423;&#29305;&#24449;&#32858;&#21512;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#30828;&#23454;&#20307;&#37325;&#25773;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#27169;&#31946;&#23454;&#20307;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#22810;&#20010;&#35757;&#32451;&#22330;&#26223;&#65288;&#21253;&#25324;&#26377;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#12289;&#36845;&#20195;&#21644;&#20302;&#36164;&#28304;&#35774;&#32622;&#65289;&#20013;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#21033;&#29992;&#27169;&#24577;&#20559;&#22909;&#21464;&#21270;&#26377;&#25928;&#25552;&#39640;&#20102;MMEA&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an important variant of entity alignment (EA), multi-modal entity alignment (MMEA) aims to discover identical entities across different knowledge graphs (KGs) with relevant images attached. We noticed that current MMEA algorithms all globally adopt the KG-level modality fusion strategies for multi-modal entity representation but ignore the variation in modality preferences for individual entities, hurting the robustness to potential noise involved in modalities (e.g., blurry images and relations). In this paper, we present MEAformer, a multi-modal entity alignment transformer approach for meta modality hybrid, which dynamically predicts the mutual correlation coefficients among modalities for entity-level feature aggregation. A modal-aware hard entity replay strategy is further proposed for addressing vague entity details. Experimental results show that our model not only achieves SOTA performance on multiple training scenarios including supervised, unsupervised, iterative, and low 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21512;&#25104;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#26469;&#22686;&#24378;&#20219;&#21153;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;&#30340;&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#27169;&#22411;PivotBot&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#20999;&#25442;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#21644;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#65292;&#22312;&#22788;&#29702;&#34701;&#21512;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.10008</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#26469;&#22686;&#24378;&#20219;&#21153;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;
&lt;/p&gt;
&lt;p&gt;
Enhancing Task Bot Engagement with Synthesized Open-Domain Dialog. (arXiv:2212.10008v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21512;&#25104;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#26469;&#22686;&#24378;&#20219;&#21153;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#24230;&#30340;&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#27169;&#22411;PivotBot&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#20999;&#25442;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#21644;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#65292;&#22312;&#22788;&#29702;&#34701;&#21512;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#21162;&#21147;&#26469;&#26500;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#22914;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#21644;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#65288;ODD&#65289;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#27169;&#20223;&#20154;&#31867;&#32423;&#21035;&#30340;&#23545;&#35805;&#65292;&#36890;&#24120;&#38656;&#35201;&#34701;&#21512;&#21508;&#31181;&#23545;&#35805;&#27169;&#24335;&#65292;&#24182;&#24314;&#31435;&#19968;&#20010;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;TOD&#21644;ODD&#65292;&#24182;&#35775;&#38382;&#19981;&#21516;&#30693;&#35782;&#28304;&#30340;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#34701;&#21512;&#20219;&#21153;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#32467;&#21512;&#20102;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;ODD&#21644;TOD&#30340;&#23545;&#35805;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;PivotBot&#65292;&#33021;&#22815;&#36866;&#24403;&#22320;&#37319;&#29992;TOD&#21644;ODD&#27169;&#24335;&#65292;&#24182;&#35775;&#38382;&#19981;&#21516;&#30340;&#30693;&#35782;&#28304;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#22788;&#29702;&#34701;&#21512;&#20219;&#21153;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;TOD&#21644;ODD&#20219;&#21153;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many efforts have been made to construct dialog systems for different types of conversations, such as task-oriented dialog (TOD) and open-domain dialog (ODD). To better mimic human-level conversations that usually fuse various dialog modes, it is essential to build a system that can effectively handle both TOD and ODD and access different knowledge sources. To address the lack of available data for the fused task, we propose a framework for automatically generating dialogues that combine knowledge-grounded ODDs and TODs in various settings. Additionally, we introduce a unified model PivotBot that is capable of appropriately adopting TOD and ODD modes and accessing different knowledge sources in order to effectively tackle the fused task. Evaluation results demonstrate the superior ability of the proposed model to switch seamlessly between TOD and ODD tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;StyleGAN&#26469;&#25552;&#21319;&#20154;&#33080;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#38024;&#23545;&#36523;&#20221;&#26377;&#38480;&#30340;&#25968;&#25454;&#38382;&#39064;&#36827;&#34892;&#30340;&#25913;&#36827;&#20197;&#21450;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#25193;&#22686;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10090</link><description>&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;StyleGAN&#25552;&#21319;&#20154;&#33080;&#35782;&#21035;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Boost Face Recognition with StyleGAN?. (arXiv:2210.10090v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;StyleGAN&#26469;&#25552;&#21319;&#20154;&#33080;&#35782;&#21035;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#38024;&#23545;&#36523;&#20221;&#26377;&#38480;&#30340;&#25968;&#25454;&#38382;&#39064;&#36827;&#34892;&#30340;&#25913;&#36827;&#20197;&#21450;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#25193;&#22686;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#12290;&#30001;&#20110;&#20154;&#33080;&#35782;&#21035;&#24212;&#29992;&#20013;&#38544;&#31169;&#30340;&#20248;&#20808;&#32423;&#65292;&#25968;&#25454;&#38480;&#21046;&#22312;&#21517;&#20154;&#32593;&#32476;&#29228;&#34411;&#19978;&#65292;&#20854;&#20013;&#23384;&#22312;&#36523;&#20221;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#30417;&#30563;&#38761;&#21629;&#28608;&#21457;&#20102;&#23558;&#30456;&#20851;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#33080;&#35782;&#21035;&#30340;&#30740;&#31350;&#12290;&#26368;&#27969;&#34892;&#30340;&#23454;&#29992;&#25216;&#24039;&#20043;&#19968;&#26159;&#36890;&#36807;&#20174;&#29983;&#25104;&#27169;&#22411;&#20013;&#32472;&#21046;&#26679;&#26412;&#26469;&#22686;&#21152;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#30041;&#36523;&#20221;&#20449;&#24687;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#23545;StyleGAN&#36827;&#34892;&#24494;&#35843;&#30340;pSp&#32534;&#30721;&#22120;&#30340;&#31616;&#21333;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#35782;&#21035;&#25216;&#26415;&#65292;&#24182;&#19982;&#23545;&#21512;&#25104;&#20154;&#33080;&#36523;&#20221;&#36827;&#34892;&#35757;&#32451;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#25910;&#38598;&#20102;&#20855;&#26377;&#21487;&#25511;&#26063;&#35028;&#26500;&#25104;&#30340;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;--AfricanFaceSet-5M&#65288;&#21253;&#21547;500&#19975;&#24352;&#19981;&#21516;&#20154;&#30340;&#22270;&#20687;&#65289;&#21644;AsianFaceSet-3M&#65288;&#21253;&#21547;300&#19975;&#24352;&#19981;&#21516;&#20154;&#30340;&#22270;&#20687;&#65289;--&#24182;&#23637;&#31034;&#20102;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art face recognition systems require vast amounts of labeled training data. Given the priority of privacy in face recognition applications, the data is limited to celebrity web crawls, which have issues such as limited numbers of identities. On the other hand, self-supervised revolution in the industry motivates research on the adaptation of related techniques to facial recognition. One of the most popular practical tricks is to augment the dataset by the samples drawn from generative models while preserving the identity. We show that a simple approach based on fine-tuning pSp encoder for StyleGAN allows us to improve upon the state-of-the-art facial recognition and performs better compared to training on synthetic face identities. We also collect large-scale unlabeled datasets with controllable ethnic constitution -AfricanFaceSet-5M (5 million images of different people) and AsianFaceSet-3M (3 million images of different people) -- and we show that pretraining on each o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21387;&#32553;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.06116</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
Patching Weak Convolutional Neural Network Models through Modularization and Composition. (arXiv:2209.06116v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21387;&#32553;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#26469;&#25913;&#36827;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#27169;&#22359;&#21270;&#26041;&#27861;CNNSplitter&#65292;&#23427;&#23558;&#20855;&#26377;$N$&#31867;&#20998;&#31867;&#20219;&#21153;&#30340;&#24378;CNN&#27169;&#22411;&#20998;&#35299;&#20026;$N$&#20010;&#36739;&#23567;&#30340;CNN&#27169;&#22359;&#12290;&#27599;&#20010;&#27169;&#22359;&#26159;&#19968;&#20010;&#23376;&#27169;&#22411;&#65292;&#21253;&#21547;&#24378;&#27169;&#22411;&#30340;&#37096;&#20998;&#21367;&#31215;&#26680;&#12290;&#20026;&#20102;&#20462;&#34917;&#22312;&#30446;&#26631;&#31867;&#21035;&#65288;TC&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#24369;CNN&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20174;&#24378;CNN&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#30456;&#24212;&#27169;&#22359;&#30456;&#32467;&#21512;&#12290;&#36825;&#26679;&#65292;&#24369;CNN&#27169;&#22411;&#35782;&#21035;TC&#30340;&#33021;&#21147;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite great success in many applications, deep neural networks are not always robust in practice. For instance, a convolutional neuron network (CNN) model for classification tasks often performs unsatisfactorily in classifying some particular classes of objects. In this work, we are concerned with patching the weak part of a CNN model instead of improving it through the costly retraining of the entire model. Inspired by the fundamental concepts of modularization and composition in software engineering, we propose a compressed modularization approach, CNNSplitter, which decomposes a strong CNN model for $N$-class classification into $N$ smaller CNN modules. Each module is a sub-model containing a part of the convolution kernels of the strong model. To patch a weak CNN model that performs unsatisfactorily on a target class (TC), we compose the weak CNN model with the corresponding module obtained from a strong CNN model. The ability of the weak CNN model to recognize the TC can thus be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2208.09418</link><description>&lt;p&gt;
SAFARI&#65306;&#40065;&#26834;&#24615;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#22810;&#21151;&#33021;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#24314;&#31435;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#36947;&#38556;&#30861;&#12290;&#23613;&#31649;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31038;&#21306;&#20570;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#35299;&#37322;&#32570;&#20047;&#40065;&#26834;&#24615;&#8212;&#8212;&#26080;&#27861;&#21306;&#20998;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#32473;&#23450;&#30340;XAI&#26041;&#27861;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#20849;&#21516;&#24212;&#23545;&#30340;&#20960;&#20010;&#25361;&#25112;&#65306;i)&#29616;&#26377;&#25351;&#26631;&#19981;&#20840;&#38754;&#65307;ii)XAI&#25216;&#26415;&#39640;&#24230;&#24322;&#36136;&#65307;iii)&#35823;&#35299;&#36890;&#24120;&#26159;&#32597;&#35265;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#20998;&#21035;&#28041;&#21450;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#12290;&#20351;&#29992;&#20855;&#26377;&#23450;&#21046;&#36866;&#24212;&#24230;&#20989;&#25968;&#30340;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#26469;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26368;&#22351;&#24773;&#20917;&#35780;&#20272;&#12290;&#20351;&#29992;&#19987;&#38376;&#29992;&#20110;&#20272;&#35745;&#32597;&#35265;&#20107;&#20214;&#27010;&#29575;&#30340;&#23376;&#38598;&#27169;&#25311;&#65288;SS&#65289;&#26469;&#36827;&#34892;&#25972;&#20307;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of Deep Learning (DL) is a barrier to trustworthy AI. Despite great efforts made by the Explainable AI (XAI) community, explanations lack robustness -- indistinguishable input perturbations may lead to different XAI results. Thus, it is vital to assess how robust DL interpretability is, given an XAI method. In this paper, we identify several challenges that the state-of-the-art is unable to cope with collectively: i) existing metrics are not comprehensive; ii) XAI techniques are highly heterogeneous; iii) misinterpretations are normally rare events. To tackle these challenges, we introduce two black-box evaluation methods, concerning the worst-case interpretation discrepancy and a probabilistic notion of how robust in general, respectively. Genetic Algorithm (GA) with bespoke fitness function is used to solve constrained optimisation for efficient worst-case evaluation. Subset Simulation (SS), dedicated to estimate rare event probabilities, is used for evaluating overa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#35843;&#35797;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.09787</link><description>&lt;p&gt;
&#21487;&#20105;&#35758;&#31070;&#32463;&#32593;&#32476;&#30340;&#22240;&#26524;&#21457;&#29616;&#19982;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery and Knowledge Injection for Contestable Neural Networks. (arXiv:2205.09787v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#35843;&#35797;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#23398;&#20064;&#21040;&#20102;&#30456;&#20851;&#30340;&#22240;&#26524;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#65292;&#32780;&#23427;&#20204;&#30340;&#40657;&#31665;&#29305;&#24615;&#20351;&#24471;&#27169;&#22411;&#26500;&#24314;&#32773;&#38590;&#20197;&#29702;&#35299;&#21644;&#35843;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#26426;&#22120;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#23454;&#29616;&#21452;&#21521;&#20114;&#21160;&#12290;&#25152;&#23398;&#27169;&#22411;&#20445;&#35777;&#31526;&#21512;&#22240;&#26524;&#22270;&#24182;&#36981;&#24490;&#19987;&#23478;&#30693;&#35782;&#65292;&#20854;&#20013;&#37096;&#20998;&#30693;&#35782;&#20063;&#21487;&#20197;&#20107;&#20808;&#32473;&#23450;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#34892;&#20026;&#36827;&#34892;&#21487;&#35270;&#21270;&#24182;&#23454;&#29616;&#30693;&#35782;&#27880;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#24182;&#25903;&#25745;&#39044;&#27979;&#30340;&#20174;&#19994;&#32773;&#36827;&#34892;&#35843;&#35797;&#12290;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#39640;&#36798;2.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have proven to be effective at solving machine learning tasks but it is unclear whether they learn any relevant causal relationships, while their black-box nature makes it difficult for modellers to understand and debug them. We propose a novel method overcoming these issues by allowing a two-way interaction whereby neural-network-empowered machines can expose the underpinning learnt causal graphs and humans can contest the machines by modifying the causal graphs before re-injecting them into the machines. The learnt models are guaranteed to conform to the graphs and adhere to expert knowledge, some of which can also be given up-front. By building a window into the model behaviour and enabling knowledge injection, our method allows practitioners to debug networks based on the causal structure discovered from the data and underpinning the predictions. Experiments with real and synthetic tabular data show that our method improves predictive performance up to 2.4x while pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#32455;&#24515;&#29702;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#26089;&#26399;&#24037;&#20316;&#30340;&#26032;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22242;&#38431;&#65292;&#36890;&#36807;&#22797;&#26434;&#31038;&#20132;&#22256;&#22659;&#30340;&#39564;&#35777;&#65292;&#21457;&#29616;&#20998;&#25104;&#22242;&#38431;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#21457;&#23637;&#21512;&#20316;&#30340;&#25919;&#31574;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21327;&#35843;&#21644;&#23398;&#20064;&#22242;&#38431;&#20869;&#30340;&#26032;&#20852;&#35282;&#33394;&#65292;&#24182;&#33719;&#24471;&#26356;&#39640;&#30340;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2205.02328</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#22242;&#38431;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Exploring the Benefits of Teams in Multiagent Learning. (arXiv:2205.02328v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#32455;&#24515;&#29702;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#26089;&#26399;&#24037;&#20316;&#30340;&#26032;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22242;&#38431;&#65292;&#36890;&#36807;&#22797;&#26434;&#31038;&#20132;&#22256;&#22659;&#30340;&#39564;&#35777;&#65292;&#21457;&#29616;&#20998;&#25104;&#22242;&#38431;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#21457;&#23637;&#21512;&#20316;&#30340;&#25919;&#31574;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21327;&#35843;&#21644;&#23398;&#20064;&#22242;&#38431;&#20869;&#30340;&#26032;&#20852;&#35282;&#33394;&#65292;&#24182;&#33719;&#24471;&#26356;&#39640;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38656;&#35201;&#21512;&#20316;&#30340;&#38382;&#39064;&#65292;&#35768;&#22810;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23454;&#26045;&#35299;&#20915;&#26041;&#26696;&#65292;&#26082;&#21487;&#20197;&#26159;&#20010;&#20307;&#26234;&#33021;&#20307;&#20043;&#38388;&#65292;&#20063;&#21487;&#20197;&#26159;&#25972;&#20010;&#20154;&#21475;&#20043;&#38388;&#65292;&#20197;&#36798;&#21040;&#20849;&#21516;&#30446;&#26631;&#12290;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#20027;&#35201;&#22312;&#20914;&#31361;&#26102;&#36827;&#34892;&#30740;&#31350;&#65307;&#28982;&#32780;&#65292;&#32452;&#32455;&#24515;&#29702;&#23398;&#65288;OP&#65289;&#24378;&#35843;&#22242;&#38431;&#22312;&#20154;&#21475;&#20013;&#30340;&#21512;&#20316;&#21644;&#21327;&#35843;&#23398;&#20064;&#19978;&#30340;&#30410;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;OP&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#20851;&#20110;&#22242;&#38431;&#30340;&#26089;&#26399;&#24037;&#20316;&#21551;&#21457;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#30340;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#36817;&#26399;&#22810;&#26234;&#33021;&#20307;RL&#20013;&#27969;&#34892;&#30340;&#22797;&#26434;&#31038;&#20132;&#22256;&#22659;&#39564;&#35777;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#26377;&#19981;&#21512;&#20316;&#30340;&#28608;&#21169;&#65292;&#20998;&#25104;&#22242;&#38431;&#30340;&#26234;&#33021;&#20307;&#20173;&#28982;&#33021;&#22815;&#21457;&#23637;&#20986;&#21512;&#20316;&#21644;&#20114;&#21161;&#30340;&#25919;&#31574;&#12290;&#27492;&#22806;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#26356;&#22909;&#22320;&#21327;&#35843;&#21644;&#23398;&#20064;&#22242;&#38431;&#20869;&#30340;&#26032;&#20852;&#35282;&#33394;&#65292;&#24182;&#33719;&#24471;&#27604;&#25152;&#26377;&#26234;&#33021;&#20307;&#21033;&#30410;&#19968;&#33268;&#26102;&#26356;&#39640;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
For problems requiring cooperation, many multiagent systems implement solutions among either individual agents or across an entire population towards a common goal. Multiagent teams are primarily studied when in conflict; however, organizational psychology (OP) highlights the benefits of teams among human populations for learning how to coordinate and cooperate. In this paper, we propose a new model of multiagent teams for reinforcement learning (RL) agents inspired by OP and early work on teams in artificial intelligence. We validate our model using complex social dilemmas that are popular in recent multiagent RL and find that agents divided into teams develop cooperative pro-social policies despite incentives to not cooperate. Furthermore, agents are better able to coordinate and learn emergent roles within their teams and achieve higher rewards compared to when the interests of all agents are aligned.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#36807;&#31243;&#32467;&#26524;&#39044;&#27979;&#20013;&#24212;&#29992;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#23545;&#20107;&#20214;&#12289;&#26696;&#20363;&#21644;&#25511;&#21046;&#27969;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2203.16073</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#22312;&#36807;&#31243;&#32467;&#26524;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#33719;&#24471;&#21487;&#35299;&#37322;&#21644;&#21487;&#20449;&#27169;&#22411;&#30340;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Explainability in Process Outcome Prediction: Guidelines to Obtain Interpretable and Faithful Models. (arXiv:2203.16073v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#36807;&#31243;&#32467;&#26524;&#39044;&#27979;&#20013;&#24212;&#29992;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#23545;&#20107;&#20214;&#12289;&#26696;&#20363;&#21644;&#25511;&#21046;&#27969;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#25511;&#39046;&#22495;&#24050;&#32463;&#24320;&#22987;&#37319;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#27169;&#22411;&#65292;&#20294;&#35780;&#20272;&#20173;&#20027;&#35201;&#22522;&#20110;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#26410;&#32771;&#34385;&#35299;&#37322;&#30340;&#21487;&#25805;&#20316;&#24615;&#21644;&#24433;&#21709;&#12290;&#26412;&#25991;&#36890;&#36807;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#26469;&#23450;&#20041;&#36807;&#31243;&#32467;&#26524;&#39044;&#27979;&#39046;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20107;&#20214;&#12289;&#26696;&#20363;&#21644;&#25511;&#21046;&#27969;&#30340;&#35282;&#24230;&#36827;&#34892;&#20998;&#26512;&#36825;&#20123;&#29305;&#24615;&#65292;&#36825;&#26159;&#20856;&#22411;&#30340;&#22522;&#20110;&#36807;&#31243;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21313;&#19977;&#20010;&#30495;&#23454;&#20107;&#20214;&#26085;&#24535;&#19978;&#23545;&#19971;&#20010;&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31995;&#21015;&#36879;&#26126;&#21644;&#38750;&#36879;&#26126;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#34917;&#20805;&#20102;&#35299;&#37322;&#24615;&#25216;&#26415;&#12290;&#25509;&#19979;&#26469;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#22871;&#21517;&#20026;X-MOP&#30340;&#25351;&#21335;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#20010;&#20154;&#38656;&#27714;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#37322;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although a recent shift has been made in the field of predictive process monitoring to use models from the explainable artificial intelligence field, the evaluation still occurs mainly through performance-based metrics, thus not accounting for the actionability and implications of the explanations. In this paper, we define explainability through the interpretability of the explanations and the faithfulness of the explainability model in the field of process outcome prediction. The introduced properties are analysed along the event, case, and control flow perspective which are typical for a process-based analysis. This allows comparing inherently created explanations with post-hoc explanations. We benchmark seven classifiers on thirteen real-life events logs, and these cover a range of transparent and non-transparent machine learning and deep learning models, further complemented with explainability techniques. Next, this paper contributes a set of guidelines named X-MOP which allows se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.01937</link><description>&lt;p&gt;
BoMD&#65306;&#36866;&#29992;&#20110;&#22024;&#26434;X&#20809;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#21253;
&lt;/p&gt;
&lt;p&gt;
BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification. (arXiv:2203.01937v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26631;&#31614;&#12289;&#22024;&#26434;CXR&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#25551;&#36848;&#31526;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#38382;&#39064;&#30340;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#20855;&#26377;&#28165;&#27905;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#36825;&#31181;&#25163;&#21160;&#27880;&#37322;&#30340;&#39640;&#25104;&#26412;&#65292;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#21487;&#33021;&#38656;&#35201;&#20381;&#36182;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#22024;&#26434;&#26631;&#31614;&#12290;&#20107;&#23454;&#19978;&#65292;&#35768;&#22810;&#33016;&#37096;X&#20809;&#20998;&#31867;&#22120;&#24050;&#32463;&#20174;&#24102;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20013;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#19981;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#23548;&#33268;&#27425;&#20248;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;CXR&#25968;&#25454;&#38598;&#22823;&#22810;&#26159;&#22810;&#26631;&#35760;&#30340;&#65292;&#22240;&#27492;&#24403;&#21069;&#35774;&#35745;&#29992;&#20110;&#22810;&#31867;&#38382;&#39064;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#19981;&#33021;&#36731;&#26494;&#22320;&#36827;&#34892;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22024;&#26434;&#22810;&#26631;&#31614;CXR&#23398;&#20064;&#65292;&#20854;&#20013;&#26816;&#27979;&#24182;&#24179;&#28369;&#22320;&#37325;&#26032;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#24120;&#35265;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#22522;&#20110;&#34955;&#30340;&#22810;&#26631;&#31614;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#20351;&#29992;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;kNN-KGE&#65292;&#23427;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26368;&#36817;&#37051;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#20801;&#35768;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#34987;&#26126;&#30830;&#22320;&#35760;&#24518;&#65292;&#32780;&#19981;&#26159;&#38544;&#34255;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#24182;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.05575</link><description>&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#25512;&#29702;&#65306;&#26368;&#36817;&#37051;&#30693;&#35782;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;kNN-KGE&#65292;&#23427;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26368;&#36817;&#37051;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#20801;&#35768;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#34987;&#26126;&#30830;&#22320;&#35760;&#24518;&#65292;&#32780;&#19981;&#26159;&#38544;&#34255;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#24182;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#36890;&#24120;&#23558;&#23454;&#20307;&#26144;&#23556;&#21040;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#35780;&#20998;&#20989;&#25968;&#39044;&#27979;&#30446;&#26631;&#23454;&#20307;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#25512;&#29702;&#20986;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#26410;&#30693;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;kNN-KGE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#23558;&#20854;&#23454;&#20307;&#20998;&#24067;&#19982;k&#20010;&#26368;&#36817;&#37051;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#26681;&#25454;&#30693;&#35782;&#23384;&#20648;&#20013;&#23454;&#20307;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#35745;&#31639;&#26368;&#36817;&#37051;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26126;&#30830;&#22320;&#35760;&#24518;&#32597;&#35265;&#25110;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#65292;&#32780;&#19981;&#26159;&#38544;&#34255;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#24402;&#32435;&#21644;&#20256;&#36882;&#24335;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#22312;&#21482;&#26377;&#23569;&#37327;&#19977;&#20803;&#32452;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#21487;&#33021;&#26356;&#23481;&#26131;&#36890;&#36807;&#26126;&#30830;&#30340;&#35760;&#24518;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous knowledge graph embedding approaches usually map entities to representations and utilize score functions to predict the target entities, yet they typically struggle to reason rare or emerging unseen entities. In this paper, we propose kNN-KGE, a new knowledge graph embedding approach with pre-trained language models, by linearly interpolating its entity distribution with k-nearest neighbors. We compute the nearest neighbors based on the distance in the entity embedding space from the knowledge store. Our approach can allow rare or emerging entities to be memorized explicitly rather than implicitly in model parameters. Experimental results demonstrate that our approach can improve inductive and transductive link prediction results and yield better performance for low-resource settings with only a few triples, which might be easier to reason via explicit memory. Code is available at https://github.com/zjunlp/KNN-KG.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2109.12509</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#28145;&#24230;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Deep Exploration for Recommendation Systems. (arXiv:2109.12509v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#24212;&#20174;&#24310;&#36831;&#21453;&#39304;&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24448;&#24448;&#20391;&#37325;&#20110;&#20174;&#29992;&#25143;&#23545;&#21333;&#20010;&#25512;&#33616;&#30340;&#21709;&#24212;&#20013;&#23398;&#20064;&#12290;&#36825;&#20123;&#24037;&#20316;&#21033;&#29992;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20294;&#25918;&#24323;&#20102;&#23398;&#20064;&#29992;&#25143;&#20043;&#21518;&#30340;&#34892;&#20026;&#12290;&#22312;&#36807;&#21435;&#30340;&#24037;&#20316;&#20013;&#65292;&#34429;&#28982;&#33268;&#21147;&#20110;&#20174;&#38543;&#21518;&#30340;&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#20294;&#32570;&#20047;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24341;&#23548;&#24182;&#33719;&#21462;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#12290;&#24403;&#22870;&#21169;&#36739;&#23569;&#26102;&#65292;&#36890;&#36807;&#24341;&#23548;&#25506;&#32034;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#21464;&#24471;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#31995;&#32479;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#22312;&#21333;&#27493;&#25506;&#32034;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26159;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#30340;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommendation systems ought to benefit by probing for and learning from delayed feedback. Research has tended to focus on learning from a user's response to a single recommendation. Such work, which leverages methods of supervised and bandit learning, forgoes learning from the user's subsequent behavior. Where past work has aimed to learn from subsequent behavior, there has been a lack of effective methods for probing to elicit informative delayed feedback. Effective exploration through probing for delayed feedback becomes particularly challenging when rewards are sparse. To address this, we develop deep exploration methods for recommendation systems. In particular, we formulate recommendation as a sequential decision problem and demonstrate benefits of deep exploration over single-step exploration. Our experiments are carried out with high-fidelity industrial-grade simulators and establish large improvements over existing algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#25512;&#26029;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#65292;&#20197;&#35782;&#21035;&#21644;&#29702;&#35299;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#12290;&#36890;&#36807;&#25351;&#32441;&#20272;&#35745;&#32593;&#32476;&#21644;&#35299;&#26512;&#32593;&#32476;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#39044;&#27979;&#29983;&#25104;&#27169;&#22411;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2106.07873</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#25512;&#26029;&#29983;&#25104;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#65306;&#29983;&#25104;&#27169;&#22411;&#30340;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images. (arXiv:2106.07873v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#25512;&#26029;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#65292;&#20197;&#35782;&#21035;&#21644;&#29702;&#35299;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#12290;&#36890;&#36807;&#25351;&#32441;&#20272;&#35745;&#32593;&#32476;&#21644;&#35299;&#26512;&#32593;&#32476;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#39044;&#27979;&#29983;&#25104;&#27169;&#22411;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#21512;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20196;&#20154;&#38590;&#20197;&#21306;&#20998;&#26159;&#30495;&#23454;&#29031;&#29255;&#36824;&#26159;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#35782;&#21035;&#21644;&#29702;&#35299;&#31713;&#25913;&#23186;&#20307;&#23545;&#20110;&#32531;&#35299;&#29983;&#25104;&#27169;&#22411;&#28508;&#22312;&#28389;&#29992;&#30340;&#31038;&#20250;&#20851;&#20999;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#20174;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#25512;&#26029;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#8220;&#27169;&#22411;&#35299;&#26512;&#8221;&#65292;&#36890;&#36807;&#20998;&#26512;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#23545;&#20154;&#31867;&#32780;&#35328;&#20284;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#32441;&#20272;&#35745;&#32593;&#32476;&#65288;FEN&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#24102;&#26377;&#22235;&#20010;&#32422;&#26463;&#30340;&#25351;&#32441;&#20272;&#35745;&#32593;&#32476;&#26469;&#20272;&#35745;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#32441;&#65292;&#20197;&#40723;&#21169;&#25351;&#32441;&#20855;&#26377;&#26399;&#26395;&#30340;&#29305;&#24615;&#65307;&#35299;&#26512;&#32593;&#32476;&#65288;PN&#65289;&#65292;&#20174;&#20272;&#35745;&#30340;&#25351;&#32441;&#20013;&#39044;&#27979;&#32593;&#32476;&#26550;&#26500;&#21644;&#25439;&#22833;&#20989;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20123;&#29983;&#25104;&#27169;&#22411;&#30340;&#22270;&#20687;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art (SOTA) Generative Models (GMs) can synthesize photo-realistic images that are hard for humans to distinguish from genuine photos. Identifying and understanding manipulated media are crucial to mitigate the social concerns on the potential misuse of GMs. We propose to perform reverse engineering of GMs to infer model hyperparameters from the images generated by these models. We define a novel problem, ``model parsing", as estimating GM network architectures and training loss functions by examining their generated images -- a task seemingly impossible for human beings. To tackle this problem, we propose a framework with two components: a Fingerprint Estimation Network (FEN), which estimates a GM fingerprint from a generated image by training with four constraints to encourage the fingerprint to have desired properties, and a Parsing Network (PN), which predicts network architecture and loss functions from the estimated fingerprints. To evaluate our approach, we collect a
&lt;/p&gt;</description></item></channel></rss>