<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#23569;&#37327;&#20154;&#31867;&#25968;&#25454;&#24320;&#22987;&#65292;&#36890;&#36807;&#22870;&#21169;&#22609;&#24418;&#21644;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#21512;&#25104;&#31867;&#20284;&#20154;&#31867;&#20915;&#31574;&#30340;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#21040;&#30005;&#33041;&#28216;&#25103;&#30340;&#36830;&#32493;&#20915;&#31574;&#20219;&#21153;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07280</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22870;&#21169;&#22609;&#24418;&#30340;&#27169;&#20223;&#23398;&#20064;&#21512;&#25104;&#31867;&#20284;&#20154;&#31867;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#36830;&#32493;&#20915;&#31574;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Synthetically Generating Human-like Data for Sequential Decision Making Tasks via Reward-Shaped Imitation Learning. (arXiv:2304.07280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#23569;&#37327;&#20154;&#31867;&#25968;&#25454;&#24320;&#22987;&#65292;&#36890;&#36807;&#22870;&#21169;&#22609;&#24418;&#21644;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#21512;&#25104;&#31867;&#20284;&#20154;&#31867;&#20915;&#31574;&#30340;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#21040;&#30005;&#33041;&#28216;&#25103;&#30340;&#36830;&#32493;&#20915;&#31574;&#20219;&#21153;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22312;&#19982;AI&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#19982;&#30005;&#33041;&#28216;&#25103;&#20132;&#20114;&#65292;&#22914;&#20309;&#21512;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#20915;&#31574;&#30340;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#20154;&#31867;&#20915;&#31574;&#25968;&#25454;&#20837;&#25163;&#65292;&#23558;&#22870;&#21169;&#22609;&#24418;&#30340;&#27010;&#24565;&#19982;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#12289;&#31867;&#20284;&#20110;&#20154;&#31867;&#20915;&#31574;&#30340;&#25968;&#25454;&#12290;&#20316;&#32773;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#22312;&#19968;&#20010;&#23567;&#22411;&#30340;&#30005;&#33041;&#28216;&#25103;&#20013;&#23436;&#25104;&#20102;&#19977;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#36830;&#32493;&#20915;&#31574;&#20219;&#21153;&#65292;&#24182;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#32463;&#39564;&#24615;&#21644;&#32479;&#35745;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#21512;&#25104;&#30340;&#25968;&#25454;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#25968;&#25454;&#65292;&#23454;&#29616;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20915;&#31574;&#21644;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of synthetically generating data that can closely resemble human decisions made in the context of an interactive human-AI system like a computer game. We propose a novel algorithm that can generate synthetic, human-like, decision making data while starting from a very small set of decision making data collected from humans. Our proposed algorithm integrates the concept of reward shaping with an imitation learning algorithm to generate the synthetic data. We have validated our synthetic data generation technique by using the synthetically generated data as a surrogate for human interaction data to solve three sequential decision making tasks of increasing complexity within a small computer game-like setup. Different empirical and statistical analyses of our results show that the synthetically generated data can substitute the human data and perform the game-playing tasks almost indistinguishably, with very low divergence, from a human performing the same tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#23460;&#20869;&#29615;&#22659;&#19979;&#36827;&#34892;&#36816;&#21160;&#30446;&#26631;&#30340;&#23450;&#20301;&#65292;&#20351;&#29992;&#20102;&#32467;&#26500;&#36816;&#21160;&#19982;&#27169;&#25311;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#30740;&#31350;&#32773;&#25972;&#21512;&#20809;&#27969;&#21644;&#30456;&#23545;&#23039;&#24577;&#22238;&#24402;&#26041;&#27861;&#24110;&#21161;&#35299;&#20915;&#20102;&#22240;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#32467;&#26500;&#31561;&#38382;&#39064;&#32780;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#20026;&#23460;&#20869;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.07250</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#20809;&#27969;&#20449;&#24687;&#20013;&#34701;&#21512;&#36816;&#21160;&#32467;&#26500;&#19982;&#27169;&#25311;&#25968;&#25454;&#30340;&#32477;&#23545;&#20301;&#32622;&#22238;&#24402;&#65292;&#35299;&#20915;&#23460;&#20869;&#29615;&#22659;&#23450;&#20301;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments. (arXiv:2304.07250v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#23460;&#20869;&#29615;&#22659;&#19979;&#36827;&#34892;&#36816;&#21160;&#30446;&#26631;&#30340;&#23450;&#20301;&#65292;&#20351;&#29992;&#20102;&#32467;&#26500;&#36816;&#21160;&#19982;&#27169;&#25311;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#30740;&#31350;&#32773;&#25972;&#21512;&#20809;&#27969;&#21644;&#30456;&#23545;&#23039;&#24577;&#22238;&#24402;&#26041;&#27861;&#24110;&#21161;&#35299;&#20915;&#20102;&#22240;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#32467;&#26500;&#31561;&#38382;&#39064;&#32780;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#20026;&#23460;&#20869;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#30340;&#23450;&#20301;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#27604;&#22914;&#26426;&#22120;&#20154;&#12289;&#34394;&#25311;&#21644;&#22686;&#24378;&#29616;&#23454;&#12289;&#21644;&#22312;&#20179;&#24211;&#20013;&#36816;&#36865;&#36135;&#29289;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#20808;&#36827;&#21457;&#23637;&#24050;&#32463;&#20351;&#24471;&#20351;&#29992;&#21333;&#30446;&#35270;&#35273;&#30456;&#26426;&#36827;&#34892;&#23450;&#20301;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#30001;&#20110;&#29615;&#22659;&#26412;&#36523;&#24341;&#36215;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#30340;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#38468;&#21152;&#20449;&#24687;&#21644;&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#22238;&#24402;&#65288;RPR&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20351;&#29992;Lucas-Kanade&#31639;&#27861;&#35745;&#31639;&#36830;&#32493;&#22270;&#20687;&#20043;&#38388;&#30340;&#20809;&#27969;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#23567;&#22411;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#30456;&#23545;&#23039;&#24577;&#12290;&#23558;&#32477;&#23545;&#23039;&#24577;&#21644;&#30456;&#23545;&#23039;&#24577;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;CWE&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#23545;&#20195;&#30721;&#28431;&#27934;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.07232</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#28431;&#27934;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT Model for Vulnerability Detection. (arXiv:2304.07232v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;CWE&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#23545;&#20195;&#30721;&#28431;&#27934;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-3&#27169;&#22411;&#22312;&#20195;&#30721;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20108;&#36827;&#21046;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#23545;CWE&#28431;&#27934;&#36827;&#34892;&#20102;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20915;&#23450;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#22240;&#20026;&#23427;&#22312;&#20854;&#20182;&#22522;&#20110;&#20195;&#30721;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#27604;&#22914;&#35299;&#20915;&#32534;&#31243;&#25361;&#25112;&#21644;&#29702;&#35299;&#39640;&#27700;&#24179;&#30340;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;ChatGPT&#27169;&#22411;&#22312;&#20195;&#30721;&#28431;&#27934;&#26816;&#27979;&#30340;&#20108;&#36827;&#21046;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#22914;&#34394;&#25311;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical report, we evaluated the performance of the ChatGPT and GPT-3 models for the task of vulnerability detection in code. Our evaluation was conducted on our real-world dataset, using binary and multi-label classification tasks on CWE vulnerabilities. We decided to evaluate the model because it has shown good performance on other code-based tasks, such as solving programming challenges and understanding code at a high level. However, we found that the ChatGPT model performed no better than a dummy classifier for both binary and multi-label classification tasks for code vulnerability detection.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;TD-MPC&#26694;&#26550;&#20869;&#20351;&#29992;&#37325;&#26500;&#20989;&#25968;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07219</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Control with Self-supervised Representation Learning. (arXiv:2304.07219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;TD-MPC&#26694;&#26550;&#20869;&#20351;&#29992;&#37325;&#26500;&#20989;&#25968;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#25105;&#20204;&#27809;&#26377;&#30475;&#21040;&#27169;&#22411;&#26080;&#20851;&#25110;&#27169;&#22411;&#22522;&#30784;&#23398;&#20064;&#26041;&#27861;&#26377;&#20219;&#20309;&#37325;&#22823;&#36827;&#23637;&#65292;&#20351;&#24471;&#20854;&#20013;&#19968;&#20010;&#30456;&#23545;&#20110;&#21478;&#19968;&#20010;&#36807;&#26102;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25152;&#20351;&#29992;&#30340;&#25216;&#26415;&#20005;&#37325;&#20381;&#36182;&#20110;&#29992;&#20363;&#22330;&#26223;&#25110;&#20854;&#20182;&#23646;&#24615;&#65292;&#20363;&#22914;&#29615;&#22659;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#26377;&#33258;&#24049;&#30340;&#20248;&#28857;&#65292;&#20363;&#22914;&#26679;&#26412;&#25928;&#29575;&#25110;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#26102;&#65292;&#21487;&#20197;&#32467;&#21512;&#21508;&#33258;&#30340;&#20248;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;TD-MPC&#26694;&#26550;&#23601;&#26159;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20363;&#23376;&#12290;&#19968;&#26041;&#38754;&#65292;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#33719;&#24471;&#33391;&#22909;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#20272;&#35745;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Q&#20989;&#25968;&#29992;&#20110;&#25552;&#20379;&#33391;&#22909;&#30340;&#38271;&#26399;&#20272;&#35745;&#12290;&#19982;MuZero&#31561;&#31639;&#27861;&#31867;&#20284;&#65292;&#20351;&#29992;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#65292;&#20854;&#20013;&#20165;&#23545;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#20197;&#20943;&#23569;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;TD-MPC&#26694;&#26550;&#20869;&#20351;&#29992;&#37325;&#26500;&#20989;&#25968;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;&#36825;&#21487;&#20197;&#21019;&#24314;&#26356;&#20855;&#20449;&#24687;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19968;&#31995;&#21015;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, we have not seen any major developments in model-free or model-based learning methods that would make one obsolete relative to the other. In most cases, the used technique is heavily dependent on the use case scenario or other attributes, e.g. the environment. Both approaches have their own advantages, for example, sample efficiency or computational efficiency. However, when combining the two, the advantages of each can be combined and hence achieve better performance. The TD-MPC framework is an example of this approach. On the one hand, a world model in combination with model predictive control is used to get a good initial estimate of the value function. On the other hand, a Q function is used to provide a good long-term estimate. Similar to algorithms like MuZero a latent state representation is used, where only task-relevant information is encoded to reduce the complexity. In this paper, we propose the use of a reconstruction function within the TD-MPC fram
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;BPM&#30740;&#31350;&#24102;&#26469;&#35832;&#22810;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.07183</link><description>&lt;p&gt;
Just Tell Me: &#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Just Tell Me: Prompt Engineering in Business Process Management. (arXiv:2304.07183v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;BPM&#30740;&#31350;&#24102;&#26469;&#35832;&#22810;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3&#21644;&#20854;&#20182;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#20063;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#65288;BPM&#65289;&#39046;&#22495;&#25104;&#21151;&#24212;&#29992;&#65292;&#20363;&#22914;&#29992;&#20110;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#21644;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#23545;&#25152;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#20013;&#21253;&#25324;&#22823;&#37327;&#21512;&#36866;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35748;&#20026;&#25552;&#31034;&#24037;&#31243;&#21487;&#20197;&#24110;&#21161;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24341;&#20837;BPM&#30740;&#31350;&#12290;&#26412;&#31687;&#25991;&#31456;&#21033;&#29992;&#36825;&#19968;&#35266;&#28857;&#65292;&#36890;&#36807;&#30830;&#23450;&#30456;&#20851;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#20026;BPM&#30740;&#31350;&#30340;&#25552;&#31034;&#24037;&#31243;&#20351;&#29992;&#21046;&#23450;&#30740;&#31350;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3 and several other language models (LMs) can effectively address various natural language processing (NLP) tasks, including machine translation and text summarization. Recently, they have also been successfully employed in the business process management (BPM) domain, e.g., for predictive process monitoring and process extraction from text. This, however, typically requires fine-tuning the employed LM, which, among others, necessitates large amounts of suitable training data. A possible solution to this problem is the use of prompt engineering, which leverages pre-trained LMs without fine-tuning them. Recognizing this, we argue that prompt engineering can help bring the capabilities of LMs to BPM research. We use this position paper to develop a research agenda for the use of prompt engineering for BPM research by identifying the associated potentials and challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07163</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;Bandit&#26041;&#27861;&#30340;&#26174;&#24335;&#22609;&#24418;&#22806;&#37096;&#24314;&#35758;&#31639;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning. (arXiv:2304.07163v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#22806;&#37096;&#25110;&#19987;&#23478;&#30340;&#24314;&#35758;&#34701;&#20837;&#21040;&#23398;&#20064;&#24403;&#20013;&#12290;&#26412;&#25991;&#23558;&#23558;&#23558;&#27492;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#31181;&#22810;&#33218;&#36172;&#21338;&#26426;&#31216;&#20026;&#22609;&#24418;&#36172;&#21338;&#26426;&#65288;shaping-bandits&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;LQR&#21644;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19977;&#31181;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#30340;&#19981;&#21516;&#21407;&#21017;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07143</link><description>&lt;p&gt;
&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on Longitudinal Car-Following Model. (arXiv:2304.07143v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#30340;&#19981;&#21516;&#21407;&#21017;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36319;&#36710;&#27169;&#22411;&#26159;&#20132;&#36890;&#20223;&#30495;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#32463;&#20869;&#32622;&#20110;&#35768;&#22810;&#37197;&#22791;ADAS&#30340;&#27773;&#36710;&#20013;&#12290;&#23545;&#36710;&#36319;&#36710;&#34892;&#20026;&#30340;&#30740;&#31350;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#30001;&#22522;&#26412;&#30340;&#36710;&#36742;&#20132;&#20114;&#36807;&#31243;&#24341;&#36215;&#30340;&#19981;&#21516;&#23439;&#35266;&#29616;&#35937;&#30340;&#26681;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#23613;&#30340;&#35843;&#26597;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#21508;&#31181;&#36710;&#36319;&#36710;&#27169;&#22411;&#20043;&#38388;&#30340;&#21306;&#21035;&#12289;&#20114;&#34917;&#24615;&#21644;&#37325;&#21472;&#20043;&#22788;&#12290;&#35813;&#23457;&#26597;&#23558;&#22312;&#19981;&#21516;&#21407;&#21017;&#20013;&#27010;&#24565;&#21270;&#30340;&#36710;&#36319;&#36710;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The car-following (CF) model is the core component for traffic simulations and has been built-in in many production vehicles with Advanced Driving Assistance Systems (ADAS). Research of CF behavior allows us to identify the sources of different macro phenomena induced by the basic process of pairwise vehicle interaction. The CF behavior and control model encompasses various fields, such as traffic engineering, physics, cognitive science, machine learning, and reinforcement learning. This paper provides a comprehensive survey highlighting differences, complementarities, and overlaps among various CF models according to their underlying logic and principles. We reviewed representative algorithms, ranging from the theory-based kinematic models, stimulus-response models, and cruise control models to data-driven Behavior Cloning (BC) and Imitation Learning (IL) and outlined their strengths and limitations. This review categorizes CF models that are conceptualized in varying principles and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07142</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Data Sampling Strategies for Training Neural Network Speech Separation Models. (arXiv:2304.07142v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20998;&#31163;&#20173;&#28982;&#26159;&#22810;&#35828;&#35805;&#20449;&#21495;&#22788;&#29702;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#35821;&#38899;&#20998;&#31163;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#19968;&#20123;&#27169;&#22411;&#38656;&#35201;&#36739;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#36739;&#39640;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#32553;&#30701;&#35757;&#32451;&#31034;&#20363;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#36825;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24212;&#29992;&#36825;&#20123;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#65288;TSL&#65289;&#38480;&#21046;&#23545;&#20004;&#20010;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#65288;SepFormer&#65292;&#19968;&#20010;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21644;Conv-TasNet&#65292;&#19968;&#20010;&#21367;&#31215;&#27169;&#22411;&#65289;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;WJS0-2Mix&#65292;WHAMR&#21644;Libri2Mix&#25968;&#25454;&#38598;&#26469;&#20998;&#26512;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#21450;&#20854;&#23545;&#35757;&#32451;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20998;&#24067;&#65292;&#24212;&#29992;&#29305;&#23450;&#30340;TSL&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#23545;&#27874;&#24418;&#36215;&#22987;&#32034;&#24341;&#36827;&#34892;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#26356;&#22810;&#29420;&#29305;&#30340;&#31034;&#20363;&#29992;&#20110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech separation remains an important area of multi-speaker signal processing. Deep neural network (DNN) models have attained the best performance on many speech separation benchmarks. Some of these models can take significant time to train and have high memory requirements. Previous work has proposed shortening training examples to address these issues but the impact of this on model performance is not yet well understood. In this work, the impact of applying these training signal length (TSL) limits is analysed for two speech separation models: SepFormer, a transformer model, and Conv-TasNet, a convolutional model. The WJS0-2Mix, WHAMR and Libri2Mix datasets are analysed in terms of signal length distribution and its impact on training efficiency. It is demonstrated that, for specific distributions, applying specific TSL limits results in better performance. This is shown to be mainly due to randomly sampling the start index of the waveforms resulting in more unique examples for tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#28857;&#20113;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#36827;&#34892;&#34920;&#38754;&#20998;&#21106;&#27979;&#35797;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#21019;&#24314;&#20102;TUM-FA\c{C}ADE&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#20419;&#36827;&#22522;&#20110;&#28857;&#20113;&#30340;&#34920;&#38754;&#20998;&#21106;&#20219;&#21153;&#30340;&#24320;&#21457;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;&#22522;&#20934;&#31867;&#22411;&#65292;&#21019;&#24314;&#26356;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.07140</link><description>&lt;p&gt;
TUM-FA\c{C}ADE&#65306;&#29992;&#20110;&#34920;&#38754;&#20998;&#21106;&#30340;&#28857;&#20113;&#22522;&#20934;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
TUM-FA\c{C}ADE: Reviewing and enriching point cloud benchmarks for fa\c{c}ade segmentation. (arXiv:2304.07140v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#28857;&#20113;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#36827;&#34892;&#34920;&#38754;&#20998;&#21106;&#27979;&#35797;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#21019;&#24314;&#20102;TUM-FA\c{C}ADE&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#20419;&#36827;&#22522;&#20110;&#28857;&#20113;&#30340;&#34920;&#38754;&#20998;&#21106;&#20219;&#21153;&#30340;&#24320;&#21457;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;&#22522;&#20934;&#31867;&#22411;&#65292;&#21019;&#24314;&#26356;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#25968;&#25454;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#22478;&#24066;&#21046;&#22270;&#26368;&#20339;&#25968;&#25454;&#31867;&#22411;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#28857;&#20113;&#25968;&#25454;&#38598;&#36890;&#24120;&#34987;&#29992;&#20110;&#21508;&#31181;&#22478;&#24066;&#35299;&#37322;&#26041;&#27861;&#30340;&#22522;&#20934;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#32773;&#25506;&#31350;&#20351;&#29992;&#28857;&#20113;&#22522;&#20934;&#26469;&#36827;&#34892;&#34920;&#38754;&#20998;&#21106;&#12290;&#31283;&#20581;&#30340;&#34920;&#38754;&#20998;&#21106;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#33539;&#22260;&#20174;&#27169;&#25311;&#33258;&#21160;&#39550;&#39542;&#21151;&#33021;&#21040;&#25991;&#21270;&#36951;&#20135;&#20445;&#25252;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20016;&#23500;&#29616;&#26377;&#28857;&#20113;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26032;&#22686;&#20102;&#38754;&#30456;&#20851;&#31867;&#21035;&#65292;&#20197;&#20415;&#20110;&#36827;&#34892;&#34920;&#38754;&#20998;&#21106;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#25193;&#23637;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#23427;&#20204;&#22312;&#34920;&#38754;&#20998;&#21106;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#26041;&#27861;&#21019;&#24314;&#20102;TUM-FA\c{C}ADE&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25193;&#23637;TUM-MLS-2016&#30340;&#21151;&#33021;&#12290;TUM-FA\c{C}ADE&#19981;&#20165;&#21487;&#20197;&#20419;&#36827;&#22522;&#20110;&#28857;&#20113;&#30340;&#34920;&#38754;&#20998;&#21106;&#20219;&#21153;&#30340;&#24320;&#21457;&#65292;&#32780;&#19988;&#25105;&#20204;&#20016;&#23500;&#28857;&#20113;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#22522;&#20934;&#31867;&#22411;&#65292;&#20026;&#22478;&#24066;&#35299;&#37322;&#26041;&#27861;&#21019;&#24314;&#26356;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point clouds are widely regarded as one of the best dataset types for urban mapping purposes. Hence, point cloud datasets are commonly investigated as benchmark types for various urban interpretation methods. Yet, few researchers have addressed the use of point cloud benchmarks for fa\c{c}ade segmentation. Robust fa\c{c}ade segmentation is becoming a key factor in various applications ranging from simulating autonomous driving functions to preserving cultural heritage. In this work, we present a method of enriching existing point cloud datasets with fa\c{c}ade-related classes that have been designed to facilitate fa\c{c}ade segmentation testing. We propose how to efficiently extend existing datasets and comprehensively assess their potential for fa\c{c}ade segmentation. We use the method to create the TUM-FA\c{C}ADE dataset, which extends the capabilities of TUM-MLS-2016. Not only can TUM-FA\c{C}ADE facilitate the development of point-cloud-based fa\c{c}ade segmentation tasks, but our 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;HLTPR@RWTH&#22242;&#38431;&#22312;DSTC9&#21644;DSTC10&#20013;&#20026;&#20219;&#21153;&#23548;&#21521;&#22411;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#25152;&#20570;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#20351;&#36873;&#25321;&#20219;&#21153;&#26356;&#26377;&#25928;&#29575;&#65292;&#22312;DSTC10&#20013;&#25552;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#36866;&#24212;&#29983;&#25104;&#22238;&#31572;&#30340;&#39118;&#26684;&#65292;&#20197;&#21450;&#25552;&#20986;&#20102;&#19968;&#20010;&#22024;&#26434;&#30340;&#36890;&#36947;&#27169;&#22411;&#26469;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22242;&#38431;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07101</link><description>&lt;p&gt;
HLTPR@RWTH&#22312;DSTC9&#21644;DSTC10&#20013;&#30340;&#20219;&#21153;&#23548;&#21521;&#22411;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10. (arXiv:2304.07101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;HLTPR@RWTH&#22242;&#38431;&#22312;DSTC9&#21644;DSTC10&#20013;&#20026;&#20219;&#21153;&#23548;&#21521;&#22411;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#25152;&#20570;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#20351;&#36873;&#25321;&#20219;&#21153;&#26356;&#26377;&#25928;&#29575;&#65292;&#22312;DSTC10&#20013;&#25552;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#36866;&#24212;&#29983;&#25104;&#22238;&#31572;&#30340;&#39118;&#26684;&#65292;&#20197;&#21450;&#25552;&#20986;&#20102;&#19968;&#20010;&#22024;&#26434;&#30340;&#36890;&#36947;&#27169;&#22411;&#26469;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22242;&#38431;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#25105;&#20204;&#22312;&#31532;9&#21644;&#31532;10&#27425;Dialog System Technology Challenges&#65288;DSTC9&#21644;DSTC10&#65289;&#20013;&#20026;&#23545;&#35805;&#22522;&#20110;&#25991;&#26723;&#30340;&#20219;&#21153;&#20316;&#20986;&#30340;&#36129;&#29486;&#12290;&#22312;&#20004;&#27425;&#36845;&#20195;&#20013;&#65292;&#20219;&#21153;&#30001;&#19977;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#65306;&#39318;&#20808;&#26816;&#27979;&#24403;&#21069;&#22238;&#21512;&#26159;&#21542;&#38656;&#35201;&#30693;&#35782;&#65292;&#20854;&#27425;&#36873;&#25321;&#30456;&#20851;&#30340;&#30693;&#35782;&#25991;&#26723;&#65292;&#31532;&#19977;&#29983;&#25104;&#22522;&#20110;&#25152;&#36873;&#25991;&#26723;&#30340;&#22238;&#31572;&#12290;&#23545;&#20110;DSTC9&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#20351;&#36873;&#25321;&#20219;&#21153;&#26356;&#26377;&#25928;&#29575;&#12290;&#20854;&#20013;&#26368;&#22909;&#30340;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#36873;&#25321;&#65292;&#23454;&#38469;&#19978;&#27604;&#21407;&#22987;&#22522;&#32447;&#25913;&#36827;&#20102;&#32467;&#26524;&#65292;&#24182;&#25552;&#39640;&#20102;24&#20493;&#36895;&#24230;&#12290;&#22312;DSTC10&#36845;&#20195;&#20013;&#65292;&#25361;&#25112;&#26159;&#35201;&#20351;&#32463;&#36807;&#20070;&#38754;&#23545;&#35805;&#35757;&#32451;&#30340;&#31995;&#32479;&#33021;&#22815;&#22312;&#22024;&#26434;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#36716;&#24405;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#36866;&#24212;&#29983;&#25104;&#22238;&#31572;&#30340;&#39118;&#26684;&#65292;&#20351;&#20854;&#19982;&#21069;&#26399;&#23545;&#35805;&#30456;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22024;&#26434;&#30340;&#36890;&#36947;&#27169;&#22411;&#26469;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper summarizes our contributions to the document-grounded dialog tasks at the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). In both iterations the task consists of three subtasks: first detect whether the current turn is knowledge seeking, second select a relevant knowledge document, and third generate a response grounded on the selected document. For DSTC9 we proposed different approaches to make the selection task more efficient. The best method, Hierarchical Selection, actually improves the results compared to the original baseline and gives a speedup of 24x. In the DSTC10 iteration of the task, the challenge was to adapt systems trained on written dialogs to perform well on noisy automatic speech recognition transcripts. Therefore, we proposed data augmentation techniques to increase the robustness of the models as well as methods to adapt the style of generated responses to fit well into the proceeding dialog. Additionally, we proposed a noisy channel
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#20960;&#31181;DNN&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;DNN&#23578;&#26080;&#27861;&#24456;&#22909;&#22320;&#22312;&#29305;&#23450;&#31867;&#22411;&#30340;&#22270;&#20687;&#20013;&#25512;&#24191;&#25277;&#35937;&#30340;&#35270;&#35273;&#20851;&#31995;&#65292;&#22240;&#27492;&#25277;&#35937;&#30340;&#35270;&#35273;&#25512;&#29702;&#20173;&#28982;&#26159;DNN&#26080;&#27861;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.07091</link><description>&lt;p&gt;
&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#24449;&#12289;&#24341;&#23548;&#27880;&#24847;&#21147;&#21644;&#22806;&#37096;&#35760;&#24518;&#23545;&#20110;&#25512;&#24191;&#35270;&#35273;&#20851;&#31995;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The role of object-centric representations, guided attention, and external memory on generalizing visual relations. (arXiv:2304.07091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07091
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#20960;&#31181;DNN&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;DNN&#23578;&#26080;&#27861;&#24456;&#22909;&#22320;&#22312;&#29305;&#23450;&#31867;&#22411;&#30340;&#22270;&#20687;&#20013;&#25512;&#24191;&#25277;&#35937;&#30340;&#35270;&#35273;&#20851;&#31995;&#65292;&#22240;&#27492;&#25277;&#35937;&#30340;&#35270;&#35273;&#25512;&#29702;&#20173;&#28982;&#26159;DNN&#26080;&#27861;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25512;&#29702;&#26159;&#35270;&#35273;&#30740;&#31350;&#30340;&#38271;&#26399;&#30446;&#26631;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24212;&#29992;&#20110;&#20174;&#22270;&#20687;&#23398;&#20064;&#35270;&#35273;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#20294;&#22312;&#23398;&#20064;&#20851;&#31995;&#30340;&#25512;&#24191;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#20351;DNN&#33021;&#22815;&#20174;&#22270;&#20687;&#20013;&#23398;&#20064;&#25277;&#35937;&#30340;&#20851;&#31995;&#65292;&#20986;&#29616;&#20102;&#20960;&#31181;&#21019;&#26032;&#30340;DNN&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;DNN&#65292;&#20854;&#20013;&#21253;&#25324;&#35832;&#22914;&#25554;&#27133;&#27880;&#24847;&#21147;&#12289;&#24490;&#29615;&#24341;&#23548;&#27880;&#24847;&#21147;&#21644;&#22806;&#37096;&#35760;&#24518;&#31561;&#26426;&#21046;&#65292;&#29992;&#20110;&#26368;&#31616;&#21333;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65306;&#20915;&#23450;&#20004;&#20010;&#29289;&#20307;&#26159;&#21542;&#30456;&#21516;&#25110;&#19981;&#21516;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#26576;&#20123;&#27169;&#22411;&#22312;&#23558;&#30456;&#21516;-&#19981;&#21516;&#20851;&#31995;&#25512;&#24191;&#21040;&#29305;&#23450;&#31867;&#22411;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#27809;&#26377;&#27169;&#22411;&#33021;&#22815;&#22312;&#20840;&#38754;&#33539;&#22260;&#20869;&#25512;&#24191;&#27492;&#20851;&#31995;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#25277;&#35937;&#30340;&#35270;&#35273;&#25512;&#29702;&#20173;&#28982;&#26159;DNN&#30340;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual reasoning is a long-term goal of vision research. In the last decade, several works have attempted to apply deep neural networks (DNNs) to the task of learning visual relations from images, with modest results in terms of the generalization of the relations learned. In recent years, several innovations in DNNs have been developed in order to enable learning abstract relation from images. In this work, we systematically evaluate a series of DNNs that integrate mechanism such as slot attention, recurrently guided attention, and external memory, in the simplest possible visual reasoning task: deciding whether two objects are the same or different. We found that, although some models performed better than others in generalizing the same-different relation to specific types of images, no model was able to generalize this relation across the board. We conclude that abstract visual reasoning remains largely an unresolved challenge for DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21382;&#21490;&#22320;&#22270;&#21644;&#26368;&#26032;&#21355;&#26143;&#24433;&#20687;&#20013;&#24314;&#31569;&#29289;&#36718;&#24275;&#36827;&#34892;&#23545;&#27604;&#65292;&#23558;&#24314;&#31569;&#29289;&#35821;&#20041;&#20449;&#24687;&#34701;&#20837;&#21464;&#21270;&#26816;&#27979;&#27969;&#31243;&#65292;&#25552;&#39640;&#24314;&#31569;&#29289;&#19982;&#38750;&#24314;&#31569;&#29289;&#29305;&#24449;&#30340;&#21487;&#20998;&#36776;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07076</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21382;&#21490;&#22320;&#22270;&#21644;&#26368;&#26032;&#24433;&#20687;&#21487;&#38752;&#24314;&#31569;&#29289;&#36718;&#24275;&#21464;&#21270;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
BCE-Net: Reliable Building Footprints Change Extraction based on Historical Map and Up-to-Date Images using Contrastive Learning. (arXiv:2304.07076v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21382;&#21490;&#22320;&#22270;&#21644;&#26368;&#26032;&#21355;&#26143;&#24433;&#20687;&#20013;&#24314;&#31569;&#29289;&#36718;&#24275;&#36827;&#34892;&#23545;&#27604;&#65292;&#23558;&#24314;&#31569;&#29289;&#35821;&#20041;&#20449;&#24687;&#34701;&#20837;&#21464;&#21270;&#26816;&#27979;&#27969;&#31243;&#65292;&#25552;&#39640;&#24314;&#31569;&#29289;&#19982;&#38750;&#24314;&#31569;&#29289;&#29305;&#24449;&#30340;&#21487;&#20998;&#36776;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22478;&#24066;&#29615;&#22659;&#24555;&#36895;&#21457;&#23637;&#21644;&#24314;&#31569;&#29289;&#25968;&#25454;&#24211;&#38656;&#27714;&#30340;&#32039;&#36843;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#21382;&#21490;&#22320;&#22270;&#21644;&#21333;&#19968;&#26368;&#26032;&#21355;&#26143;&#24433;&#20687;&#24314;&#31569;&#29289;&#36718;&#24275;&#65292;&#23558;&#24314;&#31569;&#29289;&#35821;&#20041;&#20449;&#24687;&#34701;&#20837;&#21464;&#21270;&#26816;&#27979;&#27969;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#24314;&#31569;&#29289;&#19982;&#38750;&#24314;&#31569;&#29289;&#29305;&#24449;&#30340;&#21487;&#20998;&#36776;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic and periodic recompiling of building databases with up-to-date high-resolution images has become a critical requirement for rapidly developing urban environments. However, the architecture of most existing approaches for change extraction attempts to learn features related to changes but ignores objectives related to buildings. This inevitably leads to the generation of significant pseudo-changes, due to factors such as seasonal changes in images and the inclination of building fa\c{c}ades. To alleviate the above-mentioned problems, we developed a contrastive learning approach by validating historical building footprints against single up-to-date remotely sensed images. This contrastive learning strategy allowed us to inject the semantics of buildings into a pipeline for the detection of changes, which is achieved by increasing the distinguishability of features of buildings from those of non-buildings. In addition, to reduce the effects of inconsistencies between historical 
&lt;/p&gt;</description></item><item><title>CornerFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19981;&#21516;&#24314;&#27169;&#31574;&#30053;&#20110;&#21333;&#20010;&#27169;&#22411;&#20013;&#34701;&#21512;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#26469;&#25552;&#21319;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07072</link><description>&lt;p&gt;
CornerFormer: &#25552;&#21319;&#35282;&#28857;&#34920;&#24449;&#20197;&#36827;&#34892;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction. (arXiv:2304.07072v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07072
&lt;/p&gt;
&lt;p&gt;
CornerFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19981;&#21516;&#24314;&#27169;&#31574;&#30053;&#20110;&#21333;&#20010;&#27169;&#22411;&#20013;&#34701;&#21512;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#26469;&#25552;&#21319;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#37325;&#24314;&#26159;&#19968;&#31181;&#38750;&#24179;&#20961;&#30340;&#23494;&#38598;&#39044;&#27979;&#38382;&#39064;&#65292;&#23427;&#20174;&#26629;&#26684;&#22270;&#20687;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#24314;&#31569;&#35282;&#28857;&#21644;&#36793;&#32536;&#65289;&#65292;&#28982;&#21518;&#30456;&#24212;&#22320;&#37325;&#24314;&#20026;&#20108;&#32500;&#24179;&#38754;&#22270;&#12290;&#19982;&#24120;&#35265;&#30340;&#20998;&#21106;&#25110;&#26816;&#27979;&#38382;&#39064;&#30456;&#27604;&#65292;&#23427;&#26174;&#33879;&#20381;&#36182;&#20110;&#21033;&#29992;&#25972;&#20307;&#20960;&#20309;&#20449;&#24687;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;transformer&#30340;&#26041;&#27861;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#31532;&#19968;&#20010;&#27169;&#22411;&#20013;&#26816;&#27979;&#35282;&#28857;&#65292;&#24182;&#22312;&#31532;&#20108;&#20010;&#27169;&#22411;&#20013;&#20998;&#31867;&#25311;&#35758;&#36793;&#32536;&#65288;&#35282;&#23545;&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23558;&#20004;&#20010;&#38454;&#27573;&#20998;&#24320;&#25104;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#20849;&#20139;&#20027;&#24178;&#32534;&#30721;&#22120;&#12290;&#19982;&#29616;&#26377;&#30340;&#24314;&#27169;&#31574;&#30053;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;&#35282;&#28857;&#34920;&#31034;&#26041;&#27861;&#65306;1&#65289;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#31890;&#24230;&#20013;&#20849;&#20139;&#29305;&#24449;&#65292;&#23427;&#22312;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#20043;&#38388;&#34701;&#21512;&#30693;&#35782;&#65307;2&#65289;&#35282;&#28857;&#20505;&#36873;&#32773;&#26681;&#25454;&#20854;&#26041;&#21521;&#20316;&#20026;&#22235;&#20010;&#28909;&#22270;&#36890;&#36947;&#25552;&#20986;&#12290;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#22343;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;CornerFormer&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;transformer-based&#27169;&#22411;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured reconstruction is a non-trivial dense prediction problem, which extracts structural information (\eg, building corners and edges) from a raster image, then reconstructs it to a 2D planar graph accordingly. Compared with common segmentation or detection problems, it significantly relays on the capability that leveraging holistic geometric information for structural reasoning. Current transformer-based approaches tackle this challenging problem in a two-stage manner, which detect corners in the first model and classify the proposed edges (corner-pairs) in the second model. However, they separate two-stage into different models and only share the backbone encoder. Unlike the existing modeling strategies, we present an enhanced corner representation method: 1) It fuses knowledge between the corner detection and edge prediction by sharing feature in different granularity; 2) Corner candidates are proposed in four heatmap channels w.r.t its direction. Both qualitative and quantita
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#23454;&#20307;&#23545;&#40784;&#31995;&#32479;SEA&#65292;&#23427;&#21253;&#25324;&#20102;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;EA&#27169;&#22411;&#24182;&#33021;&#22815;&#20351;&#29992;&#25143;&#36731;&#26494;&#24314;&#31435;&#12289;&#35780;&#20272;&#33258;&#24049;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;GNN&#30340;EA&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07065</link><description>&lt;p&gt;
SEA: &#19968;&#20010;&#21487;&#25193;&#23637;&#23454;&#20307;&#23545;&#40784;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SEA: A Scalable Entity Alignment System. (arXiv:2304.07065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07065
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#23454;&#20307;&#23545;&#40784;&#31995;&#32479;SEA&#65292;&#23427;&#21253;&#25324;&#20102;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;EA&#27169;&#22411;&#24182;&#33021;&#22815;&#20351;&#29992;&#25143;&#36731;&#26494;&#24314;&#31435;&#12289;&#35780;&#20272;&#33258;&#24049;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;GNN&#30340;EA&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#26088;&#22312;&#22312;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#25214;&#21040;&#30456;&#24212;&#30340;&#23454;&#20307;&#12290;&#29616;&#26377;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#32534;&#30721;&#23454;&#20307;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#22312;&#20840;&#25209;&#37327;&#27169;&#24335;&#19979;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#23454;&#20307;&#23545;&#40784;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#22686;&#24378;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#23454;&#20307;&#23545;&#40784;&#31995;&#32479;SEA&#12290;&#23427;&#33021;&#22815;(i)&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23454;&#20307;&#23545;&#40784;&#65292;(ii)&#21152;&#36895;&#24402;&#19968;&#21270;&#21644;&#35780;&#20272;&#36807;&#31243;&#65292;(iii)&#20026;&#29992;&#25143;&#25552;&#20379;&#28165;&#26224;&#30340;&#32467;&#26524;&#20197;&#20272;&#35745;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#21442;&#25968;&#35774;&#32622;&#12290;SEA&#21482;&#38656;&#35201;&#19968;&#20010;&#22270;&#24418;&#21345;&#23601;&#21487;&#20197;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;SEA&#21253;&#25324;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#65292;&#24182;&#20026;&#29992;&#25143;&#25552;&#20379;&#24555;&#36895;&#24314;&#31435;&#21644;&#35780;&#20272;&#33258;&#24049;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;SEA&#20801;&#35768;&#29992;&#25143;&#22312;&#19981;&#28041;&#21450;&#22797;&#26434;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#23454;&#20307;&#23545;&#40784;&#65292;&#22914;&#36127;&#25277;&#26679;&#21644;GPU&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA) aims to find equivalent entities in different knowledge graphs (KGs). State-of-the-art EA approaches generally use Graph Neural Networks (GNNs) to encode entities. However, most of them train the models and evaluate the results in a fullbatch fashion, which prohibits EA from being scalable on largescale datasets. To enhance the usability of GNN-based EA models in real-world applications, we present SEA, a scalable entity alignment system that enables to (i) train large-scale GNNs for EA, (ii) speed up the normalization and the evaluation process, and (iii) report clear results for users to estimate different models and parameter settings. SEA can be run on a computer with merely one graphic card. Moreover, SEA encompasses six state-of-the-art EA models and provides access for users to quickly establish and evaluate their own models. Thus, SEA allows users to perform EA without being involved in tedious implementations, such as negative sampling and GPU-accelerated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07063</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Existential First Order Queries Inference on Knowledge Graphs. (arXiv:2304.07063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#22320;&#65292;&#22238;&#31572;&#19968;&#38454;&#36923;&#36753;&#20844;&#24335;&#26159;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#28165;&#26224;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#32452;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#23558;&#36923;&#36753;&#36816;&#31639;&#35270;&#20026;&#38598;&#21512;&#36816;&#31639;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#30740;&#31350;&#36981;&#24490;&#30456;&#21516;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#32570;&#20047;&#20174;&#36923;&#36753;&#35282;&#24230;&#36827;&#34892;&#31995;&#32479;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20808;&#21069;&#30740;&#31350;&#35843;&#26597;&#30340;&#26597;&#35810;&#33539;&#22260;&#65292;&#24182;&#20934;&#30830;&#22320;&#30830;&#23450;&#20102;&#23427;&#19982;&#25972;&#20010;&#23384;&#22312;&#24615;&#20844;&#24335;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#26032;&#20844;&#24335;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#35752;&#35770;&#20102;&#21516;&#26102;&#20986;&#29616;&#30340;&#26032;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Specifically, answering first-order logic formulas is of particular interest because of its clear syntax and semantics. Recently, the query embedding method has been proposed which learns the embedding of a set of entities and treats logic operations as set operations. Though there has been much research following the same methodology, it lacks a systematic inspection from the standpoint of logic. In this paper, we characterize the scope of queries investigated previously and precisely identify the gap between it and the whole family of existential formulas. Moreover, we develop a new dataset containing ten new formulas and discuss the new challenges coming simultaneously. Finally, we propose a new search algorithm from fuzzy logic theory which is capable of solving new formulas and outperforming the previous methods in existing formulas.
&lt;/p&gt;</description></item><item><title>DroidBot-GPT&#26159;&#19968;&#27454;&#21033;&#29992;GPT&#27169;&#22411;&#33258;&#21160;&#21270;Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#26377;&#26395;&#25552;&#39640;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07061</link><description>&lt;p&gt;
DroidBot-GPT&#65306;&#22522;&#20110;GPT&#30340;Android UI&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
DroidBot-GPT: GPT-powered UI Automation for Android. (arXiv:2304.07061v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07061
&lt;/p&gt;
&lt;p&gt;
DroidBot-GPT&#26159;&#19968;&#27454;&#21033;&#29992;GPT&#27169;&#22411;&#33258;&#21160;&#21270;Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#26377;&#26395;&#25552;&#39640;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DroidBot-GPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31867;&#20284;GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#21270;&#19982;Android&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20132;&#20114;&#30340;&#24037;&#20855;&#12290;&#32473;&#23450;&#25152;&#38656;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;DroidBot-GPT&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#23558;&#24212;&#29992;&#31243;&#24207;GUI&#29366;&#24577;&#20449;&#24687;&#21644;&#26234;&#33021;&#25163;&#26426;&#23631;&#24149;&#19978;&#21487;&#29992;&#30340;&#25805;&#20316;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#24182;&#35201;&#27714;LLM&#36873;&#25321;&#21160;&#20316;&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;LLM&#36890;&#24120;&#21463;&#36807;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#21253;&#25324;&#21508;&#31181;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#25805;&#20316;&#25351;&#21335;&#65292;&#22240;&#27492;&#23427;&#20855;&#26377;&#26681;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#20316;&#20986;&#21512;&#29702;&#21160;&#20316;&#36873;&#25321;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#33258;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#23545;DroidBot-GPT&#36827;&#34892;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;10&#20010;&#31867;&#21035;&#30340;17&#20010;Android&#24212;&#29992;&#31243;&#24207;&#30340;33&#20010;&#20219;&#21153;&#12290;&#23427;&#21487;&#20197;&#25104;&#21151;&#23436;&#25104;39.39%&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#24179;&#22343;&#37096;&#20998;&#23436;&#25104;&#36827;&#24230;&#32422;&#20026;66.76%&#12290;&#37492;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23436;&#20840;&#33258;&#21160;&#30340;&#65292;&#24182;&#19988;&#29992;&#20110;&#35757;&#32451;LLM&#30340;&#25968;&#25454;&#26159;&#24191;&#27867;&#21487;&#29992;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;DroidBot-GPT&#22312;&#25913;&#21892;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large language models (LLMs) to automate the interactions with Android mobile applications. Given a natural language description of a desired task, DroidBot-GPT can automatically generate and execute actions that navigate the app to complete the task. It works by translating the app GUI state information and the available actions on the smartphone screen to natural language prompts and asking the LLM to make a choice of actions. Since the LLM is typically trained on a large amount of data including the how-to manuals of diverse software applications, it has the ability to make reasonable choices of actions based on the provided information. We evaluate DroidBot-GPT with a self-created dataset that contains 33 tasks collected from 17 Android applications spanning 10 categories. It can successfully complete 39.39% of the tasks, and the average partial completion progress is about 66.76%. Given the fact that our method is f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.07056</link><description>&lt;p&gt;
&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#24863;&#30693;&#36136;&#37327;&#35780;&#20272;&#65306;&#22522;&#20934;&#21644;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method. (arXiv:2304.07056v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#38656;&#27714;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#25104;&#21151;&#20351;&#24471;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#28151;&#21512;&#35270;&#39057;&#32534;&#30721;&#33539;&#22260;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#31354;&#38388;&#21644;&#26102;&#38388;&#22495;&#20013;&#25197;&#26354;&#31867;&#22411;&#30340;&#26497;&#22823;&#22810;&#26679;&#24615;&#65292;&#20174;&#20256;&#32479;&#30340;&#28151;&#21512;&#32534;&#30721;&#26694;&#26550;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#32473;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;VQA&#65289;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#36825;&#26159;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#12290;&#35813;&#25968;&#25454;&#24211;&#21253;&#21547; 3,240 &#20010;&#21387;&#32553;&#30340;&#38754;&#37096;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#22810;&#20010;&#21387;&#32553;&#32423;&#21035;&#65292;&#36825;&#20123;&#29255;&#27573;&#26469;&#33258; 135 &#20010;&#28304;&#35270;&#39057;&#65292;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an exponential increase in the demand for face video compression, and the success of artificial intelligence has expanded the boundaries beyond traditional hybrid video coding. Generative coding approaches have been identified as promising alternatives with reasonable perceptual rate-distortion trade-offs, leveraging the statistical priors of face videos. However, the great diversity of distortion types in spatial and temporal domains, ranging from the traditional hybrid coding frameworks to generative models, present grand challenges in compressed face video quality assessment (VQA). In this paper, we introduce the large-scale Compressed Face Video Quality Assessment (CFVQA) database, which is the first attempt to systematically understand the perceptual quality and diversified compression distortions in face videos. The database contains 3,240 compressed face video clips in multiple compression levels, which are derived from 135 source videos with diversif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;&#30340;&#31532;&#20108;&#23626;&#27604;&#36187;&#32467;&#26524;&#65292;&#39640;&#36136;&#37327;&#30340;SYNS-Patches&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;&#27604;&#36187;&#38590;&#24230;&#65292;&#25152;&#26377;&#25552;&#20132;&#20316;&#21697;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.07051</link><description>&lt;p&gt;
&#31532;&#20108;&#23626;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
The Second Monocular Depth Estimation Challenge. (arXiv:2304.07051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;&#30340;&#31532;&#20108;&#23626;&#27604;&#36187;&#32467;&#26524;&#65292;&#39640;&#36136;&#37327;&#30340;SYNS-Patches&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;&#27604;&#36187;&#38590;&#24230;&#65292;&#25152;&#26377;&#25552;&#20132;&#20316;&#21697;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;&#65288;MDEC&#65289;&#30340;&#31532;&#20108;&#23626;&#27604;&#36187;&#32467;&#26524;&#12290;&#26412;&#27425;&#27604;&#36187;&#25509;&#21463;&#20219;&#20309;&#24418;&#24335;&#26041;&#24335;&#30340;&#30417;&#30563;&#65292;&#21253;&#25324;&#20840;&#30417;&#30563;&#12289;&#33258;&#30417;&#30563;&#12289;&#22810;&#20219;&#21153;&#25110;&#20195;&#29702;&#28145;&#24230;&#12290;&#27604;&#36187;&#30340;&#25968;&#25454;&#38598;&#22522;&#20110;SYNS-Patches&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#36136;&#37327;&#30340;&#23494;&#38598;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#29615;&#22659;&#22810;&#26679;&#24615;&#12290;&#27604;&#36187;&#25910;&#21040;&#20102;8&#20010;&#29420;&#29305;&#30340;&#25552;&#20132;&#20316;&#21697;&#65292;&#25152;&#26377;&#22522;&#20110;&#28857;&#20113;&#25110;&#22522;&#20110;&#22270;&#20687;&#30340;&#25351;&#26631;&#34920;&#29616;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#27700;&#24179;&#12290;&#26368;&#20339;&#30417;&#30563;&#25552;&#20132;&#20316;&#21697;&#30340;&#30456;&#23545;F-&#20998;&#25968;&#25552;&#39640;&#20102;27.62&#65285;&#65292;&#32780;&#26368;&#20339;&#33258;&#30417;&#30563;&#25552;&#20132;&#20316;&#21697;&#25552;&#39640;&#20102;16.61&#65285;&#65292;&#36825;&#20123;&#32467;&#26524;&#20195;&#34920;&#20102;&#30495;&#27491;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the results for the second edition of the Monocular Depth Estimation Challenge (MDEC). This edition was open to methods using any form of supervision, including fully-supervised, self-supervised, multi-task or proxy depth. The challenge was based around the SYNS-Patches dataset, which features a wide diversity of environments with high-quality dense ground-truth. This includes complex natural environments, e.g. forests or fields, which are greatly underrepresented in current benchmarks.  The challenge received eight unique submissions that outperformed the provided SotA baseline on any of the pointcloud- or image-based metrics. The top supervised submission improved relative F-Score by 27.62%, while the top self-supervised improved it by 16.61%. Supervised submissions generally leveraged large collections of datasets to improve data diversity. Self-supervised submissions instead updated the network architecture and pretrained backbones. These results represent a si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#32447;&#30456;&#20851;&#30340;&#26377;&#25439;&#21387;&#32553;&#25216;&#26415;&#26469;&#38477;&#20302;&#23556;&#30005;&#24178;&#28041;&#21487;&#35265;&#24230;&#25968;&#25454;&#20307;&#31215;&#30340;&#26041;&#27861;&#65292;&#23558;&#25972;&#20010;&#21487;&#35265;&#24230;&#25968;&#25454;&#34920;&#31034;&#20026;&#22522;&#32447;&#30340;&#25968;&#25454;&#30697;&#38453;&#38598;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#30340;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#30041;&#35270;&#22330;&#36793;&#32536;&#30340;&#27169;&#31946;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.07050</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#23556;&#30005;&#24178;&#28041;&#25968;&#25454;&#30340;&#26377;&#25439;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossy Compression of Large-Scale Radio Interferometric Data. (arXiv:2304.07050v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#32447;&#30456;&#20851;&#30340;&#26377;&#25439;&#21387;&#32553;&#25216;&#26415;&#26469;&#38477;&#20302;&#23556;&#30005;&#24178;&#28041;&#21487;&#35265;&#24230;&#25968;&#25454;&#20307;&#31215;&#30340;&#26041;&#27861;&#65292;&#23558;&#25972;&#20010;&#21487;&#35265;&#24230;&#25968;&#25454;&#34920;&#31034;&#20026;&#22522;&#32447;&#30340;&#25968;&#25454;&#30697;&#38453;&#38598;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#30340;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#30041;&#35270;&#22330;&#36793;&#32536;&#30340;&#27169;&#31946;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#22522;&#32447;&#30456;&#20851;&#30340;&#26377;&#25439;&#21387;&#32553;&#25216;&#26415;&#26469;&#20943;&#23569;&#21487;&#35265;&#24230;&#25968;&#25454;&#30340;&#20307;&#31215;&#65292;&#21516;&#26102;&#20445;&#30041;&#35270;&#22330;&#36793;&#32536;&#30340;&#27169;&#31946;&#25928;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#30697;&#38453;&#31209;&#21644;&#20302;&#31209;&#36817;&#20284;&#25551;&#36848;&#21407;&#22987;&#21487;&#35265;&#24230;&#25968;&#25454;&#30340;&#22522;&#26412;&#20998;&#37327;&#21644;&#20855;&#20307;&#30340;&#22825;&#31354;&#20998;&#24067;&#30697;&#38453;&#12290;&#22240;&#27492;&#65292;&#25972;&#20010;&#21487;&#35265;&#24230;&#25968;&#25454;&#34987;&#34920;&#31034;&#20026;&#22522;&#32447;&#30340;&#25968;&#25454;&#30697;&#38453;&#38598;&#21512;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#24352;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21387;&#32553;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#31616;&#21333;&#30340;$SVD$&#21644;&#21629;&#21517;&#20026;$BDSVD$&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes to reduce visibility data volume using a baseline-dependent lossy compression technique that preserves smearing at the edges of the field-of-view. We exploit the relation of the rank of a matrix and the fact that a low-rank approximation can describe the raw visibility data as a sum of basic components where each basic component corresponds to a specific Fourier component of the sky distribution. As such, the entire visibility data is represented as a collection of data matrices from baselines, instead of a single tensor. The proposed methods are formulated as follows: provided a large dataset of the entire visibility data; the first algorithm, named $simple~SVD$ projects the data into a regular sampling space of rank$-r$ data matrices. In this space, the data for all the baselines has the same rank, which makes the compression factor equal across all baselines. The second algorithm, named $BDSVD$ projects the data into an irregular sampling space of rank$-r_{pq}$ da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairRec&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#25903;&#25345;&#23545;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20844;&#24179;&#24615;&#27979;&#35797;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#26032;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21457;&#29616;DRS&#20013;&#29420;&#29305;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07030</link><description>&lt;p&gt;
FairRec&#65306;&#29992;&#20110;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
FairRec: Fairness Testing for Deep Recommender Systems. (arXiv:2304.07030v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairRec&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#25903;&#25345;&#23545;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20844;&#24179;&#24615;&#27979;&#35797;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#26032;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21457;&#29616;DRS&#20013;&#29420;&#29305;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;DRS&#65289;&#22312;&#24037;&#19994;&#30028;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20026;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#20415;&#21033;&#65292;&#20294;&#21516;&#26102;&#20063;&#23384;&#22312;&#22810;&#37325;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;&#22238;&#22768;&#23460;&#8221;&#21644;&#8220;&#39532;&#20462;&#25928;&#24212;&#8221;&#65292;&#20854;&#20013;&#8220;&#20844;&#24179;&#24615;&#8221;&#30340;&#27010;&#24565;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24050;&#32463;&#20026;&#20256;&#32479;&#30340;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#24320;&#21457;&#20102;&#35768;&#22810;&#20844;&#24179;&#24615;&#27010;&#24565;&#21644;&#30456;&#24212;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;DRS&#26469;&#35828;&#20960;&#20046;&#26080;&#27861;&#24212;&#29992;&#12290;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#30446;&#21069;&#20173;&#32570;&#20047;&#23545;&#29616;&#26377;&#20844;&#24179;&#24615;&#27010;&#24565;&#19982;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#30340;&#22810;&#20803;&#21270;&#27979;&#35797;&#35201;&#27714;&#20043;&#38388;&#30340;&#31995;&#32479;&#29702;&#35299;&#21644;&#26144;&#23556;&#65292;&#26356;&#19981;&#29992;&#35828;&#36827;&#19968;&#27493;&#30340;&#27979;&#35797;&#25110;&#35843;&#35797;&#27963;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FairRec&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#25903;&#25345;&#20174;&#22810;&#20010;&#33258;&#23450;&#20041;&#35282;&#24230;&#23545;DRS&#36827;&#34892;&#20844;&#24179;&#24615;&#27979;&#35797;&#65292;&#20363;&#22914;&#27169;&#22411;&#25928;&#29992;&#12289;&#39033;&#30446;&#22810;&#26679;&#24615;&#12289;&#39033;&#30446;&#21463;&#27426;&#36814;&#31243;&#24230;&#31561;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#32452;&#26032;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#38024;&#23545;DRS&#20855;&#20307;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21457;&#29616;&#25512;&#33616;&#31995;&#32479;&#29420;&#29305;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#21644;&#20943;&#36731;DRS&#20013;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based recommender systems (DRSs) are increasingly and widely deployed in the industry, which brings significant convenience to people's daily life in different ways. However, recommender systems are also shown to suffer from multiple issues,e.g., the echo chamber and the Matthew effect, of which the notation of "fairness" plays a core role.While many fairness notations and corresponding fairness testing approaches have been developed for traditional deep classification models, they are essentially hardly applicable to DRSs. One major difficulty is that there still lacks a systematic understanding and mapping between the existing fairness notations and the diverse testing requirements for deep recommender systems, not to mention further testing or debugging activities. To address the gap, we propose FairRec, a unified framework that supports fairness testing of DRSs from multiple customized perspectives, e.g., model utility, item diversity, item popularity, etc. We also pr
&lt;/p&gt;</description></item><item><title>SimpLex&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#31616;&#21270;&#33521;&#25991;&#21477;&#23376;&#30340;&#26032;&#22411;&#31616;&#21270;&#26550;&#26500;&#65292;&#23427;&#20351;&#29992;&#35789;&#23884;&#20837;&#25110;&#21477;&#23376;&#36716;&#25442;&#22120;&#26469;&#29983;&#25104;&#31616;&#21270;&#21477;&#23376;&#24182;&#38598;&#25104;&#21040;&#26131;&#29992;&#30340;&#36719;&#20214;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#20013;&#21457;&#29616;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;SARI&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#21017;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.07002</link><description>&lt;p&gt;
SimpLex&#65306;&#19968;&#31181;&#35789;&#27719;&#25991;&#26412;&#31616;&#21270;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SimpLex: a lexical text simplification architecture. (arXiv:2304.07002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07002
&lt;/p&gt;
&lt;p&gt;
SimpLex&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#31616;&#21270;&#33521;&#25991;&#21477;&#23376;&#30340;&#26032;&#22411;&#31616;&#21270;&#26550;&#26500;&#65292;&#23427;&#20351;&#29992;&#35789;&#23884;&#20837;&#25110;&#21477;&#23376;&#36716;&#25442;&#22120;&#26469;&#29983;&#25104;&#31616;&#21270;&#21477;&#23376;&#24182;&#38598;&#25104;&#21040;&#26131;&#29992;&#30340;&#36719;&#20214;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#20013;&#21457;&#29616;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;SARI&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#21017;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#26159;&#23558;&#32473;&#23450;&#21477;&#23376;&#25110;&#25991;&#26412;&#29983;&#25104;&#26131;&#20110;&#29702;&#35299;&#30340;&#21477;&#23376;&#30340;&#36807;&#31243;&#12290;&#31616;&#21270;&#30340;&#30446;&#30340;&#26159;&#22312;&#19981;&#25439;&#22833;&#21547;&#20041;&#25110;&#32454;&#24494;&#24046;&#21035;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#32473;&#23450;&#25991;&#26412;&#25110;&#21477;&#23376;&#30340;&#35789;&#27719;&#21644;&#35821;&#27861;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SimpLex&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#31616;&#21270;&#33521;&#25991;&#21477;&#23376;&#30340;&#26032;&#22411;&#31616;&#21270;&#26550;&#26500;&#12290;&#20026;&#20102;&#29983;&#25104;&#31616;&#21270;&#21477;&#23376;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20351;&#29992;&#35789;&#23884;&#20837;&#65288;&#21363;Word2Vec&#65289;&#21644;&#22256;&#24785;&#24230;&#25110;&#21477;&#23376;&#36716;&#25442;&#22120;&#65288;&#21363;BERT&#12289;RoBERTa&#21644;GPT2&#65289;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#20043;&#19968;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#38598;&#25104;&#21040;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#12289;&#26131;&#20110;&#20351;&#29992;&#30340;&#36719;&#20214;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#65288;&#21363;SARI&#21644;&#22256;&#24785;&#24230;&#38477;&#20302;&#65289;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;&#20174;&#23454;&#39564;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;SARI&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20174;&#22256;&#24785;&#24230;&#26041;&#38754;&#26469;&#30475;&#65292;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text simplification (TS) is the process of generating easy-to-understand sentences from a given sentence or piece of text. The aim of TS is to reduce both the lexical (which refers to vocabulary complexity and meaning) and syntactic (which refers to the sentence structure) complexity of a given text or sentence without the loss of meaning or nuance. In this paper, we present \textsc{SimpLex}, a novel simplification architecture for generating simplified English sentences. To generate a simplified sentence, the proposed architecture uses either word embeddings (i.e., Word2Vec) and perplexity, or sentence transformers (i.e., BERT, RoBERTa, and GPT2) and cosine similarity. The solution is incorporated into a user-friendly and simple-to-use software. We evaluate our system using two metrics, i.e., SARI, and Perplexity Decrease. Experimentally, we observe that the transformer models outperform the other models in terms of the SARI score. However, in terms of Perplexity, the Word-Embeddings-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; H2TNE &#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#26102;&#24577;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#23884;&#20837;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#26102;&#38388;&#21644;&#24322;&#36136;&#24615;&#21452;&#37325;&#32422;&#26463;&#30340;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#19982;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.06970</link><description>&lt;p&gt;
H2TNE&#65306;&#26102;&#24577;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
H2TNE: Temporal Heterogeneous Information Network Embedding in Hyperbolic Spaces. (arXiv:2304.06970v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; H2TNE &#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#26102;&#24577;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#23884;&#20837;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#26102;&#38388;&#21644;&#24322;&#36136;&#24615;&#21452;&#37325;&#32422;&#26463;&#30340;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#19982;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;temporal HIN&#65289;&#23884;&#20837;&#65292;&#26088;&#22312;&#23558;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#21508;&#31181;&#31867;&#22411;&#33410;&#28857;&#34920;&#31034;&#20026;&#20302;&#32500;&#31354;&#38388;&#65292;&#24182;&#21516;&#26102;&#20445;&#30041;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#35768;&#22810;&#20851;&#20110;&#26102;&#38388;HIN&#23884;&#20837;&#30340;&#21162;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#19968;&#20123;&#21487;&#35266;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#32593;&#32476;&#37117;&#26174;&#31034;&#20986;&#20998;&#23618;&#23646;&#24615;&#21644;&#24130;&#24459;&#20998;&#24067;&#65292;&#24182;&#19981;&#26159;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#31561;&#36317;&#30340;&#12290;&#26368;&#36817;&#65292;&#21452;&#26354;&#31354;&#38388;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#23545;&#20855;&#26377;&#20998;&#23618;&#21644;&#24130;&#24459;&#32467;&#26500;&#30340;&#25968;&#25454;&#26159;&#26377;&#25928;&#30340;&#12290;&#21463;&#36825;&#20010;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#26354;&#24322;&#26500;&#26102;&#38388;&#32593;&#32476;&#23884;&#20837;&#65288;H2TNE&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#24577;HIN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#26102;&#38388;&#21644;&#24322;&#36136;&#24615;&#21452;&#37325;&#32422;&#26463;&#30340;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#26469;&#25429;&#25417;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#28982;&#21518;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Temporal heterogeneous information network (temporal HIN) embedding, aiming to represent various types of nodes of different timestamps into low dimensional spaces while preserving structural and semantic information, is of vital importance in diverse real-life tasks. Researchers have made great efforts on temporal HIN embedding in Euclidean spaces and got some considerable achievements. However, there is always a fundamental conflict that many real-world networks show hierarchical property and power-law distribution, and are not isometric of Euclidean spaces. Recently, representation learning in hyperbolic spaces has been proved to be valid for data with hierarchical and power-law structure. Inspired by this character, we propose a hyperbolic heterogeneous temporal network embedding (H2TNE) model for temporal HINs. Specifically, we leverage a temporally and heterogeneously double-constrained random walk strategy to capture the structural and semantic information, and then calculate th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21333;&#30446;&#22270;&#20687;&#28145;&#24230;&#20272;&#35745;&#65292;&#25506;&#32034;&#20102;&#22810;&#31181;&#25193;&#23637;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#35745;&#21010;&#23454;&#29616;&#23039;&#24577;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#25216;&#26415;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#28145;&#24230;&#22270;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06966</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21333;&#30446;&#22270;&#20687;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning based Depth Estimation from Monocular Images. (arXiv:2304.06966v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21333;&#30446;&#22270;&#20687;&#28145;&#24230;&#20272;&#35745;&#65292;&#25506;&#32034;&#20102;&#22810;&#31181;&#25193;&#23637;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#35745;&#21010;&#23454;&#29616;&#23039;&#24577;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#25216;&#26415;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#28145;&#24230;&#22270;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20272;&#35745;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22914;&#30446;&#26631;&#36319;&#36394;&#12289;&#22686;&#24378;&#29616;&#23454;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#28145;&#24230;&#22270;&#65292;&#32473;&#23450;&#19968;&#20010;&#20108;&#32500;&#21333;&#30446; RGB &#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#12290;&#20256;&#32479;&#30340;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#32447;&#32034;&#65292;&#20351;&#29992;&#20102;&#35832;&#22914;&#26497;&#32447;&#20960;&#20309;&#31561;&#27010;&#24565;&#12290;&#38543;&#30528;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#65292;&#28145;&#24230;&#20272;&#35745;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21487;&#33021;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#20197;&#21450;&#26159;&#21542;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#24230;&#37327;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#65292;&#25105;&#20204;&#27491;&#22312;&#32771;&#34385;&#23454;&#29616;&#23039;&#24577;&#20272;&#35745;&#12289;&#39640;&#25928;&#20122;&#20687;&#32032;&#21367;&#31215;&#25554;&#20540;&#12289;&#35821;&#20041;&#20998;&#21106;&#20272;&#35745;&#25216;&#26415;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#32454;&#31890;&#24230;&#21644;&#26356;&#20840;&#23616;&#19968;&#33268;&#30340;&#28145;&#24230;&#22270;&#39044;&#27979;&#12290;&#25105;&#20204;&#36824;&#35745;&#21010;&#25918;&#24323;&#30456;&#26426;&#20869;&#21442;&#25968;&#65292;&#24182;&#35843;&#26597;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#20272;&#35745;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#26159;&#20174;&#21333;&#30446; RGB &#22270;&#20687;&#33258;&#21160;&#29983;&#25104;&#30340;&#65292;&#26080;&#38656;&#22320;&#38754;&#30495;&#23454;&#28145;&#24230;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth Estimation has wide reaching applications in the field of Computer vision such as target tracking, augmented reality, and self-driving cars. The goal of Monocular Depth Estimation is to predict the depth map, given a 2D monocular RGB image as input. The traditional depth estimation methods are based on depth cues and used concepts like epipolar geometry. With the evolution of Convolutional Neural Networks, depth estimation has undergone tremendous strides. In this project, our aim is to explore possible extensions to existing SoTA Deep Learning based Depth Estimation Models and to see whether performance metrics could be further improved. In a broader sense, we are looking at the possibility of implementing Pose Estimation, Efficient Sub-Pixel Convolution Interpolation, Semantic Segmentation Estimation techniques to further enhance our proposed architecture and to provide fine-grained and more globally coherent depth map predictions. We also plan to do away with camera intrinsic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#24182;&#35780;&#20272;&#25552;&#31034;&#24037;&#31243;&#21644;&#26657;&#20934;&#31574;&#30053;&#23545;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20116;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#27599;&#31181;&#31574;&#30053;&#20542;&#21521;&#20110;&#26576;&#20123;&#27169;&#22411;&#65292;&#20294;&#32852;&#21512;&#25928;&#26524;&#20026;&#36127;&#12290;</title><link>http://arxiv.org/abs/2304.06962</link><description>&lt;p&gt;
&#29992;&#20110;&#38646;&#26679;&#26412;&#24120;&#35782;&#25512;&#29702;&#30340;&#25552;&#31034;&#24037;&#31243;&#21644;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning. (arXiv:2304.06962v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24182;&#35780;&#20272;&#25552;&#31034;&#24037;&#31243;&#21644;&#26657;&#20934;&#31574;&#30053;&#23545;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20116;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#27599;&#31181;&#31574;&#30053;&#20542;&#21521;&#20110;&#26576;&#20123;&#27169;&#22411;&#65292;&#20294;&#32852;&#21512;&#25928;&#26524;&#20026;&#36127;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#21644;&#26657;&#20934;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22810;&#39033;&#36873;&#25321;&#24120;&#35782;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20174;&#23454;&#38469;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#30740;&#31350;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#27599;&#31181;&#31574;&#30053;&#37117;&#20542;&#21521;&#20110;&#26576;&#20123;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#32852;&#21512;&#25928;&#26524;&#22823;&#22810;&#20026;&#36127;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering and calibration make large language models excel at reasoning tasks, including multiple choice commonsense reasoning. From a practical perspective, we investigate and evaluate these strategies on smaller language models. Through experiments on five commonsense reasoning benchmarks, we find that each strategy favors certain models, but their joint effects are mostly negative.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;AutoSparse&#30340;&#33258;&#21160;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#26799;&#24230;&#36864;&#28779;&#27861;&#26469;&#26435;&#34913;&#31232;&#30095;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;ResNet50&#21644;MobileNetV1&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.06941</link><description>&lt;p&gt;
AUTOSPARSE:&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#31232;&#30095;&#35757;&#32451;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks. (arXiv:2304.06941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;AutoSparse&#30340;&#33258;&#21160;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#26799;&#24230;&#36864;&#28779;&#27861;&#26469;&#26435;&#34913;&#31232;&#30095;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;ResNet50&#21644;MobileNetV1&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#35757;&#32451;&#26159;&#20943;&#23569;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#25104;&#26412;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#23398;&#20064;&#38408;&#20540;&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#27169;&#22411;&#20013;&#20869;&#22312;&#31232;&#30095;&#24615;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26799;&#24230;&#36864;&#28779;&#27861;&#65288;GA&#65289;&#65292;&#20854;&#20013;&#25513;&#30721;&#26435;&#37325;&#30340;&#26799;&#24230;&#25353;&#38750;&#32447;&#24615;&#26041;&#24335;&#32553;&#23567;&#12290; GA&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#31232;&#30095;&#35825;&#23548;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#32654;&#30340;&#26435;&#34913;&#31232;&#30095;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;GA&#19982;&#26368;&#26032;&#30340;&#21487;&#23398;&#20064;&#20462;&#21098;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#31216;&#20026;AutoSparse&#30340;&#33258;&#21160;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;ImageNet-1K&#19978;&#30340;&#31232;&#30095;ResNet50&#21644;MobileNetV1&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;/&#25110;&#35757;&#32451;/&#25512;&#29702;FLOPS&#20943;&#23569;&#65292;&#20363;&#22914;AutoSparse&#22312;80&#65285;&#30340;&#31232;&#30095;&#19979;&#65292;ResNet50&#22312;ImageNet&#19978;&#23454;&#29616;&#20102;&#65288;2&#20493;&#65292;7&#20493;&#65289;&#30340;&#65288;&#35757;&#32451;&#65292;&#25512;&#29702;&#65289;FLOPS&#20943;&#23569;&#12290;&#26368;&#21518;&#65292;AutoSparse&#22312;&#21019;&#26032;&#24615;&#31232;&#30095;&#39046;&#22495;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse training is emerging as a promising avenue for reducing the computational cost of training neural networks. Several recent studies have proposed pruning methods using learnable thresholds to efficiently explore the non-uniform distribution of sparsity inherent within the models. In this paper, we propose Gradient Annealing (GA), where gradients of masked weights are scaled down in a non-linear manner. GA provides an elegant trade-off between sparsity and accuracy without the need for additional sparsity-inducing regularization. We integrated GA with the latest learnable pruning methods to create an automated sparse training algorithm called AutoSparse, which achieves better accuracy and/or training/inference FLOPS reduction than existing learnable pruning methods for sparse ResNet50 and MobileNetV1 on ImageNet-1K: AutoSparse achieves (2x, 7x) reduction in (training,inference) FLOPS for ResNet50 on ImageNet at 80% sparsity. Finally, AutoSparse outperforms sparse-to-sparse SotA me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CiPR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#36328;&#23454;&#20363;&#27491;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;(GCD)&#30340;&#38382;&#39064;&#12290;&#36873;&#25321;&#37051;&#23621;&#32858;&#31867;(SNC)&#31639;&#27861;&#22312;&#27492;&#36807;&#31243;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.06928</link><description>&lt;p&gt;
CiPR:&#19968;&#31181;&#20855;&#26377;&#36328;&#23454;&#20363;&#27491;&#20851;&#31995;&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;.
&lt;/p&gt;
&lt;p&gt;
CiPR: An Efficient Framework with Cross-instance Positive Relations for Generalized Category Discovery. (arXiv:2304.06928v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06928
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CiPR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#36328;&#23454;&#20363;&#27491;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;(GCD)&#30340;&#38382;&#39064;&#12290;&#36873;&#25321;&#37051;&#23621;&#32858;&#31867;(SNC)&#31639;&#27861;&#22312;&#27492;&#36807;&#31243;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#30340;&#38382;&#39064;&#12290;GCD&#32771;&#34385;&#20102;&#33258;&#21160;&#32858;&#31867;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#24320;&#25918;&#19990;&#30028;&#38382;&#39064;&#65292;&#22312;&#35813;&#25968;&#25454;&#38598;&#20013;&#65292;&#26410;&#26631;&#35760;&#25968;&#25454;&#21253;&#21547;&#26469;&#33258;&#26032;&#31867;&#21035;&#21644;&#24050;&#26631;&#35760;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#27809;&#26377;&#24050;&#30693;&#31867;&#21035;&#25968;&#30340;GCD&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CiPR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#20013;&#34987;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#30340;&#36328;&#23454;&#20363;&#27491;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#26469;&#24341;&#23548;&#34920;&#31034;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#36328;&#23454;&#20363;&#20851;&#31995;&#20197;&#20419;&#36827;&#34920;&#31034;&#23398;&#20064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#20998;&#23618;&#32858;&#31867;&#31639;&#27861;&#65292;&#31216;&#20026;&#36873;&#25321;&#37051;&#23621;&#32858;&#31867;&#65288;SNC&#65289;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#20174;&#30001;&#36873;&#25321;&#37051;&#23621;&#26500;&#36896;&#30340;&#22270;&#20013;&#30340;&#36830;&#36890;&#20998;&#37327;&#20013;&#29983;&#25104;&#32858;&#31867;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;SNC&#20197;&#20415;&#23545;&#20855;&#26377;&#32473;&#23450;&#31867;&#30340;&#26410;&#26631;&#35760;&#23454;&#20363;&#36827;&#34892;&#26631;&#31614;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the issue of generalized category discovery (GCD). GCD considers the open-world problem of automatically clustering a partially labelled dataset, in which the unlabelled data contain instances from novel categories and also the labelled classes. In this paper, we address the GCD problem without a known category number in the unlabelled data. We propose a framework, named CiPR, to bootstrap the representation by exploiting Cross-instance Positive Relations for contrastive learning in the partially labelled data which are neglected in existing methods. First, to obtain reliable cross-instance relations to facilitate the representation learning, we introduce a semi-supervised hierarchical clustering algorithm, named selective neighbor clustering (SNC), which can produce a clustering hierarchy directly from the connected components in the graph constructed by selective neighbors. We also extend SNC to be capable of label assignment for the unlabelled instances with the given clas
&lt;/p&gt;</description></item><item><title>YOLO-Drone&#26159;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#26816;&#27979;&#23567;&#23610;&#24230;&#29289;&#20307;&#30340;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;&#26080;&#20154;&#26426;&#24179;&#21488;&#19978;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#20197;&#24191;&#20041;&#20132;&#38598;&#32852;&#30431;(GIOU)&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.06925</link><description>&lt;p&gt;
YOLO-Drone: &#39640;&#31354;&#21363;&#26102;&#26816;&#27979;&#23494;&#38598;&#23567;&#29289;&#20307;&#30340;&#26080;&#20154;&#26426;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
YOLO-Drone:Airborne real-time detection of dense small objects from high-altitude perspective. (arXiv:2304.06925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06925
&lt;/p&gt;
&lt;p&gt;
YOLO-Drone&#26159;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#26816;&#27979;&#23567;&#23610;&#24230;&#29289;&#20307;&#30340;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;&#26080;&#20154;&#26426;&#24179;&#21488;&#19978;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#20197;&#24191;&#20041;&#20132;&#38598;&#32852;&#30431;(GIOU)&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#36828;&#31243;&#24863;&#24212;&#30446;&#26631;&#25506;&#27979;&#25216;&#26415;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#24182;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29289;&#20307;&#22823;&#23567;&#12289;&#22270;&#20687;&#38477;&#22122;&#21644;&#23454;&#26102;&#24615;&#31561;&#22240;&#32032;&#30340;&#25361;&#25112;&#65292;&#23567;&#23610;&#24230;&#29289;&#20307;&#30340;&#21487;&#38752;&#26816;&#27979;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;(YOLO-Drone)&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#31181;&#26032;&#30340;&#26080;&#20154;&#26426;&#24179;&#21488;&#20197;&#21450;&#19968;&#31181;&#29305;&#23450;&#30340;&#20809;&#28304;(&#30789;&#22522;&#37329;LED)&#12290;YOLO-Drone&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#65306;1)&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#39592;&#24178;Darknet59&#65307;2)&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;MSPP-FPN&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#21644;&#19977;&#20010;&#25193;&#24352;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#65307;3)&#20351;&#29992;&#24191;&#20041;&#20132;&#38598;&#32852;&#30431;(Generalized Intersection over Union&#65292;GIOU)&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#24615;&#33021;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;YOLO-Drone&#22312;&#23567;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unmanned Aerial Vehicles (UAVs), specifically drones equipped with remote sensing object detection technology, have rapidly gained a broad spectrum of applications and emerged as one of the primary research focuses in the field of computer vision. Although UAV remote sensing systems have the ability to detect various objects, small-scale objects can be challenging to detect reliably due to factors such as object size, image degradation, and real-time limitations. To tackle these issues, a real-time object detection algorithm (YOLO-Drone) is proposed and applied to two new UAV platforms as well as a specific light source (silicon-based golden LED). YOLO-Drone presents several novelties: 1) including a new backbone Darknet59; 2) a new complex feature aggregation module MSPP-FPN that incorporated one spatial pyramid pooling and three atrous spatial pyramid pooling modules; 3) and the use of Generalized Intersection over Union (GIoU) as the loss function. To evaluate performance, two bench
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#32422;&#26463;&#30340;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;&#12290;&#31639;&#27861;&#22522;&#20110;&#22312;&#28151;&#21512;&#31354;&#38388;&#20013;&#29983;&#38271;&#65288;&#25628;&#32034;&#65289;&#28216;&#25103;&#26641;&#65292;&#20197;&#21512;&#25104;&#19968;&#31181;&#21453;&#24212;&#65288;&#40065;&#26834;&#65289;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#30446;&#26631;&#24182;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.06876</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#21453;&#24212;&#32508;&#21512;&#31639;&#27861;&#24212;&#29992;&#20110;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Reactive Synthesis for Nondeterministic Hybrid Systems. (arXiv:2304.06876v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#32422;&#26463;&#30340;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;&#12290;&#31639;&#27861;&#22522;&#20110;&#22312;&#28151;&#21512;&#31354;&#38388;&#20013;&#29983;&#38271;&#65288;&#25628;&#32034;&#65289;&#28216;&#25103;&#26641;&#65292;&#20197;&#21512;&#25104;&#19968;&#31181;&#21453;&#24212;&#65288;&#40065;&#26834;&#65289;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#30446;&#26631;&#24182;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#32422;&#26463;&#30340;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#28151;&#21512;&#31995;&#32479;&#30340;&#28436;&#21270;&#35270;&#20026;&#19968;&#20010;&#21452;&#20154;&#28216;&#25103;&#65292;&#20854;&#20013;&#38750;&#30830;&#23450;&#24615;&#26159;&#19968;&#20010;&#23545;&#25163;&#29609;&#23478;&#65292;&#20854;&#30446;&#26631;&#26159;&#38459;&#27490;&#23454;&#29616;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#30446;&#26631;&#12290;&#26088;&#22312;&#21512;&#25104;&#19968;&#31181;&#33719;&#32988;&#31574;&#30053;&#8212;&#8212;&#19968;&#31181;&#21453;&#24212;&#65288;&#40065;&#26834;&#65289;&#31574;&#30053;&#65292;&#23427;&#20445;&#35777;&#22312;&#23545;&#25163;&#29609;&#23478;&#30340;&#25152;&#26377;&#21487;&#33021;&#31227;&#21160;&#19979;&#28385;&#36275;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#22312;&#28151;&#21512;&#31354;&#38388;&#20013;&#29983;&#38271;&#65288;&#25628;&#32034;&#65289;&#28216;&#25103;&#26641;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#26041;&#27861;&#19982;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#21644;&#25913;&#36827;&#37096;&#20998;&#31574;&#30053;&#30340;&#26032;&#22411;&#20056;&#23458;&#33329;&#26426;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#30340;&#26465;&#20214;&#19979;&#65292;&#31639;&#27861;&#26159;&#27010;&#29575;&#19978;&#23436;&#22791;&#30340;&#65292;&#21363;&#65292;&#22914;&#26524;&#23384;&#22312;&#33719;&#32988;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25214;&#21040;&#23427;&#12290;&#26696;&#20363;&#30740;&#31350;&#21644;&#22522;&#20934;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a sampling-based strategy synthesis algorithm for nondeterministic hybrid systems with complex continuous dynamics under temporal and reachability constraints. We view the evolution of the hybrid system as a two-player game, where the nondeterminism is an adversarial player whose objective is to prevent achieving temporal and reachability goals. The aim is to synthesize a winning strategy -- a reactive (robust) strategy that guarantees the satisfaction of the goals under all possible moves of the adversarial player. The approach is based on growing a (search) game-tree in the hybrid space by combining a sampling-based planning method with a novel bandit-based technique to select and improve on partial strategies. We provide conditions under which the algorithm is probabilistically complete, i.e., if a winning strategy exists, the algorithm will almost surely find it. The case studies and benchmark results show that the algorithm is general and consistently outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.06848</link><description>&lt;p&gt;
CAR-DESPOT: &#38024;&#23545;&#28151;&#26434;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#24037;&#20316;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#32771;&#34385;&#38543;&#26426;&#34892;&#20026;&#30340;&#21487;&#33021;&#32467;&#26524;&#65292;&#24182;&#26681;&#25454;&#30495;&#23454;&#30340;&#19990;&#30028;&#29366;&#24577;&#30340;&#37096;&#20998;&#35266;&#23519;&#36827;&#34892;&#20915;&#31574;&#12290;&#22240;&#26524;&#28151;&#28102;&#30340;&#38382;&#39064;&#26159;&#36827;&#34892;&#20934;&#30830;&#21644;&#24378;&#20581;&#30340;&#34892;&#20026;&#39044;&#27979;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#36825;&#20123;&#38543;&#26426;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26126;&#30830;&#30340;&#22240;&#26524;&#35821;&#20041;&#65292;POMDP&#35268;&#21010;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#28151;&#28102;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#34920;&#29616;&#19981;&#20339;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;CAR-DESPOT&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;POMDP&#35268;&#21010;&#31243;&#24207;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots operating in real-world environments must reason about possible outcomes of stochastic actions and make decisions based on partial observations of the true world state. A major challenge for making accurate and robust action predictions is the problem of confounding, which if left untreated can lead to prediction errors. The partially observable Markov decision process (POMDP) is a widely-used framework to model these stochastic and partially-observable decision-making problems. However, due to a lack of explicit causal semantics, POMDP planning methods are prone to confounding bias and thus in the presence of unobserved confounders may produce underperforming policies. This paper presents a novel causally-informed extension of "anytime regularized determinized sparse partially observable tree" (AR-DESPOT), a modern anytime online POMDP planner, using causal modelling and inference to eliminate errors caused by unmeasured confounder variables. We further propose a method to lear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#19978;&#24212;&#29992;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#65292;&#26469;&#33258;&#19981;&#21516;&#21098;&#26525;&#26041;&#27861;&#30340;&#26550;&#26500;&#22312;&#20219;&#21153;&#24615;&#33021;&#19978;&#27809;&#26377;&#26174;&#30528;&#24046;&#24322;&#65292;&#36845;&#20195;&#32467;&#26500;&#21098;&#26525;&#21487;&#33021;&#19981;&#26159;&#23454;&#29616;&#26368;&#20248;&#32467;&#26500;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.06840</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Structured Pruning for Multi-Task Deep Neural Networks. (arXiv:2304.06840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#19978;&#24212;&#29992;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#65292;&#26469;&#33258;&#19981;&#21516;&#21098;&#26525;&#26041;&#27861;&#30340;&#26550;&#26500;&#22312;&#20219;&#21153;&#24615;&#33021;&#19978;&#27809;&#26377;&#26174;&#30528;&#24046;&#24322;&#65292;&#36845;&#20195;&#32467;&#26500;&#21098;&#26525;&#21487;&#33021;&#19981;&#26159;&#23454;&#29616;&#26368;&#20248;&#32467;&#26500;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#30456;&#23545;&#20110;&#21333;&#20010;&#21333;&#20219;&#21153;DNN&#27169;&#22411;&#65292;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#35745;&#31639;&#21644;&#23384;&#20648;&#20248;&#21183;&#65292;&#20294;&#26159;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#21387;&#32553;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;&#35768;&#22810;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#23454;&#29616;&#21333;&#20219;&#21153;&#27169;&#22411;&#30340;&#21152;&#36895;&#65292;&#20294;&#26159;&#23545;&#20110;&#22810;&#20219;&#21153;&#32593;&#32476;&#30340;&#21098;&#26525;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#21333;&#20219;&#21153;&#28388;&#27874;&#22120;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;MTL&#30340;&#28388;&#27874;&#22120;&#21098;&#26525;&#20934;&#21017;&#26469;&#20272;&#35745;&#28388;&#27874;&#22120;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#36845;&#20195;&#21098;&#26525;&#31574;&#30053;&#20351;&#29992;&#20004;&#31181;&#21098;&#26525;&#26041;&#27861;&#26469;&#21098;&#26525;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#20180;&#32454;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#19979;&#65292;&#24403;&#21442;&#25968;&#25968;&#37327;&#30456;&#20284;&#26102;&#65292;&#26469;&#33258;&#19981;&#21516;&#21098;&#26525;&#26041;&#27861;&#30340;&#26550;&#26500;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#24615;&#33021;&#19978;&#27809;&#26377;&#26174;&#30528;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36845;&#20195;&#32467;&#26500;&#21098;&#26525;&#21487;&#33021;&#19981;&#26159;&#23454;&#29616;&#22810;&#20219;&#21153;&#32593;&#32476;&#26368;&#20248;&#32467;&#26500;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although multi-task deep neural network (DNN) models have computation and storage benefits over individual single-task DNN models, they can be further optimized via model compression. Numerous structured pruning methods are already developed that can readily achieve speedups in single-task models, but the pruning of multi-task networks has not yet been extensively studied. In this work, we investigate the effectiveness of structured pruning on multi-task models. We use an existing single-task filter pruning criterion and also introduce an MTL-based filter pruning criterion for estimating the filter importance scores. We prune the model using an iterative pruning strategy with both pruning methods. We show that, with careful hyper-parameter tuning, architectures obtained from different pruning methods do not have significant differences in their performances across tasks when the number of parameters is similar. We also show that iterative structure pruning may not be the best way to ac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25972;&#21512;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;&#25209;&#37327;&#36716;&#24405;&#32452;&#23398;&#39044;&#27979;&#24739;&#32773;&#29983;&#23384;&#29575;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#26631;&#35760;&#21270;&#36716;&#24405;&#32452;&#23398;&#21644;&#25429;&#33719;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06819</link><description>&lt;p&gt;
&#24314;&#27169;&#29983;&#29289;&#36890;&#36335;&#21644;&#32452;&#32455;&#23398;&#20043;&#38388;&#30340;&#31264;&#23494;&#22810;&#27169;&#24577;&#20132;&#20114;&#20197;&#39044;&#27979;&#23384;&#27963;&#29575;
&lt;/p&gt;
&lt;p&gt;
Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction. (arXiv:2304.06819v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25972;&#21512;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;&#25209;&#37327;&#36716;&#24405;&#32452;&#23398;&#39044;&#27979;&#24739;&#32773;&#29983;&#23384;&#29575;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#26631;&#35760;&#21270;&#36716;&#24405;&#32452;&#23398;&#21644;&#25429;&#33719;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#25209;&#37327;&#36716;&#24405;&#32452;&#23398;&#65288;bulk transcriptomics&#65289;&#20197;&#39044;&#27979;&#24739;&#32773;&#29983;&#23384;&#29575;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#24739;&#32773;&#39044;&#21518;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#19981;&#21516;&#24615;&#36136;&#65292;&#36825;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;WSIs&#20195;&#34920;&#32959;&#30244;&#30340;&#39640;&#32500;&#31354;&#38388;&#25551;&#36848;&#65292;&#32780;&#25209;&#37327;&#36716;&#24405;&#32452;&#23398;&#21017;&#20195;&#34920;&#35813;&#32959;&#30244;&#20869;&#30340;&#22522;&#22240;&#34920;&#36798;&#27700;&#24179;&#30340;&#20840;&#23616;&#25551;&#36848;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22914;&#20309;&#20197;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#23545;&#36716;&#24405;&#32452;&#23398;&#36827;&#34892;&#26631;&#35760;&#21270;&#65311;&#65288;2&#65289;&#22914;&#20309;&#25429;&#25417;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#23494;&#38598;&#22810;&#27169;&#24577;&#20132;&#20114;&#65311;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20174;&#36716;&#24405;&#32452;&#23398;&#20013;&#23398;&#20064;&#29983;&#29289;&#36890;&#36335;&#26631;&#35760;&#65292;&#20197;&#32534;&#30721;&#29305;&#23450;&#30340;&#32454;&#32990;&#21151;&#33021;&#12290;&#32467;&#21512;&#32534;&#30721;WSI&#20013;&#19981;&#21516;&#24418;&#24577;&#27169;&#24335;&#30340;&#32452;&#32455;&#23398;&#22359;&#26631;&#35760;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#26500;&#25104;&#20102;&#19979;&#28216;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#30340;&#36866;&#24403;&#25512;&#29702;&#21333;&#20803;&#12290;&#25105;&#20204;&#25552;&#20986;&#34701;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Integrating whole-slide images (WSIs) and bulk transcriptomics for predicting patient survival can improve our understanding of patient prognosis. However, this multimodal task is particularly challenging due to the different nature of these data: WSIs represent a very high-dimensional spatial description of a tumor, while bulk transcriptomics represent a global description of gene expression levels within that tumor. In this context, our work aims to address two key challenges: (1) how can we tokenize transcriptomics in a semantically meaningful and interpretable way?, and (2) how can we capture dense multimodal interactions between these two modalities? Specifically, we propose to learn biological pathway tokens from transcriptomics that can encode specific cellular functions. Together with histology patch tokens that encode the different morphological patterns in the WSI, we argue that they form appropriate reasoning units for downstream interpretability analyses. We propose fusing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26816;&#27979;&#27169;&#22411;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#27979;&#35797;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#22806;&#37096;&#20998;&#24067;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.06813</link><description>&lt;p&gt;
&#27169;&#22411;&#29305;&#23450;&#35270;&#35282;&#19979;&#30340;&#32479;&#19968;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Out-Of-Distribution Detection: A Model-Specific Perspective. (arXiv:2304.06813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26816;&#27979;&#27169;&#22411;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#27979;&#35797;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#22806;&#37096;&#20998;&#24067;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#19981;&#23646;&#20110;&#35757;&#32451;&#20998;&#24067;&#24182;&#19981;&#21487;&#38752;&#39044;&#27979;&#30340;&#27979;&#35797;&#26679;&#20363;&#12290;&#34429;&#28982;&#24050;&#26377;&#22823;&#37327;&#30456;&#20851;&#24037;&#20316;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#21482;&#20851;&#27880;&#26469;&#33258;&#35821;&#20041;&#36716;&#25442;&#65288;&#22914;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#65289;&#30340;OOD&#20363;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21487;&#33021;&#30340;&#21407;&#22240;&#65288;&#22914;&#21327;&#21464;&#37327;&#36716;&#25442;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#30740;&#31350;OOD&#26816;&#27979;&#12290;&#25105;&#20204;&#24314;&#35758;&#19981;&#26159;&#26816;&#27979;&#29305;&#23450;&#21407;&#22240;&#23548;&#33268;&#30340;OOD&#20363;&#23376;&#65292;&#32780;&#26159;&#26816;&#27979;&#24050;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#22120;&#65289;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#20363;&#23376;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26159;&#21542;&#24212;&#35813;&#26816;&#27979;&#21644;&#25298;&#32477;&#27979;&#35797;&#20363;&#23376;&#26159;&#8220;&#27169;&#22411;&#29305;&#23450;&#8221;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#32479;&#19968;&#20102;&#30001;&#35821;&#20041;&#21464;&#21270;&#21644;&#21327;&#21464;&#37327;&#21464;&#21270;&#24341;&#36215;&#30340;OOD&#20363;&#23376;&#30340;&#26816;&#27979;&#65292;&#24182;&#23494;&#20999;&#20851;&#27880;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection aims to identify test examples that do not belong to the training distribution and are thus unlikely to be predicted reliably. Despite a plethora of existing works, most of them focused only on the scenario where OOD examples come from semantic shift (e.g., unseen categories), ignoring other possible causes (e.g., covariate shift). In this paper, we present a novel, unifying framework to study OOD detection in a broader scope. Instead of detecting OOD examples from a particular cause, we propose to detect examples that a deployed machine learning model (e.g., an image classifier) is unable to predict correctly. That is, whether a test example should be detected and rejected or not is ``model-specific''. We show that this framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and closely addresses the concern of applying a machine learning model to uncontrolled environments. We provide an extensive analysis that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35774;&#35745;&#38750;&#32447;&#24615;&#20809;&#23376;&#26230;&#20307;&#21644;&#27893;&#28006;&#20809;&#26463;&#29983;&#25104;&#39640;&#32500;&#37327;&#23376;&#24577;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#29289;&#29702;&#32422;&#26463;&#21644;&#21487;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#28436;&#31034;&#20102;&#22914;&#20309;&#29983;&#25104;&#26368;&#22823;&#32416;&#32544;&#24577;&#12290;&#36825;&#20026;&#25511;&#21046;&#20219;&#24847;&#37327;&#23376;&#24577;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#65292;&#24182;&#22312;&#20840;&#20809;&#23398;&#30456;&#24178;&#25511;&#21046;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06810</link><description>&lt;p&gt;
&#39640;&#32500;&#37327;&#23376;&#24577;&#24037;&#31243;&#30340;&#38750;&#32447;&#24615;&#20809;&#23376;&#26230;&#20307;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Designing Nonlinear Photonic Crystals for High-Dimensional Quantum State Engineering. (arXiv:2304.06810v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35774;&#35745;&#38750;&#32447;&#24615;&#20809;&#23376;&#26230;&#20307;&#21644;&#27893;&#28006;&#20809;&#26463;&#29983;&#25104;&#39640;&#32500;&#37327;&#23376;&#24577;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#29289;&#29702;&#32422;&#26463;&#21644;&#21487;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#28436;&#31034;&#20102;&#22914;&#20309;&#29983;&#25104;&#26368;&#22823;&#32416;&#32544;&#24577;&#12290;&#36825;&#20026;&#25511;&#21046;&#20219;&#24847;&#37327;&#23376;&#24577;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#65292;&#24182;&#22312;&#20840;&#20809;&#23398;&#30456;&#24178;&#25511;&#21046;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#21463;&#29289;&#29702;&#32422;&#26463;&#21644;&#21487;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#23376;&#20809;&#23398;&#20013;&#30340;&#33258;&#21457;&#21442;&#37327;&#19979;&#36716;&#25442; (SPDC)&#65292;&#29983;&#25104; D &#32500; qudit &#29366;&#24577;&#12290;&#25105;&#20204;&#35268;&#36991;&#20102;&#29289;&#29702;&#36807;&#31243;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#25152;&#24102;&#26469;&#30340;&#20219;&#20309;&#38480;&#21046;&#65292;&#24182;&#24182;&#20837;&#20102;&#19968;&#32452;&#38543;&#26426;&#21160;&#21147;&#23398;&#26041;&#31243;&#65292;&#25511;&#21046;&#20854;&#22312; SPDC &#21704;&#23494;&#39039;&#19979;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#32467;&#26500;&#21270;&#30340;&#38750;&#32447;&#24615;&#20809;&#23376;&#26230;&#20307; (NLPCs) &#21644;&#24418;&#29366;&#21270;&#30340;&#27893;&#28006;&#20809;&#26463;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65307;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#31354;&#38388;&#33258;&#30001;&#24230;&#20013;&#29983;&#25104;&#26368;&#22823;&#32416;&#32544;&#24577;&#12290;&#23398;&#20064; NLPC &#32467;&#26500;&#20026;&#22609;&#36896;&#21644;&#25511;&#21046;&#20219;&#24847;&#37327;&#23376;&#24577;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#65292;&#24182;&#19988;&#20351;&#24471;&#29983;&#25104;&#24577;&#30340;&#20840;&#20809;&#23398;&#30456;&#24178;&#25511;&#21046;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#20174;&#24222;&#22823;&#30340;&#26230;&#20307;&#25193;&#23637;&#21040;&#34180;&#22411;&#20803;&#34920;&#38754;&#65292;&#24182;&#19988;&#21487;&#33021;&#24212;&#29992;&#20110;&#20854;&#20182;&#20849;&#20139;&#31867;&#20284;&#21704;&#23494;&#39039;&#37327;&#30340;&#37327;&#23376;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel, physically-constrained and differentiable approach for the generation of D-dimensional qudit states via spontaneous parametric down-conversion (SPDC) in quantum optics. We circumvent any limitations imposed by the inherently stochastic nature of the physical process and incorporate a set of stochastic dynamical equations governing its evolution under the SPDC Hamiltonian. We demonstrate the effectiveness of our model through the design of structured nonlinear photonic crystals (NLPCs) and shaped pump beams; and show, theoretically and experimentally, how to generate maximally entangled states in the spatial degree of freedom. The learning of NLPC structures offers a promising new avenue for shaping and controlling arbitrary quantum states and enables all-optical coherent control of the generated states. We believe that this approach can readily be extended from bulky crystals to thin Metasurfaces and potentially applied to other quantum systems sharing a similar Ham
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22320;&#29702;&#31354;&#38388;AI&#20013;&#24320;&#21457;&#22522;&#30784;&#27169;&#22411;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#27979;&#35797;&#20102;&#22810;&#31181;FMs&#22312;&#22320;&#29702;&#23376;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#20294;&#22312;&#21457;&#23637;&#20013;&#20063;&#38754;&#20020;&#30528;&#32570;&#23569;&#25968;&#25454;&#38598;&#21644;&#38656;&#35201;&#19987;&#19994;&#25216;&#26415;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.06798</link><description>&lt;p&gt;
&#35770;&#22522;&#30784;&#27169;&#22411;&#22312;&#22320;&#29702;&#31354;&#38388;AI&#20013;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence. (arXiv:2304.06798v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22320;&#29702;&#31354;&#38388;AI&#20013;&#24320;&#21457;&#22522;&#30784;&#27169;&#22411;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#27979;&#35797;&#20102;&#22810;&#31181;FMs&#22312;&#22320;&#29702;&#23376;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#20294;&#22312;&#21457;&#23637;&#20013;&#20063;&#38754;&#20020;&#30528;&#32570;&#23569;&#25968;&#25454;&#38598;&#21644;&#38656;&#35201;&#19987;&#19994;&#25216;&#26415;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#26159;&#25351;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#12289;&#23569;&#26679;&#26412;&#29978;&#33267;&#38646;&#26679;&#26412;&#23398;&#20064;&#36866;&#29992;&#20110;&#24191;&#27867;&#19979;&#28216;&#20219;&#21153;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22823;&#33719;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#23578;&#26410;&#35265;&#21040;&#20026;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#65288;GeoAI&#65289;&#24320;&#21457;&#22522;&#30784;&#27169;&#22411;&#30340;&#23581;&#35797;&#12290;&#26412;&#25991;&#25506;&#35752;&#24320;&#21457;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#20197;&#24212;&#23545;GeoAI&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22312;&#22810;&#20010;&#22320;&#29702;&#31354;&#38388;&#23376;&#22495;&#20013;&#36827;&#34892;&#19971;&#39033;&#20219;&#21153;&#30340;&#27979;&#35797;&#65292;&#21253;&#25324;&#22320;&#29702;&#35821;&#20041;&#12289;&#20581;&#24247;&#22320;&#29702;&#23398;&#12289;&#22478;&#24066;&#22320;&#29702;&#23398;&#21644;&#36965;&#24863;&#31561;&#65292;&#30740;&#31350;&#20102;&#29616;&#26377;&#35768;&#22810;FMs&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20165;&#28041;&#21450;&#25991;&#26412;&#27169;&#24577;&#30340;&#19968;&#20123;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#65288;&#20363;&#22914;&#22320;&#21517;&#35782;&#21035;&#12289;&#20301;&#32622;&#25551;&#36848;&#35782;&#21035;&#20197;&#21450;&#32654;&#22269;&#24030;&#32423;/&#21439;&#32423;&#30196;&#21574;&#30151;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65289;&#20013;&#65292;&#36825;&#20123;&#20219;&#21153;&#26080;&#20851;&#30340;LLM&#20063;&#21487;&#20197;&#32988;&#20219;&#20219;&#21153;&#29305;&#23450;&#30340;&#23436;&#20840;&#23450;&#21046;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#20026;&#22320;&#29702;&#31354;&#38388;AI&#24320;&#21457;FMs&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#20047;&#22823;&#35268;&#27169;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#38598;&#21644;&#38656;&#35201;&#19987;&#38376;&#30340;&#22320;&#29702;&#31354;&#38388;&#24494;&#35843;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22810;&#27169;&#24577;FMs&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#65292;&#20197;&#24800;&#21450;&#22320;&#29702;&#31354;&#38388;AI&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges of developing multimodal foundation models for GeoAI. We first investigate the potential of many existing FMs by testing their performances on seven tasks across multiple geospatial subdomains including Geospatial Semantics, Health Geography, Urban Geography, and Remote Sensing. Our results indicate that on several geospatial tasks that only involve text modality such as toponym recognition, location description recognition, and US state-level/county-level dementia time series forecasting, these task-agnostic LLMs can outperform task-specific fully-s
&lt;/p&gt;</description></item><item><title>ChatGPT&#20351;&#29992;&#35895;&#27468;&#23398;&#26415;&#30340;&#24341;&#29992;&#35745;&#25968;&#26469;&#24341;&#36848;&#29615;&#22659;&#31185;&#23398;&#20013;&#30340;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#25991;&#31456;&#21644;&#26399;&#21002;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#25918;&#22823;&#39532;&#22826;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.06794</link><description>&lt;p&gt;
ChatGPT&#20381;&#38752;&#35895;&#27468;&#23398;&#26415;&#30340;&#24341;&#29992;&#35745;&#25968;&#26469;&#24341;&#36848;&#26368;&#32463;&#20856;&#30340;&#25991;&#31456;&#21644;&#26399;&#21002;&#65292;&#23548;&#33268;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#20250;&#25918;&#22823;&#29615;&#22659;&#31185;&#23398;&#20013;&#30340;&#39532;&#22826;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
ChatGPT cites the most-cited articles and journals, relying solely on Google Scholar's citation counts. As a result, AI may amplify the Matthew Effect in environmental science. (arXiv:2304.06794v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06794
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20351;&#29992;&#35895;&#27468;&#23398;&#26415;&#30340;&#24341;&#29992;&#35745;&#25968;&#26469;&#24341;&#36848;&#29615;&#22659;&#31185;&#23398;&#20013;&#30340;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#25991;&#31456;&#21644;&#26399;&#21002;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#25918;&#22823;&#39532;&#22826;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#65288;GPT&#65289;&#24050;&#25104;&#20026;&#36817;&#24180;&#26469;&#26368;&#21463;&#30633;&#30446;&#30340;&#21019;&#26032;&#20043;&#19968;&#65292;&#20840;&#29699;&#25317;&#26377;&#36229;&#36807;1&#20159;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;GPT&#30340;&#20449;&#24687;&#26469;&#28304;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#29615;&#22659;&#31185;&#23398;&#39046;&#22495;&#20869;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35201;&#27714;GPT&#35782;&#21035;&#29615;&#22659;&#31185;&#23398;&#39046;&#22495;&#20869;&#26368;&#37325;&#35201;&#30340;10&#20010;&#23376;&#23398;&#31185;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35201;&#27714;&#20854;&#25776;&#20889;&#27599;&#20010;&#23376;&#23398;&#31185;&#30340;&#31185;&#23398;&#32508;&#36848;&#25991;&#31456;&#65292;&#27599;&#31687;&#25991;&#31456;&#21253;&#25324;25&#20010;&#21442;&#32771;&#25991;&#29486;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24341;&#29992;&#27425;&#25968;&#12289;&#20986;&#29256;&#26085;&#26399;&#21644;&#25991;&#31456;&#25152;&#21457;&#34920;&#30340;&#26399;&#21002;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;GPT&#20542;&#21521;&#20110;&#24341;&#29992;&#29615;&#22659;&#31185;&#23398;&#20013;&#24341;&#29992;&#27425;&#25968;&#36739;&#39640;&#30340;&#20986;&#29256;&#29289;&#65292;&#20854;&#20013;&#24341;&#29992;&#27425;&#25968;&#30340;&#20013;&#20301;&#25968;&#20026;1184.5&#12290;&#23427;&#36824;&#23545;&#36739;&#26087;&#30340;&#20986;&#29256;&#29289;&#34920;&#29616;&#20986;&#20559;&#22909;&#65292;&#20986;&#29256;&#24180;&#20221;&#20013;&#20301;&#25968;&#20026;2010&#24180;&#65292;&#24182;&#19988;&#20027;&#35201;&#21442;&#32771;&#22791;&#21463;&#23562;&#37325;&#30340;&#26399;&#21002;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT (GPT) has become one of the most talked-about innovations in recent years, with over 100 million users worldwide. However, there is still limited knowledge about the sources of information GPT utilizes. As a result, we carried out a study focusing on the sources of information within the field of environmental science. In our study, we asked GPT to identify the ten most significant subdisciplines within the field of environmental science. We then asked it to compose a scientific review article on each subdiscipline, including 25 references. We proceeded to analyze these references, focusing on factors such as the number of citations, publication date, and the journal in which the work was published. Our findings indicate that GPT tends to cite highly-cited publications in environmental science, with a median citation count of 1184.5. It also exhibits a preference for older publications, with a median publication year of 2010, and predominantly refers to well-respected journals 
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#21551;&#21457;&#26426;&#21046;&#30340;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#21253;&#25324;&#31232;&#30095;&#19981;&#37325;&#21472;&#34920;&#31034;&#12289;&#36203;&#24067;&#23398;&#20064;&#12289;&#31361;&#35302;&#24041;&#22266;&#21644;&#37325;&#25773;&#31561;&#26426;&#21046;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#24207;&#21015;&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06738</link><description>&lt;p&gt;
&#19968;&#39033;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#65306;&#33041;&#21551;&#21457;&#26426;&#21046;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#21644;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Study of Biologically Plausible Neural Network: The Role and Interactions of Brain-Inspired Mechanisms in Continual Learning. (arXiv:2304.06738v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#21551;&#21457;&#26426;&#21046;&#30340;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#21253;&#25324;&#31232;&#30095;&#19981;&#37325;&#21472;&#34920;&#31034;&#12289;&#36203;&#24067;&#23398;&#20064;&#12289;&#31361;&#35302;&#24041;&#22266;&#21644;&#37325;&#25773;&#31561;&#26426;&#21046;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#24207;&#21015;&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25797;&#38271;&#19981;&#26029;&#20174;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#33719;&#21462;&#12289;&#24041;&#22266;&#21644;&#20445;&#30041;&#20449;&#24687;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21017;&#34920;&#29616;&#20986;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#21644;&#23427;&#20204;&#30340;&#20154;&#24037;&#23545;&#24212;&#29289;&#22312;&#31361;&#35302;&#22797;&#26434;&#24615;&#12289;&#20449;&#24687;&#22788;&#29702;&#21644;&#23398;&#20064;&#26426;&#21046;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#24615;&#33021;&#19978;&#30340;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#29983;&#29289;&#21487;&#34892;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#23436;&#20840;&#20852;&#22859;&#21644;&#25233;&#21046;&#31070;&#32463;&#20803;&#30340;&#21333;&#29420;&#31181;&#32676;&#65292;&#36981;&#24490;&#25140;&#23572;&#21407;&#21017;&#65292;&#24182;&#20026;&#20852;&#22859;&#30340;&#38181;&#20307;&#31070;&#32463;&#20803;&#22686;&#21152;&#20102;&#31867;&#20284;&#26641;&#31361;&#30340;&#32467;&#26500;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21050;&#28608;&#22788;&#29702;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#21463;&#33041;&#21551;&#21457;&#30340;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#31232;&#30095;&#19981;&#37325;&#21472;&#34920;&#31034;&#12289;&#36203;&#24067;&#23398;&#20064;&#12289;&#31361;&#35302;&#24041;&#22266;&#21644;&#37325;&#25773;&#20276;&#38543;&#23398;&#20064;&#20107;&#20214;&#30340;&#36807;&#21435;&#28608;&#27963;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22522;&#20110;&#24207;&#21015;&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#32771;&#34385;&#36825;&#20123;&#26426;&#21046;&#22312;&#29983;&#29289;&#21487;&#34892;&#30340;&#26694;&#26550;&#20013;&#30456;&#20114;&#20316;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans excel at continually acquiring, consolidating, and retaining information from an ever-changing environment, whereas artificial neural networks (ANNs) exhibit catastrophic forgetting. There are considerable differences in the complexity of synapses, the processing of information, and the learning mechanisms in biological neural networks and their artificial counterparts, which may explain the mismatch in performance. We consider a biologically plausible framework that constitutes separate populations of exclusively excitatory and inhibitory neurons that adhere to Dale's principle, and the excitatory pyramidal neurons are augmented with dendritic-like structures for context-dependent processing of stimuli. We then conduct a comprehensive study on the role and interactions of different mechanisms inspired by the brain, including sparse non-overlapping representations, Hebbian learning, synaptic consolidation, and replay of past activations that accompanied the learning event. Our s
&lt;/p&gt;</description></item><item><title>&#20803;&#23398;&#20064;&#27169;&#22411;&#26159;&#26500;&#24314;&#20154;&#31867;&#35748;&#30693;&#27169;&#22411;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#36125;&#21494;&#26031;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.06729</link><description>&lt;p&gt;
&#35748;&#30693;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Meta-Learned Models of Cognition. (arXiv:2304.06729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06729
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#27169;&#22411;&#26159;&#26500;&#24314;&#20154;&#31867;&#35748;&#30693;&#27169;&#22411;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#36125;&#21494;&#26031;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#21453;&#22797;&#20132;&#20114;&#32780;&#23398;&#20064;&#23398;&#20064;&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#32780;&#19981;&#26159;&#25163;&#21160;&#35774;&#35745;&#23427;&#20204;&#12290;&#36817;&#24180;&#26469;&#65292;&#36825;&#20010;&#26694;&#26550;&#24050;&#32463;&#25104;&#20026;&#26500;&#24314;&#20154;&#31867;&#35748;&#30693;&#27169;&#22411;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36824;&#32570;&#20047;&#19968;&#20010;&#20851;&#20110;&#20803;&#23398;&#20064;&#35748;&#30693;&#27169;&#22411;&#30340;&#36830;&#36143;&#30340;&#30740;&#31350;&#35745;&#21010;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#32508;&#21512;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#24314;&#31435;&#36825;&#26679;&#30340;&#30740;&#31350;&#35745;&#21010;&#12290;&#25105;&#20204;&#20381;&#38752;&#19977;&#20010;&#20851;&#38190;&#25903;&#26609;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25351;&#20986;&#20803;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#36125;&#21494;&#26031;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20010;&#32467;&#26524;&#19981;&#20165;&#24847;&#21619;&#30528;&#20219;&#20309;&#21487;&#20197;&#36890;&#36807;&#36125;&#21494;&#26031;&#27169;&#22411;&#35299;&#37322;&#30340;&#34892;&#20026;&#29616;&#35937;&#20063;&#21487;&#20197;&#36890;&#36807;&#20803;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#65292;&#32780;&#19988;&#36824;&#20801;&#35768;&#25105;&#20204;&#19982;&#35748;&#30693;&#30340;&#29702;&#24615;&#20998;&#26512;&#24314;&#31435;&#24378;&#36830;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20803;&#23398;&#20064;&#26694;&#26550;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#20960;&#20010;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35748;&#20026;&#20803;&#23398;&#20064;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Meta-learning is a framework for learning learning algorithms through repeated interactions with an environment as opposed to designing them by hand. In recent years, this framework has established itself as a promising tool for building models of human cognition. Yet, a coherent research program around meta-learned models of cognition is still missing. The purpose of this article is to synthesize previous work in this field and establish such a research program. We rely on three key pillars to accomplish this goal. We first point out that meta-learning can be used to construct Bayes-optimal learning algorithms. This result not only implies that any behavioral phenomenon that can be explained by a Bayesian model can also be explained by a meta-learned model but also allows us to draw strong connections to the rational analysis of cognition. We then discuss several advantages of the meta-learning framework over traditional Bayesian methods. In particular, we argue that meta-learning can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CyberHD&#30340;&#36229;&#32500;&#35745;&#31639;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20197;&#26174;&#33879;&#26356;&#20302;&#30340;&#32500;&#24230;&#25429;&#25417;&#32593;&#32476;&#23041;&#32961;&#30340;&#22797;&#26434;&#27169;&#24335;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#30828;&#20214;&#38169;&#35823;&#23481;&#38169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06728</link><description>&lt;p&gt;
&#26368;&#26032;&#36827;&#23637;&#65306;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#30340;&#36229;&#32500;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Late Breaking Results: Scalable and Efficient Hyperdimensional Computing for Network Intrusion Detection. (arXiv:2304.06728v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CyberHD&#30340;&#36229;&#32500;&#35745;&#31639;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20197;&#26174;&#33879;&#26356;&#20302;&#30340;&#32500;&#24230;&#25429;&#25417;&#32593;&#32476;&#23041;&#32961;&#30340;&#22797;&#26434;&#27169;&#24335;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#30828;&#20214;&#38169;&#35823;&#23481;&#38169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23433;&#20840;&#24050;&#32463;&#25104;&#20026;&#24037;&#19994;&#30028;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#30001;&#20110;&#23433;&#20840;&#39046;&#22495;&#30340;&#22797;&#26434;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#24448;&#24448;&#26080;&#27861;&#21450;&#26102;&#26816;&#27979;&#21040;&#32593;&#32476;&#23041;&#32961;&#12290;&#21463;&#33041;&#21551;&#21457;&#30340;&#36229;&#32500;&#35745;&#31639;&#24050;&#34987;&#24341;&#20837;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#24456;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#36229;&#32500;&#35745;&#31639;&#26041;&#27861;&#20351;&#29992;&#38745;&#24577;&#32534;&#30721;&#22120;&#65292;&#38656;&#35201;&#26497;&#39640;&#30340;&#32500;&#24230;&#21644;&#25968;&#30334;&#27425;&#35757;&#32451;&#36845;&#20195;&#25165;&#33021;&#36798;&#21040;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#23398;&#20064;&#25928;&#29575;&#25439;&#22833;&#65292;&#20197;&#21450;&#26816;&#27979;&#25915;&#20987;&#26102;&#20986;&#29616;&#24040;&#22823;&#30340;&#24310;&#36831;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36229;&#32500;&#35745;&#31639;&#23398;&#20064;&#26694;&#26550;CyberHD&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20877;&#29983;&#19981;&#37325;&#35201;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#20197;&#26174;&#33879;&#26356;&#20302;&#30340;&#32500;&#24230;&#25429;&#25417;&#32593;&#32476;&#23041;&#32961;&#30340;&#22797;&#26434;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20840;&#24687;&#20998;&#24067;&#20026;CyberHD&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#30828;&#20214;&#38169;&#35823;&#23481;&#38169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybersecurity has emerged as a critical challenge for the industry. With the large complexity of the security landscape, sophisticated and costly deep learning models often fail to provide timely detection of cyber threats on edge devices. Brain-inspired hyperdimensional computing (HDC) has been introduced as a promising solution to address this issue. However, existing HDC approaches use static encoders and require very high dimensionality and hundreds of training iterations to achieve reasonable accuracy. This results in a serious loss of learning efficiency and causes huge latency for detecting attacks. In this paper, we propose CyberHD, an innovative HDC learning framework that identifies and regenerates insignificant dimensions to capture complicated patterns of cyber threats with remarkably lower dimensionality. Additionally, the holographic distribution of patterns in high dimensional space provides CyberHD with notably high robustness against hardware errors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06671</link><description>&lt;p&gt;
&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;&#21644;&#36845;&#20195;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25511;&#21046;&#26159;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#22312;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20855;&#26377;&#31867;&#20284;&#31354;&#38388;&#37197;&#32622;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#24847;&#19981;&#30830;&#23450;&#30340;&#24067;&#23616;&#30340;&#31163;&#32447;&#20998;&#24067;&#26679;&#26412;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#29616;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayoutBench&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35786;&#26029;&#30340;&#22522;&#20934;&#65292;&#23427;&#26816;&#26597;&#20102;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#65306;&#25968;&#37327;&#65292;&#20301;&#32622;&#65292;&#22823;&#23567;&#21644;&#24418;&#29366;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#26368;&#36817;&#20195;&#34920;&#24615;&#30340;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#33391;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#19978;&#30340;&#23545;&#35937;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#65292;&#23427;&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#23637;&#31034;&#20986;&#22312;LayoutBench&#30340;OOD&#24067;&#23616;&#19978;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#34920;&#26126;IterInpaint&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#22810;&#26679;&#21644;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#22270;&#20687;&#21644;&#21487;&#25511;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35745;&#31639;&#26426;&#24037;&#31243;&#20837;&#38376;&#35838;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23427;&#21487;&#20197;&#22238;&#31572;&#26222;&#36890;&#27010;&#24565;&#30340;&#38382;&#39064;&#65292;&#20294;&#26080;&#27861;&#22788;&#29702;&#24102;&#26377;&#22270;&#34920;&#25110;&#22270;&#24418;&#30340;&#38382;&#39064;&#65292;&#20063;&#26080;&#27861;&#36827;&#34892;&#23454;&#39564;&#23460;&#30340;&#29616;&#22330;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.06122</link><description>&lt;p&gt;
&#20998;&#26512;ChatGPT&#22312;&#35745;&#31639;&#26426;&#24037;&#31243;&#20837;&#38376;&#35838;&#31243;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Analyzing ChatGPT's Aptitude in an Introductory Computer Engineering Course. (arXiv:2304.06122v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35745;&#31639;&#26426;&#24037;&#31243;&#20837;&#38376;&#35838;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23427;&#21487;&#20197;&#22238;&#31572;&#26222;&#36890;&#27010;&#24565;&#30340;&#38382;&#39064;&#65292;&#20294;&#26080;&#27861;&#22788;&#29702;&#24102;&#26377;&#22270;&#34920;&#25110;&#22270;&#24418;&#30340;&#38382;&#39064;&#65292;&#20063;&#26080;&#27861;&#36827;&#34892;&#23454;&#39564;&#23460;&#30340;&#29616;&#22330;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26368;&#36817;&#21463;&#21040;&#20102;&#20844;&#20247;&#21644;&#23398;&#26415;&#30028;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#19988;&#21548;&#36215;&#26469;&#20687;&#20154;&#31867;&#22238;&#31572;&#30340;&#21508;&#31181;&#38382;&#39064;&#30340;&#25991;&#26412;&#31572;&#26696;&#12290; ChatGPT&#22312;&#23398;&#26415;&#25110;&#35838;&#22530;&#29615;&#22659;&#20013;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#29978;&#33267;&#29983;&#25104;&#25972;&#31687;&#35770;&#25991;&#30340;&#28508;&#22312;&#29992;&#36884;&#25110;&#28389;&#29992;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#20154;&#25991;&#23398;&#31185;&#12289;&#21830;&#23398;&#38498;&#25110;&#21307;&#23398;&#38498;&#30340;&#24212;&#29992;&#65292;&#20294;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35745;&#31639;&#26426;&#24037;&#31243;&#20837;&#38376;&#35838;&#31243;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20837;&#38376;&#32423;&#35745;&#31639;&#26426;&#24037;&#31243;&#35838;&#31243;&#20013;&#22238;&#31572;&#27979;&#39564;&#12289;&#20316;&#19994;&#12289;&#32771;&#35797;&#21644;&#23454;&#39564;&#23460;&#38382;&#39064;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT&#22312;&#35810;&#38382;&#26222;&#36890;&#27010;&#24565;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#26174;&#28982;&#65292;&#20316;&#20026;&#19968;&#20010;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#24037;&#20855;&#65292;&#23427;&#26080;&#27861;&#22788;&#29702;&#20855;&#26377;&#22270;&#34920;&#25110;&#22270;&#24418;&#30340;&#38382;&#39064;&#65292;&#20063;&#26080;&#27861;&#29983;&#25104;&#22270;&#34920;&#21644;&#22270;&#24418;&#12290;&#21516;&#26102;&#65292;&#36825;&#20010;&#24037;&#20855;&#20063;&#26080;&#27861;&#36827;&#34892;&#23454;&#39564;&#23460;&#30340;&#29616;&#22330;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has recently gathered attention from the general public and academia as a tool that is able to generate plausible and human-sounding text answers to various questions. One potential use, or abuse, of ChatGPT is in answering various questions or even generating whole essays and research papers in an academic or classroom setting. While recent works have explored the use of ChatGPT in the context of humanities, business school, or medical school, this work explores how ChatGPT performs in the context of an introductory computer engineering course. This work assesses ChatGPT's aptitude in answering quizzes, homework, exam, and laboratory questions in an introductory-level computer engineering course. This work finds that ChatGPT can do well on questions asking about generic concepts. However, predictably, as a text-only tool, it cannot handle questions with diagrams or figures, nor can it generate diagrams and figures. Further, also clearly, the tool cannot do hands-on lab experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#29992;&#20110;&#25552;&#20379;&#33258;&#36866;&#24212;&#30340;&#25945;&#32946;&#25903;&#25345;&#65292;&#23588;&#20854;&#23545;&#20110;&#26368;&#21021;&#25104;&#32489;&#36739;&#20302;&#30340;&#23398;&#29983;&#20855;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.04933</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#23548;&#21592;&#22312;&#25968;&#23398;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#25903;&#25345;&#20102;&#20302;&#25104;&#32489;&#23398;&#29983;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task. (arXiv:2304.04933v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#29992;&#20110;&#25552;&#20379;&#33258;&#36866;&#24212;&#30340;&#25945;&#32946;&#25903;&#25345;&#65292;&#23588;&#20854;&#23545;&#20110;&#26368;&#21021;&#25104;&#32489;&#36739;&#20302;&#30340;&#23398;&#29983;&#20855;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#38480;&#21046;&#20351;&#24471;&#20026;&#25152;&#26377;&#23398;&#29983;&#25552;&#20379;&#20010;&#24615;&#21270;&#25945;&#23398;&#21464;&#24471;&#22256;&#38590;&#12290;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#25104;&#20026;&#20943;&#23569;&#21457;&#23637;&#25104;&#26412;&#12289;&#25552;&#39640;&#26234;&#33021;&#36741;&#23548;&#36719;&#20214;&#25928;&#26524;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#26088;&#22312;&#20026;&#23398;&#29983;&#25552;&#20379;&#27491;&#30830;&#30340;&#25903;&#25345;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22914;&#20309;&#22312;&#21465;&#36848;&#25925;&#20107;&#32447;&#36719;&#20214;&#20013;&#20026;&#23398;&#20064;&#8220;&#23481;&#31215;&#8221;&#27010;&#24565;&#30340;&#23398;&#29983;&#25552;&#20379;&#33258;&#36866;&#24212;&#25945;&#32946;&#25903;&#25345;&#12290;&#36890;&#36807;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#25105;&#20204;&#20063;&#25552;&#21462;&#20102;&#26377;&#20851;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#27934;&#35265;&#65292;&#35777;&#26126;&#20102;&#25152;&#24471;&#25919;&#31574;&#22312;&#19981;&#21516;&#30340;&#23398;&#29983;&#32676;&#20307;&#20013;&#20855;&#26377;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#36825;&#20004;&#39033;&#30740;&#31350;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#25925;&#20107;&#31995;&#32479;&#23545;&#26368;&#21021;&#30340;&#39044;&#27979;&#20998;&#25968;&#26368;&#20302;&#30340;&#23398;&#29983;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#65292;&#36825;&#34920;&#26126;&#20102;AI&#36866;&#24212;&#24182;&#20026;&#20302;&#25104;&#32489;&#23398;&#29983;&#25552;&#20379;&#25903;&#25345;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource limitations make it hard to provide all students with one of the most effective educational interventions: personalized instruction. Reinforcement learning could be a key tool to reduce the development cost and improve the effectiveness of, intelligent tutoring software that aims to provide the right support, at the right time, to a student. Here we illustrate that deep reinforcement learning can be used to provide adaptive pedagogical support to students learning about the concept of volume in a narrative storyline software. Using explainable artificial intelligence tools, we also extracted interpretable insights about the pedagogical policy learned, and we demonstrate that the resulting policy had similar performance in a different student population. Most importantly, in both studies the reinforcement-learning narrative system had the largest benefit for those students with the lowest initial pretest scores, suggesting the opportunity for AI to adapt and provide support for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#21516;&#26102;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#24615;&#33021;&#21644;&#19981;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2304.04071</link><description>&lt;p&gt;
&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#25913;&#36827;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#24615;&#33021;&#19981;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Performance Insensitivity of Large-scale Multiobjective Optimization via Monte Carlo Tree Search. (arXiv:2304.04071v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#21516;&#26102;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#24615;&#33021;&#21644;&#19981;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;(LSMOP)&#30340;&#29305;&#28857;&#26159;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20914;&#31361;&#30446;&#26631;&#24182;&#28041;&#21450;&#25968;&#30334;&#20010;&#20915;&#31574;&#21464;&#37327;&#12290; &#35768;&#22810;&#24037;&#31243;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#21487;&#20197;&#24314;&#27169;&#20026;LSMOP&#12290;&#21516;&#26102;&#65292;&#24037;&#31243;&#24212;&#29992;&#35201;&#27714;&#24615;&#33021;&#19981;&#25935;&#24863;&#12290;&#36825;&#36890;&#24120;&#24847;&#21619;&#30528;&#31639;&#27861;&#36816;&#34892;&#30340;&#32467;&#26524;&#19981;&#20165;&#22312;&#24615;&#33021;&#26041;&#38754;&#23545;&#27599;&#27425;&#36816;&#34892;&#37117;&#24456;&#22909;&#65292;&#32780;&#19988;&#22810;&#27425;&#36816;&#34892;&#30340;&#24615;&#33021;&#19981;&#24212;&#27874;&#21160;&#22826;&#22823;&#65292;&#21363;&#31639;&#27861;&#21576;&#29616;&#33391;&#22909;&#30340;&#19981;&#25935;&#24863;&#24615;&#12290;&#32771;&#34385;&#21040;&#27599;&#27425;&#36816;&#34892;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#27492;&#25913;&#36827;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#31639;&#27861;&#30340;&#19981;&#25935;&#24863;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#21482;&#20851;&#27880;&#20110;&#25552;&#39640;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#19981;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large-scale multiobjective optimization problem (LSMOP) is characterized by simultaneously optimizing multiple conflicting objectives and involving hundreds of decision variables. {Many real-world applications in engineering fields can be modeled as LSMOPs; simultaneously, engineering applications require insensitivity in performance.} This requirement usually means that the results from the algorithm runs should not only be good for every run in terms of performance but also that the performance of multiple runs should not fluctuate too much, i.e., the algorithm shows good insensitivity. Considering that substantial computational resources are requested for each run, it is essential to improve upon the performance of the large-scale multiobjective optimization algorithm, as well as the insensitivity of the algorithm. However, existing large-scale multiobjective optimization algorithms solely focus on improving the performance of the algorithms, leaving the insensitivity characteri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03997</link><description>&lt;p&gt;
REDf&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network. (arXiv:2304.03997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19990;&#30028;&#21521;&#26356;&#21487;&#25345;&#32493;&#30340;&#33021;&#28304;&#26410;&#26469;&#21457;&#23637;&#65292;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#28304;&#32435;&#20837;&#30005;&#32593;&#30340;&#38598;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38388;&#27463;&#24615;&#20351;&#30005;&#32593;&#31649;&#29702;&#21644;&#30830;&#20445;&#31283;&#23450;&#30340;&#30005;&#21147;&#20379;&#24212;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#26469;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#33021;&#47071;&#38656;&#27714;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20123;&#32593;&#32476;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#22235;&#20010;&#21382;&#21490;&#33021;&#37327;&#38656;&#27714;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#30340;&#33021;&#28304;&#20998;&#37197;&#20844;&#21496;&#65292;&#21253;&#25324;&#32654;&#22269;&#30005;&#21147;&#12289;Commonwealth Edison&#12289;Dayton Power and Light&#20197;&#21450;&#23486;&#22805;&#27861;&#23612;&#20122;-&#26032;&#27901;&#35199;-&#39532;&#37324;&#20848;&#20114;&#32852;&#32593;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;REDf&#27169;&#22411;&#19982;&#20854;&#20182;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;REDf&#27169;&#22411;&#22312;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12289;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#20915;&#23450;&#31995;&#25968;&#31561;&#20934;&#30830;&#24230;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;REDf&#21487;&#20197;&#20316;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#30340;&#21487;&#38752;&#24037;&#20855;&#65292;&#24182;&#25552;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#32435;&#20837;&#26234;&#33021;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future. However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity. In this paper, we propose a deep learning-based approach for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. We use long short-term memory networks, which are well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four datasets of historical energy demand data from different energy distribution companies including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is also compared with two 
&lt;/p&gt;</description></item><item><title>InstructBio&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#26469;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#20851;&#27880;&#65292;&#36991;&#20813;&#20381;&#36182;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03906</link><description>&lt;p&gt;
InstructBio&#65306;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
InstructBio: A Large-scale Semi-supervised Learning Paradigm for Biochemical Problems. (arXiv:2304.03906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03906
&lt;/p&gt;
&lt;p&gt;
InstructBio&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#26469;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#20851;&#27880;&#65292;&#36991;&#20813;&#20381;&#36182;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#30340;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#22987;&#32456;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#22312;&#22823;&#22411;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#24378;&#21147;&#30340;&#20219;&#21153;&#26080;&#20851;&#27169;&#22411;&#65292;&#20294;&#22312;&#21521;&#19979;&#28216;&#20219;&#21153;&#36716;&#31227;&#30693;&#35782;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructBio&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26679;&#20363;&#12290;&#23427;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#26469;&#25552;&#20379;&#20266;&#26631;&#31614;&#21487;&#38752;&#24615;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#12290;&#36825;&#20123;&#32622;&#20449;&#24230;&#20998;&#25968;&#28982;&#21518;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#30340;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#30340;&#20851;&#27880;&#65292;&#36991;&#20813;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#20197;&#21450;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;InstructBio&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26041;&#38754;&#65292;&#22312;&#27963;&#24615;&#24748;&#23830;&#20272;&#35745;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of artificial intelligence for science, it is consistently an essential challenge to face a limited amount of labeled data for real-world problems. The prevailing approach is to pretrain a powerful task-agnostic model on a large unlabeled corpus but may struggle to transfer knowledge to downstream tasks. In this study, we propose InstructMol, a semi-supervised learning algorithm, to take better advantage of unlabeled examples. It introduces an instructor model to provide the confidence ratios as the measurement of pseudo-labels' reliability. These confidence scores then guide the target model to pay distinct attention to different data points, avoiding the over-reliance on labeled data and the negative influence of incorrect pseudo-annotations. Comprehensive experiments show that InstructBio substantially improves the generalization ability of molecular models, in not only molecular property predictions but also activity cliff estimations, demonstrating the superiority of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#25968;&#25454;&#36755;&#20837;&#24466;&#24351;&#27169;&#22411;&#65292;&#23398;&#20064;&#24182;&#25512;&#26029;&#20986;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#33258;&#23450;&#20041;&#22270;&#20687;&#65292;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2304.00186</link><description>&lt;p&gt;
&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Subject-driven Text-to-Image Generation via Apprenticeship Learning. (arXiv:2304.00186v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#25968;&#25454;&#36755;&#20837;&#24466;&#24351;&#27169;&#22411;&#65292;&#23398;&#20064;&#24182;&#25512;&#26029;&#20986;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#33258;&#23450;&#20041;&#22270;&#20687;&#65292;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;DreamBooth&#65289;&#22312;&#36890;&#36807;&#38024;&#23545;&#30446;&#26631;&#20027;&#39064;&#24494;&#35843;&#8220;&#19987;&#23478;&#27169;&#22411;&#8221;&#65292;&#29983;&#25104;&#39640;&#24230;&#33258;&#23450;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#24456;&#26114;&#36149;&#65292;&#22240;&#20026;&#27599;&#20010;&#20027;&#39064;&#37117;&#24517;&#39035;&#23398;&#20064;&#19968;&#20010;&#26032;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26367;&#20195;&#20027;&#39064;&#29305;&#23450;&#24494;&#35843;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#12290;&#32473;&#23450;&#19968;&#20010;&#26032;&#20027;&#39064;&#30340;&#23569;&#37327;&#28436;&#31034;&#65292;SuTI&#21487;&#20197;&#21363;&#26102;&#29983;&#25104;&#19981;&#21516;&#22330;&#26223;&#20013;&#20027;&#39064;&#30340;&#26032;&#29256;&#26412;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#20027;&#39064;&#29305;&#23450;&#30340;&#20248;&#21270;&#12290;SuTI&#30001;&#8220;&#24466;&#24351;&#23398;&#20064;&#8221;&#39537;&#21160;&#65292;&#20854;&#20013;&#20174;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21333;&#20010;&#30340;&#24466;&#24351;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20114;&#32852;&#32593;&#25366;&#25496;&#20102;&#25968;&#30334;&#19975;&#20010;&#22270;&#20687;&#31751;&#65292;&#27599;&#20010;&#22270;&#20687;&#31751;&#37117;&#32858;&#28966;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#35270;&#35273;&#20027;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#36825;&#20123;&#31751;&#26469;&#35757;&#32451;&#22823;&#37327;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#35270;&#35273;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#24466;&#24351;&#27169;&#22411;&#36890;&#36807;&#25512;&#26029;&#22522;&#20110;&#20854;&#25991;&#26412;&#25551;&#36848;&#30340;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#24182;&#29983;&#25104;&#22270;&#20687;&#26469;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SuTI&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#19981;&#21516;&#20027;&#39064;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#27604;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with \emph{in-context} learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by {\em apprenticeship learning}, where a single apprentice model is learned from data generated by massive amount of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train massive amount of expert models specialized on diff
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#32844;&#19994;&#22260;&#26827;&#36873;&#25163;&#30340;&#31227;&#21160;&#20915;&#31574;&#21457;&#29616;&#65292;&#22312;&#36229;&#20154;&#24037;&#26234;&#33021;&#38382;&#19990;&#21518;&#65292;&#20154;&#31867;&#24320;&#22987;&#20570;&#20986;&#26174;&#33879;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#26032;&#39062;&#30340;&#20915;&#31574;&#26356;&#39057;&#32321;&#22320;&#21457;&#29983;&#65292;&#21487;&#33021;&#24847;&#21619;&#30528;&#36229;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21487;&#20197;&#25913;&#21464;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07462</link><description>&lt;p&gt;
&#36229;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#26032;&#22855;&#24615;&#26469;&#25913;&#36827;&#20154;&#31867;&#20915;&#31574;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Superhuman Artificial Intelligence Can Improve Human Decision Making by Increasing Novelty. (arXiv:2303.07462v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#32844;&#19994;&#22260;&#26827;&#36873;&#25163;&#30340;&#31227;&#21160;&#20915;&#31574;&#21457;&#29616;&#65292;&#22312;&#36229;&#20154;&#24037;&#26234;&#33021;&#38382;&#19990;&#21518;&#65292;&#20154;&#31867;&#24320;&#22987;&#20570;&#20986;&#26174;&#33879;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#26032;&#39062;&#30340;&#20915;&#31574;&#26356;&#39057;&#32321;&#22320;&#21457;&#29983;&#65292;&#21487;&#33021;&#24847;&#21619;&#30528;&#36229;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21487;&#20197;&#25913;&#21464;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23558;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#20915;&#31574;&#65292;&#24182;&#19988;&#26377;&#21738;&#20123;&#26426;&#21046;&#21487;&#29992;&#20110;&#25903;&#25345;&#36825;&#31181;&#24433;&#21709;&#65311;&#25105;&#20204;&#22312;&#19968;&#20010;&#39046;&#22495;&#20013;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#39046;&#22495;&#30340;AI&#24050;&#32463;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#36807;&#21435;71&#24180;&#65288;1950-2021&#65289;&#32844;&#19994;&#22260;&#26827;&#36873;&#25163;&#25152;&#20570;&#30340;&#36229;&#36807;580&#19975;&#20010;&#31227;&#21160;&#20915;&#31574;&#12290;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#36229;&#20154;&#24037;&#26234;&#33021;&#31243;&#24207;&#26469;&#20272;&#35745;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20154;&#31867;&#20915;&#31574;&#36136;&#37327;&#65292;&#29983;&#25104;&#20102;580&#20159;&#20010;&#21453;&#20107;&#23454;&#30340;&#28216;&#25103;&#27169;&#24335;&#65292;&#24182;&#23558;&#23454;&#38469;&#20154;&#31867;&#20915;&#31574;&#30340;&#32988;&#29575;&#19982;&#21453;&#20107;&#23454;&#30340;AI&#20915;&#31574;&#30340;&#32988;&#29575;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36229;&#20154;&#24037;&#26234;&#33021;&#38382;&#19990;&#21518;&#65292;&#20154;&#31867;&#24320;&#22987;&#20570;&#20986;&#26174;&#33879;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#26102;&#38388;&#19978;&#26816;&#26597;&#20102;&#20154;&#31867;&#29609;&#23478;&#30340;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#22312;&#36229;&#20154;&#24037;&#26234;&#33021;&#38382;&#19990;&#21518;&#65292;&#26032;&#39062;&#30340;&#20915;&#31574;&#65288;&#21363;&#20197;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#31227;&#21160;&#65289;&#26356;&#39057;&#32321;&#22320;&#21457;&#29983;&#65292;&#24182;&#19982;&#26356;&#39640;&#30340;&#20915;&#31574;&#36136;&#37327;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#20154;&#24037;&#26234;&#33021;&#31243;&#24207;&#30340;&#21457;&#23637;&#21487;&#33021;&#20250;&#25913;&#21464;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
How will superhuman artificial intelligence (AI) affect human decision making? And what will be the mechanisms behind this effect? We address these questions in a domain where AI already exceeds human performance, analyzing more than 5.8 million move decisions made by professional Go players over the past 71 years (1950-2021). To address the first question, we use a superhuman AI program to estimate the quality of human decisions across time, generating 58 billion counterfactual game patterns and comparing the win rates of actual human decisions with those of counterfactual AI decisions. We find that humans began to make significantly better decisions following the advent of superhuman AI. We then examine human players' strategies across time and find that novel decisions (i.e., previously unobserved moves) occurred more frequently and became associated with higher decision quality after the advent of superhuman AI. Our findings suggest that the development of superhuman AI programs ma
&lt;/p&gt;</description></item><item><title>MCTS-GEB&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#37325;&#20889;&#31995;&#32479;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#26500;&#24314;&#26368;&#20248;&#30340;E&#22270;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#20013;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04651</link><description>&lt;p&gt;
MCTS-GEB&#65306;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26159;&#19968;&#20010;&#22909;&#30340;E&#22270;&#26500;&#24314;&#22120;
&lt;/p&gt;
&lt;p&gt;
MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder. (arXiv:2303.04651v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04651
&lt;/p&gt;
&lt;p&gt;
MCTS-GEB&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#37325;&#20889;&#31995;&#32479;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#26500;&#24314;&#26368;&#20248;&#30340;E&#22270;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#20013;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#20889;&#31995;&#32479;&#24191;&#27867;&#20351;&#29992;&#31561;&#24335;&#39281;&#21644;&#25216;&#26415;&#26469;&#20248;&#21270;&#37325;&#20889;&#39034;&#24207;&#65292;&#20294;&#26159;&#24403;E&#22270;&#27809;&#26377;&#39281;&#21644;&#26102;&#65292;&#26080;&#27861;&#20195;&#34920;&#25152;&#26377;&#21487;&#33021;&#30340;&#37325;&#20889;&#26426;&#20250;&#65292;&#20250;&#37325;&#26032;&#24341;&#20837;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MCTS-GEB&#65292;&#19968;&#20010;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#20110;E&#22270;&#26500;&#24314;&#30340;&#36890;&#29992;&#37325;&#20889;&#31995;&#32479;&#12290;MCTS-GEB&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#39640;&#25928;&#35268;&#21010;&#26368;&#20248;&#30340;E&#22270;&#26500;&#24314;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#38454;&#27573;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;MCTS-GEB&#37117;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rewrite systems [6, 10, 12] have been widely employing equality saturation [9], which is an optimisation methodology that uses a saturated e-graph to represent all possible sequences of rewrite simultaneously, and then extracts the optimal one. As such, optimal results can be achieved by avoiding the phase-ordering problem. However, we observe that when the e-graph is not saturated, it cannot represent all possible rewrite opportunities and therefore the phase-ordering problem is re-introduced during the construction phase of the e-graph. To address this problem, we propose MCTS-GEB, a domain-general rewrite system that applies reinforcement learning (RL) to e-graph construction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3] to efficiently plan for the optimal e-graph construction, and therefore it can effectively eliminate the phase-ordering problem at the construction phase and achieve better performance within a reasonable time. Evaluation in two different domains 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.13942</link><description>&lt;p&gt;
Inseq&#65306;&#19968;&#20010;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Inseq: An Interpretability Toolkit for Sequence Generation Models. (arXiv:2302.13942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36807;&#21435;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27969;&#34892;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#32780;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#19987;&#38376;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Inseq&#65292;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#20351;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#26222;&#21450;&#21270;&#12290;Inseq&#33021;&#22815;&#30452;&#35266;&#19988;&#20248;&#21270;&#22320;&#25552;&#21462;&#27969;&#34892;&#30340;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#30340;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#23427;&#26469;&#31361;&#20986;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24182;&#22312;GPT-2&#20013;&#23450;&#20301;&#20107;&#23454;&#30693;&#35782;&#12290;&#30001;&#20110;&#20854;&#25903;&#25345;&#23545;&#27604;&#29305;&#24449;&#24402;&#22240;&#31561;&#21069;&#27839;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#25509;&#21475;&#65292;&#22240;&#27492;Inseq&#21487;&#20197;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#65292;&#38598;&#20013;&#20248;&#33391;&#23454;&#36341;&#65292;&#24182;&#23454;&#29616;&#20844;&#27491;&#21644;&#21487;&#37325;&#22797;&#30340;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.05743</link><description>&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20165;&#20381;&#38752;&#36317;&#31163;&#30697;&#38453;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24120;&#29992;&#20110;&#28041;&#21450;&#22270;&#24418;&#20960;&#20309;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#34429;&#28982;&#20960;&#20309;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#21253;&#21547;&#23436;&#25972;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26080;&#27861;&#23398;&#20064;&#36825;&#31181;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#26032;&#39062;&#30340;&#23545;&#31216;&#20960;&#20309;&#22270;&#30340;&#23478;&#26063;&#65292;&#25193;&#23637;&#20102;MPNN&#26080;&#27861;&#21306;&#20998;&#20854;&#36317;&#31163;&#30697;&#38453;&#30340;&#21453;&#20363;&#23478;&#26063;&#65292;&#24182;&#25552;&#20986;$k$-DisGNNs&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#20960;&#20309;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;$k$-DisGNNs&#30340;&#29305;&#27530;&#24773;&#20917;&#32479;&#19968;&#36215;&#26469;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#37027;&#20123;&#26368;&#21021;&#20026;&#20302;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;GNN&#27169;&#22411;&#35774;&#35745;&#30340;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;Cox-Weibull&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27494;&#22120;&#31995;&#32479;&#21487;&#38752;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#27494;&#22120;&#31995;&#32479;&#29305;&#24449;&#26469;&#25913;&#21892;&#39044;&#27979;&#24615;&#32500;&#20462;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#27169;&#22411;&#30340;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2301.01850</link><description>&lt;p&gt;
&#24102;&#26377;Cox-Weibull&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27494;&#22120;&#31995;&#32479;&#21487;&#38752;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Bayesian Weapon System Reliability Modeling with Cox-Weibull Neural Network. (arXiv:2301.01850v5 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;Cox-Weibull&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27494;&#22120;&#31995;&#32479;&#21487;&#38752;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#27494;&#22120;&#31995;&#32479;&#29305;&#24449;&#26469;&#25913;&#21892;&#39044;&#27979;&#24615;&#32500;&#20462;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#27169;&#22411;&#30340;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;DeepSurv&#65289;&#23558;&#27494;&#22120;&#31995;&#32479;&#29305;&#24449;&#65288;&#22914;&#27494;&#22120;&#31995;&#32479;&#21046;&#36896;&#21830;&#12289;&#37096;&#32626;&#26102;&#38388;&#21644;&#22320;&#28857;&#12289;&#23384;&#20648;&#26102;&#38388;&#21644;&#22320;&#28857;&#31561;&#65289;&#25972;&#21512;&#21040;&#21442;&#25968;&#21270;&#30340;Cox-Weibull&#21487;&#38752;&#24615;&#27169;&#22411;&#20013;&#65292;&#20197;&#25913;&#21892;&#39044;&#27979;&#24615;&#32500;&#20462;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;MC-dropout&#31561;&#36864;&#28779;&#26041;&#27861;&#26469;&#23558;Weibull&#21442;&#25968;&#21442;&#25968;&#21270;&#24182;&#24320;&#21457;&#20102;&#21478;&#19968;&#31181;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#20197;&#36827;&#34892;&#27604;&#36739;&#30446;&#30340;&#12290;&#22240;&#20026;&#27494;&#22120;&#31995;&#32479;&#27979;&#35797;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#31243;&#24207;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21306;&#38388;&#34987;&#23457;&#26597;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#23427;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26399;&#38388;&#21512;&#24182;&#20102;Weibull&#21442;&#25968;&#30340;MCMC&#21462;&#26679;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20998;&#31867;&#25351;&#26631;&#65292;&#22914;&#25509;&#25910;&#22120;&#24037;&#20316;&#29305;&#24615;&#65288;ROC&#65289;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#12289;&#31934;&#24230;-&#21484;&#22238;&#65288;PR&#65289;AUC&#21644;F&#20998;&#25968;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#24120;&#20248;&#20110;&#20256;&#32479;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#22914;XGBoost&#21644;&#24403;&#21069;&#26631;&#20934;&#30340;&#26465;&#20214;Weibull&#27010;&#29575;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to integrate weapon system features (such as weapon system manufacturer, deployment time and location, storage time and location, etc.) into a parameterized Cox-Weibull [1] reliability model via a neural network, like DeepSurv [2], to improve predictive maintenance. In parallel, we develop an alternative Bayesian model by parameterizing the Weibull parameters with a neural network and employing dropout methods such as Monte-Carlo (MC)-dropout for comparative purposes. Due to data collection procedures in weapon system testing we employ a novel interval-censored log-likelihood which incorporates Monte-Carlo Markov Chain (MCMC) [3] sampling of the Weibull parameters during gradient descent optimization. We compare classification metrics such as receiver operator curve (ROC) area under the curve (AUC), precision-recall (PR) AUC, and F scores to show our model generally outperforms traditional powerful models such as XGBoost and the current standard conditional Weibull probabili
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#27979;&#37327;&#27963;&#21160;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#36710;&#36742;&#21644;&#26080;&#32447;&#30005;&#35775;&#38382;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#21508;&#31181;V2X&#30740;&#31350;&#20026;&#36710;&#36742;&#21644;&#24037;&#19994;&#36890;&#20449;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10343</link><description>&lt;p&gt;
Berlin V2X&#65306;&#22810;&#36710;&#36742;&#21644;&#26080;&#32447;&#30005;&#35775;&#38382;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Berlin V2X: A Machine Learning Dataset from Multiple Vehicles and Radio Access Technologies. (arXiv:2212.10343v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10343
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#27979;&#37327;&#27963;&#21160;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#36710;&#36742;&#21644;&#26080;&#32447;&#30005;&#35775;&#38382;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#21508;&#31181;V2X&#30740;&#31350;&#20026;&#36710;&#36742;&#21644;&#24037;&#19994;&#36890;&#20449;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#36890;&#20449;&#30340;&#21457;&#23637;&#24050;&#34987;&#39044;&#26399;&#20381;&#38752;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#20197;&#20351;&#26080;&#32447;&#32593;&#32476;&#32452;&#20214;&#37319;&#21462;&#20027;&#21160;&#20915;&#31574;&#21644;&#34892;&#21160;&#65292;&#20197;&#32500;&#25345;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;&#27492;&#22806;&#65292;&#23558;&#20986;&#29616;&#22312;&#36710;&#36742;&#21644;&#24037;&#19994;&#36890;&#20449;&#39046;&#22495;&#30340;&#26032;&#29992;&#20363;&#12290;&#20855;&#20307;&#22312;&#36710;&#36742;&#36890;&#20449;&#39046;&#22495;&#65292;&#36710;&#36742;&#23545;&#19968;&#20999;&#65288;V2X&#65289;&#26041;&#26696;&#23558;&#20250;&#20174;&#36825;&#20123;&#36827;&#23637;&#20013;&#21463;&#30410;&#21290;&#27973;&#12290;&#32771;&#34385;&#21040;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#27979;&#37327;&#27963;&#21160;&#65292;&#20026;&#21508;&#31181;&#22810;&#26679;&#30340;&#22522;&#20110;ML&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25152;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#22478;&#24066;&#29615;&#22659;&#19979;&#22522;&#31449;&#65288;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#36816;&#33829;&#21830;&#65289;&#21644;&#37051;&#36817;&#38142;&#36335;&#26080;&#32447;&#30005;&#35775;&#38382;&#25216;&#26415;&#30340;GPS&#23450;&#20301;&#26080;&#32447;&#27979;&#37327;&#65292;&#22240;&#27492;&#21487;&#20197;&#36827;&#34892;&#21508;&#31181;&#19981;&#21516;&#30340;V2X&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#26631;&#26377;&#26631;&#31614;&#65292;&#24182;&#20197;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#38468;&#26377;&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of wireless communications into 6G and beyond is expected to rely on new machine learning (ML)-based capabilities. These can enable proactive decisions and actions from wireless-network components to sustain quality-of-service (QoS) and user experience. Moreover, new use cases in the area of vehicular and industrial communications will emerge. Specifically in the area of vehicle communication, vehicle-to-everything (V2X) schemes will benefit strongly from such advances. With this in mind, we have conducted a detailed measurement campaign that paves the way to a plethora of diverse ML-based studies. The resulting datasets offer GPS-located wireless measurements across diverse urban environments for both cellular (with two different operators) and sidelink radio access technologies, thus enabling a variety of different studies towards V2X. The datasets are labeled and sampled with a high time resolution. Furthermore, we make the data publicly available with all the necessar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2212.03932</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#34892;&#20026;&#31574;&#30053;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#32447;&#35780;&#20272;&#20013;&#65292;&#38656;&#35201;&#35780;&#20272;&#30446;&#26631;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#38656;&#35201;&#20351;&#29992;&#30001;&#34892;&#20026;&#31574;&#30053;&#37319;&#38598;&#30340;&#26679;&#26412;&#25968;&#25454;&#12290;&#20256;&#32479;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#30001;&#20110;&#35745;&#31639;&#21160;&#20316;&#27010;&#29575;&#27604;&#20540;&#30340;&#20056;&#31215;&#32780;&#23548;&#33268;&#26041;&#24046;&#22686;&#21152;&#65292;&#20174;&#32780;&#22312;&#28041;&#21450;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#20986;&#29616;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In off-policy reinforcement learning, a behaviour policy performs exploratory interactions with the environment to obtain state-action-reward samples which are then used to learn a target policy that optimises the expected return. This leads to a problem of off-policy evaluation, where one needs to evaluate the target policy from samples collected by the often unrelated behaviour policy. Importance sampling is a traditional statistical technique that is often applied to off-policy evaluation. While importance sampling estimators are unbiased, their variance increases exponentially with the horizon of the decision process due to computing the importance weight as a product of action probability ratios, yielding estimates with low accuracy for domains involving long-term planning. This paper proposes state-based importance sampling (SIS), which drops the action probability ratios of sub-trajectories with "negligible states" -- roughly speaking, those for which the chosen actions have no 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; CLIP-Sculptor &#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#21644;&#32534;&#36753;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;&#65292;&#19988;&#19981;&#38656;&#35201; (&#25991;&#26412;&#65292;&#24418;&#29366;) &#23545;&#35757;&#32451;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;&#25913;&#36827;&#30340;&#25351;&#23548;&#26041;&#27861;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.01427</link><description>&lt;p&gt;
CLIP-Sculptor: &#33258;&#28982;&#35821;&#35328;&#38646;&#26679;&#26412;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;
&lt;/p&gt;
&lt;p&gt;
CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language. (arXiv:2211.01427v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01427
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; CLIP-Sculptor &#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#21644;&#32534;&#36753;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;&#65292;&#19988;&#19981;&#38656;&#35201; (&#25991;&#26412;&#65292;&#24418;&#29366;) &#23545;&#35757;&#32451;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;&#25913;&#36827;&#30340;&#25351;&#23548;&#26041;&#27861;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#19977;&#32500;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#29983;&#25104;&#30340;&#24418;&#29366;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102; CLIP-Sculptor&#65292;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#38656;&#35201;&#65288;&#25991;&#26412;&#65292;&#24418;&#29366;&#65289;&#23545;&#65292;&#23601;&#21487;&#20197;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;&#65292;&#23427;&#37319;&#29992;&#22810;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#39318;&#20808;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#65292;&#28982;&#21518;&#25552;&#21319;&#20998;&#36776;&#29575;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24418;&#29366;&#20445;&#30495;&#24230;&#65292;&#32780;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#20351;&#29992;&#20102; transformer &#26469;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807; CLIP &#30340;&#22270;&#20687;-&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#26465;&#20214;&#21270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#38656;&#35201;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;-&#22810;&#26679;&#24615;&#30340;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102; CLIP-Sculptor &#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312; https://ivl.cs.brown.edu/#/projects/clip-sculptor &#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and diversity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for improved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a transformer conditioned on CLIP's image-text embedding space. We also present a novel variant of classifier-free guidance, which improves the accuracy-diversity trade-off. Finally, we perform extensive experiments demonstrating that CLIP-Sculptor outperforms state-of-the-art baselines. The code is available at https://ivl.cs.brown.edu/#/projects/clip-sculptor.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.08471</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#34701;&#21512;&#25552;&#39640;&#35821;&#20041;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;BERT&#65292;&#22312;&#35821;&#20041;&#21477;&#23376;&#21305;&#37197;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#21516;&#26102;&#65292;&#20381;&#36182;&#24615;&#20808;&#39564;&#30693;&#35782;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#20063;&#26174;&#31034;&#20986;&#26222;&#36941;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#20381;&#36182;&#24615;&#20808;&#39564;&#32467;&#26500;&#26377;&#25928;&#22320;&#38598;&#25104;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#65292;&#20173;&#26410;&#30830;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DAFA&#30340;&#20381;&#36182;&#22686;&#24378;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36825;&#23558;&#20381;&#36182;&#32467;&#26500;&#26126;&#30830;&#22320;&#24341;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#21040;&#35821;&#20041;&#20449;&#24687;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;DAFA&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#25935;&#24863;&#33539;&#24335;&#26469;&#26500;&#24314;&#19968;&#20010;&#20381;&#36182;&#30697;&#38453;&#65292;&#20197;&#26657;&#20934;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#23427;&#37319;&#29992;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#26469;&#38598;&#25104;&#33719;&#21462;&#30340;&#20381;&#36182;&#20449;&#24687;&#21644;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;DAFA&#37325;&#26500;&#20102;&#27880;&#24847;&#21147;&#35745;&#31639;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained models like BERT have achieved great progress on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also shown general benefits in multiple NLP tasks. However, how to efficiently integrate dependency prior structure into pre-trained models to better model complex semantic matching relations is still unsettled. In this paper, we propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion \textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency structure into pre-trained models and adaptively fuses it with semantic information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a structure-sensitive paradigm to construct a dependency matrix for calibrating attention weights. It adopts an adaptive fusion module to integrate the obtained dependency information and the original semantic signals. Moreover, DAFA reconstructs the attention calculation flow and provides better interpretability. By applying it o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#26377;&#38480;&#36895;&#31232;&#30095;&#37327;&#23376;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#38543;&#26426;&#20108;&#37096;&#22270;&#19978;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#22810;&#20010;&#32422;&#26463;&#38480;&#21046;&#12290;&#22312;&#28385;&#36275;&#38454;&#27573;&#20013;&#25214;&#21040;&#30340;&#31232;&#30095;&#30721;&#33021;&#23454;&#29616;&#28034;&#25273;&#22122;&#22768;&#30340;&#36890;&#36947;&#23481;&#37327;&#65292;&#19988;&#20013;&#31561;&#22823;&#23567;&#30340;&#26377;&#38480;&#36895;&#31232;&#30095;&#37327;&#23376;&#30721;&#24456;&#23481;&#26131;&#25214;&#21040;&#12290;</title><link>http://arxiv.org/abs/2207.03562</link><description>&lt;p&gt;
&#8220;&#26377;&#38480;&#36895;&#31232;&#30095;&#37327;&#23376;&#30721;&#22810;&#22914;&#29275;&#27611;&#8221;
&lt;/p&gt;
&lt;p&gt;
Finite-rate sparse quantum codes aplenty. (arXiv:2207.03562v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#26377;&#38480;&#36895;&#31232;&#30095;&#37327;&#23376;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#38543;&#26426;&#20108;&#37096;&#22270;&#19978;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#22810;&#20010;&#32422;&#26463;&#38480;&#21046;&#12290;&#22312;&#28385;&#36275;&#38454;&#27573;&#20013;&#25214;&#21040;&#30340;&#31232;&#30095;&#30721;&#33021;&#23454;&#29616;&#28034;&#25273;&#22122;&#22768;&#30340;&#36890;&#36947;&#23481;&#37327;&#65292;&#19988;&#20013;&#31561;&#22823;&#23567;&#30340;&#26377;&#38480;&#36895;&#31232;&#30095;&#37327;&#23376;&#30721;&#24456;&#23481;&#26131;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#38543;&#26426;&#22810;&#37327;&#23376;&#27604;&#29305;&#31283;&#23450;&#22120;&#30721;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#27714;&#35299;&#38543;&#26426;&#20108;&#37096;&#22270;&#19978;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CSP&#65289;&#12290;&#36825;&#20010;&#26694;&#26550;&#20801;&#35768;&#25105;&#20204;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#31283;&#23450;&#22120;&#20132;&#25442;&#65292;$X/Z$ &#24179;&#34913;&#65292;&#26377;&#38480;&#36895;&#24230;&#65292;&#31232;&#30095;&#24230;&#21644;&#26368;&#22823;&#24230;&#25968;&#32422;&#26463;&#12290;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340; CSP &#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#23384;&#22312;&#28385;&#36275;&#38408;&#20540;&#30340;&#26377;&#21147;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#28385;&#36275;&#38454;&#27573;&#30340;&#33539;&#22260;&#38543;&#30528;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#22312;&#36825;&#20010;&#38454;&#27573;&#20013;&#65292;&#25214;&#21040;&#31232;&#30095;&#30340;&#30721;&#21464;&#24471;&#24456;&#23481;&#26131;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#28385;&#36275;&#38454;&#27573;&#20013;&#25214;&#21040;&#30340;&#31232;&#30095;&#30721;&#23454;&#38469;&#19978;&#23454;&#29616;&#20102;&#28034;&#25273;&#22122;&#22768;&#30340;&#36890;&#36947;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20013;&#31561;&#22823;&#23567;&#30340;&#26377;&#38480;&#36895;&#31232;&#30095;&#37327;&#23376;&#30721;&#24456;&#23481;&#26131;&#25214;&#21040;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#29983;&#25104;&#20855;&#26377;&#33258;&#23450;&#20041;&#23646;&#24615;&#30340;&#22909;&#30721;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#23436;&#25972;&#21644;&#21487;&#23450;&#21046;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#38480;&#36895;&#24230;&#31232;&#30095;&#37327;&#23376;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a methodology for generating random multi-qubit stabilizer codes based on solving a constraint satisfaction problem (CSP) on random bipartite graphs. This framework allows us to enforce stabilizer commutation, $X/Z$ balancing, finite rate, sparsity, and maximum-degree constraints simultaneously in a CSP that we can then solve numerically. Using a state-of-the-art CSP solver, we obtain convincing evidence for the existence of a satisfiability threshold. Furthermore, the extent of the satisfiable phase increases with the number of qubits. In that phase, finding sparse codes becomes an easy problem. Moreover, we observe that the sparse codes found in the satisfiable phase practically achieve the channel capacity for erasure noise. Our results show that intermediate-size finite-rate sparse quantum codes are easy to find, while also demonstrating a flexible methodology for generating good codes with custom properties. We therefore establish a complete and customizable pipeline 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#32447;&#26041;&#27861;&#30340;&#31574;&#30053;&#25552;&#21319;&#20445;&#35777;&#21644;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#36890;&#36807;&#26679;&#26412;&#37325;&#29992;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.13714</link><description>&lt;p&gt;
&#24102;&#29702;&#35770;&#25903;&#25345;&#30340;&#26679;&#26412;&#37325;&#29992;&#30340;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse. (arXiv:2206.13714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13714
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#32447;&#26041;&#27861;&#30340;&#31574;&#30053;&#25552;&#21319;&#20445;&#35777;&#21644;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#36890;&#36807;&#26679;&#26412;&#37325;&#29992;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#25913;&#21892;&#22797;&#26434;&#31995;&#32479;&#36816;&#34892;&#30340;&#28508;&#21147;&#65292;&#32780;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#34920;&#20102;&#19968;&#31181;&#27969;&#34892;&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#31867;&#21035;&#22312;&#23454;&#38469;&#25511;&#21046;&#37096;&#32626;&#30340;&#20004;&#20010;&#37325;&#35201;&#35201;&#27714;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65306;&#65288;i&#65289;&#23454;&#38469;&#24615;&#33021;&#20445;&#35777;&#21644;&#65288;ii&#65289;&#25968;&#25454;&#25928;&#29575;&#12290;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#36890;&#36807;&#26679;&#26412;&#37325;&#29992;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#20294;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#32780;&#22312;&#32447;&#31574;&#30053;&#31639;&#27861;&#20445;&#35777;&#20102;&#35757;&#32451;&#26399;&#38388;&#30340;&#36817;&#20284;&#31574;&#30053;&#25913;&#36827;&#65292;&#20294;&#21463;&#21040;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24179;&#34913;&#36825;&#20123;&#31454;&#20105;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31867;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22312;&#32447;&#26041;&#27861;&#30340;&#31574;&#30053;&#25552;&#21319;&#20445;&#35777;&#21644;&#26679;&#26412;&#37325;&#29992;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;DeepMind C&#30340;&#22810;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#36827;&#34892; extensive &#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#31867;&#31639;&#27861;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven, learning-based control methods offer the potential to improve operations in complex systems, and model-free deep reinforcement learning represents a popular approach to data-driven control. However, existing classes of algorithms present a trade-off between two important deployment requirements for real-world control: (i) practical performance guarantees and (ii) data efficiency. Off-policy algorithms make efficient use of data through sample reuse but lack theoretical guarantees, while on-policy algorithms guarantee approximate policy improvement throughout training but suffer from high sample complexity. In order to balance these competing goals, we develop a class of Generalized Policy Improvement algorithms that combines the policy improvement guarantees of on-policy methods with the efficiency of sample reuse. We demonstrate the benefits of this new class of algorithms through extensive experimental analysis on a variety of continuous control tasks from the DeepMind C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#30340;&#26694;&#26550;&#65292;&#24182;&#25351;&#20986;&#23454;&#39564;&#35774;&#32622;&#38382;&#39064;&#21644;&#26410;&#33021;&#27491;&#30830;&#32771;&#34385;&#25968;&#25454;&#21464;&#24322;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2204.07610</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Sources of Irreproducibility in Machine Learning: A Review. (arXiv:2204.07610v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#30340;&#26694;&#26550;&#65292;&#24182;&#25351;&#20986;&#23454;&#39564;&#35774;&#32622;&#38382;&#39064;&#21644;&#26410;&#33021;&#27491;&#30830;&#32771;&#34385;&#25968;&#25454;&#21464;&#24322;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#35768;&#22810;&#21457;&#24067;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#26159;&#19981;&#21487;&#37325;&#22797;&#30340;&#12290;&#26041;&#27861;&#35770;&#38382;&#39064;&#20197;&#21450;&#26410;&#33021;&#27491;&#30830;&#32771;&#34385;&#31639;&#27861;&#26412;&#36523;&#25110;&#20854;&#23454;&#29616;&#24341;&#20837;&#30340;&#21464;&#24322;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#37325;&#22797;&#24615;&#30340;&#20027;&#35201;&#36129;&#29486;&#22240;&#32032;&#12290;&#38382;&#39064;&#65306;&#19981;&#23384;&#22312;&#23558;&#23454;&#39564;&#35774;&#35745;&#36873;&#25321;&#19982;&#20854;&#23545;&#32467;&#35770;&#30340;&#28508;&#22312;&#24433;&#21709;&#32852;&#31995;&#36215;&#26469;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#32570;&#20047;&#36825;&#26679;&#30340;&#26694;&#26550;&#65292;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#23454;&#39564;&#32467;&#26524;&#21644;&#25551;&#36848;&#23454;&#39564;&#30340;&#38480;&#21046;&#20250;&#26356;&#21152;&#22256;&#38590;&#12290;&#32570;&#20047;&#36825;&#26679;&#30340;&#26694;&#26550;&#20063;&#20351;&#24471;&#29420;&#31435;&#30740;&#31350;&#20154;&#21592;&#38590;&#20197;&#31995;&#32479;&#22320;&#24402;&#22240;&#20110;&#22833;&#36133;&#30340;&#21487;&#37325;&#22797;&#24615;&#23454;&#39564;&#30340;&#21407;&#22240;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#29702;&#35299;&#21738;&#20123;&#23454;&#39564;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#21457;&#29616;&#65292;&#24182;&#36890;&#36807;&#27492;&#29702;&#35299;&#22914;&#20309;&#20998;&#26512;&#21487;&#37325;&#22797;&#24615;&#23454;&#39564;&#30340;&#32467;&#35770;&#65292;&#20174;&#32780;&#24110;&#21161;&#20998;&#26512;&#32467;&#35770;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#31639;&#27861;&#36873;&#25321;&#12289;&#25968;&#25454;&#21464;&#24322;&#12289;&#23454;&#39564;&#35774;&#32622;&#21644;&#23454;&#29616;&#32454;&#33410;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#20851;&#20110;&#21487;&#37325;&#22797;&#24615;&#30340;&#26368;&#36817;&#25991;&#29486;&#65292;&#30830;&#23450;&#20102;&#24120;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#30340;&#24433;&#21709;&#30340;&#20363;&#23376;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#23454;&#39564;&#35774;&#32622;&#38382;&#39064;&#21644;&#26410;&#33021;&#27491;&#30830;&#32771;&#34385;&#25968;&#25454;&#21464;&#24322;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#12290;&#32467;&#35770;&#65306;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#24182;&#22686;&#21152;&#32467;&#26524;&#30340;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Many published machine learning studies are irreproducible. Issues with methodology and not properly accounting for variation introduced by the algorithm themselves or their implementations are attributed as the main contributors to the irreproducibility.Problem: There exist no theoretical framework that relates experiment design choices to potential effects on the conclusions. Without such a framework, it is much harder for practitioners and researchers to evaluate experiment results and describe the limitations of experiments. The lack of such a framework also makes it harder for independent researchers to systematically attribute the causes of failed reproducibility experiments. Objective: The objective of this paper is to develop a framework that enable applied data science practitioners and researchers to understand which experiment design choices can lead to false findings and how and by this help in analyzing the conclusions of reproducibility experiments. Method: We
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#22343;&#26041;&#35823;&#24046;&#65288;LMSE&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20989;&#25968;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#26356;&#31283;&#23450;&#12289;&#20855;&#26377;&#26356;&#24378;&#30340;&#25910;&#25947;&#24615;&#21644;&#26356;&#22909;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.05748</link><description>&lt;p&gt;
&#31283;&#20581;&#35757;&#32451;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#31616;&#26126;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Concise Logarithmic Loss Function for Robust Training of Anomaly Detection Model. (arXiv:2201.05748v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#22343;&#26041;&#35823;&#24046;&#65288;LMSE&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20989;&#25968;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#26356;&#31283;&#23450;&#12289;&#20855;&#26377;&#26356;&#24378;&#30340;&#25910;&#25947;&#24615;&#21644;&#26356;&#22909;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#33021;&#22815;&#22312;&#27809;&#26377;&#25110;&#26368;&#23569;&#39046;&#22495;&#30693;&#35782;&#24773;&#20917;&#19979;&#24314;&#31435;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26356;&#31283;&#23450;&#22320;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#35813;&#23450;&#20041;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25110;&#25439;&#22833;&#20989;&#25968;&#12290;&#23545;&#20110;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20989;&#25968;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#25968;&#22343;&#26041;&#35823;&#24046;&#65288;LMSE&#65289;&#65292;&#20197;&#26356;&#31283;&#23450;&#22320;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#30740;&#31350;&#28085;&#30422;&#20102;&#25968;&#23398;&#27604;&#36739;&#65292;&#21453;&#21521;&#20256;&#25773;&#30340;&#24046;&#20998;&#22495;&#21487;&#35270;&#21270;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25439;&#22833;&#25910;&#25947;&#21644;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#31561;&#21508;&#20010;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#22312;&#24635;&#20307;&#19978;&#65292;LMSE&#22312;&#25439;&#22833;&#25910;&#25947;&#24378;&#24230;&#12289;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;MSE&#20989;&#25968;&#12290;LMSE&#20989;&#25968;&#39044;&#35745;&#21487;&#36866;&#29992;&#20110;&#35757;&#32451;&#21508;&#31181;&#31867;&#22411;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning-based algorithms are widely adopted due to the advantage of being able to establish anomaly detection models without or with minimal domain knowledge of the task. Instead, to train the artificial neural network more stable, it should be better to define the appropriate neural network structure or the loss function. For the training anomaly detection model, the mean squared error (MSE) function is adopted widely. On the other hand, the novel loss function, logarithmic mean squared error (LMSE), is proposed in this paper to train the neural network more stable. This study covers a variety of comparisons from mathematical comparisons, visualization in the differential domain for backpropagation, loss convergence in the training process, and anomaly detection performance. In an overall view, LMSE is superior to the existing MSE function in terms of strongness of loss convergence, anomaly detection performance. The LMSE function is expected to be applicable for train
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#23450;&#21046;&#21270;&#21046;&#36896;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#26234;&#33021;&#24037;&#21378;&#30340;&#29305;&#28857;&#65292;&#21253;&#25324;&#33258;&#25105;&#24863;&#30693;&#12289;&#36816;&#33829;&#20248;&#21270;&#12289;&#21160;&#24577;&#37325;&#26500;&#21644;&#26234;&#33021;&#20915;&#31574;&#65292;&#24182;&#21487;&#23454;&#29616;&#26234;&#33021;&#29983;&#20135;&#12289;&#32852;&#32593;&#21327;&#20316;&#21644;&#25193;&#23637;&#26381;&#21153;&#27169;&#22411;&#31561;&#19994;&#21153;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2108.03383</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23450;&#21046;&#21270;&#21046;&#36896;&#24037;&#21378;&#65306;&#20851;&#38190;&#25216;&#26415;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence-Driven Customized Manufacturing Factory: Key Technologies, Applications, and Challenges. (arXiv:2108.03383v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.03383
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#23450;&#21046;&#21270;&#21046;&#36896;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#26234;&#33021;&#24037;&#21378;&#30340;&#29305;&#28857;&#65292;&#21253;&#25324;&#33258;&#25105;&#24863;&#30693;&#12289;&#36816;&#33829;&#20248;&#21270;&#12289;&#21160;&#24577;&#37325;&#26500;&#21644;&#26234;&#33021;&#20915;&#31574;&#65292;&#24182;&#21487;&#23454;&#29616;&#26234;&#33021;&#29983;&#20135;&#12289;&#32852;&#32593;&#21327;&#20316;&#21644;&#25193;&#23637;&#26381;&#21153;&#27169;&#22411;&#31561;&#19994;&#21153;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#25209;&#37327;&#29983;&#20135;&#33539;&#24335;&#26080;&#27861;&#28789;&#27963;&#28385;&#36275;&#20010;&#24615;&#21270;&#23458;&#25143;&#38656;&#27714;&#12290;&#26032;&#19968;&#20195;&#26234;&#33021;&#24037;&#21378;&#39044;&#35745;&#25903;&#25345;&#26032;&#30340;&#22810;&#21697;&#31181;&#21644;&#23567;&#25209;&#37327;&#23450;&#21046;&#29983;&#20135;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#36890;&#36807;&#21152;&#36895;&#21046;&#36896;&#21644;&#20449;&#24687;&#36890;&#20449;&#25216;&#26415;&#30340;&#25972;&#21512;&#65288;&#21253;&#25324;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#25511;&#21046;&#65289;&#65292;&#23454;&#29616;&#26356;&#39640;&#38468;&#21152;&#20540;&#30340;&#21046;&#36896;&#12290;&#23450;&#21046;&#21270;&#26234;&#33021;&#24037;&#21378;&#30340;&#29305;&#28857;&#26159;&#33258;&#25105;&#24863;&#30693;&#12289;&#36816;&#33829;&#20248;&#21270;&#12289;&#21160;&#24577;&#37325;&#26500;&#21644;&#26234;&#33021;&#20915;&#31574;&#12290;AI&#25216;&#26415;&#23558;&#20351;&#21046;&#36896;&#31995;&#32479;&#33021;&#22815;&#24863;&#30693;&#29615;&#22659;&#12289;&#36866;&#24212;&#22806;&#37096;&#38656;&#27714;&#65292;&#24182;&#25552;&#21462;&#21253;&#25324;&#26234;&#33021;&#29983;&#20135;&#12289;&#32852;&#32593;&#21327;&#20316;&#21644;&#25193;&#23637;&#26381;&#21153;&#27169;&#22411;&#22312;&#20869;&#30340;&#21152;&#24037;&#30693;&#35782;&#21644;&#19994;&#21153;&#27169;&#22411;&#12290;&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;AI&#22312;&#23450;&#21046;&#21270;&#21046;&#36896;&#65288;CM&#65289;&#20013;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional production paradigm of large batch production does not offer flexibility towards satisfying the requirements of individual customers. A new generation of smart factories is expected to support new multi-variety and small-batch customized production modes. For that, Artificial Intelligence (AI) is enabling higher value-added manufacturing by accelerating the integration of manufacturing and information communication technologies, including computing, communication, and control. The characteristics of a customized smart factory are to include self-perception, operations optimization, dynamic reconfiguration, and intelligent decision-making. The AI technologies will allow manufacturing systems to perceive the environment, adapt to external needs, and extract the processed knowledge, including business models, such as intelligent production, networked collaboration, and extended service models.  This paper focuses on the implementation of AI in customized manufacturing (CM)
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35299;&#20915;&#28041;&#21450;&#20960;&#20309;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#28041;&#21450;&#20960;&#20309;&#30340;COP&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35299;&#20915;&#36136;&#37327;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2107.01759</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#23398;&#20064;&#20960;&#20309;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning Geometric Combinatorial Optimization Problems using Self-attention and Domain Knowledge. (arXiv:2107.01759v2 [cs.CG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.01759
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35299;&#20915;&#28041;&#21450;&#20960;&#20309;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#28041;&#21450;&#20960;&#20309;&#30340;COP&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35299;&#20915;&#36136;&#37327;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65288;COP&#65289;&#26159;&#21508;&#20010;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#26377;&#35768;&#22810;&#23581;&#35797;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COP&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#21644;&#26032;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#22522;&#20110;&#20960;&#20309;&#30340;COP&#12290;&#35813;&#27169;&#22411;&#30340;&#35774;&#35745;&#20351;&#24471;&#20854;&#33021;&#22815;&#39640;&#25928;&#22320;&#20351;&#29992;&#32534;&#30721;&#22120;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#20960;&#20309;COP&#20013;&#30340;&#28857;&#23545;&#28857;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#25490;&#24207;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#27495;&#20041;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#35268;&#24459;&#21644;&#26377;&#25928;&#22320;&#23398;&#20064;&#24207;&#21015;&#12290;&#20960;&#20309;COP&#28041;&#21450;&#38656;&#35201;&#28385;&#36275;&#30340;&#20960;&#20309;&#35201;&#27714;&#12290;&#22312;&#35299;&#30721;&#22120;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#30693;&#35782;&#30340;&#26032;&#30340;&#25513;&#34109;&#26041;&#26696;&#65292;&#20197;&#22312;&#38382;&#39064;&#30340;&#20960;&#20309;&#35201;&#27714;&#26410;&#34987;&#28385;&#36275;&#26102;&#25552;&#20379;&#39640;&#20195;&#20215;&#12290;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#28041;&#21450;&#20960;&#20309;&#30340;COP&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20960;&#20309;COP&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#36136;&#37327;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization problems (COPs) are an important research topic in various fields. In recent times, there have been many attempts to solve COPs using deep learning-based approaches. We propose a novel neural network model that solves COPs involving geometry based on self-attention and a new attention mechanism. The proposed model is designed such that the model efficiently learns point-to-point relationships in COPs involving geometry using self-attention in the encoder. We propose efficient input and output sequence ordering methods that reduce ambiguities such that the model learns the sequences more regularly and effectively. Geometric COPs involve geometric requirements that need to be satisfied. In the decoder, a new masking scheme using domain knowledge is proposed to provide a high penalty when the geometric requirement of the problem is not satisfied. The proposed neural net is a flexible framework that can be applied to various COPs involving geometry. We conduct ex
&lt;/p&gt;</description></item></channel></rss>