<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#32511;&#33394;&#35745;&#31639;&#39046;&#22495;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#35745;&#31639;&#36164;&#28304;&#21644;&#29615;&#22659;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00447</link><description>&lt;p&gt;
&#20851;&#20110;&#32511;&#33394;&#35745;&#31639;&#30340;&#26426;&#36935;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Opportunities of Green Computing: A Survey. (arXiv:2311.00447v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#32511;&#33394;&#35745;&#31639;&#39046;&#22495;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#35745;&#31639;&#36164;&#28304;&#21644;&#29615;&#22659;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#25216;&#26415;&#21644;&#30740;&#31350;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12289;&#35821;&#38899;&#21512;&#25104;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#32773;&#20851;&#27880;&#20110;&#36861;&#27714;&#26032;&#30340;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#32467;&#26524;&#65292;&#23548;&#33268;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#12290;&#23545;&#20110;&#39640;&#35745;&#31639;&#33021;&#21147;&#30340;&#38656;&#27714;&#23548;&#33268;&#26356;&#39640;&#30340;&#30899;&#25490;&#25918;&#65292;&#24182;&#36890;&#36807;&#38459;&#27490;&#36164;&#37329;&#26377;&#38480;&#30340;&#23567;&#22411;&#25110;&#20013;&#22411;&#30740;&#31350;&#26426;&#26500;&#21644;&#20844;&#21496;&#21442;&#19982;&#30740;&#31350;&#26469;&#30772;&#22351;&#30740;&#31350;&#20844;&#24179;&#24615;&#12290;&#20026;&#24212;&#23545;&#35745;&#31639;&#36164;&#28304;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#29615;&#22659;&#24433;&#21709;&#30340;&#25361;&#25112;&#65292;&#32511;&#33394;&#35745;&#31639;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#32511;&#33394;&#35745;&#31639;&#20013;&#20351;&#29992;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;G&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has achieved significant advancements in technology and research with the development over several decades, and is widely used in many areas including computing vision, natural language processing, time-series analysis, speech synthesis, etc. During the age of deep learning, especially with the arise of Large Language Models, a large majority of researchers' attention is paid on pursuing new state-of-the-art (SOTA) results, resulting in ever increasing of model size and computational complexity. The needs for high computing power brings higher carbon emission and undermines research fairness by preventing small or medium-sized research institutions and companies with limited funding in participating in research. To tackle the challenges of computing resources and environmental impact of AI, Green Computing has become a hot research topic. In this survey, we give a systematic overview of the technologies used in Green Computing. We propose the framework of G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13121</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21152;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20687;Transformer&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#23545;&#20110;&#20854;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#35813;&#27169;&#22411;&#24320;&#22987;&#35745;&#31639;&#36739;&#26202;&#65292;&#20294;&#25191;&#34892;&#36895;&#24230;&#38750;&#24120;&#24555;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20104;&#20197;&#35299;&#37322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#21644;&#25968;&#23398;&#24314;&#27169;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#24191;&#27867;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#22810;&#23618;Transformer&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31454;&#20105;&#23616;&#37096;&#23398;&#20064;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#38598;&#20307;&#23398;&#20064;&#20013;&#30340;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20010;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.12802</link><description>&lt;p&gt;
&#19968;&#31181;&#38598;&#20307;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#25928;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
An effective theory of collective deep learning. (arXiv:2310.12802v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12802
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31454;&#20105;&#23616;&#37096;&#23398;&#20064;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#38598;&#20307;&#23398;&#20064;&#20013;&#30340;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20010;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#32806;&#21512;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20013;&#38598;&#20307;&#23398;&#20064;&#30340;&#20986;&#29616;&#26159;&#23545;&#29289;&#29702;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#31038;&#20250;&#23398;&#30340;&#24191;&#27867;&#24433;&#21709;&#30340;&#19968;&#39033;&#21162;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#20010;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#21442;&#25968;&#30340;&#23616;&#37096;&#23398;&#20064;&#21160;&#24577;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#20043;&#38388;&#30340;&#31454;&#20105;&#65292;&#23558;&#20960;&#20010;&#26368;&#36817;&#30340;&#20998;&#25955;&#31639;&#27861;&#36827;&#34892;&#20102;&#21387;&#32553;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#19982;&#20855;&#26377;&#28140;&#28781;&#38543;&#26426;&#24615;&#30340;Ginzburg-Landau&#27169;&#22411;&#31867;&#20284;&#30340;&#32447;&#24615;&#32593;&#32476;&#30340;&#26377;&#25928;&#29702;&#35770;&#65292;&#25512;&#23548;&#20986;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#31895;&#31890;&#21270;&#34892;&#20026;&#12290;&#36825;&#20010;&#26694;&#26550;&#39044;&#27979;&#20102;&#21442;&#25968;&#35299;&#30340;&#65288;&#28145;&#24230;&#20381;&#36182;&#30340;&#65289;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#25581;&#31034;&#20102;&#38598;&#20307;&#23398;&#20064;&#30456;&#30340;&#24320;&#22987;&#65292;&#20197;&#21450;&#28145;&#24230;&#24341;&#36215;&#30340;&#20020;&#30028;&#28857;&#24310;&#36831;&#21644;&#24494;&#35266;&#23398;&#20064;&#36335;&#24452;&#30340;&#40065;&#26834;&#24418;&#29366;&#12290;&#25105;&#20204;&#22312;&#29616;&#23454;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unraveling the emergence of collective learning in systems of coupled artificial neural networks is an endeavor with broader implications for physics, machine learning, neuroscience and society. Here we introduce a minimal model that condenses several recent decentralized algorithms by considering a competition between two terms: the local learning dynamics in the parameters of each neural network unit, and a diffusive coupling among units that tends to homogenize the parameters of the ensemble. We derive the coarse-grained behavior of our model via an effective theory for linear networks that we show is analogous to a deformed Ginzburg-Landau model with quenched disorder. This framework predicts (depth-dependent) disorder-order-disorder phase transitions in the parameters' solutions that reveal the onset of a collective learning phase, along with a depth-induced delay of the critical point and a robust shape of the microscopic learning path. We validate our theory in realistic ensembl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10378</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26174;&#31034;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#29992;&#25143;&#20174;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;PLM&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65288;CLC&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#19968;&#33268;&#24615;&#65288;RankC&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#35780;&#20272;&#36328;&#35821;&#35328;&#38388;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20915;&#23450;CLC&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#27169;&#22411;&#23618;&#38754;&#21644;&#35821;&#35328;&#23545;&#23618;&#38754;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#35821;&#35328;&#20013;&#30340;&#20107;&#23454;&#25506;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#33021;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#32534;&#36753;&#22312;PLMs&#20013;&#25554;&#20837;&#26032;&#30340;&#20107;&#23454;&#20851;&#32852;&#36827;&#34892;&#20102;&#19968;&#20010;CLC&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23545;&#19968;&#23567;&#37096;&#20998;&#20107;&#23454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#65292;&#21033;&#29992;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#21487;&#20197;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#22343;&#34913;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#19979;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#36890;&#20449;&#12290;&#36825;&#19968;&#32467;&#35770;&#31283;&#20581;&#65292;&#24182;&#23545;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#31639;&#27861;&#38388;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24066;&#22330;&#20013;&#30340;&#32463;&#27982;&#23398;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07867</link><description>&lt;p&gt;
&#24265;&#20215;&#23545;&#35805;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cheap Talking Algorithms. (arXiv:2310.07867v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#65292;&#21033;&#29992;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#21487;&#20197;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#22343;&#34913;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#19979;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#36890;&#20449;&#12290;&#36825;&#19968;&#32467;&#35770;&#31283;&#20581;&#65292;&#24182;&#23545;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#31639;&#27861;&#38388;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24066;&#22330;&#20013;&#30340;&#32463;&#27982;&#23398;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27169;&#25311;&#29420;&#31435;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20811;&#21171;&#31119;&#24503;&#21644;&#32034;&#36125;&#23572;&#65288;1982&#65289;&#30340;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19968;&#20010;&#21457;&#36865;&#32773;&#21644;&#19968;&#20010;&#25509;&#25910;&#32773;&#19968;&#36215;&#36827;&#34892;&#35757;&#32451;&#65292;&#25910;&#25947;&#21040;&#25509;&#36817;&#28216;&#25103;&#20808;&#39564;&#26368;&#20248;&#22343;&#34913;&#30340;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#36890;&#20449;&#22312;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#31243;&#24230;&#32473;&#20986;&#30340;&#32435;&#20160;&#22343;&#34913;&#19979;&#65292;&#25353;&#29031;&#26368;&#22823;&#31243;&#24230;&#36827;&#34892;&#12290;&#36825;&#19968;&#32467;&#35770;&#23545;&#36229;&#21442;&#25968;&#21644;&#28216;&#25103;&#30340;&#22791;&#36873;&#35268;&#33539;&#31283;&#20581;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#31639;&#27861;&#38388;&#26032;&#20852;&#36890;&#20449;&#24037;&#20316;&#20197;&#21450;&#30001;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32452;&#25104;&#30340;&#24066;&#22330;&#20013;&#30340;&#23467;&#26007;&#32463;&#27982;&#23398;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We simulate behaviour of independent reinforcement learning algorithms playing the Crawford and Sobel (1982) game of strategic information transmission. We show that a sender and a receiver training together converge to strategies close to the exante optimal equilibrium of the game. Hence, communication takes place to the largest extent predicted by Nash equilibrium given the degree of conflict of interest between agents. The conclusion is shown to be robust to alternative specifications of the hyperparameters and of the game. We discuss implications for theories of equilibrium selection in information transmission games, for work on emerging communication among algorithms in computer science and for the economics of collusions in markets populated by artificially intelligent agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#23398;&#20064;&#30340;&#29289;&#29702;&#21407;&#29702;&#27169;&#22411;&#65292;&#19981;&#20165;&#33021;&#35782;&#21035;&#30456;&#20851;&#24615;&#65292;&#36824;&#33021;&#23637;&#29616;&#22240;&#26524;&#20851;&#31995;&#65292;&#20174;&#32780;&#20026;&#26426;&#22120;&#36741;&#21161;&#30340;&#31185;&#23398;&#21457;&#29616;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.04069</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22240;&#26524;&#27169;&#22411;&#25512;&#26029;&#29289;&#29702;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Inferring physical laws by artificial intelligence based causal models. (arXiv:2309.04069v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#23398;&#20064;&#30340;&#29289;&#29702;&#21407;&#29702;&#27169;&#22411;&#65292;&#19981;&#20165;&#33021;&#35782;&#21035;&#30456;&#20851;&#24615;&#65292;&#36824;&#33021;&#23637;&#29616;&#22240;&#26524;&#20851;&#31995;&#65292;&#20174;&#32780;&#20026;&#26426;&#22120;&#36741;&#21161;&#30340;&#31185;&#23398;&#21457;&#29616;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#36827;&#27493;&#20026;&#31185;&#23398;&#30740;&#31350;&#25171;&#24320;&#20102;&#35768;&#22810;&#26032;&#30340;&#36884;&#24452;&#65292;&#24182;&#20026;&#30693;&#35782;&#21019;&#36896;&#36807;&#31243;&#22686;&#28155;&#20102;&#26032;&#30340;&#32500;&#24230;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#26368;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#30340;ML&#24212;&#29992;&#20027;&#35201;&#22312;&#20851;&#32852;&#20998;&#26512;&#39046;&#22495;&#65292;&#24402;&#32467;&#20026;&#22797;&#26434;&#25968;&#25454;&#25311;&#21512;&#12290;Judea Pearl&#25351;&#20986;&#65292;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#24517;&#39035;&#28041;&#21450;&#21040;&#24178;&#39044;&#21644;&#24819;&#35937;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#20219;&#20309;&#26426;&#22120;&#36741;&#21161;&#30340;&#31185;&#23398;&#21457;&#29616;&#37117;&#24517;&#39035;&#21253;&#25324;&#22240;&#26524;&#20998;&#26512;&#21644;&#24178;&#39044;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#23398;&#20064;&#30340;&#29289;&#29702;&#21407;&#29702;&#27169;&#22411;&#65292;&#23427;&#19981;&#20165;&#33021;&#35782;&#21035;&#30456;&#20851;&#24615;&#65292;&#36824;&#33021;&#23637;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#24178;&#39044;&#21407;&#21017;&#26469;&#30740;&#31350;&#19968;&#20123;&#33879;&#21517;&#29289;&#29702;&#29616;&#35937;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#25216;&#26415;&#19981;&#20165;&#33021;&#25214;&#20986;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
The advances in Artificial Intelligence (AI) and Machine Learning (ML) have opened up many avenues for scientific research, and are adding new dimensions to the process of knowledge creation. However, even the most powerful and versatile of ML applications till date are primarily in the domain of analysis of associations and boil down to complex data fitting. Judea Pearl has pointed out that Artificial General Intelligence must involve interventions involving the acts of doing and imagining. Any machine assisted scientific discovery thus must include casual analysis and interventions. In this context, we propose a causal learning model of physical principles, which not only recognizes correlations but also brings out casual relationships. We use the principles of causal inference and interventions to study the cause-and-effect relationships in the context of some well-known physical phenomena. We show that this technique can not only figure out associations among data, but is also able
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#27169;&#25311;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#21464;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14991</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#24341;&#20837;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#65292;&#20197;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence. (arXiv:2308.14991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#27169;&#25311;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#21464;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#36171;&#20104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#24378;&#22823;&#36866;&#24212;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#19968;&#20010;&#29702;&#24819;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#22312;&#35760;&#24518;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21487;&#22609;&#24615;&#20043;&#38388;&#20445;&#25345;&#36866;&#24403;&#24179;&#34913;&#65292;&#24182;&#33719;&#24471;&#36275;&#22815;&#30340;&#20860;&#23481;&#24615;&#26469;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#20998;&#24067;&#12290;&#29616;&#26377;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#20445;&#25345;&#35760;&#24518;&#31283;&#23450;&#24615;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20294;&#20173;&#38590;&#20197;&#20687;&#29983;&#29289;&#26234;&#33021;&#65288;BI&#65289;&#37027;&#26679;&#28789;&#27963;&#22320;&#36866;&#24212;&#22686;&#37327;&#21464;&#21270;&#12290;&#36890;&#36807;&#24314;&#27169;&#19968;&#20010;&#33021;&#22815;&#20027;&#21160;&#35843;&#33410;&#36951;&#24536;&#30340;&#31283;&#20581;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#23398;&#20064;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#20998;&#24067;&#20013;&#36866;&#24403;&#34928;&#20943;&#26087;&#35760;&#24518;&#26469;&#25913;&#21892;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#21327;&#35843;&#22810;&#23398;&#20064;&#32773;&#26550;&#26500;&#26469;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#26174;&#25552;&#39640;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#31361;&#35302;&#35843;&#33410;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning aims to empower artificial intelligence (AI) with strong adaptability to the real world. For this purpose, a desirable solution should properly balance memory stability with learning plasticity, and acquire sufficient compatibility to capture the observed distributions. Existing advances mainly focus on preserving memory stability to overcome catastrophic forgetting, but remain difficult to flexibly accommodate incremental changes as biological intelligence (BI) does. By modeling a robust Drosophila learning system that actively regulates forgetting with multiple learning modules, here we propose a generic approach that appropriately attenuates old memories in parameter distributions to improve learning plasticity, and accordingly coordinates a multi-learner architecture to ensure solution compatibility. Through extensive theoretical and empirical validation, our approach not only clearly enhances the performance of continual learning, especially over synaptic regula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#20013;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#25216;&#26415;&#21152;&#36895;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#65292;&#38024;&#23545;&#20449;&#22122;&#27604;&#36866;&#20013;&#33267;&#36739;&#39640;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#19981;&#25439;&#22833;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11100</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#20013;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Using Early Exits for Fast Inference in Automatic Modulation Classification. (arXiv:2308.11100v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#20013;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#25216;&#26415;&#21152;&#36895;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#65292;&#38024;&#23545;&#20449;&#22122;&#27604;&#36866;&#20013;&#33267;&#36739;&#39640;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#19981;&#25439;&#22833;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#33258;&#20027;&#22320;&#23545;&#26080;&#32447;&#39057;&#35889;&#19978;&#20256;&#36755;&#30340;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#12290;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#25552;&#21462;&#22797;&#26434;&#30340;&#26080;&#32447;&#20449;&#21495;&#29305;&#24449;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#22810;&#22320;&#23558;&#20854;&#29992;&#20110;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35745;&#31639;&#23494;&#38598;&#19988;&#25512;&#29702;&#24310;&#36831;&#36739;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#29992;&#20110;&#33258;&#21160;&#35843;&#21046;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#24212;&#29992;&#26089;&#26399;&#36864;&#20986;&#25216;&#26415;&#20197;&#21152;&#36895;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#22235;&#31181;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#21644;&#38024;&#23545;&#35813;&#38382;&#39064;&#30340;&#23450;&#21046;&#22810;&#20998;&#25903;&#35757;&#32451;&#31639;&#27861;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#36866;&#20013;&#33267;&#36739;&#39640;&#30340;&#20449;&#21495;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20998;&#31867;&#65292;&#19981;&#38656;&#35201;&#28145;&#24230;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#36864;&#20986;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic modulation classification (AMC) plays a critical role in wireless communications by autonomously classifying signals transmitted over the radio spectrum. Deep learning (DL) techniques are increasingly being used for AMC due to their ability to extract complex wireless signal features. However, DL models are computationally intensive and incur high inference latencies. This paper proposes the application of early exiting (EE) techniques for DL models used for AMC to accelerate inference. We present and analyze four early exiting architectures and a customized multi-branch training algorithm for this problem. Through extensive experimentation, we show that signals with moderate to high signal-to-noise ratios (SNRs) are easier to classify, do not require deep architectures, and can therefore leverage the proposed EE architectures. Our experimental results demonstrate that EE techniques can significantly reduce the inference speed of deep neural networks without sacrificing class
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#20102;&#20004;&#31181;&#34920;&#22411;&#35782;&#21035;&#27169;&#22411;PhenoBCBERT&#21644;PhenoGPT&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#35268;&#21017;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20020;&#24202;&#34920;&#22411;&#26415;&#35821;&#65292;&#21253;&#25324;HPO&#26410;&#35760;&#24405;&#30340;&#26032;&#26415;&#35821;&#12290;</title><link>http://arxiv.org/abs/2308.06294</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20020;&#24202;&#31508;&#35760;&#20013;&#34920;&#22411;&#35782;&#21035;&#33021;&#21147;&#65306;PhenoBCBERT&#21644;PhenoGPT
&lt;/p&gt;
&lt;p&gt;
Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT. (arXiv:2308.06294v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#20102;&#20004;&#31181;&#34920;&#22411;&#35782;&#21035;&#27169;&#22411;PhenoBCBERT&#21644;PhenoGPT&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#35268;&#21017;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20020;&#24202;&#34920;&#22411;&#26415;&#35821;&#65292;&#21253;&#25324;HPO&#26410;&#35760;&#24405;&#30340;&#26032;&#26415;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20551;&#35774;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#23454;&#29616;&#23545;&#20020;&#24202;&#34920;&#22411;&#26415;&#35821;&#30340;&#33258;&#21160;&#26816;&#27979;&#65292;&#21253;&#25324;&#26410;&#22312;HPO&#20013;&#35760;&#24405;&#30340;&#26415;&#35821;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#27169;&#22411;&#65306;PhenoBCBERT&#65292;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;Bio+Clinical BERT&#20316;&#20026;&#20854;&#39044;&#35757;&#32451;&#27169;&#22411;&#65307;&#20197;&#21450;PhenoGPT&#65292;&#19968;&#31181;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#19981;&#21516;&#30340;GPT&#27169;&#22411;&#65288;&#21253;&#25324;&#24320;&#28304;&#29256;&#26412;&#22914;GPT-J&#12289;Falcon&#21644;LLaMA&#65292;&#20197;&#21450;&#20851;&#38381;&#28304;&#29256;&#26412;&#22914;GPT-3&#21644;GPT-3.5&#65289;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#36817;&#24320;&#21457;&#30340;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;HPO&#35782;&#21035;&#24037;&#20855;PhenoTagger&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#26356;&#22810;&#30340;&#34920;&#22411;&#27010;&#24565;&#65292;&#21253;&#25324;HPO&#26410;&#25551;&#36848;&#30340;&#26032;&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35828;&#26126;&#22914;&#20309;&#35782;&#21035;&#21644;&#25552;&#21462;&#26032;&#30340;&#34920;&#22411;&#20449;&#24687;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#24403;&#21069;&#22522;&#20110;BERT&#21644;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#34920;&#22411;&#26631;&#35760;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We hypothesize that large language models (LLMs) based on the transformer architecture can enable automated detection of clinical phenotype terms, including terms not documented in the HPO. In this study, we developed two types of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERT as its pre-trained model, and PhenoGPT, a GPT-based model that can be initialized from diverse GPT models, including open-source versions such as GPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 and GPT-3.5. We compared our methods with PhenoTagger, a recently developed HPO recognition tool that combines rule-based and deep learning methods. We found that our methods can extract more phenotype concepts, including novel ones not characterized by HPO. We also performed case studies on biomedical literature to illustrate how new phenotype information can be recognized and extracted. We compared current BERT-based versus GPT-based models for phenotype tagging, in multipl
&lt;/p&gt;</description></item><item><title>FDAPT&#26159;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#12290;&#23545;&#20110;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;FDAPT&#33021;&#22815;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#25552;&#20986;&#30340;FFDAPT&#31639;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#23637;&#29616;&#20986;&#19982;&#26631;&#20934;FDAPT&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20063;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.06933</link><description>&lt;p&gt;
FDAPT: &#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FDAPT: Federated Domain-adaptive Pre-training for Language Models. (arXiv:2307.06933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06933
&lt;/p&gt;
&lt;p&gt;
FDAPT&#26159;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#12290;&#23545;&#20110;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;FDAPT&#33021;&#22815;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#25552;&#20986;&#30340;FFDAPT&#31639;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#23637;&#29616;&#20986;&#19982;&#26631;&#20934;FDAPT&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20063;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;DAPT&#65289;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26356;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;FDAPT&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FDAPT&#22312;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#37117;&#33021;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20923;&#32467;&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;FFDAPT&#65289;&#12290;FFDAPT&#24179;&#22343;&#25552;&#39640;&#20102;12.1%&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;FDAPT&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#19968;&#33324;&#24615;&#33021;&#27874;&#21160;&#20445;&#25345;&#22312;1%&#20197;&#19979;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#24037;&#20316;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Domain-adaptive Pre-training (DAPT) with Federated Learning (FL) can enhance model adaptation by leveraging more sensitive and distributed data while preserving data privacy. However, few studies have focused on this method. Therefore, we conduct the first comprehensive empirical study to evaluate the performance of Federated Domain-adaptive Pre-training (FDAPT). We demonstrate that FDAPT can maintain competitive downstream task performance to the centralized baseline in both IID and non-IID situations. Furthermore, we propose a novel algorithm, Frozen Federated Domain-adaptive Pre-training (FFDAPT). FFDAPT improves the computational efficiency by 12.1% on average and exhibits similar downstream task performance to standard FDAPT, with general performance fluctuations remaining less than 1%. Finally, through a critical evaluation of our work, we identify promising future research directions for this new research area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#25480;&#27880;&#24847;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#26469;&#28040;&#38500;&#31163;&#32676;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;transformer&#30340;&#21487;&#37327;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12929</link><description>&lt;p&gt;
&#21487;&#37327;&#21270;Transformer&#65306;&#36890;&#36807;&#24110;&#21161;&#27880;&#24847;&#21147;&#22836;&#8220;&#20160;&#20040;&#20063;&#19981;&#20570;&#8221;&#21435;&#38500;&#31163;&#32676;&#20540;
&lt;/p&gt;
&lt;p&gt;
Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#25480;&#27880;&#24847;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#26469;&#28040;&#38500;&#31163;&#32676;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;transformer&#30340;&#21487;&#37327;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;Transformer&#27169;&#22411;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26174;&#33879;&#25512;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#30001;&#20110;&#20854;&#35268;&#27169;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#65292;&#20294;&#36825;&#26159;&#20197;&#26497;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#20026;&#20195;&#20215;&#30340;&#12290;&#37327;&#21270;&#26159;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#26102;&#38388;&#21644;&#23384;&#20648;&#22120;&#28040;&#32791;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;transformer&#27169;&#22411;&#24448;&#24448;&#23398;&#20064;&#21040;&#20854;&#28608;&#27963;&#20013;&#30340;&#24378;&#31163;&#32676;&#20540;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#37327;&#21270;&#12290;&#20026;&#20445;&#25345;&#21487;&#25509;&#21463;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#31163;&#32676;&#20540;&#30340;&#23384;&#22312;&#38656;&#35201;&#23558;&#28608;&#27963;&#32622;&#20110;&#26356;&#39640;&#30340;&#27604;&#29305;&#23485;&#24230;&#25110;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#23383;&#26684;&#24335;&#65292;&#36827;&#34892;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#20854;&#20182;&#21464;&#36890;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24378;&#31163;&#32676;&#20540;&#19982;&#29305;&#23450;&#27880;&#24847;&#22836;&#34892;&#20026;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#20123;&#22836;&#35797;&#22270;&#23398;&#20064;&#8220;&#26080;&#25805;&#20316;&#8221;&#25110;&#20165;&#20165;&#26159;&#37096;&#20998;&#27531;&#24046;&#26356;&#26032;&#12290;&#20026;&#20102;&#23454;&#29616;&#27880;&#24847;&#21147;&#22836;&#20013;&#38656;&#35201;&#30340;&#31934;&#30830;&#38646;&#20301;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#25945;&#25480;&#27880;&#24847;&#21147;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#30340;&#37327;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#29992;&#20302;&#31934;&#24230;&#37327;&#21270;&#29978;&#33267;&#26159;&#24378;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a "no-op" or just a partial update of the residual. To achieve the exact zeros needed in the attention 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.11167</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35823;&#23548;&#65306;&#20351;&#29992;Only Connect Wall&#25968;&#25454;&#38598;&#25506;&#32034;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#21644;Einstellung&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20154;&#24037;&#26234;&#33021;&#35806;&#29983;&#20197;&#26469;&#65292;&#23545;&#20154;&#31867;&#20223;&#30495;&#26234;&#33021;&#30340;&#36861;&#27714;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25345;&#20037;&#35805;&#39064;&#12290;&#26368;&#26032;&#19968;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#28436;&#36827;&#21644;&#26032;&#20852;&#33021;&#21147;&#23558;&#36825;&#20010;&#20027;&#39064;&#20174;&#23398;&#26415;&#30028;&#24102;&#21040;&#20102;&#25991;&#21270;&#26102;&#20195;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;NLP&#35780;&#20272;&#22522;&#20934;&#20219;&#21153;&#27979;&#35797;&#20102;&#20154;&#31867;&#20223;&#30495;&#34892;&#20026;&#30340;&#19968;&#20123;&#26041;&#38754;&#65288;&#20363;&#22914;BIG-bench&#30340;&#8220;&#31867;&#20154;&#34892;&#20026;&#8221;&#20219;&#21153;&#65289;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#19968;&#20010;&#20219;&#21153;&#32771;&#23519;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20154;&#31867;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#26159;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#30740;&#31350;&#36739;&#20026;&#28145;&#20837;&#30340;&#20027;&#39064;&#65292;&#26631;&#20934;&#21270;&#27979;&#35797;&#20027;&#35201;&#20351;&#29992;&#23558;&#32447;&#32034;&#35789;&#20043;&#38388;&#30340;&#65288;&#24322;&#26500;&#65289;&#36830;&#25509;&#33021;&#21147;&#20316;&#20026;&#21019;&#36896;&#24615;&#30340;&#24230;&#37327;&#12290;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#65292;&#26263;&#31034;&#24615;&#30340;&#35823;&#23548;&#24615;&#21050;&#28608;-&#34987;&#31216;&#20026;&#8220;&#35825;&#23548;&#35823;&#35299;&#8221;&#30340;&#24178;&#25200;&#22240;&#32032;-&#36890;&#36807;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#38459;&#30861;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20107;&#20808;&#35753;&#21442;&#19982;&#32773;&#25509;&#35302;&#21040;&#26377;&#30456;&#20284;&#25340;&#20889;&#30340;&#38169;&#35823;&#22240;&#32032;&#26469;&#23454;&#39564;&#24615;&#22320;&#35825;&#23548;&#36825;&#26679;&#30340;&#22266;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;(IFM)&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#20197;&#23454;&#29616;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#12290;</title><link>http://arxiv.org/abs/2306.04027</link><description>&lt;p&gt;
&#22240;&#23376;&#22270;&#27169;&#22411;&#35270;&#35282;&#19979;&#30340;&#24178;&#39044;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Intervention Generalization: A View from Factor Graph Models. (arXiv:2306.04027v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;(IFM)&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#20197;&#23454;&#29616;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21644;&#35266;&#23519;&#25968;&#25454;&#25512;&#24191;&#21040;&#26032;&#30340;&#26465;&#20214;&#12290;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25552;&#20379;&#36275;&#22815;&#22810;&#30340;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35770;&#19978;&#21487;&#33021;&#26368;&#32456;&#23398;&#20064;&#20174;&#26032;&#30340;&#23454;&#39564;&#26465;&#20214;&#21040;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30340;&#26144;&#23556;&#65292;&#20294;&#26159;&#22788;&#29702;&#22823;&#37327;&#21487;&#33021;&#30340;&#24178;&#39044;&#32452;&#21512;&#31354;&#38388;&#24456;&#22256;&#38590;&#12290;&#22312;&#20856;&#22411;&#30340;&#31232;&#30095;&#23454;&#39564;&#35774;&#35745;&#19979;&#65292;&#22914;&#26524;&#19981;&#20381;&#36182;&#20110;&#37325;&#30340;&#35268;&#21017;&#21270;&#25110;&#20808;&#39564;&#20998;&#24067;&#65292;&#36825;&#31181;&#26144;&#23556;&#26159;&#19981;&#36866;&#24403;&#30340;&#12290;&#36825;&#26679;&#30340;&#20551;&#35774;&#21487;&#33021;&#26159;&#21487;&#38752;&#30340;&#65292;&#20063;&#21487;&#33021;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#24456;&#38590;&#36777;&#25252;&#25110;&#27979;&#35797;&#12290;&#26412;&#25991;&#20174;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#35821;&#35328;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#22914;&#20309;&#20445;&#35777;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#12290;&#20551;&#35774;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#26159;&#23427;&#24456;&#26041;&#20415;&#22320;&#22788;&#29702;&#20102;&#22823;&#37327;&#21487;&#33021;&#30340;&#24178;&#39044;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we take a close look at how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated $\textit{interventional factor model}$ (IFM) may not always be informative, but it conveniently abs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01665</link><description>&lt;p&gt;
SourceP&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#26234;&#33021;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;
&lt;/p&gt;
&lt;p&gt;
SourceP: Smart Ponzi Schemes Detection on Ethereum Using Pre-training Model with Data Flow. (arXiv:2306.01665v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21306;&#22359;&#38142;&#25216;&#26415;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20856;&#22411;&#30340;&#37329;&#34701;&#39575;&#23616;&#24222;&#20857;&#39575;&#23616;&#20063;&#22312;&#21306;&#22359;&#38142;&#24179;&#21488;&#20197;&#22826;&#22346;&#19978;&#20986;&#29616;&#12290;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#37096;&#32626;&#30340;&#36825;&#31181;&#24222;&#20857;&#39575;&#23616;&#65292;&#20063;&#31216;&#20026;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#65292;&#24050;&#32463;&#36896;&#25104;&#20102;&#22823;&#37327;&#30340;&#32463;&#27982;&#25439;&#22833;&#21644;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20197;&#22826;&#22346;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#23383;&#33410;&#30721;&#29305;&#24449;&#12289;&#25805;&#20316;&#30721;&#29305;&#24449;&#12289;&#36134;&#25143;&#29305;&#24449;&#21644;&#20132;&#26131;&#34892;&#20026;&#29305;&#24449;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SourceP&#65292;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#22312;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#20316;&#20026;&#29305;&#24449;&#65292;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#25506;&#32034;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#21487;&#33021;&#24615;&#12290;SourceP&#38477;&#20302;&#20102;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#30340;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#38590;&#24230;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As blockchain technology becomes more and more popular, a typical financial scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum. This Ponzi scheme deployed through smart contracts, also known as the smart Ponzi scheme, has caused a lot of economic losses and negative impacts. Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on bytecode features, opcode features, account features, and transaction behavior features of smart contracts, and such methods lack interpretability and sustainability. In this paper, we propose SourceP, a method to detect smart Ponzi schemes on the Ethereum platform using pre-training models and data flow, which only requires using the source code of smart contracts as features to explore the possibility of detecting smart Ponzi schemes from another direction. SourceP reduces the difficulty of data acquisition and feature extraction of existing detection methods while increasing the interpretability of the model. 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26550;&#26500;&#20462;&#25913;&#21644;&#26032;&#39062;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.15001</link><description>&lt;p&gt;
&#22797;&#25968;&#20540;&#33258;&#32534;&#30721;&#22120;&#23545;&#29289;&#20307;&#21457;&#29616;&#30340;&#23545;&#27604;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Contrastive Training of Complex-Valued Autoencoders for Object Discovery. (arXiv:2305.15001v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15001
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26550;&#26500;&#20462;&#25913;&#21644;&#26032;&#39062;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#20013;&#24515;&#27169;&#22411;&#20351;&#29992;&#25554;&#27133;&#21644;&#27880;&#24847;&#21147;&#36335;&#30001;&#36827;&#34892;&#32465;&#23450;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#26377;&#20960;&#20010;&#27010;&#24565;&#24615;&#30340;&#23616;&#38480;&#24615;&#65306;&#25554;&#27133;&#30340;&#25968;&#37327;&#26159;&#30828;&#32534;&#30721;&#30340;&#65307;&#25152;&#26377;&#25554;&#27133;&#30340;&#23481;&#37327;&#30456;&#31561;&#65307;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65307;&#25554;&#27133;&#20869;&#27809;&#26377;&#30446;&#26631;&#32423;&#21035;&#30340;&#20851;&#31995;&#22240;&#32032;&#12290;&#21407;&#21017;&#19978;&#65292;&#22522;&#20110;&#21516;&#27493;&#24615;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22797;&#25968;&#20540;&#28608;&#27963;&#22312;&#20854;&#30456;&#20301;&#20998;&#37327;&#20013;&#23384;&#20648;&#32465;&#23450;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#22522;&#20110;&#21516;&#27493;&#24615;&#30340;&#27169;&#22411;&#30340;&#24037;&#20316;&#31034;&#20363;&#21482;&#26159;&#26368;&#36817;&#25165;&#26377;&#65292;&#32780;&#19988;&#23454;&#38469;&#19978;&#20173;&#28982;&#38480;&#20110;&#29609;&#20855;&#28784;&#24230;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#23384;&#20648;&#19981;&#21040;&#19977;&#20010;&#29289;&#20307;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26550;&#26500;&#20462;&#25913;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26497;&#22823;&#22320;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#21457;&#29616;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art object-centric models use slots and attention-based routing for binding. However, this class of models has several conceptual limitations: the number of slots is hardwired; all slots have equal capacity; training has high computational cost; there are no object-level relational factors within slots. Synchrony-based models in principle can address these limitations by using complex-valued activations which store binding information in their phase components. However, working examples of such synchrony-based models have been developed only very recently, and are still limited to toy grayscale datasets and simultaneous storage of less than three objects in practice. Here we introduce architectural modifications and a novel contrastive learning method that greatly improve the state-of-the-art synchrony-based model. For the first time, we obtain a class of synchrony-based models capable of discovering objects in an unsupervised manner in multi-object color datasets 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14979</link><description>&lt;p&gt;
&#23610;&#24230;&#24456;&#37325;&#35201;&#65306;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#35299;&#37322;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scale Matters: Attribution Meets the Wavelet Domain to Explain Model Sensitivity to Image Corruptions. (arXiv:2305.14979v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#30001;&#20110;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#23646;&#24615;&#26041;&#27861;&#23545;&#20110;&#35299;&#37322;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#24378;&#20581;&#24615;&#39046;&#22495;&#30340;&#25991;&#29486;&#20165;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#25439;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#23457;&#26597;&#27169;&#22411;&#30340;&#34892;&#20026;&#33021;&#21147;&#23545;&#20110;&#25552;&#39640;&#29992;&#25143;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Wavelet sCale Attribution Method (WCAM)&#65292;&#23427;&#26159;&#20174;&#20687;&#32032;&#22495;&#21040;&#31354;&#38388;&#23610;&#24230;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#27010;&#25324;&#12290;&#22312;&#31354;&#38388;&#23610;&#24230;&#22495;&#20013;&#36827;&#34892;&#23646;&#24615;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#20851;&#27880;&#28857;&#21644;&#23610;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;WCAM&#35299;&#37322;&#20102;&#27169;&#22411;&#22312;&#22270;&#20687;&#30772;&#22351;&#19979;&#30340;&#22833;&#25928;&#65292;&#30830;&#23450;&#20102;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#36890;&#36807;&#32553;&#25918;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown remarkable performance in computer vision, but their deployment in real-world scenarios is challenging due to their sensitivity to image corruptions. Existing attribution methods are uninformative for explaining the sensitivity to image corruptions, while the literature on robustness only provides model-based explanations. However, the ability to scrutinize models' behavior under image corruptions is crucial to increase the user's trust. Towards this end, we introduce the Wavelet sCale Attribution Method (WCAM), a generalization of attribution from the pixel domain to the space-scale domain. Attribution in the space-scale domain reveals where and on what scales the model focuses. We show that the WCAM explains models' failures under image corruptions, identifies sufficient information for prediction, and explains how zoom-in increases accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#25351;&#30340;&#26159;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#39044;&#27979;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2305.10564</link><description>&lt;p&gt;
&#23545;&#25918;&#24323;&#20998;&#31867;&#22120;&#36827;&#34892;&#21453;&#20107;&#23454;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Counterfactually Comparing Abstaining Classifiers. (arXiv:2305.10564v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#25351;&#30340;&#26159;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#39044;&#27979;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#24323;&#20998;&#31867;&#22120;&#21487;&#20197;&#36873;&#25321;&#22312;&#19981;&#30830;&#23450;&#26102;&#25918;&#24323;&#23545;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#38382;&#39064;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20445;&#30041;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#40657;&#30418;&#25918;&#24323;&#20998;&#31867;&#22120;&#26102;&#65292;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#32771;&#34385;&#20998;&#31867;&#22120;&#22312;&#23427;&#30340;&#25918;&#24323;&#39044;&#27979;&#19978;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#24403;&#25918;&#23556;&#31185;&#21307;&#29983;&#19981;&#30830;&#23450;&#20854;&#35786;&#26029;&#25110;&#24403;&#39550;&#39542;&#21592;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#19981;&#27880;&#24847;&#26102;&#65292;&#36825;&#20123;&#32570;&#22833;&#30340;&#39044;&#27979;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#22260;&#32469;&#30528;&#23450;&#20041;&#19968;&#20010;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#21363;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#30340;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#25351;&#23450;&#20102;&#26465;&#20214;... (&#27492;&#22788;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
Abstaining classifiers have the option to abstain from making predictions on inputs that they are unsure about. These classifiers are becoming increasingly popular in high-stake decision-making problems, as they can withhold uncertain predictions to improve their reliability and safety. When evaluating black-box abstaining classifier(s), however, we lack a principled approach that accounts for what the classifier would have predicted on its abstentions. These missing predictions are crucial when, e.g., a radiologist is unsure of their diagnosis or when a driver is inattentive in a self-driving car. In this paper, we introduce a novel approach and perspective to the problem of evaluating and comparing abstaining classifiers by treating abstentions as missing data. Our evaluation approach is centered around defining the counterfactual score of an abstaining classifier, defined as the expected performance of the classifier had it not been allowed to abstain. We specify the conditions unde
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#24320;&#21457;&#30340;Segment Anything Model&#65288;SAM&#65289;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#26377;&#25928;&#20998;&#21106;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#23545;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#39033;&#30740;&#31350;&#25506;&#35752;SAM&#22312;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#39033;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.13000</link><description>&lt;p&gt;
&#20174;&#31354;&#38388;&#20013;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Segment anything, from space?. (arXiv:2304.13000v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13000
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;Segment Anything Model&#65288;SAM&#65289;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#26377;&#25928;&#20998;&#21106;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#23545;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#39033;&#30740;&#31350;&#25506;&#35752;SAM&#22312;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#39033;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20026;&#35270;&#35273;&#20219;&#21153;&#19987;&#38376;&#24320;&#21457;&#30340;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#34987;&#31216;&#20026;&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#12290;SAM&#21487;&#20197;&#26681;&#25454;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#20998;&#21106;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#12290;&#20316;&#32773;&#20204;&#22312;&#22823;&#37327;&#30340;&#35270;&#35273;&#22522;&#20934;&#20219;&#21153;&#19978;&#30740;&#31350;&#20102;SAM&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;SAM&#36890;&#24120;&#36798;&#21040;&#20102;&#19982;&#30446;&#26631;&#20219;&#21153;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#30456;&#20284;&#25110;&#26377;&#26102;&#29978;&#33267;&#36229;&#36234;&#20854;&#35782;&#21035;&#31934;&#24230;&#12290;SAM&#22312;&#20998;&#21106;&#26041;&#38754;&#30340;&#21331;&#36234;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#20174;&#20107;&#33258;&#28982;&#22270;&#20687;&#30740;&#31350;&#30340;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SAM&#30340;&#21331;&#36234;&#24615;&#33021;&#26159;&#21542;&#25193;&#23637;&#21040;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#65292;&#24182;&#24110;&#21161;&#25351;&#23548;&#31038;&#21306;&#23545;&#20854;&#21457;&#23637;&#30340;&#22238;&#24212;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#22810;&#26679;&#21270;&#21644;&#24191;&#27867;&#30740;&#31350;&#36807;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#30740;&#31350;SAM&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SAM&#36890;&#24120;&#22312;&#31354;&#20013;&#22270;&#20687;&#19978;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#34920;&#29616;&#65292;&#23613;&#31649;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20250;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the first foundation model developed specifically for vision tasks was developed, termed the "Segment Anything Model" (SAM). SAM can segment objects in input imagery based upon cheap input prompts, such as one (or more) points, a bounding box, or a mask. The authors examined the zero-shot image segmentation accuracy of SAM on a large number of vision benchmark tasks and found that SAM usually achieved recognition accuracy similar to, or sometimes exceeding, vision models that had been trained on the target tasks. The impressive generalization of SAM for segmentation has major implications for vision researchers working on natural imagery. In this work, we examine whether SAM's impressive performance extends to overhead imagery problems, and help guide the community's response to its development. We examine SAM's performance on a set of diverse and widely-studied benchmark tasks. We find that SAM does often generalize well to overhead imagery, although it fails in some cases d
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.07880</link><description>&lt;p&gt;
Sabi&#225;: &#33889;&#33796;&#29273;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sabi\'a: Portuguese Large Language Models. (arXiv:2304.07880v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07880
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#26029;&#25552;&#39640;&#65292;&#8221;&#19968;&#20992;&#20999;&#8220;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#20027;&#27969;&#12290;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#20840;&#29699;&#20351;&#29992;&#30340;&#35821;&#35328;&#25968;&#37327;&#38750;&#24120;&#24222;&#22823;&#65292;&#24182;&#19988;&#20854;&#20013;&#24456;&#22810;&#35821;&#35328;&#37117;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#20027;&#35201;&#30340;&#20570;&#27861;&#26159;&#23545;&#22810;&#31181;&#35821;&#35328;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#20570;&#27861;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#35777;&#26126;&#20102;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#29992;3%&#25110;&#26356;&#23569;&#30340;&#21407;&#22987;&#39044;&#35757;&#32451;&#39044;&#31639;&#22312;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#19978;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;GPT-J&#21644;LLaMA&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Poeta&#65288;&#19968;&#22871;&#30001;14&#20010;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#22871;&#20214;&#65289;&#19978;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34920;&#29616;&#19978;&#36828;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;Sabi&#225;-65B&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#24050;&#32463;&#35774;&#24819;&#20102;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#32463;&#36807;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the capabilities of language models continue to advance, it is conceivable that "one-size-fits-all" model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA models on Portuguese texts using 3% or less of their original pretraining budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin. Our best model, Sabi\'a-65B, performs on par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the target language as well as transl
&lt;/p&gt;</description></item><item><title>PLEX&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#21644;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#65292;&#22312;&#23398;&#20064;&#36890;&#29992;&#30340;&#25805;&#32437;&#20363;&#31243;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35270;&#39057;&#28436;&#31034;&#23398;&#20064;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.08789</link><description>&lt;p&gt;
PLEX&#65306;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#30340;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining. (arXiv:2303.08789v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08789
&lt;/p&gt;
&lt;p&gt;
PLEX&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#21644;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#65292;&#22312;&#23398;&#20064;&#36890;&#29992;&#30340;&#25805;&#32437;&#20363;&#31243;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35270;&#39057;&#28436;&#31034;&#23398;&#20064;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20016;&#23500;&#30340;&#34920;&#24449;&#26159;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#20851;&#38190;&#65292;&#20294;&#29616;&#26377;&#30340;&#27169;&#22411;&#26550;&#26500;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#26469;&#23398;&#20064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29702;&#24819;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#21508;&#31181;&#24050;&#27880;&#37322;&#20219;&#21153;&#30340;&#19987;&#23478;&#35270;&#35273;-&#21160;&#20316;&#28436;&#31034;&#65292;&#26159;&#31232;&#32570;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;PLEX&#65292;&#23427;&#26159;&#20174;&#20219;&#21153;&#19981;&#21487;&#30693;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#20013;&#23398;&#20064;&#30340;&#65292;&#20276;&#38543;&#30528;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#8212;&#8212;&#36825;&#26159;&#19968;&#31181;&#25968;&#37327;&#21487;&#35266;&#30340;&#19982;&#26426;&#22120;&#20154;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;PLEX&#32972;&#21518;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#35266;&#23519;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36712;&#36857;&#19979;&#65292;&#26377;&#21161;&#20110;&#35825;&#23548;&#28508;&#22312;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#35757;&#32451;&#26426;&#22120;&#20154;&#25191;&#34892;&#19982;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#25805;&#20316;&#20363;&#31243;&#65292;&#32780;&#22810;&#26679;&#21270;&#30340;&#20165;&#20026;&#35270;&#39057;&#28436;&#31034;&#20165;&#21487;&#20197;&#26377;&#25928;&#22320;&#25945;&#20250;&#26426;&#22120;&#20154;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;&#19982;&#22823;&#22810;&#25968;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#22521;&#35757;&#20316;&#21697;&#19981;&#21516;&#65292;PLEX&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#30340;&#24863;&#35273;&#36816;&#21160;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal robotic manipulation training data, which comes in the form of expert visuomotor demonstrations for a variety of annotated tasks, is scarce. In this work we propose PLEX, a transformer-based architecture that learns from task-agnostic visuomotor trajectories accompanied by a much larger amount of task-conditioned object manipulation videos -- a type of robotics-relevant data available in quantity. The key insight behind PLEX is that the trajectories with observations and actions help induce a latent feature space and train a robot to execute task-agnostic manipulation routines, while a diverse set of video-only demonstrations can efficiently teach the robot how to plan in this feature space for a wide variety of tasks. In contrast to most works on robotic manipulation pretraining, PLEX learns a generalizable sensorimotor multi-task polic
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#32593;&#26684;&#26159;&#19968;&#31181;&#20419;&#36827;&#25968;&#25454;&#27665;&#20027;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#27010;&#24565;&#65292;&#20854;&#21160;&#26426;&#22240;&#32032;&#21253;&#25324;&#21162;&#21147;&#25104;&#20026;&#26356;&#20855;&#25968;&#25454;&#39537;&#21160;&#24615;&#65292;&#25361;&#25112;&#21253;&#25324;&#21521;&#32852;&#37030;&#27835;&#29702;&#30340;&#36716;&#21464;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#26368;&#20339;&#23454;&#36341;&#30340;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#20854;&#28508;&#22312;&#30340;&#19994;&#21153;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.01713</link><description>&lt;p&gt;
&#25968;&#25454;&#32593;&#26684;&#65306;&#21160;&#26426;&#22240;&#32032;&#12289;&#25361;&#25112;&#21644;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Data Mesh: Motivational Factors, Challenges, and Best Practices. (arXiv:2302.01713v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01713
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32593;&#26684;&#26159;&#19968;&#31181;&#20419;&#36827;&#25968;&#25454;&#27665;&#20027;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#27010;&#24565;&#65292;&#20854;&#21160;&#26426;&#22240;&#32032;&#21253;&#25324;&#21162;&#21147;&#25104;&#20026;&#26356;&#20855;&#25968;&#25454;&#39537;&#21160;&#24615;&#65292;&#25361;&#25112;&#21253;&#25324;&#21521;&#32852;&#37030;&#27835;&#29702;&#30340;&#36716;&#21464;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#26368;&#20339;&#23454;&#36341;&#30340;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#20854;&#28508;&#22312;&#30340;&#19994;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26085;&#30410;&#37325;&#35201;&#65292;&#32452;&#32455;&#21162;&#21147;&#25104;&#20026;&#26356;&#20855;&#25968;&#25454;&#39537;&#21160;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25968;&#25454;&#26550;&#26500;&#24182;&#19981;&#19968;&#23450;&#35774;&#35745;&#29992;&#20110;&#24212;&#23545;&#25968;&#25454;&#21644;&#20998;&#26512;&#29992;&#20363;&#30340;&#35268;&#27169;&#21644;&#33539;&#22260;&#12290;&#20107;&#23454;&#19978;&#65292;&#29616;&#26377;&#26550;&#26500;&#24120;&#24120;&#26080;&#27861;&#23454;&#29616;&#23427;&#20204;&#25152;&#25215;&#35834;&#30340;&#20215;&#20540;&#12290;&#25968;&#25454;&#32593;&#26684;&#26159;&#19968;&#20010;&#31038;&#20250;&#25216;&#26415;&#27010;&#24565;&#65292;&#20854;&#20013;&#21253;&#21547;&#26550;&#26500;&#26041;&#38754;&#30340;&#20869;&#23481;&#65292;&#20197;&#20419;&#36827;&#25968;&#25454;&#27665;&#20027;&#21270;&#65292;&#24182;&#20351;&#32452;&#32455;&#30495;&#27491;&#25104;&#20026;&#25968;&#25454;&#39537;&#21160;&#22411;&#12290;&#30001;&#20110;&#25968;&#25454;&#32593;&#26684;&#30340;&#27010;&#24565;&#20173;&#28982;&#26159;&#26032;&#39062;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#26469;&#33258;&#23454;&#22320;&#30340;&#32463;&#39564;&#35777;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32570;&#23569;&#20102;&#35299;&#24341;&#20837;&#25968;&#25454;&#32593;&#26684;&#30340;&#21160;&#26426;&#22240;&#32032;&#12289;&#30456;&#20851;&#25361;&#25112;&#12289;&#26368;&#20339;&#23454;&#36341;&#12289;&#20854;&#19994;&#21153;&#24433;&#21709;&#21644;&#28508;&#22312;&#21407;&#22411;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;15&#20301;&#34892;&#19994;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34892;&#19994;&#19987;&#23478;&#22312;&#21521;&#32852;&#37030;&#27835;&#29702;&#30340;&#36716;&#21464;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing importance of data and artificial intelligence, organizations strive to become more data-driven. However, current data architectures are not necessarily designed to keep up with the scale and scope of data and analytics use cases. In fact, existing architectures often fail to deliver the promised value associated with them. Data mesh is a socio-technical concept that includes architectural aspects to promote data democratization and enables organizations to become truly data-driven. As the concept of data mesh is still novel, it lacks empirical insights from the field. Specifically, an understanding of the motivational factors for introducing data mesh, the associated challenges, best practices, its business impact, and potential archetypes, is missing. To address this gap, we conduct 15 semi-structured interviews with industry experts. Our results show, among other insights, that industry experts have difficulties with the transition toward federated governance ass
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;GAN&#20351;&#29992;CNN&#22788;&#29702;&#22270;&#20687;&#26102;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25104;&#21151;&#29983;&#25104;&#20102;&#35270;&#35273;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#22914;&#20309;&#34920;&#31034;&#21644;&#36755;&#20837;&#36825;&#20123;&#20449;&#24687;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;&#21021;&#22987;&#23618;&#26159;&#35299;&#37322;CNN&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2301.08067</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35299;&#37322;CNN&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpreting CNN Predictions using Conditional Generative Adversarial Networks. (arXiv:2301.08067v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;GAN&#20351;&#29992;CNN&#22788;&#29702;&#22270;&#20687;&#26102;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25104;&#21151;&#29983;&#25104;&#20102;&#35270;&#35273;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#22914;&#20309;&#34920;&#31034;&#21644;&#36755;&#20837;&#36825;&#20123;&#20449;&#24687;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;&#21021;&#22987;&#23618;&#26159;&#35299;&#37322;CNN&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#35757;&#32451;&#26469;&#29983;&#25104;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#35270;&#35273;&#35299;&#37322;&#12290;&#20026;&#20102;&#29702;&#35299;CNN&#65292;&#25105;&#20204;&#29992;CNN&#22788;&#29702;&#22270;&#20687;&#36827;&#34892;&#39044;&#27979;&#30340;&#20449;&#24687;&#26469;&#35757;&#32451;GAN&#12290;&#25552;&#20379;&#36825;&#20123;&#20449;&#24687;&#26377;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#22914;&#20309;&#23558;&#36825;&#20123;&#20449;&#24687;&#34920;&#31034;&#20026;&#21487;&#36755;&#20837;GAN&#30340;&#24418;&#24335;&#65292;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#34920;&#31034;&#36755;&#20837;GAN&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#32047;&#35745;&#24179;&#22343;&#20013;&#38388;&#35299;&#37322;&#26144;&#23556;&#26469;&#24320;&#21457;&#20102;&#36866;&#21512;&#30340;CNN&#26550;&#26500;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#36873;&#25321;GAN&#36755;&#20837;&#34920;&#31034;&#21644;&#36873;&#25321;&#26377;&#25928;&#35757;&#32451;&#31574;&#30053;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;CNN&#30340;&#36890;&#29992;&#29305;&#24449;&#65292;&#24182;&#19982;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#36136;&#37327;&#21644;&#25968;&#37327;&#35780;&#20272;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#21021;&#22987;&#23618;&#26159;&#35299;&#37322;CNN&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel method that trains a conditional Generative Adversarial Network (GAN) to generate visual interpretations of a Convolutional Neural Network (CNN). To comprehend a CNN, the GAN is trained with information on how the CNN processes an image when making predictions. Supplying that information has two main challenges: how to represent this information in a form that is feedable to the GANs and how to effectively feed the representation to the GAN. To address these issues, we developed a suitable representation of CNN architectures by cumulatively averaging intermediate interpretation maps. We also propose two alternative approaches to feed the representations to the GAN and to choose an effective training strategy. Our approach learned the general aspects of CNNs and was agnostic to datasets and CNN architectures. The study includes both qualitative and quantitative evaluations and compares the proposed GANs with state-of-the-art approaches. We found that the initial layer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#37325;&#35201;&#33410;&#28857;&#21644;&#36793;&#30340;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;A*Net&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.04798</link><description>&lt;p&gt;
A*Net&#65306;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs. (arXiv:2206.04798v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#37325;&#35201;&#33410;&#28857;&#21644;&#36793;&#30340;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;A*Net&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#23545;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#29702;&#19968;&#30452;&#30001;&#23884;&#20837;&#26041;&#27861;&#20027;&#23548;&#12290;&#34429;&#28982;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#20855;&#26377;&#23884;&#20837;&#26041;&#27861;&#25152;&#32570;&#20047;&#30340;&#24402;&#32435;&#33021;&#21147;&#65292;&#20294;&#20854;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#25351;&#25968;&#32423;&#36335;&#24452;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#36335;&#24452;&#26041;&#27861;&#12290;&#21463;&#21040;A*&#31639;&#27861;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;A*Net&#23398;&#20064;&#20102;&#19968;&#20010;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#33410;&#28857;&#21644;&#36793;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#36873;&#25321;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#27604;&#20363;&#21487;&#20197;&#25351;&#23450;&#65292;&#20197;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#20256;&#23548;&#24615;&#21644;&#24402;&#32435;&#24615;&#30693;&#35782;&#22270;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;A*Net&#22312;&#20165;&#35775;&#38382;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;10%&#33410;&#28857;&#21644;10%&#36793;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#22522;&#20110;&#36335;&#24452;&#26041;&#27861;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#30334;&#19975;&#32423;&#25968;&#25454;&#38598;ogbl-wikikg2&#19978;&#65292;A*Net&#19981;&#20165;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#36824;&#23454;&#29616;&#20102;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A* algorithm for shortest path problems, our A*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A*Net not only achieves a new state-of-the-art result, but also converges 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#32852;&#37030;&#33976;&#39311;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#30693;&#35782;&#24046;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.07028</link><description>&lt;p&gt;
&#25506;&#32034;&#26080;&#20195;&#29702;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#22312;&#32852;&#37030;&#33976;&#39311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation. (arXiv:2204.07028v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#32852;&#37030;&#33976;&#39311;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#30693;&#35782;&#24046;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm, &#22312;&#27492;&#26381;&#21153;&#22120;&#21608;&#26399;&#24615;&#22320;&#25910;&#38598;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;, &#32780;&#19981;&#32452;&#35013;&#20854;&#31169;&#26377;&#25968;&#25454;. &#26377;&#38480;&#30340;&#36890;&#35759;&#21644;&#20010;&#24615;&#21270;&#38656;&#27714;&#23545;FL&#25552;&#20986;&#20102;&#20005;&#23803;&#25361;&#25112;. &#32852;&#37030;&#33976;&#39311; (FD) &#34987;&#25552;&#20986;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;, &#19982;&#27492;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#20132;&#25442;&#30693;&#35782;, &#25903;&#25345;&#24322;&#26500;&#26412;&#22320;&#27169;&#22411;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36890;&#35759;&#24320;&#38144;. &#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FD&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#20195;&#29702;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#20013;&#36890;&#24120;&#26159;&#19981;&#21487;&#29992;&#30340;. &#19968;&#20123;&#26368;&#36817;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#30340;FD&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#39069;&#22806;&#30340;&#20844;&#20849;&#25968;&#25454;&#30340;&#38656;&#27714;, &#20294;&#30001;&#20110;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#32780;&#20135;&#29983;&#20102;&#26126;&#26174;&#30340;&#24046;&#24322;, &#23548;&#33268;&#26381;&#21153;&#22120;&#19978;&#30340;&#27169;&#22411;&#34920;&#31034;&#19981;&#26126;&#30830;&#65292;&#24182;&#19988;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;.
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-preserving machine learning paradigm in which the server periodically aggregates local model parameters from clients without assembling their private data.  Constrained communication and personalization requirements pose severe challenges to FL. Federated distillation (FD) is proposed to simultaneously address the above two problems, which exchanges knowledge between the server and clients, supporting heterogeneous local models while significantly reducing communication overhead. However, most existing FD methods require a proxy dataset, which is often unavailable in reality.  A few recent proxy-data-free FD approaches can eliminate the need for additional public data, but suffer from remarkable discrepancy among local knowledge due to client-side model heterogeneity, leading to ambiguous representation on the server and inevitable accuracy degradation.  To tackle this issue, we propose a proxy-data-free FD algorithm based on distributed knowledge c
&lt;/p&gt;</description></item></channel></rss>