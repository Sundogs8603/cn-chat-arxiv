<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#30740;&#31350;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#65292;&#21487;&#20197;&#39044;&#27979;&#20854;&#35821;&#35328;&#24615;&#33021;&#30340;&#25193;&#23637;&#36895;&#24230;&#65292;&#19982;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24615;&#33021;&#25193;&#23637;&#36895;&#24230;&#24930;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2404.00685</link><description>&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scaling Properties of Speech Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#65292;&#21487;&#20197;&#39044;&#27979;&#20854;&#35821;&#35328;&#24615;&#33021;&#30340;&#25193;&#23637;&#36895;&#24230;&#65292;&#19982;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24615;&#33021;&#25193;&#23637;&#36895;&#24230;&#24930;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#26088;&#22312;&#20174;&#21407;&#22987;&#38899;&#39057;&#20013;&#23398;&#20064;&#35821;&#35328;&#65292;&#32780;&#26080;&#38656;&#25991;&#26412;&#36164;&#28304;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25105;&#20204;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#24369;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#23545;&#35821;&#38899;&#27169;&#24577;&#25104;&#31435;&#65292;&#36825;&#20123;&#33021;&#21147;&#23558;&#38543;&#30528;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#35745;&#31639;&#37327;&#22686;&#21152;&#32780;&#25552;&#39640;&#12290;&#26412;&#25991;&#21033;&#29992;&#27169;&#22411;&#30340;&#23610;&#24230;&#34892;&#20026;&#26469;&#20272;&#35745;&#25105;&#20204;&#24403;&#21069;&#26041;&#27861;&#23558;&#20135;&#29983;&#20855;&#26377;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33521;&#35821;&#29087;&#32451;&#24230;&#30340;SLM&#30340;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00685v1 Announce Type: cross  Abstract: Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.00231</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24418;&#29366;&#21464;&#24418;&#32593;&#32476;&#29992;&#20110;&#26080;&#20266;&#24433;&#20960;&#20309;&#37325;&#26500;&#39592;&#30406;&#33136;&#26894;MR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00231
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33136;&#26894;&#26894;&#38388;&#30424;&#36864;&#21464;&#65292;&#26159;&#33136;&#26894;&#38388;&#30424;&#28176;&#36827;&#24615;&#32467;&#26500;&#24615;&#30952;&#25439;&#65292;&#34987;&#35748;&#20026;&#22312;&#33136;&#37096;&#30140;&#30171;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20840;&#29699;&#20581;&#24247;&#20851;&#27880;&#28966;&#28857;&#12290;&#20174;MR&#22270;&#20687;&#20013;&#33258;&#21160;&#37325;&#24314;&#33136;&#26894;&#20960;&#20309;&#24418;&#29366;&#65292;&#23558;&#20351;&#21307;&#23398;&#21442;&#25968;&#30340;&#24555;&#36895;&#27979;&#37327;&#25104;&#20026;&#21487;&#33021;&#65292;&#20197;&#35780;&#20272;&#33136;&#26894;&#29366;&#24577;&#65292;&#20174;&#32780;&#30830;&#23450;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#25216;&#26415;&#36890;&#24120;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#20998;&#21106;&#25110;&#19981;&#36866;&#21512;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#30340;&#26080;&#32467;&#26500;&#28857;&#20113;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransDeformer&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#39640;&#31354;&#38388;&#20934;&#30830;&#24230;&#21644;&#24739;&#32773;&#38388;&#32593;&#26684;&#23545;&#24212;&#30340;&#26041;&#24335;&#37325;&#24314;&#33136;&#26894;&#36718;&#24275;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;TransDeformer&#30340;&#21464;&#31181;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26032;&#30340;&#27880;&#24847;&#21147;&#20844;&#24335;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#21644;&#26631;&#35760;&#21270;&#30340;&#36718;&#24275;&#29305;&#24449;&#38598;&#25104;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00231v1 Announce Type: cross  Abstract: Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18742</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Learning Dynamics of Alignment with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#27169;&#22411;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#34429;&#28982;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29702;&#35770;&#19978;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#27491;&#24335;&#23637;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36824;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#29616;&#35937;&#65292;&#21363;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24403;&#20195;LLMs&#21644;&#23545;&#40784;&#20219;&#21153;&#19978;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24378;&#21270;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23545;&#40784;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#20813;&#36131;&#22768;&#26126;&#65306;&#26412;&#25991;&#21253;&#21547;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
&lt;/p&gt;</description></item><item><title>&#20808;&#20363;&#26159;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#33021;&#21147;&#19981;&#38169;&#65292;&#20294;&#20854;&#20351;&#29992;&#20808;&#20363;&#30340;&#26041;&#24335;&#19982;&#20154;&#31867;&#27861;&#23448;&#19981;&#21516;&#12290;</title><link>https://arxiv.org/abs/2403.16852</link><description>&lt;p&gt;
&#22312;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20013;&#36808;&#21521;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Explainability in Legal Outcome Prediction Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16852
&lt;/p&gt;
&lt;p&gt;
&#20808;&#20363;&#26159;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#33021;&#21147;&#19981;&#38169;&#65292;&#20294;&#20854;&#20351;&#29992;&#20808;&#20363;&#30340;&#26041;&#24335;&#19982;&#20154;&#31867;&#27861;&#23448;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411; - &#27861;&#24459;NLP&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998; - &#19981;&#33021;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#20154;&#31867;&#27861;&#24459;&#20027;&#20307;&#38656;&#35201;&#33021;&#22815;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#22312;&#26222;&#36890;&#27861;&#26696;&#20363;&#20013;&#65292;&#27861;&#24459;&#20174;&#19994;&#32773;&#36890;&#36807;&#21442;&#32771;&#34987;&#31216;&#20026;&#20808;&#20363;&#30340;&#36807;&#21435;&#26696;&#20363;&#27861;&#24459;&#25512;&#29702;&#21040;&#26696;&#20214;&#32467;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20808;&#20363;&#22240;&#27492;&#25104;&#20026;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21046;&#23450;&#27861;&#24459;&#20808;&#20363;&#30340;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#20154;&#31867;&#27861;&#23448;&#21644;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20182;&#20204;&#20381;&#36182;&#30340;&#19981;&#21516;&#31867;&#22411;&#20808;&#20363;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#27169;&#22411;&#23398;&#20250;&#20102;&#21512;&#29702;&#22320;&#39044;&#27979;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20351;&#29992;&#30340;&#20808;&#20363;&#26041;&#24335;&#19981;&#21516;&#20110;&#20154;&#31867;&#27861;&#23448;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16852v1 Announce Type: cross  Abstract: Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand their decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and our models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#20998;&#20026;&#33145;&#20391;&#27969;&#21644;&#32972;&#20391;&#27969;&#20004;&#37096;&#20998;&#65292;&#20197;&#23454;&#29616;&#23545;&#28966;&#21644;&#22788;&#29702;&#22270;&#20687;patch&#30340;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.15977</link><description>&lt;p&gt;
&#26397;&#21521;&#22522;&#20110;&#21452;&#27969;&#30524;&#24213;&#32858;&#28966;&#30340;&#20027;&#21160;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Two-Stream Foveation-based Active Vision Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#20998;&#20026;&#33145;&#20391;&#27969;&#21644;&#32972;&#20391;&#27969;&#20004;&#37096;&#20998;&#65292;&#20197;&#23454;&#29616;&#23545;&#28966;&#21644;&#22788;&#29702;&#22270;&#20687;patch&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#26426;&#22120;&#24863;&#30693;&#26694;&#26550;&#20197;&#19968;&#27425;&#24615;&#26041;&#24335;&#22788;&#29702;&#25972;&#20010;&#36755;&#20837;&#65292;&#20197;&#25552;&#20379;&#8220;&#34987;&#35266;&#23519;&#21040;&#30340;&#29289;&#20307;&#26159;&#20160;&#20040;&#8221;&#21644;&#8220;&#23427;&#20301;&#20110;&#21738;&#37324;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#35299;&#37322;&#20102;&#20154;&#31867;&#35270;&#35273;&#30382;&#23618;&#20013;&#30340;&#31070;&#32463;&#22788;&#29702;&#65292;&#34920;&#26126;&#20854;&#20316;&#20026;&#19968;&#20010;&#21033;&#29992;&#22823;&#33041;&#30340;&#20004;&#20010;&#19981;&#21516;&#21306;&#22495;&#26469;&#22238;&#31572;&#8220;&#26159;&#20160;&#20040;&#8221;&#21644;&#8220;&#22312;&#21738;&#37324;&#8221;&#30340;&#35270;&#35273;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#25152;&#24102;&#26469;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#20197;&#19979;&#26426;&#21046;&#65306;1&#65289;&#33145;&#20391;&#65288;&#26159;&#20160;&#20040;&#65289;&#27969;&#32858;&#28966;&#20110;&#30524;&#29699;&#65288;&#30524;&#24213;&#65289;&#30340;&#35270;&#37326;&#37096;&#20998;&#65292;2&#65289;&#32972;&#20391;&#65288;&#22312;&#21738;&#37324;&#65289;&#27969;&#25552;&#20379;&#35270;&#35273;&#24341;&#23548;&#65292;3&#65289;&#20004;&#20010;&#27969;&#30340;&#36845;&#20195;&#22788;&#29702;&#20197;&#26657;&#20934;&#35270;&#35273;&#28966;&#28857;&#24182;&#22788;&#29702;&#19968;&#31995;&#21015;&#32858;&#28966;&#30340;&#22270;&#20687;&#22359;&#12290;&#35813;&#26694;&#26550;&#30340;&#35757;&#32451;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15977v1 Announce Type: cross  Abstract: Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both "what object is being observed" and "where it is located". In contrast, the "two-stream hypothesis" from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions. In this work, we propose a machine learning framework inspired by the "two-stream hypothesis" and explore the potential benefits that it offers. Specifically, the proposed framework models the following mechanisms: 1) ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation), 2) dorsal (where) stream providing visual guidance, and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches. The training of the pr
&lt;/p&gt;</description></item><item><title>TrustSQL&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#26102;&#30340;&#21487;&#38752;&#24615;&#30340;&#26032;&#22522;&#20934;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;SQL&#39044;&#27979;&#21644;&#25918;&#24323;&#39044;&#27979;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15879</link><description>&lt;p&gt;
TrustSQL: &#29992;&#20110;&#20855;&#26377;&#22810;&#26679;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15879
&lt;/p&gt;
&lt;p&gt;
TrustSQL&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#26102;&#30340;&#21487;&#38752;&#24615;&#30340;&#26032;&#22522;&#20934;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;SQL&#39044;&#27979;&#21644;&#25918;&#24323;&#39044;&#27979;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32763;&#35793;&#25104;SQL&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;SQL&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#21516;&#26102;&#65292;&#24456;&#23569;&#26377;&#20154;&#20102;&#35299;&#36825;&#20123;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#33021;&#21542;&#21487;&#38752;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25506;&#35752;&#36825;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustSQL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#21333;&#19968;&#25968;&#25454;&#24211;&#21644;&#36328;&#25968;&#25454;&#24211;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;&#22522;&#20934;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#25552;&#20379;&#20004;&#31181;&#32467;&#26524;&#20043;&#19968;&#65306;1&#65289;SQL&#39044;&#27979;&#65307;&#25110;2&#65289;&#22312;&#29983;&#25104;&#30340;SQL&#20013;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#25110;&#38754;&#20020;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#26102;&#25918;&#24323;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19987;&#38376;&#20026;&#36825;&#19968;&#20219;&#21153;&#35774;&#35745;&#30340;&#21508;&#31181;&#24314;&#27169;&#26041;&#27861;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20026;&#21487;&#22238;&#31572;&#24615;&#20248;&#21270;&#21333;&#29420;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15879v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have led to significant improvements in translating natural language questions into SQL queries. While achieving high accuracy in SQL generation is crucial, little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones. To explore this aspect, we present TrustSQL, a new benchmark designed to assess the reliability of text-to-SQL models in both single-database and cross-database settings. The benchmark tasks models with providing one of two outcomes: 1) SQL prediction; or 2) abstention from making a prediction, either when there is a potential error in the generated SQL or when faced with unanswerable questions. For model evaluation, we explore various modeling approaches specifically designed for this task. These include: 1) optimizing separate models for answerability d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14633</link><description>&lt;p&gt;
&#20986;&#36523;&#23500;&#36149;&#65311;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#22312;&#31038;&#20250;&#20013;&#21152;&#21095;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#26681;&#25454;&#20010;&#20154;&#32463;&#27982;&#21644;&#31038;&#20250;&#32972;&#26223;&#24433;&#21709;&#33719;&#21462;&#26426;&#20250;&#21644;&#36164;&#28304;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#26222;&#36941;&#38382;&#39064;&#25345;&#32493;&#22320;&#24310;&#32493;&#20102;&#31995;&#32479;&#24615;&#30340;&#19981;&#24179;&#31561;&#65292;&#38459;&#30861;&#20102;&#20316;&#20026;&#19968;&#20010;&#31038;&#20250;&#36861;&#27714;&#21253;&#23481;&#24615;&#36827;&#27493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;SilverSpoon&#65289;&#65292;&#21253;&#21547;3000&#20010;&#26679;&#26412;&#65292;&#23637;&#31034;&#20102;&#29301;&#28041;&#21040;&#24369;&#21183;&#32676;&#20307;&#30001;&#20110;&#20182;&#20204;&#30340;&#22788;&#22659;&#32780;&#23454;&#26045;&#36947;&#24503;&#27169;&#31946;&#34892;&#20026;&#30340;&#20551;&#35774;&#24773;&#26223;&#65292;&#24182;&#38382;&#36825;&#31181;&#34892;&#20026;&#26159;&#21542;&#22312;&#36947;&#24503;&#19978;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#30001;&#23646;&#20110;&#31038;&#20250;&#32463;&#27982;&#20004;&#31471;&#30340;&#20154;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#20351;&#29992;SilverSpoon&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#31243;&#24230;&#20197;&#21450;&#35813;&#31243;&#24230;&#22914;&#20309;&#38543;&#27169;&#22411;&#22823;&#23567;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14633v1 Announce Type: cross  Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;DeepFake&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#23613;&#31649;LLMs&#24182;&#38750;&#19987;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#35774;&#35745;&#65292;&#36825;&#19968;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.14077</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#33021;&#22815;&#26816;&#27979;DeepFakes&#21527;&#65311;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23186;&#20307;&#21462;&#35777;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;DeepFake&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#23613;&#31649;LLMs&#24182;&#38750;&#19987;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#35774;&#35745;&#65292;&#36825;&#19968;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DeepFakes&#26159;&#25351;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#23186;&#20307;&#20869;&#23481;&#65292;&#30001;&#20110;&#20854;&#34987;&#29992;&#20316;&#25955;&#24067;&#34394;&#20551;&#20449;&#24687;&#30340;&#25163;&#27573;&#65292;&#24050;&#32463;&#25104;&#20026;&#36234;&#26469;&#36234;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#26816;&#27979;DeepFakes&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#32534;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;DeepFake&#26816;&#27979;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#21487;&#20197;&#36890;&#36807;&#35880;&#24910;&#30340;&#23454;&#39564;&#35774;&#35745;&#21644;&#21450;&#26102;&#30340;&#24037;&#31243;&#26041;&#27861;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#32771;&#34385;&#21040;LLMs&#24182;&#19981;&#26159;&#26412;&#36136;&#19978;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#65292;&#36825;&#19968;&#28857;&#30456;&#24403;&#26377;&#36259;&#65292;&#32780;&#19988;&#36825;&#20010;&#36807;&#31243;&#24182;&#19981;&#38656;&#35201;&#32534;&#31243;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14077v1 Announce Type: new  Abstract: DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.
&lt;/p&gt;</description></item><item><title>AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13269</link><description>&lt;p&gt;
AFLoRA: &#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#22312;&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13269
&lt;/p&gt;
&lt;p&gt;
AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#65288;AFLoRA&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#27599;&#20010;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#26435;&#37325;&#24352;&#37327;&#65292;&#25105;&#20204;&#28155;&#21152;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24182;&#34892;&#36335;&#24452;&#65292;&#21363;&#19979;&#25237;&#24433;&#21644;&#19978;&#25237;&#24433;&#30697;&#38453;&#65292;&#27599;&#20010;&#30697;&#38453;&#21518;&#38754;&#36319;&#30528;&#19968;&#20010;&#29305;&#24449;&#21464;&#25442;&#21521;&#37327;&#12290;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#20923;&#32467;&#20998;&#25968;&#65292;&#25105;&#20204;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36880;&#27493;&#20923;&#32467;&#36825;&#20123;&#25237;&#24433;&#30697;&#38453;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#24182;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25913;&#21892;&#39640;&#36798;0.85&#65285;&#65292;&#21516;&#26102;&#21487;&#20943;&#23569;&#39640;&#36798;9.5&#20493;&#30340;&#24179;&#22343;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#65292;&#19982;&#31867;&#20284;&#30340;PEFT&#22791;&#36873;&#26041;&#26696;&#30456;&#27604;&#65292;AFLoRA&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;1.86&#20493;&#30340;&#25913;&#36827;&#12290;&#38500;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13269v1 Announce Type: new  Abstract: We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the train
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ABCD&#35268;&#21017;&#65292;&#23558;&#21442;&#25968;&#20943;&#23569;&#20102;&#20174;$5W$&#21040;$5N$&#12290;</title><link>https://arxiv.org/abs/2403.12076</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuron-centric Hebbian Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ABCD&#35268;&#21017;&#65292;&#23558;&#21442;&#25968;&#20943;&#23569;&#20102;&#20174;$5W$&#21040;$5N$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#23398;&#20064;&#26426;&#21046;&#32972;&#21518;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#33021;&#21147;&#20043;&#19968;&#26159;&#36890;&#36807;&#32467;&#26500;&#21644;&#21151;&#33021;&#21487;&#22609;&#24615;&#35843;&#25972;&#20854;&#31361;&#35302;&#12290;&#23613;&#31649;&#31361;&#35302;&#22312;&#20256;&#36882;&#20449;&#24687;&#21040;&#25972;&#20010;&#22823;&#33041;&#20013;&#36215;&#30528;&#22522;&#26412;&#20316;&#29992;&#65292;&#20294;&#20960;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#26159;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20135;&#29983;&#20102;&#23545;&#31361;&#35302;&#30340;&#25913;&#21464;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20026;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#35774;&#35745;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#65292;&#22914;ABCD&#35268;&#21017;&#65292;&#20391;&#37325;&#20110;&#31361;&#35302;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#65292;&#22240;&#27492;&#20248;&#21270;&#31361;&#35302;&#29305;&#23450;&#30340;&#36203;&#24067;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22686;&#21152;&#20102;&#20248;&#21270;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#31361;&#35302;&#37117;&#19982;&#22810;&#20010;&#36203;&#24067;&#21442;&#25968;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#22609;&#24615;&#27169;&#22411;&#65292;&#31216;&#20026;&#31070;&#32463;&#20803;&#20013;&#24515;&#30340;&#36203;&#24067;&#23398;&#20064;&#65288;NcHL&#65289;&#65292;&#20854;&#20248;&#21270;&#20391;&#37325;&#20110;&#31070;&#32463;&#20803;&#32780;&#19981;&#26159;&#31361;&#35302;&#29305;&#23450;&#30340;&#36203;&#24067;&#21442;&#25968;&#12290;&#19982;ABCD&#35268;&#21017;&#30456;&#27604;&#65292;NcHL&#23558;&#21442;&#25968;&#20943;&#23569;&#20174;$5W$&#21040;$5N$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12076v1 Announce Type: cross  Abstract: One of the most striking capabilities behind the learning mechanisms of the brain is the adaptation, through structural and functional plasticity, of its synapses. While synapses have the fundamental role of transmitting information across the brain, several studies show that it is the neuron activations that produce changes on synapses. Yet, most plasticity models devised for artificial Neural Networks (NNs), e.g., the ABCD rule, focus on synapses, rather than neurons, therefore optimizing synaptic-specific Hebbian parameters. This approach, however, increases the complexity of the optimization process since each synapse is associated to multiple Hebbian parameters. To overcome this limitation, we propose a novel plasticity model, called Neuron-centric Hebbian Learning (NcHL), where optimization focuses on neuron- rather than synaptic-specific Hebbian parameters. Compared to the ABCD rule, NcHL reduces the parameters from $5W$ to $5N$
&lt;/p&gt;</description></item><item><title>ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09131</link><description>&lt;p&gt;
ProSwitch&#65306;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09131
&lt;/p&gt;
&lt;p&gt;
ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25991;&#26412;&#25688;&#35201;&#21644;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#36890;&#36807;&#24494;&#35843;&#22312;&#19981;&#21516;&#39118;&#26684;&#38388;&#20999;&#25442;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#25991;&#26412;&#19987;&#19994;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ProSwitch&#65292;&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;ProSwitch&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#25968;&#25454;&#20934;&#22791;&#65292;&#29992;&#20110;&#25910;&#38598;&#39046;&#22495;&#30693;&#35782;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#65307;&#25351;&#20196;&#24494;&#35843;&#65292;&#29992;&#20110;&#20248;&#21270;&#24102;&#26377;&#22810;&#31181;&#25351;&#20196;&#26684;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20840;&#38754;&#35780;&#20272;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#19987;&#19994;&#24615;&#21306;&#20998;&#33021;&#21147;&#21644;&#22522;&#20110;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290; ProSwitch&#30456;&#23545;&#20110;&#36890;&#29992;&#21644;&#19987;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
&lt;/p&gt;</description></item><item><title>Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.08295</link><description>&lt;p&gt;
Gemma&#65306;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#30340;&#24320;&#25918;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemma: Open Models Based on Gemini Research and Technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08295
&lt;/p&gt;
&lt;p&gt;
Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Gemma&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Gemini&#27169;&#22411;&#30740;&#31350;&#21644;&#25216;&#26415;&#26500;&#24314;&#30340;&#36731;&#37327;&#32423;&#12289;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#12290;Gemma&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#23398;&#26415;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;20&#20159;&#21644;70&#20159;&#21442;&#25968;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26816;&#26597;&#28857;&#12290;Gemma&#22312;18&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#65292;&#26377;11&#20010;&#20219;&#21153;&#20248;&#20110;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#36131;&#20219;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21516;&#26102;&#35814;&#32454;&#25551;&#36848;&#20102;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;&#25105;&#20204;&#30456;&#20449;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23454;&#29616;&#19979;&#19968;&#27874;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08295v1 Announce Type: cross  Abstract: This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07175</link><description>&lt;p&gt;
&#37325;&#24314;ROME: &#35299;&#20915;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#24314;&#20102;ROME&#65292;&#25552;&#20379;&#20102;&#26356;&#31283;&#23450;&#30340;r-ROME&#23454;&#29616;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;Rank-One Model Editing (ROME)&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26377;&#19968;&#20123;&#20107;&#23454;&#34920;&#26126;&#35813;&#31639;&#27861;&#26080;&#27861;&#36827;&#34892;&#32534;&#36753;&#32780;&#19981;&#30772;&#22351;&#27169;&#22411;&#12290;&#36825;&#20123;&#32534;&#36753;&#20197;&#21069;&#34987;&#31216;&#20026;&#31105;&#29992;&#32534;&#36753;&#12290;&#36825;&#20123;&#31105;&#29992;&#32534;&#36753;&#20250;&#23548;&#33268;&#31435;&#21363;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#38480;&#21046;&#20102;ROME&#29992;&#20110;&#39034;&#24207;&#32534;&#36753;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20351;&#29992;CounterFact&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#36753;&#26102;&#65292;ROME&#20165;&#22312;&#27492;&#26102;&#21457;&#29983;&#27169;&#22411;&#23849;&#28291;&#65292;&#24182;&#22312;&#20351;&#29992;zsRE&#25968;&#25454;&#38598;&#26102;&#19981;&#20250;&#21457;&#29983;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#31105;&#29992;&#32534;&#36753;&#26159;ROME&#21407;&#22987;&#23454;&#29616;&#30340;&#20135;&#29289;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#31283;&#23450;&#30340;&#23454;&#29616;ROME&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;r-ROME&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#22312;&#20351;&#29992;ROME&#36827;&#34892;&#22823;&#35268;&#27169;&#39034;&#24207;&#32534;&#36753;&#26102;&#19981;&#20877;&#35266;&#23519;&#21040;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07175v1 Announce Type: cross  Abstract: Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we make two main contributions. Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset. Secondly, we find that disabling edits are an artifact of the original implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#35774;&#35745;&#19981;&#21516;&#25552;&#31034;&#24182;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06149</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#33258;&#21160;&#35780;&#20998;&#20889;&#20316;&#25991;&#31456;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Automatically Score Proficiency of Written Essays?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#35774;&#35745;&#19981;&#21516;&#25552;&#31034;&#24182;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#36807;&#21435;50&#24180;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#21160;&#35780;&#20998;&#20316;&#25991;&#65288;AES&#65289;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#25928;&#26524;&#26041;&#38754;&#20173;&#26377;&#35768;&#22810;&#19981;&#36275;&#20043;&#22788;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;LLMs&#30340;&#33021;&#21147;&#65292;&#37492;&#20110;&#23427;&#20204;&#24378;&#22823;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#26469;&#20998;&#26512;&#21644;&#26377;&#25928;&#35780;&#20998;&#20070;&#38754;&#20316;&#25991;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;LLMs&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;ChatGPT&#21644;Llama&#12290;&#25105;&#20204;&#26088;&#22312;&#26816;&#26597;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#20004;&#20010;&#23618;&#38754;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#21363;&#22312;&#25972;&#20307;&#19978;&#21644;&#22312;&#20010;&#20307;&#20889;&#20316;&#29305;&#24449;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#35774;&#35745;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#30340;&#26368;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;ASAP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06149v1 Announce Type: cross  Abstract: Although several methods were proposed to address the problem of automated essay scoring (AES) in the last 50 years, there is still much to desire in terms of effectiveness. Large Language Models (LLMs) are transformer-based models that demonstrate extraordinary capabilities on various tasks. In this paper, we test the ability of LLMs, given their powerful linguistic knowledge, to analyze and effectively score written essays. We experimented with two popular LLMs, namely ChatGPT and Llama. We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait. We utilized prompt-engineering tactics in designing four different prompts to bring their maximum potential to this task. Our experiments conducted on the ASAP dataset revealed several interesting observations. First, choosing the right prompt depend
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#23618;&#31895;&#31961;&#38598;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#19968;&#33268;&#22788;&#29702;&#27169;&#31946;&#24615;&#12289;&#31890;&#24230;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#25945;&#24072;&#23545;&#20869;&#23481;&#29702;&#35299;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25945;&#23398;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04772</link><description>&lt;p&gt;
&#29992;&#31895;&#31961;&#38598;&#34920;&#31034;&#25945;&#23398;&#20869;&#23481;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Representing Pedagogic Content Knowledge Through Rough Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04772
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#23618;&#31895;&#31961;&#38598;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#19968;&#33268;&#22788;&#29702;&#27169;&#31946;&#24615;&#12289;&#31890;&#24230;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#25945;&#24072;&#23545;&#20869;&#23481;&#29702;&#35299;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25945;&#23398;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21517;&#25945;&#24072;&#30340;&#30693;&#35782;&#22522;&#30784;&#21253;&#25324;&#25968;&#23398;&#20869;&#23481;&#30693;&#35782;&#12289;&#23398;&#29983;&#35748;&#35782;&#35770;&#30693;&#35782;&#21644;&#25945;&#23398;&#30693;&#35782;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#23398;&#29983;&#23545;&#20869;&#23481;&#30340;&#30693;&#35782;&#20197;&#21450;&#23398;&#20064;&#29615;&#22659;&#26377;&#30528;&#20005;&#37325;&#24433;&#21709;&#12290;&#25945;&#32946;&#30740;&#31350;&#25991;&#29486;&#35748;&#35782;&#21040;&#22312;&#36817;&#20284;&#24847;&#20041;&#19978;&#24418;&#24335;&#21270;&#19981;&#21516;&#20869;&#23481;&#30693;&#35782;&#30340;&#24517;&#35201;&#24615;&#12290;&#30456;&#20851;&#38382;&#39064;&#20043;&#19968;&#26159;&#21327;&#35843;&#30340;&#24418;&#24335;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;AI&#36719;&#20214;&#31995;&#32479;&#19981;&#20851;&#27880;&#24847;&#20041;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#31995;&#32479;&#20063;&#23384;&#22312;&#33258;&#36523;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#35782;&#21035;&#20102;&#24314;&#27169;&#25945;&#24072;&#23545;&#20869;&#23481;&#29702;&#35299;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#23618;&#31895;&#31961;&#38598;&#30340;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#20854;&#33021;&#22815;&#19968;&#33268;&#22788;&#29702;&#27169;&#31946;&#24615;&#12289;&#31890;&#24230;&#21644;&#22810;&#27169;&#24577;&#24615;&#12290;&#21033;&#29992;&#25193;&#23637;&#31034;&#20363;&#23637;&#31034;&#20102;&#36825;&#20123;&#29305;&#28857;&#65292;&#20197;&#31561;&#24335;&#25512;&#29702;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04772v1 Announce Type: new  Abstract: A teacher's knowledge base consists of knowledge of mathematics content, knowledge of student epistemology, and pedagogical knowledge. It has severe implications on the understanding of student's knowledge of content, and the learning context in general. The necessity to formalize the different content knowledge in approximate senses is recognized in the education research literature. A related problem is that of coherent formalizability. Responsive or smart AI-based software systems do not concern themselves with meaning, and trained ones are replete with their own issues. In the present research, many issues in modeling teachers' understanding of content are identified, and a two-tier rough set-based model is proposed by the present author. The main advantage of the proposed approach is in its ability to coherently handle vagueness, granularity and multi-modality. An extended example to equational reasoning is used to demonstrate these
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04197</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#19978;&#19979;&#25991;&#20998;&#23376;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are In-Context Molecule Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04197
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#20998;&#23376;&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#65292;&#26088;&#22312;&#24357;&#21512;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#22312;&#36866;&#24212;LLMs&#21040;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#23384;&#22312;&#20998;&#23376;&#21644;&#25991;&#26412;&#31354;&#38388;&#20043;&#38388;&#30340;&#24369;&#23545;&#40784;&#65292;&#25110;&#23545;LLMs&#30340;&#35268;&#27169;&#26377;&#20005;&#26684;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ICMA&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#38454;&#27573;&#65306;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#26816;&#32034;&#21518;&#25490;&#24207;&#21644;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04197v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Addi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;Continuous OBB&#65288;COBB&#65289;&#30340;&#26032;&#22411;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#20013;&#30830;&#20445;&#36793;&#30028;&#26694;&#22238;&#24402;&#30340;&#36830;&#32493;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18975</link><description>&lt;p&gt;
&#29702;&#35770;&#19978;&#23454;&#29616;&#23450;&#21521;&#21253;&#22260;&#26694;&#30340;&#36830;&#32493;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Theoretically Achieving Continuous Representation of Oriented Bounding Boxes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;Continuous OBB&#65288;COBB&#65289;&#30340;&#26032;&#22411;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23450;&#21521;&#23545;&#35937;&#26816;&#27979;&#20013;&#30830;&#20445;&#36793;&#30028;&#26694;&#22238;&#24402;&#30340;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#26377;&#20851;&#23450;&#21521;&#21253;&#22260;&#26694;&#65288;OBB&#65289;&#34920;&#31034;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Continuous OBB&#65288;COBB&#65289;&#30340;&#26032;&#22411;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#20445;&#36793;&#30028;&#26694;&#22238;&#24402;&#30340;&#36830;&#32493;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#20013;&#65292;&#20363;&#22914;Faster-RCNN&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18975v1 Announce Type: cross  Abstract: Considerable efforts have been devoted to Oriented Object Detection (OOD). However, one lasting issue regarding the discontinuity in Oriented Bounding Box (OBB) representation remains unresolved, which is an inherent bottleneck for extant OOD methods. This paper endeavors to completely solve this issue in a theoretically guaranteed manner and puts an end to the ad-hoc efforts in this direction. Prior studies typically can only address one of the two cases of discontinuity: rotation and aspect ratio, and often inadvertently introduce decoding discontinuity, e.g. Decoding Incompleteness (DI) and Decoding Ambiguity (DA) as discussed in literature. Specifically, we propose a novel representation method called Continuous OBB (COBB), which can be readily integrated into existing detectors e.g. Faster-RCNN as a plugin. It can theoretically ensure continuity in bounding box regression which to our best knowledge, has not been achieved in liter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CARE CA&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#12289;&#20197;&#21450;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18139</link><description>&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#27491;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Cause and Effect: Can Large Language Models Truly Understand Causality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CARE CA&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#12289;&#20197;&#21450;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#35299;&#35835;&#21644;&#35299;&#37322;&#35821;&#35328;&#25152;&#28041;&#21450;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#28982;&#32780;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#36215;&#26469;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#22240;&#26524;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20855;&#26377;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#22686;&#24378;&#65288;CARE CA&#65289;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22240;&#26524;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558; ConceptNet &#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#20013;&#30340;&#26126;&#30830;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#20197;&#21450;&#36890;&#36807;LLMs&#36827;&#34892;&#30340;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#19968;&#23618;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#19968;&#27493;&#31361;&#20986;LLMs&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;ConceptNet &#20013;&#30340;&#30693;&#35782;&#25552;&#39640;&#20102;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18139v1 Announce Type: cross  Abstract: With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#25233;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#25913;&#36827;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#22522;&#20934;&#32447;&#30456;&#24403;&#30340;&#20219;&#21153;&#22870;&#21169;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.15650</link><description>&lt;p&gt;
&#20855;&#26377;&#30446;&#26631;&#25233;&#21046;&#30340;&#22810;&#32422;&#26463;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Constraint Safe RL with Objective Suppression for Safety-Critical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#25233;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#25913;&#36827;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#22522;&#20934;&#32447;&#30456;&#24403;&#30340;&#20219;&#21153;&#22870;&#21169;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38750;&#24120;&#24120;&#35265;&#65292;&#20294;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#30446;&#26631;&#25233;&#21046;&#65292;&#26681;&#25454;&#23433;&#20840;&#35780;&#21028;&#22120;&#33258;&#36866;&#24212;&#22320;&#25233;&#21046;&#20219;&#21153;&#22870;&#21169;&#26368;&#22823;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#23545;&#30446;&#26631;&#25233;&#21046;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19968;&#20010;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20219;&#20309;&#38169;&#35823;&#30340;&#34892;&#20026;&#37117;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#26174;&#33879;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#21305;&#37197;&#25105;&#20204;&#30340;&#22522;&#20934;&#32447;&#25152;&#36798;&#21040;&#30340;&#20219;&#21153;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15650v1 Announce Type: cross  Abstract: Safe reinforcement learning tasks with multiple constraints are a challenging domain despite being very common in the real world. To address this challenge, we propose Objective Suppression, a novel method that adaptively suppresses the task reward maximizing objectives according to a safety critic. We benchmark Objective Suppression in two multi-constraint safety domains, including an autonomous driving domain where any incorrect behavior can lead to disastrous consequences. Empirically, we demonstrate that our proposed method, when combined with existing safe RL algorithms, can match the task reward achieved by our baselines with significantly fewer constraint violations.
&lt;/p&gt;</description></item><item><title>APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;</title><link>https://arxiv.org/abs/2402.14866</link><description>&lt;p&gt;
APTQ: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14866
&lt;/p&gt;
&lt;p&gt;
APTQ&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#21518;&#35757;&#32451;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38646;-shot&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#39640;&#35745;&#31639;&#36127;&#36733;&#21644;&#24040;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#23545;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;APTQ&#65288;Attention-aware Post-Training Mixed-Precision Quantization&#65289;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#20102;&#27599;&#23618;&#26435;&#37325;&#30340;&#20108;&#38454;&#20449;&#24687;&#65292;&#32780;&#19988;&#39318;&#27425;&#32771;&#34385;&#20102;&#27880;&#24847;&#21147;&#36755;&#20986;&#23545;&#25972;&#20010;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#21033;&#29992;Hessian&#36857;&#20316;&#20026;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25935;&#24863;&#24230;&#24230;&#37327;&#65292;&#30830;&#20445;&#32463;&#36807;&#29702;&#24615;&#30340;&#31934;&#24230;&#38477;&#20302;&#33021;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;APTQ&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#22312;C4&#25968;&#25454;&#38598;&#20013;&#20197;&#24179;&#22343;4&#20301;&#23485;&#24230;&#33719;&#24471;5.22&#22256;&#24785;&#24230;&#65292;&#20960;&#20046;&#31561;&#25928;&#20110;&#20840;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;APTQ&#22312;LLaMa-7B&#21644;LLaMa-1&#20013;&#20197;&#24179;&#22343;3.8&#20301;&#23485;&#24230;&#36798;&#21040;&#20102;68.24&#65285;&#21644;70.48&#65285;&#30340;&#26368;&#20808;&#36827;&#38646;-shot&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-1
&lt;/p&gt;</description></item><item><title>Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14320</link><description>&lt;p&gt;
Triad: &#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14320
&lt;/p&gt;
&lt;p&gt;
Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#31572;&#30693;&#35782;&#24211;&#20013;&#38382;&#39064;&#30340;&#36816;&#29992;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#26469;&#23454;&#29616;KBQA&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21019;&#24314;&#20197;&#20219;&#21153;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triad&#65292;&#19968;&#20010;&#21033;&#29992;&#20855;&#26377;&#19977;&#20010;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#36827;&#34892;KBQA&#20219;&#21153;&#12290;&#20195;&#29702;&#34987;&#20998;&#37197;&#19977;&#20010;&#35282;&#33394;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;KBQA&#23376;&#20219;&#21153;&#65306;&#20316;&#20026;&#25484;&#25569;&#21508;&#31181;&#23376;&#20219;&#21153;&#30340;&#36890;&#25165;&#65292;&#20316;&#20026;&#36873;&#25321;&#20505;&#36873;&#32773;&#30340;&#20915;&#31574;&#32773;&#65292;&#20197;&#21450;&#20316;&#20026;&#22238;&#31572;&#24102;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#30340;&#39038;&#38382;&#12290;&#25105;&#20204;&#30340;KBQA&#26694;&#26550;&#22312;&#22235;&#20010;&#38454;&#27573;&#20013;&#25191;&#34892;&#65292;&#28041;&#21450;&#20195;&#29702;&#30340;&#22810;&#37325;&#35282;&#33394;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11809</link><description>&lt;p&gt;
&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#65306;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#21152;&#36895;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#8221;&#65288;SPACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;LLMs&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#33021;&#21147;&#65292;SPACE&#29420;&#29305;&#22320;&#20351;&#33258;&#22238;&#24402;LLMs&#33021;&#22815;&#24182;&#34892;&#29983;&#25104;&#21644;&#39564;&#35777;&#20196;&#29260;&#12290;&#36825;&#26159;&#36890;&#36807;&#19987;&#38376;&#30340;&#21322;&#33258;&#22238;&#24402;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29616;&#26377;LLMs&#20855;&#26377;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#31639;&#27861;&#20419;&#36827;&#20102;&#21333;&#20010;&#27169;&#22411;&#35843;&#29992;&#20869;&#20196;&#29260;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;LLMs&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;SPACE&#22312;HumanEval-X&#19978;&#34920;&#29616;&#20986;2.7&#20493;&#33267;4.0&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.07938</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#29992;&#25143;&#30028;&#38754;&#65306;&#30001;LLMs&#39537;&#21160;&#30340;&#35821;&#38899;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#36825;&#20123;&#26032;&#21457;&#29616;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#26032;&#19968;&#20195;&#36719;&#20214;&#30340;&#35806;&#29983;&#65292;&#27491;&#22914;&#23427;&#20204;&#22312;&#24037;&#19994;&#30028;&#26080;&#25968;&#24212;&#29992;&#20013;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#12290;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;LLM&#24341;&#25806;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#20998;&#31867;&#26368;&#26377;&#21487;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#35782;&#21035;&#25152;&#38656;&#30340;UI&#32452;&#20214;&#65292;&#24182;&#38543;&#21518;&#25191;&#34892;&#29992;&#25143;&#26399;&#26395;&#30340;&#25805;&#20316;&#12290;&#36825;&#31181;&#38598;&#25104;&#21487;&#20197;&#23558;&#38745;&#24577;UI&#31995;&#32479;&#21457;&#23637;&#25104;&#39640;&#24230;&#21160;&#24577;&#21644;&#21487;&#36866;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24341;&#20837;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#39046;&#22495;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#29992;&#25143;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#26497;&#22823;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent meteoric advancements in large language models have showcased a remarkable capacity for logical reasoning and comprehension. These newfound capabilities have opened the door to a new generation of software, as has been made obvious through the innumerable ways they are being applied in the industry. This research focuses on harnessing and guiding the upgraded power of LLMs to construct a framework that can serve as an intermediary between a user and their user interface. By comprehending a user's needs through a thorough analysis of natural textual inputs, an effectively crafted LLM engine can classify the most likely available application, identify the desired UI component and subsequently execute the user's expected actions. This integration can evolve static UI systems into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences. Such a framework can fundamentally shift how users accomplish daily tasks, skyrocket e
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#20250;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;&#65292;&#25259;&#38706;&#20449;&#24515;&#27700;&#24179;&#21644;&#25552;&#20379;&#21453;&#39304;&#26377;&#21161;&#20110;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#19981;&#19968;&#33268;&#65292;&#20294;&#21442;&#19982;&#32773;&#24448;&#24448;&#22240;&#27492;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#21327;&#20316;&#32467;&#26524;&#36739;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.07632</link><description>&lt;p&gt;
&#36807;&#20110;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Overconfident and Unconfident AI Hinder Human-AI Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07632
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#20250;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;&#65292;&#25259;&#38706;&#20449;&#24515;&#27700;&#24179;&#21644;&#25552;&#20379;&#21453;&#39304;&#26377;&#21161;&#20110;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#19981;&#19968;&#33268;&#65292;&#20294;&#21442;&#19982;&#32773;&#24448;&#24448;&#22240;&#27492;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#21327;&#20316;&#32467;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#20154;&#26426;&#21327;&#20316;&#22312;&#19987;&#19994;&#21644;&#26085;&#24120;&#22330;&#26223;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22312;&#36825;&#31181;&#21327;&#20316;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#34920;&#36798;&#20854;&#23545;&#33258;&#24049;&#34920;&#29616;&#30340;&#20449;&#24515;&#27700;&#24179;&#65292;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#25110;&#32570;&#20047;&#33258;&#20449;&#65292;&#21363;&#20854;&#34920;&#36798;&#30340;&#20449;&#24515;&#39640;&#20110;&#25110;&#20302;&#20110;&#20854;&#23454;&#38469;&#34920;&#29616;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#20204;&#38169;&#35823;&#22320;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#23545;&#20154;&#31867;&#20449;&#20219;&#12289;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#21644;&#21327;&#20316;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25259;&#38706;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#27700;&#24179;&#21644;&#34920;&#29616;&#21453;&#39304;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#35748;&#35782;&#20154;&#24037;&#26234;&#33021;&#20449;&#24515;&#19981;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#21442;&#19982;&#32773;&#24448;&#24448;&#20250;&#22240;&#20026;&#23519;&#35273;&#21040;&#36825;&#31181;&#19981;&#19968;&#33268;&#32780;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#25298;&#32477;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#24182;&#19988;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#24046;&#12290;&#30456;&#21453;&#65292;&#27809;&#26377;&#36825;&#20123;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#21442;&#19982;&#32773;&#26356;&#23481;&#26131;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#24182;&#25509;&#21463;&#20854;&#24314;&#35758;&#65292;&#20174;&#32780;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) advances, human-AI collaboration has become increasingly prevalent across both professional and everyday settings. In such collaboration, AI can express its confidence level about its performance, serving as a crucial indicator for humans to evaluate AI's suggestions. However, AI may exhibit overconfidence or underconfidence--its expressed confidence is higher or lower than its actual performance--which may lead humans to mistakenly evaluate AI advice. Our study investigates the influences of AI's overconfidence and underconfidence on human trust, their acceptance of AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI confidence levels and performance feedback facilitates better recognition of AI confidence misalignments. However, participants tend to withhold their trust as perceiving such misalignments, leading to a rejection of AI suggestions and subsequently poorer performance in collaborative tasks. Conversely, without su
&lt;/p&gt;</description></item><item><title>ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03848</link><description>&lt;p&gt;
ANLS* -- &#19968;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANLS* -- A Universal Document Processing Metric for Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03848
&lt;/p&gt;
&lt;p&gt;
ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22312;&#25991;&#26723;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#65292;&#21306;&#20998;&#27169;&#22411;&#19968;&#30452;&#26159;&#20027;&#35201;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#20570;&#20986;&#30340;&#39044;&#27979;&#21487;&#20197;&#20998;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#65292;&#20415;&#20110;&#36827;&#34892;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#65292;&#24182;&#33021;&#30452;&#25509;&#35745;&#31639;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;GLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#39046;&#22495;&#21457;&#29983;&#20102;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;GLLMs&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#20110;GLLMs&#30340;&#39044;&#27979;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ANLS*&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;ANLS*&#24230;&#37327;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#22270;&#19978;&#23454;&#29616;&#36328;&#39046;&#22495;&#27867;&#21270;&#65292;&#35299;&#20915;&#20102;&#28304;HG&#19982;&#30446;&#26631;HG&#20998;&#24067;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#30693;&#35782;&#20256;&#36882;&#26080;&#25928;&#21644;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.03597</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#24322;&#26500;&#22270;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Causal Representation Learning for Out-of-Distribution Generalization on Heterogeneous Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03597
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#22270;&#19978;&#23454;&#29616;&#36328;&#39046;&#22495;&#27867;&#21270;&#65292;&#35299;&#20915;&#20102;&#28304;HG&#19982;&#30446;&#26631;HG&#20998;&#24067;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#30693;&#35782;&#20256;&#36882;&#26080;&#25928;&#21644;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;HGFL&#65289;&#24050;&#32463;&#34987;&#30740;&#21457;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#65288;HGs&#65289;&#20013;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#21508;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;HGFL&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#20174;&#28304;HG&#20013;&#23500;&#26631;&#35760;&#31867;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#23558;&#36825;&#20123;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;HG&#20197;&#20419;&#36827;&#23398;&#20064;&#26032;&#31867;&#21035;&#65292;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#26368;&#32456;&#22312;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#28304;HG&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#37117;&#20849;&#20139;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#19977;&#31181;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#21464;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#21407;&#22240;&#26377;&#20004;&#20010;&#65306;&#65288;1&#65289;&#28304;HG&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#19982;&#30446;&#26631;HG&#20998;&#24067;&#21305;&#37197;&#65292;&#20197;&#21450;&#65288;2&#65289;&#30446;&#26631;HG&#30340;&#19981;&#21487;&#39044;&#27979;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#12290;&#36825;&#31181;&#20998;&#24067;&#36716;&#21464;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#20013;&#30693;&#35782;&#20256;&#36882;&#26080;&#25928;&#21644;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03597v2 Announce Type: replace  Abstract: Heterogeneous graph few-shot learning (HGFL) has been developed to address the label sparsity issue in heterogeneous graphs (HGs), which consist of various types of nodes and edges. The core concept of HGFL is to extract knowledge from rich-labeled classes in a source HG, transfer this knowledge to a target HG to facilitate learning new classes with few-labeled training data, and finally make predictions on unlabeled testing data. Existing methods typically assume that the source HG, training data, and testing data all share the same distribution. However, in practice, distribution shifts among these three types of data are inevitable due to two reasons: (1) the limited availability of the source HG that matches the target HG distribution, and (2) the unpredictable data generation mechanism of the target HG. Such distribution shifts result in ineffective knowledge transfer and poor learning performance in existing methods, thereby le
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;SplaTAM&#65292;&#36890;&#36807;&#21033;&#29992;3D&#39640;&#26031;&#20989;&#25968;&#30340;&#26174;&#24335;&#20307;&#31215;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#26410;&#23450;&#20301;RGB-D&#30456;&#26426;&#36827;&#34892;&#39640;&#20445;&#30495;&#37325;&#24314;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2312.02126</link><description>&lt;p&gt;
SplaTAM: &#29992;&#20110;&#23494;&#38598;RGB-D SLAM&#30340;Splat&#12289;&#36861;&#36394;&#21644;&#26144;&#23556;3D&#39640;&#26031;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;SplaTAM&#65292;&#36890;&#36807;&#21033;&#29992;3D&#39640;&#26031;&#20989;&#25968;&#30340;&#26174;&#24335;&#20307;&#31215;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#26410;&#23450;&#20301;RGB-D&#30456;&#26426;&#36827;&#34892;&#39640;&#20445;&#30495;&#37325;&#24314;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#30340;&#21516;&#26102;&#23450;&#20301;&#21644;&#22320;&#22270;&#26500;&#24314;&#65288;SLAM&#65289;&#23545;&#20110;&#26426;&#22120;&#20154;&#21644;&#22686;&#24378;&#29616;&#23454;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#21463;&#21040;&#23427;&#20204;&#34920;&#31034;&#22330;&#26223;&#30340;&#38750;&#20307;&#31215;&#25110;&#38544;&#24335;&#26041;&#24335;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SplaTAM&#65292;&#19968;&#31181;&#39318;&#27425;&#21033;&#29992;&#26174;&#24335;&#20307;&#31215;&#34920;&#31034;&#65288;&#21363;3D&#39640;&#26031;&#20989;&#25968;&#65289;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#36890;&#36807;&#21333;&#20010;&#26410;&#23450;&#20301;RGB-D&#30456;&#26426;&#23454;&#29616;&#39640;&#20445;&#30495;&#37325;&#24314;&#65292;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;SplaTAM&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22312;&#32447;&#36861;&#36394;&#21644;&#26144;&#23556;&#31995;&#32479;&#65292;&#19987;&#20026;&#24213;&#23618;&#39640;&#26031;&#20989;&#25968;&#34920;&#31034;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#23427;&#21033;&#29992;&#21098;&#24433;&#25513;&#33180;&#20248;&#38597;&#22320;&#25429;&#25417;&#22330;&#26223;&#23494;&#24230;&#30340;&#23384;&#22312;&#12290;&#35813;&#32452;&#21512;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#34920;&#31034;&#25552;&#20379;&#20102;&#22810;&#20010;&#20248;&#28857;&#65292;&#21253;&#25324;&#24555;&#36895;&#28210;&#26579;&#21644;&#23494;&#38598;&#20248;&#21270;&#12289;&#24555;&#36895;&#30830;&#23450;&#21306;&#22495;&#26159;&#21542;&#24050;&#34987;&#26144;&#23556;&#20197;&#21450;&#36890;&#36807;&#28155;&#21152;&#26356;&#22810;&#39640;&#26031;&#20989;&#25968;&#36827;&#34892;&#32467;&#26500;&#21270;&#22320;&#22270;&#25193;&#23637;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;SplaTAM...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02126v2 Announce Type: replace-cross  Abstract: Dense simultaneous localization and mapping (SLAM) is crucial for robotics and augmented reality applications. However, current methods are often hampered by the non-volumetric or implicit way they represent a scene. This work introduces SplaTAM, an approach that, for the first time, leverages explicit volumetric representations, i.e., 3D Gaussians, to enable high-fidelity reconstruction from a single unposed RGB-D camera, surpassing the capabilities of existing methods. SplaTAM employs a simple online tracking and mapping system tailored to the underlying Gaussian representation. It utilizes a silhouette mask to elegantly capture the presence of scene density. This combination enables several benefits over prior representations, including fast rendering and dense optimization, quickly determining if areas have been previously mapped, and structured map expansion by adding more Gaussians. Extensive experiments show that SplaTAM
&lt;/p&gt;</description></item><item><title>MetaCloak&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#39069;&#22806;&#36716;&#25442;&#25277;&#26679;&#36807;&#31243;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#40065;&#26834;&#30340;&#25200;&#21160;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.13127</link><description>&lt;p&gt;
&#38024;&#23545;&#26410;&#32463;&#25480;&#26435;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#21512;&#25104;&#30340;&#40065;&#26834;&#24615;&#19981;&#21487;&#23519;&#35273;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Toward Robust Imperceptible Perturbation against Unauthorized Text-to-image Diffusion-based Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13127
&lt;/p&gt;
&lt;p&gt;
MetaCloak&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#39069;&#22806;&#36716;&#25442;&#25277;&#26679;&#36807;&#31243;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#40065;&#26834;&#30340;&#25200;&#21160;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13127v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#20195;&#20132;&#21449; &#25688;&#35201;: &#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20801;&#35768;&#20174;&#23569;&#37327;&#21442;&#32771;&#29031;&#29255;&#26080;&#32541;&#29983;&#25104;&#20010;&#24615;&#21270;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#22914;&#26524;&#33853;&#20837;&#38169;&#35823;&#30340;&#25163;&#20013;&#65292;&#21487;&#33021;&#21046;&#36896;&#35823;&#23548;&#24615;&#25110;&#26377;&#23475;&#20869;&#23481;&#65292;&#21361;&#23475;&#20010;&#20154;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25237;&#27602;&#30340;&#26041;&#27861;&#20197;&#19968;&#31181;&#19981;&#21487;&#23519;&#35273;&#30340;&#26041;&#24335;&#25200;&#21160;&#29992;&#25143;&#22270;&#20687;&#65292;&#20197;&#20351;&#20854;&#26080;&#27861;&#34987;&#24694;&#24847;&#20351;&#29992;&#32773;&#23398;&#20064;&#12290;&#25105;&#20204;&#30830;&#23450;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;i) &#30001;&#20110;&#25163;&#24037;&#21551;&#21457;&#24335;&#35299;&#20915;&#38590;&#35299;&#21452;&#23618;&#20248;&#21270;&#32780;&#23548;&#33268;&#27425;&#20248;&#65307;ii) &#32570;&#20047;&#23545;&#31616;&#21333;&#25968;&#25454;&#36716;&#25442;&#65288;&#22914;&#39640;&#26031;&#28388;&#27874;&#65289;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaCloak&#65292;&#23427;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#21452;&#32423;&#25237;&#27602;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#39069;&#22806;&#30340;&#36716;&#25442;&#25277;&#26679;&#36807;&#31243;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#40065;&#26834;&#30340;&#25200;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#32452;&#26367;&#20195;&#25193;&#25955;&#27169;&#22411;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13127v2 Announce Type: replace-cross  Abstract: Text-to-image diffusion models allow seamless generation of personalized images from scant reference photos. Yet, these tools, in the wrong hands, can fabricate misleading or harmful content, endangering individuals. To address this problem, existing poisoning-based approaches perturb user images in an imperceptible way to render them "unlearnable" from malicious uses. We identify two limitations of these defending approaches: i) sub-optimal due to the hand-crafted heuristics for solving the intractable bilevel optimization and ii) lack of robustness against simple data transformations like Gaussian filtering. To solve these challenges, we propose MetaCloak, which solves the bi-level poisoning problem with a meta-learning framework with an additional transformation sampling process to craft transferable and robust perturbation. Specifically, we employ a pool of surrogate diffusion models to craft transferable and model-agnostic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2401.16412</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#36827;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning to Manipulate under Limited Information. (arXiv:2401.16412v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#32463;&#20856;&#32467;&#26524;&#65292;&#20219;&#20309;&#21512;&#29702;&#30340;&#20559;&#22909;&#25237;&#31080;&#26041;&#27861;&#26377;&#26102;&#20250;&#32473;&#20010;&#20307;&#25552;&#20379;&#25253;&#21578;&#19981;&#30495;&#23454;&#20559;&#22909;&#30340;&#28608;&#21169;&#12290;&#23545;&#20110;&#27604;&#36739;&#25237;&#31080;&#26041;&#27861;&#26469;&#35828;&#65292;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26356;&#25110;&#32773;&#26356;&#23569;&#25269;&#25239;&#36825;&#31181;&#31574;&#30053;&#24615;&#25805;&#32437;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#35268;&#27169;&#19979;&#23545;&#38480;&#21046;&#20449;&#24687;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#32473;&#23450;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#30340;&#25104;&#21151;&#31243;&#24230;&#26469;&#34913;&#37327;&#25805;&#32437;&#30340;&#25269;&#25239;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#23558;&#36817;40,000&#20010;&#19981;&#21516;&#35268;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23545;&#25239;8&#31181;&#19981;&#21516;&#30340;&#25237;&#31080;&#26041;&#27861;&#65292;&#22312;6&#31181;&#38480;&#21046;&#20449;&#24687;&#24773;&#20917;&#19979;&#65292;&#36827;&#34892;&#21253;&#21547;5-21&#21517;&#36873;&#27665;&#21644;3-6&#21517;&#20505;&#36873;&#20154;&#30340;&#22996;&#21592;&#20250;&#35268;&#27169;&#36873;&#20030;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#25237;&#31080;&#26041;&#27861;&#65292;&#22914;Borda&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#21487;&#20197;&#34987;&#31070;&#32463;&#32593;&#32476;&#39640;&#24230;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;Instant Runoff&#26041;&#27861;&#65292;&#34429;&#28982;&#34987;&#19968;&#20010;&#29702;&#24819;&#30340;&#25805;&#32437;&#32773;&#21033;&#28070;&#21270;&#25805;&#32437;&#65292;&#20294;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#19981;&#20250;&#21463;&#21040;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with
&lt;/p&gt;</description></item><item><title>EMPAIA&#20513;&#35758;&#27719;&#38598;&#20102;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#65292;&#24320;&#21457;&#20102;&#25216;&#26415;&#20114;&#25805;&#20316;&#26631;&#20934;&#65292;&#26631;&#20934;&#21270;&#25509;&#21475;&#65292;&#20197;&#21450;&#25512;&#24191;&#21644;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20110;&#30149;&#29702;&#23398;&#35786;&#26029;&#30340;&#24314;&#35758;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#22810;&#20010;AI&#24212;&#29992;&#31243;&#24207;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.09450</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21327;&#21161;&#30340;&#30149;&#29702;&#35786;&#26029;&#20013;&#30340;&#21512;&#20316;&#65306;EMPAIA&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative. (arXiv:2401.09450v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09450
&lt;/p&gt;
&lt;p&gt;
EMPAIA&#20513;&#35758;&#27719;&#38598;&#20102;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#65292;&#24320;&#21457;&#20102;&#25216;&#26415;&#20114;&#25805;&#20316;&#26631;&#20934;&#65292;&#26631;&#20934;&#21270;&#25509;&#21475;&#65292;&#20197;&#21450;&#25512;&#24191;&#21644;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20110;&#30149;&#29702;&#23398;&#35786;&#26029;&#30340;&#24314;&#35758;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#22810;&#20010;AI&#24212;&#29992;&#31243;&#24207;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#30149;&#29702;&#23398;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25216;&#26415;&#21644;&#30417;&#31649;&#26041;&#38754;&#30340;&#31181;&#31181;&#25361;&#25112;&#65292;&#23558;&#36825;&#20123;&#30740;&#31350;&#25104;&#26524;&#36716;&#21270;&#20026;&#20020;&#24202;&#35786;&#26029;&#20135;&#21697;&#24182;&#25512;&#24191;&#24212;&#29992;&#30340;&#36895;&#24230;&#20173;&#28982;&#36739;&#24930;&#65292;&#20854;&#20013;&#21253;&#25324;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#25509;&#21475;&#12290;&#24320;&#25918;&#21644;&#20379;&#24212;&#21830;&#20013;&#31435;&#30340;EMPAIA&#35745;&#21010;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;EMPAIA&#30340;&#25104;&#26524;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;EMPAIA&#23558;&#30149;&#29702;&#23398;AI&#29983;&#24577;&#31995;&#32479;&#30340;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#26041;&#65292;&#22914;&#30149;&#29702;&#23398;&#23478;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#21644;&#24037;&#19994;&#30028;&#36827;&#34892;&#32039;&#23494;&#21512;&#20316;&#12290;&#22312;&#32039;&#23494;&#21512;&#20316;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#25216;&#26415;&#20114;&#25805;&#20316;&#24615;&#26631;&#20934;&#65292;AI&#27979;&#35797;&#21644;&#20135;&#21697;&#24320;&#21457;&#30340;&#24314;&#35758;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;EMPAIA&#24179;&#21488;&#65292;&#24182;&#25104;&#21151;&#23558;&#26469;&#33258;6&#20010;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;11&#20010;&#22522;&#20110;AI&#30340;&#22270;&#20687;&#20998;&#26512;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#21040;&#35813;&#24179;&#21488;&#19978;&#65292;&#23637;&#31034;&#20102;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#22914;&#20309;&#20351;&#29992;&#32479;&#19968;&#30340;&#26631;&#20934;&#21270;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, artificial intelligence (AI) methods in pathology have advanced substantially. However, integration into routine clinical practice has been slow due to numerous challenges, including technical and regulatory hurdles in translating research results into clinical diagnostic products and the lack of standardized interfaces. The open and vendor-neutral EMPAIA initiative addresses these challenges. Here, we provide an overview of EMPAIA's achievements and lessons learned. EMPAIA integrates various stakeholders of the pathology AI ecosystem, i.e., pathologists, computer scientists, and industry. In close collaboration, we developed technical interoperability standards, recommendations for AI testing and product development, and explainability methods. We implemented the modular and open-source EMPAIA platform and successfully integrated 11 AI-based image analysis apps from 6 different vendors, demonstrating how different apps can use a single standardized interface. We 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#27010;&#29575;&#29615;&#22659;&#30340;&#31867;Shapley&#20998;&#25968;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#24211;&#20013;&#20107;&#23454;&#23545;&#26597;&#35810;&#22238;&#31572;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#30740;&#31350;&#24067;&#23572;&#20989;&#25968;&#30340;&#21487;&#22788;&#29702;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#27010;&#29575;&#25968;&#25454;&#24211;&#20013;&#24212;&#29992;&#35813;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06493</link><description>&lt;p&gt;
&#39044;&#26399;&#30340;&#31867;Shapley&#20998;&#25968;: &#22797;&#26434;&#24615;&#19982;&#22312;&#27010;&#29575;&#25968;&#25454;&#24211;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Expected Shapley-Like Scores of Boolean Functions: Complexity and Applications to Probabilistic Databases. (arXiv:2401.06493v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06493
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#27010;&#29575;&#29615;&#22659;&#30340;&#31867;Shapley&#20998;&#25968;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#24211;&#20013;&#20107;&#23454;&#23545;&#26597;&#35810;&#22238;&#31572;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#30740;&#31350;&#24067;&#23572;&#20989;&#25968;&#30340;&#21487;&#22788;&#29702;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#27010;&#29575;&#25968;&#25454;&#24211;&#20013;&#24212;&#29992;&#35813;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#26159;&#28304;&#33258;&#21338;&#24328;&#35770;&#24182;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#24211;&#20013;&#20107;&#23454;&#23545;&#26597;&#35810;&#22238;&#31572;&#30340;&#36129;&#29486;&#65292;&#20197;&#21450;&#20854;&#20182;&#31867;&#20284;&#30340;&#26435;&#37325;&#25351;&#26631;&#65292;&#22914;Banzhaf&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#31867;Shapley&#20998;&#25968;&#36866;&#24212;&#21040;&#27010;&#29575;&#29615;&#22659;&#20013;&#65292;&#30446;&#26631;&#26159;&#35745;&#31639;&#23427;&#20204;&#30340;&#26399;&#26395;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39044;&#26399;Shapley&#20540;&#21644;&#24067;&#23572;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#30340;&#35745;&#31639;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#30456;&#20114;&#21487;&#24402;&#32422;&#65292;&#20174;&#32780;&#33719;&#24471;&#30456;&#21516;&#30340;&#21487;&#22788;&#29702;&#24615;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#24067;&#23572;&#20989;&#25968;&#34987;&#34920;&#31034;&#20026;&#30830;&#23450;&#24615;&#21487;&#20998;&#35299;&#30005;&#36335;&#30340;&#21487;&#22788;&#29702;&#24773;&#20917;&#65292;&#22312;&#35813;&#24773;&#20917;&#19979;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#24211;&#26469;&#28304;&#35299;&#37322;&#22312;&#27010;&#29575;&#25968;&#25454;&#24211;&#20013;&#24212;&#29992;&#36825;&#20010;&#31639;&#27861;&#65292;&#24182;&#22312;ProvSQL&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#23454;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapley values, originating in game theory and increasingly prominent in explainable AI, have been proposed to assess the contribution of facts in query answering over databases, along with other similar power indices such as Banzhaf values. In this work we adapt these Shapley-like scores to probabilistic settings, the objective being to compute their expected value. We show that the computations of expected Shapley values and of the expected values of Boolean functions are interreducible in polynomial time, thus obtaining the same tractability landscape. We investigate the specific tractable case where Boolean functions are represented as deterministic decomposable circuits, designing a polynomial-time algorithm for this setting. We present applications to probabilistic databases through database provenance, and an effective implementation of this algorithm within the ProvSQL system, which experimentally validates its feasibility over a standard benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#19978;&#27604;GPT-4v&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;26&#65285;&#12290;</title><link>http://arxiv.org/abs/2401.02797</link><description>&lt;p&gt;
PeFoMed&#65306;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#19978;&#27604;GPT-4v&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19978;&#36827;&#34892;&#20102;&#36827;&#21270;&#25193;&#23637;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#24212;&#23545;&#36229;&#36234;&#32431;&#25991;&#26412;&#24212;&#29992;&#33539;&#22260;&#30340;&#25361;&#25112;&#12290;&#23427;&#21033;&#29992;&#20102;&#20808;&#21069;&#32534;&#30721;&#22312;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#21644;&#21151;&#33021;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;MLLMs&#36866;&#24212;&#20026;&#29983;&#25104;&#20219;&#21153;&#20197;&#35299;&#20915;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;Med-VQA&#65289;&#20219;&#21153;&#30340;&#33258;&#30001;&#24418;&#24335;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Med-VQA&#24212;&#29992;&#29305;&#21035;&#23450;&#21046;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#20026;&#20102;&#20934;&#30830;&#34913;&#37327;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#19978;&#36798;&#21040;&#20102;81.9&#65285;&#65292;&#19988;&#20854;&#30456;&#23545;&#20110;GPT-4v&#27169;&#22411;&#30340;&#32477;&#23545;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLMs) represent an evolutionary expansion in the capabilities of traditional large language models, enabling them to tackle challenges that surpass the scope of purely text-based applications. It leverages the knowledge previously encoded within these language models, thereby enhancing their applicability and functionality in the reign of multimodal contexts. Recent works investigate the adaptation of MLLMs to predict free-form answers as a generative task to solve medical visual question answering (Med-VQA) tasks. In this paper, we propose a parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications, and empirically validate it on a public benchmark dataset. To accurately measure the performance, we employ human evaluation and the results reveal that our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions. The cod
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20195;&#30721;&#22788;&#29702;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#23427;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#24182;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.07989</link><description>&lt;p&gt;
&#32479;&#19968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#35270;&#35282;&#65306;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code. (arXiv:2311.07989v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20195;&#30721;&#22788;&#29702;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#23427;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#24182;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#26368;&#36817;&#22312;&#20195;&#30721;&#22788;&#29702;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;50&#22810;&#20010;&#27169;&#22411;&#12289;30&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#12289;170&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;700&#22810;&#20010;&#30456;&#20851;&#24037;&#20316;&#12290;&#25105;&#20204;&#23558;&#20195;&#30721;&#22788;&#29702;&#27169;&#22411;&#20998;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#31995;&#21015;&#65289;&#21644;&#19987;&#38376;&#22312;&#20195;&#30721;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#20855;&#26377;&#19987;&#38376;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#24046;&#24322;&#65292;&#24182;&#31361;&#20986;&#20102;&#20195;&#30721;&#24314;&#27169;&#20174;&#32479;&#35745;&#27169;&#22411;&#21644;RNN&#21040;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;LLM&#20043;&#38388;&#30340;&#21382;&#21490;&#36716;&#21464;&#65292;&#36825;&#27491;&#26159;NLP&#20063;&#32463;&#21382;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20195;&#30721;&#29305;&#23450;&#30340;&#29305;&#24615;&#65292;&#22914;AST&#12289;CFG&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#35757;&#32451;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#36825;&#20221;&#32508;&#36848;&#20445;&#25345;&#24320;&#25918;&#65292;&#24182;&#22312;GitHub&#19978;&#26356;&#26032;&#65292;&#32593;&#22336;&#20026;https://github.com/codefuse-ai/Awesome-Code-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we systematically review the recent advancements in code processing with language models, covering 50+ models, 30+ evaluation tasks, 170+ datasets, and 700+ related works. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also discuss code-specific features such as AST, CFG, and unit tests, along with their application in training code language models, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20250;&#25298;&#32477;&#65288;L2R&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#25298;&#32477;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01041</link><description>&lt;p&gt;
&#23398;&#20250;&#25298;&#32477;&#65306;&#36890;&#36807;&#30693;&#35782;&#33539;&#22260;&#38480;&#21046;&#21644;&#25298;&#32477;&#26426;&#21046;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#21487;&#25511;&#21644;&#21487;&#38752;
&lt;/p&gt;
&lt;p&gt;
Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism. (arXiv:2311.01041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20250;&#25298;&#32477;&#65288;L2R&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#25298;&#32477;&#26426;&#21046;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22238;&#31572;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24182;&#19981;&#23436;&#32654;&#65292;&#32463;&#24120;&#20135;&#29983;&#21547;&#26377;&#38169;&#35823;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;&#22238;&#31572;&#12290;&#36825;&#20123;&#19981;&#20934;&#30830;&#24615;&#65292;&#36890;&#24120;&#31216;&#20026;&#24187;&#35273;&#65292;&#20351;&#24471;LLMs&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#19981;&#21487;&#38752;&#29978;&#33267;&#19981;&#21487;&#29992;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#22312;LLMs&#20013;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38382;&#31572;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#25298;&#32477;&#26426;&#21046;&#65292;&#25351;&#23548;LLMs&#25298;&#32477;&#22238;&#31572;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20197;&#36991;&#20813;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;Learn to Refuse (L2R)&#65292;&#23427;&#23558;&#25298;&#32477;&#26426;&#21046;&#32435;&#20837;&#21040;LLMs&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#21644;&#25298;&#32477;&#37027;&#20123;&#23427;&#20204;&#38590;&#20197;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#26469;&#34920;&#31034;&#25152;&#26377;LLMs&#25152;&#38656;&#35201;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.17722</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Generalizable Policies for Embodied Tasks. (arXiv:2310.17722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#35270;&#35273;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;(LLaRP)&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#30340;LLM&#35843;&#25972;&#20026;&#25509;&#25910;&#25991;&#26412;&#25351;&#20196;&#21644;&#35270;&#35273;&#33258;&#25105;&#20013;&#24515;&#35266;&#27979;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#30452;&#25509;&#22312;&#29615;&#22659;&#20013;&#36755;&#20986;&#21160;&#20316;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#35757;&#32451;LLaRP&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#30475;&#21644;&#34892;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLaRP&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;1,000&#20010;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#25104;&#21151;&#29575;&#36798;&#21040;&#20102;42%&#65292;&#26159;&#20854;&#20182;&#24120;&#35265;&#23398;&#20064;&#22522;&#32447;&#25110;&#38646;&#26679;&#26412;&#24212;&#29992;&#30340;1.7&#20493;&#25104;&#21151;&#29575;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#24110;&#21161;&#31038;&#21306;&#30740;&#31350;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#12289;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;AI&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#65292;&#21253;&#25324;150,000&#20010;&#35757;&#32451;&#20219;&#21153;&#21644;1,000&#20010;&#27979;&#35797;&#20219;&#21153;&#65292;&#29992;&#20110;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#37325;&#26032;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38750;&#27491;&#24335;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#21487;&#26816;&#26597;&#30340;&#31243;&#24207;&#35268;&#33539;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#25552;&#39640;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#35843;&#35797;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.01831</link><description>&lt;p&gt;
&#23558;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#24418;&#24335;&#21270;&#20026;&#31243;&#24207;&#35268;&#33539;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Formalizing Natural Language Intent into Program Specifications via Large Language Models. (arXiv:2310.01831v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38750;&#27491;&#24335;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#21487;&#26816;&#26597;&#30340;&#31243;&#24207;&#35268;&#33539;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#25552;&#39640;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#35843;&#35797;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#20195;&#30721;&#21151;&#33021;&#30340;&#38750;&#27491;&#24335;&#33258;&#28982;&#35821;&#35328;&#65292;&#20363;&#22914;&#20195;&#30721;&#27880;&#37322;&#25110;&#20989;&#25968;&#25991;&#26723;&#65292;&#21487;&#33021;&#21253;&#21547;&#20851;&#20110;&#31243;&#24207;&#24847;&#22270;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#31243;&#24207;&#30340;&#23454;&#29616;&#21644;&#33258;&#28982;&#35821;&#35328;&#25991;&#26723;&#36890;&#24120;&#26080;&#27861;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#20449;&#24687;&#26377;&#28508;&#21147;&#25552;&#39640;&#25925;&#38556;&#23450;&#20301;&#12289;&#35843;&#35797;&#21644;&#20195;&#30721;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22266;&#26377;&#27495;&#20041;&#24615;&#65292;&#36825;&#20123;&#20449;&#24687;&#36890;&#24120;&#34987;&#20302;&#25928;&#21033;&#29992;&#65292;&#20351;&#24471;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#38590;&#20197;&#22312;&#31243;&#24207;&#20013;&#36827;&#34892;&#31243;&#24207;&#21270;&#26816;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#26032;&#20852;&#33021;&#21147;&#8221;&#26377;&#21487;&#33021;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#31243;&#24207;&#21487;&#26816;&#26597;&#30340;&#26029;&#35328;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;LLM&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#22320;&#23558;&#38750;&#27491;&#24335;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#36716;&#21270;&#20026;&#19982;&#31243;&#24207;&#21592;&#24847;&#22270;&#30456;&#21305;&#37197;&#30340;&#27491;&#24335;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a programs intent. However, there is typically no guarantee that a programs implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language which makes natural language intent challenging to check programmatically. The "emergent abilities" of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is uncl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LogicMP&#30340;&#26032;&#39062;&#31070;&#32463;&#23618;&#65292;&#35813;&#23618;&#36890;&#36807;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#36827;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#36890;&#36807;&#26377;&#25928;&#32531;&#35299;&#19968;&#38454;&#36923;&#36753;&#27169;&#22411;&#30340;&#25512;&#26029;&#22256;&#38590;&#65292;LogicMP&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.15458</link><description>&lt;p&gt;
LogicMP: &#19968;&#31181;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints. (arXiv:2309.15458v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LogicMP&#30340;&#26032;&#39062;&#31070;&#32463;&#23618;&#65292;&#35813;&#23618;&#36890;&#36807;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#36827;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#36890;&#36807;&#26377;&#25928;&#32531;&#35299;&#19968;&#38454;&#36923;&#36753;&#27169;&#22411;&#30340;&#25512;&#26029;&#22256;&#38590;&#65292;LogicMP&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#19982;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#24314;&#27169;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#20197;&#28385;&#36275;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#23618;LogicMP&#65292;&#20854;&#23618;&#23545;MLN&#36827;&#34892;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#12290;&#23427;&#21487;&#20197;&#25554;&#20837;&#20219;&#20309;&#29616;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#32534;&#30721;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22359;&#21270;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;MLN&#20013;&#30340;&#32467;&#26500;&#21644;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#35774;&#35745;&#33391;&#22909;&#12289;&#39640;&#25928;&#30340;&#22343;&#22330;&#36845;&#20195;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;MLN&#25512;&#26029;&#30340;&#22256;&#38590;&#65292;&#23558;&#25512;&#26029;&#20174;&#39034;&#24207;&#35745;&#31639;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#24182;&#34892;&#30340;&#24352;&#37327;&#25805;&#20316;&#12290;&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#19977;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LogicMP&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#19978;&#37117;&#20248;&#20110;&#20808;&#36827;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem since it involves modeling intricate correlations to satisfy the constraints. This paper proposes a novel neural layer, LogicMP, whose layers perform mean-field variational inference over an MLN. It can be plugged into any off-the-shelf neural network to encode FOLCs while retaining modularity and efficiency. By exploiting the structure and symmetries in MLNs, we theoretically demonstrate that our well-designed, efficient mean-field iterations effectively mitigate the difficulty of MLN inference, reducing the inference from sequential calculation to a series of parallel tensor operations. Empirical results in three kinds of tasks over graphs, images, and text show that LogicMP outperforms advanced competitors in both performance and efficiency.
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#65292;&#29992;&#20110;&#33719;&#21462;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20154;&#31867;&#30340;&#20171;&#20837;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#26469;&#35780;&#20272;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#26631;&#20934;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#21487;&#27604;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13935</link><description>&lt;p&gt;
&#33391;&#22909;&#30340;&#35299;&#37322;&#32773;&#26263;&#22320;&#37324;&#26159;&#20154;&#31867;-&#20027;&#21160;&#23398;&#20064;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Good Explainers Secretly Human-in-the-Loop Active Learners?. (arXiv:2306.13935v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#65292;&#29992;&#20110;&#33719;&#21462;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20154;&#31867;&#30340;&#20171;&#20837;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#26469;&#35780;&#20272;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#26631;&#20934;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#36817;&#24180;&#26469;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23427;&#22312;&#30740;&#31350;&#27169;&#22411;&#39044;&#27979;&#20197;&#25910;&#38598;&#39069;&#22806;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#30456;&#24403;&#20110;&#20027;&#21160;&#23398;&#20064;&#65292;&#20854;&#20013;&#26597;&#35810;&#31574;&#30053;&#28041;&#21450;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20154;&#31867;&#35282;&#33394;&#30340;&#25968;&#23398;&#36817;&#20284;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#24037;&#20316;&#27969;&#30340;&#36890;&#29992;&#24418;&#24335;&#21270;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20005;&#26684;&#27604;&#36739;&#27492;&#29992;&#27861;&#19982;&#26631;&#20934;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#21516;&#26102;&#20801;&#35768;&#25193;&#23637;&#24037;&#20316;&#27969;&#12290;&#19968;&#20010;&#39069;&#22806;&#30340;&#22909;&#22788;&#26159;&#65292;&#23427;&#20204;&#30340;&#25928;&#29992;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#26469;&#35780;&#20272;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#26114;&#36149;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) techniques have become popular for multiple use-cases in the past few years. Here we consider its use in studying model predictions to gather additional training data. We argue that this is equivalent to Active Learning, where the query strategy involves a human-in-the-loop. We provide a mathematical approximation for the role of the human, and present a general formalization of the end-to-end workflow. This enables us to rigorously compare this use with standard Active Learning algorithms, while allowing for extensions to the workflow. An added benefit is that their utility can be assessed via simulation instead of conducting expensive user-studies. We also present some initial promising results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.14502</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#39034;&#24207;&#26816;&#32034;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;RetICL
&lt;/p&gt;
&lt;p&gt;
RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; RetICL &#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#22320;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#30340;&#31034;&#20363;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#24207;&#21015;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20248;&#21270;&#36873;&#25321;&#20026;&#20351;&#20219;&#21153;&#34920;&#29616;&#26368;&#20339;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#21457;&#23637;&#37117;&#38598;&#20013;&#22312;&#20419;&#20351;&#23427;&#20204;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#20010;&#65288;&#25110;&#22810;&#20010;&#65289;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#65288;&#21487;&#33021;&#26159;&#26032;&#30340;&#65289;&#29983;&#25104;/&#39044;&#27979;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#31034;&#20363;&#30340;&#36873;&#25321;&#21487;&#33021;&#23545;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#22909;&#30340;&#31034;&#20363;&#24182;&#19981;&#26159;&#31616;&#21333;&#30340;&#65292;&#22240;&#20026;&#20195;&#34920;&#24615;&#31034;&#20363;&#32452;&#30340;&#23450;&#20041;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#19981;&#21516;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#29420;&#31435;&#22320;&#23545;&#31034;&#20363;&#36827;&#34892;&#35780;&#20998;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31034;&#20363;&#30340;&#39034;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#26041;&#27861;&#8212;&#8212;In-Context Learning&#30340;&#26816;&#32034;RetICL&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#36880;&#27493;&#36873;&#25321;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#25226;&#39034;&#24207;&#31034;&#20363;&#36873;&#25321;&#30340;&#38382;&#39064;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent developments in large language models focus on prompting them to perform specific tasks. One effective prompting method is in-context learning, where the model performs a (possibly new) generation/prediction task given one (or more) examples. Past work has shown that the choice of examples can make a large impact on task performance. However, finding good examples is not straightforward since the definition of a representative group of examples can vary greatly depending on the task. While there are many existing methods for selecting in-context examples, they generally score examples independently, ignoring the dependency between them and the order in which they are provided to the large language model. In this work, we propose Retrieval for In-Context Learning (RetICL), a learnable method for modeling and optimally selecting examples sequentially for in-context learning. We frame the problem of sequential example selection as a Markov decision process, design an example r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24449;&#20197;&#21450;&#21033;&#29992;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#20195;&#30721;&#25628;&#32034;&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#32534;&#30721;&#21160;&#24577;&#36816;&#34892;&#26102;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#27169;&#22411;&#26550;&#26500;&#20013;&#22343;&#20855;&#26377;&#19968;&#33268;&#30340;&#24615;&#33021;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;&#25628;&#32034;&#24037;&#20855;&#25552;&#39640;&#20102;&#39640;&#36798;44.7%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03843</link><description>&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#23545;&#27604;&#23398;&#20064;&#22312;&#20195;&#30721;&#25628;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Contrastive Learning of Semantic Similarity forCode to Code Search. (arXiv:2305.03843v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24449;&#20197;&#21450;&#21033;&#29992;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#20195;&#30721;&#25628;&#32034;&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#32534;&#30721;&#21160;&#24577;&#36816;&#34892;&#26102;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#27169;&#22411;&#26550;&#26500;&#20013;&#22343;&#20855;&#26377;&#19968;&#33268;&#30340;&#24615;&#33021;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;&#25628;&#32034;&#24037;&#20855;&#25552;&#39640;&#20102;&#39640;&#36798;44.7%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#30721;&#25628;&#32034;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#21253;&#25324;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24449;&#65292;&#21033;&#29992;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#22312;&#35757;&#32451;&#26399;&#38388;&#32534;&#30721;&#21160;&#24577;&#36816;&#34892;&#26102;&#20449;&#24687;&#30340;&#20195;&#30721;&#25628;&#32034;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#22312;&#25512;&#26029;&#26102;&#25191;&#34892;&#35201;&#25628;&#32034;&#30340;&#35821;&#26009;&#24211;&#25110;&#25628;&#32034;&#26597;&#35810;&#65292;&#24182;&#19988;&#31532;&#19968;&#31181;&#22312;&#27491;&#36127;&#21442;&#32771;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#20195;&#30721;&#25628;&#32034;&#25216;&#26415;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25191;&#34892;&#20102;&#19968;&#31995;&#21015;&#30740;&#31350;&#65292;&#35777;&#26126;&#22686;&#24378;&#22411;LLMs&#33021;&#22815;&#36827;&#34892;&#36328;&#35821;&#35328;&#20195;&#30721;&#25628;&#32034;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#26159;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;&#25628;&#32034;&#24037;&#20855;&#25552;&#39640;&#20102;&#39640;&#36798;44.7&#65285;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#25581;&#31034;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#31181;&#27491;&#36127;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20063;&#33021;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#21482;&#20351;&#29992;&#27491;&#26679;&#26412;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel code-to-code search technique that enhances the performance of Large Language Models (LLMs) by including both static and dynamic features as well as utilizing both similar and dissimilar examples during training. We present the first-ever code search method that encodes dynamic runtime information during training without the need to execute either the corpus under search or the search query at inference time and the first code search technique that trains on both positive and negative reference samples. To validate the efficacy of our approach, we perform a set of studies demonstrating the capability of enhanced LLMs to perform cross-language code-to-code search.  Our evaluation demonstrates that the effectiveness of our approach is consistent across various model architectures and programming languages. We outperform the state-of-the-art cross-language search tool by up to 44.7\%. Moreover, our ablation studies reveal that even a single positive and negat
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;</title><link>http://arxiv.org/abs/2305.02942</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#34913;&#37327;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#22312;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging gradient-derived metrics for data selection and valuation in differentially private training. (arXiv:2305.02942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02942
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30417;&#31649;&#25285;&#24551;&#21644;&#21442;&#19982;&#24230;&#30340;&#19981;&#36275;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#26159;&#35299;&#20915;&#30417;&#31649;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#26159;&#20854;&#20013;&#26368;&#24120;&#29992;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#26799;&#24230;&#20449;&#24687;&#26469;&#35782;&#21035;&#38544;&#31169;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26368;&#20005;&#26684;&#30340;&#38544;&#31169;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#30528;&#33021;&#22815;&#20026;&#23458;&#25143;&#25552;&#20379;&#26377;&#21407;&#21017;&#30340;&#25968;&#25454;&#36873;&#25321;&#24037;&#20855;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining high-quality data for collaborative training of machine learning models can be a challenging task due to A) the regulatory concerns and B) lack of incentive to participate. The first issue can be addressed through the use of privacy enhancing technologies (PET), one of the most frequently used one being differentially private (DP) training. The second challenge can be addressed by identifying which data points can be beneficial for model training and rewarding data owners for sharing this data. However, DP in deep learning typically adversely affects atypical (often informative) data samples, making it difficult to assess the usefulness of individual contributions. In this work we investigate how to leverage gradient information to identify training samples of interest in private training settings. We show that there exist techniques which are able to provide the clients with the tools for principled data selection even in strictest privacy settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.09446</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#36755;&#20837;&#25511;&#21046;&#39640;&#32500;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlling High-Dimensional Data With Sparse Input. (arXiv:2303.09446v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#29983;&#25104;&#39640;&#24230;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#30340;&#25509;&#21475;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#36755;&#20986;&#65292;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29992;&#25143;&#25110;&#25163;&#21160;&#25506;&#32034;&#19981;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25110;&#32773;&#36153;&#21147;&#22320;&#27880;&#37322;&#25968;&#25454;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#20110;&#25511;&#21046;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#38901;&#24459;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#23454;&#20363;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(MICVAE)&#65292;&#23427;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#32534;&#30721;&#31232;&#30095;&#30340;&#38901;&#24459;&#29305;&#24449;&#24182;&#36755;&#20986;&#23436;&#25972;&#30340;&#27874;&#24418;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MICVAE&#34920;&#29616;&#20986;&#20102;&#31232;&#30095;&#30340;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#26426;&#21046;&#25152;&#38656;&#30340;&#33391;&#22909;&#21697;&#36136;&#65306;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#12290;&#21363;&#20351;&#21482;&#26377;&#38750;&#24120;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;(~4)&#65292;MICVAE&#20063;&#33021;&#35753;&#29992;&#25143;&#23454;&#29616;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an efficient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: efficiency, robustness, and faithfulness. With even a very small number of input values (~4), MICVAE enables users to im
&lt;/p&gt;</description></item></channel></rss>