<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;MAC&#21327;&#35758;&#20998;&#31867;&#65292;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65306;&#38754;&#21521;&#20219;&#21153;&#30340;&#31070;&#32463;&#21327;&#35758;&#65292;&#31070;&#32463;&#32593;&#32476;&#23548;&#21521;&#30340;&#31526;&#21495;&#21327;&#35758;&#21644;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#21327;&#35758;&#12290;&#35813;&#20998;&#31867;&#26088;&#22312;&#25506;&#32034;&#27599;&#20010;&#32423;&#21035;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#23427;&#20204;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.09506</link><description>&lt;p&gt;
&#38754;&#21521;6G&#30340;&#35821;&#20041;&#36890;&#20449;&#21327;&#35758;&#65306;&#20174;&#21327;&#35758;&#23398;&#20064;&#21040;&#38754;&#21521;&#35821;&#35328;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Semantic Communication Protocols for 6G: From Protocol Learning to Language-Oriented Approaches. (arXiv:2310.09506v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;MAC&#21327;&#35758;&#20998;&#31867;&#65292;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65306;&#38754;&#21521;&#20219;&#21153;&#30340;&#31070;&#32463;&#21327;&#35758;&#65292;&#31070;&#32463;&#32593;&#32476;&#23548;&#21521;&#30340;&#31526;&#21495;&#21327;&#35758;&#21644;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#21327;&#35758;&#12290;&#35813;&#20998;&#31867;&#26088;&#22312;&#25506;&#32034;&#27599;&#20010;&#32423;&#21035;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#23427;&#20204;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35745;&#21363;&#23558;&#21040;&#26469;&#30340;6G&#31995;&#32479;&#23558;&#38754;&#20020;&#21508;&#31181;&#38750;&#38745;&#24577;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#36825;&#32473;&#20256;&#32479;&#30340;&#23186;&#20171;&#35775;&#38382;&#25511;&#21046;&#65288;MAC&#65289;&#21327;&#35758;&#24102;&#26469;&#20102;&#22256;&#25200;&#65292;&#22240;&#20026;MAC&#21327;&#35758;&#26159;&#38745;&#24577;&#21644;&#39044;&#23450;&#20041;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;MAC&#21327;&#35758;&#65292;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#20219;&#21153;&#23450;&#21046;&#20449;&#20196;&#28040;&#24687;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;MAC&#21327;&#35758;&#20998;&#20026;&#19977;&#20010;&#32423;&#21035;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20998;&#31867;&#65306;&#31532;1&#32423;MAC&#65306;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#26500;&#24314;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#31070;&#32463;&#21327;&#35758;&#65307;&#31532;2&#32423;MAC&#65306;&#36890;&#36807;&#23558;&#31532;1&#32423;MAC&#30340;&#36755;&#20986;&#36716;&#25442;&#20026;&#26174;&#24335;&#31526;&#21495;&#24320;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#23548;&#21521;&#30340;&#31526;&#21495;&#21327;&#35758;&#65307;&#31532;3&#32423;MAC&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#21327;&#35758;&#12290;&#36890;&#36807;&#36825;&#31181;&#20998;&#31867;&#65292;&#25105;&#20204;&#26088;&#22312;&#28145;&#20837;&#25506;&#35752;&#27599;&#20010;&#32423;&#21035;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#23427;&#20204;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#20174;&#20449;&#24687;&#35770;&#21644;&#30456;&#20851;&#21407;&#29702;&#20197;&#21450;&#36873;&#23450;&#30340;&#26696;&#20363;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
The forthcoming 6G systems are expected to address a wide range of non-stationary tasks. This poses challenges to traditional medium access control (MAC) protocols that are static and predefined. In response, data-driven MAC protocols have recently emerged, offering ability to tailor their signaling messages for specific tasks. This article presents a novel categorization of these data-driven MAC protocols into three levels: Level 1 MAC. task-oriented neural protocols constructed using multi-agent deep reinforcement learning (MADRL); Level 2 MAC. neural network-oriented symbolic protocols developed by converting Level 1 MAC outputs into explicit symbols; and Level 3 MAC. language-oriented semantic protocols harnessing large language models (LLMs) and generative models. With this categorization, we aim to explore the opportunities and challenges of each level by delving into their foundational techniques. Drawing from information theory and associated principles as well as selected case
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.09499</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27425;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (arXiv:2310.09499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09499
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#20013;&#30340;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#37327;&#21270;&#12289;&#21098;&#26525;&#21644;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;LLMs&#30340;&#25928;&#29575;&#25104;&#20026;LLM&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;LLMs&#21098;&#26525;&#33267;&#33267;&#23569;50%&#30340;&#31232;&#30095;&#24615;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#26681;&#25454;&#25935;&#24863;&#24230;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#31232;&#30095;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#38477;&#20302;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#24403;&#31232;&#30095;&#24230;&#38750;&#24120;&#39640;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Various Large Language Models(LLMs) from the Generative Pretrained Transformer~(GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50\% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#30340;&#36880;&#28857;&#12289;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25490;&#21517;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#36880;&#28857;&#26041;&#27861;&#30340;&#25928;&#29575;&#39640;&#20294;&#25928;&#26524;&#24046;&#65292;&#36880;&#23545;&#26041;&#27861;&#25928;&#26524;&#22909;&#20294;&#35745;&#31639;&#22797;&#26434;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09497</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#30340;&#39640;&#25928;&#38598;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models. (arXiv:2310.09497v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#30340;&#36880;&#28857;&#12289;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25490;&#21517;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#36880;&#28857;&#26041;&#27861;&#30340;&#25928;&#29575;&#39640;&#20294;&#25928;&#26524;&#24046;&#65292;&#36880;&#23545;&#26041;&#27861;&#25928;&#26524;&#22909;&#20294;&#35745;&#31639;&#22797;&#26434;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;&#26679;&#26412;&#25991;&#26723;&#25490;&#21517;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#12290;&#38024;&#23545;&#22522;&#20110;LLM&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#36880;&#28857;&#65292;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#22312;&#19968;&#20010;&#19968;&#33268;&#30340;&#23454;&#39564;&#26694;&#26550;&#20869;&#36827;&#34892;&#20102;&#23545;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#26631;&#35760;&#28040;&#32791;&#65292;&#24310;&#36831;&#31561;&#22240;&#32032;&#12290;&#36825;&#31181;&#39318;&#27425;&#30340;&#27604;&#36739;&#35780;&#20272;&#35753;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#27599;&#31181;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#22266;&#26377;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36880;&#28857;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#24471;&#20998;&#24456;&#39640;&#65292;&#20294;&#22312;&#26377;&#25928;&#24615;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#36880;&#23545;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;LLM&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;LLM&#25512;&#29702;&#30340;&#27425;&#25968;&#21644;&#25490;&#21517;&#36807;&#31243;&#20013;&#30340;&#25552;&#31034;&#26631;&#35760;&#28040;&#32791;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate impressive effectiveness in zero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting approaches have been proposed for LLM-based zero-shot ranking. Our study begins by thoroughly evaluating these existing approaches within a consistent experimental framework, considering factors like model size, token consumption, latency, among others. This first-of-its-kind comparative evaluation of these approaches allows us to identify the trade-offs between effectiveness and efficiency inherent in each approach. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. To further enhance the efficiency of LLM-based zero-shot ranking, we propose a novel Setwise prompting approach. Our approach reduces the number of LLM inferences and the amount of prompt token consumption during the rankin
&lt;/p&gt;</description></item><item><title>Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09486</link><description>&lt;p&gt;
Mirage: &#22270;&#20998;&#31867;&#30340;&#27169;&#22411;&#26080;&#20851;&#22270;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09486
&lt;/p&gt;
&lt;p&gt;
Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNNs&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#25968;&#25454;&#21644;&#35745;&#31639;&#38656;&#27714;&#37327;&#24456;&#22823;&#12290;&#24613;&#38656;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;GNN&#30340;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#22270;&#33976;&#39311;&#26159;&#20026;&#27492;&#30446;&#30340;&#32780;&#21162;&#21147;&#65292;&#26088;&#22312;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#21021;&#27493;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;(1)&#29616;&#26377;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#26412;&#36523;&#20381;&#36182;&#20110;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#23601;&#30772;&#22351;&#20102;&#22270;&#33976;&#39311;&#30340;&#21069;&#25552;&#12290;(2)&#33976;&#39311;&#36807;&#31243;&#23545;&#30446;&#26631;GNN&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20855;&#26377;&#29305;&#24322;&#24615;&#65292;&#22240;&#27492;&#23545;&#24314;&#27169;&#27969;&#31243;&#30340;&#21464;&#21270;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;Mirage&#30340;&#22270;&#20998;&#31867;&#33976;&#39311;&#31639;&#27861;&#26469;&#36991;&#20813;&#36825;&#20123;&#38480;&#21046;&#12290;Mirage&#24314;&#31435;&#22312;&#19968;&#20010;&#27934;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#21363;&#19968;&#20010;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#23558;&#36755;&#20837;&#22270;&#20998;&#35299;&#20026;&#35745;&#31639;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
&lt;/p&gt;</description></item><item><title>HIO-SDF&#26159;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#22686;&#37327;&#22312;&#32447;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#22823;&#22411;&#12289;&#22797;&#26434;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#24037;&#20316;&#31354;&#38388;&#65292;&#24182;&#33021;&#22815;&#20197;&#22312;&#32447;&#22686;&#37327;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2310.09463</link><description>&lt;p&gt;
HIO-SDF&#65306;&#23618;&#27425;&#22686;&#37327;&#22312;&#32447;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;
&lt;/p&gt;
&lt;p&gt;
HIO-SDF: Hierarchical Incremental Online Signed Distance Fields. (arXiv:2310.09463v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09463
&lt;/p&gt;
&lt;p&gt;
HIO-SDF&#26159;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#22686;&#37327;&#22312;&#32447;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#22823;&#22411;&#12289;&#22797;&#26434;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#24037;&#20316;&#31354;&#38388;&#65292;&#24182;&#33021;&#22815;&#20197;&#22312;&#32447;&#22686;&#37327;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#33391;&#22909;&#30340;&#22823;&#22411;&#22797;&#26434;&#31227;&#21160;&#26426;&#22120;&#20154;&#24037;&#20316;&#31354;&#38388;&#30340;&#34920;&#31034;&#24517;&#39035;&#26159;&#31354;&#38388;&#39640;&#25928;&#30340;&#65292;&#21516;&#26102;&#33021;&#22815;&#32534;&#30721;&#30456;&#20851;&#30340;&#20960;&#20309;&#32454;&#33410;&#12290;&#24403;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#26102;&#65292;&#23427;&#38656;&#35201;&#20197;&#22312;&#32447;&#22686;&#37327;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HIO-SDF&#65292;&#19968;&#31181;&#23558;&#29615;&#22659;&#34920;&#31034;&#20026;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#30446;&#21069;SDF&#30340;&#26368;&#20808;&#36827;&#34920;&#31034;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#25110;&#20307;&#32032;&#32593;&#26684;&#12290;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36830;&#32493;&#22320;&#34920;&#31034;SDF&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24456;&#38590;&#20197;&#22686;&#37327;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#65292;&#22240;&#20026;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#20250;&#24536;&#35760;&#20043;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#37096;&#20998;&#65292;&#38500;&#38750;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20256;&#24863;&#22120;&#21382;&#21490;&#29992;&#20110;&#35757;&#32451;&#12290;&#22522;&#20110;&#20307;&#32032;&#30340;&#34920;&#31034;&#19981;&#20855;&#26377;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#32454;&#33410;&#20016;&#23500;&#30340;&#22823;&#22411;&#29615;&#22659;&#20013;&#19981;&#26159;&#31354;&#38388;&#39640;&#25928;&#30340;&#12290;HIO-SDF&#21033;&#29992;&#23618;&#27425;&#26041;&#27861;&#32467;&#21512;&#20102;&#36825;&#20123;&#34920;&#31034;&#30340;&#20248;&#21183;&#65292;&#20351;&#29992;&#31895;&#31961;&#30340;&#20307;&#32032;&#32593;&#26684;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#29615;&#22659;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
A good representation of a large, complex mobile robot workspace must be space-efficient yet capable of encoding relevant geometric details. When exploring unknown environments, it needs to be updatable incrementally in an online fashion. We introduce HIO-SDF, a new method that represents the environment as a Signed Distance Field (SDF). State of the art representations of SDFs are based on either neural networks or voxel grids. Neural networks are capable of representing the SDF continuously. However, they are hard to update incrementally as neural networks tend to forget previously observed parts of the environment unless an extensive sensor history is stored for training. Voxel-based representations do not have this problem but they are not space-efficient especially in large environments with fine details. HIO-SDF combines the advantages of these representations using a hierarchical approach which employs a coarse voxel grid that captures the observed parts of the environment toget
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26694;&#26550;CausalReinforceNet&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20197;&#25552;&#39640;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#20132;&#26131;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09462</link><description>&lt;p&gt;
&#19968;&#20010;&#36171;&#20104;&#22240;&#26524;&#20998;&#26512;&#33021;&#21147;&#30340;&#22686;&#24378;&#23398;&#20064;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65306;&#22686;&#24378;&#33258;&#21160;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
A Framework for Empowering Reinforcement Learning Agents with Causal Analysis: Enhancing Automated Cryptocurrency Trading. (arXiv:2310.09462v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26694;&#26550;CausalReinforceNet&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20197;&#25552;&#39640;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#20132;&#26131;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#20132;&#26131;&#26041;&#27861;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#20013;&#24320;&#21457;&#30408;&#21033;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#38024;&#23545;&#20116;&#31181;&#28909;&#38376;&#30340;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#65288;&#21363;&#27604;&#29305;&#24065;&#20197;&#22806;&#30340;&#21152;&#23494;&#36135;&#24065;&#65289;&#65306;&#24065;&#23433;&#24065;&#12289;&#20197;&#22826;&#22346;&#12289;&#33713;&#29305;&#24065;&#12289;&#29790;&#27874;&#24065;&#21644;&#27888;&#36798;&#24065;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CausalReinforceNet&#65292;&#19968;&#20010;&#34987;&#26500;&#24314;&#20026;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#26694;&#26550;&#12290;&#20316;&#20026;&#20132;&#26131;&#31995;&#32479;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;CausalReinforceNet&#26694;&#26550;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#22312;&#29305;&#24449;&#24037;&#31243;&#36807;&#31243;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#35782;&#21035;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#21464;&#21160;&#30340;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#23558;&#27010;&#29575;&#24615;&#20215;&#26684;&#26041;&#21521;&#20449;&#21495;&#32435;&#20837;&#26694;&#26550;&#20013;&#65292;&#20197;&#22686;&#24378;&#20132;&#26131;&#31995;&#32479;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in artificial intelligence-enhanced trading methods, developing a profitable automated trading system remains challenging in the rapidly evolving cryptocurrency market. This study aims to address these challenges by developing a reinforcement learning-based automated trading system for five popular altcoins~(cryptocurrencies other than Bitcoin): Binance Coin, Ethereum, Litecoin, Ripple, and Tether. To this end, we present CausalReinforceNet, a framework framed as a decision support system. Designed as the foundational architecture of the trading system, the CausalReinforceNet framework enhances the capabilities of the reinforcement learning agent through causal analysis. Within this framework, we use Bayesian networks in the feature engineering process to identify the most relevant features with causal relationships that influence cryptocurrency price movements. Additionally, we incorporate probabilistic price direction signals from dynamic Bayesian networks to enhance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LgTS&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20026;&#26080;&#27861;&#35775;&#38382;&#29615;&#22659;&#36716;&#25442;&#21160;&#21147;&#23398;&#30340;RL&#20195;&#29702;&#25552;&#20379;&#23376;&#30446;&#26631;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#25945;&#23548;RL&#20195;&#29702;&#23398;&#20064;&#19968;&#32452;&#25104;&#21151;&#30340;&#31574;&#30053;&#26469;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.09454</link><description>&lt;p&gt;
LgTS&#65306;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#23376;&#30446;&#26631;&#36827;&#34892;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65292;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents. (arXiv:2310.09454v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LgTS&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20026;&#26080;&#27861;&#35775;&#38382;&#29615;&#22659;&#36716;&#25442;&#21160;&#21147;&#23398;&#30340;RL&#20195;&#29702;&#25552;&#20379;&#23376;&#30446;&#26631;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#25945;&#23548;RL&#20195;&#29702;&#23398;&#20064;&#19968;&#32452;&#25104;&#21151;&#30340;&#31574;&#30053;&#26469;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#38656;&#35201;&#39640;&#32423;&#35268;&#21010;&#30340;&#38382;&#39064;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21033;&#29992;LLM&#36827;&#34892;&#27492;&#31867;&#35268;&#21010;&#20219;&#21153;&#30340;&#25216;&#26415;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#20551;&#35774;&#65292;&#27604;&#22914;&#38656;&#35201;&#35775;&#38382;&#20801;&#35768;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#20449;&#24687;&#20165;&#21521;LLM&#25552;&#20379;&#30456;&#20851;&#19988;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#24517;&#39035;&#37319;&#29992;&#30830;&#23450;&#24615;&#26041;&#27861;&#25191;&#34892;LLM&#30340;&#21709;&#24212;&#65292;&#20363;&#22914;&#20351;&#29992;&#29616;&#26377;&#31574;&#30053;&#25110;&#35745;&#21010;&#25805;&#20316;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LgTS&#65288;LLM&#24341;&#23548;&#30340;&#24072;&#29983;&#23398;&#20064;&#65289;&#36825;&#19968;&#20840;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#25506;&#32034;&#20102;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20026;&#26080;&#27861;&#35775;&#38382;&#29615;&#22659;&#36716;&#25442;&#21160;&#21147;&#23398;&#30340;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#25552;&#20379;&#20102;&#23376;&#30446;&#26631;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;RL&#20195;&#29702;&#21033;&#29992;&#24072;&#29983;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#19968;&#32452;&#25104;&#21151;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the st
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#21644;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20419;&#36827;&#30693;&#35782;&#20256;&#36882;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.09436</link><description>&lt;p&gt;
&#28151;&#21512;&#20219;&#21153;&#30340;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#23376;&#32593;&#32476;&#21457;&#29616;&#21644;&#36719;&#25513;&#34109;
&lt;/p&gt;
&lt;p&gt;
Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks. (arXiv:2310.09436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09436
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#21644;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20419;&#36827;&#30693;&#35782;&#20256;&#36882;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26377;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;: &#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20419;&#36827;&#30693;&#35782;&#20256;&#36882;&#12290;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#20110;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#19968;&#20123;&#24037;&#20316;&#20063;&#38024;&#23545;&#20219;&#21153;&#30456;&#20284;&#26102;&#30340;&#30693;&#35782;&#20256;&#36882;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#21482;&#26377;&#19968;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#36830;&#32493;&#23398;&#20064;&#28151;&#21512;&#20219;&#21153;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#20173;&#28982;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;/&#25110;&#26377;&#38480;&#30340;&#30693;&#35782;&#20256;&#36882;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#20004;&#32773;&#12290;&#23427;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#26469;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#20445;&#30041;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#24182;&#20351;&#26032;&#20219;&#21153;&#33021;&#22815;&#20511;&#21161;&#36807;&#21435;&#30340;&#30693;&#35782;&#23454;&#29616;&#30693;&#35782;&#20256;&#36882;&#12290;&#20351;&#29992;&#20998;&#31867;&#12289;&#29983;&#25104;&#12289;&#20449;&#24687;&#25552;&#21462;&#21450;&#20854;&#28151;&#21512; (&#21363;&#24322;&#26500;&#20219;&#21153;) &#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) has two main objectives: preventing catastrophic forgetting (CF) and encouraging knowledge transfer (KT). The existing literature mainly focused on overcoming CF. Some work has also been done on KT when the tasks are similar. To our knowledge, only one method has been proposed to learn a sequence of mixed tasks. However, these techniques still suffer from CF and/or limited KT. This paper proposes a new CL method to achieve both. It overcomes CF by isolating the knowledge of each task via discovering a subnetwork for it. A soft-masking mechanism is also proposed to preserve the previous knowledge and to enable the new task to leverage the past knowledge to achieve KT. Experiments using classification, generation, information extraction, and their mixture (i.e., heterogeneous tasks) show that the proposed method consistently outperforms strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20851;&#38190;&#35789;&#39537;&#21160;&#30340;&#21477;&#23376;&#36873;&#25321;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;BERT&#30340;&#35270;&#35273;&#38382;&#31572;&#12290;&#36890;&#36807;&#21033;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#25935;&#24863;&#20851;&#38190;&#35789;&#30340;&#21477;&#23376;&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#22312;&#25991;&#26723;&#20013;&#35782;&#21035;&#20986;&#22238;&#31572;&#38382;&#39064;&#30340;&#30456;&#20851;&#20803;&#32032;&#65292;&#24182;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09432</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#38190;&#35789;&#39537;&#21160;&#30340;&#21477;&#23376;&#36873;&#25321;&#22686;&#24378;&#22522;&#20110;BERT&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Enhancing BERT-Based Visual Question Answering through Keyword-Driven Sentence Selection. (arXiv:2310.09432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20851;&#38190;&#35789;&#39537;&#21160;&#30340;&#21477;&#23376;&#36873;&#25321;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;BERT&#30340;&#35270;&#35273;&#38382;&#31572;&#12290;&#36890;&#36807;&#21033;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#25935;&#24863;&#20851;&#38190;&#35789;&#30340;&#21477;&#23376;&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#22312;&#25991;&#26723;&#20013;&#35782;&#21035;&#20986;&#22238;&#31572;&#38382;&#39064;&#30340;&#30456;&#20851;&#20803;&#32032;&#65292;&#24182;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#31454;&#36187;&#35299;&#20915;&#20102;&#22312;&#22810;&#39029;&#25991;&#26723;&#20013;&#33258;&#21160;&#26816;&#27979;&#20803;&#32032;&#20043;&#38388;&#30340;&#29238;&#23376;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#35782;&#21035;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#25552;&#20986;&#30340;&#29305;&#23450;&#38382;&#39064;&#30340;&#25991;&#26723;&#20803;&#32032;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;PoliTo&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#25506;&#32034;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#29305;&#21046;&#30340;&#25277;&#26679;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#26469;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#21253;&#21547;&#25935;&#24863;&#20851;&#38190;&#35789;&#19988;&#19982;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340;&#20851;&#38190;&#35789;&#30456;&#21516;&#30340;&#21477;&#23376;&#65292;&#20363;&#22914;&#23545;&#34920;&#26684;&#25110;&#22270;&#20687;&#30340;&#24341;&#29992;&#12290;&#30001;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#19982;&#22522;&#32447;&#30456;&#27604;&#21462;&#24471;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#27492;&#20219;&#21153;&#30340;&#31215;&#26497;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Document-based Visual Question Answering competition addresses the automatic detection of parent-child relationships between elements in multi-page documents. The goal is to identify the document elements that answer a specific question posed in natural language. This paper describes the PoliTo's approach to addressing this task, in particular, our best solution explores a text-only approach, leveraging an ad hoc sampling strategy. Specifically, our approach leverages the Masked Language Modeling technique to fine-tune a BERT model, focusing on sentences containing sensitive keywords that also occur in the questions, such as references to tables or images. Thanks to the effectiveness of this approach, we are able to achieve high performance compared to baselines, demonstrating how our solution contributes positively to this task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09430</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#24050;&#32463;&#23558;&#20154;&#24037;&#31995;&#32479;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#21040;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#21517;&#20026;"ReClor-plus"&#12289;"LogiQA-plus"&#21644;"LogiQAv2-plus"&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#19977;&#20010;&#23376;&#38598;&#65306;&#31532;&#19968;&#20010;&#26159;&#36873;&#39033;&#38543;&#26426;&#25171;&#20081;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#27491;&#30830;&#36873;&#39033;&#26367;&#25442;&#20026;"&#27809;&#26377;&#20854;&#20182;&#36873;&#39033;&#26159;&#27491;&#30830;&#30340;"&#65292;&#31532;&#19977;&#20010;&#26159;&#21069;&#20004;&#20010;&#23376;&#38598;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37492;&#21035;&#21644;&#29983;&#25104;&#22411;&#30340;LLMs&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31616;&#21333;&#30340;&#25216;&#24039;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#24456;&#38590;&#22238;&#31572;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25200;&#21160;&#24341;&#20837;&#20219;&#21153;&#21464;&#21270;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#27893;&#31449;&#35843;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#39640;&#23454;&#26102;&#25511;&#21046;&#30340;&#21516;&#26102;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#21644;&#36816;&#33829;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#30001;&#20110;&#32570;&#20047;&#25910;&#25947;&#24615;&#20445;&#35777;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#30340;&#27700;&#21147;&#31995;&#32479;&#27169;&#25311;&#27169;&#22411;&#23545;&#20110;&#23454;&#26045;&#24378;&#21270;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21069;&#30340;&#24212;&#29992;&#21463;&#21040;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.09412</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#23454;&#38469;&#27700;&#21147;&#31995;&#32479;&#20013;&#27893;&#31449;&#30340;&#21487;&#25345;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hybrid Reinforcement Learning for Optimizing Pump Sustainability in Real-World Water Distribution Networks. (arXiv:2310.09412v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#27893;&#31449;&#35843;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#39640;&#23454;&#26102;&#25511;&#21046;&#30340;&#21516;&#26102;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#21644;&#36816;&#33829;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#30001;&#20110;&#32570;&#20047;&#25910;&#25947;&#24615;&#20445;&#35777;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#30340;&#27700;&#21147;&#31995;&#32479;&#27169;&#25311;&#27169;&#22411;&#23545;&#20110;&#23454;&#26045;&#24378;&#21270;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21069;&#30340;&#24212;&#29992;&#21463;&#21040;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#27700;&#21147;&#31995;&#32479;&#20013;&#27893;&#31449;&#35843;&#24230;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#25552;&#39640;&#23454;&#26102;&#25511;&#21046;&#30340;&#21516;&#26102;&#36981;&#23432;&#29289;&#29702;&#36816;&#34892;&#32422;&#26463;&#65292;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#21644;&#36816;&#33829;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;&#22522;&#20110;&#36827;&#21270;&#21644;&#36951;&#20256;&#31639;&#27861;&#65292;&#24448;&#24448;&#30001;&#20110;&#32570;&#20047;&#25910;&#25947;&#24615;&#20445;&#35777;&#32780;&#34920;&#29616;&#19981;&#20339;&#12290;&#30456;&#21453;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#21644;&#20943;&#23569;&#25512;&#26029;&#26102;&#38388;&#26041;&#38754;&#20855;&#26377;&#31361;&#20986;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#23454;&#26045;&#24378;&#21270;&#23398;&#20064;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#27700;&#21147;&#31995;&#32479;&#27169;&#25311;&#27169;&#22411;&#65292;&#20197;&#21069;&#30340;&#24212;&#29992;&#21463;&#21040;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#35823;&#24046;&#21487;&#33021;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23398;&#20064;&#21040;&#35823;&#23548;&#24615;&#30340;&#27169;&#24335;&#21644;&#34892;&#20026;&#65292;&#24182;&#25512;&#33616;&#27425;&#20248;&#30340;&#36816;&#34892;&#31574;&#30053;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#8220;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#8221;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article addresses the pump-scheduling optimization problem to enhance real-time control of real-world water distribution networks (WDNs). Our primary objectives are to adhere to physical operational constraints while reducing energy consumption and operational costs. Traditional optimization techniques, such as evolution-based and genetic algorithms, often fall short due to their lack of convergence guarantees. Conversely, reinforcement learning (RL) stands out for its adaptability to uncertainties and reduced inference time, enabling real-time responsiveness. However, the effective implementation of RL is contingent on building accurate simulation models for WDNs, and prior applications have been limited by errors in simulation training data. These errors can potentially cause the RL agent to learn misleading patterns and actions and recommend suboptimal operational strategies. To overcome these challenges, we present an improved "hybrid RL" methodology. This method integrates th
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#20063;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09411</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#35843;&#26597;&#25991;&#26412;&#25688;&#35201;&#30340;&#29616;&#29366;&#65306;&#19968;&#39033;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review. (arXiv:2310.09411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09411
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#20063;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#35821;&#35328;&#25968;&#25454;&#30340;&#22797;&#26434;&#34920;&#31034;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#19982;&#20256;&#32479;&#30340;NLP&#26041;&#27861;&#30456;&#21453;&#65292;&#28145;&#24230;&#23398;&#20064;NLP&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#35821;&#35328;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#35821;&#35328;&#25968;&#25454;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;&#24182;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#20351;&#23427;&#20204;&#38750;&#24120;&#36866;&#21512;NLP&#24212;&#29992;&#12290;&#30001;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#25351;&#25968;&#22686;&#38271;&#21644;&#23545;&#31616;&#27905;&#12289;&#36830;&#36143;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#25991;&#26412;&#25688;&#35201;&#24050;&#25104;&#20026;NLP&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#12290;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has revolutionized natural language processing (NLP) by enabling the development of models that can learn complex representations of language data, leading to significant improvements in performance across a wide range of NLP tasks. Deep learning models for NLP typically use large amounts of data to train deep neural networks, allowing them to learn the patterns and relationships in language data. This is in contrast to traditional NLP approaches, which rely on hand-engineered features and rules to perform NLP tasks. The ability of deep neural networks to learn hierarchical representations of language data, handle variable-length input sequences, and perform well on large datasets makes them well-suited for NLP applications. Driven by the exponential growth of textual data and the increasing demand for condensed, coherent, and informative summaries, text summarization has been a critical research area in the field of NLP. Applying deep learning to text su
&lt;/p&gt;</description></item><item><title>CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.09401</link><description>&lt;p&gt;
CIDER: &#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26041;&#27861;&#29992;&#20110;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
CIDER: Category-Guided Intent Disentanglement for Accurate Personalized News Recommendation. (arXiv:2310.09401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09401
&lt;/p&gt;
&lt;p&gt;
CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#36825;&#22312;&#32531;&#35299;&#29992;&#25143;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#26041;&#38754;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#29992;&#25143;&#21644;&#26032;&#38395;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20294;&#20197;&#19979;&#25361;&#25112;&#24456;&#23569;&#34987;&#30740;&#31350;&#65306;&#65288;C1&#65289;&#22914;&#20309;&#20934;&#30830;&#29702;&#35299;&#19968;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#21253;&#21547;&#30340;&#22810;&#20010;&#24847;&#22270;&#65311;&#20197;&#21450;&#65288;C2&#65289;&#22914;&#20309;&#21306;&#20998;&#29992;&#25143;&#28857;&#20987;&#21382;&#21490;&#20013;&#23545;&#26032;&#38395;&#25991;&#31456;&#26377;&#19981;&#21516;&#21518;&#38405;&#35835;&#20559;&#22909;&#30340;&#24773;&#20917;&#65311;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65288;CIDER&#65289;&#65292;&#23427;&#21033;&#29992;&#65288;1&#65289;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26469;&#35299;&#20915;&#65288;C1&#65289;&#21644;&#65288;2&#65289;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#35299;&#20915;&#65288;C2&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31867;&#21035;&#39044;&#27979;&#32435;&#20837;CIDER&#30340;&#35757;&#32451;&#36807;&#31243;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#22686;&#24378;&#24847;&#22270;&#20998;&#31163;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation aims to assist users in finding news articles that align with their interests, which plays a pivotal role in mitigating users' information overload problem. Although many recent works have been studied for better user and news representations, the following challenges have been rarely studied: (C1) How to precisely comprehend a range of intents coupled within a news article? and (C2) How to differentiate news articles with varying post-read preferences in users' click history? To tackle both challenges together, in this paper, we propose a novel personalized news recommendation framework (CIDER) that employs (1) category-guided intent disentanglement for (C1) and (2) consistency-based news representation for (C2). Furthermore, we incorporate a category prediction into the training process of CIDER as an auxiliary task, which provides supplementary supervisory signals to enhance intent disentanglement. Extensive experiments on two real-world datasets rev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#31070;&#32463;&#25910;&#21457;&#22120;&#20043;&#38388;&#23545;&#40784;&#35821;&#20041;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23616;&#37096;&#24494;&#35843;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.09394</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#35821;&#20041;&#23545;&#40784;&#29992;&#20110;&#24377;&#24615;&#22810;&#29992;&#25143;&#35821;&#20041;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Semantics Alignment via Split Learning for Resilient Multi-User Semantic Communication. (arXiv:2310.09394v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#31070;&#32463;&#25910;&#21457;&#22120;&#20043;&#38388;&#23545;&#40784;&#35821;&#20041;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23616;&#37096;&#24494;&#35843;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#35821;&#20041;&#36890;&#20449;&#30340;&#30740;&#31350;&#24120;&#24120;&#20381;&#36182;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#21457;&#22120;&#65292;&#22914;&#28145;&#24230;&#32852;&#21512;&#28304;&#21644;&#20449;&#36947;&#32534;&#30721;&#65288;DeepJSCC&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#25910;&#21457;&#22120;&#19981;&#21516;&#65292;&#36825;&#20123;&#31070;&#32463;&#25910;&#21457;&#22120;&#21487;&#20197;&#20351;&#29992;&#23454;&#38469;&#30340;&#28304;&#25968;&#25454;&#21644;&#20449;&#36947;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#20854;&#33021;&#22815;&#25552;&#21462;&#21644;&#20256;&#36882;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#31070;&#32463;&#25910;&#21457;&#22120;&#22266;&#26377;&#22320;&#20559;&#21521;&#20110;&#29305;&#23450;&#30340;&#28304;&#25968;&#25454;&#21644;&#20449;&#36947;&#65292;&#20351;&#24471;&#19981;&#21516;&#30340;&#25910;&#21457;&#22120;&#24456;&#38590;&#29702;&#35299;&#39044;&#26399;&#30340;&#35821;&#20041;&#65292;&#23588;&#20854;&#26159;&#22312;&#21021;&#22987;&#36935;&#21040;&#26102;&#12290;&#20026;&#20102;&#22312;&#22810;&#20010;&#31070;&#32463;&#25910;&#21457;&#22120;&#20043;&#38388;&#23545;&#40784;&#35821;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#21644;&#23616;&#37096;&#31070;&#32463;&#32593;&#32476;&#24494;&#35843;&#25216;&#26415;&#12290;&#22312;&#36825;&#31181;&#31216;&#20026;&#20855;&#26377;&#23618;&#20923;&#32467;&#30340;SL&#65288;SLF&#65289;&#30340;&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#32534;&#30721;&#22120;&#19979;&#36733;&#19968;&#20010;&#19981;&#23545;&#40784;&#30340;&#35299;&#30721;&#22120;&#65292;&#24182;&#22312;&#26412;&#22320;&#24494;&#35843;&#36825;&#20123;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#19968;&#37096;&#20998;&#12290;&#36890;&#36807;&#35843;&#25972;&#36825;&#20010;&#27604;&#20363;&#65292;SLF&#21487;&#20197;&#25511;&#21046;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;&#20223;&#30495;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on semantic communication commonly rely on neural network (NN) based transceivers such as deep joint source and channel coding (DeepJSCC). Unlike traditional transceivers, these neural transceivers are trainable using actual source data and channels, enabling them to extract and communicate semantics. On the flip side, each neural transceiver is inherently biased towards specific source data and channels, making different transceivers difficult to understand intended semantics, particularly upon their initial encounter. To align semantics over multiple neural transceivers, we propose a distributed learning based solution, which leverages split learning (SL) and partial NN fine-tuning techniques. In this method, referred to as SL with layer freezing (SLF), each encoder downloads a misaligned decoder, and locally fine-tunes a fraction of these encoder-decoder NN layers. By adjusting this fraction, SLF controls computing and communication costs. Simulation results confirm t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;Spatial Reasoning Integrated Generator (SPRING)&#65292;&#29992;&#20110;&#35774;&#35745;&#29983;&#25104;&#12290;SPRING&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#32422;&#26463;&#28385;&#36275;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#35268;&#26684;&#21644;&#23454;&#29992;&#35201;&#27714;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.09383</link><description>&lt;p&gt;
&#23558;&#31526;&#21495;&#25512;&#29702;&#25972;&#21512;&#21040;&#31070;&#32463;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#35774;&#35745;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Integrating Symbolic Reasoning into Neural Generative Models for Design Generation. (arXiv:2310.09383v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09383
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;Spatial Reasoning Integrated Generator (SPRING)&#65292;&#29992;&#20110;&#35774;&#35745;&#29983;&#25104;&#12290;SPRING&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#32422;&#26463;&#28385;&#36275;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#35268;&#26684;&#21644;&#23454;&#29992;&#35201;&#27714;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29983;&#25104;&#38656;&#35201;&#23558;&#31070;&#32463;&#21644;&#31526;&#21495;&#25512;&#29702;&#32039;&#23494;&#32467;&#21512;&#65292;&#22240;&#20026;&#33391;&#22909;&#30340;&#35774;&#35745;&#24517;&#39035;&#28385;&#36275;&#26174;&#24335;&#29992;&#25143;&#38656;&#27714;&#21644;&#38544;&#21547;&#30340;&#32654;&#23398;&#12289;&#23454;&#29992;&#24615;&#21644;&#20415;&#21033;&#24615;&#35268;&#21017;&#12290;&#24403;&#21069;&#30001;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#33258;&#21160;&#21270;&#35774;&#35745;&#24037;&#20855;&#33021;&#22815;&#29983;&#25104;&#21560;&#24341;&#20154;&#30340;&#35774;&#35745;&#65292;&#20294;&#19981;&#33021;&#28385;&#36275;&#29992;&#25143;&#30340;&#35268;&#26684;&#21644;&#23454;&#29992;&#35201;&#27714;&#12290;&#31526;&#21495;&#25512;&#29702;&#24037;&#20855;&#65288;&#22914;&#32422;&#26463;&#32534;&#31243;&#65289;&#19981;&#33021;&#24863;&#30693;&#22270;&#20687;&#20013;&#30340;&#20302;&#32423;&#35270;&#35273;&#20449;&#24687;&#25110;&#25429;&#25417;&#21040;&#32654;&#23398;&#31561;&#24494;&#22937;&#26041;&#38754;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Spatial Reasoning Integrated Generator (SPRING)&#29992;&#20110;&#35774;&#35745;&#29983;&#25104;&#12290;SPRING&#22312;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#20013;&#23884;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#21644;&#31526;&#21495;&#25972;&#21512;&#30340;&#31354;&#38388;&#25512;&#29702;&#27169;&#22359;&#12290;&#31354;&#38388;&#25512;&#29702;&#27169;&#22359;&#36890;&#36807;&#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24182;&#36890;&#36807;&#31526;&#21495;&#32422;&#26463;&#28385;&#36275;&#26469;&#20915;&#23450;&#35201;&#29983;&#25104;&#30340;&#23545;&#35937;&#30340;&#20301;&#32622;&#65292;&#20197;&#36793;&#30028;&#26694;&#30340;&#24418;&#24335;&#34920;&#31034;&#12290;&#23558;&#31526;&#21495;&#25512;&#29702;&#23884;&#20837;&#31070;&#32463;&#29983;&#25104;&#20445;&#35777;&#20102;SPRING&#30340;&#36755;&#20986;&#28385;&#36275;&#29992;&#25143;&#30340;&#35268;&#26684;&#21644;&#23454;&#29992;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Design generation requires tight integration of neural and symbolic reasoning, as good design must meet explicit user needs and honor implicit rules for aesthetics, utility, and convenience. Current automated design tools driven by neural networks produce appealing designs, but cannot satisfy user specifications and utility requirements. Symbolic reasoning tools, such as constraint programming, cannot perceive low-level visual information in images or capture subtle aspects such as aesthetics. We introduce the Spatial Reasoning Integrated Generator (SPRING) for design generation. SPRING embeds a neural and symbolic integrated spatial reasoning module inside the deep generative network. The spatial reasoning module decides the locations of objects to be generated in the form of bounding boxes, which are predicted by a recurrent neural network and filtered by symbolic constraint satisfaction. Embedding symbolic reasoning into neural generation guarantees that the output of SPRING satisfi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23458;&#25143;&#31471;&#36873;&#25321;&#20013;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#20540;&#65292;&#20197;&#28385;&#36275;&#19968;&#23450;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#38271;&#26399;&#24179;&#22343;&#21442;&#19982;&#12290;</title><link>http://arxiv.org/abs/2310.09370</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#35774;&#32622;&#19979;&#30340;&#36817;&#20248;&#24046;&#20998;&#38544;&#31169;&#23458;&#25143;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Near-optimal Differentially Private Client Selection in Federated Settings. (arXiv:2310.09370v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23458;&#25143;&#31471;&#36873;&#25321;&#20013;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#20540;&#65292;&#20197;&#28385;&#36275;&#19968;&#23450;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#38271;&#26399;&#24179;&#22343;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#23458;&#25143;&#36873;&#25321;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#32852;&#37030;&#32593;&#32476;&#65292;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#65292;&#23458;&#25143;&#31471;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#21327;&#35843;&#23436;&#25104;&#20219;&#21153;&#65307;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#26681;&#25454;&#20854;&#20559;&#22909;&#65288;&#26412;&#22320;&#35745;&#31639;&#21644;&#27010;&#29575;&#24847;&#22270;&#65289;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20915;&#23450;&#26159;&#21542;&#21442;&#19982;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#12290;&#25152;&#24320;&#21457;&#30340;&#31639;&#27861;&#20026;&#23458;&#25143;&#31471;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#20540;&#65292;&#20197;&#28385;&#36275;&#19968;&#23450;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#38271;&#26399;&#24179;&#22343;&#21442;&#19982;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21576;&#29616;&#23454;&#39564;&#32467;&#26524;&#26469;&#26816;&#39564;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an iterative differentially private algorithm for client selection in federated settings. We consider a federated network wherein clients coordinate with a central server to complete a task; however, the clients decide whether to participate or not at a time step based on their preferences -- local computation and probabilistic intent. The algorithm does not require client-to-client information exchange. The developed algorithm provides near-optimal values to the clients over long-term average participation with a certain differential privacy guarantee. Finally, we present the experimental results to check the algorithm's efficacy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#22312;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#20381;&#36182;&#20110;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2310.09358</link><description>&lt;p&gt;
&#20309;&#26102;&#25165;&#33021;&#20351;&#21095;&#26412;&#22312;&#38169;&#35823;&#35268;&#33539;&#19979;&#20445;&#25345;&#31283;&#23450;? (arXiv:2310.09358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
When are Bandits Robust to Misspecification?. (arXiv:2310.09358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09358
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#22312;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#20381;&#36182;&#20110;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#22870;&#21169;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#20915;&#31574;&#38382;&#39064;&#65292;&#22914;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#12290;&#36890;&#24120;&#30340;&#20551;&#35774;&#26159;&#21487;&#34892;&#24615;&#65292;&#21363;&#34892;&#20026;&#30340;&#30495;&#23454;&#22870;&#21169;&#23436;&#20840;&#30001;&#26576;&#20010;&#21442;&#25968;&#21270;&#27169;&#22411;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#31867;&#20043;&#38388;&#23384;&#22312;&#65288;&#21487;&#33021;&#26174;&#33879;&#65289;&#30340;&#35823;&#24046;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20381;&#36182;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#22312;&#21363;&#20351;&#22870;&#21169;&#23384;&#22312;&#20005;&#37325;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#35777;&#27425;&#32447;&#24615;&#65288;&#27425;&#20110;&#26102;&#38388;&#33539;&#22260;&#65289;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;&#36825;&#19982;&#29616;&#26377;&#30340;&#38024;&#23545;&#38169;&#35823;&#35268;&#33539;&#30340;&#26368;&#22351;&#24773;&#20917;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#26174;&#31034;&#36951;&#25022;&#36793;&#30028;&#38543;&#26102;&#38388;&#25104;&#32447;&#24615;&#27604;&#20363;&#22686;&#38271;&#65292;&#24182;&#19988;&#35828;&#26126;&#23384;&#22312;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#24378;&#30423;&#38382;&#39064;&#23454;&#20363;&#38598;&#21512;&#22312;&#38169;&#35823;&#35268;&#33539;&#19979;&#20173;&#28982;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parametric feature-based reward models are widely employed by algorithms for decision making settings such as bandits and contextual bandits. The typical assumption under which they are analysed is realizability, i.e., that the true rewards of actions are perfectly explained by some parametric model in the class. We are, however, interested in the situation where the true rewards are (potentially significantly) misspecified with respect to the model class. For parameterized bandits and contextual bandits, we identify sufficient conditions, depending on the problem instance and model class, under which classic algorithms such as $\epsilon$-greedy and LinUCB enjoy sublinear (in the time horizon) regret guarantees under even grossly misspecified rewards. This is in contrast to existing worst-case results for misspecified bandits which show regret bounds that scale linearly with time, and shows that there can be a nontrivially large set of bandit instances that are robust to misspecificati
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#27604;&#38646;-shot&#30340;&#38646;&#35757;&#32451;&#25968;&#25454;&#26041;&#24335;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25152;&#26377;&#24773;&#26223;&#19979;&#37117;&#34920;&#29616;&#20986;&#36739;&#22823;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.09350</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaption for Neural Information Retrieval. (arXiv:2310.09350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09350
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#27604;&#38646;-shot&#30340;&#38646;&#35757;&#32451;&#25968;&#25454;&#26041;&#24335;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25152;&#26377;&#24773;&#26223;&#19979;&#37117;&#34920;&#29616;&#20986;&#36739;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#38656;&#35201;&#26114;&#36149;&#30340;&#23545;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#27880;&#37322;&#30340;&#25968;&#25454;&#25165;&#33021;&#20445;&#25345;&#31454;&#20105;&#21147;&#12290;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#23383;&#31526;&#20018;&#25805;&#20316;&#36827;&#34892;&#26597;&#35810;&#29983;&#25104;&#30340;&#21512;&#25104;&#27880;&#37322;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#30340;&#30456;&#23545;&#20248;&#21183;&#23578;&#26410;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#25991;&#20351;&#29992;&#30456;&#21516;&#30340;&#31070;&#32463;IR&#26550;&#26500;&#30452;&#25509;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;BEIR&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#21253;&#25324;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#25506;&#32034;&#20102;&#20004;&#31181;&#24773;&#26223;&#65306;&#38646;-shot&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30417;&#30563;&#31995;&#32479;&#22312;&#22823;&#35268;&#27169;&#30340;&#31867;&#20284;&#39046;&#22495;&#25968;&#25454;&#38598;&#65288;MS-MARCO&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;; &#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38500;&#20102;MS-MARCO&#65292;&#31995;&#32479;&#36824;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25152;&#26377;&#24773;&#26223;&#20013;&#37117;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#19982;&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#24212;&#29992;&#30417;&#30563;IR&#31995;&#32479;&#30456;&#27604;&#65292;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural information retrieval requires costly annotated data for each target domain to be competitive. Synthetic annotation by query generation using Large Language Models or rule-based string manipulation has been proposed as an alternative, but their relative merits have not been analysed. In this paper, we compare both methods head-to-head using the same neural IR architecture. We focus on the BEIR benchmark, which includes test datasets from several domains with no training data, and explore two scenarios: zero-shot, where the supervised system is trained in a large out-of-domain dataset (MS-MARCO); and unsupervised domain adaptation, where, in addition to MS-MARCO, the system is fine-tuned in synthetic data from the target domain. Our results indicate that Large Language Models outperform rule-based methods in all scenarios by a large margin, and, more importantly, that unsupervised domain adaptation is effective compared to applying a supervised IR system in a zero-shot fashion. I
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;&#30340;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.09343</link><description>&lt;p&gt;
&#23545;&#24120;&#35782;&#24863;&#30693;&#23545;&#35805;&#20195;&#29702;&#30340;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents. (arXiv:2310.09343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;&#30340;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20154;&#31867;&#21270;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38656;&#35201;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#22238;&#24212;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#26679;&#30340;&#36830;&#36143;&#24615;&#21644;&#20449;&#24687;&#21547;&#37327;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#21363;&#20351;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19968;&#20010;&#21333;&#36339;&#20869;&#35782;&#21035;&#21644;&#32858;&#21512;&#20851;&#38190;&#35777;&#25454;&#30340;&#20219;&#21153;&#20063;&#26159;&#20855;&#26377;&#30456;&#24403;&#22823;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#20010;&#22797;&#26434;&#24615;&#30340;&#21407;&#22240;&#26159;&#36825;&#26679;&#30340;&#35777;&#25454;&#20998;&#25955;&#22312;&#23545;&#35805;&#30340;&#22810;&#20010;&#36718;&#27425;&#20013;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#22810;&#20010;&#36339;&#20013;&#36827;&#34892;&#25972;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#28966;&#28857;&#26159;&#20419;&#36827;&#23545;&#35805;&#19978;&#30340;&#22810;&#36339;&#25512;&#29702;&#65292;&#21363;&#23545;&#35805;&#24605;&#36335;&#65288;CoT&#65289;&#25512;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#19981;&#21487;&#38752;&#30340;&#25945;&#24072;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;DOCTOR&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#30340;CoT&#29702;&#30001;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-like chatbots necessitate the use of commonsense reasoning in order to effectively comprehend and respond to implicit information present within conversations. Achieving such coherence and informativeness in responses, however, is a non-trivial task. Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge. This complexity arises because such evidence is scattered across multiple turns in a conversation, thus necessitating integration over multiple hops. Hence, our focus is to facilitate such multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought (CoT) reasoning. To this end, we propose a knowledge distillation framework that leverages LLMs as unreliable teachers and selectively distills consistent and helpful rationales via alignment filters. We further present DOCTOR, a DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for response generation. We
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09342</link><description>&lt;p&gt;
&#20026;&#31243;&#24207;&#39564;&#35777;&#23545;LLM&#29983;&#25104;&#30340;&#24490;&#29615;&#19981;&#21464;&#24335;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking LLM-Generated Loop Invariants for Program Verification. (arXiv:2310.09342v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#24402;&#32435;&#24490;&#29615;&#19981;&#21464;&#37327;&#26159;&#33258;&#21160;&#21270;&#31243;&#24207;&#39564;&#35777;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;gpt-3.5&#25110;gpt-4&#65289;&#33021;&#22815;&#22312;0-shot&#29615;&#22659;&#19979;&#20026;&#19968;&#31867;&#31243;&#24207;&#21512;&#25104;&#24490;&#29615;&#19981;&#21464;&#37327;&#65292;&#20294;&#38656;&#35201;&#22810;&#20010;&#26679;&#26412;&#25165;&#33021;&#29983;&#25104;&#27491;&#30830;&#30340;&#19981;&#21464;&#37327;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#35843;&#29992;&#31243;&#24207;&#39564;&#35777;&#22120;&#26469;&#24314;&#31435;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25490;&#21517;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#38382;&#39064;&#23450;&#20041;&#21306;&#20998;&#27491;&#30830;&#30340;&#24402;&#32435;&#19981;&#21464;&#37327;&#21644;&#38169;&#35823;&#30340;&#23581;&#35797;&#12290;&#35813;&#25490;&#21517;&#22120;&#32463;&#36807;&#23545;&#27604;&#25490;&#21517;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#37325;&#26032;&#25490;&#21517;&#26426;&#21046;&#26174;&#33879;&#25552;&#39640;&#20102;&#27491;&#30830;&#19981;&#21464;&#37327;&#22312;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#20013;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#22823;&#24133;&#20943;&#23569;&#20102;&#23545;&#39564;&#35777;&#22120;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing inductive loop invariants is fundamental to automating program verification. In this work, we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number of calls to a program verifier to establish an invariant. To address this issue, we propose a {\it re-ranking} approach for the generated results of LLMs. We have designed a ranker that can distinguish between correct inductive invariants and incorrect attempts based on the problem definition. The ranker is optimized as a contrastive ranker. Experimental results demonstrate that this re-ranking mechanism significantly improves the ranking of correct invariants among the generated candidates, leading to a notable reduction in the number of calls to a verifier.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#29983;&#25104;&#33945;&#29305;&#21345;&#27931;(IGMC)&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#24182;&#23558;&#20854;&#36755;&#20986;&#28155;&#21152;&#21040;&#25968;&#25454;&#38598;&#20013;&#65292;IGMC&#33021;&#35745;&#31639;&#38543;&#26426;&#21464;&#37327;&#26399;&#26395;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#22312;MNIST&#25968;&#23383;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09338</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification using Generative Approach. (arXiv:2310.09338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09338
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#29983;&#25104;&#33945;&#29305;&#21345;&#27931;(IGMC)&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#24182;&#23558;&#20854;&#36755;&#20986;&#28155;&#21152;&#21040;&#25968;&#25454;&#38598;&#20013;&#65292;IGMC&#33021;&#35745;&#31639;&#38543;&#26426;&#21464;&#37327;&#26399;&#26395;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#24182;&#22312;MNIST&#25968;&#23383;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#37327;&#29983;&#25104;Monte Carlo (IGMC) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#26469;&#27979;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;IGMC&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36755;&#20986;&#28155;&#21152;&#21040;&#25968;&#25454;&#38598;&#20013;&#65292;&#20197;&#35745;&#31639;&#38543;&#26426;&#21464;&#37327;&#26399;&#26395;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;IGMC&#30456;&#23545;&#20110;&#26679;&#26412;&#22823;&#23567;&#21644;&#25277;&#26679;&#28145;&#24230;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#30001;&#20110;&#20854;&#19982;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#30340;&#20860;&#23481;&#24615;&#65292;IGMC&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;MNIST&#25968;&#23383;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;IGMC&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Incremental Generative Monte Carlo (IGMC) method, designed to measure uncertainty in deep neural networks using deep generative approaches. IGMC iteratively trains generative models, adding their output to the dataset, to compute the posterior distribution of the expectation of a random variable. We provide a theoretical guarantee of the convergence rate of IGMC relative to the sample size and sampling depth. Due to its compatibility with deep generative approaches, IGMC is adaptable to both neural network classification and regression tasks. We empirically study the behavior of IGMC on the MNIST digit classification task.
&lt;/p&gt;</description></item><item><title>ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2310.09298</link><description>&lt;p&gt;
ByteStack-ID: &#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#21033;&#29992;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection. (arXiv:2310.09298v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09298
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;"ByteStack-ID"&#65292;&#19968;&#31181;&#19987;&#20026;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;ByteStack-ID&#26680;&#24515;&#26159;&#21033;&#29992;&#20174;&#36127;&#36733;&#25968;&#25454;&#30340;&#39057;&#29575;&#20998;&#24067;&#29983;&#25104;&#30340;&#28784;&#24230;&#22270;&#20687;&#65292;&#36825;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#25216;&#26415;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#22797;&#26434;&#25968;&#25454;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#22522;&#20110;&#25968;&#25454;&#21253;&#32423;&#20449;&#24687;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#27969;&#37327;&#25968;&#25454;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#22522;&#26412;&#22534;&#21472;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;ByteStack-ID&#19982;&#20256;&#32479;&#30340;&#22534;&#21472;&#26041;&#27861;&#19981;&#21516;&#12290;&#23427;&#23558;&#38468;&#21152;&#30340;&#20803;&#23398;&#20064;&#22120;&#23618;&#26080;&#32541;&#38598;&#25104;&#21040;&#36830;&#25509;&#30340;&#22522;&#30784;&#23398;&#20064;&#22120;&#20013;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#20248;&#21270;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces "ByteStack-ID," a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstandin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09297</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#35748;&#30693;&#65306;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#25512;&#29702;&#31070;&#32463;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms. (arXiv:2310.09297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23558;&#24403;&#21069;&#30340;&#36755;&#20837;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#65292;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#24863;&#30693;&#21040;&#30340;&#20449;&#24687;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#65292;&#36825;&#26159;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#12290;&#21463;&#21040;&#20154;&#33041;&#35760;&#24518;&#31995;&#32479;&#21644;&#35748;&#30693;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#30340;PMI&#26694;&#26550;&#12290;&#29305;&#21035;&#22320;&#65292;&#35760;&#24518;&#27169;&#22359;&#21253;&#25324;&#24037;&#20316;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20854;&#20013;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#38454;&#30340;&#32467;&#26500;&#26469;&#20445;&#30041;&#26356;&#22810;&#30340;&#32047;&#31215;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#36890;&#36807;&#21487;&#21306;&#20998;&#30340;&#31454;&#20105;&#20889;&#20837;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#24863;&#30693;&#26356;&#26032;&#24037;&#20316;&#35760;&#24518;&#65292;&#20043;&#21518;&#36890;&#36807;&#22806;&#31215;&#20851;&#32852;&#19982;&#38271;&#26399;&#35760;&#24518;&#34701;&#21512;&#65292;&#36991;&#20813;&#20869;&#23384;&#28322;&#20986;&#24182;&#26368;&#23567;&#21270;&#20449;&#24687;&#20914;&#31361;&#12290;&#22312;&#25512;&#29702;&#27169;&#22359;&#20013;&#65292;&#30456;&#20851;&#20449;&#24687;&#20174;&#20004;&#20010;&#21333;&#29420;&#30340;&#35760;&#24518;&#28304;&#26816;&#32034;&#24182;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#26356;&#20840;&#38754;&#21644;&#31934;&#30830;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, averting memory overflow and minimizing information conflicts. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#23548;&#33322;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20174;&#25991;&#26412;&#25351;&#20196;&#21040;&#21160;&#20316;&#24863;&#30693;&#36793;&#30028;&#26694;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#26080;&#38656;&#24494;&#35843;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21010;&#20998;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#65292;&#29983;&#25104;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#20197;&#29983;&#25104;&#21487;&#34892;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.08873</link><description>&lt;p&gt;
&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models. (arXiv:2310.08873v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#23548;&#33322;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20174;&#25991;&#26412;&#25351;&#20196;&#21040;&#21160;&#20316;&#24863;&#30693;&#36793;&#30028;&#26694;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#26080;&#38656;&#24494;&#35843;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21010;&#20998;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#65292;&#29983;&#25104;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#20197;&#29983;&#25104;&#21487;&#34892;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#23548;&#33322;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(GPT-3.5)&#21644;&#24320;&#25918;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(&#22522;&#20110;Grounding DINO)&#21019;&#24314;&#20102;&#19968;&#20010;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#65292;&#29992;&#20110;&#36827;&#34892;&#26377;&#25928;&#30340;&#36335;&#24452;&#35268;&#21010;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#36890;&#36807;&#22823;&#22411;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20174;&#25991;&#26412;&#25351;&#20196;&#65288;&#20363;&#22914;&#8220;&#20320;&#33021;&#36890;&#36807;&#31383;&#24088;&#32473;&#25105;&#36865;&#33647;&#21527;&#65311;&#8221;&#65289;&#21040;&#20855;&#26377;&#21160;&#20316;&#24863;&#30693;&#23646;&#24615;&#30340;&#36793;&#30028;&#26694;&#65288;&#20363;&#22914;&#31383;&#24088;&#65289;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#23558;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#20998;&#25104;&#20004;&#37096;&#20998;&#65306;&#21487;&#36890;&#34892;&#21644;&#19981;&#21487;&#36890;&#34892;&#37096;&#20998;&#65292;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#29992;&#20110;&#29983;&#25104;&#21487;&#34892;&#36335;&#24452;&#12290;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#24555;&#36895;&#37096;&#32626;&#20110;&#20132;&#20114;&#24335;&#23548;&#33322;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;&#22810;&#20010;&#21487;&#36890;&#34892;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like "Can you pass through the curtains to deliver medicines to me?", to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable object
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08782</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#39537;&#21160;&#29983;&#20135;&#21147;&#65306;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning. (arXiv:2310.08782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#22522;&#30784;&#35774;&#26045;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20462;&#21098;&#65288;DP&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20986;&#29616;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#21024;&#38500;&#20887;&#20313;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;DP&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19979;&#28216;&#30446;&#26631;&#20219;&#21153;&#20013;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#21644;&#23436;&#25972;&#24494;&#35843;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20462;&#21098;&#28304;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;DP&#38382;&#39064;&#20173;&#28982;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;DP&#21644;&#36801;&#31227;&#23398;&#20064;&#35270;&#20026;&#29420;&#31435;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#23558;DP&#19982;&#36801;&#31227;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;DP&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;DP&#26041;&#27861;&#65292;&#21363;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#65292;&#29992;&#20110;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, i.e., how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;TD3&#31639;&#27861;&#30340;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#65292;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#20013;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#31283;&#23450;&#25910;&#25947;&#21644;&#25913;&#36827;&#23433;&#20840;&#24615;&#33021;&#65292;&#24182;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#31561;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08595</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#20132;&#21449;&#36335;&#21475;&#23548;&#33322;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation. (arXiv:2310.08595v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;TD3&#31639;&#27861;&#30340;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#65292;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#20013;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#31283;&#23450;&#25910;&#25947;&#21644;&#25913;&#36827;&#23433;&#20840;&#24615;&#33021;&#65292;&#24182;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#31561;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23494;&#38598;&#20132;&#36890;&#22330;&#26223;&#20013;&#65292;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#26102;&#22320;&#20351;AVs&#20570;&#20986;&#23433;&#20840;&#39640;&#25928;&#30340;&#20915;&#31574;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;TD3&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20302;&#25104;&#26412;&#12289;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;T&#22411;&#36335;&#21475;&#19978;&#30340;&#39640;&#25928;&#23433;&#20840;&#23548;&#33322;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25105;&#20204;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#19978;&#23545;&#25105;&#20204;&#30340;TD3&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;&#35813;&#26041;&#27861;&#21576;&#29616;&#20986;&#31283;&#23450;&#30340;&#25910;&#25947;&#24615;&#21644;&#25913;&#36827;&#30340;&#23433;&#20840;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#23494;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;AV&#33021;&#22815;&#26377;&#25928;&#22320;&#23548;&#33322;T&#22411;&#36335;&#21475;&#65292;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23545;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#39046;&#22495;&#30340;&#24212;&#29992;&#36129;&#29486;&#20102;&#26032;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs). Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time. Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm. We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities. Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost. This study contributes to the growing body of knowledge on reinforceme
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#36866;&#29992;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#22788;&#29702;&#36825;&#20123;&#24046;&#24322;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08459</link><description>&lt;p&gt;
&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Heterogeneous Transfer Learning. (arXiv:2310.08459v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08459
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#36866;&#29992;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#22788;&#29702;&#36825;&#20123;&#24046;&#24322;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;&#24212;&#29992;&#22312;&#24456;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#23427;&#21033;&#29992;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#30446;&#26631;&#39046;&#22495;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#20849;&#20139;&#30693;&#35782;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#20004;&#20010;&#39046;&#22495;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#65292;&#21363;&#21516;&#36136;&#36801;&#31227;&#23398;&#20064;&#65292;&#20294;&#36825;&#24182;&#19981;&#24635;&#26159;&#29616;&#23454;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;&#36890;&#24120;&#65292;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#22312;&#29305;&#24449;&#31354;&#38388;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#33719;&#21462;&#20855;&#26377;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#21516;&#29305;&#24449;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#28304;&#39046;&#22495;&#25968;&#25454;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#26114;&#36149;&#12290;&#23545;&#36825;&#20123;&#24046;&#24322;&#36827;&#34892;&#38543;&#24847;&#30340;&#28040;&#38500;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#25110;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#24212;&#23545;&#36825;&#31181;&#24046;&#24322;&#30340;&#26041;&#27861;&#24050;&#32463;&#23853;&#38706;&#22836;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios. The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies. These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption. Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain. Arbitrary elimination of these differences is not always feasible or optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SegLoc&#65292;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26032;&#39062;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.08421</link><description>&lt;p&gt;
SegLoc: &#26032;&#39062;&#30340;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SegLoc: Novel Visual Self-supervised Learning Scheme for Dense Prediction Tasks of Security Inspection X-ray Images. (arXiv:2310.08421v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08421
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SegLoc&#65292;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26032;&#39062;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#23637;&#24402;&#21151;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#30340;&#25972;&#21512;&#12290;&#23613;&#31649;&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20294;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#30456;&#27604;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#36824;&#19981;&#33021;&#20445;&#25345;&#30456;&#24212;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#36229;&#36234;&#26377;&#30417;&#30563;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#25913;&#36827;&#37117;&#23616;&#38480;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#33268;&#21147;&#20110;&#35780;&#20272;&#35745;&#31639;&#26426;&#35270;&#35273;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#21253;&#21547;&#31867;&#21035;&#20154;&#20687;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#23588;&#20854;&#26159;ImageNet&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;SegLoc&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, remarkable advancements of artificial intelligence have been attributed to the integration of self-supervised learning scheme. Despite impressive achievements within NLP, yet SSL in computer vision has not been able to stay on track comparatively. Recently, integration of contrastive learning on top of existing SSL models has established considerable progress in computer vision through which visual SSL models have outperformed their supervised counterparts. Nevertheless, most of these improvements were limited to classification tasks, and also, few works have been dedicated to evaluation of SSL models in real-world scenarios of computer vision, while the majority of works are centered around datasets containing class-wise portrait images, most notably, ImageNet. Consequently, in this work, we have considered dense prediction task of semantic segmentation in security inspection x-ray images to evaluate our proposed model Segmentation Localization. Based upon the model Instance L
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.08419</link><description>&lt;p&gt;
&#22312;&#20108;&#21313;&#20010;&#26597;&#35810;&#20013;&#30772;&#35299;&#40657;&#30418;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Black Box Large Language Models in Twenty Queries. (arXiv:2310.08419v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08419
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#30772;&#35299;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#36843;&#20351;LLMs&#36229;&#36234;&#20854;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#36825;&#20123;&#28431;&#27934;&#23545;&#20110;&#29702;&#35299;&#22266;&#26377;&#24369;&#28857;&#24182;&#38450;&#27490;&#26410;&#26469;&#30340;&#19981;&#24403;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Automatic Iterative Refinement&#65288;PAIR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20165;&#36890;&#36807;&#23545;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#31639;&#27861;&#29983;&#25104;&#35821;&#20041;&#30772;&#35299;&#12290;PAIR&#21463;&#21040;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#20351;&#29992;&#25915;&#20987;&#32773;LLM&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#30446;&#26631;LLM&#30340;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#25915;&#20987;&#32773;LLM&#36890;&#36807;&#36845;&#20195;&#26597;&#35810;&#30446;&#26631;LLM&#26469;&#26356;&#26032;&#21644;&#25913;&#36827;&#20505;&#36873;&#30772;&#35299;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#36825;&#27604;&#29616;&#26377;&#31639;&#27861;&#39640;&#25928;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;PAIR&#36824;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#30772;&#35299;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbrea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#30340;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120; (H-STFormer) &#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#24314;&#27169;&#20102;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#22686;&#37327;&#23398;&#20064;&#21644;&#36716;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08328</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for Traffic Flow Prediction. (arXiv:2310.08328v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08328
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#30340;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120; (H-STFormer) &#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#24314;&#27169;&#20102;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#22686;&#37327;&#23398;&#20064;&#21644;&#36716;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#26680;&#24515;&#25216;&#26415;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#26159;&#26102;&#31354;&#25968;&#25454;&#65292;&#19981;&#20165;&#19982;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#31354;&#38388;&#20301;&#32622;&#30456;&#20851;&#65292;&#32780;&#19988;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#26159;&#26377;&#25928;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#25152;&#26377;&#22266;&#26377;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#30340;&#22686;&#37327;&#23398;&#20064;&#20960;&#20046;&#27809;&#26377;&#23581;&#35797;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#20063;&#24456;&#38590;&#36716;&#21270;&#21040;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20219;&#21153;&#20013;&#12290;&#21463;&#21040;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#25361;&#25112;&#21644;&#36947;&#36335;&#32593;&#32476;&#22266;&#26377;&#23646;&#24615;&#30340;&#28508;&#21147;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120;&#65288;H-STFormer&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
As a core technology of Intelligent Transportation System (ITS), traffic flow prediction has a wide range of applications. Traffic flow data are spatial-temporal, which are not only correlated to spatial locations in road networks, but also vary with temporal time indices. Existing methods have solved the challenges in traffic flow prediction partly, focusing on modeling spatial-temporal dependencies effectively, while not all intrinsic properties of traffic flow data are utilized fully. Besides, there are very few attempts at incremental learning of spatial-temporal data mining, and few previous works can be easily transferred to the traffic flow prediction task. Motivated by the challenge of incremental learning methods for traffic flow prediction and the underutilization of intrinsic properties of road networks, we propose a Transport-Hub-aware Spatial-Temporal adaptive graph transFormer (H-STFormer) for traffic flow prediction. Specifically, we first design a novel spatial self-att
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Promptor&#65292;&#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#20811;&#26381;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;GPT-3.5&#20026;&#20363;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#36890;&#36807;&#25552;&#31034;&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2310.08101</link><description>&lt;p&gt;
Promptor:&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques. (arXiv:2310.08101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Promptor&#65292;&#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#20811;&#26381;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;GPT-3.5&#20026;&#20363;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#36890;&#36807;&#25552;&#31034;&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#26085;&#24120;&#25968;&#23383;&#20132;&#20114;&#20013;&#65292;&#25991;&#26412;&#36755;&#20837;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20351;&#25991;&#26412;&#36755;&#20837;&#26356;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#27969;&#30021;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#26234;&#33021;&#21151;&#33021;&#65292;&#21253;&#25324;&#21477;&#23376;&#39044;&#27979;&#21644;&#29992;&#25143;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36825;&#20123;&#39640;&#32423;&#21151;&#33021;&#30340;&#24120;&#35268;&#65292;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#24517;&#35201;&#24615;&#20063;&#22686;&#21152;&#20102;&#12290;&#21033;&#29992;GPT-3.5&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#19968;&#29420;&#29305;&#30340;&#29305;&#24615;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#31034;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#65292;&#28040;&#38500;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#22240;&#27492;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#25991;&#26412;&#39044;&#27979;&#25216;&#26415;&#12290;&#25105;&#20204;&#26368;&#21021;&#23637;&#31034;&#20102;&#20165;&#36890;&#36807;&#25552;&#31034;GPT-3.5&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#24403;&#65292;&#22312;&#21518;&#20004;&#31181;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text entry is an essential task in our day-to-day digital interactions. Numerous intelligent features have been developed to streamline this process, making text entry more effective, efficient, and fluid. These improvements include sentence prediction and user personalization. However, as deep learning-based language models become the norm for these advanced features, the necessity for data collection and model fine-tuning increases. These challenges can be mitigated by harnessing the in-context learning capability of large language models such as GPT-3.5. This unique feature allows the language model to acquire new skills through prompts, eliminating the need for data collection and fine-tuning. Consequently, large language models can learn various text prediction techniques. We initially showed that, for a sentence prediction task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is comparable with a fine-tuned GPT-3.5 model, with the latter two methods requiring costly 
&lt;/p&gt;</description></item><item><title>Sentinel&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#24182;&#23450;&#20041;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#26469;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;Sentinel&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.08097</link><description>&lt;p&gt;
Sentinel: &#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#32858;&#21512;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Sentinel: An Aggregation Function to Secure Decentralized Federated Learning. (arXiv:2310.08097v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08097
&lt;/p&gt;
&lt;p&gt;
Sentinel&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#24182;&#23450;&#20041;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#26469;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;Sentinel&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24555;&#36895;&#25972;&#21512;&#21040;&#32593;&#32476;&#20013;&#28085;&#30422;&#20102;&#32593;&#32476;&#31649;&#29702;&#12289;&#26381;&#21153;&#36136;&#37327;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#21508;&#20010;&#26041;&#38754;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#21327;&#20316;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#28857;&#22833;&#25928;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;FL&#21644;DFL&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24615;&#21463;&#21040;&#27745;&#26579;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23545;&#20854;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#38024;&#23545;&#38598;&#20013;&#24335;FL&#36827;&#34892;&#35774;&#35745;&#65292;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;DFL&#30340;&#29305;&#28857;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;Sentinel&#65292;&#19968;&#31181;&#22312;DFL&#20013;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;Sentinel&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#65292;&#21253;&#25324;&#30456;&#20284;&#24615;&#36807;&#28388;&#12289;&#24341;&#23548;&#39564;&#35777;&#21644;&#26631;&#20934;&#21270;&#65292;&#20197;&#38450;&#27490;&#24694;&#24847;&#27169;&#22411;&#26356;&#26032;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#23545;Sentinel&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid integration of Federated Learning (FL) into networking encompasses various aspects such as network management, quality of service, and cybersecurity while preserving data privacy. In this context, Decentralized Federated Learning (DFL) emerges as an innovative paradigm to train collaborative models, addressing the single point of failure limitation. However, the security and trustworthiness of FL and DFL are compromised by poisoning attacks, negatively impacting its performance. Existing defense mechanisms have been designed for centralized FL and they do not adequately exploit the particularities of DFL. Thus, this work introduces Sentinel, a defense strategy to counteract poisoning attacks in DFL. Sentinel leverages the accessibility of local data and defines a three-step aggregation protocol consisting of similarity filtering, bootstrap validation, and normalization to safeguard against malicious model updates. Sentinel has been evaluated with diverse datasets and various 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07818</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#35782;&#21035;&#19982;&#21477;&#23376;&#32467;&#26500;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models. (arXiv:2310.07818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07818
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#31867;&#27604;&#22312;&#20154;&#31867;&#35748;&#30693;&#21644;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#23545;&#20110;&#8220;A&#23545;B&#23601;&#20687;C&#23545;D&#8221;&#36825;&#31181;&#24418;&#24335;&#30340;&#35789;&#35821;&#31867;&#27604;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28041;&#21450;&#26356;&#38271;&#25991;&#26412;&#30340;&#31867;&#27604;&#65292;&#22914;&#21477;&#23376;&#21644;&#21477;&#23376;&#38598;&#21512;&#65292;&#20256;&#36798;&#31867;&#27604;&#24847;&#20041;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#31038;&#21306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35782;&#21035;&#27492;&#31867;&#31867;&#27604;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#31350;&#12290;&#27492;&#22806;&#65292;LLMs&#22312;&#20854;&#23884;&#20837;&#20013;&#32534;&#30721;&#35821;&#35328;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLMs&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#31867;&#27604;&#35782;&#21035;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#33021;&#21147;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of ``A is to B as C is to D.'' However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20102;Siamese&#32593;&#32476;&#21644;Grad-CAM&#65292;&#33021;&#22815;&#25552;&#20379;&#30456;&#20284;&#24615;&#20998;&#25968;&#20197;&#21450;&#35270;&#35273;&#19978;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#26377;&#28508;&#21147;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.07678</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;:&#25972;&#21512;Siamese&#32593;&#32476;&#21644;Grad-CAM
&lt;/p&gt;
&lt;p&gt;
Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM. (arXiv:2310.07678v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20102;Siamese&#32593;&#32476;&#21644;Grad-CAM&#65292;&#33021;&#22815;&#25552;&#20379;&#30456;&#20284;&#24615;&#20998;&#25968;&#20197;&#21450;&#35270;&#35273;&#19978;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#26377;&#28508;&#21147;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#20687;&#24212;&#29992;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#38590;&#20197;&#29702;&#35299;&#20026;&#20309;&#35748;&#20026;&#20004;&#20010;&#22270;&#20687;&#30456;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#30340;&#27010;&#24565;&#65292;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#30456;&#20284;&#24615;&#20998;&#25968;&#20197;&#21450;&#35270;&#35273;&#19978;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;Siamese&#32593;&#32476;&#21644;Grad-CAM&#25972;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#30456;&#20284;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#22312;&#21033;&#30410;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#26694;&#26550;&#25552;&#20379;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35752;&#35770;&#65292;&#20197;&#36741;&#21161;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of image-based applications in various domains, the need for accurate and interpretable image similarity measures has become increasingly critical. Existing image similarity models often lack transparency, making it challenging to understand the reasons why two images are considered similar. In this paper, we propose the concept of explainable image similarity, where the goal is the development of an approach, which is capable of providing similarity scores along with visual factual and counterfactual explanations. Along this line, we present a new framework, which integrates Siamese Networks and Grad-CAM for providing explainable image similarity and discuss the potential benefits and challenges of adopting this approach. In addition, we provide a comprehensive discussion about factual and counterfactual explanations provided by the proposed framework for assisting decision making. The proposed approach has the potential to enhance the interpretability, trustwor
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#40657;&#30418;&#23376;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#65292;&#23458;&#35266;&#35780;&#20272;&#20102;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.07534</link><description>&lt;p&gt;
XAI&#26041;&#27861;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human-Centered Evaluation of XAI Methods. (arXiv:2310.07534v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07534
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#40657;&#30418;&#23376;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#65292;&#23458;&#35266;&#35780;&#20272;&#20102;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#35299;&#26512;&#28145;&#24230;&#23398;&#20064;&#20013;&#25152;&#35859;&#30340;&#8220;&#40657;&#30418;&#23376;&#8221;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#37322;&#21508;&#31181;&#20219;&#21153;&#30340;&#20915;&#31574;&#12290;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20250;&#35782;&#21035;&#24182;&#24378;&#35843;&#23545;&#20998;&#31867;&#22120;&#39044;&#27979;&#24433;&#21709;&#26368;&#22823;&#30340;&#20851;&#38190;&#20687;&#32032;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#20154;&#31867;&#34892;&#20026;&#30456;&#20284;&#65306;&#24403;&#25105;&#20204;&#34987;&#35201;&#27714;&#35299;&#37322;&#20998;&#31867;&#22270;&#20687;&#30340;&#29702;&#30001;&#26102;&#65292;&#25105;&#20204;&#36890;&#24120;&#20250;&#25351;&#20986;&#26368;&#26174;&#33879;&#30340;&#29305;&#24449;&#25110;&#26041;&#38754;&#12290;&#21033;&#29992;&#36825;&#31181;&#31867;&#20284;&#24615;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35797;&#22270;&#23458;&#35266;&#22320;&#35780;&#20272;&#19977;&#31181;&#39046;&#20808;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#65288;1&#65289;&#20856;&#22411;&#23616;&#37096;&#32593;&#32476;&#12289;&#65288;2&#65289;&#36974;&#25377;&#21644;&#65288;3&#65289;&#23618;&#27425;&#30456;&#20851;&#20256;&#25773;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#25152;&#31361;&#20986;&#30340;&#21306;&#22495;&#21487;&#33021;&#24046;&#24322;&#24456;&#22823;&#65292;&#20294;&#23427;&#20204;&#37117;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called "black boxes" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offe
&lt;/p&gt;</description></item><item><title>&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.07325</link><description>&lt;p&gt;
&#30452;&#25509;&#36923;&#36753;&#23646;&#24615;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65306;gelu-4l&#20013;&#30340;&#20869;&#23384;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07325
&lt;/p&gt;
&lt;p&gt;
&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;4&#23618;transformer&#20013;&#20869;&#23384;&#31649;&#29702;&#30340;&#20855;&#20307;&#35777;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32452;&#20214;&#19968;&#33268;&#22320;&#31227;&#38500;&#21069;&#38754;&#32452;&#20214;&#30340;&#36755;&#20986;&#65292;&#36825;&#26159;&#19968;&#31181;&#28165;&#29702;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#24615;&#25216;&#26415;Direct Logit Attribution&#25552;&#20379;&#20102;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#31181;&#25216;&#26415;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#28165;&#29702;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPEHF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#29616;&#26377;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.07123</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation for Human Feedback. (arXiv:2310.07123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPEHF&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#29616;&#26377;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#20449;&#21495;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31163;&#32447;&#35757;&#32451;&#21644;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#30340;&#32553;&#23567;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#36890;&#36807;&#20165;&#20351;&#29992;&#31163;&#32447;&#36712;&#36857;&#20272;&#35745;&#30446;&#26631;&#65288;&#35780;&#20272;&#65289;&#31574;&#30053;&#30340;&#24615;&#33021;&#21644;/&#25110;&#25490;&#21517;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#21644;&#31574;&#30053;&#27979;&#35797;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#65292;&#22312;&#22312;&#32447;&#37096;&#32626;&#25104;&#26412;&#36739;&#39640;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;OPE&#26041;&#27861;&#22312;&#20272;&#35745;&#20154;&#31867;&#21453;&#39304;&#65288;HF&#65289;&#20449;&#21495;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;HF&#21487;&#33021;&#20250;&#21463;&#21040;&#22810;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#21482;&#26159;&#31232;&#30095;&#21487;&#29992;&#30340;&#65307;&#32780;&#19981;&#21516;&#20110;&#20195;&#29702;&#23450;&#20041;&#30340;&#29615;&#22659;&#22870;&#21169;&#65288;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#65289;&#65292;&#29615;&#22659;&#22870;&#21169;&#36890;&#24120;&#26159;&#22312;&#21442;&#25968;&#20989;&#25968;&#25110;&#20998;&#24067;&#19978;&#20915;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;HF&#20449;&#21495;&#30340;&#24615;&#36136;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;OPE&#20272;&#35745;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;HF&#30340;OPE&#26694;&#26550;&#65292;&#23427;&#37325;&#26032;&#20351;&#29992;&#29616;&#26377;&#30340;OPE&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#35780;&#20272;HF&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;
&lt;/p&gt;
&lt;p&gt;
Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and is only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we deve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30340;&#20223;&#30495;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#25193;&#23637;&#21644;&#27880;&#37322;&#36923;&#36753;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#31561;&#38382;&#39064;&#12290;&#19982;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#31574;&#30053;&#36136;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#24314;&#27169;&#21644;&#21033;&#29992;&#38750;&#39532;&#23572;&#21487;&#22827;&#21160;&#21147;&#23398;&#21644;&#21363;&#26102;&#21160;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20195;&#29702;&#34892;&#20026;&#32467;&#26524;&#30165;&#36857;&#12290;</title><link>http://arxiv.org/abs/2310.06835</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#35821;&#20041;&#38750;&#39532;&#23572;&#21487;&#22827;&#20223;&#30495;&#20195;&#29702;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Semantic Non-Markovian Simulation Proxy for Reinforcement Learning. (arXiv:2310.06835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30340;&#20223;&#30495;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#25193;&#23637;&#21644;&#27880;&#37322;&#36923;&#36753;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#31561;&#38382;&#39064;&#12290;&#19982;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#31574;&#30053;&#36136;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#24314;&#27169;&#21644;&#21033;&#29992;&#38750;&#39532;&#23572;&#21487;&#22827;&#21160;&#21147;&#23398;&#21644;&#21363;&#26102;&#21160;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20195;&#29702;&#34892;&#20026;&#32467;&#26524;&#30165;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#24456;&#22810;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#31561;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#22312;&#26576;&#20123;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#32570;&#28857;&#22823;&#22810;&#26469;&#33258;&#20110;&#27169;&#25311;&#22120;&#32780;&#19981;&#26159;RL&#35757;&#32451;&#31639;&#27861;&#26412;&#36523;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#27880;&#36923;&#36753;&#30340;&#26102;&#38388;&#25193;&#23637;&#30340;&#35821;&#20041;&#20195;&#29702;&#26469;&#36827;&#34892;&#20223;&#30495;&#12290;&#19982;&#20004;&#20010;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#23398;&#20064;&#30340;&#31574;&#30053;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33021;&#22815;&#24314;&#27169;&#21644;&#21033;&#29992;&#38750;&#39532;&#23572;&#21487;&#22827;&#21160;&#21147;&#23398;&#21644;&#21363;&#26102;&#21160;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#33021;&#22815;&#35299;&#37322;&#20195;&#29702;&#34892;&#20026;&#32467;&#26524;&#30340;&#30165;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in reinforcement learning (RL) have shown much promise across a variety of applications. However, issues such as scalability, explainability, and Markovian assumptions limit its applicability in certain domains. We observe that many of these shortcomings emanate from the simulator as opposed to the RL training algorithms themselves. As such, we propose a semantic proxy for simulation based on a temporal extension to annotated logic. In comparison with two high-fidelity simulators, we show up to three orders of magnitude speed-up while preserving the quality of policy learned. In addition, we show the ability to model and leverage non-Markovian dynamics and instantaneous actions while providing an explainable trace describing the outcomes of the agent actions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#27010;&#24565;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#26368;&#23567;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.05351</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#31867;&#21035;&#19979;&#30340;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Generalized Neural Collapse for a Large Number of Classes. (arXiv:2310.05351v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#27010;&#24565;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#26368;&#23567;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#25552;&#20379;&#20102;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#26368;&#21518;&#19968;&#23618;&#34920;&#31034;&#65288;&#21363;&#29305;&#24449;&#65289;&#21644;&#20998;&#31867;&#22120;&#26435;&#37325;&#30340;&#20248;&#38597;&#25968;&#23398;&#25551;&#36848;&#12290;&#36825;&#31181;&#32467;&#26524;&#19981;&#20165;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#65292;&#36824;&#28608;&#21457;&#20102;&#25913;&#36827;&#23454;&#38469;&#28145;&#24230;&#27169;&#22411;&#30340;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#31070;&#32463;&#23849;&#28291;&#30340;&#29616;&#26377;&#32463;&#39564;&#21644;&#29702;&#35770;&#30740;&#31350;&#37117;&#38598;&#20013;&#20110;&#31867;&#21035;&#25968;&#30456;&#23545;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#36739;&#23567;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#23558;&#31070;&#32463;&#23849;&#28291;&#25193;&#23637;&#21040;&#31867;&#21035;&#25968;&#36828;&#22823;&#20110;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#30340;&#24773;&#20917;&#65292;&#36825;&#22312;&#35821;&#35328;&#27169;&#22411;&#12289;&#26816;&#32034;&#31995;&#32479;&#21644;&#20154;&#33080;&#35782;&#21035;&#24212;&#29992;&#20013;&#24191;&#27867;&#20986;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#23637;&#29616;&#20986;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#65292;&#20854;&#20013;&#26368;&#23567;&#30340;&#19968;&#23545;&#20854;&#20182;&#31867;&#21035;&#38388;&#36793;&#30028;&#20540;&#34987;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#39564;&#35777;&#23454;&#38469;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#30340;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#20197;&#34920;&#26126;&#8230;.
&lt;/p&gt;
&lt;p&gt;
Neural collapse provides an elegant mathematical characterization of learned last layer representations (a.k.a. features) and classifier weights in deep classification models. Such results not only provide insights but also motivate new techniques for improving practical deep models. However, most of the existing empirical and theoretical studies in neural collapse focus on the case that the number of classes is small relative to the dimension of the feature space. This paper extends neural collapse to cases where the number of classes are much larger than the dimension of feature space, which broadly occur for language models, retrieval systems, and face recognition applications. We show that the features and classifier exhibit a generalized neural collapse phenomenon, where the minimum one-vs-rest margins is maximized.We provide empirical study to verify the occurrence of generalized neural collapse in practical deep neural networks. Moreover, we provide theoretical study to show tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.05317</link><description>&lt;p&gt;
&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26469;&#22686;&#24378;&#38271;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#20316;&#20026;&#19968;&#31181;&#26041;&#24335;&#65292;&#23558;&#29983;&#25104;&#27969;&#27700;&#32447;&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#22120;&#20174;&#22810;&#20010;&#32467;&#26524;&#20013;&#37319;&#26679;&#21487;&#21464;&#30340;&#20998;&#27573;&#65292;&#37319;&#26679;&#27010;&#29575;&#22522;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26500;&#24314;&#19987;&#29992;&#35789;&#27719;&#30340;&#31574;&#30053;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35789;&#27719;&#21512;&#24182;&#21327;&#35758;&#65292;&#21487;&#20197;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#25972;&#21512;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#35789;&#27493;&#39588;&#20013;&#12290;&#36890;&#36807;&#23545;&#20013;&#33521;&#25991;&#24515;&#29702;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26041;&#27861;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#65292;&#26368;&#39640;&#21487;&#36798;60%&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#35789;&#26041;&#27861;&#19982;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24471;&#21040;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#23433;&#20840;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20113;&#22522;&#30784;&#35774;&#26045;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#23433;&#20840;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#36890;&#20449;&#12290;&#23427;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#65292;&#36991;&#20813;&#20102;&#30452;&#25509;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.05269</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65306;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#30340;&#21069;&#27839;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications. (arXiv:2310.05269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05269
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#23433;&#20840;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20113;&#22522;&#30784;&#35774;&#26045;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#23433;&#20840;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#36890;&#20449;&#12290;&#23427;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#65292;&#36991;&#20813;&#20102;&#30452;&#25509;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#23458;&#25143;&#21644;&#20027;&#26426;&#36830;&#25509;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#39046;&#22495;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#23433;&#20840;&#20998;&#24067;&#24335;ML&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38544;&#31169;&#23433;&#20840;&#24615;&#12290;FL&#36890;&#36807;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#23558;ML&#27169;&#22411;&#20256;&#36755;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#65292;&#26377;&#25928;&#22320;&#23558;&#20113;&#22522;&#30784;&#35774;&#26045;&#19982;&#20013;&#24515;&#21270;&#21644;&#20998;&#25955;&#21270;&#31995;&#32479;&#30340;&#27969;&#31243;&#22788;&#29702;&#21644;&#25968;&#25454;&#23384;&#20648;&#38656;&#27714;&#30456;&#32467;&#21512;&#65292;&#27880;&#37325;&#21487;&#20280;&#32553;&#24615;&#12289;&#38544;&#31169;&#32771;&#34385;&#21644;&#32463;&#27982;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;&#22312;&#24403;&#21069;&#30340;FL&#23454;&#29616;&#20013;&#65292;&#25968;&#25454;&#25152;&#26377;&#32773;&#22312;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#20197;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#21442;&#25968;&#30340;&#24418;&#24335;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#12290;&#36825;&#31181;&#21019;&#26032;&#28040;&#38500;&#20102;&#19982;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#23458;&#25143;&#21644;&#21442;&#19982;&#32773;&#30452;&#25509;&#19982;&#20113;&#20013;&#24515;&#36890;&#20449;&#21407;&#22987;&#21644;&#28508;&#22312;&#26426;&#23494;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#19981;&#20165;&#38477;&#20302;&#20102;&#19982;&#19982;&#20256;&#32479;&#20113;&#20013;&#24515;&#36890;&#20449;&#30456;&#20851;&#30340;&#36153;&#29992;&#65292;&#36824;&#20445;&#25252;&#20102;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of machine learning (ML) systems featuring client-host connections, the enhancement of privacy security can be effectively achieved through federated learning (FL) as a secure distributed ML methodology. FL effectively integrates cloud infrastructure to transfer ML models onto edge servers using blockchain technology. Through this mechanism, it guarantees the streamlined processing and data storage requirements of both centralized and decentralized systems, with an emphasis on scalability, privacy considerations, and cost-effective communication. In current FL implementations, data owners locally train their models, and subsequently upload the outcomes in the form of weights, gradients, and parameters to the cloud for overall model aggregation. This innovation obviates the necessity of engaging Internet of Things (IoT) clients and participants to communicate raw and potentially confidential data directly with a cloud center. This not only reduces the costs associated with 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05128</link><description>&lt;p&gt;
&#23454;&#20363;&#21644;&#26631;&#31614;: &#38024;&#23545;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification. (arXiv:2310.05128v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65288;HMTC&#65289;&#26088;&#22312;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#36817;&#26399;&#20851;&#20110;HMTC&#30340;&#26041;&#27861;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#20197;&#21322;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#25991;&#26412;&#21644;&#26631;&#31614;&#23884;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#26045;&#21152;&#36807;&#24230;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26679;&#26412;&#30340;&#29983;&#25104;&#24448;&#24448;&#24341;&#20837;&#22122;&#22768;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#21516;&#19968;&#25209;&#27425;&#20013;&#30456;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26041;&#27861;&#26159;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#26631;&#31614;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\textbf{HJCL}$&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22635;&#34917;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;HMTC&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#20180;&#32454;&#26500;&#36896;&#25209;&#27425;&#26469;&#28385;&#36275;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a $\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive $\textbf{L}$earning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the 
&lt;/p&gt;</description></item><item><title>EMOFM&#26159;&#19968;&#20010;&#38598;&#25104;MLP&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#28151;&#21512;&#22120;&#23454;&#29616;&#20102;&#23383;&#27573;&#21644;&#31867;&#22411;&#29305;&#24449;&#30340;&#34701;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04482</link><description>&lt;p&gt;
EMOFM: &#24102;&#26377;&#22522;&#20110;&#29305;&#24449;&#30340;&#28151;&#21512;&#22120;&#30340;&#38598;&#25104;MLP&#27169;&#22411;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EMOFM: Ensemble MLP mOdel with Feature-based Mixers for Click-Through Rate Prediction. (arXiv:2310.04482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04482
&lt;/p&gt;
&lt;p&gt;
EMOFM&#26159;&#19968;&#20010;&#38598;&#25104;MLP&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#28151;&#21512;&#22120;&#23454;&#29616;&#20102;&#23383;&#27573;&#21644;&#31867;&#22411;&#29305;&#24449;&#30340;&#34701;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36947;&#19968;&#30340;CTI&#31454;&#36187;&#20851;&#27880;&#30340;&#26159;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#25968;&#30334;&#19975;&#26465;&#35760;&#24405;&#65292;&#27599;&#26465;&#35760;&#24405;&#20013;&#30340;&#27599;&#20010;&#23383;&#27573;&#29305;&#24449;&#37117;&#30001;&#21704;&#24076;&#25972;&#25968;&#32452;&#25104;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#32593;&#32476;&#26041;&#27861;&#30340;&#20851;&#38190;&#21487;&#33021;&#26159;&#25353;&#31867;&#22411;&#25552;&#21462;&#29305;&#24449;&#21644;&#36328;&#19981;&#21516;&#23383;&#27573;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#12290;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#33021;&#22815;&#25552;&#21462;&#23383;&#27573;&#29305;&#24449;&#65292;&#20294;&#26080;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#29305;&#24449;&#12290;&#21463;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#33258;&#28982;&#34701;&#21512;&#29305;&#24615;&#21644;&#22522;&#20110;Transformer&#32467;&#26500;&#30340;&#39640;&#25928;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25554;&#20214;&#28151;&#21512;&#22120;&#29992;&#20110;&#23383;&#27573;/&#31867;&#22411;&#29305;&#24449;&#34701;&#21512;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#23383;&#27573;&#21644;&#31867;&#22411;&#28151;&#21512;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#21363;EMOFM&#65288;&#24102;&#26377;&#22522;&#20110;&#29305;&#24449;&#30340;&#28151;&#21512;&#22120;&#30340;&#38598;&#25104;MLP&#27169;&#22411;&#65289;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23545;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21487;&#35270;&#21270;&#20102;&#20248;&#21270;&#36807;&#31243;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;EMOFM&#20248;&#20110;&#27604;&#36739;&#22522;&#32447;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Track one of CTI competition is on click-through rate (CTR) prediction. The dataset contains millions of records and each field-wise feature in a record consists of hashed integers for privacy. For this task, the keys of network-based methods might be type-wise feature extraction and information fusion across different fields. Multi-layer perceptrons (MLPs) are able to extract field feature, but could not efficiently fuse features. Motivated by the natural fusion characteristic of cross attention and the efficiency of transformer-based structures, we propose simple plug-in mixers for field/type-wise feature fusion, and thus construct an field&amp;type-wise ensemble model, namely EMOFM (Ensemble MLP mOdel with Feature-based Mixers). In the experiments, the proposed model is evaluated on the dataset, the optimization process is visualized and ablation studies are explored. It is shown that EMOFM outperforms compared baselines. In the end, we discuss on future work. WARNING: The comparison mi
&lt;/p&gt;</description></item><item><title>Swin-Tempo&#26159;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;&#12290;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32954;&#32467;&#33410;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03365</link><description>&lt;p&gt;
Swin-Tempo: &#20351;&#29992;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet. (arXiv:2310.03365v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03365
&lt;/p&gt;
&lt;p&gt;
Swin-Tempo&#26159;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;Swin Transformer-Enhanced UNet&#23558;CT&#25195;&#25551;&#20013;&#30340;&#32954;&#32467;&#33410;&#26816;&#27979;&#20316;&#20026;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#26102;&#38388;&#24863;&#30693;&#12290;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32954;&#32467;&#33410;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#20855;&#26377;&#26497;&#39640;&#30340;&#33268;&#27515;&#29575;&#65292;&#26089;&#26399;&#26816;&#27979;&#23545;&#20110;&#38450;&#27835;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#32780;&#35328;&#65292;&#35782;&#21035;&#32954;&#32467;&#33410;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#20182;&#20204;&#24448;&#24448;&#20381;&#36182;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#26469;&#36827;&#34892;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#31995;&#32479;&#24050;&#32463;&#20986;&#29616;&#65292;&#24110;&#21161;&#21307;&#29983;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#35782;&#21035;&#32954;&#32467;&#33410;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32593;&#32476;&#24448;&#24448;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#23548;&#33268;&#35823;&#25253;&#21644;&#28431;&#25253;&#29575;&#36739;&#39640;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#20248;&#21183;&#12290;&#21463;&#35270;&#39057;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;3D CT&#22270;&#20687;&#35270;&#20026;&#19968;&#20010;&#35270;&#39057;&#65292;&#23558;&#27599;&#20010;&#20999;&#29255;&#35270;&#20026;&#24103;&#65292;&#23558;&#32954;&#32467;&#33410;&#35270;&#20026;&#30446;&#26631;&#65292;&#23454;&#29616;&#19968;&#20010;&#26102;&#24207;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is highly lethal, emphasizing the critical need for early detection. However, identifying lung nodules poses significant challenges for radiologists, who rely heavily on their expertise and experience for accurate diagnosis. To address this issue, computer-aided diagnosis systems based on machine learning techniques have emerged to assist doctors in identifying lung nodules from computed tomography (CT) scans. Unfortunately, existing networks in this domain often suffer from computational complexity, leading to high rates of false negatives and false positives, limiting their effectiveness. To address these challenges, we present an innovative model that harnesses the strengths of both convolutional neural networks and vision transformers. Inspired by object detection in videos, we treat each 3D CT image as a video, individual slices as frames, and lung nodules as objects, enabling a time-series application. The primary objective of our work is to overcome hardware limitati
&lt;/p&gt;</description></item><item><title>&#22823;&#33041;&#20855;&#26377;&#19968;&#31995;&#21015;&#37325;&#22797;&#30340;&#35268;&#33539;&#35745;&#31639;&#21333;&#20803;&#65292;&#20294;&#31070;&#32463;&#34920;&#31034;&#26159;&#20998;&#24067;&#24335;&#30340;&#65292;&#22240;&#27492;&#22914;&#20309;&#23450;&#20041;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20174;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#20013;&#25512;&#26029;&#20986;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#22312;&#31639;&#27861;&#32423;&#21035;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#12290;&#36890;&#36807;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#65292;&#21487;&#20197;&#25214;&#21040;&#31070;&#32463;&#27963;&#21160;&#19982;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#22240;&#26524;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03186</link><description>&lt;p&gt;
&#25512;&#27979;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Inferring Inference. (arXiv:2310.03186v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03186
&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#20855;&#26377;&#19968;&#31995;&#21015;&#37325;&#22797;&#30340;&#35268;&#33539;&#35745;&#31639;&#21333;&#20803;&#65292;&#20294;&#31070;&#32463;&#34920;&#31034;&#26159;&#20998;&#24067;&#24335;&#30340;&#65292;&#22240;&#27492;&#22914;&#20309;&#23450;&#20041;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20174;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#20013;&#25512;&#26029;&#20986;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#22312;&#31639;&#27861;&#32423;&#21035;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#12290;&#36890;&#36807;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#65292;&#21487;&#20197;&#25214;&#21040;&#31070;&#32463;&#27963;&#21160;&#19982;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#22240;&#26524;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#30005;&#36335;&#22270;&#26696;&#34920;&#26126;&#22823;&#33041;&#20855;&#26377;&#19968;&#31995;&#21015;&#37325;&#22797;&#30340;&#35268;&#33539;&#35745;&#31639;&#21333;&#20803;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#34920;&#31034;&#26159;&#20998;&#24067;&#24335;&#30340;&#65292;&#22240;&#27492;&#30456;&#20851;&#35745;&#31639;&#21487;&#33021;&#20165;&#19982;&#21333;&#20010;&#31070;&#32463;&#20803;&#21464;&#25442;&#38388;&#25509;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#23450;&#20041;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#35745;&#31639;&#30340;&#35268;&#33539;&#21644;&#31639;&#27861;&#29702;&#35770;&#25972;&#21512;&#21040;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#20174;&#22823;&#35268;&#27169;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#20013;&#25512;&#26029;&#20986;&#35268;&#33539;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#22312;&#35268;&#33539;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#20551;&#35774;&#22823;&#33041;&#21019;&#24314;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20869;&#37096;&#27169;&#22411;&#65292;&#20551;&#35774;&#35299;&#37322;&#20854;&#24863;&#23448;&#36755;&#20837;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#24863;&#23448;&#36755;&#20837;&#26469;&#25512;&#26029;&#28508;&#22312;&#21407;&#22240;&#12290;&#22312;&#31639;&#27861;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#36825;&#20010;&#25512;&#29702;&#36807;&#31243;&#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36882;&#31639;&#27861;&#12290;&#36890;&#36807;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#65288;i&#65289;&#31070;&#32463;&#27963;&#21160;&#19982;&#24863;&#30693;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#22240;&#26524;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patterns of microcircuitry suggest that the brain has an array of repeated canonical computational units. Yet neural representations are distributed, so the relevant computations may only be related indirectly to single-neuron transformations. It thus remains an open challenge how to define canonical distributed computations. We integrate normative and algorithmic theories of neural computation into a mathematical framework for inferring canonical distributed computations from large-scale neural activity patterns. At the normative level, we hypothesize that the brain creates a structured internal model of its environment, positing latent causes that explain its sensory inputs, and uses those sensory inputs to infer the latent causes. At the algorithmic level, we propose that this inference process is a nonlinear message-passing algorithm on a graph-structured model of the world. Given a time series of neural activity during a perceptual inference task, our framework finds (i) the neura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20449;&#29992;&#21345;&#36829;&#32422;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.02956</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20449;&#29992;&#21345;&#35780;&#20998;&#39044;&#27979;&#65306;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Credit card score prediction using machine learning models: A new dataset. (arXiv:2310.02956v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20449;&#29992;&#21345;&#36829;&#32422;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20449;&#29992;&#21345;&#30340;&#20351;&#29992;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#20026;&#20102;&#26368;&#23567;&#21270;&#28508;&#22312;&#39118;&#38505;&#65292;&#24613;&#38656;&#20449;&#29992;&#21345;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20449;&#29992;&#21345;&#36829;&#32422;&#39044;&#27979;&#31995;&#32479;&#30340;&#24212;&#29992;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#30740;&#31350;&#22312;&#26032;&#25552;&#20986;&#30340;&#20449;&#29992;&#21345;&#35780;&#20998;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#21253;&#25324;&#20449;&#29992;&#21345;&#20132;&#26131;&#21382;&#21490;&#21644;&#23458;&#25143;&#26723;&#26696;&#65292;&#24182;&#20351;&#29992;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#31070;&#32463;&#32593;&#32476;&#12289;XGBoost&#21644;LightGBM&#12290;&#20026;&#20102;&#20934;&#22791;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30495;&#27491;&#38451;&#24615;&#29575;&#26041;&#38754;&#65292;MLP&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;LightGBM&#21644;XGBoost&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of credit cards has recently increased, creating an essential need for credit card assessment methods to minimize potential risks. This study investigates the utilization of machine learning (ML) models for credit card default prediction system. The main goal here is to investigate the best-performing ML model for new proposed credit card scoring dataset. This new dataset includes credit card transaction histories and customer profiles, is proposed and tested using a variety of machine learning algorithms, including logistic regression, decision trees, random forests, multi layer perceptron (MLP) neural network, XGBoost, and LightGBM. To prepare the data for machine learning models, we perform data pre-proccessing, feature extraction, feature selection, and data balancing techniques. Experimental results demonstrate that MLP outperforms logistic regression, decision trees, random forests, LightGBM, and XGBoost in terms of predictive performance in true positive rate, achieving 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12290;SAF&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#19982;Spike Representation&#21644;OTTT&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;SNNs&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02772</link><description>&lt;p&gt;
&#29992;&#20110;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks. (arXiv:2310.02772v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12290;SAF&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#19982;Spike Representation&#21644;OTTT&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;SNNs&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#35757;&#32451;&#33539;&#24335;&#65292;&#21363;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#12290;&#24050;&#30693;SNNs&#20855;&#26377;&#39640;&#33021;&#25928;&#20294;&#38590;&#20197;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;&#35768;&#22810;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26102;&#38388;&#19978;&#30340;&#22312;&#32447;&#35757;&#32451;&#65288;OTTT&#65289;&#26159;&#19968;&#31181;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25233;&#21046;&#20869;&#23384;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;GPU&#19978;&#39640;&#25928;&#35745;&#31639;&#65292;OTTT&#38656;&#35201;&#36827;&#34892;&#33033;&#20914;&#24207;&#21015;&#25805;&#20316;&#21644;&#33033;&#20914;&#24207;&#21015;&#21152;&#26435;&#27714;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;OTTT&#19982;Spike Representation&#65288;&#21478;&#19968;&#31181;&#35757;&#32451;&#26041;&#27861;&#65289;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#65292;&#20294;&#19982;Spike Representation&#30340;&#29702;&#35770;&#19968;&#33268;&#24615;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21363;SAF&#21487;&#20197;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20943;&#23569;&#19968;&#21322;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;SAF&#20998;&#21035;&#19982;Spike Representation&#21644;OTTT&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#35748;&#20102;......
&lt;/p&gt;
&lt;p&gt;
In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#37325;&#25918;&#26080;&#38556;&#30861;&#27979;&#35797;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20687;&#32032;&#30340;&#29992;&#25143;&#30028;&#38754;&#29702;&#35299;&#27169;&#22411;&#25191;&#34892;&#27979;&#35797;&#24182;&#29983;&#25104;&#21487;&#23548;&#33322;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#24320;&#21457;&#20154;&#21592;&#21644;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#26080;&#38556;&#30861;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02424</link><description>&lt;p&gt;
AXNav: &#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#37325;&#25918;&#26080;&#38556;&#30861;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AXNav: Replaying Accessibility Tests from Natural Language. (arXiv:2310.02424v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#37325;&#25918;&#26080;&#38556;&#30861;&#27979;&#35797;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20687;&#32032;&#30340;&#29992;&#25143;&#30028;&#38754;&#29702;&#35299;&#27169;&#22411;&#25191;&#34892;&#27979;&#35797;&#24182;&#29983;&#25104;&#21487;&#23548;&#33322;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#24320;&#21457;&#20154;&#21592;&#21644;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#26080;&#38556;&#30861;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#32773;&#21644;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#36890;&#24120;&#20381;&#36182;&#25163;&#21160;&#27979;&#35797;&#26469;&#22312;&#20135;&#21697;&#29983;&#21629;&#21608;&#26399;&#20013;&#27979;&#35797;&#26080;&#38556;&#30861;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#27979;&#35797;&#21487;&#33021;&#24456;&#20047;&#21619;&#65292;&#33539;&#22260;&#24222;&#22823;&#65292;&#24182;&#19988;&#24456;&#38590;&#23433;&#25490;&#22312;&#20854;&#20182;&#24320;&#21457;&#37324;&#31243;&#30865;&#20043;&#38388;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#29992;&#25143;&#30028;&#38754;&#65292;&#20294;&#25454;&#25105;&#20204;&#20102;&#35299;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#20154;&#25506;&#32034;&#36807;&#23427;&#20204;&#22312;&#25511;&#21046;&#36741;&#21161;&#25216;&#26415;&#20197;&#25903;&#25345;&#26080;&#38556;&#30861;&#27979;&#35797;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#39033;&#24418;&#25104;&#24615;&#30740;&#31350;&#24320;&#22987;&#65292;&#25506;&#35752;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#26080;&#38556;&#30861;&#27979;&#35797;&#24037;&#20316;&#27969;&#31243;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20197;&#25163;&#21160;&#26080;&#38556;&#30861;&#27979;&#35797;&#20026;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#8220;&#22312;VoiceOver&#20013;&#25628;&#32034;&#19968;&#20010;&#33410;&#30446;&#8221;&#65289;&#65292;&#24182;&#20351;&#29992;LLM&#32467;&#21512;&#22522;&#20110;&#20687;&#32032;&#30340;&#29992;&#25143;&#30028;&#38754;&#29702;&#35299;&#27169;&#22411;&#26469;&#25191;&#34892;&#27979;&#35797;&#24182;&#29983;&#25104;&#31456;&#33410;&#21010;&#20998;&#30340;&#21487;&#23548;&#33322;&#35270;&#39057;&#12290;&#22312;&#27599;&#20010;&#35270;&#39057;&#20013;&#65292;&#20026;&#20102;&#24110;&#21161;&#36136;&#37327;&#20445;&#35777;&#27979;&#35797;&#20154;&#21592;&#65292;&#25105;&#20204;&#24212;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#26631;&#35760;ac&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers and quality assurance testers often rely on manual testing to test accessibility features throughout the product lifecycle. Unfortunately, manual testing can be tedious, often has an overwhelming scope, and can be difficult to schedule amongst other development milestones. Recently, Large Language Models (LLMs) have been used for a variety of tasks including automation of UIs, however to our knowledge no one has yet explored their use in controlling assistive technologies for the purposes of supporting accessibility testing. In this paper, we explore the requirements of a natural language based accessibility testing workflow, starting with a formative study. From this we build a system that takes as input a manual accessibility test (e.g., ``Search for a show in VoiceOver'') and uses an LLM combined with pixel-based UI Understanding models to execute the test and produce a chaptered, navigable video. In each video, to help QA testers we apply heuristics to detect and flag ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.02299</link><description>&lt;p&gt;
3D&#29289;&#29702;&#31995;&#32479;&#20013;&#23398;&#20064;&#23545;&#31216;&#24615;&#30772;&#32570;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31561;&#20215;&#27169;&#22411;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#23436;&#32654;&#23545;&#31216;&#24615;&#30340;&#20551;&#35774;&#26377;&#26102;&#21487;&#33021;&#20250;&#38480;&#21046;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#19982;&#36825;&#20123;&#23545;&#31216;&#24615;&#19981;&#23436;&#20840;&#19968;&#33268;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#20102;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#21367;&#31215;&#25216;&#26415;&#33021;&#22815;&#22312;&#20445;&#25345;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25581;&#31034;&#30456;&#21464;&#20013;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#65292;&#36824;&#21487;&#20197;&#22312;&#27969;&#20307;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MIS-AVoiDD&#30340;&#27169;&#24577;&#19981;&#21464;&#21644;&#29305;&#23450;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#35270;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#36890;&#36807;&#20805;&#20998;&#20851;&#27880;&#38899;&#39057;&#21644;&#35270;&#35273;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#25805;&#20316;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02234</link><description>&lt;p&gt;
MIS-AVoiDD:&#38899;&#35270;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#27169;&#24577;&#19981;&#21464;&#21644;&#29305;&#23450;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MIS-AVoiDD: Modality Invariant and Specific Representation for Audio-Visual Deepfake Detection. (arXiv:2310.02234v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MIS-AVoiDD&#30340;&#27169;&#24577;&#19981;&#21464;&#21644;&#29305;&#23450;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#35270;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#36890;&#36807;&#20805;&#20998;&#20851;&#27880;&#38899;&#39057;&#21644;&#35270;&#35273;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#25805;&#20316;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26159;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#31639;&#27861;&#29983;&#25104;&#30340;&#21512;&#25104;&#23186;&#20307;&#65292;&#23545;&#31038;&#20250;&#21644;&#25919;&#27835;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#38500;&#20102;&#38754;&#37096;&#25805;&#20316;&#21644;&#21512;&#25104;&#35821;&#38899;&#65292;&#26368;&#36817;&#36824;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#20266;&#36896;&#65292;&#20854;&#20013;&#38899;&#39057;&#25110;&#35270;&#35273;&#27169;&#24577;&#34987;&#25805;&#20316;&#12290;&#20026;&#27492;&#65292;&#27491;&#22312;&#30740;&#31350;&#19968;&#31181;&#26032;&#19968;&#20195;&#30340;&#22810;&#27169;&#24577;&#38899;&#35270;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#65292;&#20849;&#21516;&#20851;&#27880;&#38899;&#39057;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#22810;&#27169;&#24577;&#25805;&#20316;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#65288;&#38899;&#35270;&#65289;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36890;&#24120;&#22522;&#20110;&#20174;&#35270;&#39057;&#20013;&#34701;&#21512;&#38899;&#39057;&#21644;&#35270;&#35273;&#27969;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#22810;&#27169;&#24577;&#26816;&#27979;&#22120;&#36890;&#24120;&#20855;&#26377;&#19982;&#21333;&#27169;&#24577;&#38899;&#39057;&#21644;&#35270;&#35273;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#24322;&#36136;&#24615;&#26412;&#36136;&#36896;&#25104;&#20102;&#27169;&#24577;&#20998;&#24067;&#24046;&#36317;&#65292;&#24182;&#23545;&#26377;&#25928;&#34701;&#21512;&#21644;&#39640;&#25928;&#24615;&#33021;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174;&#34920;&#31034;&#23618;&#38754;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfakes are synthetic media generated using deep generative algorithms and have posed a severe societal and political threat. Apart from facial manipulation and synthetic voice, recently, a novel kind of deepfakes has emerged with either audio or visual modalities manipulated. In this regard, a new generation of multimodal audio-visual deepfake detectors is being investigated to collectively focus on audio and visual data for multimodal manipulation detection. Existing multimodal (audio-visual) deepfake detectors are often based on the fusion of the audio and visual streams from the video. Existing studies suggest that these multimodal detectors often obtain equivalent performances with unimodal audio and visual deepfake detectors. We conjecture that the heterogeneous nature of the audio and visual signals creates distributional modality gaps and poses a significant challenge to effective fusion and efficient performance. In this paper, we tackle the problem at the representation lev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.02071</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond. (arXiv:2310.02071v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#25913;&#36827;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#30001;&#20110;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20687;GPT4-Vision&#36825;&#26679;&#30340;MLLM&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;MLLMs&#33021;&#21542;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#22788;&#29702;&#20855;&#36523;&#20915;&#31574;&#65292;&#24182;&#19988;LLMs&#21644;MLLMs&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#21542;&#33021;&#22686;&#24378;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;PCA-EVAL&#30340;&#26032;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#20174;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#35282;&#24230;&#35780;&#20272;&#20855;&#36523;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOLMES&#65292;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#21033;&#29992;MLLMs&#21644;APIs&#33719;&#21462;&#22810;&#27169;&#24577;&#20449;&#24687;&#20197;&#36827;&#34892;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#27604;&#36739;&#20102;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#21644;HOLMES&#65292;&#24182;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#20013;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01957</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36827;&#34892;&#39550;&#39542;&#65306;&#34701;&#21512;&#23545;&#35937;&#32423;&#21521;&#37327;&#27169;&#24577;&#20197;&#35299;&#37322;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving. (arXiv:2310.01957v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#20013;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#65292;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#39550;&#39542;&#24773;&#22659;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;10k&#20010;&#39550;&#39542;&#24773;&#22659;&#30340;160k&#20010;&#38382;&#31572;&#23545;&#65292;&#36825;&#20123;&#38382;&#31572;&#23545;&#19982;&#30001;RL&#20195;&#29702;&#25910;&#38598;&#30340;&#39640;&#36136;&#37327;&#25511;&#21046;&#21629;&#20196;&#21644;&#30001;&#25945;&#24072;LLM&#65288;GPT-3.5&#65289;&#29983;&#25104;&#30340;&#38382;&#39064;&#31572;&#26696;&#23545;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#29992;&#21521;&#37327;&#23383;&#24149;&#35821;&#35328;&#25968;&#25454;&#26469;&#23545;&#40784;&#25968;&#23383;&#21521;&#37327;&#27169;&#24577;&#21644;&#38745;&#24577;LLM&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39550;&#39542;&#38382;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#22522;&#20110;LLM&#30340;&#39550;&#39542;&#34892;&#20026;&#29983;&#25104;&#19982;&#20256;&#32479;&#34892;&#20026;&#20811;&#38534;&#30456;&#27604;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmar
&lt;/p&gt;</description></item><item><title>LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01852</link><description>&lt;p&gt;
LanguageBind:&#36890;&#36807;&#22522;&#20110;&#35821;&#20041;&#23545;&#40784;&#30340;&#35821;&#35328;&#23558;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#25193;&#23637;&#21040;N&#27169;&#24577;&#65288;arXiv:2310.01852v1[cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01852
&lt;/p&gt;
&lt;p&gt;
LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;-&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VL&#39044;&#35757;&#32451;&#26694;&#26550;&#38590;&#20197;&#23558;&#20854;&#25193;&#23637;&#21040;&#38500;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#22806;&#30340;&#22810;&#27169;&#24577;&#65288;N&#27169;&#24577;&#65292;N&gt;=3&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LanguageBind&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#32445;&#24102;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#24577;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#32034;&#65292;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;VL&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#12290;&#32467;&#26524;&#26159;&#65292;&#25152;&#26377;&#27169;&#24577;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#34429;&#28982;LanguageBind&#21487;&#20197;&#25193;&#23637;VL&#27169;&#24577;&#21040;N&#27169;&#24577;&#65292;&#20294;&#25105;&#20204;&#36824;&#38656;&#35201;&#19968;&#20010;&#24102;&#26377;&#20197;&#35821;&#35328;&#20026;&#20013;&#24515;&#30340;&#23545;&#40784;&#25968;&#25454;&#23545;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VIDAL-10M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35270;&#39057;&#12289;&#32418;&#22806;&#12289;&#28145;&#24230;&#12289;&#38899;&#39057;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#35328;&#25968;&#25454;&#65292;&#21629;&#21517;&#20026;VIDAL-10M&#12290;
&lt;/p&gt;
&lt;p&gt;
The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N&gt;=3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with Video, Infrared, Depth, Audio and their corresponding Language, naming as VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#29983;&#25104;&#28304;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20854;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.01701</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#36328;&#36234;&#39046;&#22495;&#65306;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transcending Domains through Text-to-Image Diffusion: A Source-Free Approach to Domain Adaptation. (arXiv:2310.01701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#29983;&#25104;&#28304;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20854;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#24212;&#29992;&#27169;&#22411;&#20174;&#30456;&#20851;&#28304;&#39046;&#22495;&#33719;&#21462;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#19981;&#20805;&#36275;&#26631;&#27880;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#65288;&#22914;HIPAA&#12289;COPPA&#12289;FERPA&#31561;&#65289;&#30340;&#19981;&#26029;&#21152;&#24378;&#24341;&#21457;&#20102;&#23545;&#22312;&#32469;&#36807;&#23545;&#28304;&#25968;&#25454;&#30340;&#30452;&#25509;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#65292;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#27169;&#22411;&#30340;&#20852;&#36259;&#65292;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;SFDA&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#39046;&#22495;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#28304;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#22312;&#26631;&#35760;&#30340;&#30446;&#26631;&#39046;&#22495;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#25509;&#36817;&#28304;&#25968;&#25454;&#30340;&#26679;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20154;&#24037;&#29983;&#25104;&#30340;&#28304;&#25968;&#25454;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#65292;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation (DA) is a method for enhancing a model's performance on a target domain with inadequate annotated data by applying the information the model has acquired from a related source domain with sufficient labeled data. The escalating enforcement of data-privacy regulations like HIPAA, COPPA, FERPA, etc. have sparked a heightened interest in adapting models to novel domains while circumventing the need for direct access to the source data, a problem known as Source-Free Domain Adaptation (SFDA). In this paper, we propose a novel framework for SFDA that generates source data using a text-to-image diffusion model trained on the target domain samples. Our method starts by training a text-to-image diffusion model on the labeled target domain samples, which is then fine-tuned using the pre-trained source model to generate samples close to the source data. Finally, we use Domain Adaptation techniques to align the artificially generated source data with the target domain data, resu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#65292;&#20462;&#22797;&#21387;&#32553;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.00867</link><description>&lt;p&gt;
(&#21160;&#24577;)&#25552;&#31034;&#21487;&#33021;&#26159;&#20462;&#22797;&#21387;&#32553;LLMs&#25152;&#38656;&#30340;&#20840;&#37096;&#12290;(arXiv:2310.00867v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
(Dynamic) Prompting might be all you need to repair Compressed LLMs. (arXiv:2310.00867v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00867
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#65292;&#20462;&#22797;&#21387;&#32553;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#26377;&#30528;&#37325;&#22823;&#30340;&#21464;&#38761;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#24378;&#35843;&#20102;&#39640;&#25928;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#21387;&#32553;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#38024;&#23545;&#26368;&#22823;&#30340;LLMs&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#25105;&#20204;&#20351;&#29992;LLaMA-7B&#21644;OPT-6.7b&#36827;&#34892;&#30340;&#27979;&#35797;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#23545;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#21387;&#32553;&#21518;&#37325;&#26032;&#35757;&#32451;&#30340;&#26435;&#34913;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#25552;&#31034;&#39537;&#21160;&#30340;&#24674;&#22797;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#20855;&#26377;&#28508;&#22312;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#23616;&#38480;&#22312;&#22256;&#24785;&#24230;&#35780;&#20272;&#21644;&#31616;&#21333;&#20219;&#21153;&#19978;&#65292;&#23545;&#25552;&#31034;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#27809;&#26377;&#32473;&#20986;&#26126;&#30830;&#30340;&#20449;&#24515;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#20851;&#38190;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLM&#21387;&#32553;&#20013;&#22825;&#30495;&#25552;&#31034;&#30340;&#33030;&#24369;&#24615;&#65292;&#21363;&#36807;&#24230;&#20381;&#36182;&#21333;&#19968;&#36755;&#20837;&#30340;&#25552;&#31034;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25512;&#29702;&#26102;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#33258;&#20027;&#36873;&#25321;&#26368;&#20339;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), while transformative for NLP, come with significant computational demands, underlining the need for efficient, training-free compression. Notably, despite the marked improvement in training-free compression for the largest of LLMs, our tests using LLaMA-7B and OPT-6.7b highlight a significant performance drop in several realistic downstream tasks. Investigation into the trade-off between resource-intensive post-compression re-training highlights the prospect of prompt-driven recovery as a lightweight adaption tool. However, existing studies, confined mainly to perplexity evaluations and simple tasks, fail to offer unequivocal confidence in the scalability and generalizability of prompting. We tackle this uncertainty in two key ways. First, we uncover the vulnerability of naive prompts in LLM compression as an over-reliance on a singular prompt per input. In response, we propose inference-time dynamic prompting (IDP), a mechanism that autonomously chooses f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Contextualized Bi-Directional Dual Transformer (CBDT) Classifier&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#26816;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20004;&#20010;Transformer&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#26377;&#20559;&#35265;&#21644;&#20013;&#31435;&#30340;&#38472;&#36848;&#65292;&#24182;&#25214;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;CBDT&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24615;&#33021;&#25552;&#21319;&#20102;2-4&#65285;&#65292;&#21516;&#26102;&#36824;&#20026;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#24212;&#29992;&#35813;&#27169;&#22411;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00347</link><description>&lt;p&gt;
&#35299;&#38145;&#20559;&#35265;&#26816;&#27979;&#65306;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#20869;&#23481;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis. (arXiv:2310.00347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00347
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Contextualized Bi-Directional Dual Transformer (CBDT) Classifier&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#26816;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20004;&#20010;Transformer&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#26377;&#20559;&#35265;&#21644;&#20013;&#31435;&#30340;&#38472;&#36848;&#65292;&#24182;&#25214;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;CBDT&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24615;&#33021;&#25552;&#21319;&#20102;2-4&#65285;&#65292;&#21516;&#26102;&#36824;&#20026;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#24212;&#29992;&#35813;&#27169;&#22411;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20559;&#35265;&#23545;&#20110;&#24378;&#21270;&#36127;&#38754;&#21051;&#26495;&#21360;&#35937;&#12289;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#24433;&#21709;&#20915;&#31574;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#22312;&#25991;&#26412;&#20013;&#36827;&#34892;&#20559;&#35265;&#26816;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#22312;&#36229;&#20986;&#20854;&#35757;&#32451;&#38598;&#33539;&#22260;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Contextualized Bi-Directional Dual Transformer&#65288;CBDT&#65289;&#20998;&#31867;&#22120;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#21033;&#29992;&#20102;&#20004;&#20010;&#21327;&#21516;&#24037;&#20316;&#30340;Transformer&#32593;&#32476;&#65306;Context Transformer&#21644;Entity Transformer&#65292;&#26088;&#22312;&#22686;&#24378;&#20559;&#35265;&#26816;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20934;&#22791;&#36981;&#24490;FAIR&#21407;&#21017;&#65292;&#30830;&#20445;&#25968;&#25454;&#20351;&#29992;&#20855;&#26377;&#36947;&#24503;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;CBDT&#23637;&#31034;&#20102;&#20854;&#22312;&#21306;&#20998;&#26377;&#20559;&#35265;&#19982;&#20013;&#31435;&#38472;&#36848;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#25351;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;2-4&#65285;&#30340;&#25552;&#21319;&#12290;&#36825;&#20026;&#23558;CBDT&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#36866;&#24212;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias detection in text is imperative due to its role in reinforcing negative stereotypes, disseminating misinformation, and influencing decisions. Current language models often fall short in generalizing beyond their training sets. In response, we introduce the Contextualized Bi-Directional Dual Transformer (CBDT) Classifier. This novel architecture utilizes two synergistic transformer networks: the Context Transformer and the Entity Transformer, aiming for enhanced bias detection. Our dataset preparation follows the FAIR principles, ensuring ethical data usage. Through rigorous testing on various datasets, CBDT showcases its ability in distinguishing biased from neutral statements, while also pinpointing exact biased lexemes. Our approach outperforms existing methods, achieving a 2-4\% increase over benchmark performances. This opens avenues for adapting the CBDT model across diverse linguistic and cultural landscapes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#26469;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26377;&#25928;&#24615;&#21644;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.00029</link><description>&lt;p&gt;
&#34701;&#20837;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#30340;&#23545;&#25239;&#39550;&#39542;&#34892;&#20026;&#29983;&#25104;&#25216;&#26415;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation. (arXiv:2310.00029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#39118;&#38505;&#35748;&#30693;&#26469;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26377;&#25928;&#24615;&#21644;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#20110;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25163;&#39550;&#39542;&#34892;&#20026;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#26292;&#38706;&#20986;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38754;&#23545;&#30340;&#26377;&#25928;&#21644;&#21512;&#29702;&#30340;&#39118;&#38505;&#20107;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#25163;&#34892;&#20026;&#65292;&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#30340;&#39118;&#38505;&#35748;&#30693;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#29256;&#26412;&#30340;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#23545;&#25163;&#31574;&#30053;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#39640;&#20445;&#30495;&#30340;&#30828;&#20214;&#22312;&#29615;&#65288;HiL&#65289;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#22522;&#20110;&#24182;&#32447;&#24773;&#26223;&#30340;&#23545;&#27604;&#26696;&#20363;&#30740;&#31350;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#25163;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#34987;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.
&lt;/p&gt;</description></item><item><title>AutoAgents&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#20219;&#21153;&#33258;&#21160;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;AI&#22242;&#38431;&#65292;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.17288</link><description>&lt;p&gt;
AutoAgents: &#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#26234;&#33021;&#20195;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoAgents: A Framework for Automatic Agent Generation. (arXiv:2309.17288v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17288
&lt;/p&gt;
&lt;p&gt;
AutoAgents&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#20219;&#21153;&#33258;&#21160;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;AI&#22242;&#38431;&#65292;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#20219;&#21153;&#35299;&#20915;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#20195;&#29702;&#26469;&#22788;&#29702;&#31616;&#21333;&#20219;&#21153;&#65292;&#38480;&#21046;&#20102;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoAgents&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#33258;&#36866;&#24212;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#20197;&#26500;&#24314;AI&#22242;&#38431;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AutoAgents&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#22810;&#20010;&#25152;&#38656;&#20195;&#29702;&#26469;&#32806;&#21512;&#20219;&#21153;&#21644;&#35282;&#33394;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#26681;&#25454;&#29983;&#25104;&#30340;&#19987;&#23478;&#20195;&#29702;&#20026;&#24403;&#21069;&#20219;&#21153;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#30456;&#20114;&#21327;&#20316;&#20197;&#39640;&#25928;&#23436;&#25104;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#35266;&#23519;&#32773;&#35282;&#33394;&#34987;&#32435;&#20837;&#26694;&#26550;&#20013;&#20197;&#21453;&#24605;&#25351;&#23450;&#30340;&#35745;&#21010;&#21644;&#20195;&#29702;&#30340;&#21709;&#24212;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AutoAgents&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#20219;&#21153;&#36866;&#24212;&#24615;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#20219;&#21153;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have enabled remarkable advances in automated task-solving with multi-agent systems. However, most existing LLM-based multi-agent approaches rely on predefined agents to handle simple tasks, limiting the adaptability of multi-agent collaboration to different scenarios. Therefore, we introduce AutoAgents, an innovative framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks. Specifically, AutoAgents couples the relationship between tasks and roles by dynamically generating multiple required agents based on task content and planning solutions for the current task based on the generated expert agents. Multiple specialized agents collaborate with each other to efficiently accomplish tasks. Concurrently, an observer role is incorporated into the framework to reflect on the designated plans and agents' responses and improve upon them. Our experiments on various benchmarks demonstrate that Au
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.16977</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reliability Quantification of Deep Reinforcement Learning-based Control. (arXiv:2309.16977v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;&#26159;&#20154;&#24037;&#26234;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;DRL&#25511;&#21046;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#24212;&#29992;&#20102;&#19968;&#31181;&#29616;&#26377;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#22122;&#22768;&#25552;&#21462;&#65292;&#20197;&#26126;&#30830;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#21487;&#38752;&#24615;&#65306;&#21442;&#32771;&#32593;&#32476;&#21644;&#35780;&#20272;&#32593;&#32476;&#12290;&#23427;&#20204;&#20855;&#26377;&#30456;&#21516;&#30340;&#32467;&#26500;&#21644;&#30456;&#21516;&#30340;&#21021;&#22987;&#21442;&#25968;&#12290;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#20004;&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#30456;&#21516;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35780;&#20272;&#32593;&#32476;&#30340;&#21442;&#25968;&#34987;&#26356;&#26032;&#65292;&#20197;&#26368;&#22823;&#21270;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#21442;&#32771;&#32593;&#32476;&#21644;&#35780;&#20272;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22522;&#20110;&#20004;&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#24046;&#24322;&#35780;&#20272;&#29305;&#23450;&#29366;&#24577;&#19979;DRL&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability quantification of deep reinforcement learning (DRL)-based control is a significant challenge for the practical application of artificial intelligence (AI) in safety-critical systems. This study proposes a method for quantifying the reliability of DRL-based control. First, an existing method, random noise distillation, was applied to the reliability evaluation to clarify the issues to be solved. Second, a novel method for reliability quantification was proposed to solve these issues. The reliability is quantified using two neural networks: reference and evaluator. They have the same structure with the same initial parameters. The outputs of the two networks were the same before training. During training, the evaluator network parameters were updated to maximize the difference between the reference and evaluator networks for trained data. Thus, the reliability of the DRL-based control for a state can be evaluated based on the difference in output between the two networks. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.16108</link><description>&lt;p&gt;
&#39057;&#36947;&#35270;&#35273;Transformer&#65306;&#19968;&#24352;&#22270;&#20540;C x 16 x 16&#20010;&#35789;
&lt;/p&gt;
&lt;p&gt;
Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26576;&#20123;&#22270;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#22914;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#65292;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#36890;&#36947;&#65292;&#27599;&#20010;&#36890;&#36947;&#37117;&#25658;&#24102;&#30528;&#35821;&#20041;&#19978;&#19981;&#21516;&#21644;&#29420;&#31435;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#36755;&#20837;&#36890;&#36947;&#30340;&#31232;&#30095;&#24615;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#36807;&#31243;&#20013;&#21487;&#33021;&#27809;&#26377;&#23494;&#38598;&#21487;&#29992;&#30340;&#36890;&#36947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#65292;&#22686;&#24378;&#20102;&#23545;&#36755;&#20837;&#36890;&#36947;&#20043;&#38388;&#30340;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;(HCS)&#20316;&#20026;&#19968;&#31181;&#38468;&#21152;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20165;&#20986;&#29616;&#37096;&#20998;&#36890;&#36947;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;ChannelViT&#29420;&#31435;&#22320;&#26500;&#24314;&#34917;&#19969;&#20196;&#29260;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36890;&#36947;&#23884;&#20837;&#23558;&#20854;&#28155;&#21152;&#21040;&#34917;&#19969;&#20196;&#29260;&#20013;&#65292;&#31867;&#20284;&#20110;&#20301;&#32622;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15757</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data. (arXiv:2309.15757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#65288;&#26377;&#65289;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#23454;&#20363;&#38388;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25512;&#26029;&#25429;&#25417;&#20869;&#22312;&#25968;&#25454;&#20851;&#31995;&#30340;&#28508;&#22312;&#22270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#20449;&#24687;&#22312;&#25972;&#20010;&#22270;&#20013;&#30340;&#26080;&#32541;&#20256;&#25773;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#24403;&#20195;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#21457;&#29616;&#23454;&#20363;&#38388;&#20851;&#31995;&#20316;&#20026;&#26500;&#24314;&#24378;&#21270;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#40065;&#26834;&#28508;&#22312;&#22270;&#30340;&#23454;&#38469;&#25163;&#27573;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of semi-supervised learning, the current approaches insufficiently exploit the potential of considering inter-instance relationships among (un)labeled data. In this work, we address this limitation by providing an approach for inferring latent graphs that capture the intrinsic data relationships. By leveraging graph-based representations, our approach facilitates the seamless propagation of information throughout the graph, enabling the effective incorporation of global and local knowledge. Through evaluations on biomedical tabular datasets, we compare the capabilities of our approach to other contemporary methods. Our work demonstrates the significance of inter-instance relationship discovery as practical means for constructing robust latent graphs to enhance semi-supervised learning techniques. Our method achieves state-of-the-art results on three biomedical datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.15701</link><description>&lt;p&gt;
HyPoradise&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#24320;&#25918;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models. (arXiv:2309.15701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#20960;&#20010;&#20844;&#24320;&#30340;&#24178;&#20928;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#38754;&#23545;&#36870;&#22659;&#26102;&#20063;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;&#33391;&#22909;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#23545;&#20110;&#35821;&#38899;&#39046;&#22495;&#30340;&#21464;&#24322;&#24615;&#24456;&#25935;&#24863;&#65292;&#22914;&#32972;&#26223;&#22122;&#22768;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22806;&#37096;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#20854;&#20013;N&#26368;&#20339;&#35299;&#30721;&#20551;&#35774;&#20026;&#30495;&#23454;&#36716;&#24405;&#39044;&#27979;&#25552;&#20379;&#20102;&#26377;&#20449;&#24687;&#37327;&#30340;&#20803;&#32032;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#31574;&#30053;&#19981;&#21516;&#65292;&#21518;&#32773;&#21482;&#33021;&#36873;&#25321;&#19968;&#20010;&#20505;&#36873;&#20551;&#35774;&#20316;&#20026;&#26368;&#32456;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.15402</link><description>&lt;p&gt;
&#20851;&#20110;&#24605;&#32500;&#38142;&#25512;&#29702;&#65306;&#36827;&#23637;&#12289;&#21069;&#27839;&#21644;&#26410;&#26469;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. (arXiv:2309.15402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#35748;&#30693;&#36807;&#31243;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#19968;&#20221;&#20840;&#38754;&#30340;&#35843;&#26597;&#25253;&#21578;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#20180;&#32454;&#24191;&#27867;&#22320;&#27010;&#36848;&#20102;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#29992;&#8220;X-of-Thought&#8221;&#26469;&#25351;&#20195;&#24191;&#20041;&#19978;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#26041;&#27861;&#30340;&#20998;&#31867;&#20307;&#31995;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#31995;&#32479;&#32452;&#32455;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#30340;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24605;&#32500;&#38142;&#22312;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#39046;&#22495;&#30340;&#21069;&#27839;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#22810;&#27169;&#24577;&#21644;&#29702;&#35770;&#31561;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#33021;&#25104;&#20026;&#23547;&#27714;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing. However, there still remains a lack of a comprehensive survey for this arena. To this end, we take the first step and present a thorough survey of this research field carefully and widely. We use X-of-Thought to refer to Chain-of-Thought in a broad sense. In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.14735</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#21516;&#26816;&#32034;&#21644;&#38382;&#31572;&#27169;&#22411;&#30340;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Artificial Intelligence for Indian Legal Question Answering (AILQA) Using Different Retrieval and QA Models. (arXiv:2309.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#38382;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#26377;&#28508;&#21147;&#25913;&#21464;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#19982;&#26696;&#20363;&#25991;&#20214;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#21360;&#24230;&#27861;&#24459;&#20307;&#31995;&#19979;&#22238;&#31572;&#27861;&#24459;&#38382;&#39064;&#30340;&#25928;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#65288;AILQA&#65289;&#24182;&#30740;&#31350;&#20102;&#24403;&#21069;&#21487;&#29992;&#30340;&#19981;&#21516;&#26816;&#32034;&#21644;QA&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21033;&#29992;OpenAI GPT&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#65292;&#32467;&#21512;&#26597;&#35810;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#26412;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#21360;&#24230;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35813;&#39046;&#22495;&#30001;&#20110;&#22797;&#26434;&#24615;&#21644;&#36164;&#28304;&#38480;&#21046;&#32780;&#38754;&#20020;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32463;&#39564;&#35777;&#35780;&#20272;&#19982;&#20174;&#23454;&#36341;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal question-answering (QA) systems have the potential to revolutionize the way legal professionals interact with case law documents. This paper conducts a comparative analysis of existing artificial intelligence models for their utility in answering legal questions within the Indian legal system, specifically focusing on Indian Legal Question Answering (AILQA) and our study investigates the efficacy of different retrieval and QA algorithms currently available. Utilizing the OpenAI GPT model as a benchmark, along with query prompts, our investigation shows that existing AILQA systems can automatically interpret natural language queries from users and generate highly accurate responses. This research is particularly focused on applications within the Indian criminal justice domain, which has its own set of challenges due to its complexity and resource constraints. In order to rigorously assess the performance of these models, empirical evaluations are complemented by feedback from pra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#34987;&#25915;&#20987;&#20197;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#19968;&#25915;&#20987;&#36824;&#21487;&#20197;&#23545;&#20854;&#20182;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24433;&#21709;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.11751</link><description>&lt;p&gt;
Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#26377;&#22810;&#24378;&#22823;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Robust is Google's Bard to Adversarial Image Attacks?. (arXiv:2309.11751v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#34987;&#25915;&#20987;&#20197;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#19968;&#25915;&#20987;&#36824;&#21487;&#20197;&#23545;&#20854;&#20182;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24433;&#21709;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#25991;&#26412;&#21644;&#20854;&#20182;&#27169;&#24577;&#65288;&#23588;&#20854;&#26159;&#35270;&#35273;&#65289;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#35273;&#27169;&#22411;&#30340;&#26410;&#35299;&#20915;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#35270;&#35273;&#36755;&#20837;&#21487;&#33021;&#20351;MLLM&#38754;&#20020;&#26356;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Google&#30340;Bard&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23427;&#26159;&#19968;&#20010;&#31454;&#20105;&#24615;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#26368;&#36817;&#21457;&#24067;&#20102;&#20854;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#21830;&#19994;MLLM&#30340;&#28431;&#27934;&#12290;&#36890;&#36807;&#25915;&#20987;&#30333;&#30418;&#23376;&#20195;&#29702;&#35270;&#35273;&#32534;&#30721;&#22120;&#25110;MLLM&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#21487;&#20197;&#20351;Bard&#20197;22&#65285;&#30340;&#25104;&#21151;&#29575;&#20165;&#22522;&#20110;&#21487;&#36716;&#31227;&#24615;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#36824;&#21487;&#20197;&#25915;&#20987;&#20854;&#20182;MLLM&#65292;&#20363;&#22914;&#65292;&#23545;Bing Chat&#30340;&#25104;&#21151;&#25915;&#20987;&#29575;&#20026;26&#65285;&#65292;&#23545;ERNIE bot&#30340;&#25104;&#21151;&#25915;&#20987;&#29575;&#20026;86&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#65292;&#21253;&#25324;&#38754;&#37096;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gold-YOLO&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#20197;&#21450;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;YOLO&#31995;&#21015;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.11331</link><description>&lt;p&gt;
Gold-YOLO: &#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#23454;&#29616;&#39640;&#25928;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism. (arXiv:2309.11331v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gold-YOLO&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#20197;&#21450;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;YOLO&#31995;&#21015;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;YOLO&#31995;&#21015;&#27169;&#22411;&#24050;&#25104;&#20026;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#30340;&#39046;&#20808;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#20462;&#25913;&#26550;&#26500;&#12289;&#22686;&#21152;&#25968;&#25454;&#21644;&#35774;&#35745;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#22522;&#32447;&#25552;&#21319;&#21040;&#20102;&#26356;&#39640;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#34429;&#28982;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;FPN&#65289;&#21644;&#36335;&#24452;&#32858;&#21512;&#32593;&#32476;&#65288;PANet&#65289;&#24050;&#32463;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#25805;&#20316;&#23454;&#29616;&#12290;&#36825;&#20010;&#26032;&#35774;&#35745;&#30340;&#27169;&#22411;&#21517;&#20026;Gold-YOLO&#65292;&#25552;&#21319;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#33021;&#21147;&#65292;&#24182;&#22312;&#25152;&#26377;&#27169;&#22411;&#23610;&#24230;&#19978;&#23454;&#29616;&#20102;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#30340;&#29702;&#24819;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;YOLO&#31995;&#21015;&#20013;&#23454;&#29616;&#20102;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;YOLO&#31995;&#21015;&#27169;&#22411;&#21487;&#20197;&#20174;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#21463;&#30410;&#12290;Gold-YOLO-N&#22312;COCO val2017&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20986;&#33394;&#30340;39.9%&#24179;&#22343;&#31934;&#24230;&#65288;AP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past years, YOLO-series models have emerged as the leading approaches in the area of real-time object detection. Many studies pushed up the baseline to a higher level by modifying the architecture, augmenting data and designing new losses. However, we find previous models still suffer from information fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation Network (PANet) have alleviated this. Therefore, this study provides an advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations. This new designed model named as Gold-YOLO, which boosts the multi-scale feature fusion capabilities and achieves an ideal balance between latency and accuracy across all model scales. Additionally, we implement MAE-style pretraining in the YOLO-series for the first time, allowing YOLOseries models could be to benefit from unsupervised pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017 datas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;Clustered FedStack&#12290;&#36890;&#36807;&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.11044</link><description>&lt;p&gt;
Clustered FedStack&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion. (arXiv:2309.11044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11044
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;Clustered FedStack&#12290;&#36890;&#36807;&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#30446;&#21069;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26368;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#22240;&#20854;&#21327;&#20316;&#23398;&#20064;&#21644;&#20445;&#25252;&#23458;&#25143;&#38544;&#31169;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#38750;&#29420;&#31435;&#21644;&#38750;&#29420;&#31435;&#20998;&#24067;&#65288;&#38750;IID&#65289;&#20197;&#21450;&#26412;&#22320;&#23458;&#25143;&#20043;&#38388;&#26631;&#31614;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#30740;&#31350;&#22242;&#38431;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;&#20351;&#29992;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#12289;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#21644;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24050;&#21457;&#34920;&#30340;Stacked Federated Learning&#65288;FedStack&#65289;&#26694;&#26550;&#30340;&#26032;&#39062;Clustered FedStack&#26694;&#26550;&#12290;&#26412;&#22320;&#23458;&#25143;&#31471;&#23558;&#20854;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#36825;&#20010;&#20840;&#23616;&#27169;&#22411;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#22522;&#20110;&#20854;&#36755;&#20986;&#23618;&#26435;&#37325;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#32858;&#31867;&#26426;&#21046;&#65292;&#20998;&#21035;&#26159;K-Means&#12289;Agglomerative&#12289;DBSCAN&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is currently one of the most popular technologies in the field of Artificial Intelligence (AI) due to its collaborative learning and ability to preserve client privacy. However, it faces challenges such as non-identically and non-independently distributed (non-IID) and data with imbalanced labels among local clients. To address these limitations, the research community has explored various approaches such as using local model parameters, federated generative adversarial learning, and federated representation learning. In our study, we propose a novel Clustered FedStack framework based on the previously published Stacked Federated Learning (FedStack) framework. The local clients send their model predictions and output layer weights to a server, which then builds a robust global model. This global model clusters the local clients based on their output layer weights using a clustering mechanism. We adopt three clustering mechanisms, namely K-Means, Agglomerative, a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#21487;&#26597;&#35810;&#23433;&#20840;&#32422;&#26463;&#27169;&#22359;&#65292;&#29992;&#20110;&#24378;&#21046;LLM&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#36981;&#23432;&#31105;&#27490;&#34892;&#20026;&#30340;&#38480;&#21046;&#65292;&#24182;&#30830;&#20445;&#28385;&#36275;&#20840;&#29699;&#24037;&#19994;&#24037;&#21378;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#37096;&#32626;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.09919</link><description>&lt;p&gt;
&#25554;&#20837;&#23433;&#20840;&#33455;&#29255;&#65306;&#24378;&#21046;LLM&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#32422;&#26463;&#26465;&#20214;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents. (arXiv:2309.09919v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09919
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#21487;&#26597;&#35810;&#23433;&#20840;&#32422;&#26463;&#27169;&#22359;&#65292;&#29992;&#20110;&#24378;&#21046;LLM&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#36981;&#23432;&#31105;&#27490;&#34892;&#20026;&#30340;&#38480;&#21046;&#65292;&#24182;&#30830;&#20445;&#28385;&#36275;&#20840;&#29699;&#24037;&#19994;&#24037;&#21378;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#37096;&#32626;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#25104;&#20026;&#21487;&#33021;&#65292;&#21363;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#26399;&#38388;LLMs&#33719;&#24471;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#19968;&#33324;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#26426;&#22120;&#20154;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;LLM&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#24050;&#32463;&#20184;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#25945;&#20250;&#26426;&#22120;&#20154;"&#20570;&#20160;&#20040;"&#65292;&#20294;&#23545;"&#19981;&#35201;&#20570;&#20160;&#20040;"&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#20110;&#20219;&#20309;&#23454;&#38469;&#24212;&#29992;&#32780;&#35328;&#65292;&#25945;&#26426;&#22120;&#20154;"&#19981;&#35201;&#20570;&#20160;&#20040;"&#21516;&#26679;&#37325;&#35201;&#65306;&#20256;&#36798;&#26377;&#20851;&#31105;&#27490;&#34892;&#20026;&#30340;&#26126;&#30830;&#25351;&#31034;&#65292;&#35780;&#20272;&#26426;&#22120;&#20154;&#23545;&#36825;&#20123;&#38480;&#21046;&#30340;&#29702;&#35299;&#65292;&#24182;&#19988;&#26368;&#37325;&#35201;&#30340;&#26159;&#30830;&#20445;&#36981;&#23432;&#36825;&#20123;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#21487;&#39564;&#35777;&#30340;&#23433;&#20840;&#25805;&#20316;&#23545;&#20110;&#28385;&#36275;ISO 61508&#31561;&#20840;&#29699;&#24037;&#19994;&#24037;&#21378;&#29615;&#22659;&#20013;&#30340;&#26426;&#22120;&#20154;&#23433;&#20840;&#37096;&#32626;&#26631;&#20934;&#33267;&#20851;&#37325;&#35201;&#12290;&#38024;&#23545;&#23558;LLM&#20195;&#29702;&#37096;&#32626;&#22312;&#21327;&#21516;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#21487;&#26597;&#35810;&#23433;&#20840;&#32422;&#26463;&#27169;&#22359;&#65292;&#21516;&#26102;&#30830;&#20445;&#23454;&#29616;&#23454;&#26102;&#32422;&#26463;&#26816;&#26597;&#21644;&#33258;&#36866;&#24212;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have enabled a new research domain, LLM agents, for solving robotics and planning tasks by leveraging the world knowledge and general reasoning abilities of LLMs obtained during pretraining. However, while considerable effort has been made to teach the robot the "dos," the "don'ts" received relatively less attention. We argue that, for any practical usage, it is as crucial to teach the robot the "don'ts": conveying explicit instructions about prohibited actions, assessing the robot's comprehension of these restrictions, and, most importantly, ensuring compliance. Moreover, verifiable safe operation is essential for deployments that satisfy worldwide standards such as ISO 61508, which defines standards for safely deploying robots in industrial factory environments worldwide. Aiming at deploying the LLM agents in a collaborative environment, we propose a queryable safety constraint module based on linear temporal logic (LTL) that simult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08636</link><description>&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;&#65288;&#31532;23&#23395;&#31532;3&#23395;&#65289;&#12290;&#65288;arXiv:2309.08636v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21382;&#21490;&#19978;&#65292;&#29087;&#32451;&#30340;&#20889;&#20316;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#36827;&#27493;&#30340;&#20851;&#38190;&#65292;&#21019;&#36896;&#24615;&#34920;&#36798;&#34987;&#35270;&#20026;&#20154;&#31867;&#25104;&#23601;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#26631;&#24535;&#30528;&#36825;&#19968;&#21465;&#20107;&#30340;&#19968;&#20010;&#36716;&#25240;&#28857;&#65292;&#21253;&#25324;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#12290;&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#26041;&#38754;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#26041;&#27861;&#22522;&#20110;&#30001;&#20154;&#31867;&#19987;&#23478;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#23450;&#37327;&#20934;&#30830;&#24615;&#21644;&#23450;&#24615;&#31934;&#30830;&#24615;&#26631;&#35760;&#12290;&#23450;&#37327;&#20934;&#30830;&#24615;&#35780;&#20272;&#20102;&#20107;&#23454;&#30340;&#27491;&#30830;&#24615;&#65292;&#32780;&#23450;&#24615;&#31934;&#30830;&#24615;&#35780;&#20272;&#20102;&#31185;&#23398;&#36129;&#29486;&#12290;&#34429;&#28982;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29305;&#21035;&#26159;ChatGPT-4&#65292;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#24615;&#65292;&#20294;&#22312;&#29983;&#25104;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#22833;&#36133;&#20102;&#12290;&#39034;&#20415;&#25552;&#19968;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#26174;&#31034;&#65292;&#38543;&#30528;ChatGPT-4&#65292;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#24050;&#32463;&#20572;&#28382;&#19981;&#21069;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#22797;&#26434;&#19988;&#21453;&#22797;&#26080;&#24120;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historically, proficient writing was deemed essential for human advancement, with creative expression viewed as one of the hallmarks of human achievement. However, recent advances in generative AI have marked an inflection point in this narrative, including for scientific writing. This article provides a comprehensive analysis of the capabilities and limitations of six AI chatbots in scholarly writing in the humanities and archaeology. The methodology was based on tagging AI generated content for quantitative accuracy and qualitative precision by human experts. Quantitative accuracy assessed the factual correctness, while qualitative precision gauged the scientific contribution. While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in recombining existing knowledge, they failed in generating original scientific content. As a side note, our results also suggest that with ChatGPT-4 the size of the LLMs has plateaued. Furthermore, the paper underscores the intricate and re
&lt;/p&gt;</description></item><item><title>RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.02671</link><description>&lt;p&gt;
RLSynC: &#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#26041;&#27861;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
RLSynC: Offline-Online Reinforcement Learning for Synthon Completion. (arXiv:2309.02671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02671
&lt;/p&gt;
&lt;p&gt;
RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26159;&#30830;&#23450;&#33021;&#22815;&#21453;&#24212;&#24418;&#25104;&#25152;&#38656;&#20135;&#29289;&#30340;&#19968;&#32452;&#21453;&#24212;&#29289;&#20998;&#23376;&#30340;&#36807;&#31243;&#12290;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#39318;&#20808;&#39044;&#27979;&#20135;&#29289;&#20013;&#30340;&#21453;&#24212;&#20013;&#24515;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#21512;&#25104;&#29289;&#37325;&#26032;&#34917;&#20840;&#25104;&#21453;&#24212;&#29289;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#24517;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39640;&#23454;&#29992;&#24615;&#65292;&#20197;&#25351;&#23548;&#21512;&#25104;&#35268;&#21010;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;RLSynC&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#26041;&#27861;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;RLSynC&#20026;&#27599;&#20010;&#21512;&#25104;&#29289;&#20998;&#37197;&#19968;&#20010;&#20195;&#29702;&#65292;&#25152;&#26377;&#20195;&#29702;&#37117;&#36890;&#36807;&#21516;&#27493;&#36827;&#34892;&#36880;&#27493;&#34892;&#21160;&#65292;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#12290;RLSynC&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#20132;&#20114;&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#20174;&#32780;&#21487;&#20197;&#25506;&#32034;&#26032;&#30340;&#21453;&#24212;&#31354;&#38388;&#12290;RLSynC&#20351;&#29992;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#21453;&#24212;&#29289;&#22312;&#21512;&#25104;&#20135;&#29289;&#26102;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the process of determining the set of reactant molecules that can react to form a desired product. Semi-template-based retrosynthesis methods, which imitate the reverse logic of synthesis reactions, first predict the reaction centers in the products, and then complete the resulting synthons back into reactants. These methods enable necessary interpretability and high practical utility to inform synthesis planning. We develop a new offline-online reinforcement learning method RLSynC for synthon completion in semi-template-based methods. RLSynC assigns one agent to each synthon, all of which complete the synthons by conducting actions step by step in a synchronized fashion. RLSynC learns the policy from both offline training episodes and online interactions which allow RLSynC to explore new reaction spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the predicted reactants in synthesizing a product, and thus guides the action search. We compare 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#21517;&#20026;&#39034;&#24207;&#28789;&#24039;&#24615;&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#19968;&#31995;&#21015;&#20114;&#19981;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#38271;&#31243;&#25805;&#20316;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20018;&#32852;&#22810;&#20010;&#29087;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#38271;&#31243;&#20219;&#21153;&#30446;&#26631;&#65292;&#24182;&#20855;&#22791;&#33258;&#20027;&#31574;&#30053;&#20999;&#25442;&#21644;&#32469;&#36807;&#22810;&#20313;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00987</link><description>&lt;p&gt;
&#20018;&#32852;&#29087;&#32451;&#25805;&#20316;&#31574;&#30053;&#23454;&#29616;&#38271;&#31243;&#25805;&#20316;&#30340;&#39034;&#24207;&#28789;&#24039;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation. (arXiv:2309.00987v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#21517;&#20026;&#39034;&#24207;&#28789;&#24039;&#24615;&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#19968;&#31995;&#21015;&#20114;&#19981;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#38271;&#31243;&#25805;&#20316;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20018;&#32852;&#22810;&#20010;&#29087;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#38271;&#31243;&#20219;&#21153;&#30446;&#26631;&#65292;&#24182;&#20855;&#22791;&#33258;&#20027;&#31574;&#30053;&#20999;&#25442;&#21644;&#32469;&#36807;&#22810;&#20313;&#38454;&#27573;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30001;&#19968;&#31995;&#21015;&#20114;&#19981;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#32452;&#25104;&#12290;&#36825;&#20123;&#38271;&#31243;&#12289;&#22797;&#26434;&#30340;&#20219;&#21153;&#20984;&#26174;&#20102;&#29087;&#32451;&#25163;&#30340;&#28508;&#21147;&#65292;&#23427;&#20204;&#20855;&#26377;&#36866;&#24212;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#25235;&#21462;&#25110;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#26080;&#32541;&#22320;&#22312;&#19981;&#21516;&#21151;&#33021;&#27169;&#24335;&#20043;&#38388;&#36807;&#28193;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29087;&#32451;&#25163;&#30340;&#39640;&#32500;&#34892;&#21160;&#31354;&#38388;&#21644;&#38271;&#31243;&#20219;&#21153;&#30340;&#22797;&#26434;&#32452;&#21512;&#21160;&#21147;&#23398;&#65292;&#20135;&#29983;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#36890;&#29992;&#31995;&#32479;&#8212;&#8212;&#39034;&#24207;&#28789;&#24039;&#24615;&#65292;&#36890;&#36807;&#20018;&#32852;&#22810;&#20010;&#29087;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#38271;&#31243;&#20219;&#21153;&#30446;&#26631;&#12290;&#35813;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#28176;&#36827;&#35843;&#20248;&#23376;&#31574;&#30053;&#30340;&#36807;&#28193;&#21487;&#34892;&#24615;&#20989;&#25968;&#65292;&#29992;&#20110;&#22686;&#24378;&#20018;&#32852;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#36824;&#33021;&#22815;&#23454;&#29616;&#33258;&#20027;&#31574;&#30053;&#20999;&#25442;&#20197;&#24212;&#23545;&#22833;&#36133;&#21644;&#32469;&#36807;&#22810;&#20313;&#30340;&#38454;&#27573;&#12290;&#23613;&#31649;&#21482;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35757;&#32451;&#20102;&#20960;&#20010;&#20219;&#21153;&#23545;&#35937;&#65292;&#35813;&#31995;&#32479;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world manipulation tasks consist of a series of subtasks that are significantly different from one another. Such long-horizon, complex tasks highlight the potential of dexterous hands, which possess adaptability and versatility, capable of seamlessly transitioning between different modes of functionality without the need for re-grasping or external tools. However, the challenges arise due to the high-dimensional action space of dexterous hand and complex compositional dynamics of the long-horizon tasks. We present Sequential Dexterity, a general system based on reinforcement learning (RL) that chains multiple dexterous policies for achieving long-horizon task goals. The core of the system is a transition feasibility function that progressively finetunes the sub-policies for enhancing chaining success rate, while also enables autonomous policy-switching for recovery from failures and bypassing redundant stages. Despite being trained only in simulation with a few task objects, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16539</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions. (arXiv:2308.16539v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22810;&#26234;&#33021;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#21338;&#24328;&#35770;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#22810;&#20010;&#25361;&#25112;&#30340;&#38459;&#30861;&#65292;&#27604;&#22914;&#26410;&#30693;&#30340;&#26234;&#33021;&#20307;&#20559;&#22909;&#21644;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29616;&#26377;&#26041;&#27861;&#32479;&#19968;&#21040;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#24418;&#24335;&#21270;&#20013;&#12290;&#22312;&#36825;&#20010;&#24418;&#24335;&#21270;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21338;&#24328;&#21442;&#25968;&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#20316;&#20026;&#24402;&#32435;&#20559;&#22909;&#12290;&#20351;&#29992;&#27169;&#25311;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#34892;&#20154;&#30456;&#20114;&#20316;&#29992;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#21338;&#24328;&#35770;&#23618;&#25913;&#21892;&#20102;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#20027;&#24178;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game theory offers an interpretable mathematical framework for modeling multi-agent interactions. However, its applicability in real-world robotics applications is hindered by several challenges, such as unknown agents' preferences and goals. To address these challenges, we show a connection between differential games, optimal control, and energy-based models and demonstrate how existing approaches can be unified under our proposed Energy-based Potential Game formulation. Building upon this formulation, this work introduces a new end-to-end learning application that combines neural networks for game-parameter inference with a differentiable game-theoretic optimization layer, acting as an inductive bias. The experiments using simulated mobile robot pedestrian interactions and real-world automated driving data provide empirical evidence that the game-theoretic layer improves the predictive performance of various neural network backbones.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#32508;&#21512;&#25253;&#36947;&#65292;&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#21452;&#37325;&#29992;&#36884;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#31038;&#21306;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.14840</link><description>&lt;p&gt;
&#35782;&#21035;&#21644;&#20943;&#36731;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Identifying and Mitigating the Security Risks of Generative AI. (arXiv:2308.14840v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14840
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#32508;&#21512;&#25253;&#36947;&#65292;&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#21452;&#37325;&#29992;&#36884;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#31038;&#21306;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#19968;&#39033;&#37325;&#22823;&#25216;&#26415;&#21457;&#26126;&#37117;&#20250;&#24102;&#26469;&#21452;&#37325;&#29992;&#36884;&#30340;&#22256;&#22659; - &#26032;&#25216;&#26415;&#26082;&#26377;&#21487;&#33021;&#34987;&#29992;&#20110;&#21892;&#33391;&#65292;&#20063;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#34892;&#20026;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#25216;&#26415;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20195;&#30721;&#34917;&#20840;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;&#65289;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#21516;&#26679;&#21487;&#20197;&#21033;&#29992;GenAI&#29983;&#25104;&#26032;&#30340;&#25915;&#20987;&#65292;&#24182;&#22686;&#21152;&#29616;&#26377;&#25915;&#20987;&#30340;&#36895;&#24230;&#21644;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#22312;Google&#20030;&#21150;&#30340;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#21457;&#29616;&#65288;&#30001;&#26031;&#22374;&#31119;&#22823;&#23398;&#21644;&#23041;&#26031;&#24247;&#26143;&#22823;&#23398;&#40614;&#36842;&#36874;&#20998;&#26657;&#20849;&#21516;&#32452;&#32455;&#65289;&#12290;&#26412;&#25991;&#24182;&#19981;&#24847;&#21619;&#30528;&#20840;&#38754;&#65292;&#32780;&#26159;&#35797;&#22270;&#32508;&#21512;&#19968;&#20123;&#26377;&#36259;&#30340;&#30740;&#35752;&#20250;&#21457;&#29616;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#20027;&#39064;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#35770;&#25991;&#26082;&#20026;&#36825;&#20010;&#37325;&#35201;&#20027;&#39064;&#30340;&#35752;&#35770;&#25552;&#20379;&#19968;&#20010;&#36215;&#28857;&#65292;&#20063;&#24341;&#36215;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every major technical invention resurfaces the dual-use dilemma -- the new technology has the potential to be used for good as well as for harm. Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks.  This paper reports the findings of a workshop held at Google (co-organized by Stanford University and the University of Wisconsin-Madison) on the dual-use dilemma posed by GenAI. This paper is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. We discuss short-term and long-term goals for the community on this topic. We hope this paper provides both a launching point for a discussion on this important topic as well as interest
&lt;/p&gt;</description></item><item><title>&#31639;&#27861;&#21457;&#29616;&#20102;&#22823;&#37327;&#36830;&#20998;&#25968;&#20844;&#24335;&#65292;&#25581;&#31034;&#20102;&#31216;&#20026;&#20445;&#23432;&#30697;&#38453;&#22330;&#30340;&#26032;&#39062;&#25968;&#23398;&#32467;&#26500;&#65292;&#32479;&#19968;&#20102;&#25968;&#21315;&#20010;&#24050;&#30693;&#30340;&#20844;&#24335;&#65292;&#29983;&#25104;&#20102;&#26080;&#38480;&#22810;&#30340;&#26032;&#20844;&#24335;&#65292;&#24182;&#23548;&#33268;&#19981;&#21516;&#25968;&#23398;&#24120;&#25968;&#20043;&#38388;&#24847;&#24819;&#19981;&#21040;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.11829</link><description>&lt;p&gt;
&#31639;&#27861;&#36741;&#21161;&#19979;&#23545;&#25968;&#23398;&#24120;&#25968;&#20043;&#38388;&#20869;&#22312;&#39034;&#24207;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Algorithm-assisted discovery of an intrinsic order among mathematical constants. (arXiv:2308.11829v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11829
&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#21457;&#29616;&#20102;&#22823;&#37327;&#36830;&#20998;&#25968;&#20844;&#24335;&#65292;&#25581;&#31034;&#20102;&#31216;&#20026;&#20445;&#23432;&#30697;&#38453;&#22330;&#30340;&#26032;&#39062;&#25968;&#23398;&#32467;&#26500;&#65292;&#32479;&#19968;&#20102;&#25968;&#21315;&#20010;&#24050;&#30693;&#30340;&#20844;&#24335;&#65292;&#29983;&#25104;&#20102;&#26080;&#38480;&#22810;&#30340;&#26032;&#20844;&#24335;&#65292;&#24182;&#23548;&#33268;&#19981;&#21516;&#25968;&#23398;&#24120;&#25968;&#20043;&#38388;&#24847;&#24819;&#19981;&#21040;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#31639;&#27861;&#23545;&#25968;&#23398;&#39046;&#22495;&#30340;&#21457;&#29616;&#21457;&#25381;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#25506;&#32034;&#22823;&#21442;&#25968;&#31354;&#38388;&#26041;&#38754;&#65292;&#20154;&#31867;&#21487;&#33021;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#25165;&#33021;&#23436;&#25104;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#21644;&#31639;&#27861;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#20154;&#31867;&#30452;&#35273;&#19982;&#35745;&#31639;&#26426;&#31639;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#20250;&#23548;&#33268;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#30340;&#21457;&#29616;&#65292;&#21542;&#21017;&#36825;&#20123;&#27010;&#24565;&#23558;&#38590;&#20197;&#23547;&#25214;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#24182;&#34892;&#30340;&#35745;&#31639;&#26426;&#31639;&#27861;&#65292;&#21457;&#29616;&#20102;&#22823;&#37327;&#30340;&#36830;&#20998;&#25968;&#20844;&#24335;&#65292;&#29992;&#20110;&#22522;&#26412;&#30340;&#25968;&#23398;&#24120;&#25968;&#12290;&#31639;&#27861;&#21457;&#29616;&#30340;&#20844;&#24335;&#25968;&#37327;&#20043;&#22810;&#25581;&#31034;&#20102;&#19968;&#20010;&#31216;&#20026;&#20445;&#23432;&#30697;&#38453;&#22330;&#30340;&#26032;&#39062;&#25968;&#23398;&#32467;&#26500;&#12290;&#36825;&#20123;&#30697;&#38453;&#22330;(1)&#32479;&#19968;&#20102;&#25968;&#21315;&#20010;&#24050;&#30693;&#30340;&#20844;&#24335;&#65292;(2)&#29983;&#25104;&#20102;&#26080;&#38480;&#22810;&#30340;&#26032;&#20844;&#24335;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;(3)&#23548;&#33268;&#20102;&#19981;&#21516;&#25968;&#23398;&#24120;&#25968;&#20043;&#38388;&#24847;&#24819;&#19981;&#21040;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
In recent decades, a growing number of discoveries in fields of mathematics have been assisted by computer algorithms, primarily for exploring large parameter spaces that humans would take too long to investigate. As computers and algorithms become more powerful, an intriguing possibility arises - the interplay between human intuition and computer algorithms can lead to discoveries of novel mathematical concepts that would otherwise remain elusive. To realize this perspective, we have developed a massively parallel computer algorithm that discovers an unprecedented number of continued fraction formulas for fundamental mathematical constants. The sheer number of formulas discovered by the algorithm unveils a novel mathematical structure that we call the conservative matrix field. Such matrix fields (1) unify thousands of existing formulas, (2) generate infinitely many new formulas, and most importantly, (3) lead to unexpected relations between different mathematical constants, including
&lt;/p&gt;</description></item><item><title>RaLLe&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;R-LLMs&#65289;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#23494;&#38598;&#22411;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10633</link><description>&lt;p&gt;
RaLLe&#65306;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models. (arXiv:2308.10633v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10633
&lt;/p&gt;
&lt;p&gt;
RaLLe&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;R-LLMs&#65289;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#23494;&#38598;&#22411;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;R-LLMs&#65289;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20107;&#23454;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29992;&#20110;&#26500;&#24314;R-LLMs&#30340;&#24211;&#25552;&#20379;&#20102;&#39640;&#32423;&#25277;&#35937;&#65292;&#20294;&#22312;&#35780;&#20272;&#21644;&#20248;&#21270;&#29305;&#23450;&#25512;&#29702;&#36807;&#31243;&#65288;&#22914;&#26816;&#32034;&#21644;&#29983;&#25104;&#65289;&#20013;&#30340;&#25552;&#31034;&#26102;&#32570;&#20047;&#36275;&#22815;&#30340;&#36879;&#26126;&#24230;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RaLLe&#65292;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;R-LLMs&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#24320;&#21457;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#12290;&#20351;&#29992;RaLLe&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#24320;&#21457;&#21644;&#35780;&#20272;R-LLMs&#65292;&#25913;&#36827;&#25163;&#24037;&#25552;&#31034;&#65292;&#35780;&#20272;&#20010;&#21035;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#23450;&#37327;&#22320;&#23458;&#35266;&#34913;&#37327;&#25972;&#20307;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21151;&#33021;&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#25552;&#39640;R-LLMs&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap, we present RaLLe, an open-source framework designed to facilitate the development, evaluation, and optimization of R-LLMs for knowledge-intensive tasks. With RaLLe, developers can easily develop and evaluate R-LLMs, improving hand-crafted prompts, assessing individual inference processes, and objectively measuring overall system performance quantitatively. By leveraging these features, developers can enhance the performance and accuracy of their R-LLMs in knowledge-intensive generation tasks. We open-source our code at https://github.com/yhoshi3/RaLLe.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.10335</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#30721;&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21644;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#38750;&#20961;&#33021;&#21147;&#12290;&#24403;&#36935;&#21040;&#32534;&#30721;&#38382;&#39064;&#26102;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#24120;&#24120;&#20250;&#21672;&#35810;LLMs&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#21162;&#21147;&#26469;&#36991;&#20813;&#35821;&#27861;&#38169;&#35823;&#24182;&#20351;&#20195;&#30721;&#19982;&#39044;&#26399;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#20294;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#23578;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#29615;&#22659;&#20013;&#65292;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#31561;&#21516;&#20110;&#21487;&#38752;&#21644;&#40065;&#26834;&#30340;&#20195;&#30721;&#12290;&#22312;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#28389;&#29992;API&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#22914;&#36164;&#28304;&#27844;&#28431;&#12289;&#31243;&#24207;&#23849;&#28291;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;LLM&#20195;&#30721;&#29983;&#25104;&#26381;&#21153;&#30340;&#29992;&#25143;&#23454;&#38469;&#19978;&#26159;&#26368;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#30475;&#20284;&#27491;&#30830;&#30340;&#20195;&#30721;&#24433;&#21709;&#30340;&#24320;&#21457;&#32773;&#8212;&#8212;&#20182;&#20204;&#36890;&#24120;&#26159;&#19981;&#29087;&#24713;LLMs&#20026;&#20182;&#20204;&#29983;&#25104;&#20195;&#30721;&#30340;API&#30340;&#21021;&#32423;&#24320;&#21457;&#32773;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;API&#30340;&#28389;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in t
&lt;/p&gt;</description></item><item><title>PoSynDA&#26694;&#26550;&#36890;&#36807;&#27169;&#25311;&#30446;&#26631;&#22495;&#20013;&#30340;3D&#23039;&#21183;&#20998;&#24067;&#21644;&#20351;&#29992;&#22810;&#20551;&#35774;&#32593;&#32476;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;&#36890;&#36807;&#25945;&#24072;-&#23398;&#29983;&#33539;&#24335;&#21644;&#20302;&#31209;&#33258;&#36866;&#24212;&#36827;&#19968;&#27493;&#20248;&#21270;&#35813;&#36807;&#31243;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09678</link><description>&lt;p&gt;
PoSynDA: &#22810;&#20551;&#35774;&#23039;&#21183;&#21512;&#25104;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#40065;&#26834;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D Human Pose Estimation. (arXiv:2308.09678v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09678
&lt;/p&gt;
&lt;p&gt;
PoSynDA&#26694;&#26550;&#36890;&#36807;&#27169;&#25311;&#30446;&#26631;&#22495;&#20013;&#30340;3D&#23039;&#21183;&#20998;&#24067;&#21644;&#20351;&#29992;&#22810;&#20551;&#35774;&#32593;&#32476;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;&#36890;&#36807;&#25945;&#24072;-&#23398;&#29983;&#33539;&#24335;&#21644;&#20302;&#31209;&#33258;&#36866;&#24212;&#36827;&#19968;&#27493;&#20248;&#21270;&#35813;&#36807;&#31243;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#22120;&#38754;&#20020;&#30528;&#36866;&#24212;&#26032;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#38598;&#20013;&#32570;&#20047;2D-3D&#23039;&#21183;&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PoSynDA&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#30446;&#26631;&#22495;&#20013;&#30340;3D&#23039;&#21183;&#20998;&#24067;&#26469;&#24357;&#21512;&#25968;&#25454;&#24046;&#36317;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;PoSynDA&#20351;&#29992;&#19968;&#20010;&#21463;&#25193;&#25955;&#21551;&#21457;&#30340;&#32467;&#26500;&#26469;&#27169;&#25311;&#30446;&#26631;&#22495;&#20013;&#30340;3D&#23039;&#21183;&#20998;&#24067;&#12290;&#36890;&#36807;&#21152;&#20837;&#22810;&#20551;&#35774;&#32593;&#32476;&#65292;PoSynDA&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23039;&#21183;&#20551;&#35774;&#65292;&#24182;&#23558;&#20854;&#19982;&#30446;&#26631;&#22495;&#23545;&#40784;&#12290;&#20026;&#27492;&#65292;&#23427;&#39318;&#20808;&#21033;&#29992;&#30446;&#26631;&#29305;&#23450;&#30340;&#28304;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#32553;&#25918;&#21644;&#20301;&#32622;&#21442;&#25968;&#65292;&#20174;&#28304;&#22495;&#20013;&#33719;&#21462;&#30446;&#26631;&#22495;&#20998;&#24067;&#25968;&#25454;&#12290;&#28982;&#21518;&#36890;&#36807;&#25945;&#24072;-&#23398;&#29983;&#33539;&#24335;&#21644;&#20302;&#31209;&#33258;&#36866;&#24212;&#36827;&#19968;&#27493;&#20248;&#21270;&#35813;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;Human3.6M&#21644;MPI-INF-3DHP&#31561;&#22522;&#20934;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;PoSynDA&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing 3D human pose estimators face challenges in adapting to new datasets due to the lack of 2D-3D pose pairs in training sets. To overcome this issue, we propose \textit{Multi-Hypothesis \textbf{P}ose \textbf{Syn}thesis \textbf{D}omain \textbf{A}daptation} (\textbf{PoSynDA}) framework to bridge this data disparity gap in target domain. Typically, PoSynDA uses a diffusion-inspired structure to simulate 3D pose distribution in the target domain. By incorporating a multi-hypothesis network, PoSynDA generates diverse pose hypotheses and aligns them with the target domain. To do this, it first utilizes target-specific source augmentation to obtain the target domain distribution data from the source domain by decoupling the scale and position parameters. The process is then further refined through the teacher-student paradigm and low-rank adaptation. With extensive comparison of benchmarks such as Human3.6M and MPI-INF-3DHP, PoSynDA demonstrates competitive performance, even comparable 
&lt;/p&gt;</description></item><item><title>&#27602;&#31661;&#34521;&#26159;&#19968;&#31181;&#26080;&#26631;&#31614;&#25915;&#20987;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#30446;&#26631;&#31867;&#21035;&#30340;&#30693;&#35782;&#12290;&#23427;&#20855;&#26377;&#20302;&#20013;&#27602;&#29575;&#21644;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.09487</link><description>&lt;p&gt;
&#27602;&#31661;&#34521;&#65306;&#19968;&#31181;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#20302;&#20013;&#27602;&#29575;&#21644;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#26080;&#26631;&#31614;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High Attack Success Rate in the Absence of Training Data. (arXiv:2308.09487v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09487
&lt;/p&gt;
&lt;p&gt;
&#27602;&#31661;&#34521;&#26159;&#19968;&#31181;&#26080;&#26631;&#31614;&#25915;&#20987;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#30446;&#26631;&#31867;&#21035;&#30340;&#30693;&#35782;&#12290;&#23427;&#20855;&#26377;&#20302;&#20013;&#27602;&#29575;&#21644;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25104;&#21151;&#21457;&#21160;&#21518;&#38376;&#25915;&#20987;&#65292;&#27880;&#20837;&#30340;&#25968;&#25454;&#38656;&#35201;&#34987;&#27491;&#30830;&#26631;&#35760;&#65292;&#21542;&#21017;&#65292;&#21363;&#20351;&#26159;&#22522;&#26412;&#30340;&#25968;&#25454;&#36807;&#28388;&#22120;&#20063;&#33021;&#36731;&#26131;&#26816;&#27979;&#20986;&#26469;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#20102;&#26080;&#26631;&#31614;&#25915;&#20987;&#30340;&#27010;&#24565;&#65292;&#36825;&#31181;&#25915;&#20987;&#26356;&#21152;&#21361;&#38505;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#25913;&#21464;&#27880;&#20837;&#25968;&#25454;&#30340;&#26631;&#31614;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#26080;&#26631;&#31614;&#21518;&#38376;&#25915;&#20987;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#23545;&#25972;&#20010;&#35757;&#32451;&#38598;&#25110;&#20854;&#20013;&#19968;&#37096;&#20998;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25915;&#20987;&#32773;&#24456;&#38590;&#25317;&#26377;&#36825;&#20123;&#29702;&#35299;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#36890;&#24120;&#26469;&#33258;&#22810;&#20010;&#29420;&#31435;&#30340;&#26469;&#28304;&#12290;&#19982;&#25152;&#26377;&#24403;&#21069;&#30340;&#26080;&#26631;&#31614;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#26631;&#31614;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;&#27602;&#31661;&#34521;&#8221;&#12290;&#27602;&#31661;&#34521;&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#20102;&#35299;&#25915;&#20987;&#30446;&#26631;&#31867;&#21035;&#65292;&#27604;&#22914;&#8220;&#34521;&#8221;&#12290;&#22312;CIFAR10&#12289;Tiny-ImageNet&#21644;TSRD&#19978;&#65292;&#20165;&#38656;&#35201;&#20998;&#21035;&#21344;&#35757;&#32451;&#38598;&#22823;&#23567;&#30340;0.1%&#12289;0.025%&#21644;0.4%&#30340;&#20013;&#27602;&#29575;&#65292;&#27602;&#31661;&#34521;&#23601;&#33021;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
To successfully launch backdoor attacks, injected data needs to be correctly labeled; otherwise, they can be easily detected by even basic data filters. Hence, the concept of clean-label attacks was introduced, which is more dangerous as it doesn't require changing the labels of injected data. To the best of our knowledge, the existing clean-label backdoor attacks largely relies on an understanding of the entire training set or a portion of it. However, in practice, it is very difficult for attackers to have it because of training datasets often collected from multiple independent sources. Unlike all current clean-label attacks, we propose a novel clean label method called 'Poison Dart Frog'. Poison Dart Frog does not require access to any training data; it only necessitates knowledge of the target class for the attack, such as 'frog'. On CIFAR10, Tiny-ImageNet, and TSRD, with a mere 0.1\%, 0.025\%, and 0.4\% poisoning rate of the training set size, respectively, Poison Dart Frog achie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25915;&#20987;&#35270;&#20026;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#27450;&#39575;&#24615;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08925</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A White-Box False Positive Adversarial Attack Method on Contrastive Loss-Based Offline Handwritten Signature Verification Models. (arXiv:2308.08925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25915;&#20987;&#35270;&#20026;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#27450;&#39575;&#24615;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#23558;&#25915;&#20987;&#35270;&#20026;&#22312;&#23494;&#20999;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#12290;&#20026;&#20102;&#24341;&#23548;&#27450;&#39575;&#24615;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25200;&#21160;&#21407;&#22987;&#26679;&#26412;&#19982;&#21512;&#25104;&#26679;&#26412;&#30340;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;&#27431;&#27663;&#36317;&#31163;&#26469;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36890;&#36807;&#20943;&#23567;&#29983;&#25104;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20445;&#35777;&#26368;&#23567;&#30340;&#25200;&#21160;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35823;&#25253;&#25915;&#20987;&#26041;&#27861;&#12289;&#20004;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#12289;&#26377;&#25928;&#30340;&#20070;&#20889;&#39118;&#26684;&#36716;&#25442;&#20197;&#21450;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we tackle the challenge of white-box false positive adversarial attacks on contrastive loss-based offline handwritten signature verification models. We propose a novel attack method that treats the attack as a style transfer between closely related but distinct writing styles. To guide the generation of deceptive images, we introduce two new loss functions that enhance the attack success rate by perturbing the Euclidean distance between the embedding vectors of the original and synthesized samples, while ensuring minimal perturbations by reducing the difference between the generated image and the original image. Our method demonstrates state-of-the-art performance in white-box attacks on contrastive loss-based offline handwritten signature verification models, as evidenced by our experiments. The key contributions of this paper include a novel false positive attack method, two new loss functions, effective style transfer in handwriting styles, and superior performance in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#24322;&#24120;&#20998;&#21106;&#30340;&#22810;&#31890;&#24230;&#36328;&#39046;&#22495;&#23545;&#40784;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#28304;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;&#21644;&#36328;&#39046;&#22495;&#24322;&#24120;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#26080;&#32541;&#38598;&#25104;&#20102;&#22810;&#39046;&#22495;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08696</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#31890;&#24230;&#36328;&#39046;&#22495;&#23545;&#40784;&#25913;&#36827;&#24322;&#24120;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Improving Anomaly Segmentation with Multi-Granularity Cross-Domain Alignment. (arXiv:2308.08696v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#24322;&#24120;&#20998;&#21106;&#30340;&#22810;&#31890;&#24230;&#36328;&#39046;&#22495;&#23545;&#40784;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#28304;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;&#21644;&#36328;&#39046;&#22495;&#24322;&#24120;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#26080;&#32541;&#38598;&#25104;&#20102;&#22810;&#39046;&#22495;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20998;&#21106;&#22312;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#23545;&#35937;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#36947;&#36335;&#24322;&#24120;&#26816;&#27979;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#22312;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#24322;&#24120;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#19982;&#30495;&#23454;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22797;&#26434;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#24322;&#24120;&#20998;&#21106;&#30340;&#22810;&#31890;&#24230;&#36328;&#39046;&#22495;&#23545;&#40784;&#65288;MGCDA&#65289;&#26694;&#26550;&#12290;&#23427;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#28304;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;&#65288;MDAT&#65289;&#27169;&#22359;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#39046;&#22495;&#24322;&#24120;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;CACL&#65289;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#26080;&#32541;&#22320;&#22312;&#22330;&#26223;&#21644;&#26679;&#26412;&#32423;&#21035;&#38598;&#25104;&#22810;&#39046;&#22495;&#25968;&#25454;&#12290;&#22810;&#28304;&#39046;&#22495;&#23545;&#25239;&#25439;&#22833;&#21644;&#21160;&#24577;&#26631;&#31614;&#24179;&#28369;&#31574;&#30053;&#34987;&#25972;&#21512;&#21040;MDAT&#27169;&#22359;&#20013;&#65292;&#20197;&#20419;&#36827;&#22312;&#22330;&#26223;&#32423;&#21035;&#19978;&#33719;&#24471;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#36890;&#36807;&#23545;&#25239;&#29983;&#25104;&#39046;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly segmentation plays a crucial role in identifying anomalous objects within images, which facilitates the detection of road anomalies for autonomous driving. Although existing methods have shown impressive results in anomaly segmentation using synthetic training data, the domain discrepancies between synthetic training data and real test data are often neglected. To address this issue, the Multi-Granularity Cross-Domain Alignment (MGCDA) framework is proposed for anomaly segmentation in complex driving environments. It uniquely combines a new Multi-source Domain Adversarial Training (MDAT) module and a novel Cross-domain Anomaly-aware Contrastive Learning (CACL) method to boost the generality of the model, seamlessly integrating multi-domain data at both scene and sample levels. Multi-source domain adversarial loss and a dynamic label smoothing strategy are integrated into the MDAT module to facilitate the acquisition of domain-invariant features at the scene level, through adver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#24322;&#24120;&#31243;&#24230;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#12290;&#27169;&#22411;&#30340;&#27744;&#21270;&#36807;&#31243;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07774</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#22270;&#32534;&#30721;&#35299;&#30721;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection. (arXiv:2308.07774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#24322;&#24120;&#31243;&#24230;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#12290;&#27169;&#22411;&#30340;&#27744;&#21270;&#36807;&#31243;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#26159;&#27744;&#21270;&#25805;&#20316;&#65292;&#23427;&#26088;&#22312;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#21516;&#26102;&#20445;&#30041;&#37325;&#35201;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#27744;&#21270;&#31574;&#30053;&#20381;&#36182;&#20110;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#33719;&#24471;&#30340;&#20998;&#37197;&#30697;&#38453;&#65292;&#35813;&#30697;&#38453;&#20855;&#26377;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#65292;&#24448;&#24448;&#23548;&#33268;&#26174;&#33879;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#27744;&#21270;&#36807;&#31243;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#24322;&#24120;&#35780;&#20998;&#20989;&#25968;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#65292;&#20174;&#32780;&#26816;&#27979;&#20986;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#21629;&#21517;&#20026;LCPool&#65292;&#23427;&#21033;&#29992;&#23616;&#37096;&#32422;&#26463;&#32447;&#24615;&#32534;&#30721;&#36827;&#34892;&#29305;&#24449;&#32534;&#30721;&#65292;&#36890;&#36807;&#27714;&#35299;&#24102;&#26377;&#23616;&#37096;&#27491;&#21017;&#21270;&#39033;&#30340;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#38382;&#39064;&#26469;&#25214;&#21040;&#32858;&#31867;&#20998;&#37197;&#30697;&#38453;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;&#23616;&#37096;&#32422;&#26463;&#65292;LCPool&#34987;&#35774;&#35745;&#25104;&#20813;&#36153;
&lt;/p&gt;
&lt;p&gt;
A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information. However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process. In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality. In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term. By enforcing locality constraints during the coding process, LCPool is designed to be free fr
&lt;/p&gt;</description></item><item><title>PokerKit&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#65292;&#25552;&#20379;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#25903;&#25345;&#21644;&#28789;&#27963;&#30340;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#65292;&#23545;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.07327</link><description>&lt;p&gt;
PokerKit: &#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#30340;&#20840;&#38754;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations. (arXiv:2308.07327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07327
&lt;/p&gt;
&lt;p&gt;
PokerKit&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#65292;&#25552;&#20379;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#25903;&#25345;&#21644;&#28789;&#27963;&#30340;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#65292;&#23545;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PokerKit&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#21644;&#25163;&#29260;&#35780;&#20272;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#21482;&#25903;&#25345;&#23569;&#37327;&#25169;&#20811;&#21464;&#20307;&#65292;&#24182;&#19988;&#22312;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;PokerKit&#36890;&#36807;&#25903;&#25345;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#30340;&#26550;&#26500;&#20379;&#29992;&#25143;&#23450;&#20041;&#33258;&#23450;&#20041;&#28216;&#25103;&#65292;&#26174;&#33879;&#25193;&#22823;&#20102;&#36825;&#19968;&#33539;&#22260;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;PokerKit&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#21253;&#25324;&#20854;&#30452;&#35266;&#30340;&#32534;&#31243;API&#65292;&#22810;&#21464;&#20307;&#28216;&#25103;&#25903;&#25345;&#20197;&#21450;&#32479;&#19968;&#30340;&#25163;&#29260;&#35780;&#20272;&#22871;&#20214;&#22312;&#19981;&#21516;&#25163;&#29260;&#31867;&#22411;&#38388;&#30340;&#24212;&#29992;&#12290;PokerKit&#30340;&#28789;&#27963;&#24615;&#20351;&#20854;&#33021;&#22815;&#22312;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#12290;PokerKit&#30340;&#21487;&#38752;&#24615;&#36890;&#36807;&#38745;&#24577;&#31867;&#22411;&#26816;&#26597;&#12289;&#24191;&#27867;&#30340;doctest&#21644;&#21333;&#20803;&#27979;&#35797;&#26469;&#30830;&#20445;&#65292;&#36798;&#21040;&#20102;97%&#30340;&#20195;&#30721;&#35206;&#30422;&#29575;&#12290;&#24341;&#20837;PokerKit&#20195;&#34920;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
PokerKit is an open-source Python library designed to overcome the restrictions of existing poker game simulation and hand evaluation tools, which typically support only a handful of poker variants and lack flexibility in game state control. In contrast, PokerKit significantly expands this scope by supporting an extensive array of poker variants and it provides a flexible architecture for users to define their custom games. This paper details the design and implementation of PokerKit, including its intuitive programmatic API, multi-variant game support, and a unified hand evaluation suite across different hand types. The flexibility of PokerKit allows for applications in diverse areas, such as poker AI development, tool creation, and online poker casino implementation. PokerKit's reliability has been established through static type checking, extensive doctests, and unit tests, achieving 97\% code coverage. The introduction of PokerKit represents a significant contribution to the field 
&lt;/p&gt;</description></item><item><title>SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03291</link><description>&lt;p&gt;
SynJax: JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
SynJax: Structured Probability Distributions for JAX. (arXiv:2308.03291v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03291
&lt;/p&gt;
&lt;p&gt;
SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#24211;&#30340;&#21457;&#23637;&#20351;&#24471;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#19987;&#27880;&#20110;&#24314;&#27169;&#65292;&#21516;&#26102;&#35753;&#24211;&#26469;&#22788;&#29702;&#38024;&#23545;&#29616;&#20195;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#20248;&#21270;&#25191;&#34892;&#30340;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#30410;&#65292;&#20363;&#22914;Transformer&#65292;&#20854;&#22522;&#26412;&#25805;&#20316;&#26131;&#20110;&#26144;&#23556;&#21040;&#21521;&#37327;&#21270;&#35745;&#31639;&#12290;&#32780;&#23545;&#20110;&#26174;&#24335;&#32771;&#34385;&#32467;&#26500;&#21270;&#23545;&#35937;&#65288;&#22914;&#26641;&#21644;&#20998;&#21106;&#65289;&#30340;&#27169;&#22411;&#65292;&#24182;&#27809;&#26377;&#21516;&#26679;&#30340;&#21463;&#30410;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23450;&#21046;&#30340;&#38590;&#20197;&#20197;&#21521;&#37327;&#21270;&#24418;&#24335;&#23454;&#29616;&#30340;&#31639;&#27861;&#12290;SynJax&#36890;&#36807;&#25552;&#20379;&#29992;&#20110;&#32467;&#26500;&#21270;&#20998;&#24067;&#30340;&#25512;&#29702;&#31639;&#27861;&#30340;&#39640;&#25928;&#21521;&#37327;&#21270;&#23454;&#29616;&#26469;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#40784;&#12289;&#26631;&#35760;&#12289;&#20998;&#21106;&#12289;&#32452;&#25104;&#26641;&#21644;&#29983;&#25104;&#26641;&#30340;&#22788;&#29702;&#12290;&#20351;&#29992;SynJax&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#21487;&#24494;&#20998;&#27169;&#22411;&#65292;&#26174;&#24335;&#22320;&#23545;&#25968;&#25454;&#30340;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#12290;&#20195;&#30721;&#21487;&#22312;https://g&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.  SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://g
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#24052;&#35199;&#20840;&#22269;&#31185;&#23398;&#31454;&#36187;&#36807;&#21435;20&#24180;&#30340;&#25968;&#25454;&#65292;&#21457;&#29616;&#23398;&#29983;&#20204;&#22312;&#31185;&#23398;&#25506;&#31350;&#20013;&#25506;&#32034;&#20102;&#24191;&#27867;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25903;&#25345;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25945;&#23398;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02962</link><description>&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#26159;&#20026;&#20102;&#20160;&#20040;&#65311;&#23545;&#31185;&#23398;&#31454;&#36187;&#20013;&#23398;&#29983;&#39033;&#30446;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Science and engineering for what? A large-scale analysis of students' projects in science fairs. (arXiv:2308.02962v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02962
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#24052;&#35199;&#20840;&#22269;&#31185;&#23398;&#31454;&#36187;&#36807;&#21435;20&#24180;&#30340;&#25968;&#25454;&#65292;&#21457;&#29616;&#23398;&#29983;&#20204;&#22312;&#31185;&#23398;&#25506;&#31350;&#20013;&#25506;&#32034;&#20102;&#24191;&#27867;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25903;&#25345;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25945;&#23398;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#31454;&#36187;&#20026;K-12&#23398;&#29983;&#25552;&#20379;&#20102;&#21442;&#19982;&#30495;&#23454;STEM&#23454;&#36341;&#30340;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#20998;&#26512;&#20102;&#24052;&#35199;&#20840;&#22269;&#31185;&#23398;&#31454;&#36187;&#36807;&#21435;20&#24180;&#20013;&#36229;&#36807;5000&#20010;&#39033;&#30446;&#30340;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#25512;&#21160;&#23398;&#29983;&#25506;&#31350;&#21644;&#35774;&#35745;&#30340;&#20027;&#35201;&#20027;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#23398;&#29983;&#20204;&#25506;&#32034;&#20102;&#24191;&#27867;&#30340;&#20027;&#39064;&#65292;&#24182;&#19988;&#38543;&#26102;&#38388;&#12289;&#22320;&#21306;&#21644;&#23398;&#26657;&#29615;&#22659;&#30340;&#21464;&#21270;&#32780;&#20135;&#29983;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32467;&#26524;&#21644;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25903;&#25345;&#36827;&#19968;&#27493;&#30340;&#31185;&#23398;&#31454;&#36187;&#30740;&#31350;&#65292;&#36824;&#21487;&#20197;&#20026;&#19981;&#21516;&#29615;&#22659;&#20013;&#25903;&#25345;&#23398;&#29983;&#24320;&#23637;&#24320;&#25918;&#24615;&#25506;&#31350;&#27963;&#21160;&#30340;&#25945;&#23398;&#21644;&#35774;&#35745;&#25552;&#20379;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Science and Engineering fairs offer K-12 students opportunities to engage with authentic STEM practices. Particularly, students are given the chance to experience authentic and open inquiry processes, by defining which themes, questions and approaches will guide their scientific endeavors. In this study, we analyzed data from over 5,000 projects presented at a nationwide science fair in Brazil over the past 20 years using topic modeling to identify the main topics that have driven students' inquiry and design. Our analysis identified a broad range of topics being explored, with significant variations over time, region, and school setting. We argue those results and proposed methodology can not only support further research in the context of science fairs, but also inform instruction and design of contexts-specific resources to support students in open inquiry experiences in different settings.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39046;&#22495;&#28418;&#31227;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;DNN&#21644;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26816;&#27979;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;... (&#25688;&#35201;&#20869;&#23481;&#30465;&#30053;)</title><link>http://arxiv.org/abs/2308.02084</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#31471;&#30340;&#39640;&#25928;&#27169;&#22411;&#36866;&#24212;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Model Adaptation for Continual Learning at the Edge. (arXiv:2308.02084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02084
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39046;&#22495;&#28418;&#31227;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;DNN&#21644;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26816;&#27979;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;... (&#25688;&#35201;&#20869;&#23481;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#22266;&#23450;&#21644;&#21305;&#37197;&#30340;&#65292;&#20294;&#36825;&#36890;&#24120;&#26159;&#38169;&#35823;&#30340;&#20551;&#35774;&#12290;&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#22312;&#30495;&#23454;&#35774;&#22791;&#19978;&#26102;&#65292;&#25968;&#25454;&#20998;&#24067;&#24120;&#24120;&#20250;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#21407;&#22240;&#26159;&#29615;&#22659;&#22240;&#32032;&#12289;&#20256;&#24863;&#22120;&#29305;&#24615;&#21644;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39046;&#22495;&#28418;&#31227;&#19979;&#30340;&#39640;&#25928;&#25345;&#32493;&#23398;&#20064;&#12290;EAR&#26694;&#26550;&#21033;&#29992;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;EAR&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#23558;DNN&#19982;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#30456;&#32467;&#21512;&#65292;&#26816;&#27979;&#20986;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;
&lt;/p&gt;
&lt;p&gt;
Most machine learning (ML) systems assume stationary and matching data distributions during training and deployment. This is often a false assumption. When ML models are deployed on real devices, data distributions often shift over time due to changes in environmental factors, sensor characteristics, and task-of-interest. While it is possible to have a human-in-the-loop to monitor for distribution shifts and engineer new architectures in response to these shifts, such a setup is not cost-effective. Instead, non-stationary automated ML (AutoML) models are needed. This paper presents the Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning under domain shifts. The EAR framework uses a fixed deep neural network (DNN) feature encoder and trains shallow networks on top of the encoder to handle novel data. The EAR framework is capable of 1) detecting when new data is out-of-distribution (OOD) by combining DNNs with hyperdimensional computing (HDC), 2) identifying l
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.01937</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01937
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#19978;&#35757;&#32451;&#19981;&#21516;&#25193;&#25955;&#27169;&#22411;&#65288;&#25110;&#25552;&#31034;&#65289;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#23396;&#31435;&#29366;&#24577;&#19979;&#12289;&#22312;&#19981;&#21516;&#26102;&#38388;&#12289;&#22312;&#19981;&#21516;&#20998;&#24067;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#21518;&#32493;&#32452;&#21512;&#20197;&#36798;&#21040;&#19982;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#30340;&#29702;&#24819;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#21482;&#21253;&#21547;&#20854;&#22312;&#35757;&#32451;&#26399;&#38388;&#25509;&#35302;&#21040;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#12290;&#29305;&#21035;&#26159;&#65292;CDM&#26159;&#31532;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#36951;&#24536;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20801;&#35768;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;CDM&#36824;&#21487;&#20197;&#30830;&#23450;&#29983;&#25104;&#29305;&#23450;&#26679;&#26412;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.15343</link><description>&lt;p&gt;
Med-HALT:&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15343
&lt;/p&gt;
&lt;p&gt;
Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20851;&#27880;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#32972;&#26223;&#19979;&#12290;&#24187;&#35273;&#25351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#20102;&#21512;&#29702;&#20294;&#26410;&#32463;&#39564;&#35777;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#21307;&#30103;&#24212;&#29992;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;Med-HALT&#65288;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24187;&#35273;&#12290;Med-HALT&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#20803;&#21270;&#30340;&#36328;&#22269;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#21307;&#30103;&#26816;&#26597;&#65292;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#12290;Med-HALT&#21253;&#25324;&#20004;&#31867;&#27979;&#35797;&#65306;&#25512;&#29702;&#21644;&#22522;&#20110;&#35760;&#24518;&#30340;&#24187;&#35273;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#20449;&#24687;&#26816;&#32034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#25991;&#26412;Davinci&#65292;GPT-3.5&#65292;LlaMa-2&#65292;MPT&#21644;Falcon&#31561;&#39046;&#20808;&#30340;LLMs&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#35265;&#35299;&#65292;&#20419;&#36827;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#27880;&#24847;&#21147;&#19982;CNN&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#22312;&#22330;&#26223;&#20998;&#31867;&#20013;&#21463;&#20219;&#21153;&#21644;&#22270;&#20687;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#21457;&#29616;&#20219;&#21153;&#30340;&#24847;&#22270;&#21644;&#22270;&#20687;&#30340;&#29305;&#24449;&#37117;&#23545;&#20108;&#32773;&#30340;&#30456;&#20284;&#24615;&#26377;&#25152;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13345</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22330;&#26223;&#20998;&#31867;&#20013;&#26159;&#21542;&#20851;&#27880;&#30456;&#20284;&#21306;&#22495;&#65306;&#20219;&#21153;&#21644;&#22270;&#20687;&#31867;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Do humans and Convolutional Neural Networks attend to similar areas during scene classification: Effects of task and image type. (arXiv:2307.13345v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#27880;&#24847;&#21147;&#19982;CNN&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#22312;&#22330;&#26223;&#20998;&#31867;&#20013;&#21463;&#20219;&#21153;&#21644;&#22270;&#20687;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#21457;&#29616;&#20219;&#21153;&#30340;&#24847;&#22270;&#21644;&#22270;&#20687;&#30340;&#29305;&#24449;&#37117;&#23545;&#20108;&#32773;&#30340;&#30456;&#20284;&#24615;&#26377;&#25152;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#26159;&#24378;&#22823;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20294;&#26159;&#20160;&#20040;&#22240;&#32032;&#20915;&#23450;&#23427;&#20204;&#26159;&#21542;&#20851;&#27880;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#22270;&#20687;&#21306;&#22495;&#21602;&#65311;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25216;&#26415;&#22240;&#32032;&#19978;&#65292;&#20294;&#20154;&#31867;&#27880;&#24847;&#21147;&#30340;&#24433;&#21709;&#22240;&#32032;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29992;&#20110;&#24341;&#21457;&#20154;&#31867;&#27880;&#24847;&#21147;&#22270;&#30340;&#20219;&#21153;&#22914;&#20309;&#19982;&#22270;&#20687;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35843;&#33410;&#20154;&#31867;&#21644;CNN&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#21464;&#21270;&#20102;&#20154;&#31867;&#20219;&#21153;&#30340;&#24847;&#22270;&#65292;&#20174;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#30340;&#33258;&#21457;&#27880;&#35270;&#21040;&#26377;&#24847;&#30340;&#27880;&#35270;&#25351;&#21521;&#65292;&#20877;&#21040;&#25163;&#21160;&#21306;&#22495;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25913;&#21464;&#20102;&#35201;&#36827;&#34892;&#20998;&#31867;&#30340;&#22270;&#20687;&#31867;&#22411;&#65292;&#21253;&#25324;&#21333;&#20010;&#26174;&#33879;&#23545;&#35937;&#12289;&#30001;&#23545;&#35937;&#32452;&#21512;&#32780;&#25104;&#30340;&#23460;&#20869;&#22330;&#26223;&#65292;&#20197;&#21450;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#31867;&#21035;&#30340;&#26223;&#35266;&#12290;&#20197;&#36825;&#31181;&#26041;&#24335;&#29983;&#25104;&#30340;&#20154;&#31867;&#27880;&#24847;&#21147;&#22270;&#19982;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;Grad-CAM&#65289;&#25581;&#31034;&#20986;&#30340;CNN&#27880;&#24847;&#21147;&#22270;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning models like Convolutional Neural Networks (CNN) are powerful image classifiers, but what factors determine whether they attend to similar image areas as humans do? While previous studies have focused on technological factors, little is known about the role of factors that affect human attention. In the present study, we investigated how the tasks used to elicit human attention maps interact with image characteristics in modulating the similarity between humans and CNN. We varied the intentionality of human tasks, ranging from spontaneous gaze during categorization over intentional gaze-pointing up to manual area selection. Moreover, we varied the type of image to be categorized, using either singular, salient objects, indoor scenes consisting of object arrangements, or landscapes without distinct objects defining the category. The human attention maps generated in this way were compared to the CNN attention maps revealed by explainable artificial intelligence (Grad-CAM). 
&lt;/p&gt;</description></item><item><title>QAmplifyNet&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#25216;&#26415;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#39640;&#25928;&#39044;&#27979;&#20379;&#24212;&#38142;&#32570;&#36135;&#12290;&#23427;&#22312;&#22788;&#29702;&#30701;&#26102;&#38388;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12906</link><description>&lt;p&gt;
QAmplifyNet&#65306;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20379;&#24212;&#38142;&#32570;&#36135;&#39044;&#27979;&#30340;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum-Classical Neural Network. (arXiv:2307.12906v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12906
&lt;/p&gt;
&lt;p&gt;
QAmplifyNet&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#25216;&#26415;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#39640;&#25928;&#39044;&#27979;&#20379;&#24212;&#38142;&#32570;&#36135;&#12290;&#23427;&#22312;&#22788;&#29702;&#30701;&#26102;&#38388;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20379;&#24212;&#38142;&#31649;&#29702;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#32570;&#36135;&#39044;&#27979;&#65292;&#20197;&#20248;&#21270;&#24211;&#23384;&#25511;&#21046;&#65292;&#38477;&#20302;&#25104;&#26412;&#24182;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23454;&#38469;&#25968;&#25454;&#25910;&#38598;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20379;&#24212;&#38142;&#32570;&#36135;&#39044;&#27979;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;QAmplifyNet&#22312;&#19968;&#20010;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#20102;&#37327;&#23376;&#21551;&#21457;&#24335;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#39044;&#27979;&#32570;&#36135;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#26174;&#31034;&#65292;QAmplifyNet&#22312;&#22788;&#29702;&#30701;&#26102;&#38388;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12289;&#37327;&#23376;&#38598;&#25104;&#12289;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#20854;&#22312;&#22788;&#29702;&#30701;&#26102;&#38388;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#20379;&#24212;&#38142;&#31649;&#29702;&#30340;&#29702;&#24819;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain management relies on accurate backorder prediction for optimizing inventory control, reducing costs, and enhancing customer satisfaction. However, traditional machine-learning models struggle with large-scale datasets and complex relationships, hindering real-world data collection. This research introduces a novel methodological framework for supply chain backorder prediction, addressing the challenge of handling large datasets. Our proposed model, QAmplifyNet, employs quantum-inspired techniques within a quantum-classical neural network to predict backorders effectively on short and imbalanced datasets. Experimental evaluations on a benchmark dataset demonstrate QAmplifyNet's superiority over classical models, quantum ensembles, quantum neural networks, and deep reinforcement learning. Its proficiency in handling short, imbalanced datasets makes it an ideal solution for supply chain management. To enhance model interpretability, we use Explainable Artificial Intelligence 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#25299;&#23637;&#20102;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#22635;&#34917;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#35299;&#37322;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2307.02131</link><description>&lt;p&gt;
&#36229;&#36234;&#24050;&#30693;&#29616;&#23454;&#65306;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#34892;&#21307;&#23398;&#30740;&#31350; (arXiv&#65306;2307.02131v2 [cs.AI] &#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research. (arXiv:2307.02131v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#25299;&#23637;&#20102;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#22635;&#34917;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#35299;&#37322;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#26088;&#22312;&#25299;&#23637;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#21033;&#29992;&#30913;&#20849;&#25391;&#25104;&#20687;&#29305;&#24449;&#26469;&#35786;&#26029;&#20799;&#31185;&#21518;&#39045;&#31389;&#33041;&#32959;&#30244;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#24050;&#32463;&#35265;&#35777;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#21644;&#23398;&#26415;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#30340;&#20154;&#31867;&#21451;&#22909;&#35299;&#37322;&#30340;&#32570;&#20047;&#26174;&#33879;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#25509;&#21463;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34701;&#20837;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20026;&#26816;&#26597;&#26367;&#20195;&#20915;&#31574;&#22330;&#26223;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#35299;&#37322;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#39564;&#35777;&#39044;&#27979;&#24182;&#28548;&#28165;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#24046;&#24322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#20445;&#25345;&#20102;&#32479;&#35745;&#23398;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study employs counterfactual explanations to explore "what if?" scenarios in medical research, with the aim of expanding our understanding beyond existing boundaries. Specifically, we focus on utilizing MRI features for diagnosing pediatric posterior fossa brain tumors as a case study. The field of artificial intelligence and explainability has witnessed a growing number of studies and increasing scholarly interest. However, the lack of human-friendly interpretations in explaining the outcomes of machine learning algorithms has significantly hindered the acceptance of these methods by clinicians in their clinical practice. To address this, our approach incorporates counterfactual explanations, providing a novel way to examine alternative decision-making scenarios. These explanations offer personalized and context-specific insights, enabling the validation of predictions and clarification of variations under diverse circumstances. Importantly, our approach maintains both statistica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Caml&#65292;&#19968;&#31181;&#22312;&#20005;&#26684;&#32422;&#26463;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#20803;&#23398;&#20064;&#30340;AutoML&#26041;&#27861;&#12290;Caml&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;AutoML&#21442;&#25968;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#29983;&#25104;&#28385;&#36275;&#32422;&#26463;&#19988;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.16913</link><description>&lt;p&gt;
&#20005;&#26684;&#32422;&#26463;&#24212;&#29992;&#20013;&#30340;AutoML
&lt;/p&gt;
&lt;p&gt;
AutoML in Heavily Constrained Applications. (arXiv:2306.16913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Caml&#65292;&#19968;&#31181;&#22312;&#20005;&#26684;&#32422;&#26463;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#20803;&#23398;&#20064;&#30340;AutoML&#26041;&#27861;&#12290;Caml&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;AutoML&#21442;&#25968;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#29983;&#25104;&#28385;&#36275;&#32422;&#26463;&#19988;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#38656;&#35201;&#23545;&#21508;&#31181;&#36229;&#21442;&#25968;&#36827;&#34892;&#20180;&#32454;&#37197;&#32622;&#65292;&#36890;&#24120;&#30001;AutoML&#31995;&#32479;&#25903;&#25345;&#65292;&#35813;&#31995;&#32479;&#20248;&#21270;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36229;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;AutoML&#31995;&#32479;&#30340;&#20108;&#38454;&#20803;&#37197;&#32622;&#65292;AutoML&#36807;&#31243;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#30446;&#21069;&#30340;AutoML&#31995;&#32479;&#26080;&#27861;&#33258;&#21160;&#36866;&#24212;&#29305;&#23450;&#29992;&#20363;&#30340;&#37197;&#32622;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20063;&#26080;&#27861;&#32534;&#35793;&#29992;&#25143;&#23450;&#20041;&#30340;&#24212;&#29992;&#32422;&#26463;&#65292;&#20197;&#30830;&#20445;&#27969;&#31243;&#21450;&#20854;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Caml&#65292;&#23427;&#20351;&#29992;&#20803;&#23398;&#20064;&#33258;&#21160;&#36866;&#24212;&#20854;&#33258;&#36523;&#30340;AutoML&#21442;&#25968;&#65292;&#27604;&#22914;&#25628;&#32034;&#31574;&#30053;&#12289;&#39564;&#35777;&#31574;&#30053;&#21644;&#25628;&#32034;&#31354;&#38388;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;Caml&#30340;&#21160;&#24577;AutoML&#31574;&#30053;&#32771;&#34385;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#65292;&#24182;&#33719;&#24471;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#28385;&#36275;&#32422;&#26463;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing a machine learning pipeline for a task at hand requires careful configuration of various hyperparameters, typically supported by an AutoML system that optimizes the hyperparameters for the given training dataset. Yet, depending on the AutoML system's own second-order meta-configuration, the performance of the AutoML process can vary significantly. Current AutoML systems cannot automatically adapt their own configuration to a specific use case. Further, they cannot compile user-defined application constraints on the effectiveness and efficiency of the pipeline and its generation. In this paper, we propose Caml, which uses meta-learning to automatically adapt its own AutoML parameters, such as the search strategy, the validation strategy, and the search space, for a task at hand. The dynamic AutoML strategy of Caml takes user-defined constraints into account and obtains constraint-satisfying pipelines with high predictive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#26080;&#27861;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#19988;&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.09597</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Clickbait Detection via Large Language Models. (arXiv:2306.09597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#26080;&#27861;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#19988;&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#35825;&#39575;&#65288;Clickbait&#65289;&#20250;&#36890;&#36807;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#29978;&#33267;&#24341;&#20154;&#20837;&#32988;&#30340;&#26631;&#39064;&#26469;&#35825;&#23548;&#29992;&#25143;&#36827;&#34892;&#28857;&#20987;&#65292;&#20960;&#20046;&#28183;&#36879;&#21040;&#25152;&#26377;&#22312;&#32447;&#20869;&#23481;&#21457;&#24067;&#32773;&#65292;&#22914;&#26032;&#38395;&#38376;&#25143;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;NLP&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;LLM&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#39640;&#36136;&#37327;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#31995;&#32479;&#36824;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;LLM&#22312;&#22810;&#20010;&#33521;&#25991;&#21644;&#20013;&#25991;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#21644;&#24494;&#35843;PLM&#26041;&#27861;&#30456;&#27604;&#65292;LLM&#26080;&#27861;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;&#19982;&#20154;&#31867;&#30452;&#35273;&#19981;&#21516;&#65292;&#23454;&#39564;&#34920;&#26126;LLM&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a serious of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot scenarios on a number of English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from the human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.
&lt;/p&gt;</description></item><item><title>DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09124</link><description>&lt;p&gt;
DIFFender&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#29992;&#20110;&#25269;&#24481;Patch&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks. (arXiv:2306.09124v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09124
&lt;/p&gt;
&lt;p&gt;
DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;Patch&#25915;&#20987;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#24320;&#21457;&#21487;&#38752;&#30340;&#38450;&#24481;&#26041;&#27861;&#20197;&#25269;&#24481;Patch&#25915;&#20987;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24403;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIFFender&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#12290;DIFFender&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;Patch&#23450;&#20301;&#21644;Patch&#24674;&#22797;&#12290;&#22312;&#23450;&#20301;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#36259;&#29305;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#23545;&#25239;&#24615;Patch&#30340;&#20301;&#32622;&#12290;&#22312;&#24674;&#22797;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#22270;&#20687;&#20013;&#30340;&#23545;&#25239;&#24615;&#21306;&#22495;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20004;&#20010;&#38454;&#27573;&#37117;&#21463;&#21040;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#31934;&#24515;&#24341;&#23548;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#32039;&#23494;&#30456;&#20114;&#20316;&#29992;&#26469;&#25552;&#39640;&#25972;&#20010;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks, particularly patch attacks, pose significant threats to the robustness and reliability of deep learning models. Developing reliable defenses against patch attacks is crucial for real-world applications, yet current research in this area is not satisfactory. In this paper, we propose DIFFender, a novel defense method that leverages a text-guided diffusion model to defend against adversarial patches. DIFFender includes two main stages: patch localization and patch restoration. In the localization stage, we find and exploit an intriguing property of the diffusion model to effectively identify the locations of adversarial patches. In the restoration stage, we employ the diffusion model to reconstruct the adversarial regions in the images while preserving the integrity of the visual content. Importantly, these two stages are carefully guided by a unified diffusion model, thus we can utilize the close interaction between them to improve the whole defense performance. Mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25512;&#23548;&#20102;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#30340;&#20027;&#21160;&#25512;&#29702;&#29256;&#26412;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#26469;&#26126;&#30830;&#35268;&#23450;&#31995;&#32479;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.08014</link><description>&lt;p&gt;
&#23454;&#29616;&#21512;&#25104;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#65292;&#31532;&#19968;&#37096;&#20998;&#65306;&#35748;&#35782;&#30446;&#26631;&#21644;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language. (arXiv:2306.08014v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25512;&#23548;&#20102;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#30340;&#20027;&#21160;&#25512;&#29702;&#29256;&#26412;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#26469;&#26126;&#30830;&#35268;&#23450;&#31995;&#32479;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30001;&#33021;&#21407;&#29702;&#65288;FEP&#65289;&#26159;&#19968;&#31181;&#25551;&#36848;&#31995;&#32479;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#30001;&#33021;&#27867;&#20989;&#32780;&#33258;&#32452;&#32455;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#12289;&#31283;&#23450;&#32467;&#26500;&#65288;&#26234;&#33021;&#65289;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#26159;FEP&#30340;&#19968;&#20010;&#25512;&#35770;&#65292;&#23427;&#26126;&#30830;&#20102;&#33021;&#22815;&#20026;&#26410;&#26469;&#36827;&#34892;&#35268;&#21010;&#65288;&#20195;&#29702;&#65289;&#30340;&#31995;&#32479;&#26159;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#21270;&#21253;&#21547;&#20449;&#24687;&#23547;&#27714;&#32452;&#20214;&#30340;&#29305;&#23450;&#33258;&#30001;&#33021;&#27867;&#20989;&#26469;&#36816;&#20316;&#30340;&#12290;&#26412;&#25991;&#26159;&#19968;&#20010;&#31995;&#21015;&#20013;&#30340;&#31532;&#19968;&#31687;&#65292;&#25105;&#20204;&#22312;&#33258;&#30001;&#24418;&#24335;&#22240;&#23376;&#22270;&#19978;&#25512;&#23548;&#20102;AIF&#30340;&#21512;&#25104;&#29256;&#26412;&#12290;&#26412;&#25991;&#37325;&#28857;&#25512;&#23548;&#20102;AIF&#25152;&#20351;&#29992;&#30340;&#33258;&#30001;&#33021;&#27867;&#20989;&#30340;&#23616;&#37096;&#29256;&#26412;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#36896;&#19968;&#20010;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#24182;&#19982;&#26377;&#20851;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30340;&#20808;&#21069;&#24037;&#20316;&#25509;&#21475;&#30340;AIF&#29256;&#26412;&#12290;&#32467;&#26524;&#28040;&#24687;&#26159;&#22312;&#25105;&#20204;&#30340;&#20276;&#20387;&#35770;&#25991;&#20013;&#24471;&#20986;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#22240;&#23376;&#22270;&#24418;&#24335;&#20013;&#23384;&#22312;&#19968;&#20010;&#32570;&#21475;&#12290;&#34429;&#28982;&#22240;&#23376;&#22270;&#34920;&#36798;&#20102;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#22312;&#25351;&#23450;&#31995;&#32479;&#30446;&#26631;&#26041;&#38754;&#32570;&#20047;&#19968;&#20010;&#22270;&#24418;&#21270;&#35821;&#35328;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22240;&#23376;&#22270;&#25551;&#36848;&#27861;&#30340;&#26032;&#25193;&#23637;&#65292;&#31216;&#20026;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#65292;&#23427;&#20351;&#31995;&#32479;&#30446;&#26631;&#24471;&#21040;&#26126;&#30830;&#35268;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Free Energy Principle (FEP) is a theoretical framework for describing how (intelligent) systems self-organise into coherent, stable structures by minimising a free energy functional. Active Inference (AIF) is a corollary of the FEP that specifically details how systems that are able to plan for the future (agents) function by minimising particular free energy functionals that incorporate information seeking components. This paper is the first in a series of two where we derive a synthetic version of AIF on free form factor graphs. The present paper focuses on deriving a local version of the free energy functionals used for AIF. This enables us to construct a version of AIF which applies to arbitrary graphical models and interfaces with prior work on message passing algorithms. The resulting messages are derived in our companion paper. We also identify a gap in the graphical notation used for factor graphs. While factor graphs are great at expressing a generative model, they have so
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#31639;&#27861;&#8212;&#8212;GLoRA&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#36866;&#37197;&#22120;&#23618;&#21644;&#21487;&#25193;&#23637;&#30340;&#32467;&#26500;&#25628;&#32034;&#20855;&#26377;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#26356;&#39640;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#22312;&#21508;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07967</link><description>&lt;p&gt;
&#19968;&#36890;&#36866;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#36890;&#29992;LoRA&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning. (arXiv:2306.07967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#31639;&#27861;&#8212;&#8212;GLoRA&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#36866;&#37197;&#22120;&#23618;&#21644;&#21487;&#25193;&#23637;&#30340;&#32467;&#26500;&#25628;&#32034;&#20855;&#26377;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#26356;&#39640;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#22312;&#21508;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36890;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20219;&#21153;&#31639;&#27861;&#8212;&#8212;&#24191;&#20041;LoRA&#65288;GLoRA&#65289;&#12290;GLoRA &#20351;&#29992;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#26469;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#21644;&#35843;&#25972;&#20013;&#38388;&#28608;&#27963;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#28789;&#27963;&#24615;&#21644;&#36328;&#24322;&#26500;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;GLoRA &#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#12289;&#27169;&#22359;&#21270;&#30340;&#12289;&#23618;&#27425;&#30340;&#32467;&#26500;&#25628;&#32034;&#26469;&#24110;&#21161;&#26377;&#25928;&#30340;&#21442;&#25968;&#35843;&#25972;&#65292;&#23398;&#20064;&#27599;&#20010;&#23618;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#23398;&#20844;&#24335;&#36215;&#28304;&#65292;GLoRA &#20855;&#26377;&#24378;&#22823;&#30340;&#36801;&#31227;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#26435;&#37325;&#21644;&#28608;&#27963;&#29366;&#24577;&#19978;&#30340;&#38468;&#21152;&#32500;&#24230;&#26469;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#12289;&#19987;&#19994;&#21644;&#32467;&#26500;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;GLoRA &#30340;&#31934;&#24230;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#36798;&#21040;&#20102;&#20248;&#36234;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26500;&#37325;&#26032;&#35774;&#35745;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#36816;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adjusts to new tasks through additional dimensions on weights and activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks, achieving superior accuracy with fewer parameters and computations on various datasets. Furthermore, our structural re-par
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05685</link><description>&lt;p&gt;
&#29992;MT-Bench&#21644;Chatbot Arena&#35780;&#20272;&#20197;LLM&#20026;&#22522;&#30784;&#30340;&#32842;&#22825;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05685
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32842;&#22825;&#21161;&#25163;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#32780;&#29616;&#26377;&#30340;&#22522;&#20934;&#26080;&#27861;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#22312;&#26356;&#21152;&#24320;&#25918;&#30340;&#38382;&#39064;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#30340;&#20351;&#29992;&#21644;&#23616;&#38480;&#24615;&#65292;&#22914;&#20301;&#32622;&#21644;&#20887;&#20313;&#20559;&#35265;&#20197;&#21450;&#26377;&#38480;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#26469;&#36801;&#31227;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#65288;&#19968;&#20010;&#22810;&#36718;&#38382;&#31572;&#38598;&#21644;&#19968;&#20010;&#20247;&#21253;&#31454;&#25216;&#24179;&#21488;&#65289;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#24378;&#22823;LLM&#35780;&#21028;&#21592;&#21487;&#20197;&#24456;&#22909;&#22320;&#21305;&#37197;&#21463;&#25511;&#21644;&#20247;&#21253;&#20154;&#31867;&#20559;&#22909;&#65292;&#36798;&#21040;&#20102;80&#65285;&#20197;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#19982;&#20154;&#31867;&#19968;&#33268;&#24615;&#27700;&#24179;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#36924;&#36817;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#24335;&#65292;&#32780;&#36825;&#20123;&#20559;&#22909;&#26159;&#38750;&#24120;&#26114;&#36149;&#33719;&#21462;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; rewarded soup &#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#20195;&#29702;&#22870;&#21169;&#65292;&#23454;&#29616;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04488</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#21512;&#22810;&#26679;&#21270;&#22870;&#21169;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#23545;&#40784;&#30340;&#22870;&#21169;&#27748;
&lt;/p&gt;
&lt;p&gt;
Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. (arXiv:2306.04488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; rewarded soup &#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#20195;&#29702;&#22870;&#21169;&#65292;&#23454;&#29616;&#24494;&#35843;&#26435;&#37325;&#25554;&#20540;&#65292;&#20174;&#32780;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#37327;&#26080;&#21442;&#32771;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26377;&#26631;&#27880;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#24378;&#21270;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#32593;&#32476;&#19982;&#39044;&#26399;&#30340;&#20351;&#29992;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#20195;&#29702;&#22870;&#21169;&#30340;&#32570;&#38519;&#21487;&#33021;&#20250;&#22952;&#30861;&#35757;&#32451;&#65292;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#65307;&#29616;&#23454;&#20219;&#21153;&#21644;&#20154;&#31867;&#24847;&#35265;&#30340;&#22810;&#26679;&#24615;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#37319;&#29992;&#22810;&#31574;&#30053;&#26041;&#27861;&#26469;&#25317;&#25265;&#22810;&#26679;&#21270;&#22870;&#21169;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#26159;&#19987;&#27880;&#20110;&#21333;&#19968;&#30340;&#20808;&#39564;&#22870;&#21169;&#65292;&#32780;&#26159;&#22312;&#25972;&#20010;&#20559;&#22909;&#31354;&#38388;&#20013;&#23454;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#24191;&#20041;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; rewarded soup&#65292;&#39318;&#20808;&#29420;&#31435;&#22320;&#19987;&#38376;&#21270;&#22810;&#20010;&#32593;&#32476;(&#27599;&#20010;&#20195;&#29702;&#22870;&#21169;&#19968;&#20010;)&#65292;&#28982;&#21518;&#22312;&#23427;&#20204;&#30340;&#26435;&#37325;&#20043;&#38388;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#12290;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#25104;&#21151;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#22810;&#26679;&#21270;&#22870;&#21169;&#26469;&#33258;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#26102;&#65292;&#26435;&#37325;&#20173;&#28982;&#20445;&#25345;&#32447;&#24615;&#36830;&#25509;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effective
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#21644;&#22270;&#21367;&#31215;&#30340;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#25552;&#21462;&#36755;&#20837;&#36335;&#24452;&#30340;&#19981;&#21516;&#23646;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.04324</link><description>&lt;p&gt;
GCT-TTE: &#22522;&#20110;&#22270;&#21367;&#31215;&#21464;&#25442;&#22120;&#30340;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GCT-TTE: Graph Convolutional Transformer for Travel Time Estimation. (arXiv:2306.04324v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#21644;&#22270;&#21367;&#31215;&#30340;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#25552;&#21462;&#36755;&#20837;&#36335;&#24452;&#30340;&#19981;&#21516;&#23646;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;GCT-TTE&#26550;&#26500;&#30340;&#20851;&#38190;&#29305;&#24449;&#26159;&#21033;&#29992;&#25429;&#25417;&#36755;&#20837;&#36335;&#24452;&#19981;&#21516;&#23646;&#24615;&#30340;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#12290;&#38500;&#20102;&#28041;&#21450;&#27169;&#22411;&#37197;&#32622;&#30340;&#24191;&#27867;&#30740;&#31350;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#29616;&#24182;&#35780;&#20272;&#20102;&#36275;&#22815;&#25968;&#37327;&#30340;&#23454;&#38469;&#22522;&#32447;&#65292;&#29992;&#20110;&#36335;&#24452;&#24863;&#30693;&#21644;&#36335;&#24452;&#30450;&#35774;&#32622;&#12290;&#25152;&#36827;&#34892;&#30340;&#35745;&#31639;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#27969;&#31243;&#30340;&#21487;&#34892;&#24615;&#65292;&#35813;&#27969;&#31243;&#22312;&#20004;&#20010;&#32771;&#34385;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;GCT-TTE&#24050;&#37096;&#32626;&#20026;Web&#26381;&#21153;&#65292;&#21487;&#29992;&#20110;&#36827;&#19968;&#27493;&#23454;&#39564;&#19982;&#29992;&#25143;&#23450;&#20041;&#36335;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new transformer-based model for the problem of travel time estimation. The key feature of the proposed GCT-TTE architecture is the utilization of different data modalities capturing different properties of an input path. Along with the extensive study regarding the model configuration, we implemented and evaluated a sufficient number of actual baselines for path-aware and path-blind settings. The conducted computational experiments have confirmed the viability of our pipeline, which outperformed state-of-the-art models on both considered datasets. Additionally, GCT-TTE was deployed as a web service accessible for further experiments with user-defined routes.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22312;&#22823;&#33041;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#25429;&#33719;&#26102;&#38388;&#24207;&#21015;&#26469;&#36827;&#34892;&#19968;&#31995;&#21015;&#20851;&#20110;&#31070;&#32463;&#20803;&#38598;&#32676;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2306.03812</link><description>&lt;p&gt;
&#22312;&#22823;&#33041;&#27169;&#22411;&#20013;&#36827;&#34892;&#24207;&#21015;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Computation with Sequences in a Model of the Brain. (arXiv:2306.03812v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03812
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22312;&#22823;&#33041;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#25429;&#33719;&#26102;&#38388;&#24207;&#21015;&#26469;&#36827;&#34892;&#19968;&#31995;&#21015;&#20851;&#20110;&#31070;&#32463;&#20803;&#38598;&#32676;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#19978;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#65292;&#20294;&#26159;&#22823;&#33041;&#23398;&#20064;&#33021;&#21147;&#30340;&#26222;&#36941;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#24555;&#36895;&#24615;&#20173;&#28982;&#26080;&#27861;&#21305;&#25932;&#12290;&#35748;&#30693;&#22914;&#20309;&#20135;&#29983;&#20110;&#31070;&#32463;&#27963;&#21160;&#26159;&#31070;&#32463;&#31185;&#23398;&#20013;&#19968;&#20010;&#26680;&#24515;&#30340;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65292;&#19982;&#26234;&#33021;&#30740;&#31350;&#23494;&#19981;&#21487;&#20998;&#12290;&#22312;Papadimitriou [2020]&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#27963;&#21160;&#24418;&#24335;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#21644;&#27169;&#25311;&#26174;&#31034;&#21487;&#20197;&#21033;&#29992;&#31070;&#32463;&#20803;&#38598;&#32676;&#30340;&#21019;&#24314;&#21644;&#25805;&#32437;&#26469;&#23454;&#29616;&#26576;&#20123;&#31616;&#21333;&#30340;&#35748;&#30693;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26234;&#33021;&#34892;&#20026;&#20381;&#36182;&#20110;&#33021;&#22815;&#35782;&#21035;&#12289;&#23384;&#20648;&#21644;&#25805;&#20316;&#26102;&#38388;&#24207;&#21015;&#30340;&#21050;&#28608;&#65288;&#20363;&#22914;&#35745;&#21010;&#12289;&#35821;&#35328;&#12289;&#23548;&#33322;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21516;&#19968;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#31361;&#35302;&#26435;&#37325;&#21644;&#21487;&#22609;&#24615;&#65292;&#26102;&#38388;&#21487;&#20197;&#33258;&#28982;&#22320;&#20197;&#20248;&#20808;&#39034;&#24207;&#36827;&#34892;&#25429;&#33719;&#65292;&#20174;&#32780;&#21487;&#20197;&#36827;&#34892;&#19968;&#31995;&#21015;&#20851;&#20110;&#31070;&#32463;&#20803;&#38598;&#32676;&#24207;&#21015;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even as machine learning exceeds human-level performance on many applications, the generality, robustness, and rapidity of the brain's learning capabilities remain unmatched. How cognition arises from neural activity is a central open question in neuroscience, inextricable from the study of intelligence itself. A simple formal model of neural activity was proposed in Papadimitriou [2020] and has been subsequently shown, through both mathematical proofs and simulations, to be capable of implementing certain simple cognitive operations via the creation and manipulation of assemblies of neurons. However, many intelligent behaviors rely on the ability to recognize, store, and manipulate temporal sequences of stimuli (planning, language, navigation, to list a few). Here we show that, in the same model, time can be captured naturally as precedence through synaptic weights and plasticity, and, as a result, a range of computations on sequences of assemblies can be carried out. In particular, r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29983;&#21629;&#21608;&#26399;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;LIBERO&#12290;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#32456;&#36523;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;LLDM&#20013;&#30340;&#20116;&#20010;&#20851;&#38190;&#30740;&#31350;&#20027;&#39064;&#65292;&#24076;&#26395;&#23427;&#33021;&#22815;&#21152;&#36895;&#26500;&#24314;&#21487;&#20197;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#23398;&#20064;&#21644;&#36866;&#24212;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.03310</link><description>&lt;p&gt;
LIBERO: &#29983;&#21629;&#21608;&#26399;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#30693;&#35782;&#36716;&#31227;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning. (arXiv:2306.03310v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29983;&#21629;&#21608;&#26399;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;LIBERO&#12290;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#32456;&#36523;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24378;&#35843;&#20102;LLDM&#20013;&#30340;&#20116;&#20010;&#20851;&#38190;&#30740;&#31350;&#20027;&#39064;&#65292;&#24076;&#26395;&#23427;&#33021;&#22815;&#21152;&#36895;&#26500;&#24314;&#21487;&#20197;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#23398;&#20064;&#21644;&#36866;&#24212;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#24314;&#31435;&#36890;&#29992;&#20195;&#29702;&#30340;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#35813;&#20195;&#29702;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#23398;&#20064;&#21644;&#36866;&#24212;&#12290;&#19982;&#20256;&#32479;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#39046;&#22495;&#30340;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#19981;&#21516;&#65292;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#32456;&#36523;&#23398;&#20064;&#65288;LLDM&#65289;&#36824;&#38656;&#35201;&#20256;&#36882;&#31243;&#24207;&#21270;&#30693;&#35782;&#65292;&#20363;&#22914;&#25805;&#20316;&#21644;&#34892;&#20026;&#12290;&#20026;&#20102;&#25512;&#36827;LLDM&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LIBERO&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#32456;&#36523;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LIBERO&#24378;&#35843;&#20102;LLDM&#20013;&#30340;&#20116;&#20010;&#20851;&#38190;&#30740;&#31350;&#20027;&#39064;&#65306;1&#65289;&#22914;&#20309;&#26377;&#25928;&#22320;&#20256;&#36882;&#22768;&#26126;&#24615;&#30693;&#35782;&#12289;&#31243;&#24207;&#24615;&#30693;&#35782;&#25110;&#20108;&#32773;&#28151;&#21512;&#20307;&#65307;2&#65289;&#22914;&#20309;&#35774;&#35745;&#26377;&#25928;&#30340;&#31574;&#30053;&#26550;&#26500;&#21644;3&#65289;LLDM&#30340;&#26377;&#25928;&#31639;&#27861;&#65307;4&#65289;&#32456;&#36523;&#23398;&#20064;&#32773;&#22312;&#20219;&#21153;&#25490;&#24207;&#26041;&#38754;&#30340;&#31283;&#20581;&#24615;&#65307;5&#65289;&#27169;&#22411;&#39044;&#35757;&#32451;&#23545;LLDM&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#31243;&#24207;&#29983;&#25104;&#31649;&#36947;&#65292;&#21487;&#20197;&#21407;&#21017;&#19978;&#29983;&#25104;&#26080;&#38480;&#22810;&#30340;&#36716;&#31227;&#22330;&#26223;&#12290;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#21253;&#25324;5&#20010;&#20219;&#21153;&#26063;&#20013;&#30340;50&#20010;&#19981;&#21516;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#26063;&#37117;&#35774;&#35745;&#20197;&#27979;&#35797;&#20854;&#20013;&#19968;&#20010;&#20116;&#20010;&#30740;&#31350;&#20027;&#39064;&#65292;&#32780;&#22312;&#27599;&#20010;&#20219;&#21153;&#26063;&#20869;&#65292;&#20219;&#21153;&#30340;&#38590;&#24230;&#21644;&#26032;&#39062;&#31243;&#24230;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20219;&#21153;&#25490;&#24207;&#23545;&#32456;&#36523;&#23398;&#20064;&#32773;&#30340;&#34920;&#29616;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#27169;&#22411;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#20219;&#21153;&#25490;&#24207;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;LIBERO&#33021;&#22815;&#20316;&#20026;&#19968;&#20010;&#31038;&#21306;&#33539;&#22260;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;LLDM&#24182;&#21152;&#36895;&#26500;&#24314;&#21487;&#20197;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#23398;&#20064;&#21644;&#36866;&#24212;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowledge of entities and concepts, lifelong learning in decision-making (LLDM) also necessitates the transfer of procedural knowledge, such as actions and behaviors. To advance research in LLDM, we introduce LIBERO, a novel benchmark of lifelong learning for robot manipulation. Specifically, LIBERO highlights five key research topics in LLDM: 1) how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both; 2) how to design effective policy architectures and 3) effective algorithms for LLDM; 4) the robustness of a lifelong learner with respect to task ordering; and 5) the effect of model pretraining for LLDM. We develop an extendible procedural generation pipeline that can in principle generate infinitely many t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01323</link><description>&lt;p&gt;
&#25581;&#31034;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#65306;&#19968;&#20010;&#23610;&#30721;&#36866;&#29992;&#20110;&#25152;&#26377;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?. (arXiv:2306.01323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#21644;&#29702;&#35770;&#35777;&#25454;&#65292;&#25903;&#25345;&#23427;&#20204;&#22312;&#25429;&#25417;&#21516;&#26500;&#21644;&#26576;&#20123;&#24322;&#26500;&#22270;&#19978;&#30340;&#32467;&#26500;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#23454;&#38469;&#20013;&#30340;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#37117;&#30001;&#21516;&#26500;&#21644;&#24322;&#26500;&#32467;&#26500;&#27169;&#24335;&#30340;&#28151;&#21512;&#33410;&#28857;&#32452;&#25104;&#65292;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#19979;&#30340;&#33410;&#28857;&#65288;&#20363;&#22914;&#22312;&#24322;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#65289;&#22312;GNN&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20998;&#26512;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#20986;&#33394;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#34920;&#29616;&#20986;&#24615;&#33021;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35782;&#21035;&#20102;&#27979;&#35797;&#23637;&#31034;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#26102;GNN&#30340;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;GNN&#30340;&#21152;&#26435;&#32858;&#21512;&#20197;&#36866;&#24212;&#24615;&#32467;&#26500;&#24046;&#24322;&#24615;&#30340;&#26032;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#21644;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#39318;&#27425;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#21040;2&#31186;&#65292;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25913;&#36827;&#27493;&#39588;&#33976;&#39311;&#26469;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.00980</link><description>&lt;p&gt;
SnapFusion&#65306;&#31227;&#21160;&#35774;&#22791;&#19978;&#20004;&#31186;&#20869;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds. (arXiv:2306.00980v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#39318;&#27425;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#21040;2&#31186;&#65292;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25913;&#36827;&#27493;&#39588;&#33976;&#39311;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#21019;&#24314;&#20986;&#24778;&#20154;&#30340;&#22270;&#20687;&#65292;&#19981;&#20122;&#20110;&#19987;&#19994;&#33402;&#26415;&#23478;&#21644;&#25668;&#24433;&#24072;&#30340;&#20316;&#21697;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36739;&#22823;&#65292;&#20855;&#26377;&#22797;&#26434;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#21313;&#20010;&#21435;&#22122;&#36845;&#20195;&#65292;&#20351;&#20854;&#35745;&#31639;&#26114;&#36149;&#19988;&#36816;&#34892;&#32531;&#24930;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#39640;&#31471;GPU&#21644;&#22522;&#20110;&#20113;&#30340;&#25512;&#29702;&#26469;&#25353;&#27604;&#20363;&#36816;&#34892;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24403;&#29992;&#25143;&#25968;&#25454;&#21457;&#36865;&#21040;&#31532;&#19977;&#26041;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#39318;&#27425;&#22312;&#19981;&#21040;2&#31186;&#38047;&#20869;&#35299;&#38145;&#20102;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25913;&#36827;&#27493;&#39588;&#33976;&#39311;&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers. However, these models are large, with complex network architectures and tens of denoising iterations, making them computationally expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run diffusion models at scale. This is costly and has privacy implications, especially when user data is sent to a third party. To overcome these challenges, we present a generic approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in less than $2$ seconds. We achieve so by introducing efficient network architecture and improving step distillation. Specifically, we propose an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, we enhance the step distillation by explori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#20248;&#21270;&#25439;&#22833;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00742</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Going Deeper with Spectral Embeddings. (arXiv:2306.00742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#20248;&#21270;&#25439;&#22833;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#22320;&#22788;&#29702;&#28023;&#37327;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#65292;&#31185;&#23398;&#23478;&#20204;&#37319;&#29992;&#34920;&#31034;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#19968;&#20123;&#24213;&#23618;&#36816;&#31639;&#30340;&#35889;&#20998;&#35299;&#20043;&#38388;&#23637;&#29616;&#20986;&#26126;&#26174;&#30340;&#32852;&#31995;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;&#26159;&#36890;&#36807;&#22312;&#25968;&#25454;&#30340;&#39030;&#37096;&#26500;&#24314;&#22270;&#24418;&#26469;&#24314;&#31435;&#26126;&#30830;&#30340;&#35889;&#23884;&#20837;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#21407;&#29702;&#21644;&#26680;&#26041;&#27861;&#26500;&#24314;&#30340;&#65292;&#36825;&#23558;&#23548;&#33268;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#31639;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#20197;&#20248;&#21270;&#22522;&#26412;&#21464;&#20998;&#25439;&#22833;&#30340;&#31639;&#27861;&#65292;&#23427;&#20204;&#20135;&#29983;&#20102;&#23454;&#38469;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#26469;&#22312;&#19968;&#27493;&#20013;&#29983;&#25104;&#26032;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make sense of millions of raw data and represent them efficiently, practitioners rely on representation learning. Recently, deep connections have been shown between these approaches and the spectral decompositions of some underlying operators. Historically, explicit spectral embeddings were built from graphs constructed on top of the data. In contrast, we propose two new methods to build spectral embeddings: one based on functional analysis principles and kernel methods, which leads to algorithms with theoretical guarantees, and the other based on deep networks trained to optimize principled variational losses, which yield practically efficient algorithms. Furthermore, we provide a new sampling algorithm that leverages learned representations to generate new samples in a single step.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00038</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;(FedCSD)
&lt;/p&gt;
&lt;p&gt;
FedCSD: A Federated Learning Based Approach for Code-Smell Detection. (arXiv:2306.00038v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#19977;&#20010;&#23454;&#39564;&#26469;&#25903;&#25345;&#36825;&#20123;&#26029;&#35328;&#65292;&#36825;&#20123;&#23454;&#39564;&#21033;&#29992;&#20102;&#19977;&#20010;&#25163;&#21160;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#26816;&#27979;&#21644;&#30740;&#31350;&#19981;&#21516;&#30340;&#20195;&#30721;&#24322;&#21619;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\%) where fewer smells exist in the training dataset, which has a noticeab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#24191;&#20041;&#22240;&#26524;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#28789;&#27963;&#30340;&#36793;&#38469;&#25935;&#24863;&#24615;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#21508;&#31181;&#22240;&#26524;&#25928;&#24212;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#22810;&#31181;&#35774;&#32622;&#65292;&#21253;&#25324;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#26102;&#21464;&#30340;&#27835;&#30103;&#12290;</title><link>http://arxiv.org/abs/2305.16988</link><description>&lt;p&gt;
&#24191;&#20041;&#22240;&#26524;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#23574;&#38160;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Sharp Bounds for Generalized Causal Sensitivity Analysis. (arXiv:2305.16988v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#24191;&#20041;&#22240;&#26524;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#28789;&#27963;&#30340;&#36793;&#38469;&#25935;&#24863;&#24615;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#21508;&#31181;&#22240;&#26524;&#25928;&#24212;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#22810;&#31181;&#35774;&#32622;&#65292;&#21253;&#25324;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#26102;&#21464;&#30340;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#23545;&#20110;&#35768;&#22810;&#23398;&#31185;&#22914;&#21307;&#23398;&#21644;&#32463;&#27982;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#25918;&#26494;&#26080;&#28151;&#28102;&#24615;&#20551;&#35774;&#65288;&#22240;&#26524;&#25935;&#24863;&#24615;&#20998;&#26512;&#65289;&#19979;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#23574;&#38160;&#30028;&#38480;&#20173;&#22312;&#30740;&#31350;&#20013;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20855;&#26377;&#23574;&#38160;&#30028;&#38480;&#30340;&#24037;&#20316;&#20165;&#38480;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#21333;&#19968;&#20108;&#20803;&#27835;&#30103;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21508;&#31181;&#35774;&#32622;&#19979;&#30340;&#26410;&#35266;&#23519;&#28151;&#28102;&#22240;&#32032;&#19979;&#30340;&#22240;&#26524;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36793;&#38469;&#25935;&#24863;&#24615;&#27169;&#22411;&#65288;MSM&#65289;&#30340;&#28789;&#27963;&#25512;&#24191;&#65292;&#28982;&#21518;&#25512;&#23548;&#20986;&#22823;&#31867;&#22240;&#26524;&#25928;&#24212;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#36825;&#21253;&#25324;&#65288;&#26465;&#20214;&#65289;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65292;&#20013;&#20171;&#20998;&#26512;&#21644;&#36335;&#24452;&#20998;&#26512;&#30340;&#25928;&#24212;&#65292;&#20197;&#21450;&#20998;&#24067;&#25928;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25935;&#24863;&#24615;&#27169;&#22411;&#36866;&#29992;&#20110;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#26102;&#21464;&#30340;&#27835;&#30103;&#12290;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#26410;&#35266;&#23519;&#28151;&#28102;&#23548;&#33268;&#30340;&#37096;&#20998;&#35782;&#21035;&#38382;&#39064;&#35299;&#37322;&#20026;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference from observational data is crucial for many disciplines such as medicine and economics. However, sharp bounds for causal effects under relaxations of the unconfoundedness assumption (causal sensitivity analysis) are subject to ongoing research. So far, works with sharp bounds are restricted to fairly simple settings (e.g., a single binary treatment). In this paper, we propose a unified framework for causal sensitivity analysis under unobserved confounding in various settings. For this, we propose a flexible generalization of the marginal sensitivity model (MSM) and then derive sharp bounds for a large class of causal effects. This includes (conditional) average treatment effects, effects for mediation analysis and path analysis, and distributional effects. Furthermore, our sensitivity model is applicable to discrete, continuous, and time-varying treatments. It allows us to interpret the partial identification problem under unobserved confounding as a distribution shift
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;ParaDiGMS&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#22810;&#20010;&#27493;&#39588;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#20351;&#35745;&#31639;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.16317</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#24182;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Parallel Sampling of Diffusion Models. (arXiv:2305.16317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;ParaDiGMS&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#22810;&#20010;&#27493;&#39588;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#20351;&#35745;&#31639;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#37319;&#26679;&#36895;&#24230;&#32531;&#24930;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;1000&#27425;&#39034;&#24207;&#21435;&#22122;&#27493;&#39588;&#25165;&#33021;&#24471;&#21040;&#19968;&#20010;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#20132;&#25442;&#35745;&#31639;&#26426;&#22788;&#29702;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29468;&#27979;&#26410;&#26469;&#30340;&#21435;&#22122;&#27493;&#39588;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#36880;&#27493;&#32454;&#21270;&#33267;&#25910;&#25947;&#30340;Picard&#36845;&#20195;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#21457;&#29616;&#65306;&#23613;&#31649;&#21435;&#22122;&#27493;&#39588;&#26377;&#39034;&#24207;&#24615;&#65292;&#20294;&#20173;&#28982;&#21487;&#20197;&#24182;&#34892;&#37319;&#26679;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ParaDiGMS&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20197;&#24182;&#34892;&#26041;&#24335;&#21435;&#22122;&#22810;&#20010;&#27493;&#39588;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#22312;&#35745;&#31639;&#22788;&#29702;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#29978;&#33267;&#36824;&#20860;&#23481;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#22522;&#20110;&#35813;&#26694;&#26550;&#26500;&#24314;&#20102;&#24459;&#24072;LLaMA&#65292;&#19968;&#31181;&#38024;&#23545;&#27861;&#24459;&#39046;&#22495;&#30340;LLM&#12290;&#36890;&#36807;&#22312;&#25345;&#32493;&#35757;&#32451;&#38454;&#27573;&#27880;&#20837;&#39046;&#22495;&#30693;&#35782;&#21644;&#35774;&#35745;&#26377;&#30417;&#30563;&#24494;&#35843;&#20219;&#21153;&#26469;&#25945;&#25480;&#19987;&#19994;&#25216;&#33021;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28155;&#21152;&#26816;&#32034;&#27169;&#22359;&#24182;&#22312;&#29983;&#25104;&#20043;&#21069;&#25552;&#21462;&#30456;&#20851;&#27861;&#24459;&#25991;&#31456;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#24187;&#35937;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15062</link><description>&lt;p&gt;
&#24459;&#24072;LLaMA&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Lawyer LLaMA Technical Report. (arXiv:2305.15062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15062
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#22522;&#20110;&#35813;&#26694;&#26550;&#26500;&#24314;&#20102;&#24459;&#24072;LLaMA&#65292;&#19968;&#31181;&#38024;&#23545;&#27861;&#24459;&#39046;&#22495;&#30340;LLM&#12290;&#36890;&#36807;&#22312;&#25345;&#32493;&#35757;&#32451;&#38454;&#27573;&#27880;&#20837;&#39046;&#22495;&#30693;&#35782;&#21644;&#35774;&#35745;&#26377;&#30417;&#30563;&#24494;&#35843;&#20219;&#21153;&#26469;&#25945;&#25480;&#19987;&#19994;&#25216;&#33021;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28155;&#21152;&#26816;&#32034;&#27169;&#22359;&#24182;&#22312;&#29983;&#25104;&#20043;&#21069;&#25552;&#21462;&#30456;&#20851;&#27861;&#24459;&#25991;&#31456;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#24187;&#35937;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#22914;LLaMA&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#37096;&#32626;&#21040;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#27861;&#24459;&#25110;&#21307;&#23398;&#26102;&#65292;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#19981;&#36275;&#21644;&#19981;&#36275;&#20197;&#35299;&#20915;&#19982;&#39046;&#22495;&#30456;&#20851;&#38382;&#39064;&#30340;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;LLMs&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#22522;&#20110;&#35813;&#26694;&#26550;&#26500;&#24314;&#20102;&#24459;&#24072;LLaMA&#65292;&#19968;&#31181;&#22522;&#20110;&#27861;&#24459;&#39046;&#22495;&#30340;LLM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#25345;&#32493;&#35757;&#32451;&#38454;&#27573;&#27880;&#20837;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#25945;&#25480;&#27169;&#22411;&#20351;&#29992;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#20219;&#21153;&#26469;&#23398;&#20064;&#19987;&#19994;&#25216;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#24187;&#35937;&#38382;&#39064;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#65292;&#22312;&#27169;&#22411;&#22238;&#31572;&#20219;&#20309;&#26597;&#35810;&#20043;&#21069;&#25552;&#21462;&#30456;&#20851;&#30340;&#27861;&#24459;&#25991;&#31456;&#12290;&#22312;&#23398;&#20064;&#39046;&#22495;&#29305;&#23450;&#25216;&#33021;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#23478;&#30340;&#32463;&#39564;&#27604;&#20174;ChatGPT&#20013;&#25552;&#28860;&#30340;&#32463;&#39564;&#26356;&#26377;&#29992;&#65292;ChatGPT&#21547;&#26377;&#25968;&#30334;&#20010;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), like LLaMA, have exhibited remarkable performance across various tasks. Nevertheless, when deployed to specific domains such as law or medicine, the models still confront the challenge of a deficiency in domain-specific knowledge and an inadequate capability to leverage that knowledge to resolve domain-related problems. In this paper, we propose a new framework to adapt LLMs to specific domains and build Lawyer LLaMA, a legal domain LLM, based on this framework. Specifically, we inject domain knowledge during the continual training stage and teach the model to learn professional skills using properly designed supervised fine-tuning tasks. Moreover, to alleviate the hallucination problem during the model's generation, we add a retrieval module and extract relevant legal articles before the model answers any queries. When learning domain-specific skills, we find that experts' experience is much more useful than experiences distilled from ChatGPT, where hundr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#31034;&#33539;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;advICL&#65292;&#36890;&#36807;&#25913;&#21464;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#26469;&#35823;&#23548;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#31034;&#33539;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.14950</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31034;&#33539;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Demonstration Attacks on Large Language Models. (arXiv:2305.14950v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#31034;&#33539;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;advICL&#65292;&#36890;&#36807;&#25913;&#21464;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#26469;&#35823;&#23548;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#31034;&#33539;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26356;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;-&#26631;&#31614;&#23545;&#20316;&#20026;&#39044;&#20808;&#26465;&#20214;&#25552;&#31034;&#65292;&#24050;&#32463;&#22312;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#26041;&#38754;&#33719;&#24471;&#26174;&#33879;&#30340;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#24341;&#20837;&#31034;&#33539;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20063;&#21487;&#33021;&#24341;&#20837;&#26032;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#25915;&#20987;&#32773;&#21487;&#20197;&#20165;&#20165;&#25805;&#32437;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#26469;&#36827;&#34892;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;ICL&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#31034;&#33539;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;advICL&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#20165;&#20165;&#25913;&#21464;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#20197;&#35823;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#31034;&#33539;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#23558;&#20250;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#31034;&#33539;&#30340;&#22266;&#26377;&#29305;&#24615;&#26159;&#21487;&#20197;&#34987;&#20351;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#30005;&#36335;&#35774;&#35745;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#20934;&#35780;&#20272;&#27169;&#22411;&#23545;&#30005;&#23376;&#20803;&#20214;&#30340;&#30693;&#35782;&#21644;&#22312;Arduino&#29983;&#24577;&#31995;&#32479;&#20013;&#35774;&#35745;&#24494;&#25511;&#21046;&#22120;&#30005;&#36335;&#21644;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20845;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35774;&#35745;&#21161;&#25163;&#22312;&#20013;&#31561;&#22797;&#26434;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#35813;&#30740;&#31350;&#20026;&#25913;&#36827;&#22797;&#26434;&#30005;&#36335;&#35774;&#35745;&#21644;&#23454;&#38469;&#24212;&#29992;&#24615;&#25552;&#20379;&#20102;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.14874</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#30005;&#36335;&#65306;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#21151;&#33021;&#40784;&#22791;&#30340;&#30005;&#23376;&#35774;&#22791;
&lt;/p&gt;
&lt;p&gt;
From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions. (arXiv:2305.14874v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#30005;&#36335;&#35774;&#35745;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#20934;&#35780;&#20272;&#27169;&#22411;&#23545;&#30005;&#23376;&#20803;&#20214;&#30340;&#30693;&#35782;&#21644;&#22312;Arduino&#29983;&#24577;&#31995;&#32479;&#20013;&#35774;&#35745;&#24494;&#25511;&#21046;&#22120;&#30005;&#36335;&#21644;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20845;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35774;&#35745;&#21161;&#25163;&#22312;&#20013;&#31561;&#22797;&#26434;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#35813;&#30740;&#31350;&#20026;&#25913;&#36827;&#22797;&#26434;&#30005;&#36335;&#35774;&#35745;&#21644;&#23454;&#38469;&#24212;&#29992;&#24615;&#25552;&#20379;&#20102;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20808;&#21069;&#26410;&#30693;&#30340;&#25216;&#33021;&#65292;&#21363;&#33021;&#22815;&#20174;&#39640;&#32423;&#25991;&#26412;&#25551;&#36848;&#20013;&#36827;&#34892;&#30005;&#36335;&#35774;&#35745;&#65292;&#31867;&#20284;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#20934;&#65306;Pins100&#65292;&#35780;&#20272;&#27169;&#22411;&#23545;&#30005;&#23376;&#20803;&#20214;&#30340;&#30693;&#35782;&#65307;Micro25&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;Arduino&#29983;&#24577;&#31995;&#32479;&#20013;&#35774;&#35745;&#24120;&#35265;&#30340;&#24494;&#25511;&#21046;&#22120;&#30005;&#36335;&#21644;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#36755;&#20837;&#12289;&#36755;&#20986;&#12289;&#20256;&#24863;&#22120;&#12289;&#30005;&#26426;&#12289;&#21327;&#35758;&#21644;&#36923;&#36753;&#12290;&#22312;&#29983;&#25104;&#23436;&#25972;&#35774;&#22791;&#26041;&#38754;&#65292;&#20687;GPT-4&#21644;Claude-V1&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;Pass@1&#26041;&#38754;&#36798;&#21040;&#20102;60%&#21040;96%&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20845;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35774;&#35745;&#21161;&#25163;&#26469;&#35774;&#35745;&#20013;&#31561;&#22797;&#26434;&#30340;&#35774;&#22791;&#65292;&#20363;&#22914;&#36752;&#23556;&#20379;&#30005;&#30340;&#38543;&#26426;&#25968;&#21457;&#29983;&#22120;&#12289;&#34920;&#24773;&#31526;&#21495;&#38190;&#30424;&#12289;&#21487;&#35265;&#20809;&#35889;&#20202;&#21644;&#20960;&#20010;&#36741;&#21161;&#35774;&#22791;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23545;&#24615;&#33021;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#65292;&#27010;&#36848;&#20102;&#35780;&#20272;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#22797;&#26434;&#30005;&#36335;&#35774;&#35745;&#21644;&#23454;&#38469;&#24212;&#29992;&#24615;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we show that contemporary language models have a previously unknown skill -- the capacity for electronic circuit design from high-level textual descriptions, akin to code generation. We introduce two benchmarks: Pins100, assessing model knowledge of electrical components, and Micro25, evaluating a model's capability to design common microcontroller circuits and code in the Arduino ecosystem that involve input, output, sensors, motors, protocols, and logic -- with models such as GPT-4 and Claude-V1 achieving between 60% to 96% Pass@1 on generating full devices. We include six case studies of using language models as a design assistant for moderately complex devices, such as a radiation-powered random number generator, an emoji keyboard, a visible spectrometer, and several assistive devices, while offering a qualitative analysis performance, outlining evaluation challenges, and suggesting areas of development to improve complex circuit design and practical utility. With thi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#20854;&#21253;&#25324;&#20102;1,508&#20010;&#26679;&#26412;&#21644;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#30142;&#30149;&#30417;&#27979;&#21644;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14480</link><description>&lt;p&gt;
BAND: &#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BAND: Biomedical Alert News Dataset. (arXiv:2305.14480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14480
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#20854;&#21253;&#25324;&#20102;1,508&#20010;&#26679;&#26412;&#21644;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#30142;&#30149;&#30417;&#27979;&#21644;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#29190;&#21457;&#23545;&#20154;&#31867;&#20581;&#24247;&#21644;&#31119;&#21033;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#25913;&#21892;&#30142;&#30149;&#30417;&#27979;&#21644;&#20102;&#35299;&#30142;&#30149;&#20256;&#25773;&#24773;&#20917;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20960;&#20010;&#30417;&#27979;&#31995;&#32479;&#26469;&#30417;&#35270;&#27599;&#26085;&#26032;&#38395;&#35686;&#25253;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32463;&#36807;&#33391;&#22909;&#27880;&#37322;&#30340;&#25253;&#21578;&#25968;&#25454;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#19982;&#30456;&#24212;&#25552;&#37266;&#25110;&#26032;&#38395;&#30340;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#26041;&#38754;&#32570;&#20047;&#20005;&#35880;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#21253;&#25324;&#26469;&#33258;&#29616;&#26377;&#25253;&#21578;&#26032;&#38395;&#25991;&#31456;&#12289;&#20844;&#24320;&#30005;&#23376;&#37038;&#20214;&#21644;&#25552;&#37266;&#30340;1,508&#20010;&#26679;&#26412;&#20197;&#21450;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#27169;&#22411;&#19987;&#23478;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#30142;&#30149;&#29190;&#21457;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;BAND&#25968;&#25454;&#38598;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infectious disease outbreaks continue to pose a significant threat to human health and well-being. To improve disease surveillance and understanding of disease spread, several surveillance systems have been developed to monitor daily news alerts and social media. However, existing systems lack thorough epidemiological analysis in relation to corresponding alerts or news, largely due to the scarcity of well-annotated reports data. To address this gap, we introduce the Biomedical Alert News Dataset (BAND), which includes 1,508 samples from existing reported news articles, open emails, and alerts, as well as 30 epidemiology-related questions. These questions necessitate the model's expert reasoning abilities, thereby offering valuable insights into the outbreak of the disease. The BAND dataset brings new challenges to the NLP world, requiring better disguise capability of the content and the ability to infer important information. We provide several benchmark tasks, including Named Entity
&lt;/p&gt;</description></item><item><title>VIP5&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#20010;&#24615;&#21270;&#27169;&#24577;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#20849;&#20139;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14302</link><description>&lt;p&gt;
VIP5&#65306;&#38754;&#21521;&#25512;&#33616;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VIP5: Towards Multimodal Foundation Models for Recommendation. (arXiv:2305.14302v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14302
&lt;/p&gt;
&lt;p&gt;
VIP5&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#20010;&#24615;&#21270;&#27169;&#24577;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#20849;&#20139;&#26550;&#26500;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#26159;&#19977;&#20010;&#37325;&#35201;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#23427;&#20204;&#20256;&#32479;&#19978;&#29420;&#31435;&#21457;&#23637;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#24314;&#27169;&#21644;&#24037;&#31243;&#26041;&#27861;&#12290;&#36825;&#22952;&#30861;&#20102;&#36825;&#20123;&#39046;&#22495;&#30452;&#25509;&#20174;&#24444;&#27492;&#30340;&#36827;&#23637;&#20013;&#21463;&#30410;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#32479;&#19968;&#19981;&#21516;&#27169;&#24577;&#21644;&#38382;&#39064;&#34920;&#36848;&#30340;&#28508;&#22312;&#36890;&#29992;&#25509;&#21475;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#21457;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;MFM&#65289;&#65292;&#32771;&#34385;&#20102;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#20010;&#24615;&#21270;&#27169;&#24577;&#65292;&#22312;P5&#25512;&#33616;&#33539;&#24335;&#19979;&#32479;&#19968;&#21508;&#31181;&#27169;&#24577;&#21644;&#25512;&#33616;&#20219;&#21153;&#65292;&#22240;&#27492;&#21629;&#21517;&#20026;VIP5&#65288;Visual P5&#65289;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#21151;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#25552;&#31034;&#26469;&#36866;&#24212;&#22810;&#20010;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other's advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and problem formulations. In light of this, we propose the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks. This will enable the processing of multiple modalities in a shared architecture for improved recommendations. To achieve this, we introduce multimodal personalized prompts to accommodate multiple modalities under 
&lt;/p&gt;</description></item><item><title>ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.14196</link><description>&lt;p&gt;
ZeroSCROLLS&#65306;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding. (arXiv:2305.14196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14196
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; ZeroSCROLLS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#20165;&#21253;&#21547;&#27979;&#35797;&#38598;&#32780;&#27809;&#26377;&#35757;&#32451;&#25110;&#24320;&#21457;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;SCROLLS&#22522;&#20934;&#27979;&#35797;&#20013;&#36866;&#24212;&#20102;&#20845;&#20010;&#20219;&#21153;&#65292;&#24182;&#28155;&#21152;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20004;&#20010;&#26032;&#30340;&#20449;&#24687;&#34701;&#21512;&#20219;&#21153;&#65292;&#20363;&#22914;&#32858;&#21512;&#27491;&#38754;&#35780;&#20215;&#30340;&#30334;&#20998;&#27604;&#12290;&#20351;&#29992;ZeroSCROLLS&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;Claude&#20248;&#20110;ChatGPT&#65292;&#24182;&#19988;GPT-4&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;ZeroSCROLLS&#30340;&#22810;&#20010;&#24320;&#25918;&#25361;&#25112;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;&#32858;&#21512;&#20219;&#21153;&#65289;&#65292;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#22240;&#20026;&#27169;&#22411;&#24456;&#38590;&#36890;&#36807;&#26420;&#32032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#30001;&#20110;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36824;&#22312;&#19981;&#26029;&#26356;&#26032;&#65292;&#25105;&#20204;&#36992;&#35831;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#26102;&#30340;ZeroSCROLLS&#25490;&#34892;&#27036;&#19978;&#35780;&#20272;&#20182;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test sets, without training or development data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#65288;CoT fine-tuning&#65289;&#26469;&#25552;&#39640;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;CoT Collection&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#21644;4&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;CoT fine-tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14045</link><description>&lt;p&gt;
CoT Collection: &#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning. (arXiv:2305.14045v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#65288;CoT fine-tuning&#65289;&#26469;&#25552;&#39640;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;CoT Collection&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#21644;4&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;CoT fine-tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#23545;&#20110;&#23567;&#20110;100&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#26102;&#65292;&#20854;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#33021;&#21147;&#19981;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24605;&#32500;&#38142;&#26465;&#30340;&#32622;&#20449;&#24230;&#35843;&#25972;&#26469;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35843;&#25972;&#25351;&#20196;&#25968;&#25454;&#38598;&#8212;&#8212;CoT Collection&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#22686;&#21152;184&#19975;&#20010;&#32622;&#20449;&#24230;&#27880;&#37322;&#21040;1060&#20010;&#20219;&#21153;&#30340;&#29616;&#26377;Flan Collection&#65288;&#21253;&#21547;9&#20010;&#24605;&#32500;&#38142;&#26465;&#20219;&#21153;&#65289;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#20351;&#29992;CoT Collection&#23545;Flan-T5&#65288;3B&#21644;11B&#65289;&#36827;&#34892;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#65292;&#20351;&#24471;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#24605;&#32500;&#38142;&#26465;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;BIG-Bench-Hard&#65288;BBH&#65289;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#20934;&#30830;&#24230;&#26041;&#38754;&#30340;&#24179;&#22343;&#25552;&#21319;&#65306;+4.34%&#65288;Flan-T5 3B&#65289;&#21644;+2.60%&#65288;Flan-T5 11B&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;CoT Collection&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;4&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#24378;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B &amp; 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.12809</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#26469;&#32763;&#36716;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Relabel Minimal Training Subset to Flip a Prediction. (arXiv:2305.12809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Yang&#31561;&#20154;&#21457;&#29616;&#65292;&#20165;&#21024;&#38500;1%&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#32763;&#36716;&#12290;&#37492;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#30340;&#26222;&#36941;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#19968;&#20010;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#21487;&#21542;&#23548;&#33268;&#27979;&#35797;&#32467;&#26524;&#32763;&#36716;&#65311;&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#36825;&#31181;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22987;&#32456;&#33021;&#22815;&#20135;&#29983;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#26426;&#21046;&#26377;&#22810;&#37325;&#20316;&#29992;&#65306;&#65288;1&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24674;&#22797;&#21487;&#33021;&#38169;&#35823;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#65307;&#65288;2&#65289;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#26412;&#25991;&#21457;&#29616;&#23376;&#38598;&#30340;&#22823;&#23567;&#19982;&#35757;&#32451;&#38598;&#20013;&#22122;&#22768;&#25968;&#25454;&#30340;&#27604;&#20363;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#20851;&#31995;&#65307;&#65288;3&#65289;&#25552;&#20379;&#20102;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#20195;&#34920;&#20102;&#23545;&#35782;&#21035;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#38382;&#39064;&#30340;&#31532;&#19968;&#27425;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yang et al. (2023) discovered that removing a mere 1% of training points can often lead to the flipping of a prediction. Given the prevalence of noisy data in machine learning models, we pose the question: can we also result in the flipping of a test prediction by relabeling a small subset of the training data before the model is trained? In this paper, utilizing the extended influence function, we propose an efficient procedure for identifying and relabeling such a subset, demonstrating consistent success. This mechanism serves multiple purposes: (1) providing a complementary approach to challenge model predictions by recovering potentially mislabeled training points; (2) evaluating model resilience, as our research uncovers a significant relationship between the subset's size and the ratio of noisy data in the training set; and (3) offering insights into bias within the training set. To the best of our knowledge, this work represents the first investigation into the problem of identi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23545;&#22810;&#22810;&#27169;&#24577;&#25688;&#35201;&#20219;&#21153;&#65288;M$^3$S&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#30693;&#35782;&#33976;&#39311;&#21644;&#38754;&#21521;&#30446;&#26631;&#30340;&#35270;&#35273;&#24314;&#27169;&#26694;&#26550;&#26469;&#35299;&#20915;&#35813;&#20219;&#21153;&#65292;&#23427;&#21516;&#26102;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21333;&#35821;&#25688;&#35201;&#65288;MMS&#65289;&#21644;&#22810;&#27169;&#24577;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;MXLS&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12767</link><description>&lt;p&gt;
D$^2$TV: &#21452;&#30693;&#35782;&#33976;&#39311;&#21644;&#38754;&#21521;&#30446;&#26631;&#30340;&#35270;&#35273;&#24314;&#27169;&#29992;&#20110;&#22810;&#23545;&#22810;&#22810;&#27169;&#24577;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
D$^2$TV: Dual Knowledge Distillation and Target-oriented Vision Modeling for Many-to-Many Multimodal Summarization. (arXiv:2305.12767v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12767
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23545;&#22810;&#22810;&#27169;&#24577;&#25688;&#35201;&#20219;&#21153;&#65288;M$^3$S&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#30693;&#35782;&#33976;&#39311;&#21644;&#38754;&#21521;&#30446;&#26631;&#30340;&#35270;&#35273;&#24314;&#27169;&#26694;&#26550;&#26469;&#35299;&#20915;&#35813;&#20219;&#21153;&#65292;&#23427;&#21516;&#26102;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21333;&#35821;&#25688;&#35201;&#65288;MMS&#65289;&#21644;&#22810;&#27169;&#24577;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;MXLS&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23545;&#22810;&#22810;&#27169;&#24577;&#25688;&#35201;(M$^3$S)&#20219;&#21153;&#26088;&#22312;&#29983;&#25104;&#20219;&#20309;&#35821;&#35328;&#30340;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#20219;&#20309;&#35821;&#35328;&#30340;&#25991;&#26723;&#36755;&#20837;&#21644;&#30456;&#24212;&#30340;&#22270;&#20687;&#24207;&#21015;&#65292;&#20854;&#26412;&#36136;&#19978;&#21253;&#25324;&#22810;&#27169;&#24577;&#21333;&#35821;&#25688;&#35201;(MMS)&#21644;&#22810;&#27169;&#24577;&#36328;&#35821;&#35328;&#25688;&#35201;(MXLS)&#20219;&#21153;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;MMS&#25110;MXLS&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;M$^3$S&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#65306;1)&#21033;&#29992;MMS&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;MXLS&#30340;&#24615;&#33021;&#65292;&#20294;&#24573;&#30053;&#20102;MMS&#30340;&#24615;&#33021;&#65307;2)&#36890;&#36807;&#38544;&#24335;&#23398;&#20064;&#25110;&#26174;&#24335;&#22797;&#26434;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#25913;&#36827;MMS&#27169;&#22411;&#65292;&#20197;&#36807;&#28388;&#19982;&#25688;&#35201;&#26080;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#23454;&#29992;&#30340;&#20219;&#21153;&#65292;&#21363;M$^3$S&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;M$^3$S&#20219;&#21153;&#30340;&#21452;&#30693;&#35782;&#33976;&#39311;&#21644;&#38754;&#21521;&#30446;&#26631;&#30340;&#35270;&#35273;&#24314;&#27169;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21452;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#30830;&#20445;&#20102;&#21516;&#26102;&#25552;&#39640;MMS&#21644;MXLS&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many-to-many multimodal summarization (M$^3$S) task aims to generate summaries in any language with document inputs in any language and the corresponding image sequence, which essentially comprises multimodal monolingual summarization (MMS) and multimodal cross-lingual summarization (MXLS) tasks. Although much work has been devoted to either MMS or MXLS and has obtained increasing attention in recent years, little research pays attention to the M$^3$S task. Besides, existing studies mainly focus on 1) utilizing MMS to enhance MXLS via knowledge distillation without considering the performance of MMS or 2) improving MMS models by filtering summary-unrelated visual features with implicit learning or explicitly complex training objectives. In this paper, we first introduce a general and practical task, i.e., M$^3$S. Further, we propose a dual knowledge distillation and target-oriented vision modeling framework for the M$^3$S task. Specifically, the dual knowledge distillation method guara
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12599</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation. (arXiv:2305.12599v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#25512;&#29702;&#30456;&#32467;&#21512;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36923;&#36753;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#20174;&#32593;&#39029;&#19978;&#25910;&#38598;&#21487;&#38752;&#30340;&#25968;&#25454;&#26469;&#24314;&#31435;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#38754;&#20020;&#22256;&#38590;&#65292;&#36827;&#32780;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36923;&#36753;&#39537;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;AMR-LDA&#12290;AMR-LDA&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#25104;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#21253;&#21547;&#20102;&#21477;&#23376;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#28982;&#21518;&#23545;&#35813;&#22270;&#36827;&#34892;&#25805;&#20316;&#20197;&#29983;&#25104;&#36923;&#36753;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#12290;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#38543;&#21518;&#34987;&#36716;&#25442;&#22238;&#25991;&#26412;&#65292;&#20174;&#32780;&#21019;&#24314;&#22686;&#24378;&#25968;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20307;&#31995;&#32467;&#26500;&#26080;&#20851;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#22686;&#24378;&#26469;&#22686;&#24378;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-3.5&#21644;GPT-4&#65289;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#26469;&#22686;&#24378;&#21028;&#21035;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining large language models with logical reasoning enhance their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logic structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into texts to create augmented data. Notably, our methodology is architecture-agnostic and enhances generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and fine-tuning discriminative large language models through 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#20570;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.11792</link><description>&lt;p&gt;
LLM&#30340;&#24605;&#36335;&#38142;&#32034;&#24341;&#29992;&#20110;&#22238;&#31572;&#28145;&#20837;&#23545;&#35805;&#38382;&#39064;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought prompting for responding to in-depth dialogue questions with LLM. (arXiv:2305.11792v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#20570;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#25552;&#38382;&#30340;&#26041;&#24335;&#21644;&#20869;&#23481;&#21487;&#20197;&#27934;&#23519;&#20182;&#20204;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#21253;&#25324;&#20154;&#26684;&#12289;&#24773;&#24863;&#21644;&#24515;&#29702;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#20197;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;6&#20010;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#23545;&#35805;&#25110;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#29992;&#25143;&#29366;&#24577;&#30340;3&#20010;&#19981;&#21516;&#26041;&#38754;&#65288;&#21253;&#25324;&#20154;&#26684;&#12289;&#24773;&#24863;&#21644;&#24515;&#29702;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20851;&#20110;&#29992;&#25143;&#29366;&#24577;&#30340;&#21709;&#24212;&#20316;&#20026;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20013;&#38388;&#25512;&#29702;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#30340;&#26032;&#39062;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The way and content in which users ask questions can provide insight into their current status, including their personality, emotions, and psychology. Instead of directly prompting the large language models (LLMs), we explore how chain-of-thought prompting helps in this scenario to perform reasoning and planning according to user status, aiming to provide a more personalized and engaging experience for the user query. To this end, we first construct a benchmark of 6 dialogue or question-answering datasets in both English and Chinese, covering 3 different aspects of user status (\textit{including} \textit{personality}, \textit{emotion}, and \textit{psychology}). Then we prompt the LLMs to generate the response regarding the user status as intermediate reasoning processing. We propose a novel demonstration selection strategy using the semantic similarity of intermediate reasoning instead of test queries. To evaluate the effectiveness and robustness of our approach, we conduct extensive e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11541</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#24212;&#29992;&#21644;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#20294;&#20854;&#22312;&#30495;&#23454;&#30340;&#24037;&#19994;&#29305;&#23450;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#24456;&#24179;&#24248;&#65292;&#22240;&#20026;&#23427;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#24456;&#23569;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;MSQA&#30340;&#22522;&#20934;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28041;&#21450;Microsoft&#20135;&#21697;&#21644;&#23458;&#25143;&#36935;&#21040;&#30340;IT&#25216;&#26415;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#34892;&#19994;&#20113;&#30340;&#29305;&#23450;QA&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#19968;&#33324;&#30340;LLM&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;LLM&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;LLM&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36981;&#24490;&#25105;&#20204;&#30340;&#27169;&#22411;&#34701;&#21512;&#26694;&#26550;&#30340;&#26041;&#27861;&#27604;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#30340;&#24120;&#29992;LLM&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average since there is no specific knowledge in it. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, which is about Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, which is not available for general LLM, so it is well suited for evaluating methods aimed at improving domain-specific capabilities of LLM. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our model fusion framework outperforms the commonly used LLM with retrieval methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.11283</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#21644;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Mean-Field Model-Based Eluder Dimension (MBED)&#30340;&#26032;&#27010;&#24565;&#65292;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;$\epsilon$&#20248;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;MFC&#25110;$\epsilon$&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#36866;&#29992;&#20110;MFG&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#19982;&#30456;&#20851;&#21442;&#25968;&#26080;&#20851;&#65292;&#19982;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#20195;&#29702;&#25968;&#37327;&#26080;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#24378;&#32467;&#26500;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#22312;tabular&#35774;&#32622;&#19979;&#65292;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#20197;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21435;&#38500;&#20551;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10563</link><description>&lt;p&gt;
&#25506;&#31350;&#30828;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding. (arXiv:2305.10563v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21435;&#38500;&#20551;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGEs&#65289;&#30340;&#36136;&#37327;&#65292;&#23427;&#20381;&#36182;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29992;&#36127;&#19977;&#20803;&#32452;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;&#22312;&#36127;&#37319;&#26679;&#30340;&#23545;&#27604;&#25439;&#22833;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#39640;&#36136;&#37327;&#65288;&#21363;&#30828;&#65289;&#36127;&#37319;&#26679;&#30340;&#21551;&#21457;&#24335;&#29983;&#25104;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;InfoNCE&#25439;&#22833;&#65292;&#26174;&#24335;&#32771;&#34385;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#30828;&#36127;&#26679;&#26412;&#26368;&#23567;&#21270;InfoNCE&#25439;&#22833;&#21487;&#20197;&#26368;&#22823;&#21270;&#32473;&#23450;&#19977;&#20803;&#32452;&#21644;&#36127;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#35777;&#26126;&#30828;&#36127;&#26679;&#26412;&#20250;&#23548;&#33268;&#20551;&#36127;&#26679;&#26412;&#65288;&#21363;&#38169;&#35823;&#30340;&#20107;&#23454;&#19977;&#20803;&#32452;&#65289;&#24182;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#32467;&#26500;&#21435;&#38500;&#20551;&#36127;&#19977;&#20803;&#32452;&#30340;&#26032;&#22411;&#36127;&#37319;&#26679;&#20998;&#24067;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#31216;&#20026;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of the knowledge graph completion task heavily depends on the quality of the knowledge graph embeddings (KGEs), which relies on self-supervised learning and augmenting the dataset with negative triples. There is a gap in literature between the theoretical analysis of negative samples on contrastive loss and heuristic generation of quality (i.e., hard) negative triples. In this paper, we modify the InfoNCE loss to explicitly account for the negative sample distribution. We show minimizing InfoNCE loss with hard negatives maximizes the KL-divergence between the given and negative triple embedding. However, we also show that hard negatives can lead to false negatives (i.e., accidentally factual triples) and reduce downstream task performance. To address this issue, we propose a novel negative sample distribution that uses the graph structure of the knowledge graph to remove the false negative triples. We call our algorithm Hardness and Structure-aware (\textbf{HaSa}) contrasti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#65292;&#20854;&#20013;&#31639;&#27861;&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#65292;&#24182;&#19988;SELM&#22312;&#21152;&#23494;&#20998;&#26512;&#26041;&#38754;&#30340;&#23433;&#20840;&#24615;&#33021;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.10445</link><description>&lt;p&gt;
&#35760;&#24518;&#26377;&#30410;&#65306;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
Memorization for Good: Encryption with Autoregressive Language Models. (arXiv:2305.10445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#65292;&#20854;&#20013;&#31639;&#27861;&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#65292;&#24182;&#19988;SELM&#22312;&#21152;&#23494;&#20998;&#26512;&#26041;&#38754;&#30340;&#23433;&#20840;&#24615;&#33021;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21487;&#20197;&#35760;&#24518;&#21644;&#32972;&#35829;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#31181;&#35760;&#24518;&#36890;&#24120;&#34987;&#35748;&#20026;&#20855;&#26377;&#19981;&#33391;&#23646;&#24615;&#65292;&#20363;&#22914;&#36807;&#24230;&#25311;&#21512;&#21644;&#20449;&#24687;&#27844;&#28431;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#35760;&#24518;&#35270;&#20026;LM&#30340;&#19968;&#31181;&#26410;&#24320;&#21457;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#33258;&#22238;&#24402;LM&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#12290;&#34429;&#28982;SELM&#19981;&#26131;&#21463;&#20256;&#32479;&#21152;&#23494;&#20998;&#26512;&#26041;&#27861;&#25915;&#30772;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#35777;&#21464;&#20307;&#65292;&#30740;&#31350;&#23427;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/OSU-NLP-Group/SELM &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-parameterized neural language models (LMs) can memorize and recite long sequences of training data. While such memorization is normally associated with undesired properties such as overfitting and information leaking, our work casts memorization as an unexplored capability of LMs. We propose the first symmetric encryption algorithm with autoregressive language models (SELM). We show that autoregressive LMs can encode arbitrary data into a compact real-valued vector (i.e., encryption) and then losslessly decode the vector to the original message (i.e., decryption) via random subspace optimization and greedy decoding. While SELM is not amenable to conventional cryptanalysis, we investigate its security through a novel empirical variant of the classic IND-CPA (indistinguishability under chosen-plaintext attack) game. Our code and datasets are available at https://github.com/OSU-NLP-Group/SELM.
&lt;/p&gt;</description></item><item><title>&#35299;&#37322;&#22411;AI&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#26679;&#26412;&#37327;&#21644;&#25512;&#24191;&#38382;&#39064;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#23545;XAI&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#30340;&#35780;&#20272;&#65292;&#38656;&#35201;&#26356;&#20855;&#21253;&#23481;&#24615;&#30340;&#29992;&#25143;&#30740;&#31350;&#21407;&#21017;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09477</link><description>&lt;p&gt;
&#35299;&#37322;&#22411;AI&#30740;&#31350;&#20013;&#26410;&#32463;&#35777;&#26126;&#30340;&#26679;&#26412;&#37327;&#21644;&#25512;&#24191;&#65306;&#26356;&#20855;&#21253;&#23481;&#24615;&#29992;&#25143;&#30740;&#31350;&#30340;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Unjustified Sample Sizes and Generalizations in Explainable AI Research: Principles for More Inclusive User Studies. (arXiv:2305.09477v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09477
&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22411;AI&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#26679;&#26412;&#37327;&#21644;&#25512;&#24191;&#38382;&#39064;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#23545;XAI&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#30340;&#35780;&#20272;&#65292;&#38656;&#35201;&#26356;&#20855;&#21253;&#23481;&#24615;&#30340;&#29992;&#25143;&#30740;&#31350;&#21407;&#21017;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20262;&#29702;&#26694;&#26550;&#35201;&#27714;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#35299;&#37322;&#12290;&#35299;&#37322;&#22411;AI&#65288;XAI&#65289;&#27169;&#22411;&#32463;&#24120;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#27979;&#35797;&#20854;&#20805;&#20998;&#24615;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#20154;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#35299;&#37322;&#38656;&#27714;&#65292;&#22240;&#27492;&#21442;&#19982;&#32773;&#26679;&#26412;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#24212;&#36275;&#22815;&#22823;&#65292;&#20197;&#20195;&#34920;&#30446;&#26631;&#20154;&#32676;&#65292;&#20197;&#23454;&#29616;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;XAI&#30740;&#31350;&#20154;&#21592;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21453;&#24605;&#21644;&#35777;&#26126;&#20854;&#26679;&#26412;&#37327;&#25110;&#36991;&#20813;&#36328;&#20154;&#32676;&#24191;&#27867;&#25512;&#24191;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;2012&#24180;&#33267;2022&#24180;&#38388;&#21457;&#34920;&#30340;220&#31687;XAI&#29992;&#25143;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#27809;&#26377;&#25552;&#20379;&#26679;&#26412;&#37327;&#30340;&#29702;&#30001;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#35770;&#25991;&#23558;&#20854;&#32467;&#35770;&#25512;&#24191;&#21040;&#30446;&#26631;&#20154;&#32676;&#20043;&#22806;&#65292;&#24182;&#19988;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#23450;&#37327;&#30740;&#31350;&#20013;&#26356;&#24191;&#27867;&#30340;&#32467;&#35770;&#19982;&#26356;&#22823;&#30340;&#26679;&#26412;&#26377;&#20851;&#12290;&#36825;&#20123;&#26041;&#27861;&#35770;&#38382;&#39064;&#21487;&#33021;&#20250;&#22952;&#30861;&#35780;&#20272;XAI&#31995;&#32479;&#26159;&#21542;&#23454;&#29616;&#20102;&#20262;&#29702;&#26694;&#26550;&#20013;&#25552;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#26356;&#20855;&#21253;&#23481;&#24615;&#30340;XAI&#30740;&#31350;&#29992;&#25143;&#30740;&#31350;&#21407;&#21017;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many ethical frameworks require artificial intelligence (AI) systems to be explainable. Explainable AI (XAI) models are frequently tested for their adequacy in user studies. Since different people may have different explanatory needs, it is important that participant samples in user studies are large enough to represent the target population to enable generalizations. However, it is unclear to what extent XAI researchers reflect on and justify their sample sizes or avoid broad generalizations across people. We analyzed XAI user studies (N = 220) published between 2012 and 2022. Most studies did not offer rationales for their sample sizes. Moreover, most papers generalized their conclusions beyond their target population, and there was no evidence that broader conclusions in quantitative studies were correlated with larger samples. These methodological problems can impede evaluations of whether XAI systems implement the explainability called for in ethical frameworks. We outline princip
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;H2CGL&#30340;&#26032;&#39062;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#25991;&#32593;&#32476;&#30340;&#21160;&#24577;&#24314;&#27169;&#21644;&#24433;&#21709;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#21644;&#24322;&#26500;&#30340;&#26041;&#24335;&#35760;&#24405;&#30446;&#26631;&#35770;&#25991;&#24180;&#24230;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#39640;&#34987;&#24341;&#35770;&#25991;&#21644;&#21442;&#32771;&#25991;&#29486;&#12289;&#24341;&#25991;&#12289;&#30446;&#26631;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23427;&#37319;&#29992;&#21152;&#26435; GIN &#26469;&#25429;&#25417;&#24322;&#26500;&#23376;&#22270;&#30340;&#21160;&#24577;&#65292;&#21516;&#26102;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01572</link><description>&lt;p&gt;
H2CGL: &#29992;&#20110;&#24341;&#25991;&#32593;&#32476;&#21160;&#24577;&#24314;&#27169;&#19982;&#24433;&#21709;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
H2CGL: Modeling Dynamics of Citation Network for Impact Prediction. (arXiv:2305.01572v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;H2CGL&#30340;&#26032;&#39062;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#25991;&#32593;&#32476;&#30340;&#21160;&#24577;&#24314;&#27169;&#21644;&#24433;&#21709;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#21644;&#24322;&#26500;&#30340;&#26041;&#24335;&#35760;&#24405;&#30446;&#26631;&#35770;&#25991;&#24180;&#24230;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#39640;&#34987;&#24341;&#35770;&#25991;&#21644;&#21442;&#32771;&#25991;&#29486;&#12289;&#24341;&#25991;&#12289;&#30446;&#26631;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23427;&#37319;&#29992;&#21152;&#26435; GIN &#26469;&#25429;&#25417;&#24322;&#26500;&#23376;&#22270;&#30340;&#21160;&#24577;&#65292;&#21516;&#26102;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30340;&#24433;&#21709;&#21147;&#36890;&#24120;&#26159;&#36890;&#36807;&#20854;&#24341;&#29992;&#25968;&#37327;&#26469;&#34913;&#37327;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20302;&#20272;&#26032;&#21457;&#34920;&#35770;&#25991;&#38543;&#26102;&#38388;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#19988;&#26410;&#33021;&#23558;&#36825;&#31181;&#24341;&#25991;&#32593;&#32476;&#30340;&#21160;&#24577;&#24615;&#32435;&#20837;&#22270;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#24180;&#24230;&#35270;&#35282;&#30340;&#30446;&#26631;&#35770;&#25991;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#65292;&#24182;&#35760;&#24405;&#20102;&#30446;&#26631;&#35770;&#25991;&#31185;&#23398;&#32972;&#26223;&#20449;&#24687;&#30340;&#24180;&#24230;&#21160;&#24577;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;&#20998;&#23618;&#24322;&#26500;&#23545;&#27604;&#22270;&#23398;&#20064;&#27169;&#22411;&#65288;H2CGL&#65289;&#65292;&#20197;&#34701;&#21512;&#24341;&#25991;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;H2CGL&#20998;&#21035;&#32858;&#21512;&#20102;&#27599;&#24180;&#30340;&#24322;&#26500;&#20449;&#24687;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#39640;&#34987;&#24341;&#35770;&#25991;&#20197;&#21450;&#21442;&#32771;&#25991;&#29486;&#21644;&#24341;&#25991;&#19982;&#30446;&#26631;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#23427;&#37319;&#29992;&#21152;&#26435;GIN&#26469;&#25429;&#25417;&#24180;&#20221;&#20043;&#38388;&#30340;&#24322;&#26500;&#23376;&#22270;&#21160;&#24577;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential impact of a paper is often quantified by how many citations it will receive. However, most commonly used models may underestimate the influence of newly published papers over time, and fail to encapsulate this dynamics of citation network into the graph. In this study, we construct hierarchical and heterogeneous graphs for target papers with an annual perspective. The constructed graphs can record the annual dynamics of target papers' scientific context information. Then, a novel graph neural network, Hierarchical and Heterogeneous Contrastive Graph Learning Model (H2CGL), is proposed to incorporate heterogeneity and dynamics of the citation network. H2CGL separately aggregates the heterogeneous information for each year and prioritizes the highly-cited papers and relationships among references, citations, and the target paper. It then employs a weighted GIN to capture dynamics between heterogeneous subgraphs over years. Moreover, it leverages contrastive learning to make
&lt;/p&gt;</description></item><item><title>DeepEIK&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#65292;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01523</link><description>&lt;p&gt;
&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#25512;&#21160;AI&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empowering AI drug discovery with explicit and implicit knowledge. (arXiv:2305.01523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01523
&lt;/p&gt;
&lt;p&gt;
DeepEIK&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#65292;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29420;&#31435;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#26174;&#24335;&#30693;&#35782;&#25110;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#30340;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#30340;&#30740;&#31350;&#36805;&#36895;&#22686;&#38271;&#12290;&#36825;&#20123;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;AI&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29420;&#31435;&#22320;&#25972;&#21512;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#20250;&#38459;&#30861;&#23545;&#20998;&#23376;&#30340;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DeepEIK&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#26469;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Recently, research on independently utilizing either explicit knowledge from knowledge graphs or implicit knowledge from biomedical literature for AI drug discovery has been growing rapidly. These approaches have greatly improved the prediction accuracy of AI models on multiple downstream tasks. However, integrating explicit and implicit knowledge independently hinders their understanding of molecules. Results: We propose DeepEIK, a unified deep learning framework that incorporates both explicit and implicit knowledge for AI drug discovery. We adopt feature fusion to process the multi-modal inputs, and leverage the attention mechanism to denoise the text information. Experiments show that DeepEIK significantly outperforms state-of-the-art methods on crucial tasks in AI drug discovery including drug-target interaction prediction, drug property prediction and protein-protein interaction prediction. Further studies show that benefiting from explicit and implicit knowledge, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#39592;&#26550;&#20113;&#30528;&#33394;&#25216;&#26415;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#27880;&#25968;&#25454;&#19978;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08799</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#65306;&#22522;&#20110;&#39592;&#26550;&#20113;&#30528;&#33394;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised 3D Action Representation Learning with Skeleton Cloud Colorization. (arXiv:2304.08799v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#39592;&#26550;&#20113;&#30528;&#33394;&#25216;&#26415;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#27880;&#25968;&#25454;&#19978;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#19977;&#32500;&#39592;&#26550;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#36880;&#28176;&#21463;&#21040;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#26631;&#27880;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#39592;&#26550;&#20113;&#30528;&#33394;&#25216;&#26415;&#23545;&#26410;&#26631;&#27880;&#30340;&#39592;&#26550;&#25968;&#25454;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D Skeleton-based human action recognition has attracted increasing attention in recent years. Most of the existing work focuses on supervised learning which requires a large number of labeled action sequences that are often expensive and time-consuming to annotate. In this paper, we address self-supervised 3D action representation learning for skeleton-based action recognition. We investigate self-supervised representation learning and design a novel skeleton cloud colorization technique that is capable of learning spatial and temporal skeleton representations from unlabeled skeleton sequence data. We represent a skeleton action sequence as a 3D skeleton cloud and colorize each point in the cloud according to its temporal and spatial orders in the original (unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud, we design an auto-encoder framework that can learn spatial-temporal features from the artificial color labels of skeleton joints effectively. Specifical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.08612</link><description>&lt;p&gt;
&#31163;&#25955;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#26725;&#26753;&#65306;&#30452;&#36890;&#27861;&#19982;&#20854;&#23427;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#20165;&#38480;&#20110;&#35745;&#31639;&#36830;&#32493;&#21464;&#37327;&#30340;&#26799;&#24230;&#65292;&#38480;&#21046;&#20102;&#28041;&#21450;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340; Straight-Through&#65288;ST&#65289;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20316;&#20026;&#26799;&#24230;&#30340;&#19968;&#38454;&#36817;&#20284;&#20540;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; ReinMax&#65292;&#23427;&#38598;&#25104;&#20102; Heun's Method&#65292;&#19968;&#31181;&#35299;ODE&#30340;&#20108;&#38454;&#25968;&#20540;&#26041;&#27861;&#65292;&#20197;&#36817;&#20284;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201; Hessian &#25110;&#20854;&#20182;&#20108;&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#39044;&#27979;&#21644;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;\ours &#22312;&#29616;&#26377;&#25216;&#26415;&#20013;&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324; ST &#21644; Straight-Through Gum&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
&lt;/p&gt;</description></item><item><title>H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#20854;&#24378;&#21270;&#20102;&#27700;&#24179;&#27880;&#37322;&#21644;&#26059;&#36716;&#27880;&#37322;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04403</link><description>&lt;p&gt;
H2RBox-v2&#65306;&#36890;&#36807;&#23545;&#31216;&#23398;&#20064;&#25552;&#39640;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
H2RBox-v2: Boosting HBox-supervised Oriented Object Detection via Symmetric Learning. (arXiv:2304.04403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04403
&lt;/p&gt;
&lt;p&gt;
H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#20854;&#24378;&#21270;&#20102;&#27700;&#24179;&#27880;&#37322;&#21644;&#26059;&#36716;&#27880;&#37322;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#36965;&#24863;&#31561;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#38656;&#27714;&#30340;&#26085;&#30410;&#22686;&#38271;&#65292;&#26377;&#21521;&#27880;&#37322;&#21464;&#24471;&#38750;&#24120;&#36153;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;&#30340;&#27700;&#24179;&#27880;&#37322;&#25968;&#25454;&#38598;&#24182;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#26816;&#27979;&#22120;H2RBox&#65292;&#29992;&#20110;&#20174;&#27700;&#24179;&#26694;Box&#20013;&#23398;&#20064;&#26059;&#36716;&#26694;RBox&#65292;&#24182;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;H2RBox-v2&#30340;&#26032;&#29256;&#26412;&#65292;&#20197;&#36827;&#19968;&#27493;&#24357;&#21512;HBox&#30417;&#30563;&#21644;RBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21033;&#29992;&#32763;&#36716;&#21644;&#26059;&#36716;&#19968;&#33268;&#24615;&#26469;&#24320;&#21457;&#36724;&#23545;&#31216;&#24615;&#26159;&#21487;&#34892;&#30340;&#65292;H2RBox-v2&#21017;&#37319;&#29992;&#19982;H2RBox&#31867;&#20284;&#30340;&#24369;&#30417;&#30563;&#20998;&#25903;&#65292;&#24182;&#23884;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#20998;&#25903;&#65292;&#23427;&#21487;&#20197;&#20174;&#23545;&#35937;&#22270;&#20687;&#20013;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#20013;&#23398;&#20064;&#26041;&#21521;&#12290;&#36890;&#36807;&#22788;&#29702;&#21608;&#36793;&#38382;&#39064;&#30340;&#27169;&#22359;&#65288;&#20363;&#22914;&#35282;&#21608;&#26399;&#24615;&#65289;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#31283;&#23450;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing demand for oriented object detection e.g. in autonomous driving and remote sensing, the oriented annotation has become a labor-intensive work. To make full use of existing horizontally annotated datasets and reduce the annotation cost, a weakly-supervised detector H2RBox for learning the rotated box (RBox) from the horizontal box (HBox) has been proposed and received great attention. This paper presents a new version, H2RBox-v2, to further bridge the gap between HBox-supervised and RBox-supervised oriented object detection. While exploiting axisymmetry via flipping and rotating consistencies is available through our theoretical analysis, H2RBox-v2, using a weakly-supervised branch similar to H2RBox, is embedded with a novel self-supervised branch that learns orientations from the symmetry inherent in the image of objects. Complemented by modules to cope with peripheral issues, e.g. angular periodicity, a stable and effective solution is achieved. To our knowledge, H
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#25366;&#25496;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20219;&#21153;&#27010;&#36848;&#12289;&#25216;&#26415;&#32508;&#36848;&#21644;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#35813;&#32508;&#36848;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2304.00485</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25366;&#25496;&#30340;&#32593;&#32476;&#23433;&#20840;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Mining for Cybersecurity: A Survey. (arXiv:2304.00485v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#25366;&#25496;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20219;&#21153;&#27010;&#36848;&#12289;&#25216;&#26415;&#32508;&#36848;&#21644;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#35813;&#32508;&#36848;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#32593;&#32476;&#25915;&#20987;&#65288;&#22914;&#24694;&#24847;&#36719;&#20214;&#12289;&#22403;&#22334;&#37038;&#20214;&#21644;&#20837;&#20405;&#31561;&#65289;&#30340;&#29467;&#22686;&#24050;&#32463;&#23545;&#31038;&#20250;&#36896;&#25104;&#20102;&#20005;&#37325;&#21518;&#26524;&#12290;&#20445;&#38556;&#32593;&#32476;&#23433;&#20840;&#24050;&#25104;&#20026;&#32452;&#32455;&#21644;&#25919;&#24220;&#30340;&#24403;&#21153;&#20043;&#24613;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34429;&#28982;&#24191;&#27867;&#24212;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#23041;&#32961;&#65292;&#20294;&#23427;&#20204;&#38590;&#20197;&#24314;&#27169;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22270;&#25366;&#25496;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#35768;&#22810;&#30740;&#31350;&#32773;&#30740;&#31350;&#20102;&#36825;&#20123;&#25216;&#26415;&#65292;&#20197;&#25429;&#25417;&#32593;&#32476;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#24635;&#32467;&#29616;&#26377;&#22522;&#20110;&#22270;&#30340;&#32593;&#32476;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#25366;&#25496;&#25216;&#26415;&#30340;&#32593;&#32476;&#23433;&#20840;&#30740;&#31350;&#32508;&#36848;&#65292;&#21253;&#25324;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#30340;&#27010;&#36848;&#65292;&#20856;&#22411;&#30340;&#22270;&#25366;&#25496;&#25216;&#26415;&#65292;&#24212;&#29992;&#23427;&#20204;&#21040;&#32593;&#32476;&#23433;&#20840;&#30340;&#19968;&#33324;&#36807;&#31243;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#23433;&#20840;&#38382;&#39064;&#30340;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The explosive growth of cyber attacks nowadays, such as malware, spam, and intrusions, caused severe consequences on society. Securing cyberspace has become an utmost concern for organizations and governments. Traditional Machine Learning (ML) based methods are extensively used in detecting cyber threats, but they hardly model the correlations between real-world cyber entities. In recent years, with the proliferation of graph mining techniques, many researchers investigated these techniques for capturing correlations between cyber entities and achieving high performance. It is imperative to summarize existing graph-based cybersecurity solutions to provide a guide for future studies. Therefore, as a key contribution of this paper, we provide a comprehensive review of graph mining for cybersecurity, including an overview of cybersecurity tasks, the typical graph mining techniques, and the general process of applying them to cybersecurity, as well as various solutions for different cybers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TPMCF&#65292;&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.18201</link><description>&lt;p&gt;
TPMCF: &#20351;&#29992;&#22810;&#28304;&#21327;&#21516;&#29305;&#24449;&#36827;&#34892;&#26102;&#38388;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TPMCF: Temporal QoS Prediction using Multi-Source Collaborative Features. (arXiv:2303.18201v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TPMCF&#65292;&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#26381;&#21153;API&#30340;&#24555;&#36895;&#37096;&#32626;&#65292;&#20010;&#24615;&#21270;&#30340;&#26381;&#21153;&#25512;&#33616;&#22312;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#30340;&#22686;&#38271;&#20013;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20915;&#23450;&#26381;&#21153;&#24615;&#33021;&#30340;&#26381;&#21153;&#36136;&#37327;(QoS)&#21442;&#25968;&#32463;&#24120;&#34987;&#29992;&#20110;&#25512;&#33616;&#65292;&#20294;&#38543;&#26102;&#38388;&#27874;&#21160;&#12290;&#22240;&#27492;&#65292;QoS&#30340;&#39044;&#27979;&#23545;&#20110;&#22312;&#31561;&#20215;&#26381;&#21153;&#20013;&#35782;&#21035;&#21512;&#36866;&#30340;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#20195;&#30340;&#26102;&#38388;QoS&#39044;&#27979;&#26041;&#27861;&#30001;&#20110;&#21508;&#31181;&#38480;&#21046;&#32780;&#24456;&#38590;&#36798;&#21040;&#26399;&#26395;&#30340;&#31934;&#24230;&#65292;&#20363;&#22914;&#26080;&#27861;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#20197;&#21450;&#25429;&#33719;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#39640;&#38454;&#26102;&#38388;&#20851;&#31995;&#12290;&#34429;&#28982;&#26368;&#36817;&#19968;&#20123;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#24314;&#27169;QoS&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20854;&#20182;&#29305;&#24449;&#65288;&#20363;&#22914;&#21327;&#20316;&#29305;&#24449;&#65289;&#26469;&#29702;&#35299;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#39044;&#27979;&#31934;&#24230;&#20250;&#38477;&#20302;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;TPMCF&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;TPMCF&#21033;&#29992;&#22810;&#28304;&#29305;&#24449;&#65288;&#21253;&#25324;&#26102;&#38388;&#12289;&#29992;&#25143;&#21644;&#26381;&#21153;&#29305;&#24449;&#65289;&#36827;&#34892;QoS&#39044;&#27979;&#12290;&#20855;&#20307;&#22320;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#39062;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#21033;&#29992;&#29992;&#25143;-&#26381;&#21153;&#20132;&#20114;&#20043;&#38388;&#30340;&#39640;&#38454;&#26102;&#38388;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#29992;&#21327;&#20316;&#29305;&#24449;&#26469;&#25429;&#25417;&#29992;&#25143;&#21644;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#21644;&#24322;&#24120;&#20540;&#12290;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;TPMCF&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, with the rapid deployment of service APIs, personalized service recommendations have played a paramount role in the growth of the e-commerce industry. Quality-of-Service (QoS) parameters determining the service performance, often used for recommendation, fluctuate over time. Thus, the QoS prediction is essential to identify a suitable service among functionally equivalent services over time. The contemporary temporal QoS prediction methods hardly achieved the desired accuracy due to various limitations, such as the inability to handle data sparsity and outliers and capture higher-order temporal relationships among user-service interactions. Even though some recent recurrent neural-network-based architectures can model temporal relationships among QoS data, prediction accuracy degrades due to the absence of other features (e.g., collaborative features) to comprehend the relationship among the user-service interactions. This paper addresses the above challenges and proposes a s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#26469;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#25506;&#32034;&#38750;&#38543;&#26426;&#25968;&#25454;&#20013;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#32534;&#30721;&#12290;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26041;&#38754;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.16045</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#31354;&#38388;&#21435;&#21367;&#31215;&#21644;&#20449;&#24687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Optimal Spatial Deconvolution and Message Reconstruction from a Large Generative Model of Models. (arXiv:2303.16045v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#26469;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#25506;&#32034;&#38750;&#38543;&#26426;&#25968;&#25454;&#20013;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#32534;&#30721;&#12290;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26041;&#38754;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#20381;&#36182;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#65292;&#24182;&#35745;&#31639;&#20986;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#21487;&#20197;&#25506;&#31350;&#38750;&#38543;&#26426;&#25968;&#25454;&#22914;&#20309;&#32534;&#30721;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#20449;&#21495;&#25110;&#20449;&#24687;&#30340;&#32500;&#24230;&#21644;&#38271;&#24230;&#23610;&#24230;&#12290;&#35813;&#26041;&#27861;&#26159;&#19982;&#21487;&#35745;&#31639;&#25110;&#21322;&#21487;&#35745;&#31639;&#30340;&#36817;&#20284;&#26041;&#27861;&#25110;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#30456;&#20851;&#20294;&#19981;&#29420;&#31435;&#30340;&#12290;&#26412;&#25991;&#30340;&#32467;&#26524;&#23545;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26377;&#24212;&#29992;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a general-purpose univariate signal deconvolution method based on the principles of an approach to Artificial General Intelligence. This approach is based on a generative model that combines information theory and algorithmic probability that required a large calculation of an estimation of a `universal distribution' to build a general-purpose model of models independent of probability distributions. This was used to investigate how non-random data may encode information about the physical properties such as dimension and length scales in which a signal or message may have been originally encoded, embedded, or generated. This multidimensional space reconstruction method is based on information theory and algorithmic probability, and it is agnostic, but not independent, with respect to the chosen computable or semi-computable approximation method or encoding-decoding scheme. The results presented in this paper are useful for applications in coding theory, particularly in ze
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13496</link><description>&lt;p&gt;
MAE&#39044;&#21069;&#32622;&#35757;&#32451;&#23545;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#26631;&#20934;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#20351;&#29992;&#25968;&#21313;&#20159;&#24352;&#22270;&#20687;&#30340;&#22823;&#35268;&#27169;&#65288;&#24369;&#65289;&#30417;&#30563;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#38454;&#27573;&#65292;&#23427;&#20351;&#29992;&#20102;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#26469;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;&#34429;&#28982;MAE&#25216;&#26415;&#20165;&#34987;&#35777;&#26126;&#33021;&#22815;&#19982;&#27169;&#22411;&#22823;&#23567;&#30456;&#32553;&#25918;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20063;&#21487;&#20197;&#38543;&#25968;&#25454;&#38598;&#22823;&#23567;&#32553;&#25918;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;MAE&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#21487;&#21516;&#26102;&#36866;&#29992;&#20110;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#12290;&#39044;&#21069;&#32622;&#35757;&#32451;&#22312;&#19968;&#31995;&#21015;&#27169;&#22411;&#35268;&#27169;&#65288;&#21442;&#25968;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65288;&#22270;&#20687;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#19978;&#19968;&#33268;&#25552;&#39640;&#20102;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#65292;&#19988;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#20854;&#22312;10&#20010;&#19981;&#21516;&#30340;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35270;&#39057;&#35782;&#21035;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video reco
&lt;/p&gt;</description></item><item><title>QDP&#26041;&#27861;&#20248;&#21270;&#20102;&#20934;&#38745;&#24577;&#21644;&#21160;&#24577;&#25805;&#20316;&#22522;&#20803;&#30340;&#36816;&#21160;&#36895;&#24230;&#21644;&#25361;&#36873;&#25918;&#32622;&#20301;&#32622;&#31561;&#21442;&#25968;&#65292;&#26377;&#21161;&#20110;&#22788;&#29702;&#23478;&#24237;&#24067;&#29289;&#26448;&#26009;&#33539;&#22260;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.13320</link><description>&lt;p&gt;
QDP&#65306;&#23398;&#20064;&#20018;&#34892;&#20248;&#21270;&#20934;&#38745;&#24577;&#21644;&#21160;&#24577;&#25805;&#20316;&#22522;&#20803;&#20197;&#36827;&#34892;&#26426;&#22120;&#20154;&#24067;&#26009;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
QDP: Learning to Sequentially Optimise Quasi-Static and Dynamic Manipulation Primitives for Robotic Cloth Manipulation. (arXiv:2303.13320v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13320
&lt;/p&gt;
&lt;p&gt;
QDP&#26041;&#27861;&#20248;&#21270;&#20102;&#20934;&#38745;&#24577;&#21644;&#21160;&#24577;&#25805;&#20316;&#22522;&#20803;&#30340;&#36816;&#21160;&#36895;&#24230;&#21644;&#25361;&#36873;&#25918;&#32622;&#20301;&#32622;&#31561;&#21442;&#25968;&#65292;&#26377;&#21161;&#20110;&#22788;&#29702;&#23478;&#24237;&#24067;&#29289;&#26448;&#26009;&#33539;&#22260;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#23450;&#20041;&#30340;&#25805;&#20316;&#22522;&#20803;&#34987;&#24191;&#27867;&#29992;&#20110;&#24067;&#26009;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#20725;&#30828;&#24230;&#25110;&#23494;&#24230;&#31561;&#24067;&#26009;&#23646;&#24615;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#36825;&#20123;&#22522;&#20803;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#35299;&#20915;&#20102;&#25361;&#36873;&#21644;&#25918;&#32622;&#20301;&#32622;&#30340;&#21442;&#25968;&#21270;&#38382;&#39064;&#65292;&#20294;&#20934;&#38745;&#24577;&#21644;&#21160;&#24577;&#25805;&#20316;&#22522;&#20803;&#30340;&#36895;&#24230;&#25110;&#36712;&#36857;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#21364;&#34987;&#24573;&#30053;&#20102;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#36825;&#20123;&#21442;&#25968;&#30340;&#20540;&#23545;&#20110;&#22788;&#29702;&#23478;&#24237;&#24067;&#29289;&#29289;&#21697;&#20013;&#23384;&#22312;&#30340;&#26448;&#26009;&#33539;&#22260;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Quasi-Dynamic Parameterisable&#65288;QDP&#65289;&#26041;&#27861;&#65292;&#23427;&#20248;&#21270;&#20102;&#21442;&#25968;&#65292;&#20363;&#22914;&#20934;&#38745;&#24577;&#21644;&#21160;&#24577;&#25805;&#20316;&#22522;&#20803;&#30340;&#36816;&#21160;&#36895;&#24230;&#20197;&#22806;&#65292;&#36824;&#21253;&#25324;&#25361;&#36873;&#21644;&#25918;&#32622;&#20301;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20018;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#39034;&#24207;&#35299;&#32806;&#32452;&#25104;&#22522;&#20803;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#24067;&#26009;&#24179;&#25972;&#21644;&#22534;&#21472;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-defined manipulation primitives are widely used for cloth manipulation. However, cloth properties such as its stiffness or density can highly impact the performance of these primitives. Although existing solutions have tackled the parameterisation of pick and place locations, the effect of factors such as the velocity or trajectory of quasi-static and dynamic manipulation primitives has been neglected. Choosing appropriate values for these parameters is crucial to cope with the range of materials present in house-hold cloth objects. To address this challenge, we introduce the Quasi-Dynamic Parameterisable (QDP) method, which optimises parameters such as the motion velocity in addition to the pick and place positions of quasi-static and dynamic manipulation primitives. In this work, we leverage the framework of Sequential Reinforcement Learning to decouple sequentially the parameters that compose the primitives. To evaluate the effectiveness of the method we focus on the task of clo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#21644;&#38750;&#24490;&#29615;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#38598;&#20307;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#23558;&#38382;&#39064;&#21010;&#20998;&#20026;&#29420;&#31435;&#23376;&#38382;&#39064;&#21644;&#27604;&#36739;&#30452;&#25509;&#25928;&#24212;&#31561;&#26041;&#24335;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;</title><link>http://arxiv.org/abs/2303.04038</link><description>&lt;p&gt;
&#22312;&#32473;&#23450;&#20855;&#26377;&#24490;&#29615;&#30340;&#38750;&#24490;&#29615;&#25688;&#35201;&#22240;&#26524;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#29992;&#20110;&#38598;&#20307;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Root Cause Identification for Collective Anomalies in Time Series given an Acyclic Summary Causal Graph with Loops. (arXiv:2303.04038v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#21644;&#38750;&#24490;&#29615;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#38598;&#20307;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#23558;&#38382;&#39064;&#21010;&#20998;&#20026;&#29420;&#31435;&#23376;&#38382;&#39064;&#21644;&#27604;&#36739;&#30452;&#25509;&#25928;&#24212;&#31561;&#26041;&#24335;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#32473;&#23450;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#21644;&#19968;&#20010;&#25277;&#35937;&#34920;&#31034;&#27491;&#24120;&#29366;&#24577;&#19979;&#21160;&#24577;&#31995;&#32479;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#38750;&#24490;&#29615;&#25688;&#35201;&#22240;&#26524;&#22270;&#30340;&#38598;&#20307;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;d-&#20998;&#31163;&#23558;&#30456;&#20851;&#24322;&#24120;&#20998;&#32452;&#65292;&#23558;&#26681;&#26412;&#21407;&#22240;&#35782;&#21035;&#38382;&#39064;&#20998;&#20026;&#22810;&#20010;&#29420;&#31435;&#23376;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#22312;&#27492;&#35774;&#32622;&#19979;&#22914;&#20309;&#30452;&#25509;&#20174;&#22270;&#20013;&#21644;&#24322;&#24120;&#20986;&#29616;&#26102;&#38388;&#20013;&#25214;&#21040;&#19968;&#20123;&#26681;&#26412;&#21407;&#22240;&#12290;&#26368;&#21518;&#65292;&#23427;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#27604;&#36739;&#27491;&#24120;&#21644;&#24322;&#24120;&#29366;&#24577;&#19979;&#30340;&#30452;&#25509;&#24433;&#21709;&#26469;&#25214;&#21040;&#20854;&#20313;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#35782;&#21035;&#30452;&#25509;&#24433;&#21709;&#30340;&#35843;&#25972;&#38598;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach for identifying the root causes of collective anomalies given observational time series and an acyclic summary causal graph which depicts an abstraction of causal relations present in a dynamic system at its normal regime. The paper first shows how the problem of root cause identification can be divided into many independent subproblems by grouping related anomalies using d-separation. Further, it shows how, under this setting, some root causes can be found directly from the graph and from the time of appearance of anomalies. Finally, it shows, how the rest of the root causes can be found by comparing direct effects in the normal and in the anomalous regime. To this end, an adjustment set for identifying direct effects is introduced. Extensive experiments conducted on both simulated and real-world datasets demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29615;&#22659;&#36716;&#25442;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24207;&#21015;&#24314;&#27169;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#22686;&#21152;&#30340;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#29615;&#22659;&#21160;&#21147;&#23398;&#27169;&#22411;&#32047;&#31215;&#35823;&#24046;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2303.03811</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#29615;&#22659;&#36716;&#25442;&#22120;&#21644;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Environment Transformer and Policy Optimization for Model-Based Offline Reinforcement Learning. (arXiv:2303.03811v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03811
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29615;&#22659;&#36716;&#25442;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24207;&#21015;&#24314;&#27169;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#22686;&#21152;&#30340;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#29615;&#22659;&#21160;&#21147;&#23398;&#27169;&#22411;&#32047;&#31215;&#35823;&#24046;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#19982;&#23454;&#38469;&#29615;&#22659;&#20132;&#20114;&#20197;&#33719;&#21462;&#25968;&#25454;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#28040;&#38500;&#20102;&#19982;&#23454;&#38469;&#29615;&#22659;&#30340;&#20132;&#20114;&#35201;&#27714;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36716;&#25442;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#29983;&#25104;&#27169;&#25311;&#30340;&#22238;&#21512;&#20197;&#21152;&#36895;&#35757;&#32451;&#12290;&#20197;&#21069;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#27010;&#29575;&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26469;&#24314;&#27169;aleatoric&#19981;&#30830;&#23450;&#24615;&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#23548;&#33268;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#30340;&#25351;&#25968;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27169;&#25311;&#38271;&#26399;&#22238;&#21512;&#26102;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32047;&#31215;&#35823;&#24046;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29615;&#22659;&#36716;&#25442;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24207;&#21015;&#24314;&#27169;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interacting with the actual environment to acquire data is often costly and time-consuming in robotic tasks. Model-based offline reinforcement learning (RL) provides a feasible solution. On the one hand, it eliminates the requirements of interaction with the actual environment. On the other hand, it learns the transition dynamics and reward function from the offline datasets and generates simulated rollouts to accelerate training. Previous model-based offline RL methods adopt probabilistic ensemble neural networks (NN) to model aleatoric uncertainty and epistemic uncertainty. However, this results in an exponential increase in training time and computing resource requirements. Furthermore, these methods are easily disturbed by the accumulative errors of the environment dynamics models when simulating long-term rollouts. To solve the above problems, we propose an uncertainty-aware sequence modeling architecture called Environment Transformer. It models the probability distribution of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;&#24341;&#23548;&#26041;&#27861;&#65292;&#22312;&#38271;&#35270;&#39057;&#20013;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328; grounding &#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20462;&#21098;&#19981;&#21487;&#25551;&#36848;&#30340;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.13372</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#24341;&#23548;&#22312;&#38271;&#35270;&#39057;&#20013;&#23450;&#20301;&#26102;&#21051;
&lt;/p&gt;
&lt;p&gt;
Localizing Moments in Long Video Via Multimodal Guidance. (arXiv:2302.13372v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;&#24341;&#23548;&#26041;&#27861;&#65292;&#22312;&#38271;&#35270;&#39057;&#20013;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328; grounding &#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20462;&#21098;&#19981;&#21487;&#25551;&#36848;&#30340;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#35268;&#27169;&#12289;&#38271;&#26684;&#24335;&#30340;MAD&#21644;Ego4D&#25968;&#25454;&#38598;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#30740;&#31350;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057; grounding &#26041;&#27861;&#22312;&#38271;&#26684;&#24335;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#65292;&#26377;&#36259;&#30340;&#21457;&#29616;&#26159;&#65306;&#24403;&#21069;&#30340; grounding &#26041;&#27861;&#21333;&#29420;&#26080;&#27861;&#22788;&#29702;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#21644;&#35774;&#32622;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#38271;&#35270;&#39057;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#38271;&#35270;&#39057;&#33258;&#28982;&#35821;&#35328; grounding &#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20462;&#21098;&#19981;&#21487;&#25551;&#36848;&#30340;&#31383;&#21475;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24341;&#23548; grounding &#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#24341;&#23548;&#27169;&#22411;&#21644;&#19968;&#20010;&#22522;&#30784; grounding &#27169;&#22411;&#12290;&#24341;&#23548;&#27169;&#22411;&#24378;&#35843;&#21487;&#25551;&#36848;&#30340;&#31383;&#21475;&#65292;&#32780;&#22522;&#30784; grounding &#27169;&#22411;&#20998;&#26512;&#30701;&#26102;&#31383;&#21475;&#65292;&#30830;&#23450;&#21738;&#20123;&#29255;&#27573;&#19982;&#32473;&#23450;&#30340;&#35821;&#35328;&#26597;&#35810;&#20934;&#30830;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#24341;&#23548;&#27169;&#22411;&#30340;&#35774;&#35745;&#65306;Query-Agnostic &#21644; Query-Dependent&#65292;&#20197;&#24179;&#34913;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent introduction of the large-scale, long-form MAD and Ego4D datasets has enabled researchers to investigate the performance of current state-of-the-art methods for video grounding in the long-form setup, with interesting findings: current grounding methods alone fail at tackling this challenging task and setup due to their inability to process long video sequences. In this paper, we propose a method for improving the performance of natural language grounding in long videos by identifying and pruning out non-describable windows. We design a guided grounding framework consisting of a Guidance Model and a base grounding model. The Guidance Model emphasizes describable windows, while the base grounding model analyzes short temporal windows to determine which segments accurately match a given language query. We offer two designs for the Guidance Model: Query-Agnostic and Query-Dependent, which balance efficiency and accuracy. Experiments demonstrate that our proposed method outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11939</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451; LM &#36827;&#34892;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#21644;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#19982; NLP &#21644; CV &#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#39046;&#22495;&#37319;&#29992;&#32479;&#19968;&#27169;&#22411;&#21363;&#21487;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#20173;&#28982;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22914;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#38459;&#30861;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#22823;&#37327;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36991;&#20813;&#25913;&#21464;&#39044;&#35757;&#32451;&#35821;&#35328;&#25110;&#22270;&#20687;&#27169;&#22411;&#20013;&#27531;&#24046;&#22359;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#21521;&#20256;&#36882;&#23618;&#12290;&#36825;&#31181;&#27169;&#22411;&#34987;&#31216;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (FPT)&#65292;&#36890;&#36807;&#23545;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FPT &#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.02173</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning based Time Series Analysis with Frequency Transformation. (arXiv:2302.02173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02173
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#39057;&#29575;&#21464;&#25442;&#30340;&#20248;&#21183;&#65292;&#22914;&#39640;&#25928;&#24615;&#21644;&#20840;&#23616;&#35270;&#35282;&#65292;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#34987;&#36805;&#36895;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23637;&#31034;&#20102;&#39057;&#29575;&#21464;&#25442;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#21644;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#31995;&#32479;&#22238;&#39038;&#21644;&#28145;&#20837;&#20998;&#26512;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#39057;&#29575;&#21464;&#25442;&#21487;&#20197;&#25552;&#21319;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#20197;&#21450;&#23427;&#22312;&#35813;&#39046;&#22495;&#30340;&#38480;&#21046;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#31995;&#32479;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20027;&#35201;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, frequency transformation (FT) has been increasingly incorporated into deep learning models to significantly enhance state-of-the-art accuracy and efficiency in time series analysis. The advantages of FT, such as high efficiency and a global view, have been rapidly explored and exploited in various time series tasks and applications, demonstrating the promising potential of FT as a new deep learning paradigm for time series analysis. Despite the growing attention and the proliferation of research in this emerging field, there is currently a lack of a systematic review and in-depth analysis of deep learning-based time series models with FT. It is also unclear why FT can enhance time series analysis and what its limitations in the field are. To address these gaps, we present a comprehensive review that systematically investigates and summarizes the recent research advancements in deep learning-based time series analysis with FT. Specifically, we explore the primary approaches us
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21333;&#20809;&#23376;&#37327;&#23376;&#34892;&#36208;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#20351;&#29992;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#37327;&#23376;&#21270;&#20102;&#39033;&#30446;&#27169;&#25311;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#32463;&#20856;&#27169;&#22411;&#33021;&#21147;&#30340;&#37327;&#23376;&#24178;&#28041;&#65292;&#20026;&#35299;&#37322;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29616;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2301.13669</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#20809;&#23376;&#37327;&#23376;&#34892;&#36208;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards interpretable quantum machine learning via single-photon quantum walks. (arXiv:2301.13669v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13669
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21333;&#20809;&#23376;&#37327;&#23376;&#34892;&#36208;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#20351;&#29992;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#37327;&#23376;&#21270;&#20102;&#39033;&#30446;&#27169;&#25311;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#32463;&#20856;&#27169;&#22411;&#33021;&#21147;&#30340;&#37327;&#23376;&#24178;&#28041;&#65292;&#20026;&#35299;&#37322;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#29616;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#34987;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#21462;&#20195;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38480;&#21046;&#65292;&#21363;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#23376;&#39033;&#30446;&#27169;&#25311;&#65288;PS&#65289;&#30340;&#21464;&#20998;&#37327;&#23376;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#23454;&#29616;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;PS&#20013;&#65292;&#20915;&#31574;&#34987;&#24314;&#27169;&#20026;&#25551;&#36848;&#20195;&#29702;&#35760;&#24518;&#30340;&#22270;&#19978;&#30340;&#38543;&#26426;&#28459;&#27493;&#12290;&#20026;&#20102;&#23454;&#29616;&#37327;&#23376;&#21270;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36890;&#36807;&#21464;&#20998;&#31639;&#27861;&#35757;&#32451;&#30340;&#21487;&#35843;&#35856;Mach-Zehnder&#24178;&#28041;&#20202;&#26230;&#26684;&#20013;&#30340;&#21333;&#20809;&#23376;&#37327;&#23376;&#34892;&#36208;&#12290;&#36890;&#36807;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37327;&#23376;&#21270;&#30340;PS&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#37327;&#23376;&#24178;&#28041;&#26469;&#33719;&#24471;&#36229;&#36234;&#20854;&#32463;&#20856;&#23545;&#24212;&#27169;&#22411;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#37327;&#23376;&#24178;&#28041;&#22312;&#35757;&#32451;&#21644;&#36861;&#36394;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65292;&#20026;&#23454;&#29616;&#35299;&#37322;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational quantum algorithms represent a promising approach to quantum machine learning where classical neural networks are replaced by parametrized quantum circuits. However, both approaches suffer from a clear limitation, that is a lack of interpretability. Here, we present a variational method to quantize projective simulation (PS), a reinforcement learning model aimed at interpretable artificial intelligence. Decision making in PS is modeled as a random walk on a graph describing the agent's memory. To implement the quantized model, we consider quantum walks of single photons in a lattice of tunable Mach-Zehnder interferometers trained via variational algorithms. Using an example from transfer learning, we show that the quantized PS model can exploit quantum interference to acquire capabilities beyond those of its classical counterpart. Finally, we discuss the role of quantum interference for training and tracing the decision making process, paving the way for realizations of int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#26412;&#22320;&#21270;&#19982;&#32534;&#36753;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#23558;&#20107;&#23454;&#26412;&#22320;&#21270;&#21040;&#29305;&#23450;&#27169;&#22411;&#21442;&#25968;&#24182;&#19981;&#33021;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#12290;&#22240;&#26524;&#36861;&#36394;&#26041;&#27861;&#24182;&#19981;&#33021;&#25351;&#23548;&#32534;&#36753;&#21738;&#20010;&#27169;&#22411;&#23618;&#26469;&#35206;&#30422;&#23384;&#20648;&#30340;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2301.04213</link><description>&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#26412;&#22320;&#21270;&#21644;&#22522;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#24046;&#24322;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#26412;&#22320;&#21270;&#33021;&#22815;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models. (arXiv:2301.04213v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#26412;&#22320;&#21270;&#19982;&#32534;&#36753;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#23558;&#20107;&#23454;&#26412;&#22320;&#21270;&#21040;&#29305;&#23450;&#27169;&#22411;&#21442;&#25968;&#24182;&#19981;&#33021;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#12290;&#22240;&#26524;&#36861;&#36394;&#26041;&#27861;&#24182;&#19981;&#33021;&#25351;&#23548;&#32534;&#36753;&#21738;&#20010;&#27169;&#22411;&#23618;&#26469;&#35206;&#30422;&#23384;&#20648;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#22823;&#37327;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#36825;&#20123;&#20449;&#24687;&#23450;&#20301;&#21040;&#27169;&#22411;&#30340;&#29305;&#23450;&#26435;&#37325;&#65292;&#22914;&#20013;&#38388;&#23618;MLP&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#32534;&#36753;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#25152;&#24314;&#35758;&#30340;&#23384;&#20648;&#20107;&#23454;&#20301;&#32622;&#30340;&#26435;&#37325;&#65292;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#23384;&#20648;&#26041;&#24335;&#12290;&#36825;&#19968;&#21457;&#29616;&#20196;&#20154;&#24847;&#22806;&#65292;&#22240;&#20026;&#25105;&#20204;&#21407;&#26412;&#26399;&#26395;&#23558;&#20107;&#23454;&#26412;&#22320;&#21270;&#21040;&#29305;&#23450;&#30340;&#27169;&#22411;&#21442;&#25968;&#21487;&#20197;&#21578;&#35785;&#25105;&#20204;&#22312;&#27169;&#22411;&#20013;&#22914;&#20309;&#25805;&#32437;&#30693;&#35782;&#65292;&#36825;&#19968;&#20551;&#35774;&#26366;&#28608;&#21457;&#36807;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#34920;&#31034;&#21435;&#22122;&#65288;&#20063;&#31216;&#20026;&#22240;&#26524;&#36861;&#36394;&#65289;&#25152;&#24471;&#20986;&#30340;&#26412;&#22320;&#21270;&#32467;&#35770;&#24182;&#19981;&#33021;&#25552;&#20379;&#20219;&#20309;&#20851;&#20110;&#24212;&#35813;&#22312;&#21738;&#20010;&#27169;&#22411;MLP&#23618;&#36827;&#34892;&#32534;&#36753;&#20197;&#35206;&#30422;&#29616;&#26377;&#23384;&#20648;&#20107;&#23454;&#30340;&#26032;&#20107;&#23454;&#30340;&#35265;&#35299;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#36807;&#21435;&#30340;&#30740;&#31350;&#22914;&#20309;&#20381;&#36182;&#22240;&#26524;&#36861;&#36394;&#26469;&#36873;&#25321;&#38656;&#35201;&#32534;&#36753;&#30340;&#27169;&#22411;&#23618;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#32534;&#36753;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights. In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit. Next, we consider several variants of the editing pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#23558;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#65292;&#29992;&#20110;&#32531;&#35299;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.09598</link><description>&lt;p&gt;
&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#29992;&#20110;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Query-as-context Pre-training for Dense Passage Retrieval. (arXiv:2212.09598v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#23558;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#65292;&#29992;&#20110;&#32531;&#35299;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#30740;&#31350;&#20986;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#26469;&#25552;&#39640;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#31616;&#21333;&#22320;&#35748;&#20026;&#26469;&#33258;&#21516;&#19968;&#25991;&#26723;&#30340;&#20004;&#20010;&#36890;&#36947;&#26159;&#30456;&#20851;&#30340;&#65292;&#32780;&#19981;&#32771;&#34385;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#29992;&#20110;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#20551;&#23450;&#20174;&#36890;&#36947;&#20013;&#25552;&#21462;&#30340;&#26597;&#35810;&#26356;&#21487;&#33021;&#19982;&#35813;&#36890;&#36947;&#30456;&#20851;&#65292;&#24182;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#12290;&#36825;&#20123;&#36890;&#36947;-&#26597;&#35810;&#23545;&#28982;&#21518;&#29992;&#20110;&#23545;&#27604;&#24615;&#25110;&#29983;&#25104;&#24615;&#19978;&#19979;&#25991;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#36890;&#36947;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#21644;&#36328;&#39046;&#22495;&#38646;-shot&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#22686;&#30410;&#65292;&#21516;&#26102;&#21152;&#36895;&#20102;&#35757;&#32451;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#20250;&#22312;https://github.com/deepset-ai/haystack&#19978;&#25552;&#20379;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods have been developed to improve the performance of dense passage retrieval by using context-supervised pre-training. These methods simply consider two passages from the same document to be relevant, without taking into account the possibility of weakly correlated pairs. Thus, this paper proposes query-as-context pre-training, a simple yet effective pre-training technique to alleviate the issue. Query-as-context pre-training assumes that the query derived from a passage is more likely to be relevant to that passage and forms a passage-query pair. These passage-query pairs are then used in contrastive or generative context-supervised pre-training. The pre-trained models are evaluated on large-scale passage retrieval benchmarks and out-of-domain zero-shot benchmarks. Experimental results show that query-as-context pre-training brings considerable gains and meanwhile speeds up training, demonstrating its effectiveness and efficiency. Our code will be available at https://g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#21462;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#30340;&#31572;&#26696;&#27010;&#29575;&#65292;&#33021;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;</title><link>http://arxiv.org/abs/2212.05762</link><description>&lt;p&gt;
&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Momentum Contrastive Pre-training for Question Answering. (arXiv:2212.05762v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#21462;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#30340;&#31572;&#26696;&#27010;&#29575;&#65292;&#33021;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25277;&#21462;&#24335;&#38382;&#31572;&#65288;QA&#65289;&#39044;&#35757;&#32451;&#26041;&#27861;&#29983;&#25104;&#31867;&#20284;&#20110;&#22635;&#31354;&#39064;&#30340;&#26597;&#35810;&#65292;&#20854;&#35821;&#27861;&#32467;&#26500;&#19982;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19981;&#21516;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#31616;&#21333;&#30340;&#20851;&#38190;&#35789;&#21305;&#37197;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#37327;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;MCROSS&#65288;Momentum Contrastive pRe-training fOr queStion anSwering&#65289;&#29992;&#20110;&#25277;&#21462;&#24335;&#30340;&#38382;&#39064;&#22238;&#31572;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MCROSS&#24341;&#20837;&#20102;&#21160;&#37327;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#31572;&#26696;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;QA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#30417;&#30563;&#21644;&#38646;-shot&#22330;&#26223;&#19979;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing pre-training methods for extractive Question Answering (QA) generate cloze-like queries different from natural questions in syntax structure, which could overfit pre-trained models to simple keyword matching. In order to address this problem, we propose a novel Momentum Contrastive pRe-training fOr queStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS introduces a momentum contrastive learning framework to align the answer probability between cloze-like and natural query-passage sample pairs. Hence, the pre-trained models can better transfer the knowledge learned in cloze-like samples to answering natural questions. Experimental results on three benchmarking QA datasets show that our method achieves noticeable improvement compared with all baselines in both supervised and zero-shot scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#23384;&#22768;&#38899;&#29305;&#24449;&#30340;&#38646;&#26679;&#26412;&#22810;&#21475;&#38899;&#36716;&#25442;&#31995;&#32479;&#65292;&#33021;&#22815;&#23558;&#35828;&#35805;&#32773;&#30340;&#21475;&#38899;&#36716;&#25442;&#20026;&#20854;&#20182;&#21475;&#38899;&#32780;&#20445;&#30041;&#20854;&#21407;&#22987;&#22768;&#38899;&#29305;&#24449;&#12290;&#35813;&#31995;&#32479;&#22312;&#21475;&#38899;&#36716;&#25442;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.13282</link><description>&lt;p&gt;
&#20445;&#23384;&#22768;&#38899;&#30340;&#38646;&#26679;&#26412;&#22810;&#21475;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Voice-preserving Zero-shot Multiple Accent Conversion. (arXiv:2211.13282v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#23384;&#22768;&#38899;&#29305;&#24449;&#30340;&#38646;&#26679;&#26412;&#22810;&#21475;&#38899;&#36716;&#25442;&#31995;&#32479;&#65292;&#33021;&#22815;&#23558;&#35828;&#35805;&#32773;&#30340;&#21475;&#38899;&#36716;&#25442;&#20026;&#20854;&#20182;&#21475;&#38899;&#32780;&#20445;&#30041;&#20854;&#21407;&#22987;&#22768;&#38899;&#29305;&#24449;&#12290;&#35813;&#31995;&#32479;&#22312;&#21475;&#38899;&#36716;&#25442;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#23398;&#20064;&#22806;&#35821;&#30340;&#20154;&#20250;&#36935;&#21040;&#21548;&#19981;&#25026;&#25110;&#32773;&#26080;&#27861;&#19982;&#27597;&#35821;&#20026;&#26576;&#19968;&#21475;&#38899;&#30340;&#20154;&#36827;&#34892;&#23545;&#35805;&#30340;&#38590;&#39064;&#12290;&#23545;&#20110;&#27597;&#35821;&#20026;&#26576;&#19968;&#21475;&#38899;&#30340;&#20154;&#26469;&#35828;&#65292;&#29702;&#35299;&#25110;&#32773;&#20351;&#29992;&#26032;&#30340;&#21475;&#38899;&#21516;&#26679;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#19968;&#31181;&#33021;&#22815;&#25913;&#21464;&#35828;&#35805;&#32773;&#21475;&#38899;&#20294;&#20445;&#30041;&#35828;&#35805;&#32773;&#22768;&#38899;&#29305;&#24449;&#65288;&#22914;&#38899;&#33394;&#21644;&#38899;&#39640;&#65289;&#30340;&#21475;&#38899;&#36716;&#25442;&#31995;&#32479;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#20363;&#22914;&#20132;&#27969;&#12289;&#35821;&#35328;&#23398;&#20064;&#21644;&#23089;&#20048;&#12290;&#29616;&#26377;&#30340;&#21475;&#38899;&#36716;&#25442;&#27169;&#22411;&#24448;&#24448;&#21516;&#26102;&#25913;&#21464;&#35828;&#35805;&#32773;&#30340;&#36523;&#20221;&#21644;&#21475;&#38899;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#23398;&#20064;&#26469;&#35299;&#32806;&#21475;&#38899;&#30456;&#20851;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20182;&#22768;&#23398;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;&#29616;&#26377;&#30340;&#21475;&#38899;&#36716;&#25442;&#27169;&#22411;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#65292;&#23427;&#21487;&#20197;&#23558;&#26410;&#30693;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#36716;&#25442;&#20026;&#22810;&#20010;&#21475;&#38899;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#21407;&#22987;&#22768;&#38899;&#29305;&#24449;&#12290;&#20027;&#35266;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#38899;&#39057;&#26356;&#25509;&#36817;&#30446;&#26631;&#21475;&#38899;&#65292;&#19988;&#26356;&#20687;&#21407;&#22987;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most people who have tried to learn a foreign language would have experienced difficulties understanding or speaking with a native speaker's accent. For native speakers, understanding or speaking a new accent is likewise a difficult task. An accent conversion system that changes a speaker's accent but preserves that speaker's voice identity, such as timbre and pitch, has the potential for a range of applications, such as communication, language learning, and entertainment. Existing accent conversion models tend to change the speaker identity and accent at the same time. Here, we use adversarial learning to disentangle accent dependent features while retaining other acoustic characteristics. What sets our work apart from existing accent conversion models is the capability to convert an unseen speaker's utterance to multiple accents while preserving its original voice identity. Subjective evaluations show that our model generates audio that sound closer to the target accent and like the 
&lt;/p&gt;</description></item><item><title>Powderworld&#26159;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#29992;&#20110;&#25552;&#20379;&#27867;&#21270;&#24615;&#30340;&#30740;&#31350;&#24179;&#21488;&#65292;&#21253;&#25324;&#19990;&#30028;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.13051</link><description>&lt;p&gt;
Powderworld&#65306;&#36890;&#36807;&#22810;&#26679;&#21270;&#20219;&#21153;&#20998;&#24067;&#26469;&#29702;&#35299;&#27867;&#21270;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Powderworld: A Platform for Understanding Generalization via Rich Task Distributions. (arXiv:2211.13051v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13051
&lt;/p&gt;
&lt;p&gt;
Powderworld&#26159;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#29992;&#20110;&#25552;&#20379;&#27867;&#21270;&#24615;&#30340;&#30740;&#31350;&#24179;&#21488;&#65292;&#21253;&#25324;&#19990;&#30028;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27867;&#21270;&#20195;&#29702;&#38656;&#35201;&#19968;&#32452;&#20016;&#23500;&#12289;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#36825;&#20123;&#20219;&#21153;&#35774;&#35745;&#19968;&#20010;&#29702;&#24819;&#30340;&#29615;&#22659;&#24456;&#22256;&#38590;&#8212;&#8212;&#29702;&#24819;&#30340;&#29615;&#22659;&#24212;&#25903;&#25345;&#19968;&#31995;&#21015;&#26032;&#20852;&#29616;&#35937;&#12289;&#20016;&#23500;&#30340;&#20219;&#21153;&#31354;&#38388;&#21644;&#24555;&#36895;&#30340;&#36816;&#34892;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Powderworld&#65292;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#12290;&#22312;Powderworld&#20869;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#28608;&#21457;&#25361;&#25112;&#30340;&#20998;&#24067;&#65292;&#19968;&#20010;&#29992;&#20110;&#19990;&#30028;&#24314;&#27169;&#65292;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;&#27599;&#20010;&#20998;&#24067;&#37117;&#21253;&#21547;&#25163;&#21160;&#35774;&#35745;&#30340;&#27979;&#35797;&#20219;&#21153;&#65292;&#20197;&#26816;&#26597;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#21487;&#33021;&#20250;&#25233;&#21046;&#39640;&#26041;&#24046;&#29615;&#22659;&#19979;&#30340;&#23398;&#20064;&#12290;Powderworld&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25903;&#25345;&#27867;&#21270;&#30740;&#31350;&#30340;&#29615;&#22659;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment' for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating challenges distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment's complexity improves generalization for world models and certain reinforcement learning agents, yet may inhibit learning in high-variance environments. Powderworld aims to support the study of generalization by providing a so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#23398;&#35282;&#24230;&#21644;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.03536</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65306;&#22522;&#20110;&#34920;&#31034;&#31354;&#38388;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces. (arXiv:2211.03536v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#23398;&#35282;&#24230;&#21644;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#20026;&#20302;&#32500;&#35821;&#20041;&#31354;&#38388;&#65292;&#29992;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22914;&#38142;&#25509;&#39044;&#27979;&#65292;&#30693;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#34917;&#20840;&#12290;&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KGE&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22522;&#20110;&#34920;&#31034;&#31354;&#38388;&#30340;&#19977;&#20010;&#25968;&#23398;&#35282;&#24230;&#65288;&#20195;&#25968;&#35282;&#24230;&#12289;&#20960;&#20309;&#35282;&#24230;&#21644;&#20998;&#26512;&#35282;&#24230;&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#20171;&#32461;&#20102;&#22522;&#26412;&#25968;&#23398;&#31354;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#65292;&#28982;&#21518;&#28145;&#20837;&#30740;&#31350;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#25968;&#23398;&#29305;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#19977;&#20010;&#31867;&#21035;&#20013;&#30340;&#19981;&#21516;KGE&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#31354;&#38388;&#20248;&#21183;&#22312;&#19981;&#21516;&#23884;&#20837;&#38656;&#27714;&#19978;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#25972;&#29702;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;KGE&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this paper, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) Algebraic perspective, (2) Geometric perspective, and (3) Analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#20840;&#29699;&#28779;&#28798;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20998;&#21106;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#65292;&#20026;&#21322;&#23395;&#24230;&#23610;&#24230;&#25552;&#20379;&#20102;&#39640;&#25928;&#20934;&#30830;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.00534</link><description>&lt;p&gt;
&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Global Wildfire Forecasting. (arXiv:2211.00534v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#20840;&#29699;&#28779;&#28798;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20998;&#21106;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#65292;&#20026;&#21322;&#23395;&#24230;&#23610;&#24230;&#25552;&#20379;&#20102;&#39640;&#25928;&#20934;&#30830;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35745;&#27668;&#20505;&#21464;&#21270;&#23558;&#36890;&#36807;&#21152;&#21095;&#28779;&#28798;&#22825;&#27668;&#24773;&#20917;&#32780;&#21152;&#21095;&#37326;&#28779;&#27963;&#21160;&#12290;&#25913;&#21892;&#25105;&#20204;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#39044;&#27979;&#37326;&#28779;&#30340;&#33021;&#21147;&#23545;&#20110;&#20943;&#36731;&#20854;&#36127;&#38754;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20840;&#29699;&#28779;&#28798;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20998;&#21106;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23637;&#31034;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#30340;&#21407;&#22411;&#65292;&#23454;&#29616;&#20102;&#21322;&#23395;&#24230;&#23610;&#24230;&#19978;&#30340;&#39044;&#27979;&#12290;&#23588;&#20854;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#33719;&#21462;&#30340;&#20840;&#29699;&#20998;&#26512;&#23601;&#32490;&#25968;&#25454;&#31435;&#26041;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#19982;&#23395;&#33410;&#24615;&#21644;&#21322;&#23395;&#24230;&#24615;&#28779;&#28798;&#39537;&#21160;&#22240;&#32032;&#65288;&#27668;&#20505;&#12289;&#26893;&#34987;&#12289;&#28023;&#27915;&#25351;&#25968;&#12289;&#19982;&#20154;&#30456;&#20851;&#30340;&#21464;&#37327;&#65289;&#20197;&#21450;2001-2021&#24180;&#30340;&#21382;&#21490;&#28903;&#27585;&#38754;&#31215;&#21644;&#37326;&#28779;&#25490;&#25918;&#30456;&#20851;&#30340;&#21508;&#31181;&#21464;&#37327;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#35270;&#20026;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#28903;&#27585;&#21306;&#22495;&#20986;&#29616;&#30340;&#21069;8&#22825;&#12289;16&#22825;&#12289;32&#22825;&#21644;64&#22825;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change is expected to aggravate wildfire activity through the exacerbation of fire weather. Improving our capabilities to anticipate wildfires on a global scale is of uttermost importance for mitigating their negative effects. In this work, we create a global fire dataset and demonstrate a prototype for predicting the presence of global burned areas on a sub-seasonal scale with the use of segmentation deep learning models. Particularly, we present an open-access global analysis-ready datacube, which contains a variety of variables related to the seasonal and sub-seasonal fire drivers (climate, vegetation, oceanic indices, human-related variables), as well as the historical burned areas and wildfire emissions for 2001-2021. We train a deep learning model, which treats global wildfire forecasting as an image segmentation task and skillfully predicts the presence of burned areas 8, 16, 32 and 64 days ahead of time. Our work motivates the use of deep learning for global burned area
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;(HI$^2$)&#29992;&#20110;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#65292;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.05521</link><description>&lt;p&gt;
&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#31264;&#23494;&#26816;&#32034;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval. (arXiv:2210.05521v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;(HI$^2$)&#29992;&#20110;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#65292;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20498;&#25490;&#25991;&#20214;&#32467;&#26500;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#30340;&#25216;&#26415;&#12290;&#23427;&#26681;&#25454;&#23884;&#20837;&#23558;&#25991;&#26723;&#32858;&#31867;&#65307;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#26597;&#35810;&#25506;&#27979;&#38468;&#36817;&#30340;&#32858;&#31867;&#65292;&#24182;&#19988;&#20165;&#23545;&#20854;&#20013;&#30340;&#25991;&#26723;&#36827;&#34892;&#21518;&#32493;&#30340;&#35299;&#30721;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#31351;&#20030;&#36941;&#21382;&#30340;&#26114;&#36149;&#20195;&#20215;&#12290;&#28982;&#32780;&#65292;&#32858;&#31867;&#36807;&#31243;&#24635;&#26159;&#26377;&#25439;&#30340;&#65292;&#36825;&#23548;&#33268;&#25506;&#27979;&#21040;&#30340;&#32858;&#31867;&#20013;&#32570;&#22833;&#20102;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26816;&#32034;&#36136;&#37327;&#12290;&#30456;&#21453;&#65292;&#35789;&#27719;&#21305;&#37197;&#65292;&#22914;&#26174;&#33879;&#35789;&#27719;&#30340;&#37325;&#21472;&#65292;&#26356;&#23481;&#26131;&#35782;&#21035;&#30456;&#20851;&#25991;&#26723;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341; (HI$^2$)&#65292;&#20854;&#20013;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#20849;&#21516;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#12290;&#20026;&#20102;&#20860;&#39038;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32858;&#31867;&#36873;&#25321;&#22120;&#21644;&#19968;&#20010;&#35789;&#27719;&#36873;&#25321;&#22120;&#65292;&#29992;&#20110;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#24555;&#36895;&#25628;&#32034;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#31639;&#27861;&#21644;&#31471;&#21040;&#31471;&#23398;&#20064;&#26469;&#25552;&#39640;&#32034;&#24341;&#36136;&#37327;.
&lt;/p&gt;
&lt;p&gt;
Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost of exhaustive traversal. However, the clustering is always lossy, which results in the miss of relevant documents in the probed clusters and hence degrades retrieval quality. In contrast, lexical matching, such as overlaps of salient terms, tends to be strong feature for identifying relevant documents. In this work, we present the Hybrid Inverted Index (HI$^2$), where the embedding clusters and salient terms work collaboratively to accelerate dense retrieval. To make best of both effectiveness and efficiency, we devise a cluster selector and a term selector, to construct compact inverted lists and efficiently searching through them. Moreover, we leverage simple unsupervised algorithms as well as end-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#33324;&#29702;&#35770;&#26469;&#38480;&#23450;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05015</link><description>&lt;p&gt;
&#31890;&#23376;&#20449;&#24565;&#36817;&#20284;POMDP&#30340;&#26368;&#20248;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimality Guarantees for Particle Belief Approximation of POMDPs. (arXiv:2210.05015v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#33324;&#29702;&#35770;&#26469;&#38480;&#23450;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#25552;&#20379;&#20102;&#29616;&#23454;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#28789;&#27963;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;POMDP&#30340;&#27714;&#35299;&#38750;&#24120;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#24403;&#29366;&#24577;&#21644;&#35266;&#27979;&#31354;&#38388;&#26159;&#36830;&#32493;&#25110;&#28151;&#21512;&#30340;&#26102;&#20505;&#65292;&#36825;&#22312;&#29289;&#29702;&#31995;&#32479;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#23613;&#31649;&#26368;&#36817;&#20351;&#29992;&#35266;&#27979;&#20284;&#28982;&#26435;&#37325;&#31574;&#21010;&#30340;&#22312;&#32447;&#37319;&#26679;POMDP&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#23454;&#29992;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20808;&#21069;&#24182;&#27809;&#26377;&#25552;&#20986;&#19968;&#33324;&#29702;&#35770;&#26469;&#21051;&#30011;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#31890;&#23376;&#28388;&#27874;&#25216;&#26415;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#38480;&#23450;&#20219;&#20309;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#36825;&#31181;PB-MDP&#21644;POMDP&#20043;&#38388;&#30340;&#22522;&#30784;&#26725;&#26753;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35299;&#20915;&#30456;&#24212;&#30340;&#31890;&#23376;&#20449;&#24565;MDP&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#23558;MDP&#31639;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#25193;&#23637;&#21040;POMDP&#20013;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#22312;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDPs) provide a flexible representation for real-world decision and control problems. However, POMDPs are notoriously difficult to solve, especially when the state and observation spaces are continuous or hybrid, which is often the case for physical systems. While recent online sampling-based POMDP algorithms that plan with observation likelihood weighting have shown practical effectiveness, a general theory characterizing the approximation error of the particle filtering techniques that these algorithms use has not previously been proposed. Our main contribution is bounding the error between any POMDP and its corresponding finite sample particle belief MDP (PB-MDP) approximation. This fundamental bridge between PB-MDPs and POMDPs allows us to adapt any sampling-based MDP algorithm to a POMDP by solving the corresponding particle belief MDP, thereby extending the convergence guarantees of the MDP algorithm to the POMDP. Practically, thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36827;&#21270;&#35745;&#31639;&#19982;&#31934;&#30830;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;0-1&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#26377;&#21069;&#36884;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2210.03918</link><description>&lt;p&gt;
&#23547;&#25214;&#21644;&#25506;&#32034;0-1&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Finding and Exploring Promising Search Space for the 0-1 Multidimensional Knapsack Problem. (arXiv:2210.03918v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36827;&#21270;&#35745;&#31639;&#19982;&#31934;&#30830;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;0-1&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#26377;&#21069;&#36884;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
0-1&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;NP&#22256;&#38590;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#24037;&#31243;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23558;&#36827;&#21270;&#35745;&#31639;&#19982;&#31934;&#30830;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;0-1&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#12290;&#23427;&#32500;&#25252;&#19968;&#32452;&#35299;&#65292;&#24182;&#21033;&#29992;&#26469;&#33258;&#32676;&#20307;&#30340;&#20449;&#24687;&#25552;&#21462;&#33391;&#22909;&#30340;&#37096;&#20998;&#20998;&#37197;&#12290;&#20026;&#20102;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#65292;&#24212;&#29992;&#31934;&#30830;&#31639;&#27861;&#26469;&#25506;&#32034;&#30001;&#33391;&#22909;&#37096;&#20998;&#20998;&#37197;&#25351;&#23450;&#30340;&#26377;&#21069;&#36884;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#26032;&#30340;&#35299;&#29992;&#20110;&#26356;&#26032;&#32676;&#20307;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#32676;&#20307;&#30340;&#25913;&#36827;&#65292;&#33391;&#22909;&#30340;&#37096;&#20998;&#20998;&#37197;&#26397;&#30528;&#26356;&#22909;&#30340;&#26041;&#21521;&#28436;&#21270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;TPTEA&#21644;DQPSO&#12290;&#23427;&#25214;&#21040;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#35299;&#65292;&#24182;&#20026;8&#20010;&#22823;&#35268;&#27169;&#21644;&#22256;&#38590;&#30340;&#23454;&#20363;&#25552;&#20379;&#20102;&#26032;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 0-1 Multidimensional Knapsack Problem (MKP) is a classical NP-hard combinatorial optimization problem with many engineering applications. In this paper, we propose a novel algorithm combining evolutionary computation with exact algorithm to solve the 0-1 MKP. It maintains a set of solutions and utilizes the information from the population to extract good partial assignments. To find high-quality solutions, an exact algorithm is applied to explore the promising search space specified by the good partial assignments. The new solutions are used to update the population. Thus, the good partial assignments evolve towards a better direction with the improvement of the population. Extensive experimentation with commonly used benchmark sets shows that our algorithm outperforms the state of the art heuristic algorithms, TPTEA and DQPSO. It finds better solutions than the existing algorithms and provides new lower bounds for 8 large and hard instances.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#65292;&#22312;&#22686;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.03029</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#22686;&#24378;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt. (arXiv:2210.03029v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03029
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#65292;&#22312;&#22686;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#25928;&#29575;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#35201;&#20040;&#36890;&#36807;&#25193;&#23637;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24635;&#25968;&#65292;&#35201;&#20040;&#22686;&#21152;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24494;&#35843;&#33719;&#21462;&#36719;&#25552;&#31034;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#20219;&#21153;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24494;&#35843;&#20026;&#27599;&#20010;&#25552;&#31034;&#35757;&#32451;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#23384;&#20648;&#19982;&#25552;&#31034;&#23884;&#20837;&#26144;&#23556;&#30340;&#35757;&#32451;&#23454;&#20363;&#26679;&#26412;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26816;&#32034;&#26368;&#25509;&#36817;&#26597;&#35810;&#23454;&#20363;&#30340;&#35757;&#32451;&#23454;&#20363;&#23545;&#24212;&#30340;&#25552;&#31034;&#23884;&#20837;&#12290;&#34429;&#28982;&#21482;&#22686;&#21152;&#20102;0.007%&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#26816;&#32034;&#36719;&#25552;&#31034;&#25552;&#39640;&#20102;T0&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;11&#20010;&#25968;&#25454;&#38598;&#20013;&#26377;10&#20010;&#34920;&#29616;&#20248;&#20110;T0&#65292;&#24182;&#19988;&#23558;T0&#22312;BIG-bench&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2.39&#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#20010;&#26377;&#24847;&#24605;&#30340;&#21457;&#29616;&#65292;&#21363;&#26816;&#32034;&#22312;&#30456;&#20284;&#31572;&#26696;&#36873;&#25321;&#26684;&#24335;&#19978;&#35757;&#32451;&#30340;&#28304;&#23884;&#20837;&#27604;&#25552;&#31034;&#23884;&#20837;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancing the zero-shot performance of instruction-following models requires heavy computation, either by scaling the total number of training datasets or the model size. In this work, we explore how retrieval of soft prompts obtained through prompt tuning can efficiently assist hard prompts in zero-shot task generalization. Specifically, we train soft prompt embeddings for each prompt through prompt tuning, store the samples of the training instances mapped with the prompt embeddings, and retrieve the corresponding prompt embedding of the training instance closest to the query instance during inference. While only adding 0.007% additional parameters, retrieval of soft prompt enhances the performance of T0 on unseen tasks by outperforming it on 10 out of 11 datasets as well as improving the mean accuracy of T0 on BIG-bench benchmark by 2.39% points. Also, we report an interesting finding that retrieving source embeddings trained on similar answer choice formats is more important than t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.15240</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Prompt&#35843;&#25972;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#24341;&#36215;&#20102;&#30740;&#31350;&#28909;&#28526;&#12290;&#19982;&#35821;&#35328;&#39046;&#22495;&#37319;&#29992;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#31574;&#30053;&#19981;&#21516;&#65292;&#22270;&#24418;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;&#22522;&#20110;Prompt&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35843;&#25972;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature (GPF) &#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#19979;&#30340;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;GPF&#22312;&#36755;&#20837;&#22270;&#24418;&#30340;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#29702;&#35770;&#19978;&#21487;&#23454;&#29616;&#19982;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#20877;&#38656;&#35201;&#26126;&#30830;&#35828;&#26126;&#27599;&#20010;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#24212;&#30340;Prompt&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#37319;&#29992;GPF&#26469;&#23454;&#29616;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#26469;&#26356;&#31995;&#32479;&#22320;&#35780;&#20272;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#20013;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.07881</link><description>&lt;p&gt;
&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#35859;&#35789;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Robustness of Signal Temporal Logic Predicates. (arXiv:2209.07881v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#26469;&#26356;&#31995;&#32479;&#22320;&#35780;&#20272;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#20013;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#30340;&#40065;&#26834;&#24615;&#19981;&#20165;&#35780;&#20272;&#20102;&#19968;&#20010;&#20449;&#21495;&#26159;&#21542;&#31526;&#21512;&#35268;&#33539;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#34913;&#37327;&#20844;&#24335;&#34987;&#28385;&#36275;&#25110;&#36829;&#21453;&#30340;&#31243;&#24230;&#30340;&#25351;&#26631;&#12290;&#40065;&#26834;&#24615;&#30340;&#35745;&#31639;&#22522;&#20110;&#23545;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#20197;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#26041;&#24335;&#23450;&#20041;&#65292;&#21363;&#19981;&#21253;&#25324;&#31995;&#32479;&#21160;&#24577;&#12290;&#32780;&#19988;&#65292;&#31934;&#30830;&#23450;&#20041;&#22797;&#26434;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32771;&#34385;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#31995;&#32479;&#30340;&#35780;&#20272;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26469;&#23398;&#20064;&#22522;&#20110;&#39044;&#20808;&#35745;&#31639;&#30340;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#32447;&#39640;&#25928;&#22320;&#35745;&#31639;&#40065;&#26834;&#24615;&#20540;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22312;&#35760;&#24405;&#30340;&#25968;&#25454;&#19978;&#20351;&#29992;&#22312;&#24418;&#24335;&#21270;&#20132;&#36890;&#35268;&#21017;&#20013;&#20351;&#29992;&#30340;&#35859;&#35789;&#30340;&#33258;&#21160;&#39550;&#39542;&#29992;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of signal temporal logic not only assesses whether a signal adheres to a specification but also provides a measure of how much a formula is fulfilled or violated. The calculation of robustness is based on evaluating the robustness of underlying predicates. However, the robustness of predicates is usually defined in a model-free way, i.e., without including the system dynamics. Moreover, it is often nontrivial to define the robustness of complicated predicates precisely. To address these issues, we propose a notion of model predictive robustness, which provides a more systematic way of evaluating robustness compared to previous approaches by considering model-based predictions. In particular, we use Gaussian process regression to learn the robustness based on precomputed predictions so that robustness values can be efficiently computed online. We evaluate our approach for the use case of autonomous driving with predicates used in formalized traffic rules on a recorded dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;Twitter&#25991;&#26412;&#20013;&#20998;&#31867;&#21069;&#25552;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RoBERTa&#27169;&#22411;&#22312;&#21069;&#25552;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.03851</link><description>&lt;p&gt;
5q032e@SMM4H'22: &#22522;&#20110;Transformer&#30340;COVID-19&#30456;&#20851;&#25512;&#25991;&#21069;&#25552;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
5q032e@SMM4H'22: Transformer-based classification of premise in tweets related to COVID-19. (arXiv:2209.03851v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;Twitter&#25991;&#26412;&#20013;&#20998;&#31867;&#21069;&#25552;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RoBERTa&#27169;&#22411;&#22312;&#21069;&#25552;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32463;&#20856;&#25361;&#25112;&#20043;&#19968;&#12290;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#20174;&#20844;&#20849;&#20449;&#24687;&#20013;&#25366;&#25496;&#20154;&#20204;&#30340;&#31435;&#22330;&#23545;&#20110;&#29702;&#35299;&#23545;&#20581;&#24247;&#21629;&#20196;&#30340;&#24577;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#25512;&#29305;&#25991;&#26412;&#20013;&#21069;&#25552;&#30340;&#20998;&#31867;&#12290;&#26412;&#24037;&#20316;&#26159;&#20316;&#20026;2022&#24180;Social Media Mining for Health (SMM4H)&#30740;&#35752;&#20250;&#30340;&#19968;&#37096;&#20998;&#23436;&#25104;&#30340;&#12290;&#25105;&#20204;&#22312;&#26500;&#24314;&#39640;&#25928;&#25429;&#25417;&#25512;&#25991;&#35821;&#20041;&#30340;&#27969;&#31243;&#26102;&#65292;&#25506;&#32034;&#20102;&#29616;&#20195;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#19968;&#20010;&#25512;&#29305;&#25968;&#25454;&#38598;&#19978;&#34920;&#26126;&#65292;&#22312;&#21069;&#25552;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;RoBERTa&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;ROC AUC&#20540;&#20026;0.807&#65292;F1&#20998;&#25968;&#20026;0.7648&#26041;&#38754;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automation of social network data assessment is one of the classic challenges of natural language processing. During the COVID-19 pandemic, mining people's stances from public messages have become crucial regarding understanding attitudes towards health orders. In this paper, the authors propose the predictive model based on transformer architecture to classify the presence of premise in Twitter texts. This work is completed as part of the Social Media Mining for Health (SMM4H) Workshop 2022. We explored modern transformer-based classifiers in order to construct the pipeline efficiently capturing tweets semantics. Our experiments on a Twitter dataset showed that RoBERTa is superior to the other transformer models in the case of the premise prediction task. The model achieved competitive performance with respect to ROC AUC value 0.807, and 0.7648 for the F1 score.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#21644;&#36716;&#31227;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25104;&#21151;&#30340;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;SNNs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#24182;&#19988;&#38750;SNN&#26550;&#26500;&#21019;&#24314;&#30340;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#19981;&#34987;SNNs&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2209.03358</link><description>&lt;p&gt;
&#25915;&#20987;&#33033;&#20914;&#65306;&#20851;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36716;&#31227;&#24615;&#19982;&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attacking the Spike: On the Transferability and Security of Spiking Neural Networks to Adversarial Examples. (arXiv:2209.03358v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#21644;&#36716;&#31227;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25104;&#21151;&#30340;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;SNNs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#24182;&#19988;&#38750;SNN&#26550;&#26500;&#21019;&#24314;&#30340;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#19981;&#34987;SNNs&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#22240;&#20854;&#39640;&#33021;&#25928;&#21644;&#26368;&#36817;&#22312;&#20998;&#31867;&#24615;&#33021;&#19978;&#30340;&#36827;&#23637;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#23545;SNNs&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#21644;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#19981;&#23436;&#21892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;&#25512;&#36827;SNNs&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#65292;&#24182;&#20570;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25104;&#21151;&#30340;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;SNNs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#24213;&#23618;&#30340;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#21363;&#20351;&#22312;&#23545;&#25239;&#24615;&#35757;&#32451;SNNs&#30340;&#24773;&#20917;&#19979;&#20063;&#19968;&#26679;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#26368;&#20339;&#30340;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23545;&#25239;&#25915;&#20987;&#22312;SNNs&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#22914;Vision Transformers(ViTs)&#21644;Big Transfer Convolutional Neural Networks(CNNs)&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#38750;SNN&#26550;&#26500;&#21019;&#24314;&#30340;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#19981;&#34987;SNNs&#35823;&#20998;&#31867;&#12290;&#31532;&#19977;&#65292;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#20849;&#24615;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have attracted much attention for their high energy efficiency and for recent advances in their classification performance. However, unlike traditional deep learning approaches, the analysis and study of the robustness of SNNs to adversarial examples remain relatively underdeveloped. In this work, we focus on advancing the adversarial attack side of SNNs and make three major contributions. First, we show that successful white-box adversarial attacks on SNNs are highly dependent on the underlying surrogate gradient technique, even in the case of adversarially trained SNNs. Second, using the best surrogate gradient technique, we analyze the transferability of adversarial attacks on SNNs and other state-of-the-art architectures like Vision Transformers (ViTs) and Big Transfer Convolutional Neural Networks (CNNs). We demonstrate that the adversarial examples created by non-SNN architectures are not misclassified often by SNNs. Third, due to the lack of an ubi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30333;&#30418;&#25915;&#20987;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20174;&#32780;&#35782;&#21035;&#20854;&#28431;&#27934;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2209.02167</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
White-Box Adversarial Policies in Deep Reinforcement Learning. (arXiv:2209.02167v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30333;&#30418;&#25915;&#20987;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20174;&#32780;&#35782;&#21035;&#20854;&#28431;&#27934;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#25239;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#23545;&#25239;&#20195;&#29702;&#26469;&#26368;&#23567;&#21270;&#30446;&#26631;&#20195;&#29702;&#30340;&#22870;&#21169;&#26469;&#24320;&#21457;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#30740;&#31350;&#20102;&#40657;&#30418;&#29256;&#26412;&#30340;&#36825;&#20123;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#20165;&#35266;&#23519;&#19990;&#30028;&#29366;&#24577;&#65292;&#24182;&#23558;&#30446;&#26631;&#20195;&#29702;&#35270;&#20026;&#29615;&#22659;&#30340;&#20219;&#20309;&#20854;&#20182;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36825;&#24182;&#27809;&#26377;&#32771;&#34385;&#38382;&#39064;&#20013;&#30340;&#38468;&#21152;&#32467;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30333;&#30418;&#25915;&#20987;&#30340;&#25991;&#29486;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#20197;&#35757;&#32451;&#26356;&#26377;&#25928;&#30340;&#23545;&#25239;&#31574;&#30053;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#65292;&#24182;&#26174;&#31034;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#20854;&#28431;&#27934;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;(1)&#25105;&#20204;&#20171;&#32461;&#20102;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#35266;&#23519;&#30446;&#26631;&#30340;&#20869;&#37096;&#29366;&#24577;&#21644;&#19990;&#30028;&#29366;&#24577;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#20351;&#29992;&#36825;&#20123;&#31574;&#30053;&#25915;&#20987;2&#20154;&#28216;&#25103;&#21644;&#29983;&#25104;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;(2)&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#40657;&#30418;&#25915;&#20987;&#30456;&#27604;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#24403;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#27604;&#36739;&#22797;&#26434;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), adversarial policies can be developed by training an adversarial agent to minimize a target agent's rewards. Prior work has studied black-box versions of these attacks where the adversary only observes the world state and treats the target agent as any other part of the environment. However, this does not take into account additional structure in the problem. In this work, we take inspiration from the literature on white-box attacks to train more effective adversarial policies. We study white-box adversarial policies and show that having access to a target agent's internal state can be useful for identifying its vulnerabilities. We make two contributions. (1) We introduce white-box adversarial policies where an attacker observes both a target's internal state and the world state at each timestep. We formulate ways of using these policies to attack agents in 2-player games and text-generating language models. (2) We demonstrate that these policies can ach
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;APEL&#26694;&#26550;&#65292;&#38750;&#31243;&#24207;&#21592;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#38388;&#25509;&#36873;&#25321;&#22797;&#26434;&#31243;&#24207;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#37325;&#26032;&#27880;&#37322;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#26102;&#36798;&#21040;&#20102;&#19982;&#19987;&#23478;&#30456;&#21516;&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#30340;&#32454;&#24494;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2205.12422</link><description>&lt;p&gt;
&#38388;&#25509;&#36890;&#36807;&#20027;&#21160;&#31034;&#20363;&#20026;&#38750;&#31243;&#24207;&#21592;&#28155;&#21152;&#26631;&#31614;&#31243;&#24207;&#65306;&#20197;Text-to-SQL&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Labeling Programs with Non-Programmers Indirectly via Active Examples: A Case Study with Text-to-SQL. (arXiv:2205.12422v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12422
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;APEL&#26694;&#26550;&#65292;&#38750;&#31243;&#24207;&#21592;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#38388;&#25509;&#36873;&#25321;&#22797;&#26434;&#31243;&#24207;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#37325;&#26032;&#27880;&#37322;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#26102;&#36798;&#21040;&#20102;&#19982;&#19987;&#23478;&#30456;&#21516;&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#30340;&#32454;&#24494;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#31243;&#24207;&#21592;&#33021;&#21542;&#20351;&#29992;&#22797;&#26434;&#31243;&#24207;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#27880;&#37322;&#20197;&#34920;&#31034;&#20854;&#21547;&#20041;&#65311;&#25105;&#20204;&#20171;&#32461;&#20102;APEL&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#38750;&#31243;&#24207;&#21592;&#20174;&#30001;&#31181;&#23376;&#35821;&#20041;&#35299;&#26512;&#22120;&#65288;&#20363;&#22914;Codex&#65289;&#29983;&#25104;&#30340;&#20505;&#36873;&#31243;&#24207;&#20013;&#36873;&#25321;&#12290;&#30001;&#20110;&#20182;&#20204;&#26080;&#27861;&#29702;&#35299;&#20505;&#36873;&#31243;&#24207;&#65292;&#25105;&#20204;&#35201;&#27714;&#20182;&#20204;&#36890;&#36807;&#26816;&#26597;&#31243;&#24207;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#26469;&#38388;&#25509;&#36873;&#25321;&#12290;&#23545;&#20110;&#27599;&#20010;&#34920;&#36798;&#24335;&#65292;APEL&#20027;&#21160;&#25628;&#32034;&#19968;&#20010;&#31616;&#21333;&#30340;&#36755;&#20837;&#65292;&#20505;&#36873;&#31243;&#24207;&#22312;&#36825;&#20010;&#36755;&#20837;&#19978;&#26356;&#20542;&#21521;&#20110;&#20135;&#29983;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#23427;&#21482;&#35201;&#27714;&#38750;&#31243;&#24207;&#21592;&#36873;&#25321;&#36866;&#24403;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#25512;&#26029;&#20986;&#21738;&#20010;&#31243;&#24207;&#26159;&#27491;&#30830;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#35843;&#20248;&#35299;&#26512;&#22120;&#12290;&#20316;&#20026;&#31532;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#25307;&#21215;&#20102;&#20154;&#31867;&#38750;&#31243;&#24207;&#21592;&#20351;&#29992;APEL&#37325;&#26032;&#27880;&#37322;SPIDER&#65292;&#19968;&#20010;&#25991;&#26412;&#21040;SQL&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#19987;&#23478;&#27880;&#37322;&#32773;&#30456;&#21516;&#30340;&#27880;&#37322;&#20934;&#30830;&#24230;&#65288;75%&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#35768;&#22810;&#24494;&#23567;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can non-programmers annotate natural language utterances with complex programs that represent their meaning? We introduce APEL, a framework in which non-programmers select among candidate programs generated by a seed semantic parser (e.g., Codex). Since they cannot understand the candidate programs, we ask them to select indirectly by examining the programs' input-ouput examples. For each utterance, APEL actively searches for a simple input on which the candidate programs tend to produce different outputs. It then asks the non-programmers only to choose the appropriate output, thus allowing us to infer which program is correct and could be used to fine-tune the parser. As a first case study, we recruited human non-programmers to use APEL to re-annotate SPIDER, a text-to-SQL dataset. Our approach achieved the same annotation accuracy as the original expert annotators (75%) and exposed many subtle errors in the original annotations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ZETAR&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#20869;&#37096;&#20154;&#21592;&#30340;&#28608;&#21169;&#65292;&#24182;&#35774;&#35745;&#23450;&#21046;&#30340;&#25512;&#33616;&#25919;&#31574;&#26469;&#25552;&#39640;&#21512;&#35268;&#24615;&#12290;&#36890;&#36807;&#21407;&#22987;&#21644;&#23545;&#20598;&#20984;&#35268;&#21010;&#65292;&#35745;&#31639;&#20986;&#26368;&#20339;&#30340;&#25512;&#33616;&#25919;&#31574;&#12290;&#35813;&#30740;&#31350;&#20026;&#29702;&#35299;&#20449;&#20219;&#12289;&#21512;&#35268;&#21644;&#28385;&#24847;&#24230;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#20869;&#37096;&#20154;&#21592;&#21512;&#35268;&#24615;&#21644;&#21487;&#35828;&#26381;&#24615;&#30340;&#35780;&#20998;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2204.02294</link><description>&lt;p&gt;
ZETAR:&#25112;&#30053;&#21644;&#33258;&#36866;&#24212;&#21512;&#35268;&#25919;&#31574;&#30340;&#24314;&#27169;&#19982;&#35745;&#31639;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
ZETAR: Modeling and Computational Design of Strategic and Adaptive Compliance Policies. (arXiv:2204.02294v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ZETAR&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#20869;&#37096;&#20154;&#21592;&#30340;&#28608;&#21169;&#65292;&#24182;&#35774;&#35745;&#23450;&#21046;&#30340;&#25512;&#33616;&#25919;&#31574;&#26469;&#25552;&#39640;&#21512;&#35268;&#24615;&#12290;&#36890;&#36807;&#21407;&#22987;&#21644;&#23545;&#20598;&#20984;&#35268;&#21010;&#65292;&#35745;&#31639;&#20986;&#26368;&#20339;&#30340;&#25512;&#33616;&#25919;&#31574;&#12290;&#35813;&#30740;&#31350;&#20026;&#29702;&#35299;&#20449;&#20219;&#12289;&#21512;&#35268;&#21644;&#28385;&#24847;&#24230;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#20869;&#37096;&#20154;&#21592;&#21512;&#35268;&#24615;&#21644;&#21487;&#35828;&#26381;&#24615;&#30340;&#35780;&#20998;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#35268;&#31649;&#29702;&#22312;&#20943;&#36731;&#20869;&#37096;&#23041;&#32961;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28608;&#21169;&#35774;&#35745;&#26159;&#19968;&#31181;&#31215;&#26497;&#30340;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20869;&#37096;&#20154;&#21592;&#30340;&#28608;&#21169;&#19982;&#38450;&#24481;&#32773;&#30340;&#23433;&#20840;&#30446;&#26631;&#30456;&#19968;&#33268;&#65292;&#28608;&#21169;&#20869;&#37096;&#20154;&#21592;&#20197;&#32452;&#32455;&#30340;&#21033;&#30410;&#20026;&#21160;&#26426;&#34892;&#21160;&#65292;&#20174;&#32780;&#23454;&#29616;&#21512;&#35268;&#12290;&#25511;&#21046;&#20154;&#32676;&#32423;&#21512;&#35268;&#30340;&#20869;&#37096;&#20154;&#21592;&#30340;&#28608;&#21169;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20182;&#20204;&#26082;&#19981;&#26159;&#31934;&#30830;&#21487;&#30693;&#30340;&#65292;&#20063;&#26080;&#27861;&#30452;&#25509;&#25511;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ZETAR&#65292;&#19968;&#20010;&#38646;&#20449;&#20219;&#30340;&#23457;&#35745;&#21644;&#25512;&#33616;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23450;&#37327;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#20869;&#37096;&#20154;&#21592;&#30340;&#28608;&#21169;&#65292;&#24182;&#35774;&#35745;&#23450;&#21046;&#30340;&#25512;&#33616;&#25919;&#31574;&#26469;&#25552;&#39640;&#20182;&#20204;&#30340;&#21512;&#35268;&#24615;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#21407;&#22987;&#21644;&#23545;&#20598;&#20984;&#35268;&#21010;&#26469;&#35745;&#31639;&#26368;&#20339;&#30340;&#23450;&#21046;&#25512;&#33616;&#25919;&#31574;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#29702;&#35770;&#22522;&#30784;&#26469;&#29702;&#35299;&#20449;&#20219;&#12289;&#21512;&#35268;&#21644;&#28385;&#24847;&#24230;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#22914;&#20309;&#35780;&#20272;&#20869;&#37096;&#20154;&#21592;&#21512;&#35268;&#24615;&#21644;&#21487;&#35828;&#26381;&#24615;&#30340;&#35780;&#20998;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compliance management plays an important role in mitigating insider threats. Incentive design is a proactive and non-invasive approach to achieving compliance by aligning an insider's incentive with the defender's security objective, which motivates (rather than commands) an insider to act in the organization's interests. Controlling insiders' incentives for population-level compliance is challenging because they are neither precisely known nor directly controllable. To this end, we develop ZETAR, a zero-trust audit and recommendation framework, to provide a quantitative approach to model insiders' incentives and design customized recommendation policies to improve their compliance. We formulate primal and dual convex programs to compute the optimal bespoke recommendation policies. We create the theoretical underpinning for understanding trust, compliance, and satisfaction, which leads to scoring mechanisms of how compliant and persuadable an insider is. After classifying insiders as m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#30340;&#20004;&#31181;&#20462;&#25913;&#65292;&#19968;&#31181;&#37319;&#29992;&#20808;&#36827;&#30340;&#32858;&#21512;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#19977;&#32500;&#29615;&#22659;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.13424</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Dealing with Sparse Rewards Using Graph Neural Networks. (arXiv:2203.13424v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#30340;&#20004;&#31181;&#20462;&#25913;&#65292;&#19968;&#31181;&#37319;&#29992;&#20808;&#36827;&#30340;&#32858;&#21512;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#19977;&#32500;&#29615;&#22659;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26412;&#36523;&#23601;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#24403;&#22870;&#21169;&#20449;&#21495;&#31232;&#30095;&#26102;&#26356;&#21152;&#22797;&#26434;&#12290;&#22823;&#22810;&#25968;&#28041;&#21450;&#19977;&#32500;&#29615;&#22659;&#23548;&#33322;&#30340;&#20219;&#21153;&#21482;&#25552;&#20379;&#26377;&#38480;&#30340;&#20449;&#24687;&#32473;&#26234;&#33021;&#20307;&#12290;&#26222;&#36941;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#20174;&#29615;&#22659;&#20013;&#25509;&#25910;&#21040;&#19968;&#20010;&#35270;&#35273;&#35266;&#23519;&#36755;&#20837;&#65292;&#24182;&#22312;&#19968;&#38598;&#32467;&#26463;&#26102;&#24471;&#21040;&#22870;&#21169;&#12290;&#33391;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#22823;&#22823;&#25913;&#21892;&#36825;&#31867;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22686;&#21152;&#22870;&#21169;&#20449;&#21495;&#23494;&#24230;&#30340;&#32463;&#20856;&#26041;&#27861;&#26159;&#29992;&#34917;&#20805;&#22870;&#21169;&#26469;&#25913;&#21892;&#22870;&#21169;&#12290;&#36825;&#31181;&#25216;&#26415;&#34987;&#31216;&#20026;&#22870;&#21169;&#22609;&#36896;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#26368;&#36817;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#30340;&#20462;&#25913;&#65306;&#19968;&#31181;&#28041;&#21450;&#20808;&#36827;&#30340;&#32858;&#21512;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#22312;&#19977;&#32500;&#29615;&#22659;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning in partially observable environments is a difficult task in itself, and can be further complicated by a sparse reward signal. Most tasks involving navigation in three-dimensional environments provide the agent with extremely limited information. Typically, the agent receives a visual observation input from the environment and is rewarded once at the end of the episode. A good reward function could substantially improve the convergence of reinforcement learning algorithms for such tasks. The classic approach to increase the density of the reward signal is to augment it with supplementary rewards. This technique is called the reward shaping. In this study, we propose two modifications of one of the recent reward shaping methods based on graph convolutional networks: the first involving advanced aggregation functions, and the second utilizing the attention mechanism. We empirically validate the effectiveness of our solutions for the task of navigation in a 3D e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#29305;&#23450;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65288;MSFI&#65289;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35299;&#37322;&#22810;&#27169;&#24335;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20915;&#31574;&#12290;&#36825;&#26377;&#21161;&#20110;&#20020;&#24202;&#29992;&#25143;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#21644;&#37325;&#35201;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#20379;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2203.06487</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24335;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#35780;&#20272;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#29616;&#26377;&#31639;&#27861;&#26159;&#21542;&#33021;&#28385;&#36275;&#20020;&#24202;&#38656;&#27714;&#65311;
&lt;/p&gt;
&lt;p&gt;
Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?. (arXiv:2203.06487v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#29305;&#23450;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65288;MSFI&#65289;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35299;&#37322;&#22810;&#27169;&#24335;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20915;&#31574;&#12290;&#36825;&#26377;&#21161;&#20110;&#20020;&#24202;&#29992;&#25143;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#21644;&#37325;&#35201;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#20379;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#21521;&#20020;&#24202;&#32456;&#31471;&#29992;&#25143;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#26159;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#65292;&#29305;&#24449;&#24402;&#22240;&#22270;&#25110;&#28909;&#22270;&#26159;&#26368;&#24120;&#35265;&#30340;&#35299;&#37322;&#24418;&#24335;&#65292;&#23427;&#31361;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#39044;&#27979;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#28909;&#22270;&#22312;&#35299;&#37322;&#22810;&#27169;&#24335;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20915;&#31574;&#26102;&#30340;&#25928;&#26524;&#22914;&#20309;&#65292;&#20854;&#20013;&#27599;&#20010;&#22270;&#20687;&#27169;&#24335;&#25110;&#36890;&#36947;&#21487;&#35270;&#21270;&#30456;&#21516;&#24213;&#23618;&#29983;&#29289;&#21307;&#23398;&#29616;&#35937;&#30340;&#19981;&#21516;&#20020;&#24202;&#20449;&#24687;&#12290;&#20102;&#35299;&#36825;&#26679;&#30340;&#27169;&#24577;&#30456;&#20851;&#29305;&#24449;&#23545;&#20020;&#24202;&#29992;&#25143;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22312;&#20020;&#24202;&#19978;&#37325;&#35201;&#20294;&#22312;&#25216;&#26415;&#19978;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#24577;&#29305;&#23450;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65288;MSFI&#65289;&#24230;&#37327;&#25351;&#26631;&#12290;&#23427;&#21253;&#21547;&#20102;&#27169;&#24577;&#20248;&#20808;&#32423;&#21644;&#27169;&#24577;&#29305;&#23450;&#29305;&#24449;&#23450;&#20301;&#30340;&#20020;&#24202;&#22270;&#20687;&#21644;&#35299;&#37322;&#35299;&#37322;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20197;&#20020;&#24202;&#38656;&#27714;&#20026;&#22522;&#30784;&#30340;&#31995;&#32479;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to explain the prediction to clinical end-users is a necessity to leverage the power of artificial intelligence (AI) models for clinical decision support. For medical images, a feature attribution map, or heatmap, is the most common form of explanation that highlights important features for AI models' prediction. However, it is unknown how well heatmaps perform on explaining decisions on multi-modal medical images, where each image modality or channel visualizes distinct clinical information of the same underlying biomedical phenomenon. Understanding such modality-dependent features is essential for clinical users' interpretation of AI decisions. To tackle this clinically important but technically ignored problem, we propose the modality-specific feature importance (MSFI) metric. It encodes clinical image and explanation interpretation patterns of modality prioritization and modality-specific feature localization. We conduct a clinical requirement-grounded, systematic evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.03583</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33016;&#37096;&#30142;&#30149;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs. (arXiv:2202.03583v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;X&#20809;&#22270;&#20687;&#30149;&#29702;&#35782;&#21035;&#26041;&#27861;&#20381;&#36182;&#20110;&#29087;&#32451;&#30340;&#20154;&#31867;&#35299;&#37322;&#65292;&#24182;&#19988;&#24448;&#24448;&#32791;&#26102;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20986;&#29616;&#20351;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#24320;&#21457;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#31867;&#31995;&#32479;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#23427;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DenseNet&#65289;&#21644;GRADCAM&#36827;&#34892;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#21069;&#32622;X&#20809;&#35757;&#32451;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#23450;&#37327;&#25351;&#26631;&#65288;&#21253;&#25324;&#21463;&#35797;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65289;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#33719;&#24471;&#20102;0.826&#30340;&#20934;&#30830;&#24230;&#12290;&#32780;&#22312;Nodule&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#26368;&#20302;&#30340;AUC&#24471;&#20998;0.655&#65292;&#20934;&#30830;&#24230;&#20026;0.66&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#20915;&#31574;&#26041;&#38754;&#24314;&#31435;&#20449;&#20219;&#65292;&#25105;&#20204;&#20351;&#29992;GRADCAM&#29983;&#25104;&#20102;&#28909;&#22270;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23545;&#35786;&#26029;&#26368;&#37325;&#35201;&#30340;X&#20809;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional methods of identifying pathologies in X-ray images rely heavily on skilled human interpretation and are often time-consuming. The advent of deep learning techniques has enabled the development of automated disease diagnosis systems, but the performance of such systems is dependent on the quality of the model and the level of interpretability it provides. In this paper, we propose a multi-label disease diagnosis model for chest X-rays using a dense convolutional neural network (DenseNet) and model interpretability using GRADCAM. We trained our model using frontal X-rays and evaluated its performance using various quantitative metrics, including the area under the receiver operating characteristic curve (AUC). Our proposed model achieved the highest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of 0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with an accuracy of 0.66. To promote model interpretability and build trust in decision maki
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#20142;&#24230;&#30340;&#30333;&#30418;&#26080;&#38480;&#21046;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#22270;&#20687;&#30340;&#20142;&#24230;&#26469;&#25915;&#20987;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#23545;&#20154;&#31867;&#24863;&#30693;&#30340;&#24418;&#29366;&#21644;&#39068;&#33394;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2201.06070</link><description>&lt;p&gt;
ALA: &#20855;&#26377;&#33258;&#28982;&#24863;&#30693;&#30340;&#23545;&#25239;&#20142;&#24230;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ALA: Naturalness-aware Adversarial Lightness Attack. (arXiv:2201.06070v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#20142;&#24230;&#30340;&#30333;&#30418;&#26080;&#38480;&#21046;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#22270;&#20687;&#30340;&#20142;&#24230;&#26469;&#25915;&#20987;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#23545;&#20154;&#31867;&#24863;&#30693;&#30340;&#24418;&#29366;&#21644;&#39068;&#33394;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25581;&#31034;&#21644;&#20462;&#22797;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#33030;&#24369;&#24615;&#26469;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#25915;&#20987;&#26679;&#26412;&#30340;&#37096;&#20998;&#26159;&#21463;Lp&#33539;&#25968;&#38480;&#21046;&#30340;&#20960;&#20046;&#19981;&#21487;&#24863;&#30693;&#30340;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39640;&#39057;&#23646;&#24615;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#21487;&#20197;&#36890;&#36807;&#21435;&#22122;&#26041;&#27861;&#36827;&#34892;&#38450;&#24481;&#65292;&#24182;&#19988;&#24456;&#38590;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#23454;&#29616;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#32570;&#38519;&#65292;&#26377;&#20123;&#24037;&#20316;&#25552;&#20986;&#20102;&#26080;&#38480;&#21046;&#25915;&#20987;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20196;&#20154;&#22833;&#26395;&#30340;&#26159;&#65292;&#36825;&#20123;&#31034;&#20363;&#36890;&#24120;&#30475;&#36215;&#26469;&#19981;&#33258;&#28982;&#65292;&#20250;&#24341;&#36215;&#35686;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25239;&#20142;&#24230;&#25915;&#20987;(ALA)&#65292;&#19968;&#31181;&#38024;&#23545;&#20462;&#25913;&#22270;&#20687;&#20142;&#24230;&#30340;&#30333;&#30418;&#26080;&#38480;&#21046;&#23545;&#25239;&#25915;&#20987;&#12290;&#23545;&#20110;&#20154;&#31867;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#30340;&#26679;&#26412;&#30340;&#24418;&#29366;&#21644;&#39068;&#33394;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#33719;&#24471;&#20855;&#26377;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#22312;&#20142;&#24230;&#21644;&#38452;&#24433;&#20851;&#31995;&#26041;&#38754;&#25552;&#20986;&#20102;&#26080;&#32422;&#26463;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most researchers have tried to enhance the robustness of DNNs by revealing and repairing the vulnerability of DNNs with specialized adversarial examples. Parts of the attack examples have imperceptible perturbations restricted by Lp norm. However, due to their high-frequency property, the adversarial examples can be defended by denoising methods and are hard to realize in the physical world. To avoid the defects, some works have proposed unrestricted attacks to gain better robustness and practicality. It is disappointing that these examples usually look unnatural and can alert the guards. In this paper, we propose Adversarial Lightness Attack (ALA), a white-box unrestricted adversarial attack that focuses on modifying the lightness of the images. The shape and color of the samples, which are crucial to human perception, are barely influenced. To obtain adversarial examples with a high attack success rate, we propose unconstrained enhancement in terms of the light and shade relationship
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#24573;&#30053;&#20102;&#21738;&#20123;&#29305;&#23450;&#25991;&#26723;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#36890;&#36807;&#30740;&#31350;&#26631;&#20934;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30340;&#21453;&#20107;&#23454;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#27599;&#20010;&#35760;&#24518;&#26679;&#26412;&#23545;&#39564;&#35777;&#38598;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#24182;&#30452;&#25509;&#25552;&#20379;&#35760;&#24518;&#26469;&#28304;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2112.12938</link><description>&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Memorization in Neural Language Models. (arXiv:2112.12938v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#24573;&#30053;&#20102;&#21738;&#20123;&#29305;&#23450;&#25991;&#26723;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#36890;&#36807;&#30740;&#31350;&#26631;&#20934;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30340;&#21453;&#20107;&#23454;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#27599;&#20010;&#35760;&#24518;&#26679;&#26412;&#23545;&#39564;&#35777;&#38598;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#24182;&#30452;&#25509;&#25552;&#20379;&#35760;&#24518;&#26469;&#28304;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#29616;&#20195;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#35760;&#24518;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#29702;&#35299;&#36825;&#31181;&#35760;&#24518;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#21644;&#23398;&#20064;&#29702;&#35770;&#30340;&#35282;&#24230;&#37117;&#24456;&#37325;&#35201;&#12290;&#22312;&#20808;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#30740;&#31350;&#20013;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#36807;&#28388;&#25481;&#8220;&#24120;&#35265;&#8221;&#35760;&#24518;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#22810;&#25968;&#35760;&#24518;&#26631;&#20934;&#19982;&#22312;&#35757;&#32451;&#38598;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#24378;&#28872;&#30456;&#20851;&#65292;&#25429;&#25417;&#21040;&#24120;&#35265;&#30701;&#35821;&#12289;&#20844;&#20849;&#30693;&#35782;&#12289;&#27169;&#26495;&#21270;&#25991;&#26412;&#25110;&#20854;&#20182;&#37325;&#22797;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#22312;&#30465;&#30053;&#29305;&#23450;&#25991;&#26723;&#36827;&#34892;&#35757;&#32451;&#26102;&#22914;&#20309;&#25913;&#21464;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#20272;&#35745;&#20102;&#27599;&#20010;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#23545;&#39564;&#35777;&#38598;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#36825;&#22914;&#20309;&#30452;&#25509;&#25552;&#20379;&#35760;&#24518;&#26469;&#28304;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural language models that are widely used in various NLP tasks risk memorizing sensitive information from their training data. Understanding this memorization is important in real world applications and also from a learning-theoretical perspective. An open question in previous studies of language model memorization is how to filter out "common" memorization. In fact, most memorization criteria strongly correlate with the number of occurrences in the training set, capturing memorized familiar phrases, public knowledge, templated texts, or other repeated data. We formulate a notion of counterfactual memorization which characterizes how a model's predictions change if a particular document is omitted during training. We identify and study counterfactually-memorized training examples in standard text datasets. We estimate the influence of each memorized training example on the validation set and on generated texts, showing how this can provide direct evidence of the source of memo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;&#36890;&#36807;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2110.03135</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#65306;&#30740;&#31350;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting. (arXiv:2110.03135v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;&#36890;&#36807;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#12290;&#36825;&#31181;&#26631;&#31614;&#22122;&#22768;&#26159;&#30001;&#20110;&#23545;&#25239;&#26679;&#26412;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#19982;&#20174;&#24178;&#20928;&#26679;&#26412;&#32487;&#25215;&#30340;&#26631;&#31614;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#36896;&#25104;&#30340; - &#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#34987;&#23545;&#25239;&#25200;&#21160;&#25197;&#26354;&#65292;&#20294;&#20174;&#24178;&#20928;&#26679;&#26412;&#32487;&#25215;&#26631;&#31614;&#30340;&#24120;&#35265;&#20570;&#27861;&#21364;&#24573;&#30053;&#20102;&#36825;&#19968;&#28857;&#12290;&#35748;&#35782;&#21040;&#26631;&#31614;&#22122;&#22768;&#26377;&#21161;&#20110;&#27934;&#23519;&#23545;&#25239;&#35757;&#32451;&#20013;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#22855;&#29305;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26631;&#31614;&#22122;&#22768;&#35270;&#35282;&#19982;&#25105;&#20204;&#23545;&#23545;&#25239;&#35757;&#32451;&#20013;&#32426;&#20803;&#21452;&#19979;&#38477;&#29616;&#35937;&#30340;&#35266;&#23519;&#30456;&#21563;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#24341;&#20837;&#26032;&#30340;&#36229;&#21442;&#25968;&#25110;&#39069;&#22806;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that label noise exists in adversarial training. Such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples - the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples. Recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. Also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. Guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting. Our method achieves consistent performance improvements across various models and datasets without introducing new hyper-parameters or additional tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#36339;&#36291;&#36830;&#25509;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2010.10274</link><description>&lt;p&gt;
&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Fairing Convolutional Networks for Anomaly Detection. (arXiv:2010.10274v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.10274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#36339;&#36291;&#36830;&#25509;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#26159;&#35768;&#22810;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#38750;&#24120;&#26377;&#25928;&#30340;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#36880;&#23618;&#20256;&#25773;&#35268;&#21017;&#22312;&#29702;&#35770;&#19978;&#21463;&#21040;&#20960;&#20309;&#22788;&#29702;&#20013;&#38544;&#24335;&#24179;&#28369;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#21253;&#25324;&#29992;&#20110;&#32858;&#21512;&#26469;&#33258;&#30456;&#37051;&#33410;&#28857;&#30340;&#20449;&#24687;&#30340;&#22270;&#21367;&#31215;&#27169;&#22359;&#21644;&#29992;&#20110;&#32452;&#21512;&#36880;&#23618;&#37051;&#23621;&#34920;&#31034;&#30340;&#36339;&#36291;&#36830;&#25509;&#27169;&#22359;&#12290;&#36825;&#20010;&#20256;&#25773;&#35268;&#21017;&#26159;&#36890;&#36807;&#38597;&#21487;&#27604;&#26041;&#27861;&#20174;&#38544;&#24335;&#24179;&#28369;&#26041;&#31243;&#30340;&#36845;&#20195;&#35299;&#23548;&#20986;&#30340;&#12290;&#38500;&#20102;&#36890;&#36807;&#32593;&#32476;&#23618;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#25429;&#33719;&#26469;&#33258;&#36828;&#31243;&#22270;&#33410;&#28857;&#30340;&#20449;&#24687;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21033;&#29992;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#20123;&#36339;&#36291;&#36830;&#25509;&#26159;&#26681;&#25454;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#26550;&#26500;&#32463;&#36807;&#35774;&#35745;&#25972;&#21512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed layerwise propagation rule of our model is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. This propagation rule is derived from the iterative solution of the implicit fairing equation via the Jacobi method. In addition to capturing information from distant graph nodes through skip connections between the network's layers, our approach exploits both the graph structure and node features for learning discriminative node representations. These skip connections are integrated by design in our proposed network archi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#36870;&#21521;&#22870;&#21169;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35810;&#38382;&#29992;&#25143;&#27604;&#36739;&#19981;&#21516;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20449;&#24687;&#37327;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#22312;&#35757;&#32451;&#29615;&#22659;&#20013;&#20445;&#35777;&#33391;&#22909;&#34892;&#20026;&#30340;&#30446;&#30340;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#24341;&#36215;&#23376;&#20248;&#34892;&#20026;&#30340;&#20559;&#22909;&#26469;&#25910;&#38598;&#39069;&#22806;&#20449;&#24687;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/1809.03060</link><description>&lt;p&gt;
&#20027;&#21160;&#36870;&#21521;&#22870;&#21169;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Active Inverse Reward Design. (arXiv:1809.03060v3 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1809.03060
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#36870;&#21521;&#22870;&#21169;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35810;&#38382;&#29992;&#25143;&#27604;&#36739;&#19981;&#21516;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20449;&#24687;&#37327;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#22312;&#35757;&#32451;&#29615;&#22659;&#20013;&#20445;&#35777;&#33391;&#22909;&#34892;&#20026;&#30340;&#30446;&#30340;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#24341;&#36215;&#23376;&#20248;&#34892;&#20026;&#30340;&#20559;&#22909;&#26469;&#25910;&#38598;&#39069;&#22806;&#20449;&#24687;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20195;&#29702;&#30340;&#35774;&#35745;&#32773;&#32463;&#24120;&#36890;&#36807;&#35797;&#38169;&#30340;&#36807;&#31243;&#26469;&#36845;&#20195;&#22870;&#21169;&#20989;&#25968;&#65292;&#30452;&#21040;&#33719;&#24471;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#20294;&#36825;&#21482;&#33021;&#22312;&#35757;&#32451;&#29615;&#22659;&#20013;&#20445;&#35777;&#33391;&#22909;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20010;&#36807;&#31243;&#32467;&#26500;&#21270;&#20026;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35810;&#38382;&#29992;&#25143;&#22312;&#19981;&#21516;&#30340;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#36827;&#34892;&#27604;&#36739;&#12290;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#20027;&#21160;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#20449;&#24687;&#37327;&#30340;&#26597;&#35810;&#26469;&#20102;&#35299;&#30495;&#23454;&#22870;&#21169;&#12290;&#19982;&#35201;&#27714;&#35774;&#35745;&#32773;&#25552;&#20379;&#26368;&#20339;&#34892;&#20026;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#24341;&#36215;&#23376;&#20248;&#34892;&#20026;&#20043;&#38388;&#30340;&#20559;&#22909;&#26469;&#25910;&#38598;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;&#22312;&#27599;&#27425;&#26597;&#35810;&#20043;&#21518;&#65292;&#25105;&#20204;&#38656;&#35201;&#36890;&#36807;&#35266;&#23519;&#35774;&#35745;&#32773;&#36873;&#25321;&#30340;&#20195;&#29702;&#22870;&#21169;&#20989;&#25968;&#26469;&#26356;&#26032;&#23545;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#30340;&#21518;&#39564;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#36870;&#21521;&#22870;&#21169;&#35774;&#35745; (IRD) &#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#26126;&#26174;&#20248;&#20110;IRD&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#21487;&#20197;&#35810;&#38382;&#35774;&#35745;&#32773;&#26377;&#20851;&#21487;&#35299;&#37322;&#12289;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#20173;&#28982;&#21487;&#20197;&#25512;&#26029;&#20986;&#38750;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designers of AI agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.
&lt;/p&gt;</description></item></channel></rss>