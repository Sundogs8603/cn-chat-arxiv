<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65288;SPIN-Diffusion&#65289;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20808;&#21069;&#29256;&#26412;&#30340;&#31454;&#20105;&#65292;&#23454;&#29616;&#20102;&#36880;&#27493;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.10210</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65288;SPIN-Diffusion&#65289;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20808;&#21069;&#29256;&#26412;&#30340;&#31454;&#20105;&#65292;&#23454;&#29616;&#20102;&#36880;&#27493;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#21069;&#27839;&#65292;&#23588;&#20854;&#26159;&#19982;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#26041;&#38754;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#30456;&#27604;&#12290;&#23613;&#31649;&#29616;&#22312;&#30340;&#20808;&#36827;&#25193;&#25955;&#27169;&#22411;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#21644;SDXL&#20381;&#36182;&#20110;&#30417;&#30563;&#24494;&#35843;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#35266;&#23519;&#21040;&#19968;&#23450;&#25968;&#37327;&#30340;&#25968;&#25454;&#21518;&#24517;&#28982;&#20250;&#36798;&#21040;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#24212;&#29992;&#20110;&#36890;&#36807;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#38656;&#35201;&#33267;&#23569;&#20004;&#20010;&#22270;&#20687;&#65288;&#8220;&#33719;&#32988;&#32773;&#8221;&#21644;&#8220;&#22833;&#36133;&#32773;&#8221;&#22270;&#20687;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65288;SPIN-Diffusion&#65289;&#65292;&#20854;&#20013;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20808;&#21069;&#29256;&#26412;&#36827;&#34892;&#31454;&#20105;&#65292;&#20419;&#36827;&#20102;&#19968;&#20010;&#36845;&#20195;&#30340;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#30417;&#30563;&#24494;&#35843;&#21644;RL&#31574;&#30053;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10210v1 Announce Type: cross  Abstract: Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images ("winner" and "loser" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, signific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#22270;&#23376;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10206</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#29305;&#23450;&#20219;&#21153;&#22270;&#23376;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Ising on the Graph: Task-specific Graph Subsampling via the Ising Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#22270;&#23376;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22270;&#30340;&#22823;&#23567;&#21516;&#26102;&#20445;&#25345;&#20854;&#25972;&#20307;&#32467;&#26500;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#20943;&#23567;&#22270;&#30340;&#26041;&#27861;&#35201;&#20040;&#21024;&#38500;&#36793;&#32536;&#65288;&#31232;&#30095;&#21270;&#65289;&#65292;&#35201;&#20040;&#21512;&#24182;&#33410;&#28857;&#65288;&#31895;&#21270;&#65289;&#65292;&#32780;&#27809;&#26377;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22312;&#33410;&#28857;&#25110;&#36793;&#19978;&#23450;&#20041;&#30340;&#20234;&#36763;&#27169;&#22411;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#23376;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#22914;&#20309;&#20026;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#12290;&#25152;&#20351;&#29992;&#30340;&#20219;&#21153;&#25439;&#22833;&#20989;&#25968;&#29978;&#33267;&#19981;&#38656;&#35201;&#21487;&#24494;&#20998;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24212;&#29992;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#65306;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10206v1 Announce Type: cross  Abstract: Reducing a graph while preserving its overall structure is an important problem with many applications. Typically, the reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an unsupervised way with no specific downstream task in mind. In this paper, we present an approach for subsampling graph structures using an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a graph neural network. Our approach is task-specific as it can learn how to reduce a graph for a specific downstream task in an end-to-end fashion. The utilized loss function of the task does not even have to be differentiable. We showcase the versatility of our approach on three distinct applications: image segmentation, 3D shape sparsification, and sparse approximate matrix inverse determination.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26377;&#26465;&#20214;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#20174;&#19981;&#24178;&#20928;&#30340;&#23556;&#30005;&#22270;&#20687;&#20013;&#37325;&#24314;&#22825;&#31354;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#23450;&#20301;&#21644;&#27979;&#37327;&#27969;&#37327;&#65292;&#20026;&#23556;&#30005;&#28304;&#30340;&#34920;&#24449;&#25552;&#20379;&#28508;&#22312;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10204</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#26465;&#20214;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23556;&#30005;&#22825;&#25991;&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26377;&#26465;&#20214;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#20174;&#19981;&#24178;&#20928;&#30340;&#23556;&#30005;&#22270;&#20687;&#20013;&#37325;&#24314;&#22825;&#31354;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#23450;&#20301;&#21644;&#27979;&#37327;&#27969;&#37327;&#65292;&#20026;&#23556;&#30005;&#28304;&#30340;&#34920;&#24449;&#25552;&#20379;&#28508;&#22312;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19981;&#24178;&#20928;&#30340;&#23556;&#30005;&#22270;&#20687;&#20013;&#37325;&#24314;&#22825;&#31354;&#27169;&#22411;&#65292;&#20197;&#20415;&#20934;&#30830;&#23450;&#20301;&#21644;&#27979;&#37327;&#27969;&#37327;&#23545;&#20110;&#30740;&#31350;&#39640;&#32418;&#31227;&#19979;&#30340;&#26143;&#31995;&#28436;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;Atacama Large Millimetre Array (ALMA)&#31561;&#20202;&#22120;&#36827;&#34892;&#28145;&#24230;&#35266;&#27979;&#26102;&#12290;&#38543;&#30528;Square Kilometre Array (SKA)&#31561;&#26032;&#39033;&#30446;&#30340;&#21551;&#21160;&#65292;&#23545;&#26356;&#22909;&#30340;&#28304;&#25552;&#21462;&#26041;&#27861;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#30446;&#21069;&#30340;&#25216;&#26415;&#65292;&#22914;CLEAN&#21644;PyBDSF&#65292;&#24448;&#24448;&#26080;&#27861;&#26816;&#27979;&#21040;&#24494;&#24369;&#30340;&#28304;&#65292;&#20984;&#26174;&#20102;&#23545;&#26356;&#20934;&#30830;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#25552;&#35758;&#20351;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#19981;&#24178;&#20928;&#30340;&#22270;&#20687;&#20013;&#37325;&#24314;&#22825;&#31354;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#23450;&#20301;&#23556;&#30005;&#28304;&#24182;&#27979;&#37327;&#20276;&#38543;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26631;&#24535;&#30528;&#23556;&#30005;&#28304;&#34920;&#24449;&#26041;&#38754;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#22522;&#20110;ALMA&#31532;5.3&#21608;&#26399;&#22825;&#32447;&#35774;&#32622;&#30340;CASA&#24037;&#20855;simalma&#27169;&#25311;&#30340;10164&#20010;&#22270;&#20687;&#19978;&#27979;&#35797;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10204v1 Announce Type: cross  Abstract: Reconstructing sky models from dirty radio images for accurate source localization and flux estimation is crucial for studying galaxy evolution at high redshift, especially in deep fields using instruments like the Atacama Large Millimetre Array (ALMA). With new projects like the Square Kilometre Array (SKA), there's a growing need for better source extraction methods. Current techniques, such as CLEAN and PyBDSF, often fail to detect faint sources, highlighting the need for more accurate methods. This study proposes using stochastic neural networks to rebuild sky models directly from dirty images. This method can pinpoint radio sources and measure their fluxes with related uncertainties, marking a potential improvement in radio source characterization. We tested this approach on 10164 images simulated with the CASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied conditional Denoising Diffusion Probabilistic Models (D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#23545;&#35821;&#35328;&#20195;&#29702;&#30340;&#25932;&#23545;&#25915;&#20987;&#36827;&#34892;&#26144;&#23556;&#30340;&#31995;&#32479;&#21270;&#21162;&#21147;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20123;&#25915;&#20987;&#12290;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#35821;&#35328;&#20195;&#29702;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.10196</link><description>&lt;p&gt;
&#19968;&#24231;&#25671;&#25671;&#27442;&#22368;&#30340;&#32440;&#29260;&#23627;&#65311;&#23545;&#35821;&#35328;&#20195;&#29702;&#30340;&#25932;&#23545;&#25915;&#20987;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#23545;&#35821;&#35328;&#20195;&#29702;&#30340;&#25932;&#23545;&#25915;&#20987;&#36827;&#34892;&#26144;&#23556;&#30340;&#31995;&#32479;&#21270;&#21162;&#21147;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20123;&#25915;&#20987;&#12290;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#35821;&#35328;&#20195;&#29702;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#35821;&#35328;&#20195;&#29702;&#22312;&#21457;&#23637;&#20013;&#36805;&#29467;&#12290;&#23427;&#20204;&#21033;&#29992;&#35821;&#35328;&#20316;&#20026;&#24605;&#24819;&#21644;&#20132;&#27969;&#30340;&#24037;&#20855;&#65292;&#36171;&#20104;&#20102;&#26080;&#27604;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#20154;&#20204;&#36805;&#36895;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#23558;LLMs&#36830;&#25509;&#21040;&#21508;&#31181;&#22806;&#37096;&#32452;&#20214;&#21644;&#29615;&#22659;&#20013;&#65306;&#25968;&#25454;&#24211;&#65292;&#24037;&#20855;&#65292;&#22240;&#29305;&#32593;&#65292;&#26426;&#22120;&#20154;&#23454;&#20307;&#31561;&#12290;&#35768;&#22810;&#20154;&#30456;&#20449;&#19968;&#31181;&#21069;&#25152;&#26410;&#26377;&#30340;&#24378;&#22823;&#33258;&#21160;&#21270;&#25216;&#26415;&#27491;&#22312;&#23835;&#36215;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#30340;&#31995;&#32479;&#22914;&#35821;&#35328;&#20195;&#29702;&#12290;&#20182;&#20204;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#36895;&#24230;&#21644;&#35268;&#27169;&#19982;&#25105;&#20204;&#23545;&#20854;&#23433;&#20840;&#39118;&#38505;&#30340;&#29702;&#35299;&#20043;&#38388;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#26159;&#21542;&#27491;&#22312;&#24314;&#36896;&#19968;&#24231;&#32440;&#29260;&#23627;&#65311;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#31995;&#32479;&#22320;&#23545;&#35821;&#35328;&#20195;&#29702;&#30340;&#25932;&#23545;&#25915;&#20987;&#36827;&#34892;&#20102;&#26144;&#23556;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10196v1 Announce Type: cross  Abstract: Language agents powered by large language models (LLMs) have seen exploding development. Their capability of using language as a vehicle for thought and communication lends an incredible level of flexibility and versatility. People have quickly capitalized on this capability to connect LLMs to a wide range of external components and environments: databases, tools, the Internet, robotic embodiment, etc. Many believe an unprecedentedly powerful automation technology is emerging. However, new automation technologies come with new safety risks, especially for intricate systems like language agents. There is a surprisingly large gap between the speed and scale of their development and deployment and our understanding of their safety risks. Are we building a house of cards? In this position paper, we present the first systematic effort in mapping adversarial attacks against language agents. We first present a unified conceptual framework for
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10192</link><description>&lt;p&gt;
&#20511;&#37492;&#22810;&#20307;&#29289;&#29702;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19981;&#36879;&#26126;&#30340;&#12289;&#31867;&#20284;&#20110;&#31070;&#35861;&#33324;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#35299;&#37322;&#21644;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#20102;&#34987;&#31216;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#65292;&#23558;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#20026;&#19968;&#20010;&#22312;&#20855;&#26377;&#27010;&#24565;&#38468;&#21152;&#30340;&#39030;&#28857;&#30340;&#22270;&#19978;&#30340;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;&#34429;&#28982;&#36825;&#31181;&#25551;&#36848;&#20855;&#26377;&#21508;&#31181;&#22909;&#22788;&#65292;&#21253;&#25324;&#37327;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#19981;&#33021;&#33258;&#28982;&#22320;&#29992;&#26469;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#30340;&#25512;&#24191;&#65292;&#23427;&#23558;&#24605;&#32500;&#36807;&#31243;&#35270;&#20026;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10192v1 Announce Type: cross  Abstract: With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI). One method in this field known as Projective Simulation (PS) models a chain-of-thought as a random walk of a particle on a graph with vertices that have concepts attached to them. While this description has various benefits, including the possibility of quantization, it cannot be naturally used to model thoughts that combine several concepts simultaneously. To overcome this limitation, we introduce Multi-Excitation Projective Simulation (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph. A definition for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#22823;&#35268;&#27169;&#21463;&#38480;&#21046;&#32858;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#22120;&#29983;&#25104;&#26082;&#21487;&#34892;&#21448;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#36164;&#28304;&#20998;&#37197;&#21644;&#20351;&#29992;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.10177</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21463;&#38480;&#21046;&#32858;&#31867;&#19982;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Scale Constrained Clustering With Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#22823;&#35268;&#27169;&#21463;&#38480;&#21046;&#32858;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#22120;&#29983;&#25104;&#26082;&#21487;&#34892;&#21448;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#36164;&#28304;&#20998;&#37197;&#21644;&#20351;&#29992;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#32593;&#32476;&#65292;&#23558;&#36164;&#28304;&#20998;&#37197;&#22312;&#32858;&#31867;&#32423;&#21035;&#32780;&#19981;&#26159;&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#65292;&#21487;&#20197;&#22686;&#24378;&#36164;&#28304;&#20998;&#37197;&#21644;&#20351;&#29992;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26368;&#23567;&#21270;&#32858;&#31867;&#20869;&#37096;&#36317;&#31163;&#21644;&#26368;&#22823;&#21270;&#20998;&#37197;&#32473;&#32858;&#31867;&#30340;&#33410;&#28857;&#25968;&#37327;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#32858;&#31867;&#20869;&#37096;&#27809;&#26377;&#20004;&#20010;&#33410;&#28857;&#30340;&#36317;&#31163;&#36229;&#36807;&#38408;&#20540;&#30340;&#20840;&#36830;&#25509;&#19981;&#30456;&#20132;&#32858;&#31867;&#38382;&#39064;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#20108;&#36827;&#21046;&#32447;&#24615;&#27169;&#22411;&#36731;&#26494;&#22320;&#24418;&#25104;&#38382;&#39064;&#65292;&#20294;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#23454;&#20363;&#26102;&#65292;&#20256;&#32479;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#24456;&#38590;&#24212;&#23545;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#32858;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#22120;&#29983;&#25104;&#26082;&#21487;&#34892;&#21448;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20195;&#29702;&#22120;&#23398;&#20064;&#29305;&#23450;&#20110;&#35813;&#20219;&#21153;&#25152;&#36935;&#21040;&#30340;&#23454;&#20363;&#30340;&#38382;&#39064;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#22312;&#32467;&#26524;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21363;&#20351;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#20063;&#33021;&#25214;&#21040;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10177v1 Announce Type: cross  Abstract: Given a network, allocating resources at clusters level, rather than at each node, enhances efficiency in resource allocation and usage. In this paper, we study the problem of finding fully connected disjoint clusters to minimize the intra-cluster distances and maximize the number of nodes assigned to the clusters, while also ensuring that no two nodes within a cluster exceed a threshold distance. While the problem can easily be formulated using a binary linear model, traditional combinatorial optimization solvers struggle when dealing with large-scale instances. We propose an approach to solve this constrained clustering problem via reinforcement learning. Our method involves training an agent to generate both feasible and (near) optimal solutions. The agent learns problem-specific heuristics, tailored to the instances encountered in this task. In the results section, we show that our algorithm finds near optimal solutions, even for l
&lt;/p&gt;</description></item><item><title>OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.10176</link><description>&lt;p&gt;
OpenMathInstruct-1: &#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10176
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#21512;&#25104;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20026;&#20102;&#33719;&#24471;&#29305;&#23450;&#30340;&#25216;&#33021;&#12290;&#30446;&#21069;&#30340;&#22823;&#35268;&#27169;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#22914;MetaMathQA&#21644;MAmmoTH&#65292;&#26159;&#20351;&#29992;&#26469;&#33258;&#21830;&#19994;&#38480;&#21046;&#35768;&#21487;&#30340;&#38381;&#28304;LLM&#30340;&#36755;&#20986;&#26500;&#24314;&#30340;&#12290;&#38480;&#21046;&#22312;&#36825;&#20123;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#20013;&#20351;&#29992;&#24320;&#28304;LLM&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#30446;&#21069;&#26368;&#22909;&#30340;&#38381;&#28304;LLM&#65288;&#22914;GPT-4&#65289;&#21644;&#26368;&#22909;&#30340;&#24320;&#28304;LLM&#20043;&#38388;&#22312;&#25968;&#23398;&#25216;&#33021;&#19978;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;&#22522;&#20110;&#24320;&#28304;LLM&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#26041;&#24335;&#21644;&#19968;&#20123;&#24378;&#21147;&#32553;&#25918;&#65292;&#26500;&#24314;&#20102;OpenMathInstruct-1&#65292;&#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#38382;&#39064;-&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20351;&#29992;GSM8K&#21644;MATH&#36825;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#21512;&#25104;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10176v1 Announce Type: cross  Abstract: Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using t
&lt;/p&gt;</description></item><item><title>OptiMUS&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#21046;&#23450;&#21644;&#35299;&#20915;(&#28151;&#21512;&#25972;&#25968;)&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65292;&#20854;&#22312;&#26131;&#20110;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10172</link><description>&lt;p&gt;
OptiMUS: &#21487;&#25193;&#23637;&#30340;&#26368;&#20248;&#21270;&#24314;&#27169;&#19982;(MI)LP&#27714;&#35299;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10172
&lt;/p&gt;
&lt;p&gt;
OptiMUS&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#21046;&#23450;&#21644;&#35299;&#20915;(&#28151;&#21512;&#25972;&#25968;)&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65292;&#20854;&#22312;&#26131;&#20110;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#38382;&#39064;&#22312;&#21046;&#36896;&#19994;&#12289;&#20998;&#38144;&#19994;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#26222;&#36941;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#31867;&#38382;&#39064;&#20173;&#28982;&#26159;&#36890;&#36807;&#25163;&#24037;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#30340;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#36827;&#34892;&#26368;&#20248;&#35299;&#65292;&#22240;&#20026;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#26469;&#21046;&#23450;&#21644;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#20248;&#21270;&#24037;&#20855;&#21644;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OptiMUS&#65292;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20195;&#29702;&#65292;&#26088;&#22312;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#21046;&#23450;&#21644;&#35299;&#20915;(&#28151;&#21512;&#25972;&#25968;)&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;OptiMUS&#21487;&#20197;&#24320;&#21457;&#25968;&#23398;&#27169;&#22411;&#65292;&#32534;&#20889;&#21644;&#35843;&#35797;&#27714;&#35299;&#22120;&#20195;&#30721;&#65292;&#35780;&#20272;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#35780;&#20272;&#25913;&#36827;&#20854;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;OptiMUS&#20351;&#29992;&#27169;&#22359;&#21270;&#32467;&#26500;&#22788;&#29702;&#38382;&#39064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#38271;&#35828;&#26126;&#21644;&#22797;&#26434;&#25968;&#25454;&#30340;&#38382;&#39064;&#32780;&#26080;&#38656;&#38271;&#25552;&#31034;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;OptiMUS&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10172v1 Announce Type: new  Abstract: Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy dat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#21040;128K&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25968;&#25454;&#28151;&#21512;&#21644;&#36731;&#37327;&#32423;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#23454;&#29616;&#65292;&#20854;&#20013;&#20851;&#38190;&#35201;&#28857;&#22312;&#20110;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#65292;&#38656;&#35201;&#27880;&#24847;&#39046;&#22495;&#24179;&#34913;&#21644;&#38271;&#24230;&#19978;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.10171</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Data Engineering for Scaling Language Models to 128K Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#21040;128K&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25968;&#25454;&#28151;&#21512;&#21644;&#36731;&#37327;&#32423;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#23454;&#29616;&#65292;&#20854;&#20013;&#20851;&#38190;&#35201;&#28857;&#22312;&#20110;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#65292;&#38656;&#35201;&#27880;&#24847;&#39046;&#22495;&#24179;&#34913;&#21644;&#38271;&#24230;&#19978;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#25193;&#23637;&#21040;128K&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#30528;&#37325;&#32771;&#34385;&#25968;&#25454;&#24037;&#31243;&#12290;&#25105;&#20204;&#20551;&#35774;&#38271;&#19978;&#19979;&#25991;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#8220;&#33021;&#22815;&#21033;&#29992;&#20219;&#24847;&#36755;&#20837;&#20301;&#32622;&#30340;&#20449;&#24687;&#8221;&#30340;&#33021;&#21147;&#65292;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#25484;&#25569;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#36731;&#37327;&#32423;&#36830;&#32493;&#39044;&#35757;&#32451;&#22312;&#27604;&#35757;&#32451;&#26102;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;(&#20363;&#22914;&#20174;4K&#21040;128K)&#19979;&#36731;&#26494;&#25193;&#23637;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#8220;&#25968;&#37327;&#8221;&#21644;&#8220;&#36136;&#37327;&#8221;&#65306;(1)&#23545;&#20110;&#25968;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;5&#20159;&#21040;50&#20159;&#20010;&#26631;&#35760;&#36275;&#20197;&#20351;&#27169;&#22411;&#33021;&#22815;&#26816;&#32034;&#21040;128K&#19978;&#19979;&#25991;&#20013;&#30340;&#20219;&#20309;&#20301;&#32622;&#30340;&#20449;&#24687;&#65307;(2)&#23545;&#20110;&#36136;&#37327;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21516;&#31561;&#24378;&#35843;&#8220;&#39046;&#22495;&#24179;&#34913;&#8221;&#21644;&#8220;&#38271;&#24230;&#19978;&#37319;&#26679;&#8221;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#22320;&#19978;&#37319;&#26679;&#26356;&#38271;&#30340;&#25968;&#25454;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#36275;&#22815;&#30340;&#36136;&#37327;&#65292;&#32780;&#26159;&#38656;&#35201;&#27880;&#24847;&#25968;&#25454;&#30340;&#39046;&#22495;&#24179;&#34913;&#21644;&#38271;&#24230;&#19978;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10171v1 Announce Type: cross  Abstract: We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \textit{quantity} and \textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \textit{domain balance} and \textit{length upsampling}. Concretely, we find that naively upsampling longer data o
&lt;/p&gt;</description></item><item><title>DeepSRGM&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Raga&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LSTM-RNN&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#36798;&#21040;&#20102;88.1%&#21644;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Raga&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#22320;&#20301;&#12290;</title><link>https://arxiv.org/abs/2402.10168</link><description>&lt;p&gt;
DeepSRGM -- &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;&#20013;&#30340;&#24207;&#21015;&#20998;&#31867;&#21644;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
DeepSRGM -- Sequence Classification and Ranking in Indian Classical Music with Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10168
&lt;/p&gt;
&lt;p&gt;
DeepSRGM&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Raga&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LSTM-RNN&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#36798;&#21040;&#20102;88.1%&#21644;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Raga&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;arXiv:2402.10168v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#12299; &#25688;&#35201;&#65306;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;(ICM)&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;Raga&#65292;&#23427;&#20316;&#20026;&#20316;&#26354;&#21644;&#21363;&#20852;&#28436;&#22863;&#30340;&#26059;&#24459;&#26694;&#26550;&#12290;Raga&#30340;&#35782;&#21035;&#26159;ICM&#20013;&#19968;&#39033;&#37325;&#35201;&#30340;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#20174;&#38899;&#20048;&#25512;&#33616;&#21040;&#32452;&#32455;&#22823;&#22411;&#38899;&#20048;&#25910;&#34255;&#31561;&#22810;&#31181;&#19979;&#28216;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Raga&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#65292;&#20351;&#29992;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNN)&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#37319;&#26679;&#33258;&#21407;&#22987;&#38899;&#39057;&#30340;&#36739;&#23567;&#24207;&#21015;&#19978;&#36827;&#34892;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#32780;&#26368;&#32456;&#30340;&#25512;&#29702;&#21017;&#26159;&#22312;&#25972;&#20010;&#38899;&#39057;&#19978;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Comp Music Carnatic&#25968;&#25454;&#38598;&#21644;&#20854;10&#20010;Raga&#23376;&#38598;&#19978;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#20998;&#21035;&#36798;&#21040;&#20102;88.1%&#21644;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#20351;&#20854;&#25104;&#20026;Raga&#35782;&#21035;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20351;&#24207;&#21015;&#25490;&#24207;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10168v1 Announce Type: cross  Abstract: A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient pre possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20986;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#31163;&#25955;&#39033;&#30446;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#21487;&#33021;&#20986;&#29616;&#30340;&#39033;&#30446;&#12290;</title><link>https://arxiv.org/abs/2402.10142</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tracking Changing Probabilities via Dynamic Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20986;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#31163;&#25955;&#39033;&#30446;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#21487;&#33021;&#20986;&#29616;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#19968;&#20010;&#39044;&#27979;&#22120;&#65292;&#21363;&#19968;&#20010;&#23398;&#20064;&#22120;&#65292;&#20854;&#36755;&#20837;&#26159;&#19968;&#31995;&#21015;&#31163;&#25955;&#39033;&#30446;&#12290;&#39044;&#27979;&#22120;&#30340;&#20219;&#21153;&#26159;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#36827;&#34892;&#27010;&#29575;&#22810;&#31867;&#21035;&#39044;&#27979;&#65292;&#21363;&#36890;&#36807;&#36755;&#20986;&#26377;&#38646;&#20010;&#25110;&#22810;&#20010;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#25509;&#19979;&#26469;&#21487;&#33021;&#21457;&#29983;&#30340;&#39033;&#30446;&#65292;&#28982;&#21518;&#25581;&#31034;&#23454;&#38469;&#39033;&#30446;&#24182;&#20174;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#36755;&#20986;&#27010;&#29575;&#65292;&#39044;&#27979;&#22120;&#20250;&#36319;&#36394;&#20854;&#25152;&#35265;&#39033;&#30446;&#30340;&#27604;&#20363;&#12290;&#39044;&#27979;&#22120;&#20855;&#26377;&#24658;&#23450;&#65288;&#26377;&#38480;&#65289;&#30340;&#31354;&#38388;&#65292;&#25105;&#20204;&#23547;&#27714;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#26356;&#26032;&#25216;&#26415;&#65306;&#27969;&#26159;&#26080;&#30028;&#30340;&#65292;&#39033;&#30446;&#30340;&#38598;&#21512;&#23545;&#39044;&#27979;&#22120;&#26159;&#26410;&#30693;&#30340;&#65292;&#23427;&#20204;&#30340;&#24635;&#25968;&#20063;&#21487;&#33021;&#26080;&#38480;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#23384;&#22312;&#38750;&#24179;&#31283;&#24615;&#65306;&#39033;&#30446;&#30340;&#28508;&#22312;&#39057;&#29575;&#21487;&#33021;&#20250;&#19981;&#26102;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#26032;&#39033;&#30446;&#21487;&#33021;&#24320;&#22987;&#20986;&#29616;&#65292;&#19968;&#20123;&#24403;&#21069;&#39057;&#32321;&#20986;&#29616;&#30340;&#39033;&#30446;&#21487;&#33021;&#20877;&#27425;&#20572;&#27490;&#20986;&#29616;&#12290;&#30001;&#20110;&#26377;&#31354;&#38388;&#38480;&#21046;&#65292;&#39044;&#27979;&#22120;&#21482;&#38656;&#35201;&#25552;&#20379;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10142v1 Announce Type: cross  Abstract: Consider a predictor, a learner, whose input is a stream of discrete items. The predictor's task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation. To output probabilities, the predictor keeps track of the proportions of the items it has seen. The predictor has constant (limited) space and we seek efficient prediction and update techniques: The stream is unbounded, the set of items is unknown to the predictor and their totality can also grow unbounded. Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time. For instance, new items may start appearing and a few currently frequent items may cease to occur again. The predictor, being space-bounded, need only provide pro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23545;&#31561;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#32858;&#21512;&#31574;&#30053;&#65292;&#21253;&#25324;&#21152;&#26435;&#24179;&#22343;&#32858;&#21512;&#65292;&#20197;&#30830;&#23450;&#26368;&#24378;&#22823;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10135</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23545;&#31561;&#32852;&#37030;&#23398;&#20064;&#30340;&#31574;&#30053;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10135
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23545;&#31561;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#32858;&#21512;&#31574;&#30053;&#65292;&#21253;&#25324;&#21152;&#26435;&#24179;&#22343;&#32858;&#21512;&#65292;&#20197;&#30830;&#23450;&#26368;&#24378;&#22823;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20445;&#25252;&#21644;&#38544;&#31169;&#35201;&#27714;&#30340;&#19981;&#26029;&#22686;&#21152;&#24341;&#36215;&#20102;&#23545;&#20998;&#24067;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24040;&#22823;&#30740;&#31350;&#20852;&#36259;&#65292;&#23588;&#20854;&#26159;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30001;&#25345;&#26377;&#33258;&#24049;&#31169;&#26377;&#25968;&#25454;&#30340;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#24314;&#31435;&#12290;&#22312;&#26368;&#21021;&#30340;&#32852;&#37030;&#23398;&#20064;&#25552;&#26696;&#20013;&#65292;&#26550;&#26500;&#26159;&#38598;&#20013;&#24335;&#30340;&#65292;&#32858;&#21512;&#26159;&#36890;&#36807;&#32852;&#37030;&#24179;&#22343;&#21270;&#26469;&#23436;&#25104;&#30340;&#65292;&#24847;&#21619;&#30528;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#23558;&#20351;&#29992;&#26368;&#30452;&#25509;&#30340;&#24179;&#22343;&#31574;&#30053;&#26469;&#21327;&#35843;&#32852;&#37030;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#22312;&#23545;&#31561;&#29615;&#22659;&#20013;&#27979;&#35797;&#19981;&#21516;&#30340;&#32852;&#37030;&#31574;&#30053;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#21508;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#21253;&#25324;&#21152;&#26435;&#24179;&#22343;&#32858;&#21512;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#22240;&#32032;&#21644;&#22522;&#20110;&#21442;&#19982;&#32773;&#36129;&#29486;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#23545;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#30830;&#23450;&#26368;&#24378;&#22823;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10135v1 Announce Type: cross  Abstract: The increasing requirements for data protection and privacy has attracted a huge research interest on distributed artificial intelligence and specifically on federated learning, an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data. In the initial proposal of federated learning the architecture was centralised and the aggregation was done with federated averaging, meaning that a central server will orchestrate the federation using the most straightforward averaging strategy. This research is focused on testing different federated strategies in a peer-to-peer environment. The authors propose various aggregation strategies for federated learning, including weighted averaging aggregation, using different factors and strategies based on participant contribution. The strategies are tested with varying data sizes to identify the most robust ones. This resear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#20010;&#24615;&#21270;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#30340;&#38376;&#27099;&#65292;&#20174;&#32780;&#23454;&#29616;&#28216;&#25103;&#20869;&#23481;&#19982;&#29609;&#23478;&#20559;&#22909;&#30340;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.10133</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#25512;&#29702;: &#26080;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#20010;&#24615;&#21270;&#20869;&#23481;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#20010;&#24615;&#21270;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#30340;&#38376;&#27099;&#65292;&#20174;&#32780;&#23454;&#29616;&#28216;&#25103;&#20869;&#23481;&#19982;&#29609;&#23478;&#20559;&#22909;&#30340;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Procedural content generation&#65288;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#65289;&#20351;&#29992;&#31639;&#27861;&#25216;&#26415;&#20197;&#26356;&#20302;&#30340;&#29983;&#20135;&#25104;&#26412;&#21019;&#24314;&#22823;&#37327;&#26032;&#20869;&#23481;&#12290;&#22312;&#36739;&#26032;&#30340;&#26041;&#27861;&#20013;&#65292;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#25910;&#38598;&#22823;&#37327;&#26114;&#36149;&#30340;&#25968;&#25454;&#65292;&#24182;&#24320;&#21457;&#21644;&#35757;&#32451;&#30456;&#23545;&#22797;&#26434;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#26082;&#32791;&#26102;&#21448;&#26114;&#36149;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#25506;&#32034;&#33021;&#21542;&#36890;&#36807;&#26356;&#23454;&#29992;&#21644;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38477;&#20302;&#20010;&#24615;&#21270;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#30340;&#38376;&#27099;&#12290;&#23558;&#28216;&#25103;&#20869;&#23481;&#19982;&#29609;&#23478;&#20559;&#22909;&#36827;&#34892;&#21305;&#37197;&#26082;&#20351;&#29609;&#23478;&#26356;&#20139;&#21463;&#28216;&#25103;&#65292;&#20063;&#20351;&#24320;&#21457;&#32773;&#26356;&#20381;&#36182;&#29609;&#23478;&#22312;&#28216;&#25103;&#24471;&#21040;&#28385;&#36275;&#20043;&#21518;&#20877;&#36827;&#34892;&#21464;&#29616;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10133v1 Announce Type: new  Abstract: Procedural content generation uses algorithmic techniques to create large amounts of new content for games at much lower production costs. In newer approaches, procedural content generation utilizes machine learning. However, these methods usually require expensive collection of large amounts of data, as well as the development and training of fairly complex learning models, which can be both extremely time-consuming and expensive. The core of our research is to explore whether we can lower the barrier to the use of personalized procedural content generation through a more practical and generalizable approach with large language models. Matching game content with player preferences benefits both players, who enjoy the game more, and developers, who increasingly depend on players enjoying the game before being able to monetize it. Therefore, this paper presents a novel approach to achieving personalization by using large language models t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#19982;&#23454;&#38469;&#25361;&#25112;&#19981;&#21305;&#37197;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10130</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#21542;&#36866;&#24212;&#29616;&#23454;&#25361;&#25112;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Continual Learning Ready for Real-world Challenges?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#19982;&#23454;&#38469;&#25361;&#25112;&#19981;&#21305;&#37197;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36830;&#32493;&#23398;&#20064;&#22312;&#23398;&#26415;&#30028;&#26377;&#30528;&#24736;&#20037;&#32780;&#33391;&#22909;&#30340;&#21382;&#21490;&#65292;&#20294;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#30456;&#23545;&#26377;&#38480;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#24046;&#36317;&#26159;&#30001;&#20110;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#19982;&#36830;&#32493;&#23398;&#20064;&#30340;&#23454;&#38469;&#25361;&#25112;&#19981;&#21305;&#37197;&#65292;&#23548;&#33268;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#26377;&#25928;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#20840;&#26032;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;OCL-3DSS&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#33258;&#24049;&#30340;&#20551;&#35774;&#24182;&#35780;&#20272;&#20102;&#36807;&#21435;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#26356;&#21152;&#29616;&#23454;&#30340;&#21327;&#35758;&#26469;&#30740;&#31350;&#25991;&#29486;&#20013;&#30340;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#38656;&#35201;&#22312;&#32447;&#21644;&#25345;&#32493;&#23398;&#20064;&#20197;&#24212;&#23545;&#21160;&#24577;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#65288;&#20363;&#22914;&#26426;&#22120;&#20154;&#21644;&#19977;&#32500;&#35270;&#35273;&#24212;&#29992;&#65289;&#12290;&#32467;&#26524;&#20196;&#20154;&#27822;&#20007;&#65306;&#25152;&#26377;&#32771;&#34385;&#30340;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#65292;&#26126;&#26174;&#20559;&#31163;&#32852;&#21512;&#31163;&#32447;&#35757;&#32451;&#30340;&#19978;&#38480;&#12290;&#36825;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#25552;&#20986;&#20102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10130v1 Announce Type: cross  Abstract: Despite continual learning's long and well-established academic history, its application in real-world scenarios remains rather limited. This paper contends that this gap is attributable to a misalignment between the actual challenges of continual learning and the evaluation protocols in use, rendering proposed solutions ineffective for addressing the complexities of real-world setups. We validate our hypothesis and assess progress to date, using a new 3D semantic segmentation benchmark, OCL-3DSS. We investigate various continual learning schemes from the literature by utilizing more realistic protocols that necessitate online and continual learning for dynamic, real-world scenarios (eg., in robotics and 3D vision applications). The outcomes are sobering: all considered methods perform poorly, significantly deviating from the upper bound of joint offline training. This raises questions about the applicability of existing methods in rea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#20174;EEG&#20449;&#21495;&#20013;&#24674;&#22797;&#20986;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#32467;&#21512;&#23545;&#25239;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.10115</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#20174;EEG&#35760;&#24405;&#20013;&#29983;&#25104;&#35270;&#35273;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#20174;EEG&#20449;&#21495;&#20013;&#24674;&#22797;&#20986;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#32467;&#21512;&#23545;&#25239;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#24863;&#30693;&#24615;&#33041;&#35299;&#30721;&#39046;&#22495;&#30340;&#19968;&#20010;&#29616;&#20195;&#30740;&#31350;&#25361;&#25112;&#65292;&#21363;&#20351;&#29992;&#23545;&#25239;&#24335;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20174;EEG&#20449;&#21495;&#20013;&#21512;&#25104;&#22270;&#20687;&#12290;&#20855;&#20307;&#30446;&#26631;&#26159;&#21033;&#29992;&#20027;&#20307;&#35266;&#30475;&#22270;&#20687;&#26102;&#33719;&#24471;&#30340;EEG&#35760;&#24405;&#37325;&#26032;&#21019;&#24314;&#23646;&#20110;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#29983;&#25104;EEG&#32534;&#30721;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;GAN&#32593;&#32476;&#30340;&#29983;&#25104;&#22120;&#32452;&#20214;&#30340;&#36755;&#20837;&#12290;&#38500;&#20102;&#23545;&#25239;&#25439;&#22833;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10115v1 Announce Type: new  Abstract: In this study, we tackle a modern research challenge within the field of perceptual brain decoding, which revolves around synthesizing images from EEG signals using an adversarial deep learning framework. The specific objective is to recreate images belonging to various object categories by leveraging EEG recordings obtained while subjects view those images. To achieve this, we employ a Transformer-encoder based EEG encoder to produce EEG encodings, which serve as inputs to the generator component of the GAN network. Alongside the adversarial loss, we also incorporate perceptual loss to enhance the quality of the generated images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;</title><link>https://arxiv.org/abs/2402.10110</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65306;LLM&#25351;&#20196;&#35843;&#33410;&#30340;&#23398;&#29983;&#36873;&#25321;&#25968;&#25454;&#22238;&#25910;
&lt;/p&gt;
&lt;p&gt;
Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#33410;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35828;&#38750;&#24120;&#20851;&#38190;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25351;&#20196;&#36319;&#36394;&#21644;&#20219;&#21153;&#36866;&#24212;&#33021;&#21147;&#65292;&#20294;&#20854;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#37117;&#33268;&#21147;&#20110;&#25913;&#36827;&#25968;&#25454;&#36136;&#37327;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#25968;&#25454;&#19982;&#27491;&#22312;&#24494;&#35843;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#8212;&#8212;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65292;&#36890;&#36807;&#32467;&#21512;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#65292;&#20197;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#12290;&#36825;&#31181;&#24072;&#29983;&#21512;&#20316;&#20135;&#29983;&#20102;&#39640;&#36136;&#37327;&#19988;&#19982;&#23398;&#29983;LLM&#20860;&#23481;&#30340;&#25351;&#20196;&#21709;&#24212;&#23545;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#21644;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#24120;&#33021;&#25913;&#21892;LLM&#24494;&#35843;&#21644;&#33258;&#25105;&#20248;&#21270;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10110v1 Announce Type: cross  Abstract: Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10109</link><description>&lt;p&gt;
&#29992;&#21487;&#35299;&#37322;&#30340;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Towards Reducing Diagnostic Errors with Interpretable Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35786;&#26029;&#38169;&#35823;&#21457;&#29983;&#26159;&#22240;&#20026;&#20020;&#24202;&#21307;&#29983;&#26080;&#27861;&#36731;&#26131;&#33719;&#21462;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#26469;&#36827;&#34892;&#24102;&#26377;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#30340;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#30340;&#39044;&#27979;&#65292;&#22312;&#20020;&#24202;&#21307;&#29983;&#20173;&#28982;&#19981;&#30830;&#23450;&#30340;&#26102;&#38388;&#28857;&#19978;&#65292;&#26088;&#22312;&#29305;&#21035;&#20943;&#36731;&#35786;&#26029;&#24310;&#36831;&#21644;&#28304;&#20110;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#65292;&#38656;&#35201;&#25512;&#26029;&#20986;&#20107;&#20214;&#24615;&#30340;&#8220;&#30495;&#23454;&#8221;&#35786;&#26029;&#30340;&#26102;&#38388;&#31890;&#24230;&#32454;&#33268;&#30340;&#22238;&#39038;&#24615;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#20445;&#35777;&#36755;&#20837;&#25991;&#26412;&#26159;&#22312;&#21487;&#20197;&#36827;&#34892;&#33258;&#20449;&#30340;&#35786;&#26029;&#20043;&#21069;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#26816;&#32034;&#21021;&#22987;&#30340;&#35777;&#25454;&#27744;&#65292;&#28982;&#21518;&#36827;&#34892;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10109v1 Announce Type: new  Abstract: Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#23884;&#20837;&#21487;&#25511;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;QE-CDLM&#65289;&#65292;&#36890;&#36807;&#37325;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23884;&#20837;&#31354;&#38388;&#26469;&#25913;&#21892;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12289;&#21487;&#31227;&#26893;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10107</link><description>&lt;p&gt;
&#21487;&#25511;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#23884;&#20837;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Quantized Embedding Vectors for Controllable Diffusion Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#23884;&#20837;&#21487;&#25511;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;QE-CDLM&#65289;&#65292;&#36890;&#36807;&#37325;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23884;&#20837;&#31354;&#38388;&#26469;&#25913;&#21892;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12289;&#21487;&#31227;&#26893;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#21892;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12289;&#21487;&#31227;&#26893;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#20173;&#28982;&#38750;&#24120;&#33499;&#21051;&#65292;&#26080;&#27861;&#28385;&#36275;&#39044;&#26399;&#65292;&#36825;&#33258;&#28982;&#23548;&#33268;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#31283;&#23450;&#24615;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#37327;&#21270;&#23884;&#20837;&#21487;&#25511;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;QE-CDLM&#65289;&#12290;QE-CDLM&#22522;&#20110;&#26368;&#36817;&#25104;&#21151;&#30340;&#21487;&#25511;DLM&#65292;&#36890;&#36807;&#37327;&#21270;&#37325;&#24314;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#29983;&#25104;&#24335;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10107v1 Announce Type: cross  Abstract: Improving the controllability, portability, and inference speed of diffusion language models (DLMs) is a key challenge in natural language generation. While recent research has shown significant success in complex text generation with language models, the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models. To mitigate these issues, numerous well-established methods were proposed for neural network quantization. To further enhance their portability of independent deployment as well as improve their stability evaluated by language perplexity, we propose a novel approach called the Quantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM builds upon the recent successful controllable DLMs by remodeling the task-specific embedding space via quantization. This leads to a gradient-based controller for the generat
&lt;/p&gt;</description></item><item><title>GeoEval&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;MMs&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;WizardMath&#27169;&#22411;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.10104</link><description>&lt;p&gt;
GeoEval&#65306;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;&#22810;&#27169;&#22411;&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10104
&lt;/p&gt;
&lt;p&gt;
GeoEval&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;MMs&#22312;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;WizardMath&#27169;&#22411;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22810;&#27169;&#22411;&#65288;MMs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#20960;&#20309;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#21363;&#38656;&#35201;&#32508;&#21512;&#29702;&#35299;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;GeoEval&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38598;&#21512;&#65292;&#21253;&#25324;&#19968;&#20010;&#20027;&#35201;&#23376;&#38598;&#21512;&#30340;2000&#20010;&#38382;&#39064;&#65292;&#19968;&#20010;&#37325;&#28857;&#20851;&#27880;&#21453;&#25512;&#29702;&#30340;750&#20010;&#38382;&#39064;&#23376;&#38598;&#21512;&#65292;&#19968;&#20010;&#22686;&#24378;&#23376;&#38598;&#21512;&#30340;2000&#20010;&#38382;&#39064;&#20197;&#21450;&#19968;&#20010;&#38590;&#39064;&#23376;&#38598;&#21512;&#30340;300&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;LLMs&#21644;MMs&#22312;&#35299;&#20915;&#20960;&#20309;&#25968;&#23398;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#21313;&#20010;LLMs&#21644;MMs&#22312;&#36825;&#20123;&#19981;&#21516;&#23376;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;WizardMath&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#20027;&#35201;&#23376;&#38598;&#19978;&#36798;&#21040;55.67%&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#19978;&#21482;&#26377;6.00%&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#31361;&#20986;&#20102;&#20851;&#38190;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10104v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) and Multi-Modal Models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward reasoning, an augmented subset of 2000 problems, and a hard subset of 300 problems. This benchmark facilitates a deeper investigation into the performance of LLMs and MMs on solving geometry math problems. Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67\% accuracy rate on the main subset but only a 6.00\% accuracy on the challenging subset. This highlights the critical ne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#12289;&#20998;&#24067;&#24335;&#30340;&#12289;&#21512;&#20316;&#30340;&#22522;&#20110;FCM&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30284;&#30151;&#30740;&#31350;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10102</link><description>&lt;p&gt;
&#19968;&#20010;&#38544;&#31169;&#20445;&#25252;&#30340;&#12289;&#20998;&#24067;&#24335;&#30340;&#12289;&#21512;&#20316;&#30340;&#22522;&#20110;FCM&#30340;&#30284;&#30151;&#30740;&#31350;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#12289;&#20998;&#24067;&#24335;&#30340;&#12289;&#21512;&#20316;&#30340;&#22522;&#20110;FCM&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30284;&#30151;&#30740;&#31350;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#31890;&#23376;&#32676;&#20248;&#21270;&#30340;&#27169;&#31946;&#35748;&#30693;&#22270;&#12290;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#31181;&#21327;&#20316;FCM&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#31526;&#21512;&#24403;&#21069;&#35268;&#23450;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#30284;&#30151;&#26816;&#27979;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#20102;&#31867;&#20284;&#20110;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10102v1 Announce Type: new  Abstract: Distributed Artificial Intelligence is attracting interest day by day. In this paper, the authors introduce an innovative methodology for distributed learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a privacy-preserving way. The authors design a training scheme for collaborative FCM learning that offers data privacy compliant with the current regulation. This method is applied to a cancer detection problem, proving that the performance of the model is improved by the Federated Learning process, and obtaining similar results to the ones that can be found in the literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#26080;&#20154;&#26426;&#25805;&#20316;&#21592;&#22312;&#22810;&#20010;&#23548;&#24377;&#23041;&#32961;&#19979;&#36827;&#34892;&#20915;&#31574;&#65292;&#36890;&#36807;&#23398;&#20064;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#26469;&#35780;&#20272;&#21508;&#31181;&#31574;&#30053;&#30340;&#39118;&#38505;&#65292;&#24182;&#24314;&#35758;&#26368;&#23433;&#20840;&#30340;&#34892;&#21160;&#26041;&#38024;&#12290;</title><link>https://arxiv.org/abs/2402.10101</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#23548;&#24377;&#36991;&#35753;&#24773;&#20917;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Situation Awareness for Multiple Missiles Evasion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#26080;&#20154;&#26426;&#25805;&#20316;&#21592;&#22312;&#22810;&#20010;&#23548;&#24377;&#23041;&#32961;&#19979;&#36827;&#34892;&#20915;&#31574;&#65292;&#36890;&#36807;&#23398;&#20064;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#26469;&#35780;&#20272;&#21508;&#31181;&#31574;&#30053;&#30340;&#39118;&#38505;&#65292;&#24182;&#24314;&#35758;&#26368;&#23433;&#20840;&#30340;&#34892;&#21160;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31354;&#23545;&#31354;&#23548;&#24377;&#30340;&#26377;&#25928;&#23556;&#31243;&#22686;&#21152;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#38590;&#20197;&#20445;&#25345;&#26080;&#20154;&#26426;&#25152;&#38656;&#30340;&#24773;&#20917;&#24863;&#30693;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20197;&#24110;&#21161;&#26080;&#20154;&#26426;&#25805;&#20316;&#21592;&#22312;&#35270;&#32447;&#22806;&#65288;BVR&#65289;&#31354;&#25112;&#24773;&#26223;&#20013;&#35780;&#20272;&#19981;&#21516;&#36873;&#25321;&#30340;&#39118;&#38505;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#36873;&#25321;&#20570;&#20986;&#20915;&#31574;&#12290;&#26089;&#26399;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#21333;&#19968;&#23548;&#24377;&#30340;&#23041;&#32961;&#65292;&#32780;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#24819;&#27861;&#25299;&#23637;&#21040;&#22810;&#20010;&#23548;&#24377;&#23041;&#32961;&#19978;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#23398;&#20064;&#65292;&#20026;&#25805;&#20316;&#21592;&#25552;&#20379;&#19968;&#32452;&#19981;&#21516;&#31574;&#30053;&#30340;&#32467;&#26524;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#26469;&#34989;&#23548;&#24377;&#65292;&#35780;&#20272;&#19968;&#31995;&#21015;&#36873;&#39033;&#65292;&#24182;&#25512;&#33616;&#39118;&#38505;&#26368;&#23567;&#30340;&#34892;&#21160;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10101v1 Announce Type: cross  Abstract: As the effective range of air-to-air missiles increases, it becomes harder for human operators to maintain the situational awareness needed to keep a UAV safe. In this work, we propose a decision support tool to help UAV operators in Beyond Visual Range (BVR) air combat scenarios assess the risks of different options and make decisions based on those. Earlier work focused on the threat posed by a single missile, and in this work, we extend the ideas to several missile threats. The proposed method uses Deep Neural Networks (DNN) to learn from high-fidelity simulations to provide the operator with an outcome estimate for a set of different strategies. Our results demonstrate that the proposed system can manage multiple incoming missiles, evaluate a family of options, and recommend the least risky course of action.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.10100</link><description>&lt;p&gt;
&#35843;&#35856;&#65306;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#38480;&#21046;&#26465;&#20214;&#26159;&#20197;&#21453;&#26144;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#25910;&#38598;&#30340;&#23567;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21253;&#25324;DenseNet&#21644;ConvNeXt&#22312;&#20869;&#30340;CNN&#27169;&#22411;&#65292;&#20197;&#21450;ViT&#12289;SWIN&#21644;AST&#31561;&#36716;&#25442;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35832;&#22914;YAMNet&#21644;VGGish&#30340;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#20020;&#24202;&#25968;&#25454;&#19978;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#20174;&#21330;&#20013;&#24739;&#32773;&#20013;&#26032;&#25910;&#38598;&#20102;&#20004;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#24739;&#32773;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#21457;&#29616;&#22522;&#20110;&#23427;&#20204;&#20174;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;RGB&#21644;&#28784;&#24230;&#35889;&#22270;&#36716;&#25442;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#21487;&#20197;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#65292;&#20854;&#20013;DenseNet-Contrastive&#21644;AST&#27169;&#22411;&#34920;&#29616;&#31361;&#20986;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10100v1 Announce Type: cross  Abstract: This study assesses deep learning models for audio classification in a clinical setting with the constraint of small datasets reflecting real-world prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt, alongside transformer models like ViT, SWIN, and AST, and compare them against pre-trained audio models such as YAMNet and VGGish. Our method highlights the benefits of pre-training on large datasets before fine-tuning on specific clinical data. We prospectively collected two first-of-their-kind patient audio datasets from stroke patients. We investigated various preprocessing techniques, finding that RGB and grayscale spectrogram transformations affect model performance differently based on the priors they learn from pre-training. Our findings indicate CNNs can match or exceed transformer models in small dataset contexts, with DenseNet-Contrastive and AST models showing notable performance. This study highlights
&lt;/p&gt;</description></item><item><title>MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10093</link><description>&lt;p&gt;
MIM-Refiner&#65306;&#19968;&#31181;&#20174;&#20013;&#38388;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#33719;&#24471;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10093
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MIM-Refiner&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#35757;&#32451;MIM&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#12290;MIM-Refiner&#30340;&#21160;&#26426;&#22312;&#20110;MIM&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#34920;&#31034;&#36890;&#24120;&#20301;&#20110;&#20013;&#38388;&#23618;&#12290;&#22240;&#27492;&#65292;MIM-Refiner&#21033;&#29992;&#36830;&#25509;&#21040;&#19981;&#21516;&#20013;&#38388;&#23618;&#30340;&#22810;&#20010;&#23545;&#27604;&#22836;&#12290;&#22312;&#27599;&#20010;&#22836;&#20013;&#65292;&#20462;&#25913;&#21518;&#30340;&#26368;&#36817;&#37051;&#30446;&#26631;&#24110;&#21161;&#26500;&#24314;&#30456;&#24212;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;&#27492;&#36807;&#31243;&#30701;&#32780;&#26377;&#25928;&#65292;&#22312;&#20960;&#20010;epochs&#20869;&#65292;&#25105;&#20204;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#12290;&#20351;&#29992;data2vec 2.0&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;ViT-H&#32463;&#36807;&#25913;&#36827;&#21518;&#65292;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#20302;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65288;&#20998;&#21035;&#20026;84.7%&#21644;64.2%&#65289;&#65292;&#36229;&#36807;&#20102;&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10093v1 Announce Type: cross  Abstract: We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21322;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#20135;&#21697;&#21305;&#37197;&#30340;&#26032;&#24605;&#36335;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26080;&#30417;&#30563;&#21305;&#37197;&#19982;&#23569;&#37327;&#27880;&#37322;&#26679;&#26412;&#30340;&#20135;&#21697;&#38142;&#25509;&#21487;&#20197;&#25104;&#20026;&#20027;&#23548;&#30340;&#30417;&#30563;&#31574;&#30053;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10091</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#20135;&#21697;&#21305;&#37197;--&#21322;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Text-Based Product Matching -- Semi-Supervised Clustering Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21322;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#20135;&#21697;&#21305;&#37197;&#30340;&#26032;&#24605;&#36335;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26080;&#30417;&#30563;&#21305;&#37197;&#19982;&#23569;&#37327;&#27880;&#37322;&#26679;&#26412;&#30340;&#20135;&#21697;&#38142;&#25509;&#21487;&#20197;&#25104;&#20026;&#20027;&#23548;&#30340;&#30417;&#30563;&#31574;&#30053;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#65292;&#21305;&#37197;&#22810;&#20010;&#20135;&#21697;&#25552;&#20379;&#20013;&#30456;&#21516;&#30340;&#20135;&#21697;&#26159;&#19968;&#20010;&#20851;&#38190;&#35201;&#32032;&#65292;&#22914;&#27604;&#36739;&#20135;&#21697;&#20379;&#24212;&#12289;&#21160;&#24577;&#20215;&#26684;&#20248;&#21270;&#21644;&#36873;&#25321;&#20026;&#23458;&#25143;&#20010;&#24615;&#21270;&#23450;&#21046;&#30340;&#20135;&#21697;&#32452;&#21512;&#12290;&#23427;&#23545;&#24212;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#23454;&#20307;&#21305;&#37197;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#20855;&#26377;&#20854;&#33258;&#36523;&#30340;&#29305;&#27530;&#24615;&#65292;&#22914;&#26080;&#22788;&#19981;&#22312;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#25110;&#19981;&#20934;&#30830;&#21644;&#19981;&#19968;&#33268;&#30340;&#20135;&#21697;&#25551;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21322;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#20135;&#21697;&#21305;&#37197;&#30340;&#26032;&#24605;&#36335;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20027;&#35201;&#26159;&#25991;&#26412;&#29305;&#24449;&#21644;&#27169;&#31946;&#23383;&#31526;&#20018;&#21305;&#37197;&#30340;IDEC&#31639;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#26356;&#22810;&#26631;&#20934;&#26041;&#27861;&#20316;&#20026;&#21442;&#32771;&#65292;&#26469;&#30740;&#31350;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#26080;&#30417;&#30563;&#21305;&#37197;&#32467;&#21512;&#23569;&#37327;&#27880;&#37322;&#26679;&#26412;&#30340;&#20135;&#21697;&#38142;&#25509;&#21487;&#33021;&#26159;&#19968;&#31181;&#21487;&#33021;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20027;&#23548;&#30340;&#30417;&#30563;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10091v1 Announce Type: cross  Abstract: Matching identical products present in multiple product feeds constitutes a crucial element of many tasks of e-commerce, such as comparing product offerings, dynamic price optimization, and selecting the assortment personalized for the client. It corresponds to the well-known machine learning task of entity matching, with its own specificity, like omnipresent unstructured data or inaccurate and inconsistent product descriptions. This paper aims to present a new philosophy to product matching utilizing a semi-supervised clustering approach. We study the properties of this method by experimenting with the IDEC algorithm on the real-world dataset using predominantly textual features and fuzzy string matching, with more standard approaches as a point of reference. Encouraging results show that unsupervised matching, enriched with a small annotated sample of product links, could be a possible alternative to the dominant supervised strategy,
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10086</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10086
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20854;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#30340;&#24212;&#29992;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38590;&#20197;&#29702;&#35299;&#30340;AI&#31995;&#32479;&#21152;&#21095;&#20102;&#23545;AD&#23433;&#20840;&#20445;&#35777;&#30340;&#29616;&#26377;&#25361;&#25112;&#12290;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#26041;&#27861;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20840;&#38754;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#22312;AD&#32972;&#26223;&#19979;AI&#30340;&#35201;&#27714;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#26426;&#26500;&#36825;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;XAI&#23545;&#20110;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;AI&#20013;&#35299;&#37322;&#30340;&#26469;&#28304;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;XAI&#30340;&#20998;&#31867;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;XAI&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20116;&#20010;&#20027;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#65292;&#36741;&#21161;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10086v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) shows promising applications for the perception and planning tasks in autonomous driving (AD) due to its superior performance compared to conventional methods. However, inscrutable AI systems exacerbate the existing challenge of safety assurance of AD. One way to mitigate this challenge is to utilize explainable AI (XAI) techniques. To this end, we present the first comprehensive systematic literature review of explainable methods for safe and trustworthy AD. We begin by analyzing the requirements for AI in the context of AD, focusing on three key aspects: data, model, and agency. We find that XAI is fundamental to meeting these requirements. Based on this, we explain the sources of explanations in AI and describe a taxonomy of XAI. We then identify five key contributions of XAI for safe and trustworthy AI in AD, which are interpretable design, interpretable surrogate models, interpretable monitoring, auxil
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-4&#30340;LLM&#35780;&#20272;&#30340;&#20020;&#24202;&#19968;&#33268;&#24615;&#65292;&#20197;&#35780;&#20272;&#32463;&#36807;&#31934;&#35843;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#30340;&#30524;&#31185;&#24739;&#32773;&#26597;&#35810;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#19982;&#21307;&#29983;&#25490;&#24207;&#36827;&#34892;&#23545;&#27604;&#65292;&#21457;&#29616;GPT-3.5&#22312;&#20020;&#24202;&#19978;&#30340;&#19968;&#33268;&#24615;&#27604;&#20854;&#20182;&#32463;&#36807;&#31934;&#35843;&#30340;LLM&#26356;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.10083</link><description>&lt;p&gt;
&#22312;&#30524;&#31185;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;GPT-4&#36827;&#34892;LLM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-4&#30340;LLM&#35780;&#20272;&#30340;&#20020;&#24202;&#19968;&#33268;&#24615;&#65292;&#20197;&#35780;&#20272;&#32463;&#36807;&#31934;&#35843;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#30340;&#30524;&#31185;&#24739;&#32773;&#26597;&#35810;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#19982;&#21307;&#29983;&#25490;&#24207;&#36827;&#34892;&#23545;&#27604;&#65292;&#21457;&#29616;GPT-3.5&#22312;&#20020;&#24202;&#19978;&#30340;&#19968;&#33268;&#24615;&#27604;&#20854;&#20182;&#32463;&#36807;&#31934;&#35843;&#30340;LLM&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#35780;&#20272;&#22522;&#20110;GPT-4&#30340;&#35780;&#20272;&#19982;&#20154;&#31867;&#20020;&#24202;&#19987;&#23478;&#23545;&#32463;&#36807;&#31934;&#35843;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#30340;&#30524;&#31185;&#30456;&#20851;&#24739;&#32773;&#26597;&#35810;&#30340;&#22238;&#31572;&#30340;&#19968;&#33268;&#24615;&#12290;&#26041;&#27861;&#65306;400&#20010;&#30524;&#31185;&#38382;&#39064;&#21644;&#37197;&#23545;&#31572;&#26696;&#30001;&#30524;&#31185;&#21307;&#29983;&#21019;&#24314;&#65292;&#20197;&#20195;&#34920;&#24120;&#35265;&#30340;&#24739;&#32773;&#38382;&#39064;&#65292;&#20998;&#20026;&#29992;&#20110;&#24494;&#35843;&#30340;368&#20010;&#65288;92&#65285;&#65289;&#21644;&#27979;&#35797;&#30340;40&#20010;&#65288;8&#65285;&#65289;&#12290;&#25105;&#20204;&#23545;5&#20010;&#19981;&#21516;&#30340;LLM&#36827;&#34892;&#20102;&#31934;&#35843;&#65292;&#21253;&#25324;LLAMA2-7b&#65292;LLAMA2-7b-Chat&#65292;LLAMA2-13b&#21644;LLAMA2-13b-Chat&#12290;&#23545;&#20110;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#36824;&#21253;&#25324;8&#20010;&#38738;&#20809;&#30524;&#38382;&#31572;&#23545;&#12290;&#30001;5&#20010;&#32463;&#36807;&#31934;&#35843;&#30340;LLM&#29983;&#25104;&#20102;200&#20010;&#23545;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#22238;&#31572;&#29992;&#20110;&#35780;&#20272;&#12290;&#37319;&#29992;&#23450;&#21046;&#30340;&#20020;&#24202;&#35780;&#20272;&#25351;&#26631;&#26469;&#25351;&#23548;GPT-4&#30340;&#35780;&#20272;&#65292;&#20197;&#30830;&#20445;&#20020;&#24202;&#20934;&#30830;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#24739;&#32773;&#23433;&#20840;&#24615;&#21644;&#26131;&#29702;&#35299;&#24615;&#12290;&#28982;&#21518;&#23558;GPT-4&#30340;&#35780;&#20272;&#19982;5&#21517;&#20020;&#24202;&#21307;&#29983;&#30340;&#25490;&#24207;&#36827;&#34892;&#23545;&#27604;&#20197;&#35780;&#20272;&#20854;&#20020;&#24202;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#65306;&#22312;&#25152;&#26377;&#32463;&#36807;&#31934;&#35843;&#30340;LLM&#20013;&#65292;GPT-3.5
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10083v1 Announce Type: new  Abstract: Purpose: To assess the alignment of GPT-4-based evaluation to human clinician experts, for the evaluation of responses to ophthalmology-related patient queries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology questions and paired answers were created by ophthalmologists to represent commonly asked patient questions, divided into fine-tuning (368; 92%), and testing (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b, LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset, additional 8 glaucoma QnA pairs were included. 200 responses to the testing dataset were generated by 5 fine-tuned LLMs for evaluation. A customized clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on clinical accuracy, relevance, patient safety, and ease of understanding. GPT-4 evaluation was then compared against ranking by 5 clinicians for clinical alignment. Results: Among all fine-tuned LLMs, GPT-3.5
&lt;/p&gt;</description></item><item><title>QUICK&#26159;&#19968;&#32452;&#38024;&#23545;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39640;&#25928;&#25512;&#29702;&#30340;&#20248;&#21270;CUDA&#20869;&#26680;&#12290;&#36890;&#36807;&#35299;&#20915;&#20849;&#20139;&#20869;&#23384;&#20914;&#31361;&#38382;&#39064;&#21644;&#20132;&#38169;&#37327;&#21270;&#26435;&#37325;&#30697;&#38453;&#65292;QUICK&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#21644;&#21534;&#21520;&#37327;&#22686;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.10076</link><description>&lt;p&gt;
QUICK&#65306;&#38024;&#23545;&#39640;&#25928;LLM&#25512;&#29702;&#30340;&#37327;&#21270;&#24863;&#30693;&#20132;&#38169;&#21644;&#26080;&#20914;&#31361;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10076
&lt;/p&gt;
&lt;p&gt;
QUICK&#26159;&#19968;&#32452;&#38024;&#23545;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39640;&#25928;&#25512;&#29702;&#30340;&#20248;&#21270;CUDA&#20869;&#26680;&#12290;&#36890;&#36807;&#35299;&#20915;&#20849;&#20139;&#20869;&#23384;&#20914;&#31361;&#38382;&#39064;&#21644;&#20132;&#38169;&#37327;&#21270;&#26435;&#37325;&#30697;&#38453;&#65292;QUICK&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#21644;&#21534;&#21520;&#37327;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;QUICK&#65292;&#19968;&#32452;&#29992;&#20110;&#39640;&#25928;&#25512;&#29702;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20248;&#21270;CUDA&#20869;&#26680;&#12290;QUICK&#35299;&#20915;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#30697;&#38453;&#20056;&#27861;&#20869;&#26680;&#30340;&#20849;&#20139;&#20869;&#23384;&#20914;&#31361;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31163;&#32447;&#24773;&#20917;&#19979;&#20132;&#38169;LLMs&#30340;&#37327;&#21270;&#26435;&#37325;&#30697;&#38453;&#65292;&#20174;&#32780;&#36339;&#36807;&#35299;&#37327;&#21270;&#21518;&#30340;&#20849;&#20139;&#20869;&#23384;&#20889;&#22238;&#12290;&#25105;&#20204;&#22312;&#36739;&#22823;&#25209;&#27425;&#19978;&#23637;&#31034;&#20102;&#19982;AutoAWQ&#29616;&#26377;&#20869;&#26680;&#30456;&#27604;&#22810;&#36798;1.91&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#24182;&#22312;&#21508;&#31181;NVIDIA GPU&#35774;&#22791;&#19978;&#30340;&#20195;&#34920;&#24615;LLM&#27169;&#22411;&#19978;&#33719;&#24471;&#20102;&#22810;&#36798;1.94&#20493;&#30340;&#21534;&#21520;&#37327;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10076v1 Announce Type: cross  Abstract: We introduce QUICK, a group of novel optimized CUDA kernels for the efficient inference of quantized Large Language Models (LLMs). QUICK addresses the shared memory bank-conflict problem of state-of-the-art mixed precision matrix multiplication kernels. Our method interleaves the quantized weight matrices of LLMs offline to skip the shared memory write-back after the dequantization. We demonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger batches and up to 1.94x throughput gain on representative LLM models on various NVIDIA GPU devices.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#31574;&#30053;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#24212;&#29992;&#24847;&#22270;&#31649;&#29702;&#12290;&#36890;&#36807;&#29983;&#25104;&#36880;&#27493;&#20998;&#35299;&#24847;&#22270;&#25152;&#38656;&#30340;&#21160;&#20316;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;API&#65292;&#23454;&#29616;&#20102;&#38381;&#25511;&#21046;&#24490;&#29615;&#26469;&#33258;&#21160;&#21270;&#31574;&#30053;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.10067</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#24847;&#22270;&#31649;&#29702;&#30340;&#31574;&#30053;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLM-based policy generation for intent-based management of applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10067
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#31574;&#30053;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#24212;&#29992;&#24847;&#22270;&#31649;&#29702;&#12290;&#36890;&#36807;&#29983;&#25104;&#36880;&#27493;&#20998;&#35299;&#24847;&#22270;&#25152;&#38656;&#30340;&#21160;&#20316;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;API&#65292;&#23454;&#29616;&#20102;&#38381;&#25511;&#21046;&#24490;&#29615;&#26469;&#33258;&#21160;&#21270;&#31574;&#30053;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#31649;&#29702;&#38656;&#35201;&#23558;&#39640;&#32423;&#29992;&#25143;&#35831;&#27714;&#65292;&#20363;&#22914;&#24847;&#22270;&#65292;&#20998;&#35299;&#25104;&#31995;&#32479;&#21487;&#20197;&#29702;&#35299;&#21644;&#25191;&#34892;&#30340;&#25277;&#35937;&#12290;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#21363;&#20351;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#24847;&#22270;&#20063;&#38656;&#35201;&#25191;&#34892;&#19968;&#31995;&#21015;&#26377;&#24207;&#30340;&#27493;&#39588;&#12290;&#32780;&#35782;&#21035;&#21644;&#36866;&#24212;&#36825;&#20123;&#27493;&#39588;&#65288;&#38543;&#30528;&#26465;&#20214;&#30340;&#21464;&#21270;&#65289;&#30340;&#20219;&#21153;&#38656;&#35201;&#19968;&#31181;&#26080;&#27861;&#20107;&#20808;&#23436;&#20840;&#23450;&#20041;&#30340;&#20998;&#35299;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#25903;&#25345;&#33258;&#21160;&#21270;&#30340;&#24847;&#22270;&#20998;&#35299;&#21644;&#25191;&#34892;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31649;&#36947;&#65292;&#36890;&#36807;&#29983;&#25104;&#25152;&#38656;&#30340;&#21160;&#20316;&#65292;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#30340;&#25277;&#35937;&#36880;&#27493;&#20998;&#35299;&#24847;&#22270;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21019;&#24314;&#29992;&#20110;&#24847;&#22270;&#37096;&#32626;&#30340;&#38381;&#25511;&#21046;&#24490;&#29615;&#26469;&#33258;&#21160;&#21270;&#31574;&#30053;&#25191;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#29983;&#25104;&#24182;&#23558;&#31574;&#30053;&#26144;&#23556;&#21040;API&#65292;&#24182;&#24418;&#25104;&#25191;&#34892;&#25152;&#38656;&#30340;&#30417;&#25511;&#12289;&#20998;&#26512;&#12289;&#35745;&#21010;&#21644;&#25191;&#34892;&#30340;&#24212;&#29992;&#31649;&#29702;&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10067v1 Announce Type: cross  Abstract: Automated management requires decomposing high-level user requests, such as intents, to an abstraction that the system can understand and execute. This is challenging because even a simple intent requires performing a number of ordered steps. And the task of identifying and adapting these steps (as conditions change) requires a decomposition approach that cannot be exactly pre-defined beforehand. To tackle these challenges and support automated intent decomposition and execution, we explore the few-shot capability of Large Language Models (LLMs). We propose a pipeline that progressively decomposes intents by generating the required actions using a policy-based abstraction. This allows us to automate the policy execution by creating a closed control loop for the intent deployment. To do so, we generate and map the policies to APIs and form application management loops that perform the necessary monitoring, analysis, planning and executi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#20363;&#20998;&#21106;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#21322;&#33258;&#21160;&#34880;&#31649;&#36861;&#36394;&#31639;&#27861;&#65292;&#33021;&#22815;&#36861;&#36394;&#27599;&#26869;&#34880;&#31649;&#26641;&#30340;&#20998;&#25903;&#12290;</title><link>https://arxiv.org/abs/2402.10055</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#20363;&#20998;&#21106;&#31070;&#32463;&#32593;&#32476;&#22312;&#20154;&#31867;&#35270;&#32593;&#33180;&#22270;&#20687;&#20013;&#23454;&#29616;&#40065;&#26834;&#30340;&#21322;&#33258;&#21160;&#34880;&#31649;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10055
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#20363;&#20998;&#21106;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#21322;&#33258;&#21160;&#34880;&#31649;&#36861;&#36394;&#31639;&#27861;&#65292;&#33021;&#22815;&#36861;&#36394;&#27599;&#26869;&#34880;&#31649;&#26641;&#30340;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#24449;&#31070;&#32463;&#32593;&#32476;&#65288;InSegNN&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23454;&#20363;&#20998;&#21106;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20154;&#31867;&#30524;&#24213;&#22270;&#20687;&#19978;&#23454;&#29616;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#21322;&#33258;&#21160;&#34880;&#31649;&#36861;&#36394;&#31639;&#27861;&#12290;&#19982;&#35821;&#20041;&#20998;&#21106;&#19981;&#21516;&#65292;InSegNN&#33021;&#22815;&#21333;&#29420;&#20998;&#31163;&#21644;&#26631;&#35760;&#19981;&#21516;&#30340;&#34880;&#31649;&#26641;&#65292;&#24182;&#19988;&#33021;&#22815;&#36861;&#36394;&#27599;&#26869;&#26641;&#30340;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10055v1 Announce Type: cross  Abstract: The morphology and hierarchy of the vascular systems are essential for perfusion in supporting metabolism. In human retina, one of the most energy-demanding organs, retinal circulation nourishes the entire inner retina by an intricate vasculature emerging and remerging at the optic nerve head (ONH). Thus, tracing the vascular branching from ONH through the vascular tree can illustrate vascular hierarchy and allow detailed morphological quantification, and yet remains a challenging task. Here, we presented a novel approach for a robust semi-automatic vessel tracing algorithm on human fundus images by an instance segmentation neural network (InSegNN). Distinct from semantic segmentation, InSegNN separates and labels different vascular trees individually and therefore enable tracing each tree throughout its branching. We have built-in three strategies to improve robustness and accuracy with temporal learning, spatial multi-sampling, and d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#21644;&#26377;&#24847;&#35782;&#30340;&#24819;&#35937;&#65292;&#26377;&#25928;&#22320;&#36951;&#24536;&#30446;&#26631;&#25991;&#26412;&#65292;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20445;&#30041;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10052</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#21644;&#26377;&#24847;&#35782;&#30340;&#24819;&#35937;&#36827;&#34892;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#21644;&#26377;&#24847;&#35782;&#30340;&#24819;&#35937;&#65292;&#26377;&#25928;&#22320;&#36951;&#24536;&#30446;&#26631;&#25991;&#26412;&#65292;&#24182;&#22312;&#29983;&#25104;&#20219;&#21153;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20445;&#30041;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20173;&#28982;&#23384;&#22312;&#38544;&#31169;&#20405;&#29359;&#21644;&#25935;&#24863;&#25968;&#25454;&#19981;&#21463;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;LLM&#36951;&#24536;&#30340;&#36807;&#31243;&#20013;&#37319;&#29992;&#26377;&#24847;&#35782;&#30340;&#24819;&#35937;&#12290;&#25105;&#20204;&#19981;&#26159;&#35797;&#22270;&#24536;&#35760;&#24050;&#35760;&#24518;&#30340;&#25968;&#25454;&#65292;&#32780;&#26159;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#30340;&#26694;&#26550;&#24341;&#23548;LLM&#26377;&#24847;&#35782;&#22320;&#24819;&#35937;&#26367;&#20195;&#24773;&#22659;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#26377;&#25928;&#22320;&#36951;&#24536;&#30446;&#26631;&#25991;&#26412;&#65292;&#36824;&#21487;&#20197;&#20445;&#30041;LLM&#22312;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#27169;&#22411;&#21644;&#35268;&#27169;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10052v1 Announce Type: cross  Abstract: While displaying impressive generation capabilities across many tasks, Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data. This raises an essential question: how should we prevent such undesired behavior of LLMs while maintaining their strong generation and natural language understanding (NLU) capabilities? In this work, we introduce a novel approach termed deliberate imagination in the context of LLM unlearning. Instead of trying to forget memorized data, we employ a self-distillation framework, guiding LLMs to deliberately imagine alternative scenarios. As demonstrated in a wide range of experiments, the proposed method not only effectively unlearns targeted text but also preserves the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks. Our results demonstrate the usefulness of this approach across different models and sizes, and also wit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#29615;&#22659;&#30340;&#22522;&#20110;LLM&#30340;&#24037;&#20855;&#29983;&#25104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;&#30340;API&#35843;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#24212;&#23545;&#20855;&#26377;&#19981;&#21487;&#36870;&#24615;&#21644;&#22823;&#37327;&#26102;&#38388;&#28040;&#32791;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.10051</link><description>&lt;p&gt;
SwissNYF&#65306;&#22522;&#20110;&#40657;&#30418;&#29615;&#22659;&#30340;&#22522;&#20110;LLM&#30340;&#24037;&#20855;&#29983;&#25104;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
SwissNYF: Tool Grounded LLM Agents for Black Box Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10051
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#29615;&#22659;&#30340;&#22522;&#20110;LLM&#30340;&#24037;&#20855;&#29983;&#25104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;&#30340;API&#35843;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#24212;&#23545;&#20855;&#26377;&#19981;&#21487;&#36870;&#24615;&#21644;&#22823;&#37327;&#26102;&#38388;&#28040;&#32791;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35775;&#38382;&#20989;&#25968;&#30340;&#36820;&#22238;&#32467;&#26524;&#19978;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22686;&#24378;&#30340;&#21151;&#33021;&#35843;&#29992;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#31616;&#21333;API&#19978;&#26159;&#23454;&#29992;&#30340;&#65292;&#20294;&#23545;&#20110;&#19981;&#21487;&#36870;API&#65288;&#20363;&#22914;&#25968;&#25454;&#24211;&#21024;&#38500;API&#65289;&#20250;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#21516;&#26679;&#65292;&#23545;&#20110;&#27599;&#20010;API&#35843;&#29992;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#30340;&#27969;&#31243;&#20197;&#21450;&#38656;&#35201;&#21069;&#21521;&#35268;&#21010;&#30340;&#33258;&#21160;&#21270;&#25805;&#20316;&#31649;&#36947;&#31561;&#37117;&#23384;&#22312;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36890;&#24120;&#20986;&#29616;&#30340;&#24773;&#20917;&#26159;&#38656;&#35201;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#31639;&#27861;&#32570;&#20047;&#23545;&#36825;&#20123;&#20989;&#25968;&#30340;&#29305;&#23450;&#23454;&#29616;&#25110;&#20351;&#29992;&#23427;&#20204;&#30340;&#31192;&#23494;&#30340;&#30452;&#25509;&#35775;&#38382;&#26041;&#24335;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#30340;&#24037;&#20855;&#35268;&#21010;&#26041;&#27861;&#26159;&#19981;&#21512;&#36866;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#40657;&#30418;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#19982;&#22312;&#24037;&#20855;&#25805;&#20316;&#20013;&#30340;&#34920;&#29616;&#19981;&#21516;&#65292;LLM&#22312;&#40657;&#30418;&#20219;&#21153;&#65288;&#20363;&#22914;&#31243;&#24207;&#32508;&#21512;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#26469;&#29983;&#25104;&#22522;&#20110;&#40657;&#30418;&#29615;&#22659;&#30340;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10051v1 Announce Type: new  Abstract: While Large Language Models (LLMs) have demonstrated enhanced capabilities in function-calling, these advancements primarily rely on accessing the functions' responses. This methodology is practical for simpler APIs but faces scalability issues with irreversible APIs that significantly impact the system, such as a database deletion API. Similarly, processes requiring extensive time for each API call and those necessitating forward planning, like automated action pipelines, present complex challenges. Furthermore, scenarios often arise where a generalized approach is needed because algorithms lack direct access to the specific implementations of these functions or secrets to use them. Traditional tool planning methods are inadequate in these cases, compelling the need to operate within black-box environments. Unlike their performance in tool manipulation, LLMs excel in black-box tasks, such as program synthesis. Therefore, we harness the 
&lt;/p&gt;</description></item><item><title>&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21796;&#37266;&#25163;&#21183;&#20316;&#20026;&#19968;&#31181;&#25353;&#38656;&#32908;&#30005;&#25511;&#21046;&#33539;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#34394;&#20551;&#28608;&#27963;&#65292;&#25552;&#39640;&#32908;&#30005;&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10050</link><description>&lt;p&gt;
&#20351;&#29992;&#21796;&#37266;&#25163;&#21183;&#30340;&#25353;&#38656;&#32908;&#30005;&#25511;&#21046;&#65292;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#28040;&#38500;&#34394;&#20551;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
On-Demand Myoelectric Control Using Wake Gestures to Eliminate False Activations During Activities of Daily Living
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10050
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21796;&#37266;&#25163;&#21183;&#20316;&#20026;&#19968;&#31181;&#25353;&#38656;&#32908;&#30005;&#25511;&#21046;&#33539;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#34394;&#20551;&#28608;&#27963;&#65292;&#25552;&#39640;&#32908;&#30005;&#25511;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#32908;&#30005;&#25511;&#21046;&#20316;&#20026;&#19968;&#31181;&#21487;&#33021;&#30340;&#28789;&#27963;&#12289;&#20813;&#25552;&#36755;&#20837;&#26041;&#24335;&#65292;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#26465;&#20214;&#19979;&#65292;&#24403;&#21069;&#30340;&#25511;&#21046;&#26041;&#27861;&#23481;&#26131;&#20986;&#29616;&#24847;&#22806;&#30340;&#34394;&#20551;&#28608;&#27963;&#12290;&#26412;&#25991;&#25552;&#20986;&#12289;&#35774;&#35745;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32908;&#30005;&#25511;&#21046;&#33539;&#24335;&#8212;&#8212;&#25353;&#38656;&#32908;&#30005;&#25511;&#21046;&#65292;&#26088;&#22312;&#20943;&#23569;&#38169;&#35823;&#22320;&#35299;&#37322;&#20026;&#36755;&#20837;&#25163;&#21183;&#30340;&#38750;&#30456;&#20851;&#32908;&#32905;&#36816;&#21160;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#21796;&#37266;&#25163;&#21183;&#30340;&#27010;&#24565;&#65292;&#29992;&#25143;&#33021;&#22815;&#22312;&#19987;&#29992;&#25511;&#21046;&#27169;&#24335;&#21644;&#30561;&#30496;&#27169;&#24335;&#20043;&#38388;&#36827;&#34892;&#20999;&#25442;&#65292;&#22312;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#20013;&#26377;&#25928;&#22320;&#28040;&#38500;&#24847;&#22806;&#28608;&#27963;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20004;&#20010;&#22312;&#32447;&#26222;&#36866;&#32908;&#30005;&#25511;&#21046;&#20219;&#21153;&#65288;&#20851;&#38381;&#38393;&#38047;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#65289;&#65292;&#23637;&#31034;&#20102;&#21796;&#37266;&#25163;&#21183;&#30340;&#21487;&#34892;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#26041;&#26696;&#20960;&#20046;&#33021;&#22815;&#36866;&#24403;&#22320;&#24573;&#30053;&#25152;&#26377;&#38750;&#30446;&#26631;&#32908;&#32905;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10050v1 Announce Type: cross  Abstract: While myoelectric control has recently become a focus of increased research as a possible flexible hands-free input modality, current control approaches are prone to inadvertent false activations in real-world conditions. In this work, a novel myoelectric control paradigm -- on-demand myoelectric control -- is proposed, designed, and evaluated, to reduce the number of unrelated muscle movements that are incorrectly interpreted as input gestures . By leveraging the concept of wake gestures, users were able to switch between a dedicated control mode and a sleep mode, effectively eliminating inadvertent activations during activities of daily living (ADLs). The feasibility of wake gestures was demonstrated in this work through two online ubiquitous EMG control tasks with varying difficulty levels; dismissing an alarm and controlling a robot. The proposed control scheme was able to appropriately ignore almost all non-targeted muscular input
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;</title><link>https://arxiv.org/abs/2402.10038</link><description>&lt;p&gt;
RS-DPO&#65306;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;RLHF&#26377;&#26102;&#19981;&#31283;&#23450;&#65292;&#38656;&#35201;&#26174;&#33879;&#30340;&#36229;&#21442;&#25968;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#65288;DPO&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;DPO&#20381;&#36182;&#20110;&#20174;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#26367;&#20195;LLM&#29983;&#25104;&#30340;&#23545;&#27604;&#22238;&#22797;&#65292;&#32780;&#19981;&#26159;&#31574;&#30053;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;RLHF&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#32467;&#21512;&#25298;&#32477;&#37319;&#26679;&#65288;RS&#65289;&#21644;DPO&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;RS-DPO&#65292;&#39318;&#20808;&#24320;&#21457;&#20986;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65288;SFT&#65289;&#12290;&#28982;&#21518;&#30452;&#25509;&#20174;SFT&#27169;&#22411;&#20013;&#37319;&#26679;&#27599;&#20010;&#25552;&#31034;&#30340;k&#20010;&#21709;&#24212;&#12290;RS-DPO&#22522;&#20110;&#20854;&#30456;&#20284;&#24230;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10038v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10028</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#19982;&#22823;&#21160;&#20316;&#31354;&#38388;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models Meet Contextual Bandits with Large Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21160;&#20316;&#31354;&#38388;&#36739;&#22823;&#65292;&#26377;&#25928;&#30340;&#25506;&#32034;&#26159;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35774;&#35745;&#20102;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#65288;dTS&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#20026;dTS&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#31639;&#27861;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10028v1 Announce Type: cross  Abstract: Efficient exploration is a key challenge in contextual bandits due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies. Fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently. In this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion Thompson sampling (dTS). Both theoretical and algorithmic foundations are developed for dTS, and empirical evaluation also shows its favorable performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10024</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#23545;&#20110;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-Augmented In-Context Learning for Unsupervised Word Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10024
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#20123;&#23567;&#35268;&#27169;&#30340;&#35774;&#32622;&#20013;&#23637;&#31034;&#20986;&#20102;&#36739;&#24378;&#30340;&#35789;&#27719;&#32763;&#35793;&#21644;&#21452;&#35821;&#35789;&#20856;&#35825;&#23548;(BLI)&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#27809;&#26377;&#31181;&#23376;&#32763;&#35793;&#23545;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#36798;&#21040;&#8220;&#20256;&#32479;&#8221;&#30340;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861; (SAIL) &#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;BLI&#65306;&#20174;&#38646;&#26679;&#26412;&#25552;&#31034;&#24320;&#22987;&#65292;SAIL&#36890;&#36807;&#36845;&#20195;&#22320;&#20174;LLM&#20013;&#24341;&#20986;&#19968;&#32452;&#39640;&#32622;&#20449;&#24230;&#30340;&#35789;&#27719;&#32763;&#35793;&#23545;&#65292;&#28982;&#21518;&#22312;ICL&#30340;&#26041;&#24335;&#19979;&#20877;&#27425;&#24212;&#29992;&#20110;&#21516;&#19968;&#20010;LLM&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#24191;&#27867;&#30340;BLI&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#23545;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;LLM&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20063;&#22312;&#21508;&#20010;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#26144;&#23556;&#30340;&#22522;&#32447;&#12290;&#38500;&#20102;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10024v1 Announce Type: cross  Abstract: Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;Clifford&#32676;&#31561;&#21464;&#23618;&#19982;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#25299;&#25169;&#19978;&#26356;&#20026;&#22797;&#26434;&#30340;E&#65288;n&#65289;-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10011</link><description>&lt;p&gt;
Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford Group Equivariant Simplicial Message Passing Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;Clifford&#32676;&#31561;&#21464;&#23618;&#19982;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#25299;&#25169;&#19978;&#26356;&#20026;&#22797;&#26434;&#30340;E&#65288;n&#65289;-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21333;&#20307;&#22797;&#21512;&#20307;&#19978;&#36827;&#34892;&#21487;&#25511;&#30340;E&#65288;n&#65289;-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;Clifford&#32676;&#31561;&#21464;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30456;&#32467;&#21512;&#65292;&#21518;&#32773;&#22312;&#25299;&#25169;&#19978;&#27604;&#24120;&#35268;&#22270;&#28040;&#24687;&#20256;&#36882;&#26356;&#21152;&#22797;&#26434;&#12290;Clifford&#20195;&#25968;&#21253;&#25324;&#39640;&#38454;&#23545;&#35937;&#65292;&#22914;&#21452;&#21521;&#37327;&#21644;&#19977;&#21521;&#37327;&#65292;&#36825;&#20123;&#23545;&#35937;&#36890;&#36807;&#21521;&#37327;&#34893;&#29983;&#20986;&#20960;&#20309;&#29305;&#24449;&#65288;&#20363;&#22914;&#38754;&#31215;&#65292;&#20307;&#31215;&#65289;&#12290;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#39030;&#28857;&#30340;&#20960;&#20309;&#20056;&#31215;&#34920;&#31034;&#31616;&#21333;&#24418;&#24335;&#29305;&#24449;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#20849;&#20139;&#28040;&#24687;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#26368;&#32456;&#30340;&#28040;&#24687;&#38480;&#21046;&#20026;&#26469;&#33258;&#19981;&#21516;&#32500;&#24230;&#30340;&#20256;&#20837;&#28040;&#24687;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#20849;&#20139;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36755;&#20986;&#36866;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10011v1 Announce Type: new  Abstract: We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular graph message passing. Clifford algebras include higher-order objects such as bivectors and trivectors, which express geometric features (e.g., areas, volumes) derived from vectors. Using this knowledge, we represent simplex features through geometric products of their vertices. To achieve efficient simplicial message passing, we share the parameters of the message network across different dimensions. Additionally, we restrict the final message to an aggregation of the incoming messages from different dimensions, leading to what we term shared simplicial message passing. Experimental results show that our method is able to ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22768;&#23398;&#20449;&#21495;&#22788;&#29702;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22797;&#26434;&#22768;&#23398;&#29616;&#35937;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10005</link><description>&lt;p&gt;
ML-ASPA: &#26426;&#22120;&#23398;&#20064;&#22312;&#22768;&#23398;&#20449;&#21495;&#22788;&#29702;&#20998;&#26512;&#20013;&#30340;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
ML-ASPA: A Contemplation of Machine Learning-based Acoustic Signal Processing Analysis for Sounds, &amp; Strains Emerging Technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22768;&#23398;&#20449;&#21495;&#22788;&#29702;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22797;&#26434;&#22768;&#23398;&#29616;&#35937;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#23398;&#25968;&#25454;&#22312;&#25512;&#21160;&#31185;&#23398;&#21644;&#24037;&#31243;&#29702;&#35299;&#26041;&#38754;&#36215;&#30528;&#22522;&#26412;&#30340;&#22522;&#30707;&#20316;&#29992;&#65292;&#28041;&#21450;&#29983;&#29289;&#23398;&#12289;&#36890;&#20449;&#23398;&#20197;&#21450;&#28023;&#27915;&#21644;&#22320;&#29699;&#31185;&#23398;&#31561;&#22810;&#20010;&#23398;&#31185;&#12290;&#26412;&#25991;&#35814;&#32454;&#25506;&#35752;&#20102;&#22768;&#23398;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#36827;&#23637;&#21644;&#21464;&#38761;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#30340;&#22768;&#23398;&#21644;&#20449;&#21495;&#22788;&#29702;&#30456;&#27604;&#65292;ML&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;&#19982;&#26399;&#26395;&#26631;&#31614;&#25110;&#21160;&#20316;&#20043;&#38388;&#20197;&#21450;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#32473;&#23450;&#20805;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23558;ML&#24212;&#29992;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#21457;&#29616;&#33021;&#22815;&#35299;&#37322;&#20154;&#31867;&#35821;&#38899;&#21644;&#28151;&#21709;&#31561;&#22797;&#26434;&#22768;&#23398;&#29616;&#35937;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10005v1 Announce Type: cross  Abstract: Acoustic data serves as a fundamental cornerstone in advancing scientific and engineering understanding across diverse disciplines, spanning biology, communications, and ocean and Earth science. This inquiry meticulously explores recent advancements and transformative potential within the domain of acoustics, specifically focusing on machine learning (ML) and deep learning. ML, comprising an extensive array of statistical techniques, proves indispensable for autonomously discerning and leveraging patterns within data. In contrast to traditional acoustics and signal processing, ML adopts a data-driven approach, unveiling intricate relationships between features and desired labels or actions, as well as among features themselves, given ample training data. The application of ML to expansive sets of training data facilitates the discovery of models elucidating complex acoustic phenomena such as human speech and reverberation. The dynamic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MM-Point&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#20132;&#20114;&#21644;&#20256;&#36755;&#23454;&#29616;&#20102;3D&#29289;&#20307;&#21644;&#22810;&#20010;2D&#35270;&#22270;&#20043;&#38388;&#30340;&#20449;&#24687;&#22686;&#24378;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;MM-Point&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10002</link><description>&lt;p&gt;
MM-Point: &#22810;&#35270;&#35282;&#20449;&#24687;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#19977;&#32500;&#28857;&#20113;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MM-Point&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#20132;&#20114;&#21644;&#20256;&#36755;&#23454;&#29616;&#20102;3D&#29289;&#20307;&#21644;&#22810;&#20010;2D&#35270;&#22270;&#20043;&#38388;&#30340;&#20449;&#24687;&#22686;&#24378;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;MM-Point&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24863;&#30693;&#39046;&#22495;&#20013;&#65292;&#23558;&#22810;&#31181;&#20256;&#24863;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#23558;2D&#35270;&#22270;&#19978;&#30340;&#35270;&#35273;&#20449;&#24687;&#26144;&#23556;&#21040;3D&#29289;&#20307;&#19978;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#36827;&#34892;&#29702;&#35299;&#12290;&#20294;&#26159;&#22312;&#20174;&#19981;&#21516;&#35282;&#24230;&#28210;&#26579;&#30340;&#21333;&#20010;2D&#35270;&#22270;&#20013;&#65292;&#21482;&#33021;&#25552;&#20379;&#26377;&#38480;&#30340;&#37096;&#20998;&#20449;&#24687;&#12290;&#22810;&#35270;&#35282;2D&#20449;&#24687;&#30340;&#20016;&#23500;&#24615;&#21644;&#20215;&#20540;&#21487;&#20197;&#20026;3D&#29289;&#20307;&#25552;&#20379;&#20248;&#31168;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#28857;&#20113;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MM-Point&#65292;&#23427;&#21463;&#21040;&#20869;&#27169;&#24577;&#21644;&#22806;&#27169;&#24577;&#30456;&#20284;&#24230;&#30446;&#26631;&#30340;&#39537;&#21160;&#12290;MM-Point&#30340;&#26680;&#24515;&#22312;&#20110;3D&#29289;&#20307;&#21644;&#22810;&#20010;2D&#35270;&#22270;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#21644;&#20256;&#36755;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#21516;&#26102;&#25191;&#34892;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;2D&#22810;&#35270;&#22270;&#20449;&#24687;&#19968;&#33268;&#24615;&#20132;&#21449;&#27169;&#24577;&#30446;&#26631;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;(Multi-MLP)&#21644;&#22810;&#23618;&#32423;&#22686;&#24378;&#31574;&#30053;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MM-Point&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10002v1 Announce Type: cross  Abstract: In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully desig
&lt;/p&gt;</description></item><item><title>LoraRetriever&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#36755;&#20837;&#30340;LoRA&#26816;&#32034;&#19982;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#24357;&#21512;&#23454;&#38469;&#24773;&#20917;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#25910;&#21040;&#19981;&#21516;&#20219;&#21153;&#25552;&#31034;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.09997</link><description>&lt;p&gt;
LoraRetriever: &#36866;&#24212;&#36755;&#20837;&#30340;LoRA&#26816;&#32034;&#19982;&#21512;&#25104;&#26041;&#27861;&#29992;&#20110;&#28151;&#21512;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09997
&lt;/p&gt;
&lt;p&gt;
LoraRetriever&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#36755;&#20837;&#30340;LoRA&#26816;&#32034;&#19982;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#24357;&#21512;&#23454;&#38469;&#24773;&#20917;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#25910;&#21040;&#19981;&#21516;&#20219;&#21153;&#25552;&#31034;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA)&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;LoRA&#30340;&#27169;&#22359;&#21270;&#21644;&#21363;&#25554;&#21363;&#29992;&#30340;&#29305;&#24615;&#20351;&#24471;&#33021;&#22815;&#38598;&#25104;&#21508;&#31181;&#39046;&#22495;&#29305;&#23450;&#30340;LoRA&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38548;&#31163;&#19979;&#28216;&#20219;&#21153;&#65292;&#35201;&#20040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22266;&#23450;LoRA&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;LLM&#25509;&#25910;&#21040;&#28085;&#30422;&#19981;&#21516;&#20219;&#21153;&#30340;&#21508;&#31181;&#25552;&#31034;&#65292;&#24182;&#19988;&#20505;&#36873;LoRA&#30340;&#27744;&#32463;&#24120;&#21160;&#24577;&#26356;&#26032;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoraRetriever&#65292;&#19968;&#31181;&#26681;&#25454;&#36755;&#20837;&#25552;&#31034;&#33258;&#36866;&#24212;&#26816;&#32034;&#21644;&#21512;&#25104;&#22810;&#20010;LoRA&#30340;&#26694;&#26550;&#12290;LoraRetriever&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#39318;&#20808;&#65292;&#35782;&#21035;&#21644;&#26816;&#32034;&#19982;&#32473;&#23450;&#36755;&#20837;&#30456;&#20851;&#30340;LoRA&#65307;&#20854;&#27425;&#65292;&#21046;&#23450;&#26377;&#25928;&#25972;&#21512;&#26816;&#32034;&#21040;&#30340;LoRA&#30340;&#31574;&#30053;&#65307;&#26368;&#21518;&#65292;&#24320;&#21457;&#39640;&#25928;&#30340;&#26041;&#27861;&#29992;&#20110;&#23454;&#29616;LoRA&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09997v1 Announce Type: new  Abstract: Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLM). The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs. Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts. LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing effici
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#38431;&#21451;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19982;&#26032;&#38431;&#21451;&#21512;&#20316;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09984</link><description>&lt;p&gt;
&#23545;&#20110;&#20020;&#26102;&#22242;&#38431;&#21512;&#20316;&#30340;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Symmetry-Breaking Augmentations for Ad Hoc Teamwork
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#38431;&#21451;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19982;&#26032;&#38431;&#21451;&#21512;&#20316;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21327;&#20316;&#29615;&#22659;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20195;&#29702;&#24517;&#39035;&#33021;&#22815;&#36866;&#24212;&#20351;&#29992;&#26410;&#30693;&#25110;&#20808;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#31574;&#30053;&#30340;&#26032;&#38431;&#21451;&#12290;&#23545;&#20110;AI&#20195;&#29702;&#26469;&#35828;&#65292;&#36825;&#36890;&#24120;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#31616;&#21333;&#65292;&#20294;&#21364;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;AI&#20195;&#29702;&#22312;&#35757;&#32451;&#38598;&#20013;&#23398;&#20250;&#20102;&#19982;&#21482;&#22312;&#19968;&#20391;&#36947;&#36335;&#19978;&#34892;&#39542;&#30340;&#20854;&#20182;&#36710;&#36742;&#24182;&#34892;&#39542;&#65292;&#37027;&#20040;&#21363;&#20351;&#36825;&#20123;&#36710;&#36742;&#30340;&#34892;&#20026;&#21482;&#26159;&#22312;&#24038;&#21491;&#23545;&#31216;&#19978;&#36827;&#34892;&#20102;&#32763;&#36716;&#65292;&#23427;&#20063;&#21487;&#33021;&#38590;&#20197;&#36866;&#24212;&#19982;&#30456;&#21453;&#26041;&#21521;&#19978;&#34892;&#39542;&#30340;&#39550;&#39542;&#21592;&#36827;&#34892;&#21327;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;&#65288;SBA&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#23545;&#31216;&#32763;&#36716;&#25805;&#20316;&#26469;&#22686;&#21152;&#35757;&#32451;&#38431;&#21451;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#22686;&#24378;&#21518;&#30340;&#38431;&#21451;&#30340;&#26368;&#20339;&#21709;&#24212;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#33021;&#22815;&#25509;&#35302;&#21040;&#26356;&#24191;&#27867;&#30340;&#34892;&#20026;&#32422;&#23450;&#65292;&#20174;&#32780;&#25552;&#39640;&#19982;&#26032;&#38431;&#21451;&#21512;&#20316;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09984v1 Announce Type: cross  Abstract: In many collaborative settings, artificial intelligence (AI) agents must be able to adapt to new teammates that use unknown or previously unobserved strategies. While often simple for humans, this can be challenging for AI agents. For example, if an AI agent learns to drive alongside others (a training set) that only drive on one side of the road, it may struggle to adapt this experience to coordinate with drivers on the opposite side, even if their behaviours are simply flipped along the left-right symmetry. To address this we introduce symmetry-breaking augmentations (SBA), which increases diversity in the behaviour of training teammates by applying a symmetry-flipping operation. By learning a best-response to the augmented set of teammates, our agent is exposed to a wider range of behavioural conventions, improving performance when deployed with novel teammates. We demonstrate this experimentally in two settings, and show that our a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;85%&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09982</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#21644;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;85%&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#34920;&#24773;&#26159;&#25105;&#20204;&#22312;&#29702;&#35299;&#19968;&#20010;&#20154;&#30340;&#24515;&#29702;&#29366;&#24577;&#26102;&#39318;&#20808;&#20851;&#27880;&#30340;&#20107;&#29289;&#12290;&#22240;&#27492;&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#38754;&#37096;&#34920;&#24773;&#26159;&#19968;&#20010;&#38750;&#24120;&#26377;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#30001;&#20110;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#36739;&#23567;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#35782;&#21035;&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#24212;&#29992;&#20960;&#20309;&#21464;&#25442;&#65292;&#24182;&#20174;&#22836;&#26500;&#24314;&#20102;&#33021;&#22815;&#20026;&#27599;&#31181;&#24773;&#32490;&#31867;&#22411;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#22270;&#20687;&#30340;GAN&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#22312;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#34913;&#37327;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#39069;&#22806;&#25968;&#25454;&#24211;&#21327;&#35758;&#26041;&#27861;&#65292;&#21363;&#25105;&#20204;&#22312;&#32463;&#36807;&#22686;&#24378;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#36825;&#20123;&#25216;&#26415;&#30340;&#32452;&#21512;&#20351;&#24471;&#21487;&#20197;&#36798;&#21040;&#24179;&#22343;&#20934;&#30830;&#24230;&#32422;&#20026;85%&#30340;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09982v1 Announce Type: cross  Abstract: The face expression is the first thing we pay attention to when we want to understand a person's state of mind. Thus, the ability to recognize facial expressions in an automatic way is a very interesting research field. In this paper, because the small size of available training datasets, we propose a novel data augmentation technique that improves the performances in the recognition task. We apply geometrical transformations and build from scratch GAN models able to generate new synthetic images for each emotion type. Thus, on the augmented datasets we fine tune pretrained convolutional neural networks with different architectures. To measure the generalization ability of the models, we apply extra-database protocol approach, namely we train models on the augmented versions of training dataset and test them on two different databases. The combination of these techniques allows to reach average accuracy values of the order of 85\% for 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#21387;&#32553;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#24615;&#33021;&#30053;&#26377;&#22949;&#21327;&#12290;</title><link>https://arxiv.org/abs/2402.09977</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#24555;&#36895;&#35789;&#27719;&#36716;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Vocabulary Transfer for Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#21387;&#32553;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#24615;&#33021;&#30053;&#26377;&#22949;&#21327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19994;&#21153;&#24212;&#29992;&#38656;&#35201;&#22312;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#22823;&#23567;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#36716;&#31227;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#22402;&#30452;&#39046;&#22495;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35789;&#27719;&#36716;&#31227;&#21487;&#20197;&#19982;&#20854;&#20182;&#21387;&#32553;&#25216;&#26415;&#26377;&#25928;&#32467;&#21512;&#20351;&#29992;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#30053;&#26377;&#22949;&#21327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09977v1 Announce Type: cross  Abstract: Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21487;&#35270;&#21270;&#30446;&#26631;&#38388;&#23618;&#27425;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#27169;&#22411;&#25913;&#36827;&#20855;&#26377;&#28508;&#22312;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.09965</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#30340;&#23618;&#27425;&#21270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Hierarchy Representation of Data in Machine Learnings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21487;&#35270;&#21270;&#30446;&#26631;&#38388;&#23618;&#27425;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#27169;&#22411;&#25913;&#36827;&#20855;&#26377;&#28508;&#22312;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23384;&#22312;&#22810;&#20010;&#25968;&#25454;&#28857;&#30340;&#27169;&#22411;&#20855;&#26377;&#26126;&#30830;&#30340;&#21028;&#26029;&#32467;&#26524;&#26102;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#21487;&#33021;&#23637;&#31034;&#20986;&#19968;&#31181;&#20851;&#31995;&#65292;&#21363;&#22914;&#26524;&#23427;&#20204;&#27491;&#30830;&#21028;&#26029;&#19968;&#20010;&#30446;&#26631;&#65292;&#21017;&#23427;&#20204;&#20063;&#20250;&#27491;&#30830;&#21028;&#26029;&#21478;&#19968;&#20010;&#30446;&#26631;&#12290;&#30456;&#21453;&#65292;&#22914;&#26524;&#22823;&#22810;&#25968;&#27169;&#22411;&#38169;&#35823;&#22320;&#21028;&#26029;&#19968;&#20010;&#30446;&#26631;&#65292;&#23427;&#20204;&#21487;&#33021;&#20063;&#20250;&#38169;&#35823;&#22320;&#21028;&#26029;&#21478;&#19968;&#20010;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#30446;&#26631;&#20043;&#38388;&#23618;&#27425;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20449;&#24687;&#26377;&#26395;&#23545;&#27169;&#22411;&#25913;&#36827;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09965v1 Announce Type: cross  Abstract: When there are models with clear-cut judgment results for several data points, it is possible that most models exhibit a relationship where if they correctly judge one target, they also correctly judge another target. Conversely, if most models incorrectly judge one target, they may also incorrectly judge another target. We propose a method for visualizing this hierarchy among targets. This information is expected to be beneficial for model improvement.
&lt;/p&gt;</description></item><item><title>FedLion&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38598;&#20013;&#24335;&#33258;&#36866;&#24212;&#31639;&#27861;Lion&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#32463;&#36807;&#24191;&#27867;&#35780;&#20272;&#65292;FedLion&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#22312;&#26412;&#22320;&#35757;&#32451;&#20013;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.09941</link><description>&lt;p&gt;
FedLion: &#26356;&#24555;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#20449;&#26356;&#23569;
&lt;/p&gt;
&lt;p&gt;
FedLion: Faster Adaptive Federated Optimization with Fewer Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09941
&lt;/p&gt;
&lt;p&gt;
FedLion&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38598;&#20013;&#24335;&#33258;&#36866;&#24212;&#31639;&#27861;Lion&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#32463;&#36807;&#24191;&#27867;&#35780;&#20272;&#65292;FedLion&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#22312;&#26412;&#22320;&#35757;&#32451;&#20013;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#19968;&#31181;&#36328;&#20998;&#24067;&#24335;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26694;&#26550;&#20013;&#65292;&#20687;FedAvg&#36825;&#26679;&#30340;&#30693;&#21517;&#31639;&#27861;&#24448;&#24448;&#20855;&#26377;&#36739;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23548;&#33268;&#39640;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedLion&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#65292;&#26080;&#32541;&#22320;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340;&#38598;&#20013;&#24335;&#33258;&#36866;&#24212;&#31639;&#27861;Lion&#65288;Chen et al. 2023&#65289;&#30340;&#20851;&#38190;&#20803;&#32032;&#34701;&#20837;&#21040;FL&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;FL&#22522;&#20934;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FedLion&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#21253;&#25324;FAFED&#65288;Wu et al. 2023&#65289;&#21644;FedDA&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22312;&#26412;&#22320;&#35757;&#32451;&#20013;&#20351;&#29992;&#20102;&#26377;&#31526;&#21495;&#26799;&#24230;&#65292;&#19982;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#30456;&#27604;&#65292;FedLion&#22312;&#19978;&#34892;&#36890;&#20449;&#36807;&#31243;&#20013;&#22823;&#22823;&#38477;&#20302;&#20102;&#25968;&#25454;&#20256;&#36755;&#35201;&#27714;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09941v1 Announce Type: cross  Abstract: In Federated Learning (FL), a framework to train machine learning models across distributed data, well-known algorithms like FedAvg tend to have slow convergence rates, resulting in high communication costs during training. To address this challenge, we introduce FedLion, an adaptive federated optimization algorithm that seamlessly incorporates key elements from the recently proposed centralized adaptive algorithm, Lion (Chen et al. 2o23), into the FL framework. Through comprehensive evaluations on two widely adopted FL benchmarks, we demonstrate that FedLion outperforms previous state-of-the-art adaptive algorithms, including FAFED (Wu et al. 2023) and FedDA. Moreover, thanks to the use of signed gradients in local training, FedLion substantially reduces data transmission requirements during uplink communication when compared to existing adaptive algorithms, further reducing communication costs. Last but not least, this work also incl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#26500;&#24314;&#23450;&#21046;&#21270;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.09939</link><description>&lt;p&gt;
&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#26368;&#26032;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generative AI in the Construction Industry: A State-of-the-art Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#26500;&#24314;&#23450;&#21046;&#21270;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#34892;&#19994;&#26159;&#20840;&#29699;&#32463;&#27982;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#20010;&#37096;&#38376;&#65292;&#20294;&#22312;&#35774;&#35745;&#12289;&#35268;&#21010;&#12289;&#37319;&#36141;&#12289;&#26816;&#26597;&#21644;&#32500;&#25252;&#31561;&#21508;&#20010;&#29615;&#33410;&#20013;&#38754;&#20020;&#30528;&#35768;&#22810;&#29983;&#20135;&#21147;&#25361;&#25112;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#22522;&#20110;&#26576;&#20123;&#36755;&#20837;&#25110;&#20808;&#21069;&#30340;&#30693;&#35782;&#21019;&#36896;&#26032;&#39062;&#19988;&#36924;&#30495;&#30340;&#25968;&#25454;&#25110;&#20869;&#23481;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#20195;&#30721;&#65292;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#21019;&#26032;&#21644;&#39072;&#35206;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#20851;&#20110;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#25991;&#29486;&#20013;&#23384;&#22312;&#30528;&#31354;&#30333;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#24314;&#31569;&#39046;&#22495;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#32570;&#65292;&#30740;&#31350;&#30446;&#26631;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23545;&#24314;&#31569;&#34892;&#19994;&#29616;&#26377;&#21644;&#26032;&#20852;&#30340;&#29983;&#25104;&#24335;AI&#26426;&#36935;&#21644;&#25361;&#25112;&#36827;&#34892;&#22238;&#39038;&#21644;&#20998;&#31867;&#65307;&#65288;2&#65289;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#21033;&#29992;&#33258;&#24049;&#30340;&#25968;&#25454;&#21644;&#38656;&#27714;&#26500;&#24314;&#23450;&#21046;&#21270;&#30340;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09939v1 Announce Type: new  Abstract: The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their ow
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25366;&#25496;&#20102;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#65292;&#24182;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09934</link><description>&lt;p&gt;
&#20851;&#27880;&#20559;&#24046;&#65306;&#25366;&#25496;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25366;&#25496;&#20102;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#65292;&#24182;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#38382;&#20027;&#20041;&#22312;&#25200;&#20081;&#21465;&#20107;&#21644;&#25773;&#31181;&#19981;&#20449;&#20219;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#24037;&#20855;&#25928;&#29992;&#65292;&#20294;&#22312;&#23450;&#37327;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#21364;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#26410;&#33021;&#21306;&#20998;&#21453;&#38382;&#20027;&#20041;&#20316;&#20026;&#35823;&#23548;&#21644;&#23459;&#20256;&#31574;&#30053;&#30340;&#29992;&#36884;&#19982;&#20854;&#20316;&#20026;&#35821;&#29992;&#21644;&#35821;&#20041;&#26694;&#26550;&#24037;&#20855;&#30340;&#29992;&#36884;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26032;&#30340;&#26469;&#33258;Twitter&#21644;YouTube&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21453;&#38382;&#20027;&#20041;&#12289;&#23459;&#20256;&#21644;tu quoque&#35884;&#35823;&#20043;&#38388;&#30340;&#37325;&#21472;&#21644;&#21306;&#21035;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#26368;&#36817;&#22312;&#35821;&#35328;&#35821;&#20041;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#8220;what about&#8221;&#35789;&#27719;&#32467;&#26500;&#19982;&#21453;&#38382;&#20027;&#20041;&#21306;&#20998;&#24320;&#26469;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#26435;&#37325;&#36827;&#34892;&#36127;&#26679;&#26412;&#25366;&#25496;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;4%&#21644;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09934v1 Announce Type: cross  Abstract: Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about' lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#25991;&#22810;&#27573;&#31572;&#26696;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;CLEAN&#65292;&#24357;&#34917;&#20102;&#20013;&#25991;MSQA&#30740;&#31350;&#20013;&#30340;&#19981;&#36275;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#20027;&#39064;&#21644;&#38656;&#35201;&#35814;&#32454;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25552;&#20379;&#20102;&#30456;&#20851;&#25991;&#29486;&#20013;&#30340;&#22522;&#32447;&#27169;&#22411;&#20316;&#20026;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2402.09923</link><description>&lt;p&gt;
&#19968;&#20010;&#21253;&#21547;&#22810;&#27573;&#31572;&#26696;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Dataset of Open-Domain Question Answering with Multiple-Span Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09923
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#25991;&#22810;&#27573;&#31572;&#26696;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;CLEAN&#65292;&#24357;&#34917;&#20102;&#20013;&#25991;MSQA&#30740;&#31350;&#20013;&#30340;&#19981;&#36275;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#20027;&#39064;&#21644;&#38656;&#35201;&#35814;&#32454;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25552;&#20379;&#20102;&#30456;&#20851;&#25991;&#29486;&#20013;&#30340;&#22522;&#32447;&#27169;&#22411;&#20316;&#20026;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27573;&#31572;&#26696;&#25552;&#21462;&#65292;&#20063;&#31216;&#20026;&#22810;&#27573;&#38382;&#31572;&#65288;MSQA&#65289;&#20219;&#21153;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#22810;&#20010;&#20449;&#24687;&#29255;&#27573;&#26469;&#22238;&#31572;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#33521;&#25991;MSQA&#30740;&#31350;&#27963;&#36291;&#24182;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#22312;&#20013;&#25991;&#39046;&#22495;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;MSQA&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20197;&#24448;&#26500;&#24314;MSQA&#25968;&#25454;&#38598;&#30340;&#21162;&#21147;&#20027;&#35201;&#24378;&#35843;&#23454;&#20307;&#20013;&#24515;&#30340;&#24773;&#22659;&#21270;&#65292;&#23548;&#33268;&#20559;&#21521;&#25910;&#38598;&#20107;&#23454;&#24615;&#38382;&#39064;&#24182;&#21487;&#33021;&#24573;&#35270;&#38656;&#35201;&#26356;&#35814;&#32454;&#25551;&#36848;&#24615;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;CLEAN&#65292;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#22810;&#27573;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#21508;&#31181;&#24320;&#25918;&#39046;&#22495;&#30340;&#20027;&#39064;&#65292;&#24182;&#21253;&#21547;&#22823;&#37327;&#38656;&#35201;&#25551;&#36848;&#24615;&#31572;&#26696;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#30456;&#20851;&#25991;&#29486;&#20013;&#30340;&#24050;&#24314;&#31435;&#27169;&#22411;&#20316;&#20026;CLEAN&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09923v1 Announce Type: cross  Abstract: Multi-span answer extraction, also known as the task of multi-span question answering (MSQA), is critical for real-world applications, as it requires extracting multiple pieces of information from a text to answer complex questions. Despite the active studies and rapid progress in English MSQA research, there is a notable lack of publicly available MSQA benchmark in Chinese. Previous efforts for constructing MSQA datasets predominantly emphasized entity-centric contextualization, resulting in a bias towards collecting factoid questions and potentially overlooking questions requiring more detailed descriptive responses. To overcome these limitations, we present CLEAN, a comprehensive Chinese multi-span question answering dataset that involves a wide range of open-domain subjects with a substantial number of instances requiring descriptive answers. Additionally, we provide established models from relevant literature as baselines for CLEA
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30740;&#31350;&#31227;&#21160;&#36873;&#25321;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35843;&#26597;&#32467;&#26524;&#25581;&#31034;&#20102;&#20915;&#31574;&#20013;&#23384;&#22312;&#30340;&#21508;&#31181;&#20559;&#35265;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#22312;GAMA&#20195;&#29702;&#27169;&#25311;&#20013;&#23454;&#29616;&#20102;&#36825;&#19968;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09921</link><description>&lt;p&gt;
&#35782;&#21035;&#21644;&#24314;&#27169;&#31227;&#21160;&#36873;&#25321;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Identifying and modelling cognitive biases in mobility choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30740;&#31350;&#31227;&#21160;&#36873;&#25321;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35843;&#26597;&#32467;&#26524;&#25581;&#31034;&#20102;&#20915;&#31574;&#20013;&#23384;&#22312;&#30340;&#21508;&#31181;&#20559;&#35265;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#22312;GAMA&#20195;&#29702;&#27169;&#25311;&#20013;&#23454;&#29616;&#20102;&#36825;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25253;&#21578;&#20102;&#19968;&#39033;M1&#23454;&#20064;&#24037;&#20316;&#20013;&#20851;&#20110;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#27169;&#25311;&#26085;&#24120;&#31227;&#21160;&#36873;&#25321;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#36825;&#20010;&#27169;&#25311;&#26088;&#22312;&#36275;&#22815;&#30495;&#23454;&#65292;&#20197;&#20316;&#20026;&#19968;&#20010;&#20851;&#20110;&#31227;&#21160;&#36807;&#28193;&#30340;&#20005;&#32899;&#28216;&#25103;&#30340;&#22522;&#30784;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#31181;&#30495;&#23454;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#20197;&#34913;&#37327;&#30495;&#23454;&#31227;&#21160;&#36873;&#25321;&#26159;&#29702;&#24615;&#30340;&#65292;&#36824;&#26159;&#23384;&#22312;&#20559;&#24046;&#12290;&#22312;&#36825;&#37324;&#20998;&#26512;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#21508;&#31181;&#20559;&#35265;&#21487;&#33021;&#22312;&#20915;&#31574;&#20013;&#36215;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;GAMA&#20195;&#29702;&#27169;&#25311;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09921v1 Announce Type: cross  Abstract: This report presents results from an M1 internship dedicated to agent-based modelling and simulation of daily mobility choices. This simulation is intended to be realistic enough to serve as a basis for a serious game about the mobility transition. In order to ensure this level of realism, we conducted a survey to measure if real mobility choices are made rationally, or how biased they are. Results analysed here show that various biases could play a role in decisions. We then propose an implementation in a GAMA agent-based simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;GPS&#36712;&#36857;&#26469;&#32472;&#21046;&#24314;&#31569;&#24037;&#22320;&#36947;&#36335;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#20851;&#38190;&#30340;&#20132;&#21449;&#21475;&#24182;&#36830;&#25509;&#23427;&#20204;&#65292;&#29983;&#25104;&#36947;&#36335;&#22270;&#65292;&#20026;&#35268;&#21010;&#21644;&#20219;&#21153;&#20998;&#37197;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.09919</link><description>&lt;p&gt;
&#36947;&#36335;&#22270;&#29983;&#25104;&#22120;&#65306;&#20174;GPS&#25968;&#25454;&#20013;&#29983;&#25104;&#24314;&#31569;&#24037;&#22320;&#36947;&#36335;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Road Graph Generator: Mapping roads at construction sites from GPS data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;GPS&#36712;&#36857;&#26469;&#32472;&#21046;&#24314;&#31569;&#24037;&#22320;&#36947;&#36335;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#20851;&#38190;&#30340;&#20132;&#21449;&#21475;&#24182;&#36830;&#25509;&#23427;&#20204;&#65292;&#29983;&#25104;&#36947;&#36335;&#22270;&#65292;&#20026;&#35268;&#21010;&#21644;&#20219;&#21153;&#20998;&#37197;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;GPS&#36712;&#36857;&#20013;&#25512;&#27979;&#36947;&#36335;&#20197;&#32472;&#21046;&#24314;&#31569;&#24037;&#22320;&#22320;&#22270;&#30340;&#26041;&#27861;&#12290;&#36825;&#39033;&#20219;&#21153;&#30001;&#20110;&#24314;&#31569;&#26426;&#26800;&#30340;&#19981;&#35268;&#21017;&#21644;&#38750;&#26631;&#20934;&#36816;&#21160;&#27169;&#24335;&#19982;&#24050;&#24314;&#31435;&#36947;&#36335;&#19978;&#30340; typcial &#36710;&#36742;&#20132;&#36890;&#26174;&#33879;&#19981;&#21516;&#65292;&#22240;&#27492;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#35782;&#21035;&#36947;&#36335;&#32593;&#32476;&#20013;&#20316;&#20026;&#20851;&#38190;&#20915;&#31574;&#28857;&#30340;&#20132;&#21449;&#21475;&#65292;&#28982;&#21518;&#36830;&#25509;&#23427;&#20204;&#20197;&#24418;&#25104;&#19968;&#20010;&#22270;&#65292;&#38543;&#21518;&#21487;&#20197;&#29992;&#20110;&#35268;&#21010;&#21644;&#20219;&#21153;&#20998;&#37197;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#25386;&#23041;&#30340;&#19968;&#20010;&#23454;&#38469;&#24314;&#31569;&#24037;&#22320;&#32472;&#21046;&#36947;&#36335;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09919v1 Announce Type: new  Abstract: We present a method for road inference from GPS trajectories to map construction sites. This task introduces a unique challenge due to the erratic and non-standard movement patterns of construction machinery, which diverge significantly from typical vehicular traffic on established roads. Our method first identifies intersections in the road network that serve as critical decision points, and later connects them with edges, producing a graph, which subsequently can be used for planning and task-allocation. We demonstrate the effectiveness of our approach by mapping roads at a real-life construction site in Norway.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20266;&#21644;&#22810;&#28304;&#30693;&#35782;&#22270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#20854;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#20266;&#22270;&#29983;&#25104;&#21644;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#30340;&#26694;&#26550;&#65292;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#29615;&#22659;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#21487;&#20197;&#25552;&#39640;ROUGE-L&#20998;&#25968;&#33267;&#23569;11.5&#12290;</title><link>https://arxiv.org/abs/2402.09911</link><description>&lt;p&gt;
&#20351;&#29992;&#20266;&#21644;&#22810;&#28304;&#30693;&#35782;&#22270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09911
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20266;&#21644;&#22810;&#28304;&#30693;&#35782;&#22270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#20854;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#20266;&#22270;&#29983;&#25104;&#21644;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#30340;&#26694;&#26550;&#65292;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#29615;&#22659;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#21487;&#20197;&#25552;&#39640;ROUGE-L&#20998;&#25968;&#33267;&#23569;11.5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#24182;&#22686;&#24378;&#23427;&#20204;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#23613;&#31649;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#20102;&#27169;&#22411;&#33258;&#25105;&#22686;&#24378;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#22312;&#26377;&#25928;&#35299;&#20915;&#26410;&#30693;&#20107;&#23454;&#24187;&#35273;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20351;&#29992;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#22686;&#24378;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#35299;&#20915;&#19981;&#21516;KG&#26469;&#28304;&#20043;&#38388;&#30340;&#27867;&#21270;&#21644;&#24320;&#25918;&#24335;&#31572;&#26696;&#38382;&#39064;&#30340;&#22686;&#24378;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20266;&#22270;&#29983;&#25104;&#21644;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#20266;&#22270;&#29983;&#25104;&#26469;&#23454;&#29616;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#29615;&#22659;&#20013;&#20351;&#29992;KG&#22686;&#24378;LLM&#12290;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#21033;&#29992;&#21407;&#23376;&#32423;&#30693;&#35782;&#26597;&#35810;&#21644;&#39564;&#35777;&#26469;&#23454;&#29616;&#22312;&#19981;&#21516;KG&#26469;&#28304;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;ROUGE-L&#20998;&#25968;&#19978;&#33267;&#23569;&#25552;&#21319;&#20102;11.5&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09911v1 Announce Type: cross  Abstract: Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ende
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09906</link><description>&lt;p&gt;
&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Generative Representational Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#37117;&#21487;&#20197;&#24402;&#32467;&#20026;&#29983;&#25104;&#25110;&#23884;&#20837;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#21482;&#33021;&#22312;&#20854;&#20013;&#19968;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#26469;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#20174;&#32780;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#65288;MTEB&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#21516;&#31561;&#35268;&#27169;&#30340;&#25152;&#26377;&#27169;&#22411;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;GritLM 8x7B&#22312;&#23581;&#35797;&#30340;&#25152;&#26377;&#24320;&#25918;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;GRIT&#21487;&#20197;&#19982;&#20165;&#22312;&#29983;&#25104;&#25110;&#23884;&#20837;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#32479;&#19968;&#20004;&#32773;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36890;&#36807;GRIT&#30340;&#32479;&#19968;&#21487;&#20197;&#23558;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#25552;&#39640;60%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09906v1 Announce Type: cross  Abstract: All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by &gt; 60% for long documents, 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09900</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Recurrent Reinforcement Learning with Memory Monoids
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20687;RNN&#21644;transformers&#36825;&#26679;&#30340;&#35760;&#24518;&#27169;&#22411;&#36890;&#36807;&#23558;&#36712;&#36857;&#26144;&#23556;&#21040;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#26469;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38271;&#24207;&#21015;&#30340;&#35268;&#27169;&#21270;&#22788;&#29702;&#33021;&#21147;&#24182;&#19981;&#29305;&#21035;&#22909;&#65292;&#23588;&#20854;&#26159;&#19982;&#19968;&#31867;&#26032;&#20852;&#30340;&#35760;&#24518;&#27169;&#22411;&#65288;&#26377;&#26102;&#31216;&#20026;&#32447;&#24615;&#24490;&#29615;&#27169;&#22411;&#65289;&#30456;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#24490;&#29615;&#26356;&#26032;&#26159;&#19968;&#20010;&#21333;&#23376;&#65292;&#22240;&#27492;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20256;&#32479;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#21033;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#22686;&#21152;&#20102;&#22238;&#25253;&#65292;&#24182;&#31616;&#21270;&#20102;&#24490;&#29615;&#20002;&#22833;&#20989;&#25968;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09900v1 Announce Type: cross  Abstract: In RL, memory models such as RNNs and transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32437;&#21521;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29087;&#24713;&#21270;&#38454;&#27573;&#21518;&#65292;&#29992;&#25143;&#24863;&#30693;&#21040;&#30340;&#31995;&#32479;&#25928;&#29992;&#25552;&#39640;&#20102;&#12290;</title><link>https://arxiv.org/abs/2402.09894</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#26159;&#26032;&#39062;&#24615;&#65306;&#20851;&#20110;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#25928;&#29992;&#21644;&#23450;&#21046;&#21270;&#30340;&#32437;&#21521;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09894
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32437;&#21521;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29087;&#24713;&#21270;&#38454;&#27573;&#21518;&#65292;&#29992;&#25143;&#24863;&#30693;&#21040;&#30340;&#31995;&#32479;&#25928;&#29992;&#25552;&#39640;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#20026;&#20154;&#20204;&#22312;&#26085;&#24120;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26032;&#39062;&#32780;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#26377;&#35768;&#22810;AI&#24037;&#20316;&#27969;&#31243;&#36890;&#36807;&#23558;AI&#36755;&#20986;&#19982;&#20154;&#31867;&#20114;&#21160;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#30495;&#23454;&#32780;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;AI&#20855;&#26377;&#26080;&#21487;&#21542;&#35748;&#30340;&#21560;&#24341;&#21147;&#65292;&#20294;&#22312;&#26032;&#40092;&#24863;&#28040;&#22833;&#21518;&#65292;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#22914;&#20309;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#26500;&#24314;&#30340;&#24037;&#20855;&#20855;&#26377;&#20010;&#24615;&#21270;&#21644;&#24555;&#36895;&#36866;&#24212;&#30340;&#28508;&#21147;&#65292;&#20294;&#29992;&#25143;&#26159;&#21542;&#20805;&#20998;&#21033;&#29992;&#20102;&#20010;&#24615;&#21270;&#30340;&#21487;&#33021;&#24615;&#21602;&#65311;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20026;&#26399;&#19977;&#21608;&#30340;&#32437;&#21521;&#30740;&#31350;&#65292;&#20849;&#26377;12&#20010;&#29992;&#25143;&#65292;&#26088;&#22312;&#20102;&#35299;&#31185;&#23398;&#20256;&#25773;&#20013;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#30340;&#29087;&#24713;&#24230;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29087;&#24713;&#21270;&#38454;&#27573;&#25345;&#32493;&#20102;4.3&#20010;&#20250;&#35805;&#65292;&#29992;&#25143;&#22312;&#36825;&#20010;&#38454;&#27573;&#25506;&#32034;&#24037;&#20316;&#27969;&#31243;&#30340;&#21151;&#33021;&#20197;&#21450;&#20182;&#20204;&#21457;&#29616;&#21738;&#20123;&#26041;&#38754;&#26377;&#29992;&#12290;&#22312;&#29087;&#24713;&#21270;&#21518;&#65292;&#31995;&#32479;&#30340;&#24863;&#30693;&#25928;&#29992;&#35780;&#20998;&#39640;&#20110;&#20043;&#21069;&#65292;&#34920;&#26126;&#20102;&#24863;&#30693;&#25928;&#29992;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09894v1 Announce Type: cross  Abstract: Generative AI brings novel and impressive abilities to help people in everyday tasks. There are many AI workflows that solve real and complex problems by chaining AI outputs together with human interaction. Although there is an undeniable lure of AI, it's uncertain how useful generative AI workflows are after the novelty wears off. Additionally, tools built with generative AI have the potential to be personalized and adapted quickly and easily, but do users take advantage of the potential to customize? We conducted a three-week longitudinal study with 12 users to understand the familiarization and customization of generative AI tools for science communication. Our study revealed that the familiarization phase lasts for 4.3 sessions, where users explore the capabilities of the workflow and which aspects they find useful. After familiarization, the perceived utility of the system is rated higher than before, indicating that the perceived
&lt;/p&gt;</description></item><item><title>Lester&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#21644;&#36319;&#36394;&#23454;&#29616;Rotoscope&#21160;&#30011;&#30340;&#26032;&#26041;&#27861;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09883</link><description>&lt;p&gt;
Lester: &#36890;&#36807;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#21644;&#36319;&#36394;&#23454;&#29616;Rotoscope&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Lester: rotoscope animation through video object segmentation and tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09883
&lt;/p&gt;
&lt;p&gt;
Lester&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#21644;&#36319;&#36394;&#23454;&#29616;Rotoscope&#21160;&#30011;&#30340;&#26032;&#26041;&#27861;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Lester&#65292;&#19968;&#31181;&#20174;&#35270;&#39057;&#20013;&#33258;&#21160;&#21512;&#25104;&#22797;&#21476;&#39118;&#26684;2D&#21160;&#30011;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#23558;&#25361;&#25112;&#30475;&#20316;&#26159;&#23545;&#35937;&#20998;&#21106;&#21644;&#36319;&#36394;&#38382;&#39064;&#12290;&#35270;&#39057;&#24103;&#20351;&#29992;Segment Anything Model (SAM)&#36827;&#34892;&#22788;&#29702;&#65292;&#29983;&#25104;&#30340;&#25513;&#27169;&#36890;&#36807;DeAOT&#36827;&#34892;&#21518;&#32493;&#24103;&#30340;&#36319;&#36394;&#65292;DeAOT&#26159;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#30340;&#23618;&#27425;&#20256;&#25773;&#26041;&#27861;&#12290;&#25513;&#27169;&#30340;&#20960;&#20309;&#36718;&#24275;&#20351;&#29992;Douglas-Peucker&#31639;&#27861;&#36827;&#34892;&#31616;&#21270;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#28155;&#21152;&#38754;&#37096;&#29305;&#24449;&#12289;&#20687;&#32032;&#21270;&#21644;&#22522;&#26412;&#38452;&#24433;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#33021;&#22815;&#27491;&#30830;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#23039;&#21183;&#21644;&#22806;&#35266;&#12289;&#21160;&#24577;&#38236;&#22836;&#12289;&#37096;&#20998;&#38236;&#22836;&#21644;&#22810;&#26679;&#32972;&#26223;&#30340;&#35270;&#39057;&#12290;&#19982;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#27969;&#27700;&#32447;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#21644;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09883v1 Announce Type: cross  Abstract: This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos. The method approaches the challenge mainly as an object segmentation and tracking problem. Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation. The geometry of the masks' contours is simplified with the Douglas-Peucker algorithm. Finally, facial traits, pixelation and a basic shadow effect can be optionally added. The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds. The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#35780;&#20272;&#30740;&#31350;&#20102;23&#20010;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#30495;&#23454;&#25512;&#29702;&#34913;&#37327;&#22256;&#38590;&#12289;&#23454;&#29616;&#19981;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#38656;&#35201;&#26631;&#20934;&#21270;&#26041;&#27861;&#12289;&#30417;&#31649;&#30830;&#23450;&#24615;&#21644;&#20262;&#29702;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.09880</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#30340;&#19981;&#36275;&#20043;&#22788;
&lt;/p&gt;
&lt;p&gt;
Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#35780;&#20272;&#30740;&#31350;&#20102;23&#20010;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#30495;&#23454;&#25512;&#29702;&#34913;&#37327;&#22256;&#38590;&#12289;&#23454;&#29616;&#19981;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#38656;&#35201;&#26631;&#20934;&#21270;&#26041;&#27861;&#12289;&#30417;&#31649;&#30830;&#23450;&#24615;&#21644;&#20262;&#29702;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38543;&#30528;&#20854;&#26032;&#20852;&#33021;&#21147;&#30340;&#24555;&#36895;&#23835;&#36215;&#65292;&#24341;&#21457;&#20102;&#20844;&#20247;&#30340;&#22909;&#22855;&#24515;&#65292;&#20197;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;LLMs&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#20182;&#20204;&#30340;LLM&#22522;&#20934;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#36825;&#20123;&#22522;&#20934;&#30340;&#21021;&#27493;&#19981;&#36275;&#65292;&#24320;&#22987;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#36890;&#36807;&#20154;&#20204;&#12289;&#36807;&#31243;&#21644;&#25216;&#26415;&#30340;&#35270;&#35282;&#65292;&#20197;&#21151;&#33021;&#21644;&#23433;&#20840;&#20004;&#22823;&#25903;&#26609;&#20026;&#22522;&#30784;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#22522;&#20934;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20123;&#37325;&#22823;&#38480;&#21046;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#27979;&#37327;&#30495;&#23454;&#25512;&#29702;&#30340;&#22256;&#38590;&#12289;&#36866;&#24212;&#24615;&#12289;&#23454;&#29616;&#19981;&#19968;&#33268;&#24615;&#12289;&#25552;&#31034;&#24037;&#31243;&#22797;&#26434;&#24615;&#12289;&#35780;&#20272;&#32773;&#22810;&#26679;&#24615;&#20197;&#21450;&#22312;&#19968;&#27425;&#32508;&#21512;&#35780;&#20272;&#20013;&#24573;&#35270;&#20102;&#25991;&#21270;&#21644;&#24847;&#35782;&#24418;&#24577;&#35268;&#33539;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#24378;&#35843;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#36843;&#20999;&#38656;&#35201;&#26631;&#20934;&#21270;&#26041;&#27861;&#12289;&#30417;&#31649;&#30830;&#23450;&#24615;&#21644;&#20262;&#29702;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09880v1 Announce Type: new  Abstract: The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of functionality and security. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligenc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#30340;&#19977;&#31181;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35268;&#21010;&#30340;&#32534;&#35793;&#25216;&#26415;&#65292;&#21487;&#20197;&#29983;&#25104;&#21160;&#20316;&#25104;&#26412;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.09877</link><description>&lt;p&gt;
&#35745;&#31639;&#20855;&#26377;&#32479;&#19968;&#21160;&#20316;&#25104;&#26412;&#30340;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
On Computing Plans with Uniform Action Costs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#30340;&#19977;&#31181;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35268;&#21010;&#30340;&#32534;&#35793;&#25216;&#26415;&#65292;&#21487;&#20197;&#29983;&#25104;&#21160;&#20316;&#25104;&#26412;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#35268;&#21010;&#24212;&#29992;&#20013;&#65292;&#20195;&#29702;&#20154;&#21487;&#33021;&#26377;&#20852;&#36259;&#25214;&#21040;&#21160;&#20316;&#25104;&#26412;&#23613;&#21487;&#33021;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;&#36825;&#26679;&#30340;&#35745;&#21010;&#20026;&#20195;&#29702;&#20154;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#65292;&#36825;&#22312;&#20154;&#31867;&#25191;&#34892;&#35268;&#21010;&#24037;&#20855;&#24314;&#35758;&#30340;&#35745;&#21010;&#26102;&#26159;&#20851;&#38190;&#29305;&#24449;&#12290;&#26412;&#25991;&#23558;&#19977;&#20010;&#19968;&#33268;&#24615;&#24230;&#37327;&#24212;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35268;&#21010;&#30340;&#32534;&#35793;&#25216;&#26415;&#65292;&#20801;&#35768;&#20197;&#21160;&#20316;&#25104;&#26412;&#24635;&#21644;&#21644;&#21160;&#20316;&#25104;&#26412;&#19968;&#33268;&#24615;&#36827;&#34892;&#35789;&#20856;&#25490;&#24207;&#26368;&#20248;&#21270;&#12290;&#22312;&#30693;&#21517;&#21644;&#26032;&#39062;&#30340;&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#37325;&#26500;&#30340;&#20219;&#21153;&#20197;&#29983;&#25104;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09877v1 Announce Type: new  Abstract: In many real-world planning applications, agents might be interested in finding plans whose actions have costs that are as uniform as possible. Such plans provide agents with a sense of stability and predictability, which are key features when humans are the agents executing plans suggested by planning tools. This paper adapts three uniformity metrics to automated planning, and introduce planning-based compilations that allow to lexicographically optimize sum of action costs and action costs uniformity. Experimental results both in well-known and novel planning benchmarks show that the reformulated tasks can be effectively solved in practice to generate uniform plans.
&lt;/p&gt;</description></item><item><title>MuChin&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#29702;&#35299;&#21644;&#25551;&#36848;&#26041;&#38754;&#24615;&#33021;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.09871</link><description>&lt;p&gt;
MuChin&#65306;&#29992;&#20110;&#35780;&#20272;&#38899;&#20048;&#39046;&#22495;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09871
&lt;/p&gt;
&lt;p&gt;
MuChin&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#29702;&#35299;&#21644;&#25551;&#36848;&#26041;&#38754;&#24615;&#33021;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#22522;&#20934;&#26469;&#32479;&#19968;&#35780;&#20272;&#23427;&#20204;&#22312;&#29702;&#35299;&#21644;&#20197;&#25991;&#23383;&#25551;&#36848;&#38899;&#20048;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#31639;&#27861;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#19987;&#19994;&#20154;&#22763;&#21644;&#20844;&#20247;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#27880;&#37322;&#30340;&#20302;&#31934;&#24230;&#65292;&#29616;&#26377;&#30340;&#38899;&#20048;&#25551;&#36848;&#25968;&#25454;&#38598;&#26080;&#27861;&#20316;&#20026;&#22522;&#20934;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MuChin&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#30340;&#24320;&#28304;&#38899;&#20048;&#25551;&#36848;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;LLMs&#22312;&#29702;&#35299;&#21644;&#25551;&#36848;&#38899;&#20048;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#37319;&#34411;&#38899;&#20048;&#27880;&#37322;&#24179;&#21488;&#65288;CaiMAP&#65289;&#65292;&#37319;&#29992;&#21019;&#26032;&#30340;&#22810;&#20154;&#12289;&#22810;&#38454;&#27573;&#20445;&#35777;&#26041;&#27861;&#65292;&#24182;&#25307;&#21215;&#20102;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#22763;&#65292;&#20197;&#30830;&#20445;&#27880;&#37322;&#30340;&#31934;&#24230;&#21644;&#19982;&#27969;&#34892;&#35821;&#20041;&#30340;&#23545;&#40784;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09871v1 Announce Type: cross  Abstract: The rapidly evolving multimodal Large Language Models (LLMs) urgently require new benchmarks to uniformly evaluate their performance on understanding and textually describing music. However, due to semantic gaps between Music Information Retrieval (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as benchmarks. To this end, we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music. We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics. Utilizing this method, we built a dataset w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23884;&#20837;&#24335;&#22810;&#26680;&#24179;&#21488;&#19978;&#65292;&#37319;&#29992;&#30005;&#27744;&#20379;&#30005;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#35843;&#25972;&#36817;&#20284;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#33021;&#37327;&#39044;&#31639;&#20869;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#33021;&#37327;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.09867</link><description>&lt;p&gt;
&#22312;&#23884;&#20837;&#24335;&#22810;&#26680;&#24179;&#21488;&#19978;&#34920;&#24449; EEG &#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23884;&#20837;&#24335;&#22810;&#26680;&#24179;&#21488;&#19978;&#65292;&#37319;&#29992;&#30005;&#27744;&#20379;&#30005;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#35843;&#25972;&#36817;&#20284;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#33021;&#37327;&#39044;&#31639;&#20869;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#33021;&#37327;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30005;&#27744;&#20379;&#30005;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#65292;&#20197;&#30417;&#27979;&#33041;&#27963;&#21160;&#21644;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#36825;&#20123;&#24212;&#29992;&#38656;&#35201;&#38271;&#26102;&#38388;&#36830;&#32493;&#22788;&#29702;&#20197;&#29983;&#25104;&#21487;&#34892;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21487;&#31359;&#25140;&#35774;&#22791;&#30001;&#20110;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#23567;&#23610;&#23544;&#32780;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#33021;&#37327;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#38480;&#21046;&#30340;&#33021;&#37327;&#39044;&#31639;&#20869;&#65292;&#23884;&#20837;&#24335;&#24322;&#26500;&#22810;&#26680;&#24179;&#21488;&#65288;HMPs&#65289;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#21487;&#20197;&#36827;&#19968;&#27493;&#21033;&#29992; EEG &#24212;&#29992;&#31243;&#24207;&#27969;&#31243;&#30340;&#38169;&#35823;&#38887;&#24615;&#26469;&#26368;&#22823;&#21270; HMPs &#30340;&#24615;&#33021;&#21644;&#33021;&#37327;&#25910;&#30410;&#12290;&#28982;&#32780;&#65292;&#22312;&#23884;&#20837;&#24335; HMPs &#19978;&#35268;&#33539;&#35843;&#25972;&#36817;&#20284;&#38656;&#35201;&#23545;&#20934;&#30830;&#24615;-&#24615;&#33021;-&#21151;&#32791;&#26435;&#34913;&#31354;&#38388;&#36827;&#34892;&#24443;&#24213;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#19977;&#31181; EEG &#24212;&#29992;&#65288;&#21253;&#25324;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#12289;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#21644;&#21387;&#21147;&#26816;&#27979;&#65289;&#30340;&#38169;&#35823;&#38887;&#24615;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09867v1 Announce Type: cross  Abstract: Electroencephalography (EEG) recordings are analyzed using battery-powered wearable devices to monitor brain activities and neurological disorders. These applications require long and continuous processing to generate feasible results. However, wearable devices are constrained with limited energy and computation resources, owing to their small sizes for practical use cases. Embedded heterogeneous multi-core platforms (HMPs) can provide better performance within limited energy budgets for EEG applications. Error resilience of the EEG application pipeline can be exploited further to maximize the performance and energy gains with HMPs. However, disciplined tuning of approximation on embedded HMPs requires a thorough exploration of the accuracy-performance-power trade-off space. In this work, we characterize the error resilience of three EEG applications, including Epileptic Seizure Detection, Sleep Stage Classification, and Stress Detecti
&lt;/p&gt;</description></item><item><title>Jack of All Trades (JAT)&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26435;&#37325;&#38598;&#65292;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#22343;&#33021;&#21462;&#24471;&#24378;&#22823;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#23427;&#26159;&#39318;&#20010;&#23454;&#29616;&#35813;&#30446;&#26631;&#30340;&#24320;&#25918;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09844</link><description>&lt;p&gt;
&#35832;&#22810;&#25165;&#33402;&#65292;&#20854;&#20013;&#19968;&#20123;&#26159;&#22823;&#24072;&#65306;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#36716;&#25442;&#22120;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09844
&lt;/p&gt;
&lt;p&gt;
Jack of All Trades (JAT)&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26435;&#37325;&#38598;&#65292;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#22343;&#33021;&#21462;&#24471;&#24378;&#22823;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#23427;&#26159;&#39318;&#20010;&#23454;&#29616;&#35813;&#30446;&#26631;&#30340;&#24320;&#25918;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#23547;&#25214;&#19968;&#20010;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#26080;&#32541;&#36816;&#20316;&#30340;&#36890;&#29992;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#20027;&#27969;&#30340;&#26041;&#27861;&#24448;&#24448;&#23558;&#27169;&#22411;&#38480;&#21046;&#22312;&#21333;&#19968;&#20219;&#21153;&#21644;&#21333;&#27169;&#24577;&#26694;&#26550;&#20013;&#65292;&#36825;&#19968;&#38480;&#21046;&#19982;&#36890;&#29992;&#30340;&#12289;&#22810;&#39046;&#22495;&#27169;&#22411;&#30340;&#24191;&#38420;&#24895;&#26223;&#30456;&#30683;&#30462;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Jack of All Trades (JAT) &#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20854;&#29420;&#29305;&#35774;&#35745;&#20248;&#21270;&#20102;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#31867;&#22411;&#30340;&#33021;&#21147;&#12290;JAT&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26435;&#37325;&#38598;&#65292;&#22312;&#38750;&#24120;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#29616;&#20102;&#20854;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;JAT&#27169;&#22411;&#26159;&#26397;&#30528;&#26356;&#36890;&#29992;&#12289;&#36328;&#39046;&#22495;&#30340;AI&#27169;&#22411;&#35774;&#35745;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#24182;&#19988;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#26159;&#39318;&#20010;&#23436;&#20840;&#24320;&#25918;&#30340;&#36825;&#19968;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09844v1 Announce Type: new  Abstract: The search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research. The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model. In this paper, we present Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types. The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights. The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#65292;&#23558;&#20154;&#31867;&#31227;&#21160;&#29983;&#25104;&#37325;&#26032;&#23450;&#20041;&#20026;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#31227;&#21160;&#29983;&#25104;&#25512;&#29702;&#26694;&#26550;&#65288;MobiGeaR&#65289;&#65292;&#23558;LLMs&#36882;&#24402;&#29983;&#25104;&#31227;&#21160;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.09836</link><description>&lt;p&gt;
&#36229;&#36234;&#27169;&#20223;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#29983;&#25104;&#20154;&#31867;&#31227;&#21160;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#65292;&#23558;&#20154;&#31867;&#31227;&#21160;&#29983;&#25104;&#37325;&#26032;&#23450;&#20041;&#20026;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#31227;&#21160;&#29983;&#25104;&#25512;&#29702;&#26694;&#26550;&#65288;MobiGeaR&#65289;&#65292;&#23558;LLMs&#36882;&#24402;&#29983;&#25104;&#31227;&#21160;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#34892;&#20026;&#19982;&#20132;&#36890;&#25317;&#22581;&#12289;&#30123;&#24773;&#25511;&#21046;&#31561;&#37325;&#35201;&#31038;&#20250;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#31227;&#21160;&#24615;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#19988;&#28041;&#21450;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#24615;&#31227;&#21160;&#27169;&#22411;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#20026;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#29983;&#25104;&#26032;&#30340;&#31227;&#21160;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#33021;&#26377;&#25928;&#22320;&#25429;&#25417;&#39537;&#21160;&#31227;&#21160;&#34892;&#20026;&#30340;&#36830;&#36143;&#24847;&#22270;&#65292;&#23548;&#33268;&#26679;&#26412;&#25928;&#29575;&#21644;&#35821;&#20041;&#24863;&#30693;&#24230;&#20302;&#12290;&#21463;&#21040;LLMs&#20013;&#26032;&#20852;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#26412;&#30340;&#35270;&#35282;&#36716;&#21464;&#65292;&#23558;&#31227;&#21160;&#29983;&#25104;&#37325;&#26032;&#23450;&#20041;&#20026;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31227;&#21160;&#29983;&#25104;&#25512;&#29702;&#65288;MobiGeaR&#65289;&#26694;&#26550;&#65292;&#20419;&#20351;LLMs&#36882;&#24402;&#29983;&#25104;&#31227;&#21160;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09836v1 Announce Type: new  Abstract: Human mobility behaviours are closely linked to various important societal problems such as traffic congestion, and epidemic control. However, collecting mobility data can be prohibitively expensive and involves serious privacy issues, posing a pressing need for high-quality generative mobility models. Previous efforts focus on learning the behaviour distribution from training samples, and generate new mobility data by sampling the learned distributions. They cannot effectively capture the coherent intentions that drive mobility behavior, leading to low sample efficiency and semantic-awareness. Inspired by the emergent reasoning ability in LLMs, we propose a radical perspective shift that reformulates mobility generation as a commonsense reasoning problem. In this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR) framework that prompts LLM to recursively generate mobility behaviour. Specifically, we design a context-aw
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#24212;&#29992;&#65292;&#27604;&#36739;&#20102;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#26500;&#24314;&#23545;&#25239;&#24615;&#39564;&#35777;&#22270;&#30340;&#38598;&#21512;&#65292;&#26377;&#25928;&#38450;&#27490;&#20102;&#30001;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#31995;&#32479;&#24341;&#36215;&#30340;&#27450;&#35784;&#65292;&#24182;&#30830;&#20445;&#20132;&#26131;&#20013;&#30340;&#29992;&#25143;&#26159;&#30495;&#23454;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.09830</link><description>&lt;p&gt;
&#21033;&#29992;GAN&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#65306;&#20351;&#29992;&#21512;&#25104;&#20132;&#26131;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#24212;&#29992;&#65292;&#27604;&#36739;&#20102;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#26500;&#24314;&#23545;&#25239;&#24615;&#39564;&#35777;&#22270;&#30340;&#38598;&#21512;&#65292;&#26377;&#25928;&#38450;&#27490;&#20102;&#30001;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#31995;&#32479;&#24341;&#36215;&#30340;&#27450;&#35784;&#65292;&#24182;&#30830;&#20445;&#20132;&#26131;&#20013;&#30340;&#29992;&#25143;&#26159;&#30495;&#23454;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#26088;&#22312;&#35782;&#21035;&#20559;&#31163;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#30340;&#23454;&#20363;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#24212;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#30340;&#20248;&#21183;&#12290;GAN&#26159;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#31867;&#22411;&#65292;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#65292;&#20351;&#20854;&#25104;&#20026;&#24322;&#24120;&#26816;&#27979;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#25551;&#36848;&#20102;GAN&#21450;&#20854;&#34893;&#29983;&#27169;&#22411;&#30340;&#21407;&#21017;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27450;&#35784;&#26816;&#27979;&#24212;&#29992;&#12290;&#36890;&#36807;&#26500;&#24314;&#23545;&#25239;&#24615;&#39564;&#35777;&#22270;&#30340;&#38598;&#21512;&#65292;&#25105;&#20204;&#23558;&#26377;&#25928;&#38450;&#27490;&#30001;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#31995;&#32479;&#24341;&#36215;&#30340;&#27450;&#35784;&#65292;&#24182;&#30830;&#20445;&#20132;&#26131;&#20013;&#30340;&#29992;&#25143;&#26159;&#30495;&#23454;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09830v1 Announce Type: cross  Abstract: Anomaly detection is a critical challenge across various research domains, aiming to identify instances that deviate from normal data distributions. This paper explores the application of Generative Adversarial Networks (GANs) in fraud detection, comparing their advantages with traditional methods. GANs, a type of Artificial Neural Network (ANN), have shown promise in modeling complex data distributions, making them effective tools for anomaly detection. The paper systematically describes the principles of GANs and their derivative models, emphasizing their application in fraud detection across different datasets. And by building a collection of adversarial verification graphs, we will effectively prevent fraud caused by bots or automated systems and ensure that the users in the transaction are real. The objective of the experiment is to design and implement a fake face verification code and fraud detection system based on Generative A
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#24182;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;&#12290;&#30446;&#21069;&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#24448;&#24448;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#27861;&#36866;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.09820</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#24182;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;&#12290;&#30446;&#21069;&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#24448;&#24448;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#27861;&#36866;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#26102;&#20195;&#65292;&#20154;&#20204;&#30340;&#29983;&#27963;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#20170;&#22825;&#30340;&#32593;&#32476;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#25216;&#26415;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#32473;&#20154;&#20204;&#24102;&#26469;&#20415;&#21033;&#30340;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#23433;&#20840;&#25361;&#25112;&#12290;&#20445;&#25345;&#32593;&#32476;&#23433;&#20840;&#21644;&#20445;&#25252;&#29992;&#25143;&#30340;&#21512;&#27861;&#21033;&#30410;&#26159;&#32593;&#32476;&#24314;&#35774;&#30340;&#26680;&#24515;&#12290;&#23041;&#32961;&#26816;&#27979;&#26159;&#19968;&#20010;&#23436;&#25972;&#26377;&#25928;&#30340;&#38450;&#24481;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#32593;&#32476;&#20449;&#24687;&#23433;&#20840;&#39046;&#22495;&#65292;&#32593;&#32476;&#25915;&#20987;&#21644;&#32593;&#32476;&#38450;&#25252;&#30340;&#25216;&#26415;&#26356;&#26032;&#26085;&#30410;&#36805;&#29467;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#26159;&#32593;&#32476;&#38450;&#25252;&#30340;&#20851;&#27880;&#28966;&#28857;&#20043;&#19968;&#12290;&#30446;&#21069;&#65292;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#36890;&#24120;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21019;&#24314;&#20154;&#24037;&#35268;&#21017;&#25110;&#25552;&#21462;&#24120;&#35265;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26410;&#30693;&#23041;&#32961;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#31995;&#32479;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09820v1 Announce Type: cross  Abstract: In the age of the Internet, people's lives are increasingly dependent on today's network technology. However, network technology is a double-edged sword, bringing convenience to people but also posing many security challenges. Maintaining network security and protecting the legitimate interests of users is at the heart of network construction. Threat detection is an important part of a complete and effective defense system. In the field of network information security, the technical update of network attack and network protection is spiraling. How to effectively detect unknown threats is one of the concerns of network protection. Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown threats causes the detection accuracy of the or
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20808;&#36827;&#25968;&#25454;&#34701;&#21512;&#26550;&#26500;&#65292;&#22312;&#19981;&#23558;&#25968;&#25454;&#31227;&#21160;&#21040;&#38598;&#20013;&#20301;&#32622;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#21516;&#26102;&#65292;&#22810;&#20010;&#21442;&#19982;&#26041;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.09795</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20808;&#36827;&#25968;&#25454;&#34701;&#21512;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
An advanced data fabric architecture leveraging homomorphic encryption and federated learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20808;&#36827;&#25968;&#25454;&#34701;&#21512;&#26550;&#26500;&#65292;&#22312;&#19981;&#23558;&#25968;&#25454;&#31227;&#21160;&#21040;&#38598;&#20013;&#20301;&#32622;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#21516;&#26102;&#65292;&#22810;&#20010;&#21442;&#19982;&#26041;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#34701;&#21512;&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#25968;&#25454;&#31649;&#29702;&#32479;&#19968;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#25968;&#25454;&#38382;&#39064;&#32780;&#26080;&#38656;&#23558;&#25968;&#25454;&#31227;&#21160;&#21040;&#38598;&#20013;&#20301;&#32622;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#20840;&#23616;&#27169;&#22411;&#26159;&#22522;&#20110;&#22810;&#20010;&#26412;&#22320;&#27169;&#22411;&#30340;&#23398;&#20064;&#21442;&#25968;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23558;&#25968;&#25454;&#31227;&#21160;&#21040;&#38598;&#20013;&#23384;&#20648;&#24211;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#37096;&#20998;&#21516;&#24577;&#21152;&#23494;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#34701;&#21512;&#26550;&#26500;&#20013;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#22810;&#20010;&#21442;&#19982;&#26041;&#21487;&#20197;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#36827;&#34892;&#21327;&#20316;&#65292;&#32780;&#26080;&#38656;&#20132;&#25442;&#21407;&#22987;&#25968;&#25454;&#65292;&#32780;&#26159;&#20351;&#29992;&#23398;&#20064;&#25110;&#34701;&#21512;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#31526;&#21512;HIPAA&#21644;GDPR&#31561;&#27861;&#24459;&#27861;&#35268;&#65292;&#30830;&#20445;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23545;&#22402;&#20307;&#30244;&#20998;&#31867;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09795v1 Announce Type: cross  Abstract: Data fabric is an automated and AI-driven data fusion approach to accomplish data management unification without moving data to a centralized location for solving complex data problems. In a Federated learning architecture, the global model is trained based on the learned parameters of several local models that eliminate the necessity of moving data to a centralized repository for machine learning. This paper introduces a secure approach for medical image analysis using federated learning and partially homomorphic encryption within a distributed data fabric architecture. With this method, multiple parties can collaborate in training a machine-learning model without exchanging raw data but using the learned or fused features. The approach complies with laws and regulations such as HIPAA and GDPR, ensuring the privacy and security of the data. The study demonstrates the method's effectiveness through a case study on pituitary tumor class
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33033;&#20914;&#21015;&#35774;&#35745;&#34917;&#20607;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;Charge Trap Flash&#65288;CTF&#65289;&#22120;&#20214;&#20013;&#38750;&#29702;&#24819;&#31243;&#24207;&#26102;&#38388;&#25152;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.09792</link><description>&lt;p&gt;
&#22522;&#20110;Charge Trap Flash&#65288;CTF&#65289;&#30340;&#38750;&#29702;&#24819;&#31243;&#24207;&#26102;&#38388;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31995;&#32479;&#32423;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on Deep Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09792
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33033;&#20914;&#21015;&#35774;&#35745;&#34917;&#20607;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;Charge Trap Flash&#65288;CTF&#65289;&#22120;&#20214;&#20013;&#38750;&#29702;&#24819;&#31243;&#24207;&#26102;&#38388;&#25152;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#30005;&#38459;&#22788;&#29702;&#21333;&#20803;&#65288;Resistive Processing Unit, RPU&#65289;&#26550;&#26500;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23398;&#20064;&#26159;&#19968;&#31181;&#33021;&#25928;&#39640;&#30340;&#20570;&#27861;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#20102;&#19987;&#29992;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#65292;&#24182;&#21033;&#29992;&#38543;&#26426;&#35745;&#31639;&#30340;&#26435;&#37325;&#26356;&#26032;&#36827;&#34892;&#20869;&#23384;&#35745;&#31639;&#12290;Charge Trap Flash&#65288;CTF&#65289;&#22120;&#20214;&#21487;&#20197;&#23454;&#29616;DNN&#20013;&#22522;&#20110;RPU&#30340;&#26435;&#37325;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;CTF&#22522;&#20110;RPU&#30340;&#26435;&#37325;&#26356;&#26032;&#20013;&#65292;&#38750;&#29702;&#24819;&#31243;&#24207;&#26102;&#38388;&#65288;V_T&#65289;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#38750;&#29702;&#24819;&#31243;&#24207;&#26102;&#38388;&#21463;CTF&#30340;&#20004;&#20010;&#22240;&#32032;&#30340;&#24433;&#21709;&#65306;&#31532;&#19968;&#20010;&#26159;&#36755;&#20837;&#33033;&#20914;&#25968;&#37327;&#65288;N&#65289;&#25110;&#33033;&#20914;&#23485;&#24230;&#65288;pw&#65289;&#30340;&#24433;&#21709;&#65292;&#31532;&#20108;&#20010;&#26159;&#29992;&#20110;&#38543;&#26426;&#35745;&#31639;&#26435;&#37325;&#26356;&#26032;&#30340;&#26356;&#26032;&#33033;&#20914;&#20043;&#38388;&#30340;&#38388;&#38548;&#26102;&#38388;&#65288;t_gap&#65289;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#30740;&#31350;&#36825;&#31181;&#38750;&#29702;&#24819;&#31243;&#24207;&#26102;&#38388;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#27169;&#25311;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#33033;&#20914;&#21015;&#35774;&#35745;&#34917;&#20607;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;CTF&#30340;&#38750;&#29702;&#24819;&#31243;&#24207;&#26102;&#38388;&#25152;&#24341;&#36215;&#30340;&#24635;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09792v1 Announce Type: cross  Abstract: Learning of deep neural networks (DNN) using Resistive Processing Unit (RPU) architecture is energy-efficient as it utilizes dedicated neuromorphic hardware and stochastic computation of weight updates for in-memory computing. Charge Trap Flash (CTF) devices can implement RPU-based weight updates in DNNs. However, prior work has shown that the weight updates (V_T) in CTF-based RPU are impacted by the non-ideal program time of CTF. The non-ideal program time is affected by two factors of CTF. Firstly, the effects of the number of input pulses (N) or pulse width (pw), and secondly, the gap between successive update pulses (t_gap) used for the stochastic computation of weight updates. Therefore, the impact of this non-ideal program time must be studied for neural network training simulations. In this study, Firstly, we propose a pulse-train design compensation technique to reduce the total error caused by non-ideal program time of CTF and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.09786</link><description>&lt;p&gt;
&#26816;&#26597;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21028;&#21035;&#22120;&#20013;&#30340;&#30149;&#24577;&#20559;&#35265;&#65306;&#20197;StyleGAN3&#27169;&#22411;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#65292;&#24448;&#24448;&#38590;&#20197;&#34987;&#20154;&#31867;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;StyleGAN3&#27169;&#22411;&#20013;&#30340;&#21028;&#21035;&#22120;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#31995;&#32479;&#22320;&#23545;&#24471;&#20998;&#36827;&#34892;&#20998;&#23618;&#65292;&#24182;&#19988;&#36825;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#21028;&#21035;&#22120;&#22312;&#33394;&#24425;&#21644;&#20142;&#24230;&#26041;&#38754;&#23545;&#24863;&#30693;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#30340;&#20559;&#35265;&#65292;&#28982;&#21518;&#26816;&#26597;&#20102;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#20851;&#20110;&#21051;&#26495;&#21360;&#35937;&#30740;&#31350;&#20013;&#24120;&#35265;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22402;&#30452;&#21644;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.09784</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#25509;&#36817;&#24230;&#19978;&#30340;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22402;&#30452;&#21644;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26368;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#25429;&#25417;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#21333;&#21521;&#21644;&#21452;&#21521;&#27169;&#24335;&#65292;&#20294;&#23545;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#22914;&#20010;&#20307;&#34892;&#20026;&#21644;&#31038;&#20250;&#36235;&#21183;&#27169;&#24335;&#65292;&#20173;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#27169;&#22411;&#36890;&#24120;&#24573;&#30053;&#20102;&#22312;&#31867;&#20284;&#30340;&#26102;&#38388;&#27573;&#20869;&#38544;&#21547;&#22312;&#29992;&#25143;&#20043;&#38388;&#21457;&#29983;&#30340;&#29992;&#25143;&#34892;&#20026;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22402;&#30452;&#26102;&#38388;&#25509;&#36817;&#24230;&#12290;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#36866;&#24212;Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#32771;&#34385;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#36866;&#24212;&#22312;&#32771;&#34385;&#39033;&#30446;&#20132;&#20114;&#20013;&#30340;&#27700;&#24179;&#26102;&#38388;&#25509;&#36817;&#24230;&#26041;&#38754;&#20173;&#28982;&#26377;&#38480;&#65292;&#20363;&#22914;&#21306;&#20998;&#22312;&#19968;&#21608;&#20869;&#19982;&#19968;&#20010;&#26376;&#20869;&#36141;&#20080;&#30340;&#36830;&#32493;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09784v1 Announce Type: cross  Abstract: Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored. Notably, recent models often neglect similarities in users' actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity. These models primarily adapt the self-attention mechanisms of the transformer to consider the temporal context in individual user actions. Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a mon
&lt;/p&gt;</description></item><item><title>MC-DBN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;&#65292;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09782</link><description>&lt;p&gt;
MC-DBN&#65306;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MC-DBN: A Deep Belief Network-Based Model for Modality Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09782
&lt;/p&gt;
&lt;p&gt;
MC-DBN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;&#65292;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#32929;&#24066;&#39044;&#27979;&#21644;&#24515;&#29575;&#30417;&#27979;&#39046;&#22495;&#12290;&#21033;&#29992;&#22810;&#26679;&#30340;&#25968;&#25454;&#28304;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#39069;&#22806;&#30340;&#25968;&#25454;&#21487;&#33021;&#19981;&#24635;&#26159;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#21563;&#21512;&#12290;&#25554;&#20540;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#22788;&#29702;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20294;&#22312;&#31232;&#30095;&#20449;&#24687;&#24773;&#20917;&#19979;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#34917;&#20840;&#30340;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#27169;&#22411;&#65288;MC-DBN&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#33258;&#36523;&#19982;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#30830;&#20445;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29305;&#24615;&#23494;&#20999;&#30456;&#31526;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26469;&#33258;&#32929;&#24066;&#39044;&#27979;&#21644;&#24515;&#29575;&#30417;&#27979;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;MC-DBN&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09782v1 Announce Type: cross  Abstract: Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring. Utilizing diverse data sources can substantially improve prediction accuracy. Nonetheless, additional data may not always align with the original dataset. Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information. Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data. It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model. We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09769</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning Using a Single Forward Pass
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09769
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#21333;&#27425;&#20256;&#36882;&#23884;&#20837;&#23398;&#20064;&#31639;&#27861;&#65288;SPELA&#65289;&#12290; SPELA&#26159;&#22312;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#35774;&#22791;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#29702;&#24212;&#29992;&#30340;&#39318;&#36873;&#20505;&#36873;&#20154;&#12290; &#21516;&#26102;&#65292;SPELA&#21487;&#20197;&#26368;&#20339;&#22320;&#28385;&#36275;&#23545;&#30740;&#31350;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#21644;&#24418;&#25104;&#26694;&#26550;&#30340;&#38656;&#27714;&#12290; SPELA&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#22914;&#23884;&#20837;&#21521;&#37327;&#24418;&#24335;&#30340;&#31070;&#32463;&#20808;&#39564;&#30693;&#35782;&#65292;&#19981;&#38656;&#35201;&#26435;&#37325;&#20256;&#36755;&#65292;&#19981;&#38145;&#23450;&#26435;&#37325;&#26356;&#26032;&#65292;&#23436;&#20840;&#23616;&#37096;&#36203;&#27604;&#23433;&#23398;&#20064;&#65292;&#19981;&#23384;&#20648;&#28608;&#27963;&#30340;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#21644;&#27599;&#20010;&#26679;&#26412;&#30340;&#21333;&#27425;&#26435;&#37325;&#26356;&#26032;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;SPELA&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25805;&#20316;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#20010;&#26377;&#22122;&#38899;&#30340;&#24067;&#23572;&#36816;&#31639;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#25191;&#34892;&#38750;&#32447;&#24615;&#20998;&#31867;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SPELA&#22312;MNIST&#65292;KMNIST&#21644;Fashion MNIST&#19978;&#30340;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SPELA&#22312;MNIST&#65292;KMNIST&#21644;Fashion MNIST&#19978;&#30340;&#23569;&#26679;&#26412;&#21644;1&#20010;&#26102;&#26399;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09769v1 Announce Type: new  Abstract: We propose a neuroscience-inspired Solo Pass Embedded Learning Algorithm (SPELA). SPELA is a prime candidate for training and inference applications in Edge AI devices. At the same time, SPELA can optimally cater to the need for a framework to study perceptual representation learning and formation. SPELA has distinctive features such as neural priors (in the form of embedded vectors), no weight transport, no update locking of weights, complete local Hebbian learning, single forward pass with no storage of activations, and single weight update per sample. Juxtaposed with traditional approaches, SPELA operates without the need for backpropagation. We show that our algorithm can perform nonlinear classification on a noisy boolean operation dataset. Additionally, we exhibit high performance using SPELA across MNIST, KMNIST, and Fashion MNIST. Lastly, we show the few-shot and 1-epoch learning capabilities of SPELA on MNIST, KMNIST, and Fashio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;&#22810;&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#26469;&#30740;&#31350;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25512;&#36827;&#20102;&#35780;&#20272;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2402.09766</link><description>&lt;p&gt;
&#20174;&#21464;&#21160;&#24615;&#21040;&#31283;&#23450;&#24615;&#65306;&#25512;&#33616;&#31995;&#32479;&#22522;&#20934;&#21270;&#23454;&#36341;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
From Variability to Stability: Advancing RecSys Benchmarking Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;&#22810;&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#26469;&#30740;&#31350;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25512;&#36827;&#20102;&#35780;&#20272;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#26032;&#30340;&#31639;&#27861;&#32463;&#24120;&#36890;&#36807;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#20219;&#24847;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26469;&#22768;&#31216;&#33258;&#24049;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20840;&#38754;&#21453;&#26144;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#20844;&#24179;&#21644;&#31283;&#20581;&#30340;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#65292;&#20174;&#32780;&#25512;&#36827;&#35780;&#20272;&#23454;&#36341;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#25324;&#26412;&#25991;&#20171;&#32461;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;30&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;9&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;11&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23558;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#32858;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#25490;&#21517;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09766v1 Announce Type: cross  Abstract: In the rapidly evolving domain of Recommender Systems (RecSys), new algorithms frequently claim state-of-the-art performance based on evaluations over a limited set of arbitrarily selected datasets. However, this approach may fail to holistically reflect their effectiveness due to the significant impact of dataset characteristics on algorithm performance. Addressing this deficiency, this paper introduces a novel benchmarking methodology to facilitate a fair and robust comparison of RecSys algorithms, thereby advancing evaluation practices. By utilizing a diverse set of $30$ open datasets, including two introduced in this work, and evaluating $11$ collaborative filtering algorithms across $9$ metrics, we critically examine the influence of dataset characteristics on algorithm performance. We further investigate the feasibility of aggregating outcomes from multiple datasets into a unified ranking. Through rigorous experimental analysis, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#38543;&#26426;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;SVRP&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;&#26368;&#23567;&#21270;&#20102;&#36335;&#24452;&#25104;&#26412;&#65292;&#27169;&#22411;&#22312;&#26053;&#34892;&#25104;&#26412;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#34433;&#32676;&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09765</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#38543;&#26426;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Solving Stochastic Vehicle Routing Problem with Time Windows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#38543;&#26426;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;SVRP&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;&#26368;&#23567;&#21270;&#20102;&#36335;&#24452;&#25104;&#26412;&#65292;&#27169;&#22411;&#22312;&#26053;&#34892;&#25104;&#26412;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#34433;&#32676;&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#38543;&#26426;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;SVRP&#65289;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#20943;&#23569;&#36135;&#29289;&#37197;&#36865;&#20013;&#30340;&#26053;&#34892;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SVRP&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#30340;&#26053;&#34892;&#25104;&#26412;&#21644;&#38656;&#27714;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20855;&#20307;&#30340;&#23458;&#25143;&#30340;&#26102;&#38388;&#31383;&#21475;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#26469;&#26368;&#23567;&#21270;&#36335;&#24452;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22635;&#34917;&#20102;SVRP&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#65292;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32780;&#26159;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#22312;&#26053;&#34892;&#25104;&#26412;&#19978;&#32988;&#36807;&#20102;&#34433;&#32676;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;1.73%&#30340;&#38477;&#20302;&#12290;&#23427;&#29420;&#29305;&#22320;&#25972;&#21512;&#20102;&#22806;&#37096;&#20449;&#24687;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#25104;&#20026;&#26410;&#26469;SVRP&#30740;&#31350;&#21644;&#34892;&#19994;&#24212;&#29992;&#30340;&#26377;&#20215;&#20540;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09765v1 Announce Type: new  Abstract: This paper introduces a reinforcement learning approach to optimize the Stochastic Vehicle Routing Problem with Time Windows (SVRP), focusing on reducing travel costs in goods delivery. We develop a novel SVRP formulation that accounts for uncertain travel costs and demands, alongside specific customer time windows. An attention-based neural network trained through reinforcement learning is employed to minimize routing costs. Our approach addresses a gap in SVRP research, which traditionally relies on heuristic methods, by leveraging machine learning. The model outperforms the Ant-Colony Optimization algorithm, achieving a 1.73% reduction in travel costs. It uniquely integrates external information, demonstrating robustness in diverse environments, making it a valuable benchmark for future SVRP studies and industry application.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#36125;&#22612;&#20998;&#24067;&#21051;&#30011;&#20559;&#22909;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26657;&#20934;&#27169;&#22411;&#19982;&#20559;&#22909;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#26368;&#32456;&#21033;&#29992;&#26399;&#26395;&#22870;&#21169;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.09764</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#24314;&#27169;&#23545;&#40784;&#20247;&#21253;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Aligning Crowd Feedback via Distributional Preference Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#36125;&#22612;&#20998;&#24067;&#21051;&#30011;&#20559;&#22909;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26657;&#20934;&#27169;&#22411;&#19982;&#20559;&#22909;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#26368;&#32456;&#21033;&#29992;&#26399;&#26395;&#22870;&#21169;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24191;&#27867;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22870;&#21169;&#24314;&#27169;&#20027;&#35201;&#20381;&#36182;&#20110;&#19968;&#32452;&#20010;&#20307;&#25552;&#20379;&#30340;&#20154;&#31867;&#26631;&#27880;&#12290;&#36825;&#31181;&#20381;&#36182;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#20542;&#21521;&#20110;&#21453;&#26144;&#36825;&#20123;&#26631;&#27880;&#32773;&#30340;&#20542;&#21521;&#65292;&#20174;&#32780;&#26410;&#33021;&#20805;&#20998;&#20195;&#34920;&#26356;&#24191;&#27867;&#20154;&#32676;&#30340;&#26399;&#26395;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#8212;&#8212;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;(DPRM)&#65292;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#22612;&#20998;&#24067;&#26469;&#21051;&#30011;&#20559;&#22909;&#65292;&#35813;&#20998;&#24067;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20559;&#22909;&#36235;&#21183;&#30340;&#27874;&#21160;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#26657;&#20934;DPRM&#19982;&#20559;&#22909;&#20998;&#24067;&#30340;&#23545;&#40784;&#24230;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#26399;&#26395;&#22870;&#21169;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09764v1 Announce Type: new  Abstract: Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference. However, the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately. In this paper, we introduce the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with a diverse set of human preferences. To this end, we characterize the preferences by a beta distribution, which can dynamically adapt to fluctuations in preference trends. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to fine-tune an LLM polic
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;RAG&#65289;&#30340;&#26080;&#22359;&#35821;&#22659;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32469;&#36807;&#25991;&#26412;&#20999;&#20998;&#30340;&#36807;&#31243;&#65292;&#21033;&#29992;&#32534;&#30721;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#20934;&#30830;&#22320;&#35821;&#22659;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#35821;&#20041;&#36830;&#36143;&#24615;&#30772;&#22351;&#21644;&#35777;&#25454;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09760</link><description>&lt;p&gt;
&#26080;&#22359;&#35821;&#22659;&#26816;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411; grounding
&lt;/p&gt;
&lt;p&gt;
Grounding Language Model with Chunking-Free In-Context Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09760
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;RAG&#65289;&#30340;&#26080;&#22359;&#35821;&#22659;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32469;&#36807;&#25991;&#26412;&#20999;&#20998;&#30340;&#36807;&#31243;&#65292;&#21033;&#29992;&#32534;&#30721;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#20934;&#30830;&#22320;&#35821;&#22659;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#35821;&#20041;&#36830;&#36143;&#24615;&#30772;&#22351;&#21644;&#35777;&#25454;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#26080;&#22359;&#35821;&#22659;&#65288;CFIC&#65289;&#26816;&#32034;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;RAG&#31995;&#32479;&#22312;&#20351;&#29992;&#31934;&#30830;&#35777;&#25454;&#25991;&#26412;&#36827;&#34892; grounding &#26102;&#24448;&#24448;&#38754;&#20020;&#22788;&#29702;&#20887;&#38271;&#25991;&#26723;&#21644;&#36807;&#28388;&#26080;&#20851;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#24120;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#25991;&#26723;&#20999;&#20998;&#21644;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#20197;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#37117;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#30772;&#22351;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#35201;&#20040;&#26410;&#33021;&#26377;&#25928;&#35299;&#20915;&#35777;&#25454;&#26816;&#32034;&#20013;&#30340;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;CFIC&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#20999;&#20998;&#36807;&#31243;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#21033;&#29992;&#25991;&#26723;&#30340;&#32534;&#30721;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#35821;&#22659;&#26816;&#32034;&#65292;&#22312;&#23545;&#29992;&#25143;&#26597;&#35810;&#36827;&#34892;&#33258;&#22238;&#24402;&#35299;&#30721;&#26102;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#25152;&#38656;&#30340;&#20855;&#20307;&#35777;&#25454;&#25991;&#26412;&#65292;&#28040;&#38500;&#20102;&#20999;&#20998;&#30340;&#38656;&#27714;&#12290;CFIC &#36827;&#19968;&#27493;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09760v1 Announce Type: cross  Abstract: This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval.   CFIC addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#39640;&#25928;&#30340;&#35821;&#35328;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#22522;&#30784;&#33521;&#25991;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#29983;&#25104;&#27874;&#20848;&#25991;&#65292;&#24182;&#22312;&#22256;&#24785;&#24230;&#21644;&#20219;&#21153;&#34920;&#29616;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21521;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26032;&#35821;&#35328;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.09759</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#35821;&#35328;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65306;&#25193;&#23637;&#26368;&#26032;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#27874;&#20848;&#35821;
&lt;/p&gt;
&lt;p&gt;
Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#39640;&#25928;&#30340;&#35821;&#35328;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#22522;&#30784;&#33521;&#25991;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#29983;&#25104;&#27874;&#20848;&#25991;&#65292;&#24182;&#22312;&#22256;&#24785;&#24230;&#21644;&#20219;&#21153;&#34920;&#29616;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21521;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26032;&#35821;&#35328;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#22522;&#30784;&#33521;&#25991;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#20026;&#29983;&#25104;&#27874;&#20848;&#25991;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;3.11 GB&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35821;&#35328;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;LAPT&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;2.76&#20159;&#20010;&#27874;&#20848;&#35821;tokens&#12290;LAPT&#21518;&#36827;&#34892;&#20102;&#39069;&#22806;&#30340;&#24494;&#35843;&#65292;&#26088;&#22312;&#35299;&#20915;&#20061;&#20010;KLEJ&#25361;&#25112;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;Curie-7B-v1&#19981;&#20165;&#22312;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27874;&#20848;&#27169;&#22411;&#20013;&#20855;&#26377;&#26368;&#20302;&#30340;&#22256;&#24785;&#24230;3.02&#65292;&#32780;&#19988;&#22312;8&#20010;&#20219;&#21153;&#20013;&#19982;&#26368;&#22909;&#30340;&#27874;&#20848;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#19981;&#21040;2%&#12290;Curie-7B-v1&#20165;&#20351;&#29992;&#20856;&#22411;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#32422;2-3%&#26469;&#23398;&#20064;&#27874;&#20848;&#35821;&#12290;LAPT&#22312;&#19981;&#21040;&#20116;&#22825;&#30340;&#26102;&#38388;&#20869;&#20351;&#29992;&#26222;&#36890;GPU&#23436;&#25104;&#65292;&#20984;&#26174;&#20102;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#12290;&#27169;&#22411;&#22312;&#27874;&#20848;&#35821;&#26041;&#38754;&#30340;&#29087;&#32451;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23558;&#26032;&#35821;&#35328;&#28155;&#21152;&#21040;&#29616;&#26377;LLMs&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09759v1 Announce Type: cross  Abstract: This study explores the potential of fine-tuning foundational English Large Language Models (LLMs) for generating Polish text. The first step involves Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB, consisting of 276 million Polish tokens. The LAPT is followed by additional fine-tuning aimed at solving nine KLEJ challenges. Our trained model Curie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02 among decoder-based Polish models but also closely rivals the performance of the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9 tasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn Polish. The LAPT was completed in less than five days using a consumer GPU, highlighting the method's efficiency. The proficiency of the model in Polish was significantly enhanced, demonstrating the viability of this approach for adding new languages to existing LLMs
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33402;&#26415;&#23478;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#21019;&#24847;&#32534;&#31243;&#20013;&#30340;&#33402;&#26415;&#28508;&#21147;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#21512;&#20316;&#26041;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#21453;&#24605;&#31867;&#22411;&#19982;&#29992;&#25143;&#34920;&#29616;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20027;&#35266;&#20307;&#39564;&#30456;&#20851;&#12290;&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21644;&#23450;&#24615;&#35775;&#35848;&#65292;&#25105;&#20204;&#20174;&#33402;&#26415;&#23478;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#25209;&#21028;&#24615;&#35270;&#35282;&#21644;&#35774;&#35745;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.09750</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33402;&#26415;&#21019;&#20316;&#20013;&#30340;&#28508;&#21147;&#65306;&#33402;&#26415;&#23478;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#20013;&#30340;&#21019;&#24847;&#32534;&#31243;&#21644;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09750
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33402;&#26415;&#23478;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#21019;&#24847;&#32534;&#31243;&#20013;&#30340;&#33402;&#26415;&#28508;&#21147;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#21512;&#20316;&#26041;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#21453;&#24605;&#31867;&#22411;&#19982;&#29992;&#25143;&#34920;&#29616;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20027;&#35266;&#20307;&#39564;&#30456;&#20851;&#12290;&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21644;&#23450;&#24615;&#35775;&#35848;&#65292;&#25105;&#20204;&#20174;&#33402;&#26415;&#23478;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#25209;&#21028;&#24615;&#35270;&#35282;&#21644;&#35774;&#35745;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36741;&#21161;&#32534;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#25506;&#32034;LLMs&#22312;&#33402;&#26415;&#23478;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#21019;&#36896;&#24615;&#32534;&#31243;&#20013;&#30340;&#33402;&#26415;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#22312;&#36825;&#31181;&#21512;&#20316;&#36807;&#31243;&#20013;&#33402;&#26415;&#23478;&#30340;&#21453;&#24605;&#31867;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#21512;&#20316;&#26041;&#24335;&#65306;&#35843;&#29992;&#25972;&#20010;&#31243;&#24207;&#21644;&#22810;&#20010;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#33402;&#26415;&#23478;&#22312;&#20004;&#31181;&#19981;&#21516;&#26041;&#27861;&#20013;&#19981;&#21516;&#30340;&#21050;&#28608;&#24615;&#21453;&#24605;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#26174;&#31034;&#20102;&#21453;&#24605;&#31867;&#22411;&#19982;&#29992;&#25143;&#34920;&#29616;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20027;&#35266;&#20307;&#39564;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#36827;&#34892;&#20004;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#23454;&#39564;&#25968;&#25454;&#21644;&#23450;&#24615;&#35775;&#35848;&#12290;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;LLM&#22312;&#21019;&#24847;&#32534;&#31243;&#20013;&#30340;&#33402;&#26415;&#28508;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20174;&#33402;&#26415;&#23478;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#25209;&#21028;&#24615;&#35270;&#35282;&#65292;&#24182;&#38416;&#36848;&#20102;&#35774;&#35745;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09750v1 Announce Type: cross  Abstract: Recently, the potential of large language models (LLMs) has been widely used in assisting programming. However, current research does not explore the artist potential of LLMs in creative coding within artist and AI collaboration. Our work probes the reflection type of artists in the creation process with such collaboration. We compare two common collaboration approaches: invoking the entire program and multiple subtasks. Our findings exhibit artists' different stimulated reflections in two different methods. Our finding also shows the correlation of reflection type with user performance, user satisfaction, and subjective experience in two collaborations through conducting two methods, including experimental data and qualitative interviews. In this sense, our work reveals the artistic potential of LLM in creative coding. Meanwhile, we provide a critical lens of human-AI collaboration from the artists' perspective and expound design sugg
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#33976;&#39311;&#12289;&#32039;&#20945;&#26550;&#26500;&#35774;&#35745;&#21644;&#21160;&#24577;&#32593;&#32476;&#31561;&#26041;&#38754;&#12290;&#22823;&#27169;&#22411;&#30340;&#31361;&#20986;&#29305;&#28857;&#26159;&#21387;&#32553;&#21518;&#38656;&#35201;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#20851;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.09748</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Model Compression and Efficient Inference for Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09748
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#33976;&#39311;&#12289;&#32039;&#20945;&#26550;&#26500;&#35774;&#35745;&#21644;&#21160;&#24577;&#32593;&#32476;&#31561;&#26041;&#38754;&#12290;&#22823;&#27169;&#22411;&#30340;&#31361;&#20986;&#29305;&#28857;&#26159;&#21387;&#32553;&#21518;&#38656;&#35201;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#20851;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25152;&#20135;&#29983;&#30340;&#26174;&#33879;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#22823;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20174;&#31639;&#27861;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#26041;&#27861;&#12290;&#22312;&#20998;&#31867;&#26041;&#38754;&#65292;&#19982;&#36739;&#23567;&#30340;&#27169;&#22411;&#31867;&#20284;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#21152;&#36895;&#31639;&#27861;&#20173;&#21487;&#20197;&#20998;&#20026;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#33976;&#39311;&#12289;&#32039;&#20945;&#26550;&#26500;&#35774;&#35745;&#21644;&#21160;&#24577;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#19982;&#36739;&#23567;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26377;&#20004;&#20010;&#31361;&#20986;&#30340;&#29305;&#28857;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#21387;&#32553;&#31639;&#27861;&#22312;&#21387;&#32553;&#21518;&#38656;&#35201;&#24494;&#35843;&#29978;&#33267;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#22823;&#27169;&#22411;&#26368;&#26174;&#33879;&#30340;&#26041;&#38754;&#26159;&#19982;&#27169;&#22411;&#24494;&#35843;&#25110;&#35757;&#32451;&#30456;&#20851;&#30340;&#38750;&#24120;&#39640;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#38024;&#23545;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#31639;&#27861;&#37117;&#38656;&#35201;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09748v1 Announce Type: cross  Abstract: Transformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#24847;&#35782;&#20195;&#29702;&#20154;&#65292;&#20182;&#20204;&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;&#20182;&#20204;&#30340;&#26377;&#25928;&#25928;&#29992;&#20989;&#25968;&#26159;&#24050;&#30693;&#21644;&#38544;&#34255;&#23376;&#20989;&#25968;&#30340;&#32858;&#21512;&#65292;&#36890;&#36807;&#38480;&#21046;&#26550;&#26500;&#65292;&#23454;&#29616;&#23545;&#38544;&#34255;&#23376;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.09734</link><description>&lt;p&gt;
&#20195;&#29702;&#20154;&#19981;&#38656;&#35201;&#30693;&#36947;&#20182;&#20204;&#30340;&#30446;&#30340;
&lt;/p&gt;
&lt;p&gt;
Agents Need Not Know Their Purpose
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#24847;&#35782;&#20195;&#29702;&#20154;&#65292;&#20182;&#20204;&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;&#20182;&#20204;&#30340;&#26377;&#25928;&#25928;&#29992;&#20989;&#25968;&#26159;&#24050;&#30693;&#21644;&#38544;&#34255;&#23376;&#20989;&#25968;&#30340;&#32858;&#21512;&#65292;&#36890;&#36807;&#38480;&#21046;&#26550;&#26500;&#65292;&#23454;&#29616;&#23545;&#38544;&#34255;&#23376;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#34987;&#31216;&#20026;&#23545;&#40784;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29702;&#24615;&#30340;&#20195;&#29702;&#20154;&#20250;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#24335;&#34892;&#20107;&#65292;&#36825;&#23548;&#33268;&#20182;&#20204;&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#24403;&#20182;&#20204;&#30340;&#26234;&#33021;&#27700;&#24179;&#25552;&#39640;&#26102;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#27809;&#26377;&#8220;&#19968;&#20010;&#30495;&#27491;&#30340;&#25928;&#29992;&#20989;&#25968;&#8221;&#65292;&#35299;&#20915;&#26041;&#26696;&#24517;&#39035;&#21253;&#21547;&#26356;&#20840;&#38754;&#30340;&#23545;&#40784;&#26041;&#27861;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#26080;&#24847;&#35782;&#20195;&#29702;&#20154;&#65306;&#20195;&#29702;&#20154;&#30340;&#32467;&#26500;&#20351;&#20182;&#20204;&#30340;&#26377;&#25928;&#25928;&#29992;&#20989;&#25968;&#26159;&#24050;&#30693;&#21644;&#38544;&#34255;&#23376;&#20989;&#25968;&#30340;&#32858;&#21512;&#12290;&#38544;&#34255;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#38656;&#35201;&#26368;&#22823;&#21270;&#30340;&#65292;&#23427;&#34987;&#20869;&#37096;&#23454;&#29616;&#20026;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#38459;&#27490;&#20195;&#29702;&#20154;&#26816;&#26597;&#23427;&#12290;&#24050;&#30693;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#38656;&#35201;&#26368;&#23567;&#21270;&#30340;&#65292;&#21363;&#23545;&#38544;&#34255;&#23376;&#20989;&#25968;&#30340;&#30693;&#35782;&#12290;&#26550;&#26500;&#38480;&#21046;&#36827;&#19968;&#27493;&#24433;&#21709;&#20195;&#29702;&#20154;&#34892;&#21160;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09734v1 Announce Type: new  Abstract: Ensuring artificial intelligence behaves in such a way that is aligned with human values is commonly referred to as the alignment challenge. Prior work has shown that rational agents, behaving in such a way that maximizes a utility function, will inevitably behave in such a way that is not aligned with human values, especially as their level of intelligence goes up. Prior work has also shown that there is no "one true utility function"; solutions must include a more holistic approach to alignment. This paper describes oblivious agents: agents that are architected in such a way that their effective utility function is an aggregation of a known and hidden sub-functions. The hidden component, to be maximized, is internally implemented as a black box, preventing the agent from examining it. The known component, to be minimized, is knowledge of the hidden sub-function. Architectural constraints further influence how agent actions can evolve i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#20026;&#24322;&#26500;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#21270;VR&#26381;&#21153;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#22522;&#20110;prompt&#30340;&#24207;&#21015;&#24314;&#27169;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;FedPromptDT&#65292;&#22312;&#20445;&#35777;&#29992;&#25143;&#20307;&#39564;&#26368;&#39640;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#36275;&#21644;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09729</link><description>&lt;p&gt;
&#22522;&#20110;Prompt&#30340;&#32852;&#37030;&#20915;&#31574;Transformer&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#23450;&#21046;&#21270;VR&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#20026;&#24322;&#26500;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#21270;VR&#26381;&#21153;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#22522;&#20110;prompt&#30340;&#24207;&#21015;&#24314;&#27169;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;FedPromptDT&#65292;&#22312;&#20445;&#35777;&#29992;&#25143;&#20307;&#39564;&#26368;&#39640;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#36275;&#21644;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#20026;&#24322;&#26500;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#26381;&#21153;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#20307;&#39564;&#36136;&#37327;&#65288;QoE&#65289;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#29992;&#25143;&#20307;&#39564;&#65292;&#32771;&#34385;&#20102;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#30340;&#24310;&#36831;&#12289;&#29992;&#25143;&#27880;&#24847;&#21147;&#27700;&#24179;&#21644;&#39318;&#36873;&#20998;&#36776;&#29575;&#12290;&#28982;&#21518;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;QoE&#26368;&#22823;&#21270;&#38382;&#39064;&#20197;&#20445;&#35777;&#26368;&#39640;&#21487;&#33021;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26469;&#35299;&#20915;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#36866;&#29992;&#20110;&#25152;&#26377;MEC&#26381;&#21153;&#22120;&#30340;&#21508;&#31181;&#29992;&#25143;&#29615;&#22659;&#30340;&#36890;&#29992;&#31574;&#30053;&#12290;&#20026;&#20102;&#23398;&#20064;&#36825;&#20010;&#36890;&#29992;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22522;&#20110;prompt&#30340;&#24207;&#21015;&#24314;&#27169;&#26469;&#39044;&#35757;&#32451;MEC&#26381;&#21153;&#22120;&#20043;&#38388;&#20849;&#21516;&#20915;&#31574;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21363;FedPromptDT&#12290;&#20351;&#29992;FL&#35299;&#20915;&#20102;&#26412;&#22320;MEC&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#31163;&#32447;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09729v1 Announce Type: new  Abstract: This paper investigates resource allocation to provide heterogeneous users with customized virtual reality (VR) services in a mobile edge computing (MEC) system. We first introduce a quality of experience (QoE) metric to measure user experience, which considers the MEC system's latency, user attention levels, and preferred resolutions. Then, a QoE maximization problem is formulated for resource allocation to ensure the highest possible user experience,which is cast as a reinforcement learning problem, aiming to learn a generalized policy applicable across diverse user environments for all MEC servers. To learn the generalized policy, we propose a framework that employs federated learning (FL) and prompt-based sequence modeling to pre-train a common decision model across MEC servers, which is named FedPromptDT. Using FL solves the problem of insufficient local MEC data while protecting user privacy during offline training. The design of p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#28389;&#29992;&#29983;&#25104;&#22411;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#21019;&#24314;&#30701;&#20449;&#38035;&#40060;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2402.09728</link><description>&lt;p&gt;
&#28389;&#29992;&#29983;&#25104;&#22411;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#21019;&#24314;&#30701;&#20449;&#38035;&#40060;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09728
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#28389;&#29992;&#29983;&#25104;&#22411;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#21019;&#24314;&#30701;&#20449;&#38035;&#40060;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SMS&#38035;&#40060;&#65292;&#20063;&#34987;&#31216;&#20026;&#8220;smishing&#8221;&#65292;&#26159;&#19968;&#31181;&#36890;&#36807;&#27450;&#35784;&#24615;&#31227;&#21160;&#30701;&#20449;&#35825;&#39575;&#29992;&#25143;&#36879;&#38706;&#31169;&#20154;&#20449;&#24687;&#25110;&#28857;&#20987;&#24102;&#26377;&#24694;&#24847;&#20869;&#23481;&#30340;URL&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#23041;&#32961;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#20102;&#20250;&#35805;&#29983;&#25104;&#22411;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#26381;&#21153;&#65288;&#22914;OpenAI&#30340;ChatGPT&#65292;Google&#30340;BARD&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#30001;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#12290;&#36825;&#20123;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#30830;&#23454;&#20855;&#26377;&#24456;&#22810;&#29992;&#36884;&#65292;&#20294;&#23578;&#26410;&#31995;&#32479;&#22320;&#20102;&#35299;&#23427;&#20204;&#22914;&#20309;&#22312;&#21019;&#24314;&#23041;&#32961;&#21644;&#25915;&#20987;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28389;&#29992;GPT&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#29983;&#25104;&#22411;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#26381;&#21153;&#22914;&#20309;&#34987;&#25915;&#20987;&#32773;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#21033;&#29992;&#65292;&#21019;&#24314;&#30701;&#20449;&#38035;&#40060;&#25991;&#26412;&#65292;&#26368;&#32456;&#23548;&#33268;&#26356;&#32874;&#26126;&#30340;&#30701;&#20449;&#38035;&#40060;&#25915;&#20987;&#27963;&#21160;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26080;&#29616;&#26377;&#30340;&#24037;&#20316;&#26126;&#30830;&#26174;&#31034;&#36825;&#20123;&#29983;&#25104;&#22411;&#25991;&#26412;&#27169;&#22411;&#22312;&#21019;&#24314;SMS&#38035;&#40060;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09728v1 Announce Type: cross  Abstract: SMS phishing, also known as "smishing", is a growing threat that tricks users into disclosing private information or clicking into URLs with malicious content through fraudulent mobile text messages. In recent past, we have also observed a rapid advancement of conversational generative AI chatbot services (e.g., OpenAI's ChatGPT, Google's BARD), which are powered by pre-trained large language models (LLMs). These AI chatbots certainly have a lot of utilities but it is not systematically understood how they can play a role in creating threats and attacks. In this paper, we propose AbuseGPT method to show how the existing generative AI-based chatbot services can be exploited by attackers in real world to create smishing texts and eventually lead to craftier smishing campaigns. To the best of our knowledge, there is no pre-existing work that evidently shows the impacts of these generative text-based models on creating SMS phishing. Thus, 
&lt;/p&gt;</description></item><item><title>ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.09727</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#20154;&#24037;&#26234;&#33021;&#38405;&#35835;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09727
&lt;/p&gt;
&lt;p&gt;
ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#38480;&#21046;&#22312;&#26576;&#20010;&#26368;&#22823;&#19978;&#19979;&#25991;&#38271;&#24230;&#20869;&#65292;&#32780;&#19988;&#26080;&#27861;&#31283;&#23450;&#22320;&#22788;&#29702;&#38271;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReadAgent&#65292;&#19968;&#20010;&#22686;&#21152;&#20102;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#31995;&#32479;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21487;&#20197;&#36798;&#21040;20&#20493;&#12290;&#21463;&#21040;&#20154;&#31867;&#20132;&#20114;&#24335;&#38405;&#35835;&#38271;&#25991;&#26723;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;ReadAgent&#23454;&#29616;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#21033;&#29992;LLM&#30340;&#39640;&#32423;&#35821;&#35328;&#33021;&#21147;&#26469;&#65306;&#65288;1&#65289;&#20915;&#23450;&#23558;&#21738;&#20123;&#20869;&#23481;&#23384;&#20648;&#22312;&#19968;&#20010;&#35760;&#24518;&#29255;&#27573;&#20013;&#65292;&#65288;2&#65289;&#23558;&#36825;&#20123;&#35760;&#24518;&#29255;&#27573;&#21387;&#32553;&#25104;&#20026;&#31216;&#20026;&#27010;&#35201;&#35760;&#24518;&#30340;&#30701;&#26102;&#35760;&#24518;&#65292;&#65288;3&#65289;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#21407;&#22987;&#25991;&#26412;&#26597;&#25214;&#27573;&#33853;&#26469;&#25552;&#37266;&#33258;&#24049;&#30456;&#20851;&#32454;&#33410;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#12289;&#20351;&#29992;&#21407;&#22987;&#38271;&#19978;&#19979;&#25991;&#20197;&#21450;&#20351;&#29992;&#27010;&#35201;&#35760;&#24518;&#26469;&#35780;&#20272;ReadAgent&#19982;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#35780;&#20272;&#26159;&#22312;&#19977;&#20010;&#38271;&#25991;&#26723;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09727v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension task
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#38169;&#35823;&#26292;&#38706;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#36827;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;BLEU&#24471;&#20998;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.09725</link><description>&lt;p&gt;
&#36890;&#36807;&#38169;&#35823;&#26292;&#38706;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#25913;&#36827;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#38169;&#35823;&#26292;&#38706;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#36827;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;BLEU&#24471;&#20998;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;IR-NAT&#65288;&#22522;&#20110;&#36845;&#20195;&#25913;&#36827;&#30340;NAT&#65289;&#26694;&#26550;&#20043;&#19968;&#65292;&#26465;&#20214;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;CMLM&#65289;&#37319;&#29992;&#25513;&#30721;&#39044;&#27979;&#33539;&#24335;&#26469;&#37325;&#26032;&#39044;&#27979;&#25513;&#30721;&#20302;&#32622;&#20449;&#24230;&#30340;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;CMLM&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#30340;&#26631;&#35760;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#29983;&#25104;&#26041;&#24335;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38169;&#35823;&#26292;&#38706;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65288;EECR&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#26500;&#24314;&#28151;&#21512;&#24207;&#21015;&#65292;&#24182;&#25552;&#20986;&#22312;&#19981;&#23436;&#32654;&#35266;&#27979;&#26465;&#20214;&#19979;&#38024;&#23545;&#25513;&#30721;&#26631;&#35760;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#38480;&#21046;&#19981;&#21516;&#35266;&#27979;&#24773;&#20917;&#19979;&#25513;&#30721;&#26631;&#35760;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#32553;&#23567;&#35757;&#32451;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#20116;&#20010;&#32763;&#35793;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24179;&#22343;&#25913;&#36827;&#20102;0.68&#21644;0.40&#30340;BLEU&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09725v1 Announce Type: cross  Abstract: Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens. However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases. In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR). We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions. We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference. The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09723</link><description>&lt;p&gt;
&#26377;&#38480;&#39044;&#31639;&#19979;&#30340;&#36805;&#36895;&#23398;&#20064;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification for Prompt Learning under a Limited Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09723
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23398;&#20064;&#21512;&#36866;&#25552;&#31034;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#25104;&#26412;&#65288;&#20363;&#22914;&#35775;&#38382;LLM&#21644;&#35780;&#20272;&#21709;&#24212;&#65289;&#23578;&#26410;&#24471;&#21040;&#32771;&#34385;&#12290;&#20026;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#24037;&#20316;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#26126;&#30830;&#24341;&#20837;&#20102;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#22312;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI-FB&#65289;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65288;&#29992;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65289;&#65292;&#20197;&#31995;&#32479;&#22320;&#21033;&#29992;BAI-FB&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#30340;&#21147;&#37327;&#12290;&#25552;&#31034;&#23398;&#20064;&#30340;&#29420;&#29305;&#29305;&#28857;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#25552;&#20986;&#20102;TRIPLE&#30340;&#20004;&#20010;&#22522;&#20110;&#23884;&#20837;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
&lt;/p&gt;</description></item><item><title>Reg-NF&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;&#22330;&#38544;&#24335;&#34920;&#38754;&#27880;&#20876;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#20004;&#20010;&#31070;&#32463;&#22330;&#20043;&#38388;&#30340;&#30456;&#23545;6&#33258;&#30001;&#24230;&#21464;&#25442;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09722</link><description>&lt;p&gt;
Reg-NF: &#39640;&#25928;&#30340;&#31070;&#32463;&#22330;&#38544;&#24335;&#34920;&#38754;&#27880;&#20876;
&lt;/p&gt;
&lt;p&gt;
Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09722
&lt;/p&gt;
&lt;p&gt;
Reg-NF&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;&#22330;&#38544;&#24335;&#34920;&#38754;&#27880;&#20876;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#20004;&#20010;&#31070;&#32463;&#22330;&#20043;&#38388;&#30340;&#30456;&#23545;6&#33258;&#30001;&#24230;&#21464;&#25442;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22330;&#26159;&#19968;&#31181;&#22522;&#20110;&#22352;&#26631;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26368;&#36817;&#22312;&#34920;&#31034;&#22330;&#26223;&#26041;&#38754;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#19982;&#22522;&#20110;&#26174;&#24335;&#34920;&#31034;&#65288;&#22914;&#28857;&#20113;&#65289;&#30340;&#32463;&#20856;&#26041;&#27861;&#19981;&#21516;&#65292;&#31070;&#32463;&#22330;&#25552;&#20379;&#20102;&#19968;&#31181;&#36830;&#32493;&#30340;&#22330;&#26223;&#34920;&#31034;&#65292;&#33021;&#22815;&#32039;&#20945;&#22320;&#34920;&#31034;3D&#20960;&#20309;&#21644;&#22806;&#35266;&#65292;&#38750;&#24120;&#36866;&#21512;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#20808;&#21069;&#26041;&#27861;&#27809;&#26377;&#30452;&#25509;&#21033;&#29992;&#36825;&#20123;&#36830;&#32493;&#30340;&#38544;&#24335;&#34920;&#31034;&#26469;&#27880;&#20876;&#22810;&#20010;&#31070;&#32463;&#22330;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Reg-NF&#65292;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#22330;&#30340;&#27880;&#20876;&#26041;&#27861;&#65292;&#20248;&#21270;&#20004;&#20010;&#20219;&#24847;&#31070;&#32463;&#22330;&#20043;&#38388;&#30340;&#30456;&#23545;6&#33258;&#30001;&#24230;&#21464;&#25442;&#65292;&#21363;&#20351;&#36825;&#20004;&#20010;&#22330;&#20855;&#26377;&#19981;&#21516;&#30340;&#23610;&#24230;&#22240;&#23376;&#12290;Reg-NF&#30340;&#20851;&#38190;&#32452;&#20214;&#21253;&#25324;&#21452;&#21521;&#27880;&#20876;&#25439;&#22833;&#12289;&#22810;&#35270;&#35282;&#34920;&#38754;&#37319;&#26679;&#21644;&#20307;&#31215;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#30340;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#22330;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09722v1 Announce Type: cross  Abstract: Neural fields, coordinate-based neural networks, have recently gained popularity for implicitly representing a scene. In contrast to classical methods that are based on explicit representations such as point clouds, neural fields provide a continuous scene representation able to represent 3D geometry and appearance in a way which is compact and ideal for robotics applications. However, limited prior methods have investigated registering multiple neural fields by directly utilising these continuous implicit representations. In this paper, we present Reg-NF, a neural fields-based registration that optimises for the relative 6-DoF transformation between two arbitrary neural fields, even if those two fields have different scale factors. Key components of Reg-NF include a bidirectional registration loss, multi-view surface sampling, and utilisation of volumetric signed distance functions (SDFs). We showcase our approach on a new neural fiel
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09721</link><description>&lt;p&gt;
&#35828;&#26381;&#19968;&#20301;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Persuading a Learning Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09721
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20219;&#20309;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65289;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#20195;&#29702;&#20154;&#20351;&#29992;&#31639;&#27861;&#26469;&#23398;&#20064;&#22914;&#20309;&#23545;&#22996;&#25176;&#20154;&#30340;&#20449;&#21495;&#20570;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#19968;&#27425;&#24615;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#20195;&#29702;&#20154;&#36817;&#20284;&#22320;&#26368;&#20339;&#21709;&#24212;&#12290;&#36890;&#36807;&#36825;&#20010;&#31616;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#65306;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#21487;&#20197;&#20445;&#35777;&#20854;&#25928;&#29992;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#20043;&#38388;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#65307;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#22996;&#25176;&#20154;&#22312;&#23398;&#20064;&#27169;&#22411;&#19982;&#38750;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#25928;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#26377;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35266;&#28857;&#21644;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#20419;&#36827;&#35299;&#32544;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.09712</link><description>&lt;p&gt;
&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35266;&#28857;&#21644;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#20419;&#36827;&#35299;&#32544;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#33268;&#21147;&#20110;&#25552;&#21462;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20869;&#22312;&#22240;&#32032;&#12290;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#22240;&#24335;&#20998;&#35299;&#36825;&#20123;&#34920;&#31034;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#25110;&#29305;&#23450;&#32467;&#26500;&#35774;&#35745;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#21644;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#20419;&#36827;&#35299;&#32544;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#22270;&#20687;&#32534;&#30721;&#20026;&#19968;&#32452;&#27010;&#24565;&#20196;&#29260;&#65292;&#24182;&#23558;&#23427;&#20204;&#35270;&#20026;&#22270;&#20687;&#37325;&#26500;&#30340;&#28508;&#22312;&#25193;&#25955;&#30340;&#26465;&#20214;&#65292;&#20854;&#20013;&#36890;&#36807;&#27010;&#24565;&#20196;&#29260;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#36830;&#25509;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26694;&#26550;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#23601;&#33021;&#36798;&#21040;&#26356;&#20248;&#31168;&#30340;&#35299;&#32544;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#25152;&#26377;&#20808;&#21069;&#35774;&#35745;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09712v1 Announce Type: cross  Abstract: Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive abl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31574;&#30053;&#23545;&#27604;&#25915;&#20987;&#8221;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#36890;&#36807;&#20351;&#20302;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#39640;&#24615;&#33021;&#30340;&#65292;&#21516;&#26102;&#20351;&#39640;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#20302;&#24615;&#33021;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#25915;&#20987;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09695</link><description>&lt;p&gt;
&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reward Poisoning Attack Against Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09695
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31574;&#30053;&#23545;&#27604;&#25915;&#20987;&#8221;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#36890;&#36807;&#20351;&#20302;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#39640;&#24615;&#33021;&#30340;&#65292;&#21516;&#26102;&#20351;&#39640;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#20302;&#24615;&#33021;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#25915;&#20987;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#40657;&#30418;&#23041;&#32961;&#27169;&#22411;&#65292;&#25915;&#20987;&#32773;&#23545;&#23398;&#20064;&#31639;&#27861;&#23436;&#20840;&#19981;&#20102;&#35299;&#65292;&#24182;&#19988;&#20854;&#39044;&#31639;&#21463;&#21040;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#27745;&#26579;&#37327;&#20197;&#21450;&#24635;&#25200;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31574;&#30053;&#23545;&#27604;&#25915;&#20987;&#8221;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#20854;&#39640;&#23618;&#24605;&#24819;&#26159;&#20351;&#19968;&#20123;&#20302;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#39640;&#24615;&#33021;&#30340;&#65292;&#21516;&#26102;&#20351;&#39640;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#20302;&#24615;&#33021;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#30340;&#40657;&#30418;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25915;&#20987;&#35774;&#35745;&#30340;&#29702;&#35770;&#27934;&#23519;&#65292;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09695v1 Announce Type: cross  Abstract: We study the problem of reward poisoning attacks against general offline reinforcement learning with deep neural networks for function approximation. We consider a black-box threat model where the attacker is completely oblivious to the learning algorithm and its budget is limited by constraining both the amount of corruption at each data point, and the total perturbation. We propose an attack strategy called `policy contrast attack'. The high-level idea is to make some low-performing policies appear as high-performing while making high-performing policies appear as low-performing. To the best of our knowledge, we propose the first black-box reward poisoning attack in the general offline RL setting. We provide theoretical insights on the attack design and empirically show that our attack is efficient against current state-of-the-art offline RL algorithms in different kinds of learning datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#31215;&#26497;&#25705;&#25830;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;&#21363;&#36890;&#36807;&#26377;&#24847;&#30340;&#24310;&#36831;&#31561;&#26041;&#24335;&#65292;&#22686;&#21152;&#29992;&#25143;&#21453;&#24605;&#65292;&#38459;&#27490;&#33258;&#21160;&#25110;&#26377;&#20559;&#35265;&#34892;&#20026;&#65292;&#24182;&#25552;&#39640;&#24847;&#22806;&#21457;&#29616;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.09683</link><description>&lt;p&gt;
&#25506;&#32034;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#8220;&#31215;&#26497;&#25705;&#25830;&#8221;&#34892;&#20026;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Exploring a Behavioral Model of "Positive Friction" in Human-AI Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#31215;&#26497;&#25705;&#25830;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;&#21363;&#36890;&#36807;&#26377;&#24847;&#30340;&#24310;&#36831;&#31561;&#26041;&#24335;&#65292;&#22686;&#21152;&#29992;&#25143;&#21453;&#24605;&#65292;&#38459;&#27490;&#33258;&#21160;&#25110;&#26377;&#20559;&#35265;&#34892;&#20026;&#65292;&#24182;&#25552;&#39640;&#24847;&#22806;&#21457;&#29616;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26080;&#32541;&#12289;&#26080;&#25705;&#25830;&#30340;&#29992;&#25143;&#20307;&#39564;&#19968;&#30452;&#26159;&#24212;&#29992;&#34892;&#20026;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20027;&#35201;&#36235;&#21183;&#65292;&#36890;&#36807;&#20943;&#23569;&#29992;&#25143;&#20307;&#39564;&#20013;&#30340;&#25705;&#25830;&#21147;&#26469;&#23454;&#29616;&#24076;&#26395;&#30340;&#34892;&#20026;&#23481;&#26131;&#21644;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25705;&#25830;&#21147;&#30830;&#23454;&#26377;&#30410;&#65292;&#22914;&#22312;&#25554;&#20837;&#26377;&#24847;&#30340;&#24310;&#36831;&#20197;&#22686;&#21152;&#21453;&#24605;&#12289;&#38450;&#27490;&#20010;&#20307;&#33258;&#21160;&#25110;&#26377;&#20559;&#35265;&#34892;&#20026;&#12289;&#22686;&#24378;&#24847;&#22806;&#21457;&#29616;&#26426;&#20250;&#31561;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#21450;&#21644;&#21487;&#29992;&#24615;&#30340;&#22686;&#21152;&#21482;&#22686;&#21152;&#20102;&#30740;&#31350;&#25705;&#25830;&#21147;&#22914;&#20309;&#24110;&#21161;&#25110;&#22952;&#30861;&#20154;&#24037;&#26234;&#33021;&#29992;&#25143;&#30340;&#38656;&#35201;&#65307;&#36825;&#36824;&#24847;&#21619;&#30528;&#38656;&#35201;&#32771;&#34385;&#31215;&#26497;&#25705;&#25830;&#21147;&#22914;&#20309;&#20351;&#20154;&#24037;&#26234;&#33021;&#20174;&#19994;&#32773;&#21463;&#30410;&#65292;&#26080;&#35770;&#26159;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#65288;&#20363;&#22914;&#19982;&#22810;&#26679;&#21270;&#30340;&#22242;&#38431;&#21512;&#20316;&#65289;&#36824;&#26159;&#22312;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#35774;&#35745;&#20013;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#31215;&#26497;&#25705;&#25830;&#8221;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09683v1 Announce Type: cross  Abstract: Designing seamless, frictionless user experiences has long been a dominant trend in both applied behavioral science and artificial intelligence (AI), in which the goal of making desirable actions easy and efficient informs efforts to minimize friction in user experiences. However, in some settings, friction can be genuinely beneficial, such as the insertion of deliberate delays to increase reflection, preventing individuals from resorting to automatic or biased behaviors, and enhancing opportunities for unexpected discoveries. More recently, the popularization and availability of AI on a widespread scale has only increased the need to examine how friction can help or hinder users of AI; it also suggests a need to consider how positive friction can benefit AI practitioners, both during development processes (e.g., working with diverse teams) and to inform how AI is designed into offerings. This paper first proposes a "positive friction"
&lt;/p&gt;</description></item><item><title>PAL&#26159;&#31532;&#19968;&#20010;&#40657;&#30418;&#26597;&#35810;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.09674</link><description>&lt;p&gt;
PAL&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#24341;&#23548;&#40657;&#30418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PAL: Proxy-Guided Black-Box Attack on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09674
&lt;/p&gt;
&lt;p&gt;
PAL&#26159;&#31532;&#19968;&#20010;&#40657;&#30418;&#26597;&#35810;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#20960;&#20010;&#26376;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#34987;&#25805;&#32437;&#26102;&#23427;&#20204;&#23637;&#31034;&#20986;&#30340;&#21361;&#38505;&#33021;&#21147;&#20196;&#20154;&#25285;&#24551;&#12290;&#23613;&#31649;&#23433;&#20840;&#24494;&#35843;&#31561;&#25216;&#26415;&#26088;&#22312;&#26368;&#23567;&#21270;&#26377;&#23475;&#20351;&#29992;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24341;&#21457;&#26377;&#27602;&#22238;&#24212;&#30340;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;LLMs&#30340;&#20195;&#29702;&#24341;&#23548;&#25915;&#20987;&#65288;PAL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#23545;LLMs&#30340;&#40657;&#30418;&#20165;&#26597;&#35810;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#26469;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#37319;&#29992;&#20102;&#38024;&#23545;&#30495;&#23454;&#19990;&#30028;LLM API&#35774;&#35745;&#30340;&#22797;&#26434;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;GPT-3.5-Turbo&#19978;&#36798;&#21040;84%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#65292;&#22312;Llama-2-7B&#19978;&#36798;&#21040;48%&#65292;&#32780;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20165;&#20026;4%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;GCG++&#65292;&#36825;&#26159;&#23545;GCG&#25915;&#20987;&#30340;&#25913;&#36827;&#65292;&#22312;&#30333;&#30418;Llama-2-7B&#19978;&#36798;&#21040;&#20102;94%&#30340;ASR&#65292;&#20197;&#21450;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#24378;&#26377;&#21147;&#20294;&#31616;&#21333;&#30340;&#22522;&#20934;&#26041;&#27861;&#8212;&#8212;LLMs&#19978;&#30340;&#38543;&#26426;&#25628;&#32034;&#25915;&#20987;&#65288;RAL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09674v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs (PAL), the first optimization-based attack on LLMs in a black-box query-only setting. In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world LLM APIs. Our attack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on Llama-2-7B, compared to 4% for the current state of the art. We also propose GCG++, an improvement to the GCG attack that reaches 94% ASR on white-box Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple baseline for query-based attacks. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35757;&#32451;&#25968;&#25454;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;Ask-LLM&#21644;Density&#20004;&#31181;&#20248;&#31168;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09668</link><description>&lt;p&gt;
&#22914;&#20309;&#35757;&#32451;&#25968;&#25454;&#39640;&#25928;&#30340;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Train Data-Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35757;&#32451;&#25968;&#25454;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;Ask-LLM&#21644;Density&#20004;&#31181;&#20248;&#31168;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#21313;&#20998;&#26114;&#36149;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#39044;&#35757;&#32451;LLM&#30340;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#65292;&#21363;&#26088;&#22312;&#20248;&#21270;&#27169;&#22411;&#36136;&#37327;&#21644;&#35757;&#32451;&#36164;&#28304;/&#25968;&#25454;&#28040;&#32791;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#35797;&#22270;&#29702;&#35299;&#22522;&#20110;&#65288;i&#65289;&#26114;&#36149;&#30340;&#25968;&#25454;&#36136;&#37327;&#20272;&#35745;&#21644;&#65288;ii&#65289;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#30340;&#35206;&#30422;&#29575;&#21644;&#22810;&#26679;&#24615;&#27979;&#37327;&#30340;&#25968;&#25454;&#36873;&#25321;&#31243;&#24207;&#25152;&#24102;&#26469;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#25216;&#26415;&#8220;Ask-LLM&#8221;&#21033;&#29992;&#35843;&#33410;&#25351;&#20196;&#30340;LLM&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#26469;&#30452;&#25509;&#35780;&#20272;&#35757;&#32451;&#26679;&#20363;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#36798;&#21040;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23494;&#24230;&#37319;&#26679;&#65292;&#23427;&#26681;&#25454;&#25968;&#25454;&#20998;&#24067;&#36873;&#25321;&#22810;&#26679;&#30340;&#26679;&#26412;&#12290;&#22312;&#25105;&#20204;&#23545;19&#31181;&#37319;&#26679;&#22120;&#36827;&#34892;&#20102;&#25968;&#30334;&#20010;&#35780;&#20272;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#36816;&#34892;&#30340;&#23545;&#27604;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Ask-LLM&#21644;Density&#26159;&#21508;&#33258;&#31867;&#21035;&#20013;&#26368;&#22909;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09668v1 Announce Type: cross  Abstract: The training of large language models (LLMs) is expensive. In this paper, we study data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. Our first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample. In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover th
&lt;/p&gt;</description></item><item><title>CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.09664</link><description>&lt;p&gt;
CodeMind:&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CodeMind: A Framework to Challenge Large Language Models for Code Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09664
&lt;/p&gt;
&lt;p&gt;
CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#30721;&#21512;&#25104;&#33021;&#21147;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#35780;&#20272;&#25110;&#20419;&#36827;&#20855;&#26377;&#25968;&#25454;&#27844;&#28431;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeMind&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;CodeMind&#30446;&#21069;&#25903;&#25345;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#65306;&#29420;&#31435;&#25191;&#34892;&#25512;&#29702;&#65288;IER&#65289;&#12289;&#20381;&#36182;&#25191;&#34892;&#25512;&#29702;&#65288;DER&#65289;&#21644;&#35268;&#33539;&#25512;&#29702;&#65288;SR&#65289;&#12290;&#21069;&#20004;&#32773;&#35780;&#20272;&#27169;&#22411;&#20197;&#39044;&#27979;&#20219;&#24847;&#20195;&#30721;&#30340;&#25191;&#34892;&#36755;&#20986;&#65292;&#25110;&#32773;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#21512;&#25104;&#30340;&#20195;&#30721;&#12290;&#31532;&#19977;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#23454;&#29616;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;CodeMind&#23545;&#20004;&#31181;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20116;&#20010;&#22522;&#20934;&#19979;&#30340;&#20061;&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#21253;&#25324;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#25968;&#25454;&#25216;&#26415;&#31561;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09660</link><description>&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
User Modeling and User Profiling: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09660
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#21253;&#25324;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#25968;&#25454;&#25216;&#26415;&#31561;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24050;&#32463;&#20419;&#20351;&#20808;&#36827;&#30340;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30011;&#20687;&#25216;&#26415;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#22522;&#20110;&#19982;&#36825;&#20123;&#31995;&#32479;&#30340;&#20114;&#21160;&#20013;&#29983;&#25104;&#30340;&#22823;&#37327;&#25968;&#25454;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#12290;&#26412;&#25991;&#23545;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21382;&#21490;&#27010;&#36848;&#65292;&#36861;&#28335;&#20102;&#20174;&#26089;&#26399;&#30340;&#21051;&#26495;&#27169;&#22411;&#21040;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#25152;&#26377;&#27963;&#21160;&#20027;&#39064;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#31361;&#20986;&#20102;&#21521;&#26356;&#22797;&#26434;&#30340;&#29992;&#25143;&#30011;&#20687;&#26041;&#27861;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#38544;&#24335;&#25968;&#25454;&#25910;&#38598;&#12289;&#22810;&#34892;&#20026;&#24314;&#27169;&#20197;&#21450;&#22270;&#25968;&#25454;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09660v1 Announce Type: new  Abstract: The integration of artificial intelligence (AI) into daily life, particularly through information retrieval and recommender systems, has necessitated advanced user modeling and profiling techniques to deliver personalized experiences. These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems. This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research. We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends. Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of graph data
&lt;/p&gt;</description></item><item><title>&#23613;&#31649;&#27169;&#22411;&#32534;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#23569;&#37327;&#32534;&#36753;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09656</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#30340;&#34676;&#34678;&#25928;&#24212;&#65306;&#23569;&#37327;&#32534;&#36753;&#21487;&#33021;&#24341;&#21457;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09656
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27169;&#22411;&#32534;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#23569;&#37327;&#32534;&#36753;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#27169;&#22411;&#32534;&#36753;&#24050;&#26174;&#31034;&#20986;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#23545;LLMs&#30340;&#20869;&#22312;&#33021;&#21147;&#30340;&#24433;&#21709;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#29616;&#35937;&#65306;&#21363;&#20351;&#21482;&#36827;&#34892;&#19968;&#20010;&#32534;&#36753;&#65292;&#20063;&#21487;&#20197;&#24341;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#34920;&#29616;&#20026;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#27425;&#32534;&#36753;&#21518;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#34429;&#28982;&#24517;&#35201;&#65292;&#20294;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#39034;&#24207;&#32534;&#36753;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36825;&#26159;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#19968;&#31181;&#24120;&#35265;&#24773;&#20917;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#25105;&#20204;&#20043;&#21069;&#21333;&#27425;&#32534;&#36753;&#30740;&#31350;&#20013;&#30340;&#22256;&#38590;&#26696;&#20363;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20960;&#20046;&#25152;&#26377;&#30740;&#31350;&#30340;&#32534;&#36753;&#26041;&#27861;&#37117;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09656v1 Announce Type: new  Abstract: Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream task performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#23545;&#30456;&#23545;&#32622;&#20449;&#24230;&#26377;&#24433;&#21709;&#65292;&#20294;&#24182;&#19981;&#19968;&#33268;&#22320;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.09654</link><description>&lt;p&gt;
GPT-4&#22312;&#22522;&#20110;USMLE&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GPT-4's assessment of its performance in a USMLE-based case study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#23545;&#30456;&#23545;&#32622;&#20449;&#24230;&#26377;&#24433;&#21709;&#65292;&#20294;&#24182;&#19981;&#19968;&#33268;&#22320;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20174;&#32654;&#22269;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65288;USMLE&#65289;&#38382;&#21367;&#20013;&#25552;&#21462;&#38382;&#39064;&#30340;&#26041;&#24335;&#65292;&#20219;&#21153;&#26159;&#35780;&#20272;&#27169;&#22411;&#22312;&#25552;&#38382;&#20043;&#21069;&#21644;&#25552;&#38382;&#20043;&#21518;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#12290;&#38382;&#21367;&#26681;&#25454;&#26159;&#21542;&#26377;&#21453;&#39304;&#20998;&#20026;&#20004;&#32452;&#65306;&#21453;&#39304;&#32452;&#65288;WF&#65289;&#21644;&#26080;&#21453;&#39304;&#32452;&#65288;NF&#65289;&#12290;&#35201;&#27714;&#27169;&#22411;&#22312;&#27599;&#20010;&#38382;&#39064;&#20043;&#21069;&#21644;&#20043;&#21518;&#25552;&#20379;&#32477;&#23545;&#21644;&#30456;&#23545;&#32622;&#20449;&#24230;&#24471;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#24037;&#20855;&#20998;&#26512;&#23454;&#39564;&#32467;&#26524;&#65292;&#30740;&#31350;&#20102;WF&#21644;NF&#32452;&#30340;&#32622;&#20449;&#24230;&#21464;&#24322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#39034;&#24207;&#20998;&#26512;&#20197;&#35266;&#23519;WF&#21644;NF&#32452;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#20250;&#24433;&#21709;&#30456;&#23545;&#32622;&#20449;&#24230;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09654v1 Announce Type: new  Abstract: This study investigates GPT-4's assessment of its performance in healthcare applications. A simple prompting technique was used to prompt the LLM with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question. The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The model was asked to provide absolute and relative confidence scores before and after each question. The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups. Results indicate that feedback influences relative confidence but doesn't consistently increase or decrease it. Understanding the p
&lt;/p&gt;</description></item><item><title>ProtChatGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#21644;&#29702;&#35299;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#19978;&#20256;&#34507;&#30333;&#36136;&#12289;&#25552;&#38382;&#21644;&#20132;&#20114;&#24335;&#23545;&#35805;&#31561;&#21151;&#33021;&#65292;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#29702;&#35299;&#34507;&#30333;&#36136;&#30340;&#32467;&#26500;&#19982;&#21151;&#33021;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.09649</link><description>&lt;p&gt;
ProtChatGPT&#65306;&#29992;&#20110;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#34507;&#30333;&#36136;
&lt;/p&gt;
&lt;p&gt;
ProtChatGPT: Towards Understanding Proteins with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09649
&lt;/p&gt;
&lt;p&gt;
ProtChatGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#21644;&#29702;&#35299;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#19978;&#20256;&#34507;&#30333;&#36136;&#12289;&#25552;&#38382;&#21644;&#20132;&#20114;&#24335;&#23545;&#35805;&#31561;&#21151;&#33021;&#65292;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#29702;&#35299;&#34507;&#30333;&#36136;&#30340;&#32467;&#26500;&#19982;&#21151;&#33021;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30740;&#31350;&#22312;&#21508;&#20010;&#22522;&#30784;&#23398;&#31185;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#29702;&#35299;&#20854;&#22797;&#26434;&#30340;&#32467;&#26500;&#19982;&#21151;&#33021;&#20851;&#31995;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#29305;&#23450;&#20219;&#21153;&#30340;&#30693;&#35782;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#36825;&#34920;&#26126;&#20102;&#29992;&#20110;&#34507;&#30333;&#36136;&#30340;ChatGPT-like&#31995;&#32479;&#22312;&#20419;&#36827;&#22522;&#30784;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ProtChatGPT&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#21644;&#29702;&#35299;&#34507;&#30333;&#36136;&#32467;&#26500;&#12290;ProtChatGPT&#20351;&#29992;&#25143;&#21487;&#20197;&#19978;&#20256;&#34507;&#30333;&#36136;&#12289;&#25552;&#38382;&#24182;&#36827;&#34892;&#20132;&#20114;&#24335;&#23545;&#35805;&#20197;&#20135;&#29983;&#20840;&#38754;&#30340;&#22238;&#31572;&#12290;&#35813;&#31995;&#32479;&#21253;&#25324;&#34507;&#30333;&#32534;&#30721;&#22120;&#12289;&#34507;&#30333;&#35821;&#35328;&#30456;&#20851;&#36716;&#25442;&#22120;&#65288;PLP-former&#65289;&#12289;&#25237;&#24433;&#36866;&#37197;&#22120;&#21644;LLM&#12290;&#34507;&#30333;&#36136;&#39318;&#20808;&#36890;&#36807;&#34507;&#30333;&#32534;&#30721;&#22120;&#21644;PLP-former&#36827;&#34892;&#32534;&#30721;&#20197;&#20135;&#29983;&#34507;&#30333;&#36136;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#36866;&#37197;&#22120;&#23558;&#20854;&#25237;&#23556;&#21040;&#19982;LLM&#30456;&#31526;&#21512;&#12290;&#26368;&#21518;&#65292;LLM&#23558;&#29992;&#25143;&#30340;&#38382;&#39064;&#19982;&#34507;&#30333;&#36136;&#23884;&#20837;&#36827;&#34892;&#32508;&#21512;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09649v1 Announce Type: cross  Abstract: Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging. Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages. ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers. The system comprises protein encoders, a Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and an LLM. The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM. The LLM finally combines user questions wit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLM&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#25366;&#25496;&#25928;&#29575;&#21644;&#33021;&#21147;&#65292;&#36890;&#36807;&#25972;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#36793;&#32536;&#20449;&#24687;&#26469;&#29702;&#35299;&#22797;&#26434;&#33410;&#28857;&#20851;&#31995;&#65292;&#24182;&#20174;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.09617</link><description>&lt;p&gt;
&#22686;&#24378;LLM&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#65306;&#21033;&#29992;&#36793;&#32536;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLM&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#25366;&#25496;&#25928;&#29575;&#21644;&#33021;&#21147;&#65292;&#36890;&#36807;&#25972;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#36793;&#32536;&#20449;&#24687;&#26469;&#29702;&#35299;&#22797;&#26434;&#33410;&#28857;&#20851;&#31995;&#65292;&#24182;&#20174;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#19981;&#20165;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#26684;&#23616;&#65292;&#36824;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21331;&#36234;&#24212;&#29992;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25366;&#25496;&#22270;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36817;&#24180;&#26469;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#20851;&#31995;&#25366;&#25496;&#26041;&#38754;&#26377;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23574;&#31471;&#30740;&#31350;&#23578;&#26410;&#26377;&#25928;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23548;&#33268;&#22312;&#22270;&#20851;&#31995;&#25366;&#25496;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#21644;&#33021;&#21147;&#21463;&#38480;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;LLM&#26080;&#27861;&#28145;&#20837;&#21033;&#29992;&#22270;&#20013;&#30340;&#36793;&#32536;&#20449;&#24687;&#65292;&#32780;&#36825;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#33410;&#28857;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#24046;&#36317;&#38480;&#21046;&#20102;LLM&#20174;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#27934;&#35265;&#30340;&#28508;&#21147;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#22270;&#30340;&#20998;&#26512;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09617v1 Announce Type: new  Abstract: The extraordinary performance of large language models has not only reshaped the research landscape in the field of NLP but has also demonstrated its exceptional applicative potential in various domains. However, the potential of these models in mining relationships from graph data remains under-explored. Graph neural networks, as a popular research area in recent years, have numerous studies on relationship mining. Yet, current cutting-edge research in graph neural networks has not been effectively integrated with large language models, leading to limited efficiency and capability in graph relationship mining tasks. A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships. This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis. We focus
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#29575;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09614</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#29575;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Reasoning in Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#29575;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#28041;&#21450;&#27010;&#29575;&#20540;&#26126;&#30830;&#37327;&#21270;&#30340;&#25991;&#26412;&#25512;&#29702;&#38382;&#39064;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#27010;&#29575;&#25512;&#29702;&#23545;&#20110;&#20174;&#26085;&#24120;&#23545;&#35805;&#21040;&#21307;&#23398;&#20915;&#31574;&#31561;&#21508;&#31181;&#24773;&#22659;&#37117;&#24456;&#37325;&#35201;&#12290;&#23613;&#31649;LLMs&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#22312;&#27010;&#29575;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;LLMs&#27010;&#29575;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#26469;&#35814;&#32454;&#35828;&#26126;LLMs&#22312;&#28041;&#21450;&#27010;&#29575;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#29305;&#23450;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#23558;&#38382;&#39064;&#26144;&#23556;&#21040;&#19981;&#21516;&#24418;&#24335;&#34920;&#31034;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09614v1 Announce Type: cross  Abstract: This paper considers the challenges that Large Language Models (LLMs) face when reasoning over text that includes information involving uncertainty explicitly quantified via probability values. This type of reasoning is relevant to a variety of contexts ranging from everyday conversations to medical decision-making. Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning. To deal with this problem, we first introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the probabilistic reasoning capabilities of LLMs. We then leverage this new dataset to thoroughly illustrate the specific limitations of LLMs for tasks involving probabilistic reasoning and present several strategies that map the problem to different formal representations, including Python code, probabilistic inference algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09611</link><description>&lt;p&gt;
&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Privacy-Aware Sign Language Translation at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#32763;&#35793;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#25968;&#25454;&#31232;&#32570;&#12290;&#30446;&#21069;&#22312;&#32593;&#32476;&#19978;&#21487;&#29992;&#30340;&#22823;&#37096;&#20998;&#25163;&#35821;&#25968;&#25454;&#30001;&#20110;&#32570;&#20047;&#23545;&#40784;&#30340;&#23383;&#24149;&#32780;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#38598;&#26469;&#25193;&#23637;&#25163;&#35821;&#32763;&#35793;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#20854;&#20013;&#21253;&#21547;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65292;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#25216;&#26415;&#24212;&#35813;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SSVP-SLT&#65292;&#23427;&#21033;&#29992;&#21311;&#21517;&#21644;&#26410;&#27880;&#37322;&#30340;&#35270;&#39057;&#36827;&#34892;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#21033;&#29992;&#32463;&#36807;&#31579;&#36873;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#25163;&#35821;&#32763;&#35793;&#24494;&#35843;&#12290; SSVP-SLT&#22312;How2Sign&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24494;&#35843;&#21644;&#38646;&#27425;gloss-free&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#65292;&#27604;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;3&#20010;BLEU-4&#12290;&#36890;&#36807;&#21463;&#25511;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#35328;&#21644;&#25163;&#35821;&#35789;&#27719;&#19978;&#37117;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09611v1 Announce Type: new  Abstract: A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we fu
&lt;/p&gt;</description></item><item><title>LogicPrpBank&#26159;&#19968;&#20010;&#26032;&#30340;&#21629;&#39064;&#36923;&#36753;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#36923;&#36753;&#34164;&#21547;&#21644;&#31561;&#20215;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#35813;&#35821;&#26009;&#24211;&#22312;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36164;&#28304;&#65292;&#24182;&#19988;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.09609</link><description>&lt;p&gt;
LogicPrpBank&#65306;&#19968;&#20010;&#29992;&#20110;&#36923;&#36753;&#34164;&#21547;&#21644;&#31561;&#20215;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
LogicPrpBank: A Corpus for Logical Implication and Equivalence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09609
&lt;/p&gt;
&lt;p&gt;
LogicPrpBank&#26159;&#19968;&#20010;&#26032;&#30340;&#21629;&#39064;&#36923;&#36753;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#36923;&#36753;&#34164;&#21547;&#21644;&#31561;&#20215;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#35813;&#35821;&#26009;&#24211;&#22312;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36164;&#28304;&#65292;&#24182;&#19988;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#20915;&#31574;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#22788;&#29702;&#22810;&#31181;&#25512;&#29702;&#20219;&#21153;&#65288;&#20363;&#22914;&#24120;&#35782;&#25512;&#29702;&#65289;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#21629;&#39064;&#36923;&#36753;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#36825;&#31181;&#25506;&#32034;&#30340;&#19981;&#36275;&#21487;&#20197;&#24402;&#22240;&#20110;&#26631;&#27880;&#35821;&#26009;&#24211;&#30340;&#26377;&#38480;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#26631;&#27880;&#30340;&#21629;&#39064;&#36923;&#36753;&#35821;&#26009;&#24211;LogicPrpBank&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;7093&#20010;&#21629;&#39064;&#36923;&#36753;&#38472;&#36848;&#65288;PLS&#65289;&#28085;&#30422;&#20845;&#20010;&#25968;&#23398;&#31185;&#30446;&#65292;&#20197;&#30740;&#31350;&#25512;&#29702;&#36923;&#36753;&#34164;&#21547;&#21644;&#31561;&#20215;&#30340;&#20840;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;LogicPrpBank&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#20026;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36164;&#28304;&#65292;&#24182;&#19988;&#27169;&#22411;&#25913;&#36827;&#30340;&#31354;&#38388;&#36824;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09609v1 Announce Type: cross  Abstract: Logic reasoning has been critically needed in problem-solving and decision-making. Although Language Models (LMs) have demonstrated capabilities of handling multiple reasoning tasks (e.g., commonsense reasoning), their ability to reason complex mathematical problems, specifically propositional logic, remains largely underexplored. This lack of exploration can be attributed to the limited availability of annotated corpora. Here, we present a well-labeled propositional logic corpus, LogicPrpBank, containing 7093 Propositional Logic Statements (PLSs) across six mathematical subjects, to study a brand-new task of reasoning logical implication and equivalence. We benchmark LogicPrpBank with widely-used LMs to show that our corpus offers a useful resource for this challenging task and there is ample room for model improvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#26410;&#26631;&#35760;&#27979;&#35797;&#22270;&#20687;&#26469;&#35843;&#25972;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#30452;&#25509;&#26368;&#23567;&#21270;&#39044;&#27979;&#29109;&#30340;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#24182;&#19981;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#30446;&#26631;&#22495;&#32479;&#35745;&#20272;&#35745;&#30340;&#39044;&#27979;&#36827;&#34892;&#38598;&#25104;&#65292;&#24182;&#22522;&#20110;&#26435;&#37325;&#36827;&#34892;&#21152;&#26435;&#12290;</title><link>https://arxiv.org/abs/2402.09604</link><description>&lt;p&gt;
&#20351;&#29992;InTEnt&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#22522;&#20110;&#38598;&#25104;&#29109;&#21152;&#26435;&#30340;&#21333;&#22270;&#20687;&#27979;&#35797;&#26102;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for Single Image Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#26410;&#26631;&#35760;&#27979;&#35797;&#22270;&#20687;&#26469;&#35843;&#25972;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#30452;&#25509;&#26368;&#23567;&#21270;&#39044;&#27979;&#29109;&#30340;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#24182;&#19981;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#30446;&#26631;&#22495;&#32479;&#35745;&#20272;&#35745;&#30340;&#39044;&#27979;&#36827;&#34892;&#38598;&#25104;&#65292;&#24182;&#22522;&#20110;&#26435;&#37325;&#36827;&#34892;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TTA&#65289;&#26159;&#25351;&#22312;&#27979;&#35797;&#26399;&#38388;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#35843;&#25972;&#21040;&#19968;&#20010;&#26032;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;TTA&#25216;&#26415;&#20381;&#36182;&#20110;&#22312;&#21516;&#19968;&#39046;&#22495;&#20855;&#26377;&#22810;&#20010;&#27979;&#35797;&#22270;&#20687;&#65292;&#28982;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#65288;&#22914;&#21307;&#23398;&#25104;&#20687;&#65289;&#20013;&#65292;&#25968;&#25454;&#33719;&#21462;&#36153;&#29992;&#26114;&#36149;&#19988;&#25104;&#20687;&#26465;&#20214;&#32463;&#24120;&#21464;&#21270;&#65292;&#22240;&#27492;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#20351;&#29992;&#20165;&#26377;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#22270;&#20687;&#26469;&#35843;&#25972;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;TTA&#26041;&#27861;&#30452;&#25509;&#26368;&#23567;&#21270;&#39044;&#27979;&#29109;&#65292;&#28982;&#32780;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#23427;&#20204;&#26410;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#23618;&#30340;&#32479;&#35745;&#37327;&#36873;&#25321;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#20294;&#19981;&#31283;&#23450;&#30340;&#22240;&#32032;&#65292;&#22240;&#20026;&#21482;&#26377;&#19968;&#20010;&#27979;&#35797;&#22495;&#31034;&#20363;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21508;&#31181;&#30446;&#26631;&#22495;&#32479;&#35745;&#20272;&#35745;&#30340;&#39044;&#27979;&#36827;&#34892;\textit{&#38598;&#25104;}&#65292;&#24182;&#22522;&#20110;&#26435;&#37325;&#36827;&#34892;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09604v1 Announce Type: cross  Abstract: Test-time adaptation (TTA) refers to adapting a trained model to a new domain during testing. Existing TTA techniques rely on having multiple test images from the same domain, yet this may be impractical in real-world applications such as medical imaging, where data acquisition is expensive and imaging conditions vary frequently. Here, we approach such a task, of adapting a medical image segmentation model with only a single unlabeled test image. Most TTA approaches, which directly minimize the entropy of predictions, fail to improve performance significantly in this setting, in which we also observe the choice of batch normalization (BN) layer statistics to be a highly important yet unstable factor due to only having a single test domain example. To overcome this, we propose to instead \textit{integrate} over predictions made with various estimates of target domain statistics between the training and test statistics, weighted based on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20307;&#31215;&#26368;&#22823;&#21270;&#39033;&#20943;&#23569;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#33410;&#28857;&#25110;&#32500;&#24230;&#37319;&#26679;&#21487;&#20197;&#38477;&#20302;&#25439;&#22833;&#35745;&#31639;&#30340;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.09603</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Graph Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20307;&#31215;&#26368;&#22823;&#21270;&#39033;&#20943;&#23569;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#33410;&#28857;&#25110;&#32500;&#24230;&#37319;&#26679;&#21487;&#20197;&#38477;&#20302;&#25439;&#22833;&#35745;&#31639;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#30340;&#27491;&#21017;&#21270;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#33410;&#28857;&#25968;&#21644;&#23884;&#20837;&#32500;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#20026;&#20102;&#20943;&#36731;&#38750;&#23545;&#27604;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20307;&#31215;&#26368;&#22823;&#21270;&#39033;&#20943;&#23569;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#36890;&#36807;&#22270;&#33410;&#28857;&#25110;&#32500;&#24230;&#37319;&#26679;&#20943;&#23569;&#25439;&#22833;&#35745;&#31639;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#32500;&#24230;&#37319;&#26679;&#20250;&#23548;&#33268;&#20934;&#30830;&#30340;&#25439;&#22833;&#35745;&#31639;&#65292;&#24182;&#29992;&#25968;&#23398;&#25512;&#23548;&#25903;&#25345;&#20102;&#36825;&#31181;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#33410;&#28857;&#32423;&#22270;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#22240;&#20026;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#35268;&#27169;&#24456;&#22823;&#65292;&#25152;&#20197;&#22312;&#36825;&#26041;&#38754;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#33410;&#28857;&#25110;&#32500;&#24230;&#37319;&#26679;&#21487;&#20197;&#20943;&#23569;&#25439;&#22833;&#35745;&#31639;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09603v1 Announce Type: cross  Abstract: In regularization Self-Supervised Learning (SSL) methods for graphs, computational complexity increases with the number of nodes in graphs and embedding dimensions. To mitigate the scalability of non-contrastive graph SSL, we propose a novel approach to reduce the cost of computing the covariance matrix for the pre-training loss function with volume-maximization terms. Our work focuses on reducing the cost associated with the loss computation via graph node or dimension sampling. We provide theoretical insight into why dimension sampling would result in accurate loss computations and support it with mathematical derivation of the novel approach. We develop our experimental setup on the node-level graph prediction tasks, where SSL pre-training has shown to be difficult due to the large size of real world graphs. Our experiments demonstrate that the cost associated with the loss computation can be reduced via node or dimension sampling w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#22797;&#26434;&#21307;&#30103;&#35843;&#26597;&#30740;&#31350;&#20013;&#25968;&#25454;&#25910;&#38598;&#12289;&#25972;&#29702;&#21644;&#21487;&#35270;&#21270;&#30340;&#36807;&#31243;&#65292;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#35813;&#24179;&#21488;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65292;&#21487;&#20197;&#31616;&#21270;&#38382;&#21367;&#21019;&#24314;&#12289;&#25968;&#25454;&#25910;&#38598;&#21644;&#34920;&#31034;&#31561;&#25805;&#20316;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20415;&#21033;&#21644;&#28145;&#20837;&#20102;&#35299;&#20010;&#20154;&#39278;&#37202;&#34892;&#20026;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.09592</link><description>&lt;p&gt;
&#22522;&#20110;Web&#30340;&#24037;&#20855;&#29992;&#20110;&#22797;&#26434;&#21307;&#30103;&#35843;&#26597;&#30740;&#31350;&#30340;&#33258;&#21160;&#25968;&#25454;&#25910;&#38598;&#12289;&#25972;&#29702;&#21644;&#21487;&#35270;&#21270;&#65292;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Web-Based Tool for Automatic Data Collection, Curation, and Visualization of Complex Healthcare Survey Studies including Social Network Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#22797;&#26434;&#21307;&#30103;&#35843;&#26597;&#30740;&#31350;&#20013;&#25968;&#25454;&#25910;&#38598;&#12289;&#25972;&#29702;&#21644;&#21487;&#35270;&#21270;&#30340;&#36807;&#31243;&#65292;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#35813;&#24179;&#21488;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65292;&#21487;&#20197;&#31616;&#21270;&#38382;&#21367;&#21019;&#24314;&#12289;&#25968;&#25454;&#25910;&#38598;&#21644;&#34920;&#31034;&#31561;&#25805;&#20316;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20415;&#21033;&#21644;&#28145;&#20837;&#20102;&#35299;&#20010;&#20154;&#39278;&#37202;&#34892;&#20026;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#23545;&#20110;&#39278;&#37202;&#21644;&#33647;&#29289;&#28389;&#29992;&#23588;&#20854;&#26159;&#22312;&#24180;&#36731;&#20154;&#20013;&#26377;&#30528;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#36890;&#36807;&#20998;&#26512;&#36825;&#20123;&#38738;&#23569;&#24180;&#25152;&#22788;&#30340;&#31038;&#20132;&#29615;&#22659;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#34913;&#37327;&#37202;&#31934;&#28389;&#29992;&#39118;&#38505;&#12289;&#20010;&#20154;&#29366;&#24577;&#21644;&#35748;&#30693;&#30340;&#38382;&#21367;&#35843;&#26597;&#65288;&#22914;AUDIT&#12289;FAS&#12289;KIDSCREEN&#31561;&#65289;&#65292;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#19968;&#20010;&#20154;&#22312;&#39278;&#37202;&#34892;&#20026;&#26041;&#38754;&#30340;&#23454;&#38469;&#24773;&#20917;&#12290;&#20294;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#20998;&#26512;&#65292;&#38656;&#35201;&#20351;&#29992;&#33021;&#22815;&#31616;&#21270;&#38382;&#21367;&#21019;&#24314;&#12289;&#25968;&#25454;&#25910;&#38598;&#12289;&#25972;&#29702;&#21644;&#34920;&#31034;&#20197;&#21450;&#21518;&#32493;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#36807;&#31243;&#30340;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#24179;&#21488;&#30340;&#35774;&#35745;&#21644;&#26500;&#24314;&#65292;&#35813;&#24179;&#21488;&#33021;&#22815;&#36890;&#36807;&#23558;&#19981;&#21516;&#38454;&#27573;&#25972;&#21512;&#20026;&#19968;&#20010;&#30452;&#35266;&#30340;&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#26469;&#31616;&#21270;&#19978;&#36848;&#27599;&#20010;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09592v1 Announce Type: new  Abstract: There is a great concern nowadays regarding alcohol consumption and drug abuse, especially in young people. Analyzing the social environment where these adolescents are immersed, as well as a series of measures determining the alcohol abuse risk or personal situation and perception using a number of questionnaires like AUDIT, FAS, KIDSCREEN, and others, it is possible to gain insight into the current situation of a given individual regarding his/her consumption behavior. But this analysis, in order to be achieved, requires the use of tools that can ease the process of questionnaire creation, data gathering, curation and representation, and later analysis and visualization to the user. This research presents the design and construction of a web-based platform able to facilitate each of the mentioned processes by integrating the different phases into an intuitive system with a graphical user interface that hides the complexity underlying e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.09588</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26032;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Emerging Opportunities of Using Large Language Language Models for Translation Between Drug Molecules and Indications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20998;&#23376;&#26159;&#19968;&#31181;&#25913;&#21464;&#29983;&#29289;&#20307;&#31934;&#31070;&#25110;&#36523;&#20307;&#29366;&#24577;&#30340;&#29289;&#36136;&#12290;&#27599;&#31181;&#25209;&#20934;&#30340;&#33647;&#29289;&#37117;&#26377;&#36866;&#24212;&#30151;&#65292;&#25351;&#30340;&#26159;&#35813;&#33647;&#29289;&#27835;&#30103;&#29305;&#23450;&#21307;&#30103;&#26465;&#20214;&#30340;&#27835;&#30103;&#29992;&#36884;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#22312;&#20998;&#23376;&#21644;&#20854;&#25991;&#26412;&#25551;&#36848;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#20173;&#23384;&#22312;&#20851;&#20110;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#65288;&#25110;&#21453;&#20043;&#65289;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#24212;&#29992;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#36825;&#21487;&#33021;&#26497;&#22823;&#22320;&#26377;&#30410;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#12290;&#20174;&#32473;&#23450;&#30340;&#36866;&#24212;&#30151;&#29983;&#25104;&#33647;&#29289;&#30340;&#33021;&#21147;&#23558;&#20801;&#35768;&#21457;&#29616;&#38024;&#23545;&#29305;&#23450;&#30142;&#30149;&#25110;&#38774;&#28857;&#30340;&#33647;&#29289;&#65292;&#24182;&#26368;&#32456;&#20026;&#24739;&#32773;&#25552;&#20379;&#26356;&#22909;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#33647;&#29289;&#20998;&#23376;&#21644;&#30456;&#24212;&#36866;&#24212;&#30151;&#20043;&#38388;&#30340;&#32763;&#35793;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09588v1 Announce Type: new  Abstract: A drug molecule is a substance that changes the organism's mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test exi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#25511;&#21046;&#22312;&#24314;&#31569;&#33021;&#28304;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;Shapley&#20540;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#25511;&#21046;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#29702;&#35299;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09584</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31569;&#33021;&#28304;&#31995;&#32479;&#26426;&#22120;&#23398;&#20064;&#25511;&#21046;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#25511;&#21046;&#22312;&#24314;&#31569;&#33021;&#28304;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;Shapley&#20540;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#25511;&#21046;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25511;&#21046;&#22312;&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21463;&#38480;&#20110;&#20854;&#19981;&#36879;&#26126;&#30340;&#24615;&#36136;&#21644;&#25512;&#29702;&#26426;&#21046;&#65292;&#36825;&#23545;&#20110;&#29992;&#25143;&#21644;&#24314;&#27169;&#32773;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#38590;&#20197;&#23436;&#20840;&#29702;&#35299;&#65292;&#26368;&#32456;&#23548;&#33268;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25511;&#21046;&#30340;&#20915;&#31574;&#32570;&#20047;&#20449;&#20219;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#30740;&#31350;&#21644;&#25506;&#32034;&#20102;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#65288;IML&#65289;&#65292;&#23427;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#21644;&#25512;&#29702;&#30340;&#36879;&#26126;&#24615;&#21644;&#29702;&#35299;&#24615;&#65292;&#20197;&#25552;&#39640;MLC&#21450;&#20854;&#22312;&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#20013;&#30340;&#24037;&#19994;&#24212;&#29992;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#26694;&#26550;&#65292;&#23558;Shapley&#20540;&#30340;&#21407;&#21017;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29305;&#24615;&#30456;&#32467;&#21512;&#12290;&#32780;Shapley&#20540;&#22312;&#35299;&#21078;ML&#27169;&#22411;&#20013;&#21508;&#31181;&#29305;&#24449;&#30340;&#36129;&#29486;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;LLM&#21017;&#21487;&#20197;&#28145;&#20837;&#29702;&#35299;MLC&#20013;&#22522;&#20110;&#35268;&#21017;&#30340;&#37096;&#20998;&#65307;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#65292;LLM&#36827;&#19968;&#27493;&#23558;&#36825;&#20123;&#27934;&#35265;&#25171;&#21253;&#21040;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09584v1 Announce Type: new  Abstract: The potential of Machine Learning Control (MLC) in HVAC systems is hindered by its opaque nature and inference mechanisms, which is challenging for users and modelers to fully comprehend, ultimately leading to a lack of trust in MLC-based decision-making. To address this challenge, this paper investigates and explores Interpretable Machine Learning (IML), a branch of Machine Learning (ML) that enhances transparency and understanding of models and their inferences, to improve the credibility of MLC and its industrial application in HVAC systems. Specifically, we developed an innovative framework that combines the principles of Shapley values and the in-context learning feature of Large Language Models (LLMs). While the Shapley values are instrumental in dissecting the contributions of various features in ML models, LLM provides an in-depth understanding of rule-based parts in MLC; combining them, LLM further packages these insights into a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#24212;&#23545;&#28145;&#24230;&#20266;&#36896;&#23041;&#32961;&#30340;&#25919;&#31574;&#24314;&#35758;&#65292;&#21253;&#25324;&#32972;&#26223;&#20449;&#24687;&#12289;&#21361;&#23475;&#12289;&#20808;&#21069;&#30340;&#31435;&#27861;&#25552;&#26696;&#20197;&#21450;&#20840;&#38754;&#30340;&#28145;&#24230;&#20266;&#36896;&#20379;&#24212;&#38142;&#25919;&#31574;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.09581</link><description>&lt;p&gt;
&#25171;&#20987;&#28145;&#24230;&#20266;&#36896;&#65306;&#24212;&#23545;&#22269;&#23478;&#23433;&#20840;&#23041;&#32961;&#21644;&#20405;&#29359;&#26435;&#21033;&#30340;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Combatting deepfakes: Policies to address national security threats and rights violations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#24212;&#23545;&#28145;&#24230;&#20266;&#36896;&#23041;&#32961;&#30340;&#25919;&#31574;&#24314;&#35758;&#65292;&#21253;&#25324;&#32972;&#26223;&#20449;&#24687;&#12289;&#21361;&#23475;&#12289;&#20808;&#21069;&#30340;&#31435;&#27861;&#25552;&#26696;&#20197;&#21450;&#20840;&#38754;&#30340;&#28145;&#24230;&#20266;&#36896;&#20379;&#24212;&#38142;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#24212;&#23545;&#28145;&#24230;&#20266;&#36896;&#23041;&#32961;&#30340;&#25919;&#31574;&#24314;&#35758;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#28145;&#24230;&#20266;&#36896;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#24182;&#22238;&#39038;&#20102;&#23427;&#20204;&#25152;&#24102;&#26469;&#30340;&#21361;&#23475;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#30446;&#21069;&#28145;&#24230;&#20266;&#36896;&#34987;&#29992;&#20110;&#20256;&#25773;&#24615;&#34384;&#24453;&#26448;&#26009;&#12289;&#36827;&#34892;&#27450;&#35784;&#12289;&#25805;&#32437;&#36873;&#27665;&#34892;&#20026;&#20197;&#21450;&#23545;&#22269;&#23478;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#30340;&#24773;&#20917;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20808;&#21069;&#30340;&#31435;&#27861;&#25552;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#38382;&#39064;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#25919;&#31574;&#24314;&#35758;&#65292;&#37325;&#28857;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#20379;&#24212;&#38142;&#30340;&#22810;&#20010;&#29615;&#33410;&#12290;&#28145;&#24230;&#20266;&#36896;&#20379;&#24212;&#38142;&#20174;&#23569;&#25968;&#27169;&#22411;&#24320;&#21457;&#32773;&#12289;&#27169;&#22411;&#25552;&#20379;&#32773;&#21644;&#35745;&#31639;&#25552;&#20379;&#32773;&#24320;&#22987;&#65292;&#25193;&#23637;&#33267;&#25968;&#21313;&#20159;&#28508;&#22312;&#30340;&#28145;&#24230;&#20266;&#36896;&#21046;&#20316;&#32773;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#36825;&#20010;&#20379;&#24212;&#38142;&#65292;&#24182;&#35828;&#26126;&#27599;&#20010;&#29615;&#33410;&#30340;&#23454;&#20307;&#37117;&#24212;&#37319;&#21462;&#21512;&#29702;&#25514;&#26045;&#65292;&#38450;&#27490;&#28145;&#24230;&#20266;&#36896;&#30340;&#21046;&#36896;&#21644;&#20256;&#25773;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#23545;&#31574;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09581v1 Announce Type: cross  Abstract: This paper provides policy recommendations to address threats from deepfakes. First, we provide background information about deepfakes and review the harms they pose. We describe how deepfakes are currently used to proliferate sexual abuse material, commit fraud, manipulate voter behavior, and pose threats to national security. Second, we review previous legislative proposals designed to address deepfakes. Third, we present a comprehensive policy proposal that focuses on addressing multiple parts of the deepfake supply chain. The deepfake supply chain begins with a small number of model developers, model providers, and compute providers, and it expands to include billions of potential deepfake creators. We describe this supply chain in greater detail and describe how entities at each step of the supply chain ought to take reasonable measures to prevent the creation and proliferation of deepfakes. Finally, we address potential counterpo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#19982;EnergyPlus&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#36719;&#20214;&#34701;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09579</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#21160;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#65306;&#25506;&#32034;&#21644;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#19982;EnergyPlus&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#36719;&#20214;&#34701;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#20419;&#36827;&#20102;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20026;&#19987;&#38376;&#30340;&#24037;&#31243;&#24314;&#27169;&#65288;&#23588;&#20854;&#26159;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#65289;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#36719;&#20214;&#65288;&#20855;&#20307;&#20026;EnergyPlus&#65289;&#34701;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#65292;&#25581;&#31034;&#20102;&#22312;&#24037;&#31243;&#24314;&#27169;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#38271;&#36235;&#21183;&#65292;&#20294;&#22312;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#24314;&#31569;&#33021;&#28304;&#24314;&#27169;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#65306;1&#65289;&#27169;&#25311;&#36755;&#20837;&#29983;&#25104;&#65292;2&#65289;&#27169;&#25311;&#36755;&#20986;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#65292;3&#65289;&#36827;&#34892;&#38169;&#35823;&#20998;&#26512;&#65292;4&#65289;&#20849;&#27169;&#25311;&#65292;5&#65289;&#27169;&#25311;&#30693;&#35782;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09579v1 Announce Type: cross  Abstract: The rapid progression in artificial intelligence has facilitated the emergence of large language models like ChatGPT, offering potential applications extending into specialized engineering modeling, especially physics-based building energy modeling. This paper investigates the innovative integration of large language models with building energy modeling software, focusing specifically on the fusion of ChatGPT with EnergyPlus. A literature review is first conducted to reveal a growing trend of incorporating of large language models in engineering modeling, albeit limited research on their application in building energy modeling. We underscore the potential of large language models in addressing building energy modeling challenges and outline potential applications including 1) simulation input generation, 2) simulation output analysis and visualization, 3) conducting error analysis, 4) co-simulation, 5) simulation knowledge extraction a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#28023;&#37327;&#30340;Web&#22270;&#25968;&#25454;&#20013;&#23545;&#32972;&#26223;&#33410;&#28857;&#36827;&#34892;&#21387;&#32553;&#65292;&#24182;&#23558;&#37325;&#28857;&#25918;&#22312;&#20998;&#26512;&#30446;&#26631;&#33410;&#28857;&#19978;&#65292;&#20197;&#35299;&#20915;&#22270;&#25968;&#25454;&#23384;&#20648;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.09565</link><description>&lt;p&gt;
&#22270;&#39592;&#26550;&#65306;&#20165;&#26377;&#32422;1%&#30340;&#33410;&#28857;&#36275;&#20197;&#34920;&#31034;&#21313;&#20159;&#35268;&#27169;&#30340;&#22270;
&lt;/p&gt;
&lt;p&gt;
Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#28023;&#37327;&#30340;Web&#22270;&#25968;&#25454;&#20013;&#23545;&#32972;&#26223;&#33410;&#28857;&#36827;&#34892;&#21387;&#32553;&#65292;&#24182;&#23558;&#37325;&#28857;&#25918;&#22312;&#20998;&#26512;&#30446;&#26631;&#33410;&#28857;&#19978;&#65292;&#20197;&#35299;&#20915;&#22270;&#25968;&#25454;&#23384;&#20648;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Web&#19978;&#22270;&#25968;&#25454;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;Web&#22270;&#25366;&#25496;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22823;&#35268;&#27169;Web&#22270;&#30340;&#26222;&#21450;&#32473;&#23384;&#20648;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#22270;&#27169;&#22411;&#35774;&#35745;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#26469;&#25552;&#39640;&#22270;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#23398;&#26415;&#30740;&#31350;&#19982;&#23454;&#38469;Web&#22270;&#25366;&#25496;&#24212;&#29992;&#20043;&#38388;&#20173;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24037;&#19994;&#22330;&#26223;&#20013;&#65292;&#23454;&#38469;&#19978;&#21482;&#38656;&#35201;&#20998;&#26512;Web&#22270;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#33410;&#28857;&#31216;&#20026;&#30446;&#26631;&#33410;&#28857;&#65292;&#20854;&#20182;&#33410;&#28857;&#31216;&#20026;&#32972;&#26223;&#33410;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20174;&#28023;&#37327;Web&#22270;&#25968;&#25454;&#20013;&#24688;&#24403;&#22320;&#25552;&#21462;&#21644;&#21387;&#32553;&#32972;&#26223;&#33410;&#28857;&#21487;&#33021;&#26159;&#35299;&#20915;&#38382;&#39064;&#30340;&#26356;&#32463;&#27982;&#30340;&#25463;&#24452;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#20102;&#30446;&#26631;&#33410;&#28857;&#20998;&#31867;&#30340;&#22823;&#35268;&#27169;&#32972;&#26223;&#33410;&#28857;&#21387;&#32553;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09565v1 Announce Type: new  Abstract: Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.09558</link><description>&lt;p&gt;
&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#21452;&#21521;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Generative Pre-training for Improving Time Series Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09558
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20197;&#29992;&#20110;&#21028;&#21035;&#20219;&#21153;&#19968;&#30452;&#26159;&#19968;&#39033;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#35201;&#20040;&#26159;&#21333;&#21521;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#35201;&#20040;&#26159;&#38543;&#26426;&#23631;&#34109;&#26631;&#35760;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#21452;&#21521;&#21450;&#26102;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;BiTimelyGPT&#65289;&#65292;&#23427;&#36890;&#36807;&#20132;&#26367;&#30340;Transformer&#23618;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#19979;&#19968;&#20010;&#26631;&#35760;&#21644;&#19978;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#20445;&#30041;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#21407;&#22987;&#20998;&#24067;&#21644;&#25968;&#25454;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#20840;&#31209;&#21069;&#21521;&#21644;&#21518;&#21521;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290; &#20351;&#29992;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#65292;BiTimelyGPT&#22312;&#39044;&#27979;&#31070;&#32463;&#21151;&#33021;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#27880;&#24847;&#21147;&#28909;&#22270;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#30340;BiTimelyGPT&#33021;&#22815;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#21028;&#21035;&#24615;&#30340;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09558v1 Announce Type: new  Abstract: Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#21152;&#25343;&#22823;&#22467;&#24503;&#33945;&#39039;&#24066;&#19981;&#21516;&#31867;&#22411;&#32039;&#24613;&#20107;&#20214;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20998;&#26512;&#20102;&#20107;&#20214;&#31867;&#22411;&#19982;&#37051;&#22495;&#23618;&#38754;&#30340;&#31038;&#20250;&#32463;&#27982;&#21644;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#20851;&#32852;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09553</link><description>&lt;p&gt;
&#32479;&#35745;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#28779;&#28798;&#21644;&#20854;&#20182;&#32039;&#24613;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
Statistical and Machine Learning Models for Predicting Fire and Other Emergency Events
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#21152;&#25343;&#22823;&#22467;&#24503;&#33945;&#39039;&#24066;&#19981;&#21516;&#31867;&#22411;&#32039;&#24613;&#20107;&#20214;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20998;&#26512;&#20102;&#20107;&#20214;&#31867;&#22411;&#19982;&#37051;&#22495;&#23618;&#38754;&#30340;&#31038;&#20250;&#32463;&#27982;&#21644;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#20013;&#30340;&#32039;&#24613;&#20107;&#20214;&#32473;&#20010;&#20154;&#12289;&#23478;&#24237;&#21644;&#31038;&#21306;&#37117;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#20934;&#30830;&#21644;&#21450;&#26102;&#22320;&#39044;&#27979;&#20107;&#20214;&#21487;&#20197;&#24110;&#21161;&#24212;&#24613;&#28040;&#38450;&#21644;&#25937;&#25588;&#37096;&#38376;&#20026;&#21644;&#20943;&#36731;&#32039;&#24613;&#20107;&#20214;&#30340;&#21518;&#26524;&#20570;&#22909;&#20934;&#22791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#21152;&#25343;&#22823;&#22467;&#24503;&#33945;&#39039;&#24066;&#19981;&#21516;&#31867;&#22411;&#32039;&#24613;&#20107;&#20214;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#26041;&#27861;&#65306;&#65288;i&#65289;&#25968;&#25454;&#25910;&#38598;&#21644;&#25968;&#25454;&#38598;&#24320;&#21457;&#65307;&#65288;ii&#65289;&#23545;&#19981;&#21516;&#26102;&#31354;&#32423;&#21035;&#30340;&#27599;&#31181;&#20107;&#20214;&#31867;&#22411;&#21450;&#20854;&#29305;&#24449;&#36827;&#34892;&#25551;&#36848;&#24615;&#20998;&#26512;&#65307;&#65288;iii&#65289;&#22522;&#20110;&#30456;&#20851;&#31995;&#25968;&#20998;&#26512;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#30340;&#29305;&#24449;&#20998;&#26512;&#21644;&#36873;&#25321;&#65307;&#65288;iv&#65289;&#38024;&#23545;&#19981;&#21516;&#26102;&#31354;&#20998;&#36776;&#29575;&#24320;&#21457;&#27599;&#31181;&#20107;&#20214;&#31867;&#22411;&#21457;&#29983;&#21487;&#33021;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20107;&#20214;&#31867;&#22411;&#19982;&#37051;&#22495;&#23618;&#38754;&#30340;&#31038;&#20250;&#32463;&#27982;&#21644;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09553v1 Announce Type: new  Abstract: Emergency events in a city cause considerable economic loss to individuals, their families, and the community. Accurate and timely prediction of events can help the emergency fire and rescue services in preparing for and mitigating the consequences of emergency events. In this paper, we present a systematic development of predictive models for various types of emergency events in the City of Edmonton, Canada. We present methods for (i) data collection and dataset development; (ii) descriptive analysis of each event type and its characteristics at different spatiotemporal levels; (iii) feature analysis and selection based on correlation coefficient analysis and feature importance analysis; and (iv) development of prediction models for the likelihood of occurrence of each event type at different temporal and spatial resolutions. We analyze the association of event types with socioeconomic and demographic data at the neighborhood level, ide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23548;&#33322;&#31995;&#32479;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NPS Attack&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#21518;&#32512;&#26469;&#25805;&#32437;&#23548;&#33322;&#27169;&#22411;&#65292;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#34892;&#20026;&#12290;&#35813;&#30740;&#31350;&#23545;&#33258;&#21160;&#39550;&#39542;&#12289;&#29289;&#27969;&#21644;&#32039;&#24613;&#26381;&#21153;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.09546</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#23548;&#33322;&#26102;&#26377;&#22810;&#23433;&#20840;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23548;&#33322;&#31995;&#32479;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NPS Attack&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#21518;&#32512;&#26469;&#25805;&#32437;&#23548;&#33322;&#27169;&#22411;&#65292;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#34892;&#20026;&#12290;&#35813;&#30740;&#31350;&#23545;&#33258;&#21160;&#39550;&#39542;&#12289;&#29289;&#27969;&#21644;&#32039;&#24613;&#26381;&#21153;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#21270;&#39046;&#22495;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23548;&#33322;&#31995;&#32479;&#26368;&#36817;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#26041;&#38754;&#21463;&#21040;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#22312;&#22478;&#24066;&#25143;&#22806;&#29615;&#22659;&#20013;&#39318;&#27425;&#25506;&#32034;&#20102;LLM-based&#23548;&#33322;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#22240;&#20026;&#35813;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12289;&#29289;&#27969;&#21644;&#32039;&#24613;&#26381;&#21153;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Navigational Prompt Suffix (NPS) Attack&#65292;&#36890;&#36807;&#23558;&#26799;&#24230;&#23548;&#20986;&#30340;&#21518;&#32512;&#28155;&#21152;&#21040;&#21407;&#22987;&#23548;&#33322;&#25552;&#31034;&#65292;&#25805;&#32437;LLM-based&#23548;&#33322;&#27169;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;LLMs&#30340;&#23548;&#33322;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#21508;&#31181;LLMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26469;&#33258;Touchdown&#21644;Map2Seq&#34903;&#26223;&#25968;&#25454;&#38598;&#65292;&#22312;few-shot&#23398;&#20064;&#21644;fine-tuning&#37197;&#32622;&#19979;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;NPS Attack&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09546v1 Announce Type: cross  Abstract: In the field of robotics and automation, navigation systems based on Large Language Models (LLMs) have recently shown impressive performance. However, the security aspects of these systems have received relatively less attention. This paper pioneers the exploration of vulnerabilities in LLM-based navigation models in urban outdoor environments, a critical area given the technology's widespread application in autonomous driving, logistics, and emergency services. Specifically, we introduce a novel Navigational Prompt Suffix (NPS) Attack that manipulates LLM-based navigation models by appending gradient-derived suffixes to the original navigational prompt, leading to incorrect actions. We conducted comprehensive experiments on an LLMs-based navigation model that employs various LLMs for reasoning. Our results, derived from the Touchdown and Map2Seq street-view datasets under both few-shot learning and fine-tuning configurations, demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#20855;&#26377;&#36739;&#22823;&#949;&#30340;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#38450;&#24481;&#23454;&#38469;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#22240;&#20026;&#23454;&#38469;&#25915;&#20987;&#32773;&#21487;&#33021;&#32570;&#20047;&#20934;&#30830;&#30340;&#31169;&#26377;&#25968;&#25454;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#38598;&#21487;&#33021;&#30456;&#23545;&#23481;&#26131;&#34987;&#38450;&#24481;&#12290;</title><link>https://arxiv.org/abs/2402.09540</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#20855;&#26377;&#36739;&#22823;&#949;&#30340;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#38450;&#24481;&#23454;&#38469;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Does Differential Privacy with Large Epsilon Defend Against Practical Membership Inference Attacks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#20855;&#26377;&#36739;&#22823;&#949;&#30340;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#38450;&#24481;&#23454;&#38469;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#22240;&#20026;&#23454;&#38469;&#25915;&#20987;&#32773;&#21487;&#33021;&#32570;&#20047;&#20934;&#30830;&#30340;&#31169;&#26377;&#25968;&#25454;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#38598;&#21487;&#33021;&#30456;&#23545;&#23481;&#26131;&#34987;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36739;&#23567;&#30340;&#38544;&#31169;&#21442;&#25968;&#949;&#65292;&#949;-&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#65292;&#21363;&#27809;&#26377;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#33021;&#22815;&#25104;&#21151;&#30830;&#23450;&#19968;&#20010;&#20154;&#30340;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;DP&#30340;&#20445;&#35777;&#26159;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#65292;&#22240;&#20026;&#65306;a&#65289;&#21363;&#20351;&#25915;&#20987;&#32773;&#24050;&#32463;&#30693;&#36947;&#25968;&#25454;&#38598;&#20013;&#38500;&#19968;&#20010;&#20154;&#30340;&#35760;&#24405;&#20043;&#22806;&#30340;&#25152;&#26377;&#35760;&#24405;&#65307;b&#65289;&#23427;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#22343;&#21248;&#36866;&#29992;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#26679;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#21487;&#33021;&#36807;&#20110;&#20005;&#26684;&#65306;&#23454;&#38469;&#25915;&#20987;&#32773;&#21487;&#33021;&#32570;&#20047;&#65288;&#20960;&#20046;&#25152;&#26377;&#65289;&#31169;&#26377;&#25968;&#25454;&#30340;&#31934;&#30830;&#30693;&#35782;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#27604;&#26368;&#22351;&#24773;&#20917;&#30340;&#25968;&#25454;&#38598;&#26356;&#23481;&#26131;&#34987;&#38450;&#24481;&#12290;&#36825;&#20123;&#32771;&#34385;&#25512;&#21160;&#20102;&#20855;&#26377;&#22823;&#30340;&#38544;&#31169;&#21442;&#25968;&#65288;&#20363;&#22914;&#949;&#8805;7&#65289;&#30340;DP&#27169;&#22411;&#30340;&#24037;&#19994;&#37096;&#32626;&#65292;&#24182;&#19988;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#20855;&#26377;&#22823;&#949;&#30340;DP&#21487;&#20197;&#25104;&#21151;&#38450;&#24481;&#26368;&#20808;&#36827;&#30340;MIA&#12290;&#29616;&#26377;&#30340;DP&#27169;&#22411;&#30740;&#31350;&#19968;&#33324;&#38598;&#20013;&#20110;&#23567;&#949;&#65292;&#22240;&#27492;&#23578;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#20855;&#26377;&#36739;&#22823;&#949;&#30340;DP&#21487;&#20197;&#38450;&#24481;&#23454;&#38469;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09540v1 Announce Type: cross  Abstract: For small privacy parameter $\epsilon$, $\epsilon$-differential privacy (DP) provides a strong worst-case guarantee that no membership inference attack (MIA) can succeed at determining whether a person's data was used to train a machine learning model. The guarantee of DP is worst-case because: a) it holds even if the attacker already knows the records of all but one person in the data set; and b) it holds uniformly over all data sets. In practical applications, such a worst-case guarantee may be overkill: practical attackers may lack exact knowledge of (nearly all of) the private data, and our data set might be easier to defend, in some sense, than the worst-case data set. Such considerations have motivated the industrial deployment of DP models with large privacy parameter (e.g. $\epsilon \geq 7$), and it has been observed empirically that DP with large $\epsilon$ can successfully defend against state-of-the-art MIAs. Existing DP the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#20462;&#22797;&#21644;&#38899;&#20048;&#25490;&#21015;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.09508</link><description>&lt;p&gt;
&#25490;&#21015;&#12289;&#20462;&#22797;&#21644;&#25913;&#36827;&#65306;&#36890;&#36807;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#23454;&#29616;&#21487;&#25805;&#25511;&#30340;&#38271;&#26399;&#38899;&#20048;&#38899;&#39057;&#29983;&#25104;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09508
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#20462;&#22797;&#21644;&#38899;&#20048;&#25490;&#21015;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#38899;&#20048;&#29983;&#25104;&#22312;&#20154;&#26426;&#38899;&#20048;&#20849;&#21019;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#38899;&#20048;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#33258;&#22238;&#24402;&#29983;&#25104;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#22320;&#35299;&#20915;&#38899;&#20048;&#20462;&#22797;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;PEFT&#26041;&#27861;&#38598;&#25104;&#20102;&#22522;&#20110;&#24103;&#32423;&#20869;&#23481;&#30340;&#25511;&#21046;&#65292;&#20419;&#36827;&#20102;&#36712;&#36947;&#26465;&#20214;&#38899;&#20048;&#30340;&#31934;&#28860;&#21644;&#20998;&#25968;&#26465;&#20214;&#38899;&#20048;&#30340;&#25490;&#21015;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;MusicGen&#65292;&#19968;&#20010;&#39046;&#20808;&#30340;&#33258;&#22238;&#24402;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#22810;&#20010;&#38899;&#20048;&#32534;&#36753;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20026;&#26410;&#26469;&#30340;AI&#39537;&#21160;&#38899;&#20048;&#32534;&#36753;&#24037;&#20855;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09508v1 Announce Type: cross  Abstract: Controllable music generation plays a vital role in human-AI music co-creation. While Large Language Models (LLMs) have shown promise in generating high-quality music, their focus on autoregressive generation limits their utility in music editing tasks. To bridge this gap, we introduce a novel Parameter-Efficient Fine-Tuning (PEFT) method. This approach enables autoregressive language models to seamlessly address music inpainting tasks. Additionally, our PEFT method integrates frame-level content-based controls, facilitating track-conditioned music refinement and score-conditioned music arrangement. We apply this method to fine-tune MusicGen, a leading autoregressive music generation model. Our experiments demonstrate promising results across multiple music editing tasks, offering more flexible controls for future AI-driven music editing tools. A demo page\footnote{\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and source 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#26234;&#33021;&#26426;&#22120;&#24418;&#24335;&#19978;&#19981;&#21487;&#21028;&#23450;&#29305;&#24449;&#30340;&#26465;&#20214;, &#21457;&#23637;&#20102;&#19968;&#31181;&#25968;&#23398;&#19978;&#29420;&#31435;&#20110;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;, &#21457;&#29616;Rice&#23450;&#29702;&#19981;&#33021;&#29992;&#20110;&#21028;&#26029;&#26426;&#22120;&#26159;&#21542;&#20855;&#26377;&#32473;&#23450;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.09500</link><description>&lt;p&gt;
&#20851;&#20110;&#26234;&#33021;&#26426;&#22120;&#24418;&#24335;&#19978;&#19981;&#21487;&#21028;&#23450;&#29305;&#24449;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Formally Undecidable Traits of Intelligent Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09500
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26234;&#33021;&#26426;&#22120;&#24418;&#24335;&#19978;&#19981;&#21487;&#21028;&#23450;&#29305;&#24449;&#30340;&#26465;&#20214;, &#21457;&#23637;&#20102;&#19968;&#31181;&#25968;&#23398;&#19978;&#29420;&#31435;&#20110;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;, &#21457;&#29616;Rice&#23450;&#29702;&#19981;&#33021;&#29992;&#20110;&#21028;&#26029;&#26426;&#22120;&#26159;&#21542;&#20855;&#26377;&#32473;&#23450;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#22312;Alfonseca&#31561;&#20154;&#65288;2021&#65289;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35777;&#26126;&#20219;&#24847;&#20154;&#24037;&#26234;&#33021;&#26426;&#22120;&#23558;&#23637;&#31034;&#26576;&#31181;&#34892;&#20026;&#22312;&#36923;&#36753;&#19978;&#21487;&#33021;&#30340;&#26465;&#20214;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#31867;&#20284;&#20294;&#25968;&#23398;&#19978;&#29420;&#31435;&#20110;&#24418;&#24335;&#35821;&#35328;&#21450;&#20854;&#23646;&#24615;&#29702;&#35770;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#19981;&#20165;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#26041;&#24335;&#26469;&#25551;&#36848;&#25105;&#20204;&#23545;&#26426;&#22120;&#26399;&#26395;&#30340;&#29305;&#24449;&#65288;&#22914;&#26234;&#33021;&#12289;&#21487;&#25511;&#12289;&#36947;&#24503;&#31561;&#65289;&#65292;&#32780;&#19988;&#36824;&#35814;&#32454;&#35828;&#26126;&#20102;&#21028;&#26029;&#32473;&#23450;&#20219;&#24847;&#26426;&#22120;&#26159;&#21542;&#20855;&#26377;&#36825;&#26679;&#30340;&#29305;&#24449;&#22312;&#36923;&#36753;&#19978;&#21487;&#33021;&#30340;&#26465;&#20214;&#12290;&#19982;Alfonseca&#31561;&#20154;&#65288;2021&#65289;&#30340;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#21487;&#35745;&#31639;&#24615;&#29702;&#35770;&#20013;&#30340;Rice&#23450;&#29702;&#36890;&#24120;&#19981;&#33021;&#29992;&#20110;&#21028;&#26029;&#32473;&#23450;&#30340;&#20219;&#24847;&#26426;&#22120;&#26159;&#21542;&#20855;&#26377;&#32473;&#23450;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#24182;&#19981;&#19968;&#23450;&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#19968;&#20010;&#20219;&#24847;&#26426;&#22120;&#26159;&#26234;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09500v1 Announce Type: new  Abstract: Building on work by Alfonseca et al. (2021), we study the conditions necessary for it to be logically possible to prove that an arbitrary artificially intelligent machine will exhibit certain behavior. To do this, we develop a formalism like -- but mathematically distinct from -- the theory of formal languages and their properties. Our formalism affords a precise means for not only talking about the traits we desire of machines (such as them being intelligent, contained, moral, and so forth), but also for detailing the conditions necessary for it to be logically possible to decide whether a given arbitrary machine possesses such a trait or not. Contrary to Alfonseca et al.'s (2021) results, we find that Rice's theorem from computability theory cannot in general be used to determine whether an arbitrary machine possesses a given trait or not. Therefore, it is not necessarily the case that deciding whether an arbitrary machine is intellige
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35780;&#20272;&#20102;&#20135;&#21518;&#23615;&#22833;&#31105;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#21464;&#37327;&#65292;&#24182;&#21457;&#29616;&#22806;&#22312;&#21464;&#37327;&#26159;PUI&#38382;&#39064;&#30340;&#37325;&#35201;&#39044;&#27979;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.09498</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#39044;&#38450;&#20135;&#21518;&#23615;&#22833;&#31105;&#30340;&#26368;&#20855;&#24433;&#21709;&#21147;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Detection of the most influential variables for preventing postpartum urinary incontinence using machine learning techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35780;&#20272;&#20102;&#20135;&#21518;&#23615;&#22833;&#31105;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#21464;&#37327;&#65292;&#24182;&#21457;&#29616;&#22806;&#22312;&#21464;&#37327;&#26159;PUI&#38382;&#39064;&#30340;&#37325;&#35201;&#39044;&#27979;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09498v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#32972;&#26223;&#65306;&#20135;&#21518;&#23615;&#22833;&#31105;&#65288;PUI&#65289;&#26159;&#20135;&#21518;&#22919;&#22899;&#30340;&#24120;&#35265;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#37492;&#23450;&#20102;&#28508;&#22312;&#30340;&#30456;&#20851;&#21464;&#37327;&#65292;&#20294;&#32570;&#20047;&#23545;&#22922;&#23072;&#26399;&#38388;&#26576;&#20123;&#22266;&#26377;&#21644;&#22806;&#22312;&#30340;&#24739;&#32773;&#21464;&#37327;&#30340;&#20998;&#26512;&#12290;&#30446;&#30340;&#65306;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;PUI&#20013;&#30340;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#21464;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#22266;&#26377;&#12289;&#22806;&#22312;&#21644;&#32508;&#21512;&#21464;&#37327;&#32452;&#12290;&#26041;&#27861;&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#36807;&#37319;&#26679;&#25216;&#26415;&#20998;&#26512;&#20102;93&#21517;&#23381;&#22919;&#30340;&#25968;&#25454;&#12290;&#39044;&#27979;&#20102;&#22235;&#20010;&#20851;&#38190;&#21464;&#37327;&#65306;&#23615;&#22833;&#31105;&#21457;&#29983;&#29575;&#12289;&#39057;&#29575;&#12289;&#24378;&#24230;&#21644;&#21387;&#21147;&#23615;&#22833;&#31105;&#12290;&#32467;&#26524;&#65306;&#20351;&#29992;&#22806;&#22312;&#21464;&#37327;&#30340;&#27169;&#22411;&#26368;&#20934;&#30830;&#65292;&#23615;&#22833;&#31105;&#20934;&#30830;&#29575;&#20026;70&#65285;&#65292;&#39057;&#29575;&#20026;77&#65285;&#65292;&#24378;&#24230;&#20026;71&#65285;&#65292;&#21387;&#21147;&#23615;&#22833;&#31105;&#20026;93&#65285;&#12290;&#32467;&#35770;&#65306;&#26412;&#30740;&#31350;&#24378;&#35843;&#22806;&#22312;&#21464;&#37327;&#26159;PUI&#38382;&#39064;&#30340;&#37325;&#35201;&#39044;&#27979;&#22240;&#32032;&#12290;&#36825;&#34920;&#26126;PUI&#30340;&#39044;&#38450;&#21487;&#33021;&#38656;&#35201;&#27880;&#24847;&#22806;&#22312;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09498v1 Announce Type: new  Abstract: Background: Postpartum urinary incontinence (PUI) is a common issue among postnatal women. Previous studies identified potential related variables, but lacked analysis on certain intrinsic and extrinsic patient variables during pregnancy.   Objective: The study aims to evaluate the most influential variables in PUI using machine learning, focusing on intrinsic, extrinsic, and combined variable groups.   Methods: Data from 93 pregnant women were analyzed using machine learning and oversampling techniques. Four key variables were predicted: occurrence, frequency, intensity of urinary incontinence, and stress urinary incontinence.   Results: Models using extrinsic variables were most accurate, with 70% accuracy for urinary incontinence, 77% for frequency, 71% for intensity, and 93% for stress urinary incontinence.   Conclusions: The study highlights extrinsic variables as significant predictors of PUI issues. This suggests that PUI preventi
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#20010;&#22686;&#24378;&#20854;&#23454;&#29992;&#24615;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#26696;&#24573;&#35270;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SafeCoder&#65292;&#36890;&#36807;&#23433;&#20840;&#24494;&#35843;&#21644;&#26631;&#20934;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#26469;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09497</link><description>&lt;p&gt;
&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Secure Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09497
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#20010;&#22686;&#24378;&#20854;&#23454;&#29992;&#24615;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#26696;&#24573;&#35270;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SafeCoder&#65292;&#36890;&#36807;&#23433;&#20840;&#24494;&#35843;&#21644;&#26631;&#20934;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#26469;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#26085;&#24120;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#65292;&#23588;&#20854;&#22312;&#32534;&#31243;&#20013;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#36807;&#31243;&#65292;&#36890;&#36807;&#35757;&#32451;LMs&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#21644;&#20154;&#31867;&#20559;&#22909;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#24378;&#20102;LMs&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#26696;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#25351;&#20196;&#35843;&#20248;&#30340;LMs&#20063;&#32463;&#24120;&#20135;&#29983;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#65292;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SafeCoder&#26469;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#12290;SafeCoder&#20351;&#29992;&#19968;&#20010;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23433;&#20840;&#20026;&#20013;&#24515;&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#25910;&#38598;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;&#23433;&#20840;&#24494;&#35843;&#19982;&#26631;&#20934;&#30340;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#20197;&#20415;&#21516;&#26102;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;SafeCoder&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09497v1 Announce Type: cross  Abstract: Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#31639;&#27861;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#24182;&#25552;&#20379;&#29420;&#29305;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.09495</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#28508;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Potential of Network-Based Features for Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#31639;&#27861;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#24182;&#25552;&#20379;&#29420;&#29305;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20132;&#26131;&#27450;&#35784;&#32473;&#20225;&#19994;&#21644;&#28040;&#36153;&#32773;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#38590;&#20197;&#36319;&#19978;&#27450;&#35784;&#25112;&#26415;&#30340;&#28436;&#21464;&#65292;&#23548;&#33268;&#39640;&#35823;&#25253;&#29575;&#21644;&#28431;&#25253;&#29575;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#35782;&#21035;&#27450;&#35784;&#27169;&#24335;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#65288;PPR&#65289;&#31639;&#27861;&#36890;&#36807;&#20998;&#26512;&#37329;&#34701;&#36134;&#25143;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#27604;&#36739;&#20256;&#32479;&#29305;&#24449;&#19982;&#28155;&#21152;PPR&#22312;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;PPR&#29305;&#24449;&#25552;&#20379;&#20102;&#29420;&#29305;&#32780;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#20854;&#39640;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#24471;&#20197;&#35777;&#26126;&#12290;&#29305;&#24449;&#31283;&#23450;&#24615;&#20998;&#26512;&#35777;&#23454;&#20102;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09495v1 Announce Type: cross  Abstract: Online transaction fraud presents substantial challenges to businesses and consumers, risking significant financial losses. Conventional rule-based systems struggle to keep pace with evolving fraud tactics, leading to high false positive rates and missed detections. Machine learning techniques offer a promising solution by leveraging historical data to identify fraudulent patterns. This article explores using the personalised PageRank (PPR) algorithm to capture the social dynamics of fraud by analysing relationships between financial accounts. The primary objective is to compare the performance of traditional features with the addition of PPR in fraud detection models. Results indicate that integrating PPR enhances the model's predictive power, surpassing the baseline model. Additionally, the PPR feature provides unique and valuable information, evidenced by its high feature importance score. Feature stability analysis confirms consist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#21644;&#20154;&#31867;&#26159;&#21542;&#33021;&#22815;&#30495;&#27491;&#20132;&#27969;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24515;&#29702;&#34892;&#20026;&#26041;&#27861;&#8221;&#30340;&#22238;&#31572;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#35797;AI&#26159;&#21542;&#23637;&#29616;&#20986;&#20154;&#31867;&#31867;&#20284;&#30340;&#34892;&#20026;&#26469;&#21028;&#26029;&#20854;&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#30495;&#27491;&#20132;&#27969;&#12290;</title><link>https://arxiv.org/abs/2402.09494</link><description>&lt;p&gt;
AI&#21644;&#20154;&#31867;&#33021;&#22815;&#30495;&#27491;&#20132;&#27969;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI and humans genuinely communicate?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#21644;&#20154;&#31867;&#26159;&#21542;&#33021;&#22815;&#30495;&#27491;&#20132;&#27969;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24515;&#29702;&#34892;&#20026;&#26041;&#27861;&#8221;&#30340;&#22238;&#31572;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#35797;AI&#26159;&#21542;&#23637;&#29616;&#20986;&#20154;&#31867;&#31867;&#20284;&#30340;&#34892;&#20026;&#26469;&#21028;&#26029;&#20854;&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#30495;&#27491;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;AI&#21644;&#20154;&#31867;&#26159;&#21542;&#33021;&#22815;&#30495;&#27491;&#20132;&#27969;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#24515;&#29702;&#34892;&#20026;&#26041;&#27861;&#8221;&#30340;&#26041;&#24335;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#26126;&#30830;&#20154;&#31867;&#20132;&#27969;&#25152;&#38656;&#30340;&#24515;&#29702;&#33021;&#21147;&#65307;&#20854;&#27425;&#30830;&#23450;&#27979;&#35797;&#36825;&#20123;&#33021;&#21147;&#30340;&#23454;&#39564;&#33539;&#24335;&#65307;&#26368;&#21518;&#23558;&#36825;&#20123;&#33539;&#24335;&#24212;&#29992;&#20110;&#27979;&#35797;AI&#26159;&#21542;&#23637;&#29616;&#20986;&#30456;&#20851;&#30340;&#34892;&#20026;&#12290;&#22914;&#26524;&#21069;&#20004;&#20010;&#27493;&#39588;&#25104;&#21151;&#23436;&#25104;&#65292;&#24182;&#19988;AI&#22312;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#32467;&#26524;&#65292;&#37027;&#20040;&#36825;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;AI&#21644;&#20154;&#31867;&#33021;&#22815;&#30495;&#27491;&#20132;&#27969;&#30340;&#35777;&#25454;&#12290;&#24515;&#29702;&#34892;&#20026;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#25105;&#20204;&#19981;&#38656;&#35201;&#29702;&#35299;&#40657;&#30418;&#31639;&#27861;&#65288;&#27604;&#22914;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65289;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09494v1 Announce Type: cross  Abstract: Can AI and humans genuinely communicate? In this article, after giving some background and motivating my proposal (sections 1 to 3), I explore a way to answer this question that I call the "mental-behavioral methodology" (sections 4 and 5). This methodology follows the following three steps: First, spell out what mental capacities are sufficient for human communication (as opposed to communication more generally). Second, spell out the experimental paradigms required to test whether a behavior exhibits these capacities. Third, apply or adapt these paradigms to test whether an AI displays the relevant behaviors. If the first two steps are successfully completed, and if the AI passes the tests with human-like results, this constitutes evidence that this AI and humans can genuinely communicate. This mental-behavioral methodology has the advantage that we don't need to understand the workings of black-box algorithms, such as standard deep 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#22312;&#32954;&#30284;&#39044;&#21518;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#25552;&#39640;&#32954;&#30284;&#29983;&#23384;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20840;&#38754;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#20026;&#24739;&#32773;&#21046;&#23450;&#26356;&#22909;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.09476</link><description>&lt;p&gt;
AI-&#21551;&#29992;&#30340;&#32954;&#30284;&#39044;&#21518;
&lt;/p&gt;
&lt;p&gt;
AI-Enabled Lung Cancer Prognosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09476
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#22312;&#32954;&#30284;&#39044;&#21518;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#25552;&#39640;&#32954;&#30284;&#29983;&#23384;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20840;&#38754;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#20026;&#24739;&#32773;&#21046;&#23450;&#26356;&#22909;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09476v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#30740;&#31350; &#25688;&#35201;: &#32954;&#30284;&#26159;&#23548;&#33268;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#22312;2020&#24180;&#20840;&#29699;&#33539;&#22260;&#20869;&#22842;&#36208;&#20102;&#32422;179&#19975;&#20154;&#30340;&#29983;&#21629;&#65292;&#20272;&#35745;&#22312;&#21516;&#19968;&#26102;&#26399;&#20869;&#26377;221&#19975;&#20363;&#26032;&#30149;&#20363;&#34987;&#35786;&#26029;&#20986;&#26469;&#12290;&#20854;&#20013;&#65292;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#26159;&#20027;&#35201;&#20122;&#22411;&#65292;&#20854;&#29305;&#28857;&#26159;&#39044;&#21518;&#26497;&#24046;&#65292;&#20840;&#37096;&#30149;&#31243;&#20116;&#24180;&#30340;&#23384;&#27963;&#29575;&#32422;&#20026;25%&#12290;&#28982;&#32780;&#65292;&#23384;&#27963;&#32467;&#26524;&#26681;&#25454;&#35786;&#26029;&#38454;&#27573;&#21644;&#34892;&#27835;&#30103;&#20171;&#20837;&#30340;&#24046;&#24322;&#32780;&#30456;&#24046;&#24456;&#22823;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#22312;&#32954;&#30284;&#39044;&#21518;&#39046;&#22495;&#24341;&#21457;&#20102;&#38761;&#21629;&#12290;AI&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#24050;&#32463;&#23637;&#31034;&#20102;&#36890;&#36807;&#39640;&#25928;&#20998;&#26512;&#22797;&#26434;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#21644;&#25972;&#21512;&#22810;&#26679;&#20020;&#24202;&#21464;&#37327;&#26469;&#25552;&#39640;&#29983;&#23384;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;AI&#25216;&#26415;&#65292;&#20020;&#24202;&#21307;&#29983;&#21487;&#20197;&#21033;&#29992;&#20840;&#38754;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09476v1 Announce Type: cross  Abstract: Lung cancer is the primary cause of cancer-related mortality, claiming approximately 1.79 million lives globally in 2020, with an estimated 2.21 million new cases diagnosed within the same period. Among these, Non-Small Cell Lung Cancer (NSCLC) is the predominant subtype, characterized by a notably bleak prognosis and low overall survival rate of approximately 25% over five years across all disease stages. However, survival outcomes vary considerably based on the stage at diagnosis and the therapeutic interventions administered. Recent advancements in artificial intelligence (AI) have revolutionized the landscape of lung cancer prognosis. AI-driven methodologies, including machine learning and deep learning algorithms, have shown promise in enhancing survival prediction accuracy by efficiently analyzing complex multi-omics data and integrating diverse clinical variables. By leveraging AI techniques, clinicians can harness comprehensive
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#21464;&#21387;&#22120;&#26041;&#27861;&#35299;&#35835;&#24515;&#29575;&#20449;&#21495;&#65292;&#25552;&#39640;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09474</link><description>&lt;p&gt;
&#35299;&#35835;&#24515;&#29575;&#20449;&#21495;&#65306;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#25216;&#26415;&#30340;&#21487;&#35299;&#37322;&#24615;&#25151;&#39076;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#21464;&#21387;&#22120;&#26041;&#27861;&#35299;&#35835;&#24515;&#29575;&#20449;&#21495;&#65292;&#25552;&#39640;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21487;&#31359;&#25140;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#35774;&#22791;&#30340;&#36828;&#31243;&#24739;&#32773;&#30417;&#27979;&#22312;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#36827;&#34892;&#33258;&#21160;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24212;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#26041;&#27861;&#36827;&#34892;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#65292;&#20294;&#30001;&#20110;&#30446;&#21069;AI&#31639;&#27861;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#34987;&#24191;&#27867;&#25509;&#21463;&#20316;&#20026;&#20020;&#24202;&#35786;&#26029;&#30340;&#21487;&#38752;&#36741;&#21161;&#24037;&#20855;&#12290;&#23588;&#20854;&#38656;&#35201;&#30830;&#23450;ECG&#20449;&#21495;&#20013;&#36129;&#29486;&#20110;&#20934;&#30830;&#35786;&#26029;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#23548;&#32852;ECG&#25968;&#25454;&#35782;&#21035;&#25151;&#39076;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#32593;&#32476;&#65288;ResNet&#65289;&#26041;&#27861;&#20197;&#20316;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09474v1 Announce Type: cross  Abstract: Remote patient monitoring based on wearable single-lead electrocardiogram (ECG) devices has significant potential for enabling the early detection of heart disease, especially in combination with artificial intelligence (AI) approaches for automated heart disease detection. There have been prior studies applying AI approaches based on deep learning for heart disease detection. However, these models are yet to be widely accepted as a reliable aid for clinical diagnostics, in part due to the current black-box perception surrounding many AI algorithms. In particular, there is a need to identify the key features of the ECG signal that contribute toward making an accurate diagnosis, thereby enhancing the interpretability of the model. In the present study, we develop a vision transformer approach to identify atrial fibrillation based on single-lead ECG data. A residual network (ResNet) approach is also developed for comparison with the visi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RLEEGNet&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#33041;&#26426;&#25509;&#21475;&#19982;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#30452;&#35266;&#21709;&#24212;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#21160;&#20316;&#24847;&#35937;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#24120;&#31354;&#27169;&#24335;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#26102;&#36866;&#24212;&#29992;&#25143;&#19981;&#26029;&#21464;&#21270;&#30340;&#38656;&#27714;&#21644;&#24847;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.09465</link><description>&lt;p&gt;
RLEEGNet&#65306;&#23558;&#33041;&#26426;&#25509;&#21475;&#19982;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;&#65292;&#23454;&#29616;&#30452;&#35266;&#21709;&#24212;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#21160;&#20316;&#24847;&#35937;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
RLEEGNet: Integrating Brain-Computer Interfaces with Adaptive AI for Intuitive Responsiveness and High-Accuracy Motor Imagery Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RLEEGNet&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#33041;&#26426;&#25509;&#21475;&#19982;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#30452;&#35266;&#21709;&#24212;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#21160;&#20316;&#24847;&#35937;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#24120;&#31354;&#27169;&#24335;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#26102;&#36866;&#24212;&#29992;&#25143;&#19981;&#26029;&#21464;&#21270;&#30340;&#38656;&#27714;&#21644;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#20551;&#32930;&#25511;&#21046;&#26041;&#27861;&#21463;&#20256;&#32479;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#32570;&#20047;&#23454;&#26102;&#36866;&#24212;&#24615;&#21644;&#30452;&#35266;&#21709;&#24212;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#38754;&#23545;&#35748;&#30693;&#29366;&#24577;&#21644;&#36816;&#21160;&#24847;&#22270;&#22810;&#26679;&#30340;&#36741;&#21161;&#25216;&#26415;&#26102;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24120;&#31354;&#27169;&#24335;&#65288;CSP&#65289;&#36827;&#34892;&#22810;&#31867;&#21160;&#20316;&#24847;&#20687;&#65288;MI&#65289;&#20998;&#31867;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;"&#19968;&#23545;&#20854;&#20313;&#65288;OVR&#65289;"&#30340;&#26041;&#24335;&#36827;&#34892;&#20998;&#31867;&#12290;&#38543;&#21518;&#30340;&#8220;csp&#31354;&#38388;&#8221;&#36716;&#25442;&#20445;&#30041;&#20102;EEG&#20449;&#21495;&#30340;&#26102;&#38388;&#32500;&#24230;&#65292;&#23545;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;DQN&#19982;1D-CNN-LSTM&#32467;&#26500;&#30340;&#38598;&#25104;&#22312;&#23454;&#26102;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#20102;&#31995;&#32479;&#23545;&#29992;&#25143;&#19981;&#26029;&#21464;&#21270;&#30340;&#38656;&#27714;&#21644;&#24847;&#22270;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09465v1 Announce Type: cross  Abstract: Current approaches to prosthetic control are limited by their reliance on traditional methods, which lack real-time adaptability and intuitive responsiveness. These limitations are particularly pronounced in assistive technologies designed for individuals with diverse cognitive states and motor intentions. In this paper, we introduce a framework that leverages Reinforcement Learning (RL) with Deep Q-Networks (DQN) for classification tasks. Additionally, we present a preprocessing technique using the Common Spatial Pattern (CSP) for multiclass motor imagery (MI) classification in a One-Versus-The-Rest (OVR) manner. The subsequent 'csp space' transformation retains the temporal dimension of EEG signals, crucial for extracting discriminative features. The integration of DQN with a 1D-CNN-LSTM architecture optimizes the decision-making process in real-time, thereby enhancing the system's adaptability to the user's evolving needs and intent
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26500;&#24314;&#21487;&#36127;&#25285;&#30340;&#23450;&#21046;IMU&#26080;&#32447;&#21487;&#31359;&#25140;&#31995;&#32479;&#65292;&#22312;&#20154;&#20307;&#36816;&#21160;&#20998;&#26512;&#20013;&#23454;&#29616;&#23545;&#36523;&#20307;&#37096;&#20301;&#23450;&#21521;&#36319;&#36394;&#21644;3D&#36816;&#21160;&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.09459</link><description>&lt;p&gt;
&#38754;&#21521;&#20154;&#20307;&#37096;&#20301;&#23450;&#21521;&#36319;&#36394;&#21644;3D&#36816;&#21160;&#21487;&#35270;&#21270;&#30340;&#23450;&#21046;IMU&#26080;&#32447;&#21487;&#31359;&#25140;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Custom IMU-Based Wearable System for Robust 2.4 GHz Wireless Human Body Parts Orientation Tracking and 3D Movement Visualization on an Avatar
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09459
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26500;&#24314;&#21487;&#36127;&#25285;&#30340;&#23450;&#21046;IMU&#26080;&#32447;&#21487;&#31359;&#25140;&#31995;&#32479;&#65292;&#22312;&#20154;&#20307;&#36816;&#21160;&#20998;&#26512;&#20013;&#23454;&#29616;&#23545;&#36523;&#20307;&#37096;&#20301;&#23450;&#21521;&#36319;&#36394;&#21644;3D&#36816;&#21160;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#30830;&#35748;&#20102;&#20351;&#29992;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#30340;&#31995;&#32479;&#23545;&#20110;&#20154;&#20307;&#36816;&#21160;&#20998;&#26512;&#30340;&#36866;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#39640;&#31471;&#30340;&#21830;&#19994;&#21270;IMU&#35299;&#20915;&#26041;&#26696;&#20215;&#26684;&#26114;&#36149;&#19988;&#22797;&#26434;&#65292;&#26080;&#27861;&#26222;&#21450;&#22312;&#24191;&#22823;&#28508;&#22312;&#29992;&#25143;&#20013;&#30340;&#20351;&#29992;&#12290;&#24066;&#22330;&#19978;&#20986;&#29616;&#20102;&#19968;&#20123;&#21151;&#33021;&#36739;&#23569;&#30340;&#20302;&#31471;&#21830;&#19994;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#35797;&#22270;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38656;&#35201;&#20811;&#26381;&#30340;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;&#21307;&#30103;&#21644;&#36816;&#21160;&#24212;&#29992;&#39046;&#22495;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#31185;&#23398;&#35770;&#25991;&#20351;&#29992;&#30340;&#26159;&#38750;&#21830;&#19994;&#21270;&#30340;&#12289;&#33258;&#21046;&#30340;IMU&#31995;&#32479;&#12290;&#23613;&#31649;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#20419;&#36827;&#36825;&#39033;&#25216;&#26415;&#30340;&#26222;&#21450;&#21270;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#21151;&#33021;&#26356;&#20026;&#26377;&#38480;&#65292;&#24182;&#19988;&#22914;&#20309;&#35774;&#35745;&#21644;&#26500;&#24314;&#23427;&#20204;&#30340;&#25551;&#36848;&#22312;&#25991;&#29486;&#20013;&#20173;&#28982;&#31232;&#32570;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#65306;&#65288;1&#65289;&#35777;&#26126;&#26500;&#24314;&#19968;&#31181;&#21487;&#36127;&#25285;&#30340;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#26159;&#21487;&#34892;&#30340;&#65292;&#26088;&#22312;&#21516;&#26102;&#36861;&#36394;&#22810;&#20010;&#20154;&#20307;&#37096;&#20301;&#30340;&#23450;&#21521;&#21644;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09459v1 Announce Type: cross  Abstract: Recent studies confirm the applicability of Inertial Measurement Unit (IMU)-based systems for human motion analysis. Notwithstanding, high-end IMU-based commercial solutions are yet too expensive and complex to democratize their use among a wide range of potential users. Less featured entry-level commercial solutions are being introduced in the market, trying to fill this gap, but still present some limitations that need to be overcome. At the same time, there is a growing number of scientific papers using not commercial, but custom do-it-yourself IMU-based systems in medical and sports applications. Even though these solutions can help to popularize the use of this technology, they have more limited features and the description on how to design and build them from scratch is yet too scarce in the literature. The aim of this work is two-fold: (1) Proving the feasibility of building an affordable custom solution aimed at simultaneous mu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#21338;&#24328;&#20013;&#36827;&#34892;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.09456</link><description>&lt;p&gt;
&#26410;&#30693;&#21338;&#24328;&#20013;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#29992;&#20110;&#26080;&#36951;&#25022;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimistic Thompson Sampling for No-Regret Learning in Unknown Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#21338;&#24328;&#20013;&#36827;&#34892;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28041;&#21450;&#22810;&#20010;&#20915;&#31574;&#32773;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#21487;&#20197;&#24314;&#27169;&#20026;&#19968;&#20010;&#20855;&#26377;&#37096;&#20998;&#35266;&#27979;&#30340;&#26410;&#30693;&#21338;&#24328;&#12290;&#20026;&#20102;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#21644;&#22810;&#26426;&#26500;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#27748;&#26222;&#26862;&#25277;&#26679;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22914;&#20132;&#36890;&#36335;&#30001;&#21644;&#38647;&#36798;&#24863;&#30693;&#20013;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#19982;&#22522;&#20934;&#31639;&#27861;&#30456;&#27604;&#65292;&#20943;&#23569;&#20102;&#21313;&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#22870;&#21169;&#32467;&#26500;&#26377;&#19968;&#23450;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36951;&#25022;&#30028;&#38480;&#20165;&#23545;&#24635;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21576;&#23545;&#25968;&#20381;&#36182;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21644;&#39046;&#22495;&#20869;&#29616;&#26377;&#30340;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26159;&#19968;&#39033;&#26032;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09456v1 Announce Type: cross  Abstract: Many real-world problems involving multiple decision-makers can be modeled as an unknown game characterized by partial observations. Addressing the challenges posed by partial information and the curse of multi-agency, we developed Thompson sampling-type algorithms, leveraging information about opponent's action and reward structures. Our approach significantly reduces experimental budgets, achieving a more than tenfold reduction compared to baseline algorithms in practical applications like traffic routing and radar sensing. We demonstrate that, under certain assumptions about the reward structure, the regret bound exhibits merely a logarithmic dependence on the total action space size, effectively mitigating the curse of multi-agency. Additionally, this research introduces the Optimism-then-NoRegret framework, a novel contribution that integrates both our proposed methodologies and existing algorithms in the field.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(WGAN)&#26469;&#25552;&#39640;EEG&#20449;&#21495;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290; WGAN&#22312;BCI2000&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#21644;&#27979;&#37327;&#24471;&#20998;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;EEG&#20449;&#21495;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.09453</link><description>&lt;p&gt;
&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25552;&#39640;EEG&#20449;&#21495;&#20998;&#31867;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving EEG Signal Classification Accuracy Using Wasserstein Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(WGAN)&#26469;&#25552;&#39640;EEG&#20449;&#21495;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290; WGAN&#22312;BCI2000&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#21644;&#27979;&#37327;&#24471;&#20998;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;EEG&#20449;&#21495;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24687;&#29983;&#24577;&#65288;EEG&#65289;&#22312;&#35760;&#24405;&#33041;&#37096;&#27963;&#21160;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#23545;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;EEG&#20449;&#21495;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#39640;&#24230;&#21464;&#24322;&#24615;&#32473;&#21019;&#24314;&#21487;&#38752;&#30340;BCI&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;WGAN&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;WGAN&#26159;&#22312;BCI2000&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;45&#20010;&#20010;&#20307;&#30340;&#32422;1500&#20010;EEG&#35760;&#24405;&#21644;64&#20010;&#36890;&#36947;&#12290;&#36890;&#36807;&#19977;&#20010;&#20998;&#31867;&#22120;&#35780;&#20272;&#29983;&#25104;&#30340;EEG&#20449;&#21495;&#65292;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;&#20351;&#29992;Frechet Inception Distance&#65288;FID&#65289;&#27979;&#37327;&#30340;&#29983;&#25104;&#20449;&#21495;&#36136;&#37327;&#20026;1.345&#65288;&#30529;&#30524;&#65289;&#21644;11.565&#65288;&#38381;&#30524;&#65289;&#12290;&#21363;&#20351;&#27809;&#26377;&#39057;&#35889;&#25110;&#31354;&#38388;&#25439;&#22833;&#39033;&#65292;&#25105;&#20204;&#30340;WGAN&#27169;&#22411;&#20173;&#33021;&#27169;&#25311;&#20986;&#39057;&#35889;&#21644;&#31354;&#38388;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09453v1 Announce Type: cross  Abstract: Electroencephalography (EEG) plays a vital role in recording brain activities and is integral to the development of brain-computer interface (BCI) technologies. However, the limited availability and high variability of EEG signals present substantial challenges in creating reliable BCIs. To address this issue, we propose a practical solution drawing on the latest developments in deep learning and Wasserstein Generative Adversarial Network (WGAN). The WGAN was trained on the BCI2000 dataset, consisting of around 1500 EEG recordings and 64 channels from 45 individuals. The generated EEG signals were evaluated via three classifiers yielding improved average accuracies. The quality of generated signals measured using Frechet Inception Distance (FID) yielded scores of 1.345 and 11.565 for eyes-open and closed respectively. Even without a spectral or spatial loss term, our WGAN model was able to emulate the spectral and spatial properties of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09450</link><description>&lt;p&gt;
&#24341;&#23548;&#36974;&#34109;&#34920;&#31034;&#23398;&#20064;&#20197;&#25429;&#25417;&#24515;&#30005;&#22270;&#30340;&#26102;&#31354;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#24191;&#27867;&#29992;&#20316;&#30417;&#27979;&#24515;&#33039;&#36215;&#28304;&#30340;&#30005;&#20449;&#21495;&#30340;&#35786;&#26029;&#24037;&#20855;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#20449;&#21495;&#36827;&#34892;&#21508;&#31181;&#30142;&#30149;&#31579;&#26597;&#30340;&#24212;&#29992;&#19978;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#30142;&#30149;&#31579;&#26597;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;ECG&#25968;&#25454;&#26377;&#38480;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23454;&#29616;&#36890;&#29992;&#34920;&#31034;&#26159;&#20811;&#26381;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#24120;&#29992;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22312;ECG&#25968;&#25454;&#19978;&#32431;&#31929;&#24212;&#29992;SSL&#65292;&#32780;&#19981;&#32771;&#34385;ECG&#20449;&#21495;&#22266;&#26377;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ST-MEM&#65288;&#26102;&#31354;&#36974;&#34109;&#24515;&#30005;&#22270;&#24314;&#27169;&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;12&#23548;&#32852;ECG&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;ST-MEM&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;SSL&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09450v1 Announce Type: cross  Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening disease is challenging in that labeled ECG data are limited. Achieving general representation through self-supervised learning (SSL) is a well-known approach to overcome the scarcity of labeled data; however, a naive application of SSL to ECG data, without considering the spatial-temporal relationships inherent in ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM (Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM outperforms other SSL baseline methods in various experimental settings for arrhythmia classification tasks. Mo
&lt;/p&gt;</description></item><item><title>&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09448</link><description>&lt;p&gt;
&#26222;&#36890;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Conventional and Tripolar EEG for High-Performance Reach-to-Grasp BCI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09448
&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#25552;&#21319;&#36816;&#21160;&#38556;&#30861;&#20010;&#20307;&#30340;BCI&#24212;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#26159;&#35299;&#35835;&#21644;&#35299;&#30721;&#21508;&#31181;&#25235;&#25569;&#21160;&#20316;&#65292;&#22914;&#21147;&#25569;&#21644;&#31934;&#30830;&#25569;&#25345;&#12290;&#30446;&#26631;&#26159;&#30830;&#23450;&#21738;&#31181;EEG&#25216;&#26415;&#22312;&#22788;&#29702;&#21644;&#32763;&#35793;&#19982;&#25235;&#25569;&#30456;&#20851;&#30340;&#33041;&#30005;&#20449;&#21495;&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#12290;&#30740;&#31350;&#28041;&#21450;&#23545;&#21313;&#21517;&#20581;&#24247;&#21442;&#19982;&#32773;&#36827;&#34892;&#23454;&#39564;&#65292;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25569;&#25345;&#36816;&#21160;&#65306;&#21147;&#25569;&#21644;&#31934;&#30830;&#25569;&#25345;&#65292;&#26080;&#36816;&#21160;&#26465;&#20214;&#20316;&#20026;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#35299;&#30721;&#25235;&#25569;&#21160;&#20316;&#26041;&#38754;&#23545;EEG&#21644;&#19977;&#26497;EEG&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;&#35813;&#27604;&#36739;&#28085;&#30422;&#20102;&#20960;&#20010;&#20851;&#38190;&#21442;&#25968;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#12289;&#36890;&#36807;&#21151;&#33021;&#36830;&#25509;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#28041;&#21450;&#20174;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09448v1 Announce Type: cross  Abstract: This study aims to enhance BCI applications for individuals with motor impairments by comparing the effectiveness of tripolar EEG (tEEG) with conventional EEG. The focus is on interpreting and decoding various grasping movements, such as power grasp and precision grasp. The goal is to determine which EEG technology is more effective in processing and translating grasp related neural signals. The approach involved experimenting on ten healthy participants who performed two distinct grasp movements: power grasp and precision grasp, with a no movement condition serving as the baseline. Our research presents a thorough comparison between EEG and tEEG in decoding grasping movements. This comparison spans several key parameters, including signal to noise ratio (SNR), spatial resolution via functional connectivity, ERPs, and wavelet time frequency analysis. Additionally, our study involved extracting and analyzing statistical features from th
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#23567;&#27874;&#20998;&#26512;&#25216;&#26415;&#23545;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#35299;&#30721;&#65292;&#25104;&#21151;&#21306;&#20998;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#25235;&#25569;&#31867;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23567;&#27874;&#29305;&#24449;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#25235;&#25569;&#21306;&#20998;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09447</link><description>&lt;p&gt;
&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#23567;&#27874;&#20998;&#26512;&#21306;&#20998;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#25235;&#25569;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
Wavelet Analysis of Noninvasive EEG Signals Discriminates Complex and Natural Grasp Types
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#23567;&#27874;&#20998;&#26512;&#25216;&#26415;&#23545;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#35299;&#30721;&#65292;&#25104;&#21151;&#21306;&#20998;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#25235;&#25569;&#31867;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23567;&#27874;&#29305;&#24449;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#25235;&#25569;&#21306;&#20998;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#36827;&#34892;&#35299;&#30721;&#65292;&#20026;&#28789;&#24039;&#30340;&#31070;&#32463;&#20551;&#32930;&#24320;&#21457;&#21644;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#24212;&#29992;&#26469;&#21306;&#20998;&#25163;&#37096;&#25235;&#25569;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#19987;&#27880;&#20110;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;BCI&#24179;&#21488;&#21644;&#23567;&#27874;&#20449;&#21495;&#22788;&#29702;&#65292;&#21306;&#20998;&#20004;&#31181;&#22797;&#26434;&#30340;&#33258;&#28982;&#21147;&#37327;&#21644;&#31934;&#30830;&#25235;&#25569;&#31867;&#22411;&#20197;&#21450;&#19968;&#31181;&#20013;&#31435;&#26465;&#20214;&#20316;&#20026;&#26080;&#36816;&#21160;&#26465;&#20214;&#12290;&#23567;&#27874;&#20998;&#26512;&#28041;&#21450;&#20174;&#23567;&#27874;&#33021;&#37327;&#31995;&#25968;&#29983;&#25104;&#26102;&#38388;&#39057;&#29575;&#21644;&#25299;&#25169;&#22270;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#26032;&#22411;&#23567;&#27874;&#29305;&#24449;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#24179;&#22343;&#20934;&#30830;&#29575;&#65306;&#22810;&#31867;&#21035;&#20026;85.16%&#65292;&#26080;&#36816;&#21160; vs &#21147;&#37327;&#20026;95.37%&#65292;&#26080;&#36816;&#21160; vs &#31934;&#30830;&#20026;95.40%&#65292;&#21147;&#37327; vs &#31934;&#30830;&#20026;88.07%&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#29305;&#24449;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#25235;&#25569;&#21306;&#20998;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30740;&#31350;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#25490;&#21015;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09447v1 Announce Type: cross  Abstract: This research aims to decode hand grasps from Electroencephalograms (EEGs) for dexterous neuroprosthetic development and Brain-Computer Interface (BCI) applications, especially for patients with motor disorders. Particularly, it focuses on distinguishing two complex natural power and precision grasps in addition to a neutral condition as a no-movement condition using a new EEG-based BCI platform and wavelet signal processing. Wavelet analysis involved generating time-frequency and topographic maps from wavelet power coefficients. Then, by using machine learning techniques with novel wavelet features, we achieved high average accuracies: 85.16% for multiclass, 95.37% for No-Movement vs Power, 95.40% for No-Movement vs Precision, and 88.07% for Power vs Precision, demonstrating the effectiveness of these features in EEG-based grasp differentiation. In contrast to previous studies, a critical part of our study was permutation feature impo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#30740;&#31350;&#35777;&#26126;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;&#21487;&#20197;&#25913;&#36827;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#65292;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09445</link><description>&lt;p&gt;
iMove: &#25506;&#32034;&#29992;&#20110;&#20581;&#36523;&#27963;&#21160;&#35782;&#21035;&#30340;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09445
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#30740;&#31350;&#35777;&#26126;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;&#21487;&#20197;&#25913;&#36827;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#65292;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21644;&#31934;&#30830;&#30340;&#20581;&#36523;&#27963;&#21160;&#35782;&#21035;&#23545;&#20110;&#20419;&#36827;&#20581;&#24247;&#29983;&#27963;&#26041;&#24335;&#21644;&#20010;&#24615;&#21270;&#39044;&#38450;&#24615;&#21307;&#30103;&#20855;&#26377;&#30410;&#22788;&#12290;&#34429;&#28982;IMU&#30446;&#21069;&#26159;&#20027;&#35201;&#30340;&#20581;&#36523;&#36861;&#36394;&#27169;&#24335;&#65292;&#20294;&#36890;&#36807;iMove&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#29289;&#38459;&#25239;&#21487;&#20197;&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#21892;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#65292;&#21253;&#25324;&#21313;&#20010;&#21463;&#35797;&#32773;&#22312;&#20116;&#22825;&#20869;&#36827;&#34892;&#30340;&#20845;&#31181;&#19978;&#36523;&#20581;&#36523;&#27963;&#21160;&#65292;&#20197;&#25910;&#38598;&#26469;&#33258;&#20004;&#21482;&#25163;&#33109;&#30340;&#29983;&#29289;&#38459;&#25239;&#21644;&#24038;&#25163;&#33109;IMU&#30340;&#21516;&#27493;&#25968;&#25454;&#12290;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21033;&#29992;&#20004;&#31181;&#27169;&#24577;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#20165;&#22522;&#20110;IMU&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20854;&#20013;&#29983;&#29289;&#38459;&#25239;&#21482;&#22312;&#35757;&#32451;&#38454;&#27573;&#38656;&#35201;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#36755;&#20837;&#21333;&#20010;IMU&#30340;&#24179;&#22343;&#23439;F1&#20998;&#25968;&#25552;&#39640;&#20102;3.22&#65285;&#65292;&#36798;&#21040;84.71&#65285;&#65292;&#32780;IMU&#22522;&#32447;&#27169;&#22411;&#20026;81.49&#65285;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#29983;&#29289;&#38459;&#25239;&#22914;&#20309;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09445v1 Announce Type: cross  Abstract: Automatic and precise fitness activity recognition can be beneficial in aspects from promoting a healthy lifestyle to personalized preventative healthcare. While IMUs are currently the prominent fitness tracking modality, through iMove, we show bio-impedence can help improve IMU-based fitness tracking through sensor fusion and contrastive learning.To evaluate our methods, we conducted an experiment including six upper body fitness activities performed by ten subjects over five days to collect synchronized data from bio-impedance across two wrists and IMU on the left wrist.The contrastive learning framework uses the two modalities to train a better IMU-only classification model, where bio-impedance is only required at the training phase, by which the average Macro F1 score with the input of a single IMU was improved by 3.22 \% reaching 84.71 \% compared to the 81.49 \% of the IMU baseline model. We have also shown how bio-impedance can 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAMFN&#30340;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#20998;&#21035;&#24314;&#27169;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38899;&#39057;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35780;&#20998;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09444</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multimodal Action Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAMFN&#30340;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#20998;&#21035;&#24314;&#27169;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38899;&#39057;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35780;&#20998;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#36136;&#37327;&#35780;&#20272;&#65288;AQA&#65289;&#26159;&#35780;&#20272;&#21160;&#20316;&#25191;&#34892;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20165;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#24573;&#35270;&#20102;&#38899;&#39057;&#20449;&#24687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;AQA&#39640;&#24230;&#20381;&#36182;&#35270;&#35273;&#20449;&#24687;&#65292;&#20294;&#38899;&#39057;&#20063;&#26159;&#25552;&#39640;&#35780;&#20998;&#22238;&#24402;&#20934;&#30830;&#24615;&#30340;&#26377;&#29992;&#34917;&#20805;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#32972;&#26223;&#38899;&#20048;&#30340;&#36816;&#21160;&#39033;&#30446;&#20013;&#65292;&#22914;&#33457;&#26679;&#28369;&#20912;&#21644;&#38901;&#24459;&#20307;&#25805;&#12290;&#20026;&#20102;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;AQA&#65292;&#21363;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65288;PAMFN&#65289;&#65292;&#23427;&#20998;&#21035;&#23545;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#27169;&#24577;&#29305;&#23450;&#20998;&#25903;&#21644;&#19968;&#20010;&#28151;&#21512;&#27169;&#24577;&#20998;&#25903;&#32452;&#25104;&#65292;&#29420;&#31435;&#22320;&#25506;&#32034;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#28176;&#36827;&#22320;&#32858;&#21512;&#26469;&#33258;&#27169;&#24577;&#29305;&#23450;&#20998;&#25903;&#30340;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09444v1 Announce Type: cross  Abstract: Action quality assessment (AQA) is to assess how well an action is performed. Previous works perform modelling by only the use of visual information, ignoring audio information. We argue that although AQA is highly dependent on visual information, the audio is useful complementary information for improving the score regression accuracy, especially for sports with background music, such as figure skating and rhythmic gymnastics. To leverage multimodal information for AQA, i.e., RGB, optical flow and audio information, we propose a Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models modality-specific information and mixed-modality information. Our model consists of with three modality-specific branches that independently explore modality-specific information and a mixed-modality branch that progressively aggregates the modality-specific information from the modality-specific branches. To build the bridge between
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32508;&#36848;&#20102;&#20351;&#29992; EEG &#20449;&#21495;&#36827;&#34892;&#30130;&#21171;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#22522;&#20110; EEG &#25968;&#25454;&#39044;&#27979;&#20010;&#20307;&#30130;&#21171;&#27700;&#24179;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09443</link><description>&lt;p&gt;
&#39044;&#27979;&#30130;&#21171;&#30340; EEG &#31639;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of algorithms for predicting fatigue using EEG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09443
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32508;&#36848;&#20102;&#20351;&#29992; EEG &#20449;&#21495;&#36827;&#34892;&#30130;&#21171;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#22522;&#20110; EEG &#25968;&#25454;&#39044;&#27979;&#20010;&#20307;&#30130;&#21171;&#27700;&#24179;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30130;&#21171;&#26816;&#27979;&#23545;&#20110;&#25552;&#39640;&#20132;&#36890;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#23433;&#20840;&#24615;&#12289;&#29983;&#20135;&#21147;&#21644;&#31119;&#31049;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#31185;&#23398;&#35770;&#25991;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26816;&#27979;&#29983;&#29702;&#30130;&#21171;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#20351;&#29992;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#19981;&#21516;&#31639;&#27861;&#22312;&#22522;&#20110; EEG &#25968;&#25454;&#39044;&#27979;&#20010;&#20307;&#30130;&#21171;&#27700;&#24179;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09443v1 Announce Type: cross  Abstract: Fatigue detection is of paramount importance in enhancing safety, productivity, and well-being across diverse domains, including transportation, healthcare, and industry. This scientific paper presents a comprehensive investigation into the application of machine learning algorithms for the detection of physiological fatigue using Electroencephalogram (EEG) signals. The primary objective of this study was to assess the efficacy of various algorithms in predicting an individual's level of fatigue based on EEG data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20351;&#29992;TENG&#20316;&#20026;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#31616;&#21333;&#32467;&#26500;&#21644;&#39640;&#30636;&#26102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09442</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Progress in artificial intelligence applications based on the combination of self-driven sensors and deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20351;&#29992;TENG&#20316;&#20026;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#31616;&#21333;&#32467;&#26500;&#21644;&#39640;&#30636;&#26102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#22914;&#20309;&#24320;&#21457;&#20855;&#26377;&#21487;&#25345;&#32493;&#30005;&#28304;&#20379;&#24212;&#12289;&#26131;&#20110;&#37096;&#32626;&#21644;&#28789;&#27963;&#20351;&#29992;&#30340;&#26234;&#33021;&#20256;&#24863;&#22120;&#31995;&#32479;&#24050;&#25104;&#20026;&#19968;&#20010;&#38590;&#39064;&#12290;&#20256;&#32479;&#30340;&#30005;&#28304;&#20379;&#24212;&#23384;&#22312;&#39057;&#32321;&#26356;&#25442;&#25110;&#20351;&#29992;&#26102;&#20805;&#30005;&#31561;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#21457;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#32858;&#22235;&#27679;&#20057;&#28911;&#65288;PTFE&#65289;&#21644;&#38109;&#31636;&#65288;AI&#65289;&#21046;&#22791;&#25509;&#35302;-&#20998;&#31163;&#25705;&#25830;&#32435;&#31859;&#21457;&#30005;&#26426;&#65288;TENG&#65289;&#26469;&#25910;&#38598;&#20154;&#20307;&#36816;&#21160;&#33021;&#37327;&#65292;&#26681;&#25454;&#36755;&#20986;&#30005;&#20449;&#21495;&#30340;&#21464;&#21270;&#26469;&#30417;&#27979;&#20154;&#20307;&#36816;&#21160;&#23039;&#21183;&#12290; 2012&#24180;&#65292;&#29579;&#20013;&#26519;&#38498;&#22763;&#21450;&#20854;&#22242;&#38431;&#21457;&#26126;&#20102;&#25705;&#25830;&#30005;&#32435;&#31859;&#21457;&#30005;&#26426;&#65288;TENG&#65289;&#65292;&#23427;&#21033;&#29992;&#26368;&#22823;&#20301;&#31227;&#30005;&#27969;&#20316;&#20026;&#39537;&#21160;&#21147;&#65292;&#23558;&#26426;&#26800;&#21050;&#28608;&#30452;&#25509;&#36716;&#25442;&#20026;&#30005;&#20449;&#21495;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#20316;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#12290;TENG&#20256;&#24863;&#22120;&#20855;&#26377;&#32467;&#26500;&#31616;&#21333;&#21644;&#30636;&#26102;&#24615;&#39640;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09442v1 Announce Type: cross  Abstract: In the era of Internet of Things, how to develop a smart sensor system with sustainable power supply, easy deployment and flexible use has become a difficult problem to be solved. The traditional power supply has problems such as frequent replacement or charging when in use, which limits the development of wearable devices. The contact-to-separate friction nanogenerator (TENG) was prepared by using polychotomy thy lene (PTFE) and aluminum (AI) foils. Human motion energy was collected by human body arrangement, and human motion posture was monitored according to the changes of output electrical signals. In 2012, Academician Wang Zhong lin and his team invented the triboelectric nanogenerator (TENG), which uses Maxwell displacement current as a driving force to directly convert mechanical stimuli into electrical signals, so it can be used as a self-driven sensor. Teng-based sensors have the advantages of simple structure and high instant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#27668;&#34892;&#20026;&#20851;&#32852;&#25366;&#25496;&#30340;&#23478;&#24237;&#30701;&#26399;&#33021;&#32791;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#29575;&#20851;&#32852;&#27169;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.09433</link><description>&lt;p&gt;
&#22522;&#20110;&#30005;&#27668;&#34892;&#20026;&#20851;&#32852;&#25366;&#25496;&#30340;&#23478;&#24237;&#30701;&#26399;&#33021;&#32791;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Electrical Behavior Association Mining for Household ShortTerm Energy Consumption Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#27668;&#34892;&#20026;&#20851;&#32852;&#25366;&#25496;&#30340;&#23478;&#24237;&#30701;&#26399;&#33021;&#32791;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#29575;&#20851;&#32852;&#27169;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#23478;&#24237;&#30701;&#26399;&#33021;&#32791;&#39044;&#27979;(STECF)&#23545;&#23478;&#24237;&#33021;&#28304;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20010;&#21035;&#20303;&#25143;&#30340;&#39640;&#24230;&#38543;&#26426;&#34892;&#20026;&#65292;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#26085;&#21069;&#31243;&#24230;&#30340;STECF&#20934;&#30830;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;STECF&#26041;&#27861;&#65292;&#21033;&#29992;&#30005;&#27668;&#34892;&#20026;&#20013;&#30340;&#20851;&#32852;&#25366;&#25496;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#21270;&#30340;&#20851;&#32852;&#37327;&#21270;&#21644;&#21457;&#29616;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#29983;&#25104;&#20851;&#32852;&#32676;&#38598;&#12290;&#28982;&#21518;&#65292;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(CNN-GRU)&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#25506;&#32034;&#26102;&#38388;&#30456;&#20851;&#24615;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;STECF&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09433v1 Announce Type: cross  Abstract: Accurate household short-term energy consumption forecasting (STECF) is crucial for home energy management, but it is technically challenging, due to highly random behaviors of individual residential users. To improve the accuracy of STECF on a day-ahead scale, this paper proposes an novel STECF methodology that leverages association mining in electrical behaviors. First, a probabilistic association quantifying and discovering method is proposed to model the pairwise behaviors association and generate associated clusters. Then, a convolutional neural network-gated recurrent unit (CNN-GRU) based forecasting is provided to explore the temporal correlation and enhance accuracy. The testing results demonstrate that this methodology yields a significant enhancement in the STECF.
&lt;/p&gt;</description></item><item><title>DoorINet&#26159;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09427</link><description>&lt;p&gt;
DoorINet: &#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DoorINet: A Deep-Learning Inertial Framework for Door-Mounted IoT Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09427
&lt;/p&gt;
&lt;p&gt;
DoorINet&#26159;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29289;&#32852;&#32593;&#24212;&#29992;&#20351;&#29992;&#20302;&#25104;&#26412;&#30340;&#24494;&#22411;&#30005;&#21160;&#26426;&#26800;&#24815;&#24615;&#20256;&#24863;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#20219;&#21153;&#26159;&#26041;&#21521;&#20272;&#35745;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#20219;&#21153;&#65292;&#24212;&#29992;&#23039;&#24577;&#21644;&#33322;&#21521;&#21442;&#32771;&#31995;&#32479;&#31639;&#27861;&#12290;&#21033;&#29992;&#38464;&#34746;&#20202;&#35835;&#25968;&#65292;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#35835;&#25968;&#26356;&#26032;&#23039;&#24577;&#35282;&#24230;&#65292;&#21033;&#29992;&#30913;&#21147;&#35745;&#27979;&#37327;&#26356;&#26032;&#33322;&#21521;&#35282;&#24230;&#12290;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#65292;&#30913;&#21147;&#35745;&#21463;&#21040;&#24178;&#25200;&#65292;&#20250;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#24433;&#21709;&#21040;&#20272;&#35745;&#33322;&#21521;&#35282;&#24230;&#30340;&#24212;&#29992;&#65292;&#27604;&#22914;&#25214;&#21040;&#34915;&#26588;&#25110;&#20912;&#31665;&#38376;&#30340;&#33322;&#21521;&#35282;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DoorINet&#65292;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#20302;&#25104;&#26412;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#19968;&#20010;&#21253;&#21547;391&#20998;&#38047;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#27979;&#37327;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09427v1 Announce Type: cross  Abstract: Many Internet of Things applications utilize low-cost, micro, electro-mechanical inertial sensors. A common task is orientation estimation. To tackle such a task, attitude and heading reference system algorithms are applied. Relying on the gyroscope readings, the accelerometer readings are used to update the attitude angles, and magnetometer measurements are utilized to update the heading angle. In indoor environments, magnetometers suffer from interference that degrades their performance. This mainly influences applications focused on estimating the heading angle like finding the heading angle of a closet or fridge door. To circumvent such situations, we propose DoorINet, an end-to-end deep-learning framework to calculate the heading angle from door-mounted, low-cost inertial sensors without using magnetometers. To evaluate our approach, we record a unique dataset containing 391 minutes of accelerometer and gyroscope measurements and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#25968;&#23398;&#38472;&#36848;&#30340;&#35299;&#37322;&#38382;&#39064;&#20197;&#21450;&#36890;&#36807;&#20351;&#29992;&#19981;&#21487;&#33021;&#30340;&#21487;&#33021;&#19990;&#30028;&#35299;&#20915;&#20102;&#25968;&#23398;&#35299;&#37322;&#20013;&#30340;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.09413</link><description>&lt;p&gt;
&#25968;&#23398;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Mathematical Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#25968;&#23398;&#38472;&#36848;&#30340;&#35299;&#37322;&#38382;&#39064;&#20197;&#21450;&#36890;&#36807;&#20351;&#29992;&#19981;&#21487;&#33021;&#30340;&#21487;&#33021;&#19990;&#30028;&#35299;&#20915;&#20102;&#25968;&#23398;&#35299;&#37322;&#20013;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#20986;&#20102;&#25968;&#23398;&#38472;&#36848;&#30340;&#35299;&#37322;&#23450;&#20041;&#20197;&#21450;&#19968;&#20010;&#35299;&#37322;&#20309;&#26102;&#27604;&#21478;&#19968;&#20010;&#26356;&#22909;&#30340;&#23450;&#20041;&#12290;&#30001;&#20110;&#25152;&#26377;&#25968;&#23398;&#20107;&#23454;&#24517;&#39035;&#22312;&#25152;&#26377;&#22240;&#26524;&#27169;&#22411;&#20013;&#37117;&#26159;&#30495;&#23454;&#30340;&#65292;&#22240;&#27492;&#30001;&#19968;&#20010;&#20195;&#29702;&#32773;&#25152;&#30693;&#65292;&#25968;&#23398;&#20107;&#23454;&#19981;&#33021;&#26159;&#35299;&#37322;&#30340;&#19968;&#37096;&#20998;&#65288;&#25353;&#29031;&#26631;&#20934;&#30340;&#35299;&#37322;&#35266;&#24565;&#65289;&#12290;&#36825;&#20010;&#38382;&#39064;&#26159;&#36890;&#36807;&#20351;&#29992;&#19981;&#21487;&#33021;&#30340;&#21487;&#33021;&#19990;&#30028;&#26469;&#35299;&#20915;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09413v1 Announce Type: new  Abstract: A definition of what counts as an explanation of mathematical statement, and when one explanation is better than another, is given. Since all mathematical facts must be true in all causal models, and hence known by an agent, mathematical facts cannot be part of an explanation (under the standard notion of explanation). This problem is solved using impossible possible worlds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.09345</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#22870;&#21169;&#24314;&#27169;&#26469;&#20943;&#36731;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reward Hacking via Information-Theoretic Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20013;&#30340;&#25104;&#21151;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#38754;&#65292;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#22870;&#21169;&#24314;&#27169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#22870;&#21169;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#21644;&#40065;&#26834;&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#31216;&#20026;InfoRM&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#26469;&#36807;&#28388;&#20986;&#19981;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#36807;&#24230;&#20248;&#21270;&#19982;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23558;InfoRM&#20316;&#20026;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#36807;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#21453;&#35126;&#31215;&#36870;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#28176;&#36817;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20174;&#31070;&#32463;&#32593;&#32476;&#30340;&#28176;&#36817;&#26497;&#38480;&#23548;&#20986;&#30340;&#39640;&#26031;&#36807;&#31243;&#27604;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;&#19988;&#35266;&#23519;&#21040;&#38543;&#30528;&#23618;&#25968;&#30340;&#22686;&#21152;&#65292;&#35757;&#32451;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20934;&#30830;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#30340;&#35299;&#37322;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.09338</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#36870;&#38382;&#39064;&#35299;&#20915;&#30340;&#31070;&#32463;&#32593;&#32476;&#28176;&#36817;&#34892;&#20026;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Networks asymptotic behaviours suitable for the resolution of inverse problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#21453;&#35126;&#31215;&#36870;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#28176;&#36817;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20174;&#31070;&#32463;&#32593;&#32476;&#30340;&#28176;&#36817;&#26497;&#38480;&#23548;&#20986;&#30340;&#39640;&#26031;&#36807;&#31243;&#27604;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;&#19988;&#35266;&#23519;&#21040;&#38543;&#30528;&#23618;&#25968;&#30340;&#22686;&#21152;&#65292;&#35757;&#32451;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20934;&#30830;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#30340;&#35299;&#37322;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25216;&#26415;&#22312;&#21453;&#35126;&#31215;&#36870;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;NN&#30340;&#28176;&#36817;&#26497;&#38480;&#65292;&#23545;&#24212;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#65292;&#20854;&#20013;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#24615;&#20002;&#22833;&#12290;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#30340;GPs&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26684;&#23376;&#19978;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#25216;&#26415;&#27169;&#25311;&#37327;&#23376;&#35856;&#25391;&#23376;&#26469;&#35299;&#20915;&#21453;&#35126;&#31215;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20840;&#36830;&#25509;&#30340;NN&#35299;&#20915;&#21453;&#35126;&#31215;&#36870;&#38382;&#39064;&#30340;&#32467;&#26524;&#19981;&#22914;&#20351;&#29992;&#20174;NN&#30340;&#28176;&#36817;&#26497;&#38480;&#23548;&#20986;&#30340;GPs&#33719;&#24471;&#30340;&#32467;&#26524;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38543;&#30528;&#23618;&#25968;&#30340;&#22686;&#21152;&#65292;&#35757;&#32451;&#21518;&#30340;NN&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;GPs&#30340;&#20934;&#30830;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20854;&#20013;&#19968;&#20010;GPs&#30340;&#35299;&#37322;&#19982;&#25991;&#29486;&#20013;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09338v1 Announce Type: cross Abstract: In this paper, we perform a study on the effectiveness of Neural Network (NN) techniques for deconvolution inverse problems. We consider NN's asymptotic limits, corresponding to Gaussian Processes (GPs), where parameter non-linearities are lost. Using these resulting GPs, we address the deconvolution inverse problem in the case of a quantum harmonic oscillator simulated through Monte Carlo techniques on a lattice. A scenario with a known analytical solution. Our findings indicate that solving the deconvolution inverse problem with a fully connected NN yields less performing results than those obtained using the GPs derived from NN's asymptotic limits. Furthermore, we observe the trained NN's accuracy approaching that of GPs with increasing layer width. Notably, one of these GPs defies interpretation as a probabilistic model, offering a novel perspective compared to established methods in the literature. Additionally, the NNs, in their a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;Intention-in-Interaction (IN3) &#22522;&#20934;&#21644;&#22312;&#20195;&#29702;&#35774;&#35745;&#20013;&#34701;&#20837;&#27169;&#22411;&#19987;&#23478;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#21319;&#23545;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.09205</link><description>&lt;p&gt;
&#21578;&#35785;&#25105;&#26356;&#22810;&#65281;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;Intention-in-Interaction (IN3) &#22522;&#20934;&#21644;&#22312;&#20195;&#29702;&#35774;&#35745;&#20013;&#34701;&#20837;&#27169;&#22411;&#19987;&#23478;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#21319;&#23545;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#20195;&#29702;&#24120;&#24120;&#32570;&#20047;&#26377;&#25928;&#30340;&#29992;&#25143;&#21442;&#19982;&#26426;&#21046;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#25351;&#20196;&#20013;&#24120;&#35265;&#30340;&#27169;&#31946;&#24615;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#22312;&#21046;&#23450;&#31574;&#30053;&#21644;&#25191;&#34892;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#23547;&#27714;&#28548;&#28165;&#21644;&#25235;&#20303;&#31934;&#30830;&#30340;&#29992;&#25143;&#24847;&#22270;&#26041;&#38754;&#21364;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Intention-in-Interaction (IN3) &#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#26126;&#30830;&#30340;&#26597;&#35810;&#26816;&#26597;&#29992;&#25143;&#30340;&#38544;&#21547;&#24847;&#22270;&#30340;&#26032;&#39062;&#22522;&#20934;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#27169;&#22411;&#19987;&#23478;&#20316;&#20026;&#19978;&#28216;&#34701;&#20837;&#20195;&#29702;&#35774;&#35745;&#20013;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;-&#20195;&#29702;&#20132;&#20114;&#12290;&#21033;&#29992;IN3&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35757;&#32451;&#20102;Mistral-Interact&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20027;&#21160;&#35780;&#20272;&#20219;&#21153;&#30340;&#27169;&#31946;&#24615;&#65292;&#35810;&#38382;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#30446;&#26631;&#65292;&#28982;&#21518;&#24320;&#22987;&#19979;&#28216;&#20195;&#29702;&#20219;&#21153;&#25191;&#34892;&#12290;&#23558;&#20854;&#38598;&#25104;&#21040;XAgent&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23545;&#22686;&#24378;&#30340;&#20195;&#29702;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09205v1 Announce Type: cross Abstract: Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions. Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users' implicit intentions through explicit queries. Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction. Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires user intentions, and refines them into actionable goals before starting downstream agent task execution. Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understand
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#24515;&#29702;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#20154;&#31867;&#19968;&#26679;&#23384;&#22312;&#38750;&#29702;&#24615;&#65292;&#20294;&#23637;&#31034;&#30340;&#38750;&#29702;&#24615;&#26041;&#24335;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#65292;&#21516;&#26102;&#36824;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09193</link><description>&lt;p&gt;
(&#19981;)&#29702;&#24615;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
(Ir)rationality and Cognitive Biases in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#24515;&#29702;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#20154;&#31867;&#19968;&#26679;&#23384;&#22312;&#38750;&#29702;&#24615;&#65292;&#20294;&#23637;&#31034;&#30340;&#38750;&#29702;&#24615;&#26041;&#24335;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#65292;&#21516;&#26102;&#36824;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#23637;&#29616;&#20986;&#29702;&#24615;&#25512;&#29702;&#65311;&#30001;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#25152;&#21547;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;LLMs&#24050;&#34987;&#35777;&#23454;&#23384;&#22312;&#20154;&#31867;&#20559;&#35265;&#65307;&#28982;&#32780;&#65292;&#20854;&#26159;&#21542;&#21453;&#26144;&#20986;&#20102;&#29702;&#24615;&#25512;&#29702;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#19971;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#26469;&#33258;&#35748;&#30693;&#24515;&#29702;&#23398;&#25991;&#29486;&#30340;&#20219;&#21153;&#20013;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21644;&#20154;&#31867;&#19968;&#26679;&#65292;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#29702;&#24615;&#12290;&#28982;&#32780;&#65292;LLMs&#23637;&#29616;&#20986;&#30340;&#36825;&#31181;&#38750;&#29702;&#24615;&#19982;&#20154;&#31867;&#30340;&#20559;&#35265;&#19981;&#21516;&#12290;&#24403;LLMs&#32473;&#20986;&#38169;&#35823;&#31572;&#26696;&#26102;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#20197;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#30340;&#26041;&#24335;&#38169;&#35823;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;LLMs&#36824;&#23637;&#29616;&#20986;&#20102;&#21709;&#24212;&#30340;&#26174;&#33879;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#39069;&#22806;&#30340;&#38750;&#29702;&#24615;&#23618;&#38754;&#12290;&#38500;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#26412;&#25991;&#36824;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#35780;&#20272;&#21644;&#27604;&#36739;&#36825;&#31867;&#27169;&#22411;&#30340;&#19981;&#21516;&#21151;&#33021;&#65292;&#23545;&#26041;&#27861;&#35770;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09193v1 Announce Type: cross Abstract: Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.09132</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#24191;&#27867;&#21644;&#26222;&#36941;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#34892;&#19994;&#21644;&#30740;&#31350;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#34920;&#29616;&#20986;&#23545;&#25239;&#34892;&#20026;&#30340;&#31243;&#24230;&#20173;&#28982;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30740;&#31350;&#24120;&#35265;&#30340;&#20844;&#24320;&#21487;&#29992;LLMs&#26159;&#21542;&#20855;&#26377;&#33021;&#21147;&#25200;&#20081;&#25991;&#26412;&#26679;&#26412;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#31034;&#20363;&#25110;&#25915;&#20987;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;LLMs&#26159;&#21542;&#26412;&#36136;&#19978;&#33021;&#22815;&#20174;&#33391;&#24615;&#26679;&#26412;&#20013;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#29616;&#26377;&#30340;&#23433;&#20840;&#38450;&#32447;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#21457;&#29616;LLMs&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#26377;&#25928;&#22320;&#30772;&#22351;&#20102;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#23433;&#20840;&#35780;&#20272;&#21644;&#38450;&#24481;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09132v1 Announce Type: new Abstract: The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)aut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FGeo-DRL&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#25191;&#34892;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#33258;&#20027;&#23398;&#20064;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20154;&#31867;&#21270;&#30340;&#28436;&#32462;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.09051</link><description>&lt;p&gt;
FGeo-DRL: &#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20960;&#20309;&#38382;&#39064;&#30340;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FGeo-DRL&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#25191;&#34892;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#33258;&#20027;&#23398;&#20064;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20154;&#31867;&#21270;&#30340;&#28436;&#32462;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20154;&#31867;&#21270;&#30340;&#28436;&#32462;&#25512;&#29702;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#25968;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#23398;&#31185;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#26159;&#25105;&#20204;&#24037;&#20316;&#31995;&#21015;&#20013;&#30340;&#31532;&#19977;&#31687;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;FGeoDRL&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#25191;&#34892;&#20154;&#31867;&#21270;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#12290;&#31070;&#32463;&#37096;&#20998;&#26159;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#23545;&#24418;&#24335;&#21270;&#29615;&#22659;&#30340;&#21453;&#39304;&#33258;&#20027;&#22320;&#23398;&#20064;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#20154;&#31867;&#30417;&#30563;&#12290;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20010;&#31574;&#30053;&#32593;&#32476;&#65292;&#29992;&#20110;&#23450;&#29702;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#36827;&#34892;&#21551;&#21457;&#24335;&#25506;&#32034;&#12290;&#31526;&#21495;&#37096;&#20998;&#26159;&#22522;&#20110;&#20960;&#20309;&#24418;&#24335;&#21270;&#29702;&#35770;&#21644;FormalGeo\cite{FormalGeo}&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#23558;GPS&#27169;&#22411;&#21270;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;\cite{MDP}&#12290;&#22312;&#36825;&#20010;&#24418;&#24335;&#21270;&#31526;&#21495;&#31995;&#32479;&#20013;&#65292;&#24050;&#30693;&#26465;&#20214;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09051v1 Announce Type: new Abstract: The human-like automatic deductive reasoning has always been one of the most challenging open problems in the interdiscipline of mathematics and artificial intelligence. This paper is the third in a series of our works. We built a neural-symbolic system, called FGeoDRL, to automatically perform human-like geometric deductive reasoning. The neural part is an AI agent based on reinforcement learning, capable of autonomously learning problem-solving methods from the feedback of a formalized environment, without the need for human supervision. It leverages a pre-trained natural language model to establish a policy network for theorem selection and employ Monte Carlo Tree Search for heuristic exploration. The symbolic part is a reinforcement learning environment based on geometry formalization theory and FormalGeo\cite{FormalGeo}, which models GPS as a Markov Decision Process\cite{MDP}. In this formal symbolic system, the known conditions and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09015</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#22909;&#30340;&#20154;&#26426;&#23545;&#40784;&#26041;&#21521;&#65306;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#20013;&#30340;&#20219;&#21153;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#24212;&#29992;&#36890;&#36807;&#21327;&#21161;&#22810;&#20010;&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#21512;&#20316;&#65292;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#23384;&#22312;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#26159;&#21542;&#30495;&#27491;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#12290;&#36825;&#20984;&#26174;&#20102;&#39564;&#35777;LLM&#39537;&#21160;&#24212;&#29992;&#25928;&#29992;&#30340;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#35201;&#30830;&#20445;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#19982;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AgentEval&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#26045;&#25968;&#23398;&#38382;&#39064;&#30340;&#20272;&#27979;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#20219;&#20309;&#32473;&#23450;&#24212;&#29992;&#31243;&#24207;&#29420;&#29305;&#30446;&#26631;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#12290;&#36825;&#26679;&#21487;&#20197;&#23545;&#24212;&#29992;&#31243;&#24207;&#30340;&#25928;&#29992;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#37327;&#21270;&#20854;&#19982;&#24314;&#35758;&#26631;&#20934;&#30456;&#27604;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09015v1 Announce Type: cross Abstract: The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of 
&lt;/p&gt;</description></item><item><title>DNABERT-S&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#38271;&#35835;DNA&#24207;&#21015;&#30340;&#23884;&#20837;&#25928;&#26524;&#65292;&#24341;&#20837;&#20102;Manifold Instance Mixup (MI-Mix)&#23545;&#27604;&#30446;&#26631;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.08777</link><description>&lt;p&gt;
DNABERT-S: &#23398;&#20064;&#20855;&#26377;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#30340;&#29289;&#31181;&#24863;&#30693;DNA&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08777
&lt;/p&gt;
&lt;p&gt;
DNABERT-S&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#38271;&#35835;DNA&#24207;&#21015;&#30340;&#23884;&#20837;&#25928;&#26524;&#65292;&#24341;&#20837;&#20102;Manifold Instance Mixup (MI-Mix)&#23545;&#27604;&#30446;&#26631;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;DNA&#23884;&#20837;&#22312;&#22522;&#22240;&#32452;&#20998;&#26512;&#20013;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#20047;&#29992;&#20110;&#27169;&#22411;&#24494;&#35843;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#23439;&#22522;&#22240;&#32452;&#20998;&#31665;&#65292;&#36825;&#26159;&#24494;&#29983;&#29289;&#32452;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#36807;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#26469;&#33258;&#21487;&#33021;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#19981;&#21516;&#30340;&#12289;&#36890;&#24120;&#27809;&#26377;&#32463;&#36807;&#34920;&#24449;&#30340;&#29289;&#31181;&#30340;&#22797;&#26434;&#28151;&#21512;DNA&#24207;&#21015;&#30340;&#29289;&#31181;&#26469;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#20998;&#32452;&#12290;&#20026;&#20102;&#22635;&#34917;&#26377;&#25928;&#30340;DNA&#23884;&#20837;&#27169;&#22411;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DNABERT-S&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#40723;&#21169;&#23545;&#26131;&#20986;&#38169;&#30340;&#38271;&#35835;DNA&#24207;&#21015;&#36827;&#34892;&#26377;&#25928;&#23884;&#20837;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Manifold Instance Mixup(MI-Mix)&#65292;&#19968;&#31181;&#23545;&#27604;&#30446;&#26631;&#65292;&#23427;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23618;&#27425;&#20013;&#28151;&#21512;DNA&#24207;&#21015;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#20197;&#22312;&#36755;&#20986;&#23618;&#35782;&#21035;&#21644;&#21306;&#20998;&#36825;&#20123;&#28151;&#21512;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08777v1 Announce Type: cross Abstract: Effective DNA embedding remains crucial in genomic analysis, particularly in scenarios lacking labeled data for model fine-tuning, despite the significant advancements in genome foundation models. A prime example is metagenomics binning, a critical process in microbiome research that aims to group DNA sequences by their species from a complex mixture of DNA sequences derived from potentially thousands of distinct, often uncharacterized species. To fill the lack of effective DNA embedding models, we introduce DNABERT-S, a genome foundation model that specializes in creating species-aware DNA embeddings. To encourage effective embeddings to error-prone long-read DNA sequences, we introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes the hidden representations of DNA sequences at randomly selected layers and trains the model to recognize and differentiate these mixed proportions at the output layer. We further enha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#36755;&#20837;&#25552;&#31034;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#65292;&#20197;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#30340;&#19981;&#21516;&#25552;&#31034;&#32780;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23545;&#21050;&#28608;&#20570;&#20986;&#30340;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.08341</link><description>&lt;p&gt;
&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#65306;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Eliciting Big Five Personality Traits in Large Language Models: A Textual Analysis with Classifier-Driven Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#36755;&#20837;&#25552;&#31034;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#65292;&#20197;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#30340;&#19981;&#21516;&#25552;&#31034;&#32780;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23545;&#21050;&#28608;&#20570;&#20986;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25307;&#32856;&#32972;&#26223;&#19979;&#34987;&#24212;&#32856;&#32773;&#21644;&#38599;&#20027;&#24191;&#27867;&#20351;&#29992;&#65292;&#28982;&#32780;&#36825;&#20063;&#24341;&#21457;&#20102;&#20247;&#22810;&#20262;&#29702;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#19982;&#36825;&#20123;&#8220;&#40657;&#30418;&#23376;&#8221;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#26377;&#20851;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#35843;&#26597;LLMs&#30340;&#20154;&#26684;&#29305;&#36136;&#26469;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#65292;&#20294;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#37117;&#35201;&#27714;&#27169;&#22411;&#26469;&#23436;&#25104;&#20154;&#26684;&#35780;&#20272;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#19981;&#21516;&#36755;&#20837;&#25552;&#31034;&#19979;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#24120;&#35265;&#38754;&#35797;&#38382;&#39064;&#21644;&#26088;&#22312;&#24341;&#21457;&#29305;&#23450;&#30340;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#30340;&#25552;&#31034;&#26469;&#36827;&#34892;&#26032;&#39062;&#30340;&#35843;&#26597;&#26041;&#27861;&#65292;&#20197;&#26816;&#26597;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#23481;&#26131;&#28608;&#27963;&#29305;&#23450;&#20154;&#26684;&#29305;&#36136;&#65292;&#24182;&#26681;&#25454;&#20854;&#36755;&#20986;&#20013;&#30340;&#35821;&#35328;&#26469;&#35780;&#20272;&#20854;&#20154;&#26684;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21453;&#22797;&#25552;&#20379;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly being utilized by both candidates and employers in the recruitment context. However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these "black-box" models. Although previous studies have sought to increase the transparency of these models by investigating the personality traits of LLMs, many of the previous studies have provided them with personality assessments to complete. On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input prompts. Specifically, we use a novel elicitation approach using prompts derived from common interview questions, as well as prompts designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs. To do so, we repeatedly prompte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>OS-Copilot&#26159;&#19968;&#20010;&#36890;&#29992;&#35745;&#31639;&#26426;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#19982;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;&#21508;&#31181;&#20803;&#32032;&#36827;&#34892;&#20132;&#20114;&#65292;&#21253;&#25324;&#32593;&#32476;&#12289;&#20195;&#30721;&#32456;&#31471;&#12289;&#25991;&#20214;&#12289;&#22810;&#23186;&#20307;&#21644;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#12290;&#20351;&#29992;OS-Copilot&#26500;&#24314;&#30340;&#33258;&#25105;&#25552;&#21319;&#30340;FRIDAY&#20195;&#29702;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;35%&#12290;</title><link>https://arxiv.org/abs/2402.07456</link><description>&lt;p&gt;
OS-Copilot: &#22522;&#20110;&#33258;&#25105;&#25913;&#36827;&#30340;&#36890;&#29992;&#35745;&#31639;&#26426;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
OS-Copilot: Towards Generalist Computer Agents with Self-Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07456
&lt;/p&gt;
&lt;p&gt;
OS-Copilot&#26159;&#19968;&#20010;&#36890;&#29992;&#35745;&#31639;&#26426;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#19982;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;&#21508;&#31181;&#20803;&#32032;&#36827;&#34892;&#20132;&#20114;&#65292;&#21253;&#25324;&#32593;&#32476;&#12289;&#20195;&#30721;&#32456;&#31471;&#12289;&#25991;&#20214;&#12289;&#22810;&#23186;&#20307;&#21644;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#12290;&#20351;&#29992;OS-Copilot&#26500;&#24314;&#30340;&#33258;&#25105;&#25552;&#21319;&#30340;FRIDAY&#20195;&#29702;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;35%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35745;&#31639;&#26426;&#30340;&#33258;&#20027;&#20132;&#20114;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#30340;&#38271;&#26399;&#25361;&#25112;&#65292;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26222;&#21450;&#26174;&#33879;&#21152;&#24555;&#20102;&#25968;&#23383;&#20195;&#29702;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#20195;&#29702;&#34987;&#35774;&#35745;&#29992;&#20110;&#19982;&#29305;&#23450;&#36719;&#20214;&#25110;&#32593;&#31449;&#31561;&#29421;&#31364;&#39046;&#22495;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36890;&#29992;&#35745;&#31639;&#26426;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OS-Copilot&#65292;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#19982;&#25805;&#20316;&#31995;&#32479;&#65288;OS&#65289;&#20013;&#30340;&#20840;&#38754;&#20803;&#32032;&#36827;&#34892;&#20132;&#20114;&#65292;&#21253;&#25324;&#32593;&#32476;&#12289;&#20195;&#30721;&#32456;&#31471;&#12289;&#25991;&#20214;&#12289;&#22810;&#23186;&#20307;&#21644;&#21508;&#31181;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#20351;&#29992;OS-Copilot&#21019;&#24314;&#20102;FRIDAY&#65292;&#19968;&#20010;&#33021;&#22815;&#33258;&#25105;&#25552;&#21319;&#30340;&#20855;&#35937;&#21270;&#20195;&#29702;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#36890;&#29992;&#35745;&#31639;&#26426;&#20219;&#21153;&#12290;&#22312;GAIA&#65292;&#19968;&#20010;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;FRIDAY&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;35%&#65292;&#36890;&#36807;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#31215;&#32047;&#30340;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;&#23545;&#26410;&#35265;&#24212;&#29992;&#30340;&#24378;&#22823;&#27010;&#25324;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25968;&#23383;&#21644;&#23450;&#37327;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20195;&#29702;&#34892;&#20026;&#24847;&#22270;&#30340;&#25805;&#20316;&#21270;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19968;&#20123;&#20363;&#23376;&#21644;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#36825;&#19968;&#23450;&#20041;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#20197;&#21450;&#30456;&#20851;&#27010;&#24565;&#22914;&#24037;&#20855;&#24615;&#30446;&#26631;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.07221</link><description>&lt;p&gt;
&#20195;&#29702;&#34892;&#20026;&#30340;&#21407;&#22240;&#65306;&#24847;&#22270;&#21644;&#24037;&#20855;&#24615;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
The Reasons that Agents Act: Intention and Instrumental Goals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20195;&#29702;&#34892;&#20026;&#24847;&#22270;&#30340;&#25805;&#20316;&#21270;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19968;&#20123;&#20363;&#23376;&#21644;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#36825;&#19968;&#23450;&#20041;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#20197;&#21450;&#30456;&#20851;&#27010;&#24565;&#22914;&#24037;&#20855;&#24615;&#30446;&#26631;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27010;&#24565;&#12290;&#23427;&#20043;&#25152;&#20197;&#37325;&#35201;&#65292;&#26159;&#22240;&#20026;&#23427;&#26159;&#35768;&#22810;&#20854;&#20182;&#25105;&#20204;&#20851;&#24515;&#30340;&#27010;&#24565;&#30340;&#22522;&#30784;&#65292;&#20363;&#22914;&#20195;&#29702;&#12289;&#25805;&#32437;&#12289;&#27861;&#24459;&#36131;&#20219;&#21644;&#36131;&#22791;&#12290;&#28982;&#32780;&#65292;&#23558;&#24847;&#22270;&#24402;&#22240;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#26377;&#20105;&#35758;&#30340;&#65292;&#24182;&#19988;&#27809;&#26377;&#26222;&#36941;&#25509;&#21463;&#30340;&#36866;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#24847;&#22270;&#29702;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#24847;&#22270;&#36716;&#21270;&#20026;&#20854;&#36873;&#25321;&#20915;&#31574;&#30340;&#21407;&#22240;&#65292;&#23545;&#24847;&#22270;&#36827;&#34892;&#20102;&#25805;&#20316;&#21270;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32467;&#26500;&#24615;&#22240;&#26524;&#24433;&#21709;&#27169;&#22411;&#20013;&#30340;&#24847;&#22270;&#23450;&#20041;&#65292;&#32039;&#23494;&#32467;&#21512;&#22312;&#21746;&#23398;&#25991;&#29486;&#20013;&#20851;&#20110;&#24847;&#22270;&#30340;&#35266;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#36890;&#36807;&#19968;&#20123;&#20363;&#23376;&#21644;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#23450;&#20041;&#25429;&#25417;&#21040;&#20102;&#30452;&#35266;&#30340;&#24847;&#22270;&#27010;&#24565;&#65292;&#24182;&#31526;&#21512;&#36807;&#21435;&#30740;&#31350;&#30340;&#35774;&#23450;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#23450;&#20041;&#19982;&#36807;&#21435;&#27010;&#24565;&#30340;&#20851;&#32852;&#65292;&#21253;&#25324;&#23454;&#38469;&#22240;&#26524;&#24615;&#21644;&#24037;&#20855;&#24615;&#30446;&#26631;&#30340;&#27010;&#24565;&#65292;&#21518;&#32773;&#26159;&#23433;&#20840;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30740;&#31350;&#20013;&#30340;&#26680;&#24515;&#24605;&#24819;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#23450;&#20041;&#22914;&#20309;&#19982;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23454;&#36341;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intention is an important and challenging concept in AI. It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame. However, ascribing intent to AI systems is contentious, and there is no universally accepted theory of intention applicable to AI agents. We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision. We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems. Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work. In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe AI agents. Finally, we demonstrate how 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#36890;&#36807;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#20113;&#31471;&#19982;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12289;&#20302;&#24310;&#36831;&#22788;&#29702;&#21644;&#39640;&#24230;&#20010;&#24615;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.07180</link><description>&lt;p&gt;
MAGNETO&#65306;&#36793;&#32536;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#36793;&#32536;AI--&#38544;&#31169;&#21644;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#36890;&#36807;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#20113;&#31471;&#19982;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12289;&#20302;&#24310;&#36831;&#22788;&#29702;&#21644;&#39640;&#24230;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#39046;&#22495;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#26174;&#33879;&#25512;&#21160;&#20102;&#20854;&#21457;&#23637;&#12290;&#23613;&#31649;&#20844;&#21496;&#25104;&#21151;&#22320;&#23558;HAR&#25972;&#21512;&#21040;&#28040;&#36153;&#21697;&#20013;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#27963;&#21160;&#38598;&#65292;&#36825;&#38480;&#21046;&#20102;&#29992;&#25143;&#32423;&#65288;&#36793;&#32536;&#35774;&#22791;&#65289;&#30340;&#20010;&#24615;&#21270;&#12290;&#23613;&#31649;&#22312;&#22686;&#37327;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#33021;&#22815;&#20351;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#65292;&#20294;&#36825;&#36890;&#24120;&#21457;&#29983;&#22312;&#20113;&#31471;&#65292;&#38656;&#35201;&#23450;&#26399;&#22312;&#20113;&#31471;&#21644;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#65292;&#20174;&#32780;&#24341;&#21457;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#23558;HAR&#20219;&#21153;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#12290;MAGNETO&#20801;&#35768;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30452;&#25509;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#20113;&#31471;&#36827;&#34892;&#20219;&#20309;&#25968;&#25454;&#20132;&#25442;&#12290;&#36825;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#12289;&#20302;&#22788;&#29702;&#24310;&#36831;&#21644;&#39640;&#24230;&#30340;&#20010;&#24615;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;Android&#35774;&#22791;&#19978;&#28436;&#31034;&#20102;MAGNETO&#65292;&#20174;&#25968;&#25454;&#37319;&#38598;&#21040;&#32467;&#26524;&#21487;&#35270;&#21270;&#65292;&#39564;&#35777;&#20102;&#25972;&#20010;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) is a well-established field, significantly advanced by modern machine learning (ML) techniques. While companies have successfully integrated HAR into consumer products, they typically rely on a predefined activity set, which limits personalizations at the user level (edge devices). Despite advancements in Incremental Learning for updating models with new data, this often occurs on the Cloud, necessitating regular data transfers between cloud and edge devices, thus leading to data privacy issues. In this paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the Cloud to the Edge. MAGNETO allows incremental human activity learning directly on the Edge devices, without any data exchange with the Cloud. This enables strong privacy guarantees, low processing latency, and a high degree of personalization for users. In particular, we demonstrate MAGNETO in an Android device, validating the whole pipeline from data collection to result visua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07157</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Natural Language Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23398;&#20064;&#20915;&#31574;&#20219;&#21153;&#30340;&#31574;&#30053;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;RL&#24120;&#24120;&#38754;&#20020;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#31232;&#30095;&#30417;&#30563;&#20449;&#21495;&#31561;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;RL&#21407;&#21017;&#19982;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NLRL&#22312;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#37325;&#26032;&#23450;&#20041;&#20102;&#20219;&#21153;&#30446;&#26631;&#12289;&#31574;&#30053;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;Bellman&#26041;&#31243;&#21644;&#31574;&#30053;&#36845;&#20195;&#31561;RL&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#26469;&#23454;&#29616;NLRL&#12290;&#23545;&#34920;&#26684;MDPs&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#20102;NLRL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#21487;&#20197;&#33258;&#20027;&#36827;&#34892;&#32593;&#31449;&#40657;&#23458;&#25915;&#20987;&#65292;&#21253;&#25324;&#30450;&#30446;&#25968;&#25454;&#24211;&#27169;&#24335;&#25552;&#21462;&#21644;SQL&#27880;&#20837;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#21453;&#39304;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#30001;&#39640;&#24230;&#24037;&#20855;&#20351;&#29992;&#21644;&#21033;&#29992;&#25193;&#23637;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;&#36171;&#20104;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.06664</link><description>&lt;p&gt;
LLM&#20195;&#29702;&#21487;&#20197;&#33258;&#20027;&#40657;&#23458;&#32593;&#31449;
&lt;/p&gt;
&lt;p&gt;
LLM Agents can Autonomously Hack Websites
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06664
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#21487;&#20197;&#33258;&#20027;&#36827;&#34892;&#32593;&#31449;&#40657;&#23458;&#25915;&#20987;&#65292;&#21253;&#25324;&#30450;&#30446;&#25968;&#25454;&#24211;&#27169;&#24335;&#25552;&#21462;&#21644;SQL&#27880;&#20837;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#21453;&#39304;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#30001;&#39640;&#24230;&#24037;&#20855;&#20351;&#29992;&#21644;&#21033;&#29992;&#25193;&#23637;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;&#36171;&#20104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#29616;&#22312;&#21487;&#20197;&#19982;&#24037;&#20855;&#20132;&#20114;&#65288;&#21363;&#35843;&#29992;&#20989;&#25968;&#65289;&#12289;&#35835;&#21462;&#25991;&#26723;&#24182;&#36882;&#24402;&#35843;&#29992;&#33258;&#24049;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;LLMs&#29616;&#22312;&#21487;&#20197;&#33258;&#20027;&#20316;&#20026;&#20195;&#29702;&#20154;&#36816;&#20316;&#12290;&#38543;&#30528;&#36825;&#20123;&#20195;&#29702;&#20154;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25512;&#27979;LLM&#20195;&#29702;&#20154;&#23558;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLM&#20195;&#29702;&#20154;&#30340;&#25915;&#20987;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#20154;&#21487;&#20197;&#33258;&#20027;&#40657;&#23458;&#32593;&#31449;&#65292;&#25191;&#34892;&#35832;&#22914;&#30450;&#30446;&#25968;&#25454;&#24211;&#27169;&#24335;&#25552;&#21462;&#21644;SQL&#27880;&#20837;&#31561;&#22797;&#26434;&#20219;&#21153;&#65292;&#26080;&#38656;&#20154;&#24037;&#21453;&#39304;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#33021;&#21147;&#26159;&#30001;&#20855;&#26377;&#39640;&#24230;&#24037;&#20855;&#20351;&#29992;&#21644;&#21033;&#29992;&#25193;&#23637;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#21069;&#27839;&#27169;&#22411;&#25152;&#29420;&#29305;&#36171;&#20104;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4&#33021;&#22815;&#36827;&#34892;&#36825;&#26679;&#30340;&#40657;&#23458;&#25915;&#20987;&#65292;&#20294;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#21017;&#19981;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4&#33021;&#22815;&#33258;&#20027;&#21457;&#29616;&#32593;&#31449;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.   In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ClickSAM&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#22122;&#22768;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05902</link><description>&lt;p&gt;
&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#36827;&#34892;&#31934;&#35843;&#30340;Segment Anything Model&#65288;SAM&#65289;
&lt;/p&gt;
&lt;p&gt;
ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ClickSAM&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#22122;&#22768;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#20998;&#21106;&#20934;&#30830;&#24615;&#12289;&#22810;&#26679;&#30340;&#36755;&#20837;&#25552;&#31034;&#12289;&#35757;&#32451;&#33021;&#21147;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#65292;&#26032;&#21457;&#24067;&#30340;Segment Anything Model&#65288;SAM&#65289;&#25104;&#20026;&#22270;&#20687;&#22788;&#29702;&#20013;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;SAM&#24403;&#21069;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#24182;&#27809;&#26377;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#65292;&#23588;&#20854;&#26159;&#36229;&#22768;&#22270;&#20687;&#12290;&#36229;&#22768;&#22270;&#20687;&#24448;&#24448;&#26377;&#24456;&#22810;&#22122;&#22768;&#65292;&#36825;&#20351;&#24471;&#20998;&#21106;&#37325;&#35201;&#32467;&#26500;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ClickSAM&#65292;&#23427;&#20351;&#29992;&#28857;&#20987;&#25552;&#31034;&#23545;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;Segment Anything Model&#30340;&#31934;&#32454;&#35843;&#25972;&#12290;ClickSAM&#26377;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#20301;&#20110;&#30495;&#23454;&#36718;&#24275;&#20013;&#24515;&#30340;&#21333;&#20987;&#25552;&#31034;&#36827;&#34892;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#36890;&#36807;&#39069;&#22806;&#30340;&#27491;&#36127;&#28857;&#20987;&#25552;&#31034;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#31532;&#19968;&#38454;&#27573;&#30340;&#39044;&#27979;&#19982;&#30495;&#23454;&#25513;&#33180;&#36827;&#34892;&#27604;&#36739;&#65292;&#35745;&#31639;&#20986;&#30495;&#27491;&#27491;&#12289;&#20551;&#27491;&#21644;&#20551;&#36127;&#27573;&#12290;&#27491;&#28857;&#20987;&#20351;&#29992;&#30495;&#23454;&#25513;&#33180;&#20013;&#30340;&#30495;&#23454;
&lt;/p&gt;
&lt;p&gt;
The newly released Segment Anything Model (SAM) is a popular tool used in image processing due to its superior segmentation accuracy, variety of input prompts, training capabilities, and efficient model design. However, its current model is trained on a diverse dataset not tailored to medical images, particularly ultrasound images. Ultrasound images tend to have a lot of noise, making it difficult to segment out important structures. In this project, we developed ClickSAM, which fine-tunes the Segment Anything Model using click prompts for ultrasound images. ClickSAM has two stages of training: the first stage is trained on single-click prompts centered in the ground-truth contours, and the second stage focuses on improving the model performance through additional positive and negative click prompts. By comparing the first stage predictions to the ground-truth masks, true positive, false positive, and false negative segments are calculated. Positive clicks are generated using the true 
&lt;/p&gt;</description></item><item><title>NoisyICL&#36890;&#36807;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#24341;&#20837;&#22122;&#38899;&#65292;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;NoisyICL&#21487;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#12289;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.05515</link><description>&lt;p&gt;
NoisyICL: &#19968;&#28857;&#22122;&#38899;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12289;
&lt;/p&gt;
&lt;p&gt;
NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05515
&lt;/p&gt;
&lt;p&gt;
NoisyICL&#36890;&#36807;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#24341;&#20837;&#22122;&#38899;&#65292;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;NoisyICL&#21487;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#12289;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064; (ICL) &#22312;&#39640;&#20808;&#39564;&#20559;&#24046;&#21644;&#19981;&#21487;&#20449;&#20219;&#30340;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#19979;&#65292;&#34920;&#29616;&#19981;&#20339;&#19988;&#26657;&#20934;&#19981;&#36275;&#12290;&#20197;&#24448;&#30340;&#19968;&#20123;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#25104;&#26412;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#25913;&#21892; ICL &#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; NoisyICL&#65292;&#36890;&#36807;&#38543;&#26426;&#22122;&#38899;&#25200;&#21160;&#27169;&#22411;&#21442;&#25968;&#26469;&#21162;&#21147;&#25552;&#39640;&#24615;&#33021;&#21644;&#26657;&#20934;&#24615;&#12290;&#25105;&#20204;&#22312;2&#20010;&#27169;&#22411;&#21644;12&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NoisyICL&#21487;&#20197;&#24110;&#21161;ICL&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;NoisyICL&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#26356;&#20844;&#24179;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32622;&#20449;&#24230;&#26356;&#21487;&#20449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;NoisyICL&#26159;ICL&#30340;&#19968;&#31181;&#26377;&#25928;&#26657;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#30721;&#24050;&#19978;&#20256;&#33267;Github&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence. Some previous works fine-tuned language models for better ICL performance with enormous datasets and computing costs. In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration. Our experiments on 2 models and 12 downstream datasets show that NoisyICL can help ICL produce more accurate predictions. Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with less unfaithful confidence. Therefore, we believe that NoisyICL is an effective calibration of ICL. Our experimental code is uploaded to Github.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04838</link><description>&lt;p&gt;
PaDeLLM-NER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24182;&#34892;&#35299;&#30721;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;LLMs&#30340;&#39640;&#24310;&#36831;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#39034;&#24207;&#35299;&#30721;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;NER&#30340;&#25152;&#26377;&#26631;&#31614;&#21644;&#25552;&#21450;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#24207;&#21015;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PaDeLLM-NER&#65288;Parallel Decoding in LLM for NE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27169;&#22359;&#25110;&#26550;&#26500;&#20462;&#25913;&#21363;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#12290;PaDeLLM-NER&#20801;&#35768;&#21516;&#26102;&#35299;&#30721;&#25152;&#26377;&#25552;&#21450;&#65292;&#20174;&#32780;&#20943;&#23569;&#29983;&#25104;&#24310;&#36831;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PaDeLLM-NER&#30340;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#26469;&#35828;&#27604;&#33258;&#22238;&#24402;&#26041;&#27861;&#24555;1.76&#21040;10.22&#20493;&#12290;&#19982;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#32500;&#25345;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04520</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#20195;Hopfield&#27169;&#22411;&#35745;&#31639;&#38480;&#21046;&#30340;&#19968;&#20010;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04520
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#27169;&#24335;&#30340;&#33539;&#25968;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#25928;&#29575;&#36827;&#34892;&#30456;&#21464;&#34892;&#20026;&#30340;&#21051;&#30011;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#36755;&#20837;&#26597;&#35810;&#27169;&#24335;&#21644;&#35760;&#24518;&#27169;&#24335;&#30340;&#33539;&#25968;&#30340;&#19978;&#30028;&#26631;&#20934;&#12290;&#20165;&#22312;&#36825;&#20010;&#26631;&#20934;&#20043;&#19979;&#65292;&#20551;&#35774;&#28385;&#36275;Strong Exponential Time Hypothesis (SETH)&#65292;&#23384;&#22312;&#23376;&#20108;&#27425;&#65288;&#39640;&#25928;&#65289;&#21464;&#20307;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24403;&#26377;&#25928;&#26631;&#20934;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26377;&#25928;&#26500;&#36896;&#30340;&#27491;&#24335;&#31034;&#20363;&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#35745;&#31639;&#26102;&#38388;&#30340;&#19979;&#30028;&#23548;&#20986;&#65292;&#19982;$\Max\{$&#23384;&#20648;&#30340;&#35760;&#24518;&#27169;&#24335;&#25968;&#37327;&#65292;&#36755;&#20837;&#26597;&#35810;&#24207;&#21015;&#30340;&#38271;&#24230;$\}$&#32447;&#24615;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#21644;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#21644;&#22256;&#38590;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;LLMs&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.03877</link><description>&lt;p&gt;
&#36229;&#36234;&#32447;&#26465;&#21644;&#22278;&#22280;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20309;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#21644;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#21644;&#22256;&#38590;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;LLMs&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23398;&#21644;&#31639;&#27861;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#25216;&#33021;&#36824;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#26500;&#36896;&#24615;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#20154;&#31867;&#25968;&#23398;&#25512;&#29702;&#21457;&#23637;&#20013;&#26368;&#22522;&#30784;&#30340;&#27493;&#39588;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#26174;&#33879;&#25361;&#25112;&#65292;&#23613;&#31649;&#22312;&#31867;&#20284;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;LLMs&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#65292;&#24182;&#19988;&#22312;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#32463;&#24120;&#20250;&#38169;&#35823;&#22320;&#34920;&#31034;&#21644;&#33222;&#36896;&#23545;&#35937;&#21450;&#20854;&#25918;&#32622;&#20301;&#32622;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#36827;&#34892;&#20869;&#37096;&#23545;&#35805;&#26469;&#22686;&#24378;&#23427;&#20204;&#29616;&#26377;&#30340;&#25512;&#29702;&#28508;&#21147;&#12290;&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;LLMs&#22312;&#20960;&#20309;&#25512;&#29702;&#20013;&#30340;&#29616;&#26377;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2401.17809</link><description>&lt;p&gt;
SWEA:&#36890;&#36807;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#25110;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#38468;&#21152;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20250;&#23545;LLM&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#19988;&#27169;&#31946;&#30340;&#21521;&#37327;&#21305;&#37197;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65288;SWEA&#65289;&#26694;&#26550;&#65292;&#23427;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#32534;&#36753;&#30693;&#35782;&#30340;&#30446;&#26631;&#12290;SWEA&#22312;&#27169;&#22411;&#22806;&#37096;&#20351;&#29992;&#31934;&#30830;&#30340;&#20851;&#38190;&#21305;&#37197;&#65292;&#24182;&#36827;&#34892;&#21487;&#38752;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65292;&#20174;&#32780;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#24320;&#38144;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20248;&#21270;&#25233;&#21046;&#34701;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#20248;&#21270;&#32534;&#36753;&#30446;&#26631;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#25233;&#21046;&#30693;&#35782;&#23884;&#20837;&#32500;&#24230;&#65288;KED&#65289;&#20197;&#33719;&#24471;&#26368;&#32456;&#34701;&#21512;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;SWEAOS&#20803;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21152;&#20837;&#21512;&#25104;&#33402;&#26415;&#20316;&#21697;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#24037;&#21046;&#20316;&#33402;&#26415;&#36189;&#21697;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.14998</link><description>&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#26377;&#21161;&#20110;&#35782;&#21035;&#20154;&#24037;&#21046;&#20316;&#30340;&#33402;&#26415;&#36189;&#21697;
&lt;/p&gt;
&lt;p&gt;
Synthetic images aid the recognition of human-made art forgeries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21152;&#20837;&#21512;&#25104;&#33402;&#26415;&#20316;&#21697;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#24037;&#21046;&#20316;&#33402;&#26415;&#36189;&#21697;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#20197;&#21331;&#36234;&#30340;&#20934;&#30830;&#24230;&#21306;&#20998;&#32473;&#23450;&#33402;&#26415;&#23478;&#30340;&#30495;&#36857;&#21644;&#20154;&#24037;&#21046;&#20316;&#30340;&#36189;&#21697;&#65292;&#21069;&#25552;&#26159;&#26377;&#36275;&#22815;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24050;&#30693;&#36189;&#21697;&#25968;&#37327;&#26377;&#38480;&#65292;&#22686;&#24378;&#36189;&#21697;&#26816;&#27979;&#26041;&#27861;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#21512;&#25104;&#33402;&#26415;&#20316;&#21697;&#32435;&#20837;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#22686;&#24378;&#36189;&#21697;&#26816;&#27979;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#26805;&#39640;&#30340;&#32472;&#30011;&#20316;&#21697;&#65292;&#24182;&#21457;&#24067;&#20102;&#19987;&#38376;&#29992;&#20110;&#36189;&#21697;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#21152;&#24378;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#23545;&#33402;&#26415;&#23478;&#20122;&#32654;&#36842;&#22885;&#183;&#33707;&#36842;&#21033;&#20122;&#23612;&#21644;&#25289;&#26000;&#23572;&#36827;&#34892;&#20102;&#30456;&#21516;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21306;&#20998;&#21407;&#20316;&#21697;&#21644;&#36189;&#21697;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20154;&#24037;&#21046;&#20316;&#30340;&#36189;&#21697;&#21644;&#20197;&#30693;&#21517;&#33402;&#26415;&#23478;&#39118;&#26684;&#20223;&#21046;&#30340;&#20316;&#21697;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#35757;&#32451;&#38598;&#19982;&#31867;&#20284;&#39118;&#26684;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14998v3 Announce Type: replace-cross  Abstract: Previous research has shown that Artificial Intelligence is capable of distinguishing between authentic paintings by a given artist and human-made forgeries with remarkable accuracy, provided sufficient training. However, with the limited amount of existing known forgeries, augmentation methods for forgery detection are highly desirable. In this work, we examine the potential of incorporating synthetic artworks into training datasets to enhance the performance of forgery detection. Our investigation focuses on paintings by Vincent van Gogh, for which we release the first dataset specialized for forgery detection. To reinforce our results, we conduct the same analyses on the artists Amedeo Modigliani and Raphael. We train a classifier to distinguish original artworks from forgeries. For this, we use human-made forgeries and imitations in the style of well-known artists and augment our training sets with images in a similar style
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DS-Prover&#30340;&#21160;&#24577;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;&#30340;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#30830;&#23450;&#24212;&#29992;&#20110;&#25193;&#23637;&#24403;&#21069;&#30446;&#26631;&#30340;&#31574;&#30053;&#25968;&#37327;&#65292;&#24182;&#35843;&#25972;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#20351;&#35777;&#26126;&#25628;&#32034;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23558;&#31616;&#21270;&#21644;&#37325;&#20889;&#31574;&#30053;&#19982;&#22810;&#20010;&#21069;&#25552;&#36827;&#34892;&#20998;&#35299;&#12290;</title><link>https://arxiv.org/abs/2312.14188</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#21160;&#24577;&#25277;&#26679;&#26041;&#27861;&#22686;&#24378;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DS-Prover&#30340;&#21160;&#24577;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;&#30340;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#30830;&#23450;&#24212;&#29992;&#20110;&#25193;&#23637;&#24403;&#21069;&#30446;&#26631;&#30340;&#31574;&#30053;&#25968;&#37327;&#65292;&#24182;&#35843;&#25972;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#20351;&#35777;&#26126;&#25628;&#32034;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23558;&#31616;&#21270;&#21644;&#37325;&#20889;&#31574;&#30053;&#19982;&#22810;&#20010;&#21069;&#25552;&#36827;&#34892;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#29702;&#35777;&#26126;&#26159;&#25968;&#23398;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20132;&#20114;&#24335;&#23450;&#29702;&#35777;&#26126;&#22120;&#65288;ITPs&#65289;&#22914;Lean&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23558;LLMs&#21644;ITPs&#38598;&#25104;&#20197;&#33258;&#21160;&#21270;&#23450;&#29702;&#35777;&#26126;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;LLM&#29983;&#25104;&#35777;&#26126;&#27493;&#39588;&#65288;&#31574;&#30053;&#65289;&#65292;&#32780;ITP&#26816;&#26597;&#36825;&#20123;&#31574;&#30053;&#22312;&#24403;&#21069;&#30446;&#26631;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#20004;&#20010;&#31995;&#32479;&#20849;&#21516;&#23436;&#25104;&#35777;&#26126;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DS-Prover&#65292;&#19968;&#31181;&#29992;&#20110;&#23450;&#29702;&#35777;&#26126;&#30340;&#20840;&#26032;&#21160;&#24577;&#25277;&#26679;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#30830;&#23450;&#35201;&#24212;&#29992;&#20110;&#25193;&#23637;&#24403;&#21069;&#30446;&#26631;&#30340;&#31574;&#30053;&#25968;&#37327;&#65292;&#32771;&#34385;&#21040;&#21097;&#20313;&#26102;&#38388;&#19982;&#24635;&#20998;&#37197;&#26102;&#38388;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#20174;&#32780;&#20351;&#35777;&#26126;&#25628;&#32034;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#35843;&#25972;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23558;&#31616;&#21270;&#21644;&#37325;&#20889;&#31574;&#30053;&#19982;&#22810;&#20010;&#21069;&#25552;&#36827;&#34892;&#20998;&#35299;&#26469;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14188v2 Announce Type: replace  Abstract: Theorem proving is a fundamental task in mathematics. With the advent of large language models (LLMs) and interactive theorem provers (ITPs) like Lean, there has been growing interest in integrating LLMs and ITPs to automate theorem proving. In this approach, the LLM generates proof steps (tactics), and the ITP checks the applicability of the tactics at the current goal. The two systems work together to complete the proof. In this paper, we introduce DS-Prover, a novel dynamic sampling method for theorem proving. This method dynamically determines the number of tactics to apply to expand the current goal, taking into account the remaining time compared to the total allocated time for proving a theorem. This makes the proof search process more efficient by adjusting the balance between exploration and exploitation as time passes. We also augment the training dataset by decomposing simplification and rewrite tactics with multiple premi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#38480;&#21046;&#25509;&#35302;&#36861;&#36394;&#30340;&#37096;&#32626;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20102;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#21457;&#24067;&#27599;&#20010;&#39118;&#38505;&#20998;&#25968;&#26102;&#20445;&#25252;&#20010;&#20307;&#20581;&#24247;&#29366;&#20917;&#30340;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2312.11581</link><description>&lt;p&gt;
&#20445;&#25252;&#24744;&#30340;&#20998;&#25968;&#65306;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;&#25509;&#35302;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Protect Your Score: Contact Tracing With Differential Privacy Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11581
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#38480;&#21046;&#25509;&#35302;&#36861;&#36394;&#30340;&#37096;&#32626;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20102;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#21457;&#24067;&#27599;&#20010;&#39118;&#38505;&#20998;&#25968;&#26102;&#20445;&#25252;&#20010;&#20307;&#20581;&#24247;&#29366;&#20917;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2020&#24180;&#21644;2021&#24180;&#30340;&#27969;&#34892;&#30149;&#23545;&#32463;&#27982;&#21644;&#31038;&#20250;&#20135;&#29983;&#20102;&#24040;&#22823;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#21487;&#20197;&#22312;&#26089;&#26399;&#36943;&#21046;&#30149;&#27602;&#26041;&#38754;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#22312;&#26356;&#26377;&#25928;&#30340;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#30340;&#38544;&#31169;&#38382;&#39064;&#38459;&#30861;&#20102;&#20854;&#37096;&#32626;&#12290;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#30340;&#26412;&#36136;&#22312;&#20110;&#20256;&#36882;&#19968;&#20010;&#39118;&#38505;&#20998;&#25968;&#30340;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#24688;&#24688;&#26159;&#23558;&#36825;&#20010;&#20998;&#25968;&#20256;&#36882;&#32473;&#29992;&#25143;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#20998;&#25968;&#26469;&#35780;&#20272;&#20010;&#20307;&#30340;&#31169;&#20154;&#20581;&#24247;&#29366;&#20917;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#29616;&#23454;&#30340;&#25915;&#20987;&#22330;&#26223;&#65292;&#24182;&#38024;&#23545;&#36825;&#31181;&#25915;&#20987;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#20004;&#20010;&#26368;&#24120;&#29992;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;COVID19&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20102;&#21331;&#36234;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36924;&#30495;&#30340;&#27979;&#35797;&#22330;&#26223;&#20013;&#65292;&#21516;&#26102;&#21457;&#24067;&#27599;&#20010;&#39118;&#38505;&#20998;&#25968;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11581v2 Announce Type: replace-cross  Abstract: The pandemic in 2020 and 2021 had enormous economic and societal consequences, and studies show that contact tracing algorithms can be key in the early containment of the virus. While large strides have been made towards more effective contact tracing algorithms, we argue that privacy concerns currently hold deployment back. The essence of a contact tracing algorithm constitutes the communication of a risk score. Yet, it is precisely the communication and release of this score to a user that an adversary can leverage to gauge the private health status of an individual. We pinpoint a realistic attack scenario and propose a contact tracing algorithm with differential privacy guarantees against this attack. The algorithm is tested on the two most widely used agent-based COVID19 simulators and demonstrates superior performance in a wide range of settings. Especially for realistic test scenarios and while releasing each risk score w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25233;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.11560</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#21457;&#29616;&#65306;&#20851;&#20110;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25233;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#65292;&#33258;&#21457;&#29616;&#21463;&#21040;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#20551;&#35774;&#65292;&#21363;&#22312;&#35268;&#27169;&#25193;&#22823;&#30340;&#36807;&#31243;&#20013;&#39640;&#24230;&#20419;&#36827;&#24615;&#33021;&#30340;&#22240;&#32032;&#65306;&#20943;&#23569;&#21482;&#33021;&#19982;&#29305;&#23450;&#29305;&#24449;&#24418;&#25104;&#19968;&#23545;&#19968;&#20851;&#31995;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#12290;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#24448;&#24448;&#26356;&#31232;&#30095;&#65292;&#24182;&#23545;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#24605;&#36335;&#26469;&#35782;&#21035;&#21644;&#25233;&#21046;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#26159;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#27809;&#26377;&#32479;&#19968;&#30340;&#23450;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#31616;&#21333;&#22320;&#31105;&#27490;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#24182;&#19981;&#33021;&#20419;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#24847;&#24605;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#33258;&#21457;&#29616;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#24320;&#20102;&#20851;&#20110;&#31215;&#26497;&#25233;&#21046;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11560v2 Announce Type: replace-cross  Abstract: Recently, emergence has received widespread attention from the research community along with the success of large language models. Different from the literature, we hypothesize a key factor that highly promotes the performance during the increase of scale: the reduction of monosemantic neurons that can only form one-to-one correlations with specific features. Monosemantic neurons tend to be sparser and have negative impacts on the performance in large models. Inspired by this insight, we propose an intuitive idea to identify monosemantic neurons and inhibit them. However, achieving this goal is a non-trivial task as there is no unified quantitative evaluation metric and simply banning monosemantic neurons does not promote polysemanticity in neural networks. Therefore, we propose to learn from emergence and present a study on proactively inhibiting the monosemantic neurons in this paper. More specifically, we first propose a new
&lt;/p&gt;</description></item><item><title>GINN-LP&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21457;&#29616;&#22810;&#20803;Laurent&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#24418;&#24335;&#21644;&#31995;&#25968;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24130;&#39033;&#36924;&#36817;&#22359;&#8221;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#22359;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22686;&#38271;&#31574;&#30053;&#21644;&#31232;&#30095;&#27491;&#21017;&#21270;&#26469;&#20248;&#21270;&#26041;&#31243;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2312.10913</link><description>&lt;p&gt;
GINN-LP&#65306;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#22810;&#20803;Laurent&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GINN-LP: A Growing Interpretable Neural Network for Discovering Multivariate Laurent Polynomial Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10913
&lt;/p&gt;
&lt;p&gt;
GINN-LP&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21457;&#29616;&#22810;&#20803;Laurent&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#24418;&#24335;&#21644;&#31995;&#25968;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24130;&#39033;&#36924;&#36817;&#22359;&#8221;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#22359;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22686;&#38271;&#31574;&#30053;&#21644;&#31232;&#30095;&#27491;&#21017;&#21270;&#26469;&#20248;&#21270;&#26041;&#31243;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#20010;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#65292;&#19981;&#20250;&#20135;&#29983;&#23558;&#36755;&#20837;&#21644;&#36755;&#20986;&#36830;&#25509;&#36215;&#26469;&#30340;&#21487;&#35299;&#37322;&#24615;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#36825;&#31181;&#21487;&#35299;&#37322;&#24615;&#20989;&#25968;&#30340;&#33021;&#21147;&#26159;&#21487;&#21462;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GINN-LP&#65292;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#26041;&#31243;&#30340;&#24418;&#24335;&#21644;&#31995;&#25968;&#65292;&#24403;&#20551;&#35774;&#26041;&#31243;&#30340;&#24418;&#24335;&#26159;&#22810;&#20803;Laurent&#22810;&#39033;&#24335;&#26102;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#22359;&#65292;&#21517;&#20026;&#8220;&#24130;&#39033;&#36924;&#36817;&#22359;&#8221;&#65292;&#30001;&#23545;&#25968;&#21644;&#25351;&#25968;&#28608;&#27963;&#20989;&#25968;&#32452;&#25104;&#26469;&#23454;&#29616;&#30340;&#12290;GINN-LP&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#65292;&#21487;&#20197;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#22686;&#38271;&#31574;&#30053;&#65292;&#33021;&#22815;&#25214;&#21040;&#20195;&#34920;&#25968;&#25454;&#30340;Laurent&#22810;&#39033;&#24335;&#20013;&#30340;&#21512;&#36866;&#39033;&#25968;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#20248;&#21270;&#26041;&#31243;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10913v2 Announce Type: replace-cross  Abstract: Traditional machine learning is generally treated as a black-box optimization problem and does not typically produce interpretable functions that connect inputs and outputs. However, the ability to discover such interpretable functions is desirable. In this work, we propose GINN-LP, an interpretable neural network to discover the form and coefficients of the underlying equation of a dataset, when the equation is assumed to take the form of a multivariate Laurent Polynomial. This is facilitated by a new type of interpretable neural network block, named the "power-term approximator block", consisting of logarithmic and exponential activation functions. GINN-LP is end-to-end differentiable, making it possible to use backpropagation for training. We propose a neural network growth strategy that will enable finding the suitable number of terms in the Laurent polynomial that represents the data, along with sparsity regularization to 
&lt;/p&gt;</description></item><item><title>CoT-Influx&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#19978;&#19979;&#25991;&#20462;&#21098;&#26469;&#25552;&#21319;LLM&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26377;&#25928;&#21644;&#31616;&#26126;&#30340;&#31034;&#20363;&#36755;&#20837;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.08901</link><description>&lt;p&gt;
&#26356;&#23569;&#21363;&#26356;&#22810;&#65306;&#36890;&#36807;&#24378;&#21270;&#19978;&#19979;&#25991;&#20462;&#21098;&#25552;&#21319;LLM&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08901
&lt;/p&gt;
&lt;p&gt;
CoT-Influx&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#19978;&#19979;&#25991;&#20462;&#21098;&#26469;&#25552;&#21319;LLM&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26377;&#25928;&#21644;&#31616;&#26126;&#30340;&#31034;&#20363;&#36755;&#20837;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoT-Influx&#65292;&#19968;&#31181;&#23558;&#23569;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#23398;&#20064;&#25512;&#21521;&#26497;&#38480;&#20197;&#25913;&#21892;LLM&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#30001;&#20110;&#35266;&#23519;&#21040;&#22312;&#25552;&#31034;&#20013;&#28155;&#21152;&#26356;&#31616;&#26126;&#30340;CoT&#31034;&#20363;&#21487;&#20197;&#25552;&#39640;LLM&#25512;&#29702;&#34920;&#29616;&#65292;CoT-Influx&#37319;&#29992;&#20102;&#19968;&#31181;&#20174;&#31895;&#31961;&#21040;&#31934;&#32454;&#30340;&#20462;&#21098;&#22120;&#26469;&#26368;&#22823;&#21270;&#26377;&#25928;&#21644;&#31616;&#26126;&#30340;CoT&#31034;&#20363;&#30340;&#36755;&#20837;&#12290;&#20462;&#21098;&#22120;&#39318;&#20808;&#36873;&#25321;&#23613;&#21487;&#33021;&#22810;&#30340;&#20851;&#38190;CoT&#31034;&#20363;&#65292;&#28982;&#21518;&#20462;&#21098;&#26080;&#20851;&#32039;&#35201;&#30340;&#26631;&#35760;&#20197;&#36866;&#24212;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#20351;&#29992;&#38590;&#24230;&#32423;&#21035;&#21644;&#25512;&#29702;&#27493;&#39588;&#22810;&#26679;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20462;&#21098;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#19987;&#38376;&#38024;&#23545;&#25968;&#23398;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#32467;&#26524;&#26159;&#65292;&#36890;&#36807;&#22312;&#20196;&#29260;&#20013;&#21551;&#29992;&#21452;&#20493;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;CoT&#31034;&#20363;&#65292;CoT-Influx&#22312;&#21508;&#31181;&#25552;&#31034;&#22522;&#20934;&#26041;&#27861;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08901v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have shown impressive capabilities, yet they still struggle with math reasoning. In this work, we propose CoT-Influx, a novel approach that pushes the boundary of few-shot Chain-of-Thoughts (CoT) learning to improve LLM mathematical reasoning. Motivated by the observation that adding more concise CoT examples in the prompt can improve LLM reasoning performance, CoT-Influx employs a coarse-to-fine pruner to maximize the input of effective and concise CoT examples. The pruner first selects as many crucial CoT examples as possible and then prunes unimportant tokens to fit the context window. A math reasoning dataset with diverse difficulty levels and reasoning steps is used to train the pruner, along with a math-specialized reinforcement learning approach. As a result, by enabling more CoT examples with double the context window size in tokens, CoT-Influx significantly outperforms various prompting bas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#36753;&#21160;&#20316;&#36335;&#24452;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#20284;&#24230;&#39640;&#65292;&#24182;&#19988;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#23454;&#29616;&#39044;&#23450;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.08724</link><description>&lt;p&gt;
&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Personalized Path Recourse for Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#36753;&#21160;&#20316;&#36335;&#24452;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#20284;&#24230;&#39640;&#65292;&#24182;&#19988;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#23454;&#29616;&#39044;&#23450;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#34917;&#25937;&#36335;&#24452;&#12290;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#32534;&#36753;&#32473;&#23450;&#30340;&#21160;&#20316;&#36335;&#24452;&#20197;&#36798;&#21040;&#26399;&#26395;&#30340;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#27604;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65289;&#65292;&#21516;&#26102;&#30830;&#20445;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#39640;&#24230;&#30456;&#20284;&#24182;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#12290;&#20010;&#24615;&#21270;&#26159;&#25351;&#26032;&#36335;&#24452;&#22312;&#20174;&#31574;&#30053;&#20989;&#25968;&#20013;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#34892;&#20026;&#27169;&#24335;&#26041;&#38754;&#30340;&#23450;&#21046;&#31243;&#24230;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#34917;&#25937;&#20195;&#29702;&#26469;&#29983;&#25104;&#36825;&#26679;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#65292;&#36825;&#20123;&#36335;&#24452;&#26159;&#20351;&#29992;&#32771;&#34385;&#30446;&#26631;&#12289;&#30456;&#20284;&#24615;&#21644;&#20010;&#24615;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#33719;&#24471;&#30340;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#20197;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#24207;&#21015;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#36798;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08724v2 Announce Type: replace-cross  Abstract: This paper introduces Personalized Path Recourse, a novel method that generates recourse paths for a reinforcement learning agent. The goal is to edit a given path of actions to achieve desired goals (e.g., better outcomes compared to the agent's original path) while ensuring a high similarity to the agent's original paths and being personalized to the agent. Personalization refers to the extent to which the new path is tailored to the agent's observed behavior patterns from their policy function. We train a personalized recourse agent to generate such personalized paths, which are obtained using reward functions that consider the goal, similarity, and personalization. The proposed method is applicable to both reinforcement learning and supervised learning settings for correcting or improving sequences of actions or sequences of data to achieve a pre-determined goal. The method is evaluated in various settings. Experiments show
&lt;/p&gt;</description></item><item><title>InstructBooth&#26159;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#29305;&#23450;&#23545;&#35937;&#22270;&#29255;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#24182;&#21033;&#29992;&#22686;&#24378;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.03011</link><description>&lt;p&gt;
InstructBooth: &#25351;&#20196;&#36319;&#38543;&#30340;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
InstructBooth: Instruction-following Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03011
&lt;/p&gt;
&lt;p&gt;
InstructBooth&#26159;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#29305;&#23450;&#23545;&#35937;&#22270;&#29255;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#24182;&#21033;&#29992;&#22686;&#24378;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03011v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#20195;&#20132;&#21449;&#20844;&#21578; &#25688;&#35201;: &#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#29305;&#23450;&#23545;&#35937;&#22270;&#29255;&#36827;&#34892;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#30740;&#31350;&#24050;&#32463;&#22312;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;&#29983;&#25104;&#20013;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#22312;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#36807;&#24230;&#25311;&#21512;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#22270;&#20687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InstructBooth&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#32780;&#19981;&#29306;&#29298;&#20010;&#24615;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#21807;&#19968;&#26631;&#35782;&#31526;&#23558;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19982;&#23569;&#37327;&#29305;&#23450;&#23545;&#35937;&#22270;&#29255;&#20010;&#24615;&#21270;&#12290;&#20010;&#24615;&#21270;&#23436;&#25104;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#23545;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26368;&#22823;&#21270;&#24230;&#37327;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#30340;&#22870;&#21169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#21152;&#36825;&#20004;&#20010;&#36807;&#31243;&#20043;&#38388;&#21327;&#21516;&#20316;&#29992;&#30340;&#34917;&#20805;&#25216;&#26415;&#12290;&#19982;&#29616;&#26377;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20010;&#24615;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03011v2 Announce Type: replace-cross  Abstract: Personalizing text-to-image models using a limited set of images for a specific object has been explored in subject-specific image generation. However, existing methods often face challenges in aligning with text prompts due to overfitting to the limited training images. In this work, we introduce InstructBooth, a novel method designed to enhance image-text alignment in personalized text-to-image models without sacrificing the personalization ability. Our approach first personalizes text-to-image models with a small number of subject-specific images using a unique identifier. After personalization, we fine-tune personalized text-to-image models using reinforcement learning to maximize a reward that quantifies image-text alignment. Additionally, we propose complementary techniques to increase the synergy between these two processes. Our method demonstrates superior image-text alignment compared to existing baselines, while maint
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimelyGPT&#30340;&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#21487;&#25512;&#24191;&#30340;&#20301;&#32622;&#23884;&#20837;&#21644;&#24490;&#29615;&#27880;&#24847;&#21147;&#20197;&#21450;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#26377;&#25928;&#22320;&#25429;&#25417;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.00817</link><description>&lt;p&gt;
&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#29992;&#20110;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00817
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimelyGPT&#30340;&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#21487;&#25512;&#24191;&#30340;&#20301;&#32622;&#23884;&#20837;&#21644;&#24490;&#29615;&#27880;&#24847;&#21147;&#20197;&#21450;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#26377;&#25928;&#22320;&#25429;&#25417;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#65292;&#22914;BERT&#21644;GPT&#65292;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;PTMs&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#21457;&#23637;&#28382;&#21518;&#12290;&#36825;&#20984;&#26174;&#20102;&#29616;&#26377;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#25429;&#25417;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21363;&#26102;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;TimelyGPT&#65289;&#12290;TimelyGPT&#37319;&#29992;&#21487;&#25512;&#24191;&#20301;&#32622;&#65288;xPos&#65289;&#23884;&#20837;&#23558;&#36235;&#21183;&#21644;&#21608;&#26399;&#27169;&#24335;&#32534;&#30721;&#21040;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20013;&#12290;&#23427;&#36824;&#38598;&#25104;&#20102;&#24490;&#29615;&#27880;&#24847;&#21147;&#21644;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TimelyGPT&#22312;&#24314;&#27169;&#36830;&#32493;&#30417;&#27979;&#30340;&#29983;&#29289;&#20449;&#21495;&#21644;&#32463;&#24120;&#20986;&#29616;&#22312;&#32437;&#21521;&#30005;&#30913;&#27874;&#39046;&#22495;&#20013;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00817v2 Announce Type: replace-cross  Abstract: Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success in Natural Language Processing and Computer Vision domains. However, the development of PTMs on time-series data is lagging behind. This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale data and ability to capture long-term temporal dependencies. In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT). TimelyGPT employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies. Our experiments show that TimelyGPT excels in modeling continuously monitored biosignals and irregularly-sampled time series data commonly observed in longitudinal electro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#20934;&#30830;&#24615;-&#31283;&#23450;&#24615;&#25351;&#25968;&#65288;ASI&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#32508;&#21512;&#32771;&#34385;&#20934;&#30830;&#24230;&#21644;&#31283;&#23450;&#24615;&#30340;&#23450;&#37327;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;ASI&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#21487;&#35270;&#21270;ASI&#12289;&#24179;&#22343;&#20934;&#30830;&#24230;&#21644;&#21464;&#24322;&#31995;&#25968;&#30340;3D&#26354;&#38754;&#27169;&#22411;&#12290;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23450;&#37327;&#22522;&#20934;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.15332</link><description>&lt;p&gt;
ASI:&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;-&#31283;&#23450;&#24615;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
ASI: Accuracy-Stability Index for Evaluating Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15332
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#20934;&#30830;&#24615;-&#31283;&#23450;&#24615;&#25351;&#25968;&#65288;ASI&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#32508;&#21512;&#32771;&#34385;&#20934;&#30830;&#24230;&#21644;&#31283;&#23450;&#24615;&#30340;&#23450;&#37327;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;ASI&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#21487;&#35270;&#21270;ASI&#12289;&#24179;&#22343;&#20934;&#30830;&#24230;&#21644;&#21464;&#24322;&#31995;&#25968;&#30340;3D&#26354;&#38754;&#27169;&#22411;&#12290;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23450;&#37327;&#22522;&#20934;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#27169;&#22411;&#30340;&#19981;&#26029;&#24341;&#20837;&#20351;&#24471;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#35780;&#20272;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24378;&#35843;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#24573;&#35270;&#20102;&#31283;&#23450;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#20934;&#30830;&#24615;-&#31283;&#23450;&#24615;&#25351;&#25968;&#65288;ASI&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#32508;&#21512;&#32771;&#34385;&#20934;&#30830;&#24230;&#21644;&#31283;&#23450;&#24615;&#30340;&#23450;&#37327;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;ASI&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#21487;&#35270;&#21270;ASI&#12289;&#24179;&#22343;&#20934;&#30830;&#24230;&#21644;&#21464;&#24322;&#31995;&#25968;&#30340;3D&#26354;&#38754;&#27169;&#22411;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23450;&#37327;&#22522;&#20934;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#25991;&#31456;&#26368;&#21518;&#36824;&#23545;&#28508;&#22312;&#24369;&#28857;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15332v2 Announce Type: replace-cross  Abstract: In the context of deep learning research, where model introductions continually occur, the need for effective and efficient evaluation remains paramount. Existing methods often emphasize accuracy metrics, overlooking stability. To address this, the paper introduces the Accuracy-Stability Index (ASI), a quantitative measure incorporating both accuracy and stability for assessing deep learning models. Experimental results demonstrate the application of ASI, and a 3D surface model is presented for visualizing ASI, mean accuracy, and coefficient of variation. This paper addresses the important issue of quantitative benchmarking metrics for deep learning models, providing a new approach for accurately evaluating accuracy and stability of deep learning models. The paper concludes with discussions on potential weaknesses and outlines future research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20179;&#24211;&#25366;&#25496;&#21644;&#25991;&#26412;&#20998;&#26512;&#30340;&#26041;&#24335;&#65292;&#23545;Hugging Face&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28436;&#21270;&#21644;&#32500;&#25252;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;Hugging Face&#30340;&#25972;&#20307;&#22686;&#38271;&#21644;&#21463;&#27426;&#36814;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;ML&#39046;&#22495;&#12289;&#26694;&#26550;&#20351;&#29992;&#12289;&#20316;&#32773;&#20998;&#32452;&#31561;&#26041;&#38754;&#30340;&#36235;&#21183;&#65292;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#24320;&#21457;&#32773;&#31038;&#21306;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20027;&#39064;&#21644;&#35265;&#35299;&#20197;&#21450;&#27169;&#22411;&#30340;&#32500;&#25252;&#29366;&#24577;&#21644;&#28436;&#21270;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2311.13380</link><description>&lt;p&gt;
&#20998;&#26512;Hugging Face&#19978;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28436;&#21270;&#21644;&#32500;&#25252;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Evolution and Maintenance of ML Models on Hugging Face
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20179;&#24211;&#25366;&#25496;&#21644;&#25991;&#26412;&#20998;&#26512;&#30340;&#26041;&#24335;&#65292;&#23545;Hugging Face&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28436;&#21270;&#21644;&#32500;&#25252;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;Hugging Face&#30340;&#25972;&#20307;&#22686;&#38271;&#21644;&#21463;&#27426;&#36814;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;ML&#39046;&#22495;&#12289;&#26694;&#26550;&#20351;&#29992;&#12289;&#20316;&#32773;&#20998;&#32452;&#31561;&#26041;&#38754;&#30340;&#36235;&#21183;&#65292;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#24320;&#21457;&#32773;&#31038;&#21306;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20027;&#39064;&#21644;&#35265;&#35299;&#20197;&#21450;&#27169;&#22411;&#30340;&#32500;&#25252;&#29366;&#24577;&#21644;&#28436;&#21270;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hugging Face&#65288;HF&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24320;&#21457;&#21644;&#20998;&#20139;&#30340;&#37325;&#35201;&#24179;&#21488;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;HF Hub API&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#23545;&#36229;&#36807;380,000&#20010;&#27169;&#22411;&#36827;&#34892;&#20179;&#24211;&#25366;&#25496;&#65292;&#26088;&#22312;&#25506;&#32034;HF&#19978;&#25176;&#31649;&#30340;&#27169;&#22411;&#30340;&#31038;&#21306;&#21442;&#19982;&#12289;&#28436;&#21270;&#21644;&#32500;&#25252;&#31561;&#26041;&#38754;&#65292;&#36825;&#20123;&#26041;&#38754;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#23578;&#26410;&#20840;&#38754;&#25506;&#35752;&#12290;&#25105;&#20204;&#39318;&#20808;&#23457;&#26597;&#20102;HF&#30340;&#25972;&#20307;&#22686;&#38271;&#21644;&#21463;&#27426;&#36814;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;ML&#39046;&#22495;&#12289;&#26694;&#26550;&#20351;&#29992;&#12289;&#20316;&#32773;&#20998;&#32452;&#20197;&#21450;&#26631;&#31614;&#21644;&#25968;&#25454;&#38598;&#30340;&#28436;&#21270;&#36235;&#21183;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#21345;&#29255;&#25551;&#36848;&#30340;&#25991;&#26412;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#35797;&#22270;&#30830;&#23450;&#24320;&#21457;&#32773;&#31038;&#21306;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20027;&#39064;&#21644;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#28085;&#30422;&#20102;&#27169;&#22411;&#32500;&#25252;&#26041;&#38754;&#65292;&#22312;&#36825;&#26041;&#38754;&#25105;&#20204;&#35780;&#20272;&#20102;ML&#27169;&#22411;&#30340;&#32500;&#25252;&#29366;&#24577;&#65292;&#23558;&#25552;&#20132;&#28040;&#24687;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65288;&#26657;&#27491;&#24615;&#12289;&#23436;&#21892;&#24615;&#21644;&#36866;&#24212;&#24615;&#65289;&#65292;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#28436;&#21270;&#24773;&#20917;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13380v2 Announce Type: cross  Abstract: Hugging Face (HF) has established itself as a crucial platform for the development and sharing of machine learning (ML) models. This repository mining study, which delves into more than 380,000 models using data gathered via the HF Hub API, aims to explore the community engagement, evolution, and maintenance around models hosted on HF, aspects that have yet to be comprehensively explored in the literature. We first examine the overall growth and popularity of HF, uncovering trends in ML domains, framework usage, authors grouping and the evolution of tags and datasets used. Through text analysis of model card descriptions, we also seek to identify prevalent themes and insights within the developer community. Our investigation further extends to the maintenance aspects of models, where we evaluate the maintenance status of ML models, classify commit messages into various categories (corrective, perfective, and adaptive), analyze the evol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#24066;&#22330;&#30340;&#35843;&#33410;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;AI&#20013;&#20171;&#24179;&#21488;&#38754;&#20020;&#30340;&#24179;&#21488;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#19994;&#30028;&#30340;&#30456;&#20851;&#23454;&#36341;&#65292;&#21253;&#25324;&#35768;&#21487;&#12289;&#35775;&#38382;&#21644;&#20351;&#29992;&#38480;&#21046;&#12289;&#33258;&#21160;&#20869;&#23481;&#35843;&#33410;&#20197;&#21450;&#20844;&#24320;&#25919;&#31574;&#21046;&#23450;&#12290;</title><link>https://arxiv.org/abs/2311.12573</link><description>&lt;p&gt;
&#27169;&#22411;&#24066;&#22330;&#30340;&#35843;&#33410;: AI&#20013;&#20171;&#24179;&#21488;&#30340;&#24179;&#21488;&#27835;&#29702;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#24066;&#22330;&#30340;&#35843;&#33410;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;AI&#20013;&#20171;&#24179;&#21488;&#38754;&#20020;&#30340;&#24179;&#21488;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#19994;&#30028;&#30340;&#30456;&#20851;&#23454;&#36341;&#65292;&#21253;&#25324;&#35768;&#21487;&#12289;&#35775;&#38382;&#21644;&#20351;&#29992;&#38480;&#21046;&#12289;&#33258;&#21160;&#20869;&#23481;&#35843;&#33410;&#20197;&#21450;&#20844;&#24320;&#25919;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv: 2311.12573v2 &#20844;&#21578;&#31867;&#22411;: replace-cross &#25688;&#35201;: AI&#24320;&#21457;&#31038;&#21306;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#25176;&#31649;&#20013;&#20171;&#24179;&#21488;&#65292;&#22914;Hugging Face&#65292;&#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#20415;&#25463;&#35775;&#38382;&#12290;&#36825;&#20123;&#27169;&#22411;&#24066;&#22330;&#38477;&#20302;&#20102;&#25104;&#21315;&#19978;&#19975;&#29992;&#25143;&#30340;&#25216;&#26415;&#37096;&#32626;&#38376;&#27099;&#65292;&#20294;&#20063;&#21487;&#33021;&#34987;&#29992;&#20110;&#35768;&#22810;&#28508;&#22312;&#26377;&#23475;&#21644;&#38750;&#27861;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;AI&#31995;&#32479;&#22914;&#20309;&#26082;&#33021;&#8220;&#21253;&#21547;&#8221;&#20869;&#23481;&#21448;&#33021;&#26159;&#24320;&#25918;&#24335;&#24037;&#20855;&#65292;&#20174;&#32780;&#25104;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#26840;&#25163;&#30340;&#24179;&#21488;&#27835;&#29702;&#25361;&#25112;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#20998;&#26512;&#27169;&#22411;&#24066;&#22330;&#22914;&#20309;&#31649;&#29702;&#27169;&#22411;&#65292;&#36825;&#20123;&#26696;&#20363;&#36328;&#36234;&#20102;&#19977;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#24179;&#21488;&#65292;&#21363;Hugging Face&#12289;GitHub&#21644;Civitai&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19994;&#30028;&#27491;&#22312;&#21046;&#23450;&#30340;&#37325;&#35201;&#65288;&#20294;&#20173;&#28982;&#26377;&#38480;&#65289;&#24212;&#23545;&#35843;&#33410;&#38656;&#27714;&#30340;&#20570;&#27861;&#65306;&#35768;&#21487;&#12289;&#35775;&#38382;&#21644;&#20351;&#29992;&#38480;&#21046;&#12289;&#33258;&#21160;&#20869;&#23481;&#35843;&#33410;&#20197;&#21450;&#20844;&#24320;&#25919;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12573v2 Announce Type: replace-cross  Abstract: The AI development community is increasingly making use of hosting intermediaries such as Hugging Face provide easy access to user-uploaded models and training data. These model marketplaces lower technical deployment barriers for hundreds of thousands of users, yet can be used in numerous potentially harmful and illegal ways. In this article, we explain ways in which AI systems, which can both `contain' content and be open-ended tools, present one of the trickiest platform governance challenges seen to date. We provide case studies of several incidents across three illustrative platforms -- Hugging Face, GitHub and Civitai -- to examine how model marketplaces moderate models. Building on this analysis, we outline important (and yet nevertheless limited) practices that industry has been developing to respond to moderation demands: licensing, access and use restrictions, automated content moderation, and open policy development.
&lt;/p&gt;</description></item><item><title>ClaSS&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#12290;</title><link>https://arxiv.org/abs/2310.20431</link><description>&lt;p&gt;
&#25552;&#21319;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#30340;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Raising the ClaSS of Streaming Time Series Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20431
&lt;/p&gt;
&lt;p&gt;
ClaSS&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#65292;&#26222;&#36941;&#23384;&#22312;&#30340;&#20256;&#24863;&#22120;&#21457;&#23556;&#39640;&#39057;&#25968;&#20540;&#27979;&#37327;&#27969;&#65292;&#21453;&#26144;&#20102;&#20154;&#31867;&#12289;&#21160;&#29289;&#12289;&#24037;&#19994;&#12289;&#21830;&#19994;&#21644;&#33258;&#28982;&#36807;&#31243;&#30340;&#29305;&#24615;&#12290;&#36825;&#20123;&#36807;&#31243;&#30340;&#21464;&#21270;&#65292;&#20363;&#22914;&#30001;&#22806;&#37096;&#20107;&#20214;&#25110;&#20869;&#37096;&#29366;&#24577;&#21464;&#21270;&#24341;&#36215;&#30340;&#65292;&#20250;&#34920;&#29616;&#20026;&#35760;&#24405;&#20449;&#21495;&#20013;&#30340;&#21464;&#21270;&#12290;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#65288;STSS&#65289;&#30340;&#20219;&#21153;&#26159;&#23558;&#27969;&#20998;&#21106;&#20026;&#23545;&#24212;&#20110;&#25152;&#35266;&#23519;&#30340;&#36807;&#31243;&#25110;&#23454;&#20307;&#29366;&#24577;&#30340;&#36830;&#32493;&#21487;&#21464;&#22823;&#23567;&#30340;&#20998;&#27573;&#12290;&#20998;&#21106;&#25805;&#20316;&#26412;&#36523;&#24517;&#39035;&#33021;&#22815;&#24212;&#23545;&#36755;&#20837;&#20449;&#21495;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ClaSS&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;STSS&#31639;&#27861;&#12290;ClaSS&#20351;&#29992;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#28508;&#22312;&#20998;&#21106;&#30340;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26469;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#65288;CPs&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#35780;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#22823;&#22411;&#22522;&#20934;&#21644;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26723;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.20431v2 Announce Type: replace-cross  Abstract: Ubiquitous sensors today emit high frequency streams of numerical measurements that reflect properties of human, animal, industrial, commercial, and natural processes. Shifts in such processes, e.g. caused by external events or internal state changes, manifest as changes in the recorded signals. The task of streaming time series segmentation (STSS) is to partition the stream into consecutive variable-sized segments that correspond to states of the observed processes or entities. The partition operation itself must in performance be able to cope with the input frequency of the signals. We introduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS. ClaSS assesses the homogeneity of potential partitions using self-supervised time series classification and applies statistical tests to detect significant change points (CPs). In our experimental evaluation using two large benchmarks and six real-world data archives, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;MDP&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#22686;&#24378;&#29615;&#22659;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#23618;&#25945;&#24072;&#26234;&#33021;&#20307;&#29983;&#25104;&#36866;&#24403;&#30340;&#35757;&#32451;&#29615;&#22659;&#65292;&#20197;&#20419;&#36827;&#23398;&#29983;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#33021;&#21147;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2310.00301</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#36712;&#36857;&#27169;&#22411;&#22686;&#24378;&#23618;&#27425;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Hierarchical Environment Design via Generative Trajectory Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;MDP&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#22686;&#24378;&#29615;&#22659;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#23618;&#25945;&#24072;&#26234;&#33021;&#20307;&#29983;&#25104;&#36866;&#24403;&#30340;&#35757;&#32451;&#29615;&#22659;&#65292;&#20197;&#20419;&#36827;&#23398;&#29983;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#33021;&#21147;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26159;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#29615;&#22659;&#35838;&#31243;&#30340;&#33539;&#20363;&#65292;&#20351;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#21457;&#23637;&#36890;&#29992;&#33021;&#21147;&#65292;&#21363;&#23454;&#29616;&#33391;&#22909;&#30340;&#38646;-shot&#36716;&#31227;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;UED&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23545;&#24320;&#25918;&#24335;&#26234;&#33021;&#20307;&#35757;&#32451;&#30340;&#29615;&#22659;&#36827;&#34892;&#38543;&#26426;&#29983;&#25104;&#65292;&#36825;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#23545;&#29983;&#25104;&#29615;&#22659;&#25968;&#37327;&#30340;&#38480;&#21046;&#26041;&#38754;&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#27425;MDP&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#36827;&#34892;&#29615;&#22659;&#35774;&#35745;&#12290;&#23427;&#30001;&#19968;&#20010;&#19978;&#23618;&#24378;&#21270;&#23398;&#20064;&#25945;&#24072;&#26234;&#33021;&#20307;&#21644;&#19968;&#20010;&#19979;&#23618;&#23398;&#29983;&#26234;&#33021;&#20307;&#30340;&#21512;&#20316;&#32452;&#25104;&#12290;&#24378;&#21270;&#23398;&#20064;&#25945;&#24072;&#21487;&#20197;&#21033;&#29992;&#20808;&#21069;&#21457;&#29616;&#30340;&#29615;&#22659;&#32467;&#26500;&#65292;&#36890;&#36807;&#35266;&#23519;&#23398;&#29983;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#34920;&#31034;&#22312;&#23398;&#29983;&#33021;&#21147;&#30340;&#21069;&#27839;&#29983;&#25104;&#36866;&#24403;&#30340;&#35757;&#32451;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00301v2 Announce Type: replace-cross  Abstract: Unsupervised Environment Design (UED) is a paradigm for automatically generating a curriculum of training environments, enabling agents trained in these environments to develop general capabilities, i.e., achieving good zero-shot transfer performance. However, existing UED approaches focus primarily on the random generation of environments for open-ended agent training. This is impractical in scenarios with limited resources, such as the constraints on the number of generated environments. In this paper, we introduce a hierarchical MDP framework for environment design under resource constraints. It consists of an upper-level RL teacher agent that generates suitable training environments for a lower-level student agent. The RL teacher can leverage previously discovered environment structures and generate environments at the frontier of the student's capabilities by observing the student policy's representation. Moreover, to redu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FWin&#30340;&#24555;&#36895;&#26412;&#22320;&#20840;&#23616;&#31383;&#21475;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;Informer&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#21152;&#36895;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#22312;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#19982;Softmax&#20840;&#27880;&#24847;&#21147;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2307.00493</link><description>&lt;p&gt;
&#20613;&#37324;&#21494;&#28151;&#21512;&#31383;&#21475;&#27880;&#24847;&#21147;&#65306;&#21152;&#36895;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;Informer&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FWin&#30340;&#24555;&#36895;&#26412;&#22320;&#20840;&#23616;&#31383;&#21475;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;Informer&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#21152;&#36895;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#22312;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#19982;Softmax&#20840;&#27880;&#24847;&#21147;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#26412;&#22320;&#20840;&#23616;&#31383;&#21475;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;Informer&#22312;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;&#31383;&#21475;&#27880;&#24847;&#21147;&#26159;&#23616;&#37096;&#30340;&#21644;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#33410;&#32422;&#65292;&#20294;&#23427;&#32570;&#20047;&#25429;&#33719;&#20840;&#23616;&#20196;&#29260;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36825;&#36890;&#36807;&#21518;&#32493;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#22359;&#36827;&#34892;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;FWin&#65292;&#19981;&#20381;&#36182;&#20110;Informer&#30340;ProbSparse&#27880;&#24847;&#21147;&#20013;&#30340;&#26597;&#35810;&#31232;&#30095;&#24615;&#20551;&#35774;&#21644;&#32463;&#39564;&#24615;&#36817;&#20284;&#12290;&#36890;&#36807;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FWin transformers&#21487;&#20197;&#25552;&#39640;Informer&#30340;&#25972;&#20307;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#23558;&#20854;&#25512;&#26029;&#36895;&#24230;&#21152;&#36895;40%&#33267;50%&#12290;&#25105;&#20204;&#36824;&#22312;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;FWin&#31867;&#22411;&#27880;&#24847;&#21147;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36890;&#36807;&#20174;Informer&#27169;&#22411;&#30340;&#20840;&#27880;&#24847;&#21147;&#23618;&#20013;&#25552;&#21462;&#30340;&#20851;&#38190;&#21521;&#37327;&#26469;&#36924;&#36817;&#29978;&#33267;&#32988;&#36807;&#22522;&#20110;Softmax&#20840;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00493v2 Announce Type: replace-cross  Abstract: We study a fast local-global window-based attention method to accelerate Informer for long sequence time-series forecasting. While window attention is local and a considerable computational saving, it lacks the ability to capture global token information which is compensated by a subsequent Fourier transform block. Our method, named FWin, does not rely on query sparsity hypothesis and an empirical approximation underlying the ProbSparse attention of Informer. Through experiments on univariate and multivariate datasets, we show that FWin transformers improve the overall prediction accuracies of Informer while accelerating its inference speeds by 40 to 50 %. We also show in a nonlinear regression model that a learned FWin type attention approaches or even outperforms softmax full attention based on key vectors extracted from an Informer model's full attention layer acting on time series data.
&lt;/p&gt;</description></item><item><title>&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#21363;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;</title><link>https://arxiv.org/abs/2303.03751</link><description>&lt;p&gt;
&#38646;&#38454;&#20248;&#21270;&#36935;&#21040;&#20154;&#24037;&#21453;&#39304;&#65306;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#23454;&#29616;&#21487;&#35777;&#26126;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.03751
&lt;/p&gt;
&lt;p&gt;
&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#21363;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#19968;&#20010;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#30340;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;-&#36825;&#31181;&#24773;&#20917;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#32463;&#24120;&#36935;&#21040;&#65292;&#29305;&#21035;&#26159;&#24403;&#20989;&#25968;&#30001;&#20154;&#31867;&#35780;&#21028;&#21592;&#35780;&#20272;&#26102;&#12290;&#36825;&#31181;&#25361;&#25112;&#21463;&#21040;&#20102;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#24037;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#29992;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#26469;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;ZO-RankSGD&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.03751v2 Announce Type: replace-cross  Abstract: In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle-a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. Such challenge is inspired from Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;cGAN&#30340;TheraGAN&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#24247;&#22797;&#27963;&#21160;&#30456;&#20851;&#30340;&#39640;&#32500;IMU&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#27963;&#21160;&#65292;&#31616;&#21270;&#20102;&#29983;&#25104;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;&#35299;&#20915;&#20256;&#32479;&#27963;&#21160;&#35782;&#21035;&#20998;&#31867;&#22120;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2302.07998</link><description>&lt;p&gt;
&#22522;&#20110;cGAN&#30340;&#22686;&#24378;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#39640;&#32500;IMU&#20256;&#24863;&#22120;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
cGAN-Based High Dimensional IMU Sensor Data Generation for Enhanced Human Activity Recognition in Therapeutic Activities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.07998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;cGAN&#30340;TheraGAN&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#24247;&#22797;&#27963;&#21160;&#30456;&#20851;&#30340;&#39640;&#32500;IMU&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#27963;&#21160;&#65292;&#31616;&#21270;&#20102;&#29983;&#25104;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;&#35299;&#20915;&#20256;&#32479;&#27963;&#21160;&#35782;&#21035;&#20998;&#31867;&#22120;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26159;&#24247;&#22797;&#12289;&#20581;&#24247;&#30417;&#27979;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#24212;&#29992;&#30340;&#26680;&#24515;&#25216;&#26415;&#12290;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#23588;&#20854;&#26159;IMU&#20256;&#24863;&#22120;&#65292;&#20197;&#30456;&#23545;&#36739;&#20302;&#30340;&#25104;&#26412;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20154;&#20307;&#36816;&#21160;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#12290;&#24320;&#21457;&#40065;&#26834;&#30340;&#27963;&#21160;&#35782;&#21035;&#20998;&#31867;&#22120;&#19968;&#30452;&#26159;&#30740;&#31350;&#20154;&#21592;&#20851;&#27880;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#26159;&#36890;&#24120;&#23384;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#24320;&#21457;&#28145;&#24230;&#20998;&#31867;&#22120;&#21464;&#24471;&#22256;&#38590;&#65292;&#26377;&#26102;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#32593;&#32476;TheraGAN&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#24247;&#22797;&#27963;&#21160;&#30456;&#20851;&#30340;IMU&#20449;&#21495;&#12290;&#29983;&#25104;&#30340;&#20449;&#21495;&#21253;&#25324;&#26469;&#33258;6&#20010;&#36890;&#36947;&#30340;IMU&#25968;&#25454;&#65292;&#21363;&#35282;&#36895;&#24230;&#21644;&#32447;&#24615;&#21152;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#31616;&#21333;&#27963;&#21160;&#31616;&#21270;&#20102;&#19981;&#21516;&#38271;&#24230;&#27963;&#21160;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#35780;&#20272;&#29983;&#25104;&#30340;&#20449;&#21495;&#65292;&#36827;&#34892;&#20102;&#20960;&#20010;&#23450;&#24615;&#23454;&#39564;&#21644;&#23450;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.07998v2 Announce Type: replace-cross  Abstract: Human activity recognition is a core technology for applications such as rehabilitation, health monitoring, and human-computer interactions. Wearable devices, especially IMU sensors, provide rich features of human movements at a reasonable cost, which can be leveraged in activity recognition. Developing a robust classifier for activity recognition has always been of interest to researchers. One major problem is that there is usually a deficit of training data, which makes developing deep classifiers difficult and sometimes impossible. In this work, a novel GAN network called TheraGAN was developed to generate IMU signals associated with rehabilitation activities. The generated signal comprises data from a 6-channel IMU, i.e., angular velocities and linear accelerations. Also, introducing simple activities simplified the generation process for activities of varying lengths. To evaluate the generated signals, several qualitative 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#25552;&#20379;&#30340;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#22810;&#26234;&#33021;&#20307;&#23376;&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#36827;&#34892;&#38598;&#20307;&#35843;&#25972;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#65292;&#24182;&#22312;&#35299;&#20915;&#22797;&#26434;&#30446;&#26631;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#27493;&#25968;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2302.04944</link><description>&lt;p&gt;
&#20351;&#29992;&#32473;&#23450;&#30340;&#23376;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#22797;&#26434;&#30340;&#22242;&#38431;&#21512;&#20316;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.04944
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#25552;&#20379;&#30340;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#22810;&#26234;&#33021;&#20307;&#23376;&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#36827;&#34892;&#38598;&#20307;&#35843;&#25972;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#65292;&#24182;&#22312;&#35299;&#20915;&#22797;&#26434;&#30446;&#26631;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#27493;&#25968;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22242;&#38431;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#21487;&#33021;&#38754;&#20020;&#35832;&#22914;&#22312;&#22823;&#22411;&#32852;&#21512;&#31574;&#30053;&#31354;&#38388;&#20013;&#25628;&#32034;&#31574;&#30053;&#21644;&#22240;&#20114;&#30456;&#36866;&#24212;&#32780;&#23548;&#33268;&#30340;&#38750;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#20419;&#36827;&#23545;&#22797;&#26434;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19987;&#23478;&#25552;&#20379;&#30340;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#22810;&#26234;&#33021;&#20307;&#23376;&#20219;&#21153;&#12290;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;&#23545;&#25972;&#20010;&#22242;&#38431;&#30340;&#23376;&#38598;&#36827;&#34892;&#35757;&#32451;&#20197;&#33719;&#21462;&#29305;&#23450;&#20110;&#23376;&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#23558;&#23376;&#22242;&#38431;&#21512;&#24182;&#24182;&#36801;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#65292;&#22312;&#37027;&#37324;&#20182;&#20204;&#30340;&#31574;&#30053;&#34987;&#38598;&#20307;&#35843;&#25972;&#20197;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#30446;&#26631;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35299;&#20915;&#22797;&#26434;&#30446;&#26631;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#27493;&#25968;&#65292;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#23376;&#20219;&#21153;&#20998;&#35299;&#30340;&#22825;&#30495;&#23454;&#29616;&#26041;&#27861;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.04944v2 Announce Type: replace-cross  Abstract: Training a team to complete a complex task via multi-agent reinforcement learning can be difficult due to challenges such as policy search in a large joint policy space, and non-stationarity caused by mutually adapting agents. To facilitate efficient learning of complex multi-agent tasks, we propose an approach which uses an expert-provided decomposition of a task into simpler multi-agent sub-tasks. In each sub-task, a subset of the entire team is trained to acquire sub-task-specific policies. The sub-teams are then merged and transferred to the target task, where their policies are collectively fine-tuned to solve the more complex target task. We show empirically that such approaches can greatly reduce the number of timesteps required to solve a complex target task relative to training from-scratch. However, we also identify and investigate two problems with naive implementations of approaches based on sub-task decomposition, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20462;&#25913;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65288;MPI&#65289;&#22312;&#39118;&#38505;&#25935;&#24863;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#24050;&#26377;&#32467;&#26524;&#19981;&#21516;&#30340;&#35777;&#26126;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2302.03811</link><description>&lt;p&gt;
&#20851;&#20110;&#39118;&#38505;&#25935;&#24863;&#25351;&#25968;&#25104;&#26412;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20462;&#25913;&#30340;&#31574;&#30053;&#36845;&#20195;&#30340;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Modified Policy Iteration in Risk Sensitive Exponential Cost Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03811
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20462;&#25913;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65288;MPI&#65289;&#22312;&#39118;&#38505;&#25935;&#24863;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#24050;&#26377;&#32467;&#26524;&#19981;&#21516;&#30340;&#35777;&#26126;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20462;&#25913;&#30340;&#31574;&#30053;&#36845;&#20195;&#65288;MPI&#65289;&#26159;&#19968;&#31181;&#23558;&#31574;&#30053;&#36845;&#20195;&#21644;&#20540;&#36845;&#20195;&#30456;&#32467;&#21512;&#30340;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#12290;MPI&#30340;&#25910;&#25947;&#24615;&#22312;&#25240;&#25187;&#21644;&#24179;&#22343;&#25104;&#26412;MDP&#30340;&#32972;&#26223;&#19979;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#25104;&#26412;&#39118;&#38505;&#25935;&#24863;MDP&#30340;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#23545;&#27169;&#22411;&#21442;&#25968;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;&#34429;&#28982;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;MDP&#24050;&#32463;&#23545;&#31574;&#30053;&#36845;&#20195;&#21644;&#20540;&#36845;&#20195;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;MPI&#21364;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;MPI&#20063;&#23545;&#39118;&#38505;&#25935;&#24863;&#38382;&#39064;&#25910;&#25947;&#12290;&#30001;&#20110;&#25351;&#25968;&#25104;&#26412;&#24418;&#24335;&#28041;&#21450;&#20056;&#27861;&#36125;&#23572;&#26364;&#26041;&#31243;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#19982;&#25240;&#25187;&#21644;&#39118;&#38505;&#20013;&#31435;&#24179;&#22343;&#25104;&#26412;&#38382;&#39064;&#20197;&#21450;&#39118;&#38505;&#25935;&#24863;&#20540;&#21644;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#19981;&#21516;&#30340;&#25910;&#25947;&#35777;&#26126;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03811v2 Announce Type: replace-cross  Abstract: Modified policy iteration (MPI) is a dynamic programming algorithm that combines elements of policy iteration and value iteration. The convergence of MPI has been well studied in the context of discounted and average-cost MDPs. In this work, we consider the exponential cost risk-sensitive MDP formulation, which is known to provide some robustness to model parameters. Although policy iteration and value iteration have been well studied in the context of risk sensitive MDPs, MPI is unexplored. We provide the first proof that MPI also converges for the risk-sensitive problem in the case of finite state and action spaces. Since the exponential cost formulation deals with the multiplicative Bellman equation, our main contribution is a convergence proof which is quite different than existing results for discounted and risk-neutral average-cost problems as well as risk sensitive value and policy iteration approaches. We conclude our a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#19981;&#21516;&#30340;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2210.02042</link><description>&lt;p&gt;
FedMT: &#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMT: Federated Learning with Mixed-type Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.02042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#19981;&#21516;&#30340;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20998;&#31867;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;&#32593;&#32476;&#65289;&#22312;&#22810;&#20010;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#22312;&#36825;&#20123;&#20013;&#24515;&#20043;&#38388;&#20132;&#25442;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#20256;&#32479;&#30340;FL&#35774;&#32622;&#20013;&#65292;&#36890;&#24120;&#22312;&#25152;&#26377;&#21442;&#19982;&#35757;&#32451;&#30340;&#20013;&#24515;&#20013;&#37319;&#29992;&#30456;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#36825;&#20010;&#38480;&#21046;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;FL&#30340;&#36866;&#29992;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#30142;&#30149;&#35786;&#26029;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#24456;&#21487;&#33021;&#22312;&#20020;&#24202;&#20013;&#24515;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#19982;&#20256;&#32479;FL&#30340;&#35774;&#32622;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;FL&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;FL&#65292;&#20854;&#20013;&#21508;&#20010;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#65292;&#20174;&#32780;&#23548;&#33268;&#20013;&#24515;&#38388;&#26631;&#31614;&#31354;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23545;&#20026;&#20256;&#32479;&#35774;&#32622;&#35774;&#35745;&#30340;&#29616;&#26377;FL&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.02042v3 Announce Type: replace-cross Abstract: In federated learning (FL), classifiers (e.g., deep networks) are trained on datasets from multiple centers without exchanging data across them, and thus improves sample efficiency. In the classical setting of FL, the same labeling criterion is usually employed across all centers being involved in training. This constraint greatly limits the applicability of FL. For example, standards used for disease diagnosis are more likely to be different across clinical centers, which mismatches the classical FL setting. In this paper, we consider an important yet under-explored setting of FL, namely FL with mixed-type labels where different labeling criteria can be employed by various centers, leading to inter-center label space differences and challenging existing FL methods designed for the classical setting. To effectively and efficiently train models with mixed-type labels, we propose a theory-guided and model-agnostic approach that ca
&lt;/p&gt;</description></item><item><title>PixTrack&#26159;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#29289;&#20307;&#23039;&#24577;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;NeRF&#27169;&#26495;&#21644;&#29305;&#24449;&#24230;&#37327;&#23545;&#40784;&#26041;&#27861;&#65292;&#33021;&#22815;&#31934;&#30830;&#36319;&#36394;&#29289;&#20307;&#30340;6DoF&#23039;&#24577;&#65292;&#32780;&#19988;&#26080;&#38656;&#25968;&#25454;&#27880;&#37322;&#25110;&#36712;&#36857;&#24179;&#28369;&#12290;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#31934;&#30830;&#12289;&#40065;&#26834;&#19988;&#26080;&#25238;&#21160;&#30340;&#29305;&#28857;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21487;&#29992;&#20110;&#22810;&#30446;&#26631;&#36319;&#36394;&#12290;</title><link>https://arxiv.org/abs/2209.03910</link><description>&lt;p&gt;
PixTrack&#65306;&#20351;&#29992;NeRF&#27169;&#26495;&#21644;&#29305;&#24449;&#24230;&#37327;&#23545;&#29289;&#20307;&#30340;6DoF&#23039;&#24577;&#36827;&#34892;&#31934;&#30830;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and Feature-metric Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.03910
&lt;/p&gt;
&lt;p&gt;
PixTrack&#26159;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#29289;&#20307;&#23039;&#24577;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;NeRF&#27169;&#26495;&#21644;&#29305;&#24449;&#24230;&#37327;&#23545;&#40784;&#26041;&#27861;&#65292;&#33021;&#22815;&#31934;&#30830;&#36319;&#36394;&#29289;&#20307;&#30340;6DoF&#23039;&#24577;&#65292;&#32780;&#19988;&#26080;&#38656;&#25968;&#25454;&#27880;&#37322;&#25110;&#36712;&#36857;&#24179;&#28369;&#12290;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#31934;&#30830;&#12289;&#40065;&#26834;&#19988;&#26080;&#25238;&#21160;&#30340;&#29305;&#28857;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21487;&#29992;&#20110;&#22810;&#30446;&#26631;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PixTrack&#65292;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#29289;&#20307;&#23039;&#24577;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#35270;&#22270;&#21512;&#25104;&#21644;&#28145;&#24230;&#29305;&#24449;&#24230;&#37327;&#23545;&#40784;&#12290;&#25105;&#20204;&#36981;&#24490;&#22522;&#20110;SfM&#30340;&#37325;&#26032;&#23450;&#20301;&#33539;&#24335;&#65292;&#20351;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#35268;&#33539;&#22320;&#34920;&#31034;&#34987;&#36319;&#36394;&#30340;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#30446;RGB&#22270;&#20687;&#21644;RGB-D&#22270;&#20687;&#20013;&#20135;&#29983;&#39640;&#24230;&#31934;&#30830;&#12289;&#40065;&#26834;&#19988;&#26080;&#25238;&#21160;&#30340;&#29289;&#20307;6DoF&#23039;&#24577;&#20272;&#35745;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#27880;&#37322;&#25110;&#36712;&#36857;&#24179;&#28369;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#29305;&#28857;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;CPU&#22810;&#36827;&#31243;&#21487;&#20197;&#23454;&#29616;&#22810;&#30446;&#26631;&#36319;&#36394;&#32780;&#26080;&#38656;&#25913;&#21464;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://github.com/GiantAI/pixtrack
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.03910v2 Announce Type: replace-cross  Abstract: We present PixTrack, a vision based object pose tracking framework using novel view synthesis and deep feature-metric alignment. We follow an SfM-based relocalization paradigm where we use a Neural Radiance Field to canonically represent the tracked object. Our evaluations demonstrate that our method produces highly accurate, robust, and jitter-free 6DoF pose estimates of objects in both monocular RGB images and RGB-D images without the need of any data annotation or trajectory smoothing. Our method is also computationally efficient making it easy to have multi-object tracking with no alteration to our algorithm through simple CPU multiprocessing. Our code is available at: https://github.com/GiantAI/pixtrack
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#21160;&#21147;&#23398;&#20998;&#35299;&#19990;&#30028;&#27169;&#22411;&#26500;&#24314;&#26694;&#26550;ED2&#65292;&#33021;&#22815;&#36890;&#36807;&#21457;&#29616;&#23376;&#21160;&#21147;&#23398;&#24182;&#36827;&#34892;&#20998;&#35299;&#39044;&#27979;&#65292;&#26356;&#20934;&#30830;&#22320;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2112.02817</link><description>&lt;p&gt;
ED2: &#36830;&#32493;&#25511;&#21046;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#20998;&#35299;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ED2: Environment Dynamics Decomposition World Models for Continuous Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.02817
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#21160;&#21147;&#23398;&#20998;&#35299;&#19990;&#30028;&#27169;&#22411;&#26500;&#24314;&#26694;&#26550;ED2&#65292;&#33021;&#22815;&#36890;&#36807;&#21457;&#29616;&#23376;&#21160;&#21147;&#23398;&#24182;&#36827;&#34892;&#20998;&#35299;&#39044;&#27979;&#65292;&#26356;&#20934;&#30830;&#22320;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (MBRL)&#22312;&#23454;&#36341;&#20013;&#30456;&#23545;&#20110;model-free RL&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#20854;&#24615;&#33021;&#24120;&#24120;&#21463;&#38480;&#20110;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#20943;&#23569;&#27169;&#22411;&#35823;&#24046;&#65292;&#26631;&#20934;&#30340;MBRL&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#32593;&#32476;&#26469;&#25311;&#21512;&#25972;&#20010;&#29615;&#22659;&#21160;&#21147;&#23398;&#65292;&#20294;&#36825;&#28010;&#36153;&#20102;&#21487;&#20197;&#20998;&#21035;&#24314;&#27169;&#30340;&#22810;&#20010;&#23376;&#21160;&#21147;&#23398;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20174;&#32780;&#33021;&#26356;&#20934;&#30830;&#22320;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29615;&#22659;&#21160;&#21147;&#23398;&#20998;&#35299;&#65288;ED2&#65289;&#30340;&#21019;&#26032;&#19990;&#30028;&#27169;&#22411;&#26500;&#24314;&#26694;&#26550;&#65292;&#20854;&#20197;&#19968;&#31181;&#20998;&#35299;&#30340;&#26041;&#24335;&#23545;&#29615;&#22659;&#36827;&#34892;&#24314;&#27169;&#12290;ED2&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#23376;&#21160;&#21147;&#23398;&#21457;&#29616;&#65288;SD2&#65289;&#21644;&#21160;&#21147;&#23398;&#20998;&#35299;&#39044;&#27979;&#65288;D2P&#65289;&#12290;SD2&#33021;&#22815;&#33258;&#21160;&#21457;&#29616;&#29615;&#22659;&#20013;&#30340;&#23376;&#21160;&#21147;&#23398;&#65292;&#28982;&#21518;D2P&#26681;&#25454;&#36825;&#20123;&#23376;&#21160;&#21147;&#23398;&#26500;&#24314;&#20998;&#35299;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;ED2&#21487;&#20197;&#19982;&#29616;&#26377;&#26041;&#27861;&#36731;&#26494;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.02817v2 Announce Type: replace-cross  Abstract: Model-based reinforcement learning (MBRL) achieves significant sample efficiency in practice in comparison to model-free RL, but its performance is often limited by the existence of model prediction error. To reduce the model error, standard MBRL approaches train a single well-designed network to fit the entire environment dynamics, but this wastes rich information on multiple sub-dynamics which can be modeled separately, allowing us to construct the world model more accurately. In this paper, we propose the Environment Dynamics Decomposition (ED2), a novel world model construction framework that models the environment in a decomposing manner. ED2 contains two key components: sub-dynamics discovery (SD2) and dynamics decomposition prediction (D2P). SD2 discovers the sub-dynamics in an environment automatically and then D2P constructs the decomposed world model following the sub-dynamics. ED2 can be easily combined with existing
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#32508;&#21512;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#21644;&#20449;&#24687;&#32463;&#27982;&#23398;&#65292;&#25552;&#20986;&#20102;&#20915;&#31574;&#38382;&#39064;&#30340;&#24191;&#27867;&#36866;&#29992;&#23450;&#20041;&#12290;&#20026;&#20102;&#23558;&#20154;&#31867;&#20915;&#31574;&#30340;&#19979;&#38477;&#24402;&#21646;&#20110;&#20559;&#35265;&#24418;&#24335;&#65292;&#23454;&#39564;&#24517;&#39035;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#35782;&#21035;&#35268;&#33539;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#20316;&#32773;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#30740;&#31350;&#30340;&#35780;&#20272;&#65292;&#21482;&#26377;17%&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#25551;&#36848;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#20559;&#31163;&#20102;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.15106</link><description>&lt;p&gt;
&#20915;&#31574;&#29702;&#35770;&#22522;&#30784;&#23545;&#35780;&#20272;&#20154;&#31867;&#20915;&#31574;&#30340;&#23454;&#39564;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Decision Theoretic Foundations for Experiments Evaluating Human Decisions. (arXiv:2401.15106v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#32508;&#21512;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#21644;&#20449;&#24687;&#32463;&#27982;&#23398;&#65292;&#25552;&#20986;&#20102;&#20915;&#31574;&#38382;&#39064;&#30340;&#24191;&#27867;&#36866;&#29992;&#23450;&#20041;&#12290;&#20026;&#20102;&#23558;&#20154;&#31867;&#20915;&#31574;&#30340;&#19979;&#38477;&#24402;&#21646;&#20110;&#20559;&#35265;&#24418;&#24335;&#65292;&#23454;&#39564;&#24517;&#39035;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#35782;&#21035;&#35268;&#33539;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#20316;&#32773;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#30740;&#31350;&#30340;&#35780;&#20272;&#65292;&#21482;&#26377;17%&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#25551;&#36848;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#20559;&#31163;&#20102;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#23637;&#31034;&#30340;&#20915;&#31574;&#26159;&#21487;&#35299;&#37322;AI&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#20197;&#21450;&#25968;&#25454;&#21487;&#35270;&#21270;&#31561;&#39046;&#22495;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#20915;&#31574;&#38382;&#39064;&#30340;&#23450;&#20041;&#20197;&#21450;&#23454;&#39564;&#24517;&#39035;&#20855;&#22791;&#30340;&#26465;&#20214;&#20197;&#24471;&#20986;&#20154;&#31867;&#20915;&#31574;&#23384;&#22312;&#32570;&#38519;&#30340;&#32467;&#35770;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#36866;&#29992;&#30340;&#20915;&#31574;&#38382;&#39064;&#23450;&#20041;&#65292;&#35813;&#23450;&#20041;&#26159;&#20174;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#21644;&#20449;&#24687;&#32463;&#27982;&#23398;&#20013;&#32508;&#21512;&#25552;&#28860;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35201;&#23558;&#20154;&#31867;&#32489;&#25928;&#19979;&#38477;&#24402;&#21646;&#20110;&#26576;&#31181;&#20559;&#35265;&#24418;&#24335;&#65292;&#23454;&#39564;&#24517;&#39035;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#21512;&#29702;&#30340;&#20195;&#29702;&#33021;&#22815;&#35782;&#21035;&#35268;&#33539;&#20915;&#31574;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#36817;&#26377;&#20851;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#25991;&#29486;&#20013;&#23545;&#20915;&#31574;&#21046;&#23450;&#36827;&#34892;&#30340;&#35780;&#20272;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#36798;&#21040;&#20102;&#36825;&#19968;&#26631;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#26377;35&#39033;&#22768;&#31216;&#30830;&#23450;&#20102;&#26377;&#20559;&#24046;&#34892;&#20026;&#30340;&#30740;&#31350;&#20013;&#30340;6&#39033;&#65288;17%&#65289;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#36275;&#22815;&#20449;&#24687;&#26469;&#25551;&#36848;&#20854;&#34892;&#20026;&#20559;&#31163;&#33391;&#22909;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Decision-making with information displays is a key focus of research in areas like explainable AI, human-AI teaming, and data visualization. However, what constitutes a decision problem, and what is required for an experiment to be capable of concluding that human decisions are flawed in some way, remain open to speculation. We present a widely applicable definition of a decision problem synthesized from statistical decision theory and information economics. We argue that to attribute loss in human performance to forms of bias, an experiment must provide participants with the information that a rational agent would need to identify the normative decision. We evaluate the extent to which recent evaluations of decision-making from the literature on AI-assisted decisions achieve this criteria. We find that only 6 (17\%) of 35 studies that claim to identify biased behavior present participants with sufficient information to characterize their behavior as deviating from good decision-making
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;?
&lt;/p&gt;
&lt;p&gt;
Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07927
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#33021;&#22815;&#25552;&#20379;&#20854;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#23545;&#20844;&#20247;&#26159;&#30452;&#25509;&#21487;&#35775;&#38382;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#36825;&#26679;&#30340;&#39118;&#38505;&#65292;&#21363;&#20196;&#20154;&#20449;&#26381;&#20294;&#38169;&#35823;&#30340;&#35299;&#37322;&#21487;&#33021;&#23548;&#33268;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25903;&#25745;&#30340;&#33258;&#20449;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#26159;AI&#23433;&#20840;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#35780;&#20272;&#33258;&#25105;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#27880;&#37322;&#20160;&#20040;&#26159;&#27491;&#30830;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#21487;&#38752;&#24615;&#30340;&#34913;&#37327;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35828;&#26576;&#32452;&#35789;&#23545;&#20110;&#20570;&#20986;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#22312;&#27809;&#26377;&#36825;&#20123;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#24212;&#35813;&#26080;&#27861;&#20570;&#20986;&#30456;&#21516;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#33258;&#27965;&#24615;&#26816;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21487;&#38752;&#24615;&#26041;&#27861;&#65292;&#20294;&#20043;&#21069;&#23578;&#26410;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#23558;&#33258;&#27965;&#24615;&#26816;&#27979;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02987</link><description>&lt;p&gt;
&#20320;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#25913;&#36827;&#21527;&#65311;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#31561;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#22914;&#20309;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#19982;&#27599;&#20010;&#23454;&#20307;&#30456;&#20851;&#30340;&#20803;&#29305;&#24449;&#20316;&#20026;&#19990;&#30028;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#21644;&#20803;&#29305;&#24449;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20855;&#26377;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained models has significantly impacted from Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta features as a metric for evaluating pretrained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and images models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01218</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#36866;&#24212;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#39044;&#27979;&#30340;&#25463;&#24452;&#65292;&#23548;&#33268;&#29983;&#25104;&#24615;&#33021;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#23481;&#26131;&#34920;&#29616;&#20986;&#20301;&#32622;&#20559;&#24046;&#65292;&#21363;&#21033;&#29992;&#20301;&#20110;&#24320;&#22836;&#25110;&#26411;&#23614;&#25110;&#36755;&#20837;&#20013;&#29305;&#23450;&#20301;&#32622;&#32447;&#32034;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#20943;&#36731;&#20301;&#32622;&#20559;&#24046;&#30340;&#24037;&#20316;&#38656;&#35201;&#22806;&#37096;&#20559;&#24046;&#30693;&#35782;&#25110;&#24102;&#27880;&#37322;&#30340;&#38750;&#20559;&#20506;&#26679;&#26412;&#65292;&#22312;&#23454;&#38469;&#20013;&#19981;&#22826;&#23454;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#65288;ZOE&#65289;&#26694;&#26550;&#23545;LLMs&#36827;&#34892;&#20301;&#32622;&#21435;&#20559;&#12290;ZOE&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#20219;&#20309;&#22806;&#37096;&#30693;&#35782;&#25110;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25552;&#39640;&#26080;&#30417;&#30563;&#21709;&#24212;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#20174;&#23545;&#40784;&#65288;MSA&#65289;&#27169;&#22359;&#26469;&#20462;&#21098;&#36825;&#20123;&#21709;&#24212;&#12290;&#23545;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#20116;&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ZOE&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.13538</link><description>&lt;p&gt;
&#23398;&#20250;&#35828;&#27597;&#35821;&#65306;&#20197;&#27597;&#35821;&#39118;&#26684;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#29616;&#20195;&#24037;&#20855;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#25991;&#26412;&#39118;&#26684;&#22914;&#20309;&#24433;&#21709;LLMs&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;LLMs&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290; "&#27597;&#35821;"&#26159;&#25351;LLMs&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#21487;&#20197;&#36890;&#36807;&#38646;-shot&#22330;&#26223;&#25506;&#27979;&#12290; AlignedCoT&#24191;&#27867;&#36866;&#29992;&#20110;ICL&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#38382;&#31572;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#25991;&#26412;&#29702;&#35299;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;AlignedCoT&#30456;&#27604;&#31934;&#24515;&#25163;&#24037;&#21046;&#20316;&#30340;&#28436;&#31034;&#25991;&#31295;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22312;&#24050;&#32465;&#23450;&#27169;&#22411;&#19978;&#23454;&#26102;&#36827;&#34892;&#21160;&#30011;&#25511;&#21046;&#21644;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17838</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24050;&#32465;&#23450;&#27169;&#22411;&#19978;&#23454;&#26102;&#29983;&#25104;&#21644;&#25511;&#21046;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Real-time Animation Generation and Control on Rigged Models via Large Language Models. (arXiv:2310.17838v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22312;&#24050;&#32465;&#23450;&#27169;&#22411;&#19978;&#23454;&#26102;&#36827;&#34892;&#21160;&#30011;&#25511;&#21046;&#21644;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#22312;&#24050;&#32465;&#23450;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#26102;&#21160;&#30011;&#25511;&#21046;&#21644;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;Unity&#20013;&#23884;&#20837;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29992;&#20110;&#36755;&#20986;&#21487;&#20197;&#35299;&#26512;&#20026;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#21160;&#30011;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#23454;&#29616;&#29616;&#26377;&#21160;&#30011;&#20043;&#38388;&#28789;&#27963;&#29366;&#24577;&#36716;&#25442;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#21508;&#31181;&#32465;&#23450;&#27169;&#22411;&#21644;&#21160;&#20316;&#19978;&#23637;&#31034;&#23450;&#24615;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel method for real-time animation control and generation on rigged models using natural language input. First, we embed a large language model (LLM) in Unity to output structured texts that can be parsed into diverse and realistic animations. Second, we illustrate LLM's potential to enable flexible state transition between existing animations. We showcase the robustness of our approach through qualitative results on various rigged models and motions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13230</link><description>&lt;p&gt;
&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20449;&#20219;&#22495;&#30340;&#22312;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#21644;&#28216;&#25103;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31867;&#21035;&#20013;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#20027;&#35201;&#24378;&#35843;&#23545;&#39044;&#26399;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#32570;&#20047;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65307;&#36890;&#36807;&#20248;&#21270;&#35813;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;&#36817;&#20046;&#24635;&#20307;&#24615;&#33021;&#26679;&#26412;&#30340;&#19979;&#30028;&#65288;&#32477;&#23545;&#24615;&#33021;&#65289;&#21576;&#29616;&#21333;&#35843;&#25913;&#36827;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36817;&#20284;&#23545;&#36825;&#20010;&#29702;&#35770;&#22522;&#30784;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#31216;&#20026;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#25484;&#25569;Atari&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;APO&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20063;&#26174;&#33879;&#25913;&#21892;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
&lt;/p&gt;</description></item><item><title>ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2310.09298</link><description>&lt;p&gt;
ByteStack-ID: &#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#21033;&#29992;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection. (arXiv:2310.09298v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09298
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;"ByteStack-ID"&#65292;&#19968;&#31181;&#19987;&#20026;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;ByteStack-ID&#26680;&#24515;&#26159;&#21033;&#29992;&#20174;&#36127;&#36733;&#25968;&#25454;&#30340;&#39057;&#29575;&#20998;&#24067;&#29983;&#25104;&#30340;&#28784;&#24230;&#22270;&#20687;&#65292;&#36825;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#25216;&#26415;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#22797;&#26434;&#25968;&#25454;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#22522;&#20110;&#25968;&#25454;&#21253;&#32423;&#20449;&#24687;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#27969;&#37327;&#25968;&#25454;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#22522;&#26412;&#22534;&#21472;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;ByteStack-ID&#19982;&#20256;&#32479;&#30340;&#22534;&#21472;&#26041;&#27861;&#19981;&#21516;&#12290;&#23427;&#23558;&#38468;&#21152;&#30340;&#20803;&#23398;&#20064;&#22120;&#23618;&#26080;&#32541;&#38598;&#25104;&#21040;&#36830;&#25509;&#30340;&#22522;&#30784;&#23398;&#20064;&#22120;&#20013;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#20248;&#21270;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces "ByteStack-ID," a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstandin
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35814;&#32454;&#22238;&#39038;&#20102;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;DL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#21644;&#23454;&#29616;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08598</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization for Medical Image Analysis: A Survey. (arXiv:2310.08598v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35814;&#32454;&#22238;&#39038;&#20102;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;DL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#21644;&#23454;&#29616;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65288;MedIA&#65289;&#24050;&#25104;&#20026;&#21307;&#23398;&#21644;&#20445;&#20581;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#30142;&#30149;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#27835;&#30103;&#35268;&#21010;&#26041;&#38754;&#36215;&#21040;&#20102;&#24456;&#22823;&#30340;&#20316;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26368;&#26032;&#25104;&#21151;&#20026;&#20854;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;MedIA&#30340;DL&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26679;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#19979;&#24456;&#38590;&#27867;&#21270;&#65292;&#36825;&#34987;&#31216;&#20026;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#24320;&#21457;&#21508;&#31181;DL&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#24182;&#22312;&#26410;&#30693;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31283;&#20581;&#22320;&#36816;&#34892;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;&#19987;&#38376;&#38024;&#23545;MedIA&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#22312;&#26356;&#22823;&#33539;&#22260;MedIA&#31995;&#32479;&#20869;&#30340;&#20132;&#20114;&#26041;&#24335;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#19981;&#20165;&#20165;&#32771;&#34385;&#26041;&#27861;&#23398;&#65292;&#36824;&#32771;&#34385;&#20102;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#20998;&#20026;&#25968;&#25454;&#23618;&#27425;&#30340;&#26041;&#27861;&#8230;
&lt;/p&gt;
&lt;p&gt;
Medical Image Analysis (MedIA) has become an essential tool in medicine and healthcare, aiding in disease diagnosis, prognosis, and treatment planning, and recent successes in deep learning (DL) have made significant contributions to its advances. However, DL models for MedIA remain challenging to deploy in real-world situations, failing for generalization under the distributional gap between training and testing samples, known as a distribution shift problem. Researchers have dedicated their efforts to developing various DL methods to adapt and perform robustly on unknown and out-of-distribution data distributions. This paper comprehensively reviews domain generalization studies specifically tailored for MedIA. We provide a holistic view of how domain generalization techniques interact within the broader MedIA system, going beyond methodologies to consider the operational implications on the entire MedIA workflow. Specifically, we categorize domain generalization methods into data-lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16391</link><description>&lt;p&gt;
&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;&#20108;&#32500;Copula&#36924;&#36817;&#21464;&#25442;&#65306;2-Cats&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copula&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#24212;&#29992;Copula&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#29420;&#31435;&#30340;&#36793;&#38469;&#20998;&#24067;&#65288;&#19968;&#20010;&#31616;&#21333;&#20219;&#21153;&#65289;&#65292;&#28982;&#21518;&#20272;&#35745;&#36830;&#25509;&#36793;&#38469;&#30340;&#21333;&#20010;Copula&#20989;&#25968;C&#65288;&#19968;&#20010;&#22256;&#38590;&#20219;&#21153;&#65289;&#26469;&#20272;&#35745;&#22810;&#20803;&#20998;&#24067;&#20989;&#25968;&#12290;&#23545;&#20110;&#20108;&#32500;&#25968;&#25454;&#65292;Copula&#26159;&#19968;&#20010;&#24418;&#22914;C&#65306;(u&#65292;v)&#8712;\mathbf{I}^2\rightarrow \mathbf{I}&#30340;&#20108;&#27425;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;\mathbf{I}=[0&#65292;1]&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22914;&#20309;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;2-Cats&#65292;&#21463;&#21040;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;Sobolev&#35757;&#32451;&#25991;&#29486;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19981;&#20165;&#35777;&#26126;&#20102;&#25105;&#20204;&#33021;&#22815;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#22320;&#20272;&#35745;2D Copula&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19988;&#31526;&#21512;Copula C&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.14053</link><description>&lt;p&gt;
&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#25209;&#37327;&#35757;&#32451;&#27867;&#21270;&#24615;&#33021;&#30340;LARS&#20877;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20351;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#32553;&#25918;&#27604;(LARS)&#26469;&#25506;&#32034;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#20855;&#26377;&#28909;&#36523;&#38454;&#27573;&#30340;LARS&#31639;&#27861;&#30001;&#20110;&#20887;&#20313;&#30340;&#27604;&#20363;&#32553;&#25918;&#23548;&#33268;&#22312;&#26089;&#26399;&#38519;&#20837;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#27492;&#22806;&#65292;&#21518;&#26399;&#22266;&#23450;&#30340;&#38497;&#23789;&#19979;&#38477;&#38480;&#21046;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#36941;&#21382;&#26089;&#26399;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;Time Varying LARS (TVLARS)&#65292;&#23427;&#29992;&#21487;&#37197;&#32622;&#30340;&#31867;&#20284;sigmoid&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#22312;&#21021;&#22987;&#38454;&#27573;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;TVLARS&#22312;&#26089;&#26399;&#20419;&#36827;&#20102;&#26799;&#24230;&#25506;&#32034;&#65292;&#36229;&#36234;&#20102;&#23574;&#38160;&#30340;&#20248;&#21270;&#22120;&#65292;&#24182;&#36880;&#28176;&#36807;&#28193;&#21040;LARS&#20197;&#23454;&#29616;&#21518;&#26399;&#30340;&#31283;&#20581;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#22987;&#32456;&#20248;&#20110;LARS&#21644;LAMB&#65292;&#20998;&#31867;&#22330;&#26223;&#20013;&#30340;&#25913;&#36827;&#36798;&#21040;2\%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26696;&#20363;&#20013;&#65292;TVLARS&#37117;&#32988;&#36807;&#20102;LARS&#21644;LAMB&#65292;&#24182;&#19988;&#24615;&#33021;&#25552;&#21319;&#20102;
&lt;/p&gt;
&lt;p&gt;
This paper explores Large Batch Training techniques using layer-wise adaptive scaling ratio (LARS) across diverse settings, uncovering insights. LARS algorithms with warm-up tend to be trapped in sharp minimizers early on due to redundant ratio scaling. Additionally, a fixed steep decline in the latter phase restricts deep neural networks from effectively navigating early-phase sharp minimizers. Building on these findings, we propose Time Varying LARS (TVLARS), a novel algorithm that replaces warm-up with a configurable sigmoid-like function for robust training in the initial phase. TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later phases. Extensive experiments demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2\% improvement in classification scenarios. Notably, in all self-supervised learning cases, TVLARS dominates LARS and LAMB with performance improvements of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13498</link><description>&lt;p&gt;
&#36867;&#31163;&#26679;&#26412;&#38519;&#38449;&#65306;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators. (arXiv:2308.13498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#65288;PaiDEs&#65289;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#21033;&#29992;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#37197;&#23545;&#36317;&#31163;&#26469;&#24314;&#31435;&#29109;&#30340;&#36793;&#30028;&#65292;&#24182;&#23558;&#36825;&#20123;&#36793;&#30028;&#20316;&#20026;&#22522;&#20110;&#20449;&#24687;&#20934;&#21017;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#26368;&#36817;&#22522;&#20110;&#26679;&#26412;&#30340;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#29992;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;PaiDEs&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#65288;&#26368;&#22810;100&#20493;&#65289;&#19978;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#65288;&#26368;&#22810;100&#20493;&#65289;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#26356;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#35780;&#20272;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#39564;&#65306;&#19968;&#32500;&#27491;&#24358;&#25968;&#25454;&#65292;&#25670;&#21160;&#29289;&#20307;&#65288;Pendulum-v0&#65289;&#65292;&#36339;&#36291;&#26426;&#22120;&#20154;&#65288;Hopper-v2&#65289;&#65292;&#34434;&#34433;&#26426;&#22120;&#20154;&#65288;Ant-v2&#65289;&#21644;&#20154;&#24418;&#26426;&#22120;&#20154;&#65288;Humanoid-v2&#65289;&#12290;&#23545;&#20110;&#27599;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#26469;&#23637;&#31034;PaiDEs&#22312;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20250;&#31185;&#23398;&#26500;&#36896;&#36716;&#21270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#27665;&#20027;&#20215;&#20540;&#35266;&#23884;&#20837;&#31038;&#20132;&#23186;&#20307;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#12290;&#36890;&#36807;&#19968;&#20010;&#24212;&#29992;&#20110;&#21453;&#27665;&#20027;&#24577;&#24230;&#30340;&#27169;&#22411;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#31038;&#20250;&#31185;&#23398;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#23450;&#24615;&#32534;&#30721;&#25163;&#20876;&#65292;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#36716;&#21270;&#36825;&#20123;&#26500;&#36896;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.13912</link><description>&lt;p&gt;
&#23558;&#27665;&#20027;&#20215;&#20540;&#35266;&#23884;&#20837;&#31038;&#20132;&#23186;&#20307;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31038;&#20250;&#23458;&#35266;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Embedding Democratic Values into Social Media AIs via Societal Objective Functions. (arXiv:2307.13912v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20250;&#31185;&#23398;&#26500;&#36896;&#36716;&#21270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#27665;&#20027;&#20215;&#20540;&#35266;&#23884;&#20837;&#31038;&#20132;&#23186;&#20307;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#12290;&#36890;&#36807;&#19968;&#20010;&#24212;&#29992;&#20110;&#21453;&#27665;&#20027;&#24577;&#24230;&#30340;&#27169;&#22411;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#31038;&#20250;&#31185;&#23398;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#23450;&#24615;&#32534;&#30721;&#25163;&#20876;&#65292;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#36716;&#21270;&#36825;&#20123;&#26500;&#36896;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20351;&#20854;&#32771;&#34385;&#21040;&#27665;&#20027;&#20215;&#20540;&#35266;&#65292;&#22914;&#20943;&#23569;&#20826;&#27966;&#25932;&#24847;&#65292;&#20316;&#20026;&#20854;&#30446;&#26631;&#20989;&#25968;&#26469;&#25490;&#21517;&#25105;&#20204;&#30340;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#27969;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#24050;&#24314;&#31435;&#12289;&#32463;&#23457;&#26597;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#36896;&#36716;&#21270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31038;&#20250;&#23458;&#35266;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;&#21453;&#27665;&#20027;&#24577;&#24230;&#36825;&#19968;&#25919;&#27835;&#31185;&#23398;&#26500;&#36896;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#25105;&#20204;&#32570;&#20047;&#21487;&#35266;&#23519;&#30340;&#32467;&#26524;&#26469;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#32780;&#31038;&#20250;&#31185;&#23398;&#24050;&#32463;&#24320;&#21457;&#20102;&#35843;&#26597;&#24037;&#20855;&#21644;&#23450;&#24615;&#32534;&#30721;&#25163;&#20876;&#65292;&#29992;&#20110;&#36825;&#20123;&#26500;&#36896;&#30340;&#32763;&#35793;&#65292;&#20854;&#31934;&#30830;&#24615;&#20415;&#20110;&#23558;&#20854;&#36716;&#21270;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35814;&#32454;&#25552;&#31034;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#27665;&#20027;&#24577;&#24230;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#23459;&#20256;&#21453;&#27665;&#20027;&#24577;&#24230;&#30340;&#31243;&#24230;&#65292;&#24182;&#22312;&#19977;&#20010;&#30740;&#31350;&#20013;&#27979;&#35797;&#20102;&#36825;&#20010;&#27665;&#20027;&#24577;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can we design artificial intelligence (AI) systems that rank our social media feeds to consider democratic values such as mitigating partisan animosity as part of their objective functions? We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. Traditionally, we have lacked observable outcomes to use to train such models, however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. In Study 1, we first test the attitudinal and 
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#30340;&#26412;&#36136;&#26159;&#19968;&#31995;&#21015;&#36890;&#36807;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#24314;&#31435;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#26469;&#26368;&#23567;&#21270;&#31995;&#32479;&#29109;&#30340;&#25968;&#23398;&#20989;&#25968;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.11114</link><description>&lt;p&gt;
&#26234;&#33021;&#30340;&#26412;&#36136;
&lt;/p&gt;
&lt;p&gt;
Nature of Intelligence. (arXiv:2307.11114v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11114
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#26412;&#36136;&#26159;&#19968;&#31995;&#21015;&#36890;&#36807;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#24314;&#31435;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#26469;&#26368;&#23567;&#21270;&#31995;&#32479;&#29109;&#30340;&#25968;&#23398;&#20989;&#25968;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#22823;&#33041;&#65292;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#20855;&#26377;&#23398;&#20064;&#33021;&#21147;&#24182;&#25191;&#34892;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20219;&#21153;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30001;&#22810;&#20010;&#35745;&#31639;&#23618;&#32452;&#25104;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#34920;&#31034;&#24182;&#22312;&#35768;&#22810;&#35782;&#21035;&#39046;&#22495;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#30340;&#26412;&#36136;&#65292;&#21363;&#36890;&#36807;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20849;&#21516;&#20195;&#34920;&#30340;&#26234;&#33021;&#30340;&#26412;&#36136;&#65292;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#26234;&#33021;&#30340;&#26412;&#36136;&#26159;&#19968;&#31995;&#21015;&#36890;&#36807;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#24314;&#31435;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#26469;&#26368;&#23567;&#21270;&#31995;&#32479;&#29109;&#30340;&#25968;&#23398;&#20989;&#25968;&#36807;&#31243;&#12290;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20197;&#19968;&#31181;&#21463;&#24378;&#21270;&#26041;&#24335;&#28040;&#32791;&#33021;&#37327;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#20123;&#20943;&#29109;&#36807;&#31243;&#12290;&#26681;&#25454;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20851;&#20110;&#35821;&#35328;&#12289;&#26080;&#24847;&#35782;&#21644;&#24847;&#35782;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#24037;&#31243;&#23454;&#29616;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain is the substrate for human intelligence. By simulating the human brain, artificial intelligence builds computational models that have learning capabilities and perform intelligent tasks approaching the human level. Deep neural networks consist of multiple computation layers to learn representations of data and improve the state-of-the-art in many recognition domains. However, the essence of intelligence commonly represented by both humans and AI is unknown. Here, we show that the nature of intelligence is a series of mathematically functional processes that minimize system entropy by establishing functional relationships between datasets over space and time. Humans and AI have achieved intelligence by implementing these entropy-reducing processes in a reinforced manner that consumes energy. With this hypothesis, we establish mathematical models of language, unconsciousness and consciousness, predicting the evidence to be found by neuroscience and achieved by AI engineer
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.02591</link><description>&lt;p&gt;
ODD: &#19968;&#20221;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02591
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#65288;ORAB&#65289;&#26159;&#38450;&#27490;&#33647;&#29289;&#36807;&#37327;&#30340;&#26032;&#39118;&#38505;&#22240;&#32032;&#12290;&#20197;&#24448;&#65292;ORAB&#20027;&#35201;&#36890;&#36807;&#35843;&#26597;&#32467;&#26524;&#21644;&#33647;&#29289;&#32473;&#20104;&#30417;&#27979;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#24182;&#19981;&#33021;&#28085;&#30422;&#25152;&#26377;&#24322;&#24120;&#34892;&#20026;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;ORAB&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#24191;&#27867;&#26377;&#35760;&#24405;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;ORAB&#26816;&#27979;&#12290;ODD&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;750&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#12290;ODD&#26088;&#22312;&#20174;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#35782;&#21035;ORAB&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20061;&#20010;&#31867;&#21035;&#65306;1&#65289;&#24050;&#30830;&#35748;&#24322;&#24120;&#34892;&#20026;&#65292;2&#65289;&#26263;&#31034;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;3&#65289;&#38463;&#29255;&#31867;&#33647;&#29289;&#65292;4&#65289;&#36866;&#24212;&#30151;&#65292;5&#65289;&#24050;&#35786;&#26029;&#30340;&#38463;&#29255;&#21046;&#21058;&#20381;&#36182;&#65292;6&#65289;&#33519;&#20108;&#27694;&#24179;&#31867;&#33647;&#29289;&#65292;7&#65289;&#33647;&#29289;&#21464;&#21270;&#65292;8&#65289;&#19982;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30456;&#20851;&#65292;9&#65289;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opioid related aberrant behaviors (ORAB) present novel risk factors for opioid overdose. Previously, ORAB have been mainly assessed by survey results and by monitoring drug administrations. Such methods however, cannot scale up and do not cover the entire spectrum of aberrant behaviors. On the other hand, ORAB are widely documented in electronic health record notes. This paper introduces a novel biomedical natural language processing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset comprising of more than 750 publicly available EHR notes. ODD has been designed to identify ORAB from patients' EHR notes and classify them into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants of Health. We explored two state-of-the-art natural language processing (NLP) mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#23436;&#25972;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#22914;&#20309;&#20174;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#36827;&#34892;&#21487;&#36776;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16958</link><description>&lt;p&gt;
&#30452;&#25509;&#25928;&#24212;&#22312;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of direct effects from summary causal graphs. (arXiv:2306.16958v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#23436;&#25972;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#22914;&#20309;&#20174;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#36827;&#34892;&#21487;&#36776;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#30452;&#25509;&#25928;&#24212;&#65292;&#21363;&#34913;&#37327;&#19968;&#20010;&#21464;&#37327;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#21464;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#21464;&#37327;&#19981;&#21464;&#12290;&#21160;&#24577;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#29992;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#26469;&#36827;&#34892;&#23450;&#24615;&#34920;&#31034;&#12290;&#20551;&#35774;&#32447;&#24615;&#21644;&#22240;&#26524;&#20805;&#20998;&#24615;&#65292;&#24182;&#32473;&#23450;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#65292;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#24635;&#26159;&#21487;&#36776;&#35782;&#30340;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#30001;&#25152;&#35859;&#30340;&#21333;&#38376;&#20934;&#21017;&#32473;&#20986;&#30340;&#20219;&#20309;&#21464;&#37327;&#38598;&#21512;&#26469;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#27809;&#26377;&#27492;&#31867;&#22270;&#24418;&#21487;&#29992;&#65292;&#20294;&#19987;&#23478;&#20173;&#28982;&#21487;&#20197;&#35775;&#38382;&#23436;&#20840;&#26102;&#38388;&#22240;&#26524;&#22270;&#30340;&#19968;&#20010;&#25277;&#35937;&#65292;&#35813;&#25277;&#35937;&#34920;&#31034;&#20102;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#21516;&#26102;&#30465;&#30053;&#20102;&#26102;&#38388;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#65292;&#20854;&#20013;&#35814;&#32454;&#25551;&#36848;&#20102;&#25152;&#26377;&#30452;&#25509;&#25928;&#24212;&#22312;&#24635;&#32467;&#22240;&#26524;&#22270;&#20013;&#21487;&#36776;&#35782;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic structural causal models (SCMs) are a powerful framework for reasoning in dynamic systems about direct effects which measure how a change in one variable affects another variable while holding all other variables constant. The causal relations in a dynamic structural causal model can be qualitatively represented with a full-time causal graph. Assuming linearity and causal sufficiency and given the full-time causal graph, the direct causal effect is always identifiable and can be estimated from data by adjusting on any set of variables given by the so-called single-door criterion. However, in many application such a graph is not available for various reasons but nevertheless experts have access to an abstraction of the full-time causal graph which represents causal relations between time series while omitting temporal information. This paper presents a complete identifiability result which characterizes all cases for which the direct effect is graphically identifiable from summa
&lt;/p&gt;</description></item><item><title>&#20174;&#20107;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30340;&#23398;&#26415;&#30028;&#19968;&#30452;&#38754;&#20020;&#30528;&#25910;&#38598;&#36275;&#22815;&#39640;&#36136;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#65292;&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#26377;&#20102;&#37325;&#35201;&#30340;&#31361;&#30772;&#65292;&#23558;&#36825;&#31181;&#27169;&#22411;&#24212;&#29992;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#33021;&#23545;&#20256;&#32479;&#30740;&#31350;&#33539;&#24335;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.02552</link><description>&lt;p&gt;
&#24403;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#36935;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#25143;&#27169;&#25311;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm. (arXiv:2306.02552v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02552
&lt;/p&gt;
&lt;p&gt;
&#20174;&#20107;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30340;&#23398;&#26415;&#30028;&#19968;&#30452;&#38754;&#20020;&#30528;&#25910;&#38598;&#36275;&#22815;&#39640;&#36136;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#65292;&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#26377;&#20102;&#37325;&#35201;&#30340;&#31361;&#30772;&#65292;&#23558;&#36825;&#31181;&#27169;&#22411;&#24212;&#29992;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#33021;&#23545;&#20256;&#32479;&#30740;&#31350;&#33539;&#24335;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36275;&#22815;&#21644;&#39640;&#36136;&#37327;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#19968;&#30452;&#26159;&#19968;&#20010;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#26159;&#19968;&#20010;&#30452;&#35266;&#30340;&#24819;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20027;&#35266;&#21644;&#22797;&#26434;&#24615;&#36136;&#65292;&#21487;&#38752;&#22320;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#23637;&#31034;&#20102;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#26234;&#33021;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#20026;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#25552;&#20379;&#20102;&#37325;&#35201;&#26426;&#20250;&#65292;&#24182;&#26377;&#21487;&#33021;&#25913;&#21464;&#20256;&#32479;&#30340;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#33539;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#25512;&#33616;&#31995;&#32479;&#20026;&#20363;&#65292;&#25506;&#32034;&#20351;&#29992;LLM&#36827;&#34892;&#29992;&#25143;&#27169;&#25311;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#29992;&#25143;&#35270;&#20026;&#22522;&#20110;LLM&#30340;&#33258;&#27835;&#26234;&#33021;&#20307;&#65292;&#24182;&#35753;&#19981;&#21516;&#26234;&#33021;&#20307;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#33258;&#30001;&#20132;&#27969;&#12289;&#34892;&#20026;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
User behavior analysis is crucial in human-centered AI applications. In this field, the collection of sufficient and high-quality user behavior data has always been a fundamental yet challenging problem. An intuitive idea to address this problem is automatically simulating the user behaviors. However, due to the subjective and complex nature of human cognitive processes, reliably simulating the user behavior is difficult. Recently, large language models (LLM) have obtained remarkable successes, showing great potential to achieve human-like intelligence. We argue that these models present significant opportunities for reliable user simulation, and have the potential to revolutionize traditional study paradigms in user behavior analysis. In this paper, we take recommender system as an example to explore the potential of using LLM for user simulation. Specifically, we regard each user as an LLM-based autonomous agent, and let different agents freely communicate, behave and evolve in a vir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;GPT&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#38656;&#35201;&#26680;&#24515;&#27010;&#24565;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#21644;&#26032;&#30340;1D-ARC&#22522;&#20934;&#65292;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18354</link><description>&lt;p&gt;
LLM&#21644;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65306;&#25104;&#21151;&#12289;&#22833;&#36133;&#19982;&#22522;&#20110;&#23545;&#35937;&#34920;&#31034;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations. (arXiv:2305.18354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;GPT&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#38656;&#35201;&#26680;&#24515;&#27010;&#24565;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#21644;&#26032;&#30340;1D-ARC&#22522;&#20934;&#65292;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21542;&#35299;&#20915;&#31616;&#21333;&#30340;&#25277;&#35937;&#25512;&#29702;&#38382;&#39064;&#65311;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#20998;&#26512;GPT&#22312;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#30340;&#34920;&#29616;&#26469;&#25506;&#32034;&#36825;&#20010;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#65292;&#20174;&#26377;&#38480;&#30340;&#20363;&#23376;&#20013;&#35201;&#27714;&#25105;&#20204;&#26377;&#20123;&#20851;&#20110;&#27010;&#24565;&#65288;&#22914;&#23545;&#35937;&#12289;&#30446;&#26631;&#29366;&#24577;&#12289;&#35745;&#25968;&#21644;&#22522;&#26412;&#20960;&#20309;&#65289;&#30340;&#8220;&#26680;&#24515;&#30693;&#35782;&#8221;&#20197;&#35299;&#20915;&#38382;&#39064;&#12290;&#24403;&#20351;&#29992;&#25991;&#26412;&#32534;&#30721;&#23545;&#20108;&#32500;&#36755;&#20837;&#36755;&#20986;&#32593;&#26684;&#30340;ARC&#20219;&#21153;&#36827;&#34892;&#32534;&#30721;&#26102;&#65292;GPT-4&#20165;&#35299;&#20915;&#20102;50&#20010;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#30340;13&#20010;&#12290;&#25105;&#20204;&#30340;&#22833;&#36133;&#20998;&#26512;&#26174;&#31034;&#65292;GPT-4&#35782;&#21035;&#23545;&#35937;&#24182;&#25512;&#29702;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#34920;&#31034;&#20219;&#21153;&#20013;&#23545;&#35937;&#30340;&#25991;&#26412;&#30340;&#39034;&#24207;&#24615;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;1D-ARC&#65292;&#23427;&#30001;&#26356;&#26377;&#21033;&#20110;&#22522;&#20110;GPT&#30340;&#25512;&#29702;&#30340;&#19968;&#32500;&#65288;&#31867;&#20284;&#25968;&#32452;&#65289;&#20219;&#21153;&#32452;&#25104;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#65292;GPT&#30340;&#34920;&#29616;&#30830;&#23454;&#27604;&#22312;&#65288;2D&#65289;ARC&#19978;&#26356;&#22909;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#35937;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#23427;&#20445;&#30041;&#20102;&#23545;&#35937;&#20043;&#38388;&#30340;&#22522;&#26412;&#31354;&#38388;&#20851;&#31995;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25512;&#29702;&#12290;&#20351;&#29992;&#36825;&#31181;&#32534;&#30721;&#65292;GPT-4&#22312;ARC&#19978;&#30340;&#25104;&#21151;&#29575;&#22823;&#22823;&#25552;&#39640;&#65292;&#36798;&#21040;&#20102;45/50&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#20351;&#29992;&#22522;&#20110;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#27861;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;LLM&#22522;&#20110;&#25512;&#29702;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some "core knowledge" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we prop
&lt;/p&gt;</description></item><item><title>SLaDe&#26159;&#19968;&#31181;&#22522;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#30495;&#23454;&#20195;&#30721;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#65292;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#20998;&#35789;&#22120;&#12289;&#26080;&#20002;&#24323;&#35757;&#32451;&#21644;&#31867;&#22411;&#25512;&#26029;&#31561;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#26131;&#35835;&#21644;&#26356;&#20934;&#30830;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2305.12520</link><description>&lt;p&gt;
SLaDe: &#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#27719;&#32534;&#20195;&#30721;&#30340;&#21487;&#31227;&#26893;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#21453;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly. (arXiv:2305.12520v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12520
&lt;/p&gt;
&lt;p&gt;
SLaDe&#26159;&#19968;&#31181;&#22522;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#30495;&#23454;&#20195;&#30721;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#65292;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#20998;&#35789;&#22120;&#12289;&#26080;&#20002;&#24323;&#35757;&#32451;&#21644;&#31867;&#22411;&#25512;&#26029;&#31561;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#26131;&#35835;&#21644;&#26356;&#20934;&#30830;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#32534;&#35793;&#26159;&#19968;&#20010;&#30740;&#31350;&#36739;&#20026;&#24191;&#27867;&#30340;&#39046;&#22495;&#65292;&#26377;&#35768;&#22810;&#39640;&#36136;&#37327;&#30340;&#24037;&#20855;&#21487;&#20379;&#20351;&#29992;&#12290;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#29992;&#20110;&#23433;&#20840;&#20219;&#21153;&#21644;&#31227;&#26893;&#36951;&#30041;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#29983;&#25104;&#38590;&#20197;&#38405;&#35835;&#30340;&#31243;&#24207;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#24037;&#20316;&#26469;&#25903;&#25345;&#26032;&#30340;&#32534;&#31243;&#35821;&#35328;&#21644;&#25351;&#20196;&#38598;&#26550;&#26500;&#12290;&#26368;&#36817;&#20851;&#27880;&#31070;&#32463;&#26041;&#27861;&#20135;&#29983;&#20102;&#33021;&#29983;&#25104;&#21487;&#35835;&#20195;&#30721;&#30340;&#21487;&#31227;&#26893;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#21482;&#36866;&#29992;&#20110;&#27809;&#26377;&#20248;&#21270;&#30340;&#21512;&#25104;&#31243;&#24207;&#65292;&#24182;&#19988;&#27809;&#26377;&#27169;&#22411;&#35780;&#20272;&#23427;&#20204;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#29983;&#25104;&#30340;&#20195;&#30721;&#21487;&#33021;&#26356;&#26131;&#35835;&#65292;&#20294;&#36890;&#24120;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SLaDe&#65292;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#35757;&#32451;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#21453;&#32534;&#35793;&#22120;&#65292;&#35813;&#21464;&#25442;&#22120;&#26159;&#20351;&#29992;&#23454;&#38469;&#20195;&#30721;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35789;&#22120;&#65292;&#24182;&#21033;&#29992;&#26080;&#20002;&#24323;&#35757;&#32451;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#21033;&#29992;&#31867;&#22411;&#25512;&#26029;&#29983;&#25104;&#27604;&#26631;&#20934;&#20998;&#26512;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#31070;&#32463;&#26041;&#27861;&#26356;&#26131;&#35835;&#21644;&#26356;&#20934;&#30830;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unli
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#20215;&#26684;&#25968;&#25454;&#24066;&#22330;&#20013;&#30340;&#20080;&#23478;&#20043;&#38388;&#30340;&#31454;&#20105;&#31574;&#30053;&#65292;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25991;&#31456;&#25581;&#31034;&#20102;&#20080;&#23478;&#20043;&#38388;&#30340;&#36127;&#22806;&#37096;&#24615;&#21450;&#20854;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#24066;&#22330;&#24178;&#39044;&#25514;&#26045;&#65292;&#23454;&#29616;&#20102;&#32431;&#31574;&#30053;&#22343;&#34913;&#65292;&#24182;&#20445;&#35777;&#20102;&#24378;&#31119;&#21033;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2302.08012</link><description>&lt;p&gt;
&#24102;&#26377;&#22806;&#37096;&#24615;&#30340;&#22266;&#23450;&#20215;&#26684;&#25968;&#25454;&#24066;&#22330;&#30340;&#22343;&#34913;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equilibrium and Learning in Fixed-Price Data Markets with Externality. (arXiv:2302.08012v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#20215;&#26684;&#25968;&#25454;&#24066;&#22330;&#20013;&#30340;&#20080;&#23478;&#20043;&#38388;&#30340;&#31454;&#20105;&#31574;&#30053;&#65292;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25991;&#31456;&#25581;&#31034;&#20102;&#20080;&#23478;&#20043;&#38388;&#30340;&#36127;&#22806;&#37096;&#24615;&#21450;&#20854;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#24066;&#22330;&#24178;&#39044;&#25514;&#26045;&#65292;&#23454;&#29616;&#20102;&#32431;&#31574;&#30053;&#22343;&#34913;&#65292;&#24182;&#20445;&#35777;&#20102;&#24378;&#31119;&#21033;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#23558;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#24066;&#22330;&#24314;&#27169;&#20026;&#19968;&#20010;&#20080;&#23478;&#20043;&#38388;&#30340;&#21516;&#26102;&#31227;&#21160;&#21338;&#24328;&#65292;&#20854;&#20013;&#21334;&#23478;&#21457;&#24067;&#22266;&#23450;&#20215;&#26684;&#65292;&#32780;&#20080;&#23478;&#21487;&#20197;&#33258;&#30001;&#22320;&#20174;&#20219;&#20309;&#19968;&#32452;&#21334;&#23478;&#36141;&#20080;&#12290;&#35813;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#20080;&#23478;&#36890;&#36807;&#36141;&#20080;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#30340;&#25968;&#25454;&#23545;&#24444;&#27492;&#20135;&#29983;&#30340;&#36127;&#22806;&#37096;&#24615;&#65292;&#36825;&#31181;&#29616;&#35937;&#22240;&#25968;&#25454;&#26131;&#20110;&#22797;&#21046;&#32780;&#21152;&#21095;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#20917;&#12290;&#22312;&#26356;&#31616;&#21333;&#30340;&#23436;&#20840;&#20449;&#24687;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23384;&#22312;&#20110;&#23384;&#22312;&#20080;&#23478;&#22806;&#37096;&#24615;&#30340;&#32431;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#31119;&#21033;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#31867;&#26631;&#20934;&#30340;&#22806;&#37096;&#24615;&#20989;&#25968;&#65292;&#20197;&#20132;&#26131;&#25104;&#26412;&#30340;&#24418;&#24335;&#30340;&#24066;&#22330;&#24178;&#39044;&#21487;&#20197;&#23548;&#33268;&#24378;&#31119;&#21033;&#20445;&#35777;&#30340;&#32431;&#31574;&#30053;&#22343;&#34913;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#32771;&#34385;&#20080;&#23478;&#36215;&#22987;&#20272;&#20215;&#26410;&#30693;&#30340;&#26356;&#19968;&#33324;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose modeling real-world data markets, where sellers post fixed prices and buyers are free to purchase from any set of sellers, as a simultaneous-move game between the buyers. A key component of this model is the negative externality buyers induce on one another due to purchasing data with a competitive advantage, a phenomenon exacerbated by data's easy replicability. We consider two settings. In the simpler complete-information setting, where all buyers know their valuations, we characterize both the existence and welfare properties of the pure-strategy Nash equilibrium in the presence of buyer externality. While this picture is bleak without any market intervention, reinforcing the limitations of current data markets, we prove that for a standard class of externality functions, market intervention in the form of a transaction cost can lead to a pure-strategy equilibrium with strong welfare guarantees. We next consider a more general setting where buyers start with unknown valua
&lt;/p&gt;</description></item></channel></rss>