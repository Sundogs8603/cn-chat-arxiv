<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#21160;&#24577;&#36710;&#36742;&#35843;&#24230;&#38382;&#39064;&#24314;&#27169;&#20026;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07508</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21160;&#24577;&#36710;&#36742;&#35843;&#24230;&#38382;&#39064;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning for the dynamic vehicle dispatching problem: An event-based approach. (arXiv:2307.07508v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#21160;&#24577;&#36710;&#36742;&#35843;&#24230;&#38382;&#39064;&#24314;&#27169;&#20026;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#36710;&#36742;&#35843;&#24230;&#38382;&#39064;&#28041;&#21450;&#20915;&#23450;&#23558;&#21738;&#20123;&#36710;&#36742;&#20998;&#37197;&#32473;&#38543;&#26426;&#20135;&#29983;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#35831;&#27714;&#12290;&#35813;&#38382;&#39064;&#20986;&#29616;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#22914;&#23558;&#21345;&#36710;&#20998;&#37197;&#32473;&#35201;&#36816;&#36755;&#30340;&#36135;&#29289;&#12289;&#24212;&#24613;&#31995;&#32479;&#21644;&#39034;&#39118;&#36710;&#26381;&#21153;&#20013;&#12290;&#26412;&#25991;&#23558;&#35813;&#38382;&#39064;&#24314;&#27169;&#20026;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#26102;&#38388;&#35270;&#20026;&#36830;&#32493;&#21464;&#37327;&#12290;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#20915;&#31574;&#26102;&#21051;&#19982;&#20107;&#20214;&#19968;&#33268;&#65292;&#20854;&#26102;&#38388;&#38388;&#38548;&#26159;&#38543;&#26426;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#20107;&#20214;&#30340;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#20915;&#31574;&#31354;&#38388;&#30340;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#24182;&#20811;&#26381;&#20102;&#25991;&#29486;&#20013;&#24120;&#25552;&#20986;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#30340;&#20854;&#20182;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#25955;&#20107;&#20214;&#27169;&#25311;&#22120;&#65292;&#24182;&#20351;&#29992;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#35757;&#32451;&#25105;&#20204;&#30340;&#20915;&#31574;&#20195;&#29702;&#12290;&#22312;&#20351;&#29992;&#32445;&#32422;&#24066;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#33719;&#24471;&#30340;&#31574;&#30053;&#19982;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic vehicle dispatching problem corresponds to deciding which vehicles to assign to requests that arise stochastically over time and space. It emerges in diverse areas, such as in the assignment of trucks to loads to be transported; in emergency systems; and in ride-hailing services. In this paper, we model the problem as a semi-Markov decision process, which allows us to treat time as continuous. In this setting, decision epochs coincide with discrete events whose time intervals are random. We argue that an event-based approach substantially reduces the combinatorial complexity of the decision space and overcomes other limitations of discrete-time models often proposed in the literature. In order to test our approach, we develop a new discrete-event simulator and use double deep q-learning to train our decision agents. Numerical experiments are carried out in realistic scenarios using data from New York City. We compare the policies obtained through our approach with heuristic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26102;&#31354;&#27880;&#24847;&#21147;&#32593;&#32476;(ISTA-Net)&#65292;&#36890;&#36807;&#21516;&#26102;&#24314;&#27169;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20132;&#20114;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#20132;&#20114;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#38480;&#21046;&#21644;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#21033;&#29992;&#20998;&#27573;&#22120;&#23558;&#22810;&#20010;&#19981;&#21516;&#23454;&#20307;&#30340;&#21160;&#20316;&#21010;&#20998;&#25104;&#20132;&#20114;&#24335;&#26102;&#31354;&#20196;&#29260;(ISTs)&#65292;&#24182;&#36890;&#36807;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#21644;3D&#21367;&#31215;&#23454;&#29616;&#20102;&#22312;ISTs&#30340;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#32852;&#21512;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07469</link><description>&lt;p&gt;
&#22522;&#20110;&#39592;&#39612;&#30340;&#20132;&#20114;&#21160;&#20316;&#35782;&#21035;&#30340;&#20132;&#20114;&#24335;&#26102;&#31354;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition. (arXiv:2307.07469v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26102;&#31354;&#27880;&#24847;&#21147;&#32593;&#32476;(ISTA-Net)&#65292;&#36890;&#36807;&#21516;&#26102;&#24314;&#27169;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20132;&#20114;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#20132;&#20114;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#38480;&#21046;&#21644;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#21033;&#29992;&#20998;&#27573;&#22120;&#23558;&#22810;&#20010;&#19981;&#21516;&#23454;&#20307;&#30340;&#21160;&#20316;&#21010;&#20998;&#25104;&#20132;&#20114;&#24335;&#26102;&#31354;&#20196;&#29260;(ISTs)&#65292;&#24182;&#36890;&#36807;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#21644;3D&#21367;&#31215;&#23454;&#29616;&#20102;&#22312;ISTs&#30340;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#32852;&#21512;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#26426;&#20132;&#20114;&#21644;&#21327;&#20316;&#20013;&#65292;&#35782;&#21035;&#20132;&#20114;&#21160;&#20316;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#21518;&#26399;&#34701;&#21512;&#21644;&#20849;&#21516;&#20851;&#27880;&#26426;&#21046;&#26469;&#25429;&#25417;&#20132;&#20114;&#20851;&#31995;&#65292;&#20294;&#23398;&#20064;&#33021;&#21147;&#26377;&#38480;&#65292;&#25110;&#32773;&#36866;&#24212;&#26356;&#22810;&#30340;&#20132;&#20114;&#23454;&#20307;&#25928;&#29575;&#20302;&#12290;&#22522;&#20110;&#27599;&#20010;&#23454;&#20307;&#30340;&#20808;&#39564;&#24050;&#30693;&#30340;&#20551;&#35774;&#65292;&#23427;&#20204;&#20063;&#32570;&#20047;&#23545;&#26356;&#19968;&#33324;&#35774;&#32622;&#30340;&#35780;&#20272;&#65292;&#20197;&#24212;&#23545;&#20027;&#20307;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26102;&#31354;&#27880;&#24847;&#21147;&#32593;&#32476;(ISTA-Net)&#65292;&#21516;&#26102;&#24314;&#27169;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20132;&#20114;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#21253;&#21547;&#19968;&#20010;&#20998;&#27573;&#22120;&#65292;&#29992;&#20110;&#20998;&#21106;&#20132;&#20114;&#24335;&#26102;&#31354;&#20196;&#29260;(ISTs)&#65292;&#36825;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#34920;&#31034;&#22810;&#20010;&#19981;&#21516;&#23454;&#20307;&#21160;&#20316;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#25193;&#23637;&#23454;&#20307;&#32500;&#24230;&#65292;ISTs&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#20132;&#20114;&#34920;&#31034;&#12290;&#20026;&#20102;&#22312;ISTs&#20013;&#30340;&#19977;&#20010;&#32500;&#24230;&#19978;&#32852;&#21512;&#23398;&#20064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19982;3D&#21367;&#31215;&#38598;&#25104;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing interactive action plays an important role in human-robot interaction and collaboration. Previous methods use late fusion and co-attention mechanism to capture interactive relations, which have limited learning capability or inefficiency to adapt to more interacting entities. With assumption that priors of each entity are already known, they also lack evaluations on a more general setting addressing the diversity of subjects. To address these problems, we propose an Interactive Spatiotemporal Token Attention Network (ISTA-Net), which simultaneously model spatial, temporal, and interactive relations. Specifically, our network contains a tokenizer to partition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to represent motions of multiple diverse entities. By extending the entity dimension, ISTs provide better interactive representations. To jointly learn along three dimensions in ISTs, multi-head self-attention blocks integrated with 3D convolutions are des
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21270;&#35009;&#21098;&#24212;&#29992;&#20110;&#32422;&#26463;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#36807;&#22810;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.07457</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21270;&#35009;&#21098;&#29992;&#20110;&#32422;&#26463;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structured Pruning of Neural Networks for Constraints Learning. (arXiv:2307.07457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21270;&#35009;&#21098;&#24212;&#29992;&#20110;&#32422;&#26463;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#36807;&#22810;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#19982;&#36816;&#31609;&#23398;&#65288;OR&#65289;&#24037;&#20855;&#30340;&#25972;&#21512;&#26041;&#38754;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#21253;&#25324;&#30284;&#30151;&#27835;&#30103;&#12289;&#31639;&#27861;&#37197;&#32622;&#21644;&#21270;&#23398;&#36807;&#31243;&#20248;&#21270;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;ML&#21644;OR&#30340;&#32452;&#21512;&#36890;&#24120;&#20381;&#36182;&#20110;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#24418;&#24335;&#34920;&#31034;ML&#27169;&#22411;&#36755;&#20986;&#12290;&#25991;&#29486;&#20013;&#30340;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20102;&#36825;&#26679;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#22788;&#29702;&#35768;&#22810;ML&#39044;&#27979;&#22120;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#21253;&#21547;&#22823;&#37327;&#21442;&#25968;&#65292;&#23548;&#33268; MIP &#24418;&#24335;&#21270;&#26041;&#27861;&#38590;&#20197;&#35299;&#20915;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#24050;&#32463;&#24341;&#20837;&#20102;&#20960;&#31181;&#25216;&#26415;&#26469;&#20943;&#23569;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#22240;&#20026;&#29616;&#20195;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#24222;&#22823;&#35268;&#27169;&#23545;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#23427;&#26126;&#26174;&#38477;&#20302;&#20102;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the integration of Machine Learning (ML) models with Operation Research (OR) tools has gained popularity across diverse applications, including cancer treatment, algorithmic configuration, and chemical process optimization. In this domain, the combination of ML and OR often relies on representing the ML model output using Mixed Integer Programming (MIP) formulations. Numerous studies in the literature have developed such formulations for many ML predictors, with a particular emphasis on Artificial Neural Networks (ANNs) due to their significant interest in many applications. However, ANNs frequently contain a large number of parameters, resulting in MIP formulations that are impractical to solve, thereby impeding scalability. In fact, the ML community has already introduced several techniques to reduce the parameter count of ANNs without compromising their performance, since the substantial size of modern ANNs presents challenges for ML applications as it significantly
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26377;&#28145;&#24230;&#38480;&#21046;&#30340;&#26234;&#33021;&#20307;&#30340;&#35748;&#30693;&#36923;&#36753;&#25193;&#23637;DBEL&#65292;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#21407;&#23376;&#26469;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#28145;&#24230;&#30340;&#26126;&#30830;&#25512;&#29702;&#65292;&#24182;&#25552;&#20379;&#20102;DBEL&#30340;&#20844;&#29702;&#21270;&#21644;&#25193;&#23637;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;DPAL&#36923;&#36753;&#20197;&#21450;&#20854;&#23427;&#20004;&#31181;&#25193;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#36825;&#20123;&#25193;&#23637;&#30340;&#19981;&#21487;&#21462;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07448</link><description>&lt;p&gt;
&#26377;&#28145;&#24230;&#38480;&#21046;&#30340;&#35748;&#30693;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Depth-bounded Epistemic Logic. (arXiv:2307.07448v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26377;&#28145;&#24230;&#38480;&#21046;&#30340;&#26234;&#33021;&#20307;&#30340;&#35748;&#30693;&#36923;&#36753;&#25193;&#23637;DBEL&#65292;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#21407;&#23376;&#26469;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#28145;&#24230;&#30340;&#26126;&#30830;&#25512;&#29702;&#65292;&#24182;&#25552;&#20379;&#20102;DBEL&#30340;&#20844;&#29702;&#21270;&#21644;&#25193;&#23637;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;DPAL&#36923;&#36753;&#20197;&#21450;&#20854;&#23427;&#20004;&#31181;&#25193;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#36825;&#20123;&#25193;&#23637;&#30340;&#19981;&#21487;&#21462;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#36923;&#36753;&#27169;&#22411;&#25551;&#36848;&#20102;&#26234;&#33021;&#20307;&#22914;&#20309;&#25512;&#29702;&#20182;&#20204;&#30340;&#20449;&#24565;&#20197;&#21450;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24565;&#12290;&#29616;&#26377;&#30340;&#36923;&#36753;&#36890;&#24120;&#20551;&#35774;&#26234;&#33021;&#20307;&#33021;&#22815;&#23436;&#32654;&#22320;&#25512;&#29702;&#26080;&#30028;&#27169;&#24577;&#28145;&#24230;&#30340;&#21629;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DBEL&#65292;&#23427;&#26159;S5&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#27169;&#25311;&#21482;&#33021;&#25512;&#29702;&#21040;&#29305;&#23450;&#27169;&#24577;&#28145;&#24230;&#30340;&#26234;&#33021;&#20307;&#12290;&#20026;&#20102;&#25903;&#25345;&#23545;&#26234;&#33021;&#20307;&#28145;&#24230;&#30340;&#26126;&#30830;&#25512;&#29702;&#65292;DBEL&#21253;&#25324;&#28145;&#24230;&#21407;&#23376;Ead&#65288;&#20195;&#29702;&#20154;a&#30340;&#28145;&#24230;&#24688;&#22909;&#20026;d&#65289;&#21644;Pad&#65288;&#20195;&#29702;&#20154;a&#30340;&#28145;&#24230;&#33267;&#23569;&#20026;d&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;DBEL&#30340;&#23436;&#22791;&#30340;&#20844;&#29702;&#21270;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;DBEL&#20197;&#25903;&#25345;&#26377;&#28145;&#24230;&#38480;&#21046;&#30340;&#20844;&#24320;&#20844;&#21578;&#65292;&#23637;&#31034;&#20102;&#32467;&#26524;DPAL&#36923;&#36753;&#22914;&#20309;&#25512;&#24191;&#20844;&#24320;&#20844;&#21578;&#36923;&#36753;&#30340;&#26631;&#20934;&#20844;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26367;&#20195;&#25193;&#23637;&#65292;&#24182;&#30830;&#23450;&#20102;&#36825;&#20123;&#25193;&#23637;&#26377;&#20294;DPAL&#27809;&#26377;&#30340;&#20004;&#20010;&#19981;&#21487;&#21462;&#30340;&#29305;&#24615;&#8212;&#8212;&#36951;&#24536;&#21644;&#30693;&#35782;&#27844;&#28431;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#36923;&#36753;&#30340;&#20844;&#29702;&#21270;&#20197;&#21450;&#21487;&#28385;&#36275;&#24615;&#21644;&#27169;&#22411;&#26816;&#39564;&#30340;&#22797;&#26434;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epistemic logics model how agents reason about their beliefs and the beliefs of other agents. Existing logics typically assume the ability of agents to reason perfectly about propositions of unbounded modal depth. We present DBEL, an extension of S5 that models agents that can reason about epistemic formulas only up to a specific modal depth. To support explicit reasoning about agent depths, DBEL includes depth atoms Ead (agent a has depth exactly d) and Pad (agent a has depth at least d). We provide a sound and complete axiomatization of DBEL.  We extend DBEL to support public announcements for bounded depth agents and show how the resulting DPAL logic generalizes standard axioms from public announcement logic. We present two alternate extensions and identify two undesirable properties, amnesia and knowledge leakage, that these extensions have but DPAL does not. We provide axiomatizations of these logics as well as complexity results for satisfiability and model checking.  Finally, we
&lt;/p&gt;</description></item><item><title>TSNet-SAC&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#32593;&#32476;&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#25351;&#23548;&#35757;&#32451;&#65292;&#36890;&#36807;&#24341;&#20837;&#28369;&#21160;&#22686;&#24378;&#32452;&#20214;&#21644;&#25193;&#23637;&#32452;&#20214;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#35299;&#20915;&#31639;&#27861;&#32570;&#38519;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25509;&#20837;&#22330;&#26223;&#12290;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;TSNet-SAC&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#32593;&#32476;&#65292;&#24182;&#20855;&#26377;&#26356;&#20248;&#30340;&#35843;&#24230;&#20915;&#31574;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2307.07445</link><description>&lt;p&gt;
TSNet-SAC: &#21033;&#29992;Transformer&#36827;&#34892;&#39640;&#25928;&#20219;&#21153;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
TSNet-SAC: Leveraging Transformers for Efficient Task Scheduling. (arXiv:2307.07445v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07445
&lt;/p&gt;
&lt;p&gt;
TSNet-SAC&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#32593;&#32476;&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#25351;&#23548;&#35757;&#32451;&#65292;&#36890;&#36807;&#24341;&#20837;&#28369;&#21160;&#22686;&#24378;&#32452;&#20214;&#21644;&#25193;&#23637;&#32452;&#20214;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#35299;&#20915;&#31639;&#27861;&#32570;&#38519;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25509;&#20837;&#22330;&#26223;&#12290;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;TSNet-SAC&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#32593;&#32476;&#65292;&#24182;&#20855;&#26377;&#26356;&#20248;&#30340;&#35843;&#24230;&#20915;&#31574;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#30340;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#20013;&#65292;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#20855;&#22791;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#24378;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#21551;&#21457;&#24335;&#31639;&#27861;&#30001;&#20110;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#26469;&#24471;&#21040;&#26368;&#20248;&#26041;&#26696;&#65292;&#22312;&#23454;&#26102;&#35843;&#24230;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#32593;&#32476;TSNet-SAC&#65292;&#20165;&#21033;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#25351;&#23548;TSNet&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28369;&#21160;&#22686;&#24378;&#32452;&#20214;&#65288;SAC&#65289;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#35299;&#20915;&#31639;&#27861;&#32570;&#38519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25193;&#23637;&#32452;&#20214;&#26469;&#22788;&#29702;&#22810;&#23610;&#24230;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#20379;&#32593;&#32476;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#24471;TSNet&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#25509;&#20837;&#22330;&#26223;&#12290;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;TSNet-SAC&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#32593;&#32476;&#65292;&#19982;&#21551;&#21457;&#24335;&#31639;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#20248;&#30340;&#35843;&#24230;&#20915;&#31574;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
In future 6G Mobile Edge Computing (MEC), autopilot systems require the capability of processing multimodal data with strong interdependencies. However, traditional heuristic algorithms are inadequate for real-time scheduling due to their requirement for multiple iterations to derive the optimal scheme. We propose a novel TSNet-SAC based on Transformer, that utilizes heuristic algorithms solely to guide the training of TSNet. Additionally, a Sliding Augment Component (SAC) is introduced to enhance the robustness and resolve algorithm defects. Furthermore, the Extender component is designed to handle multi-scale training data and provide network scalability, enabling TSNet to adapt to different access scenarios. Simulation demonstrates that TSNet-SAC outperforms existing networks in accuracy and robustness, achieving superior scheduling-making latency compared to heuristic algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#36827;&#34892;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38646;/&#23569;&#26679;&#26412;&#20998;&#23376;&#20998;&#31867;&#21644;&#29983;&#25104;&#26032;&#30340;&#35299;&#37322;&#26469;&#25512;&#36827;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.07443</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22686;&#24378;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Empower Molecular Property Prediction?. (arXiv:2307.07443v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#36827;&#34892;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38646;/&#23569;&#26679;&#26412;&#20998;&#23376;&#20998;&#31867;&#21644;&#29983;&#25104;&#26032;&#30340;&#35299;&#37322;&#26469;&#25512;&#36827;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22240;&#20854;&#22312;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#25913;&#21464;&#28508;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20256;&#32479;&#19978;&#65292;&#20998;&#23376;&#22270;&#21487;&#20197;&#34920;&#31034;&#20026;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#25110;SMILES&#25991;&#26412;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#23613;&#31649;&#21033;&#29992;LLMs&#26469;&#29702;&#35299;&#29992;SMILES&#34920;&#31034;&#30340;&#20998;&#23376;&#26159;&#33258;&#28982;&#30340;&#65292;&#20294;LLMs&#22914;&#20309;&#24433;&#21709;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25506;&#32034;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#38646;/&#23569;&#26679;&#26412;&#20998;&#23376;&#20998;&#31867;&#21644;&#20351;&#29992;LLMs&#29983;&#25104;&#30340;&#26032;&#35299;&#37322;&#20316;&#20026;&#20998;&#23376;&#34920;&#31034;&#20004;&#20010;&#35282;&#24230;&#25512;&#36827;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#31034;LLMs&#36827;&#34892;&#19978;&#19979;&#25991;&#20998;&#23376;&#20998;&#31867;&#24182;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#20026;&#21407;&#22987;SMILES&#29983;&#25104;&#35821;&#20041;&#20016;&#23500;&#30340;&#35299;&#37322;&#65292;&#24182;&#21033;&#29992;&#27492;&#26469;&#24494;&#35843;&#23567;&#35268;&#27169;&#30340;LM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction has gained significant attention due to its transformative potential in multiple scientific disciplines. Conventionally, a molecule graph can be represented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revolutionized the field of NLP. Although it is natural to utilize LLMs to assist in understanding molecules represented by SMILES, the exploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objective through two perspectives: zero/few-shot molecular classification, and using the new explanations generated by LLMs as representations of molecules. To be specific, we first prompt LLMs to do in-context molecular classification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#22686;&#24378;&#20987;&#24358;&#25351;&#24377;&#30340;&#23454;&#26102;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#21513;&#20182;&#20307;&#20987;&#25171;&#30340;&#35782;&#21035;&#21644;&#23884;&#20837;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07426</link><description>&lt;p&gt;
&#22768;&#23398;&#21513;&#20182;&#30340;&#23454;&#26102;&#25970;&#20987;&#25216;&#26415;&#35782;&#21035;&#21644;&#23884;&#20837;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Real-time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar. (arXiv:2307.07426v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#22686;&#24378;&#20987;&#24358;&#25351;&#24377;&#30340;&#23454;&#26102;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#21513;&#20182;&#20307;&#20987;&#25171;&#30340;&#35782;&#21035;&#21644;&#23884;&#20837;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;RT-MIR&#65289;&#22312;&#22686;&#24378;&#20256;&#32479;&#22768;&#23398;&#20048;&#22120;&#30340;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#22686;&#24378;&#20987;&#24358;&#25351;&#24377;&#30340;&#23454;&#26102;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#65292;&#23558;&#22768;&#23398;&#21513;&#20182;&#28436;&#22863;&#19982;&#21513;&#20182;&#20307;&#20987;&#25171;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20026;&#22686;&#24378;&#20048;&#22120;&#34920;&#28436;&#30340;&#23454;&#26102;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21046;&#23450;&#20102;&#20960;&#20010;&#35774;&#35745;&#30446;&#26631;&#65306;&#65288;i&#65289;&#22240;&#26524;&#32422;&#26463;&#65292;&#65288;ii&#65289;&#24863;&#30693;&#19978;&#21487;&#24573;&#30053;&#30340;&#38899;&#21160;&#24310;&#36831;&#65292;&#65288;iii&#65289;&#25511;&#21046;&#20146;&#23494;&#24615;&#25903;&#25345;&#65292;&#65288;iv&#65289;&#21512;&#25104;&#25511;&#21046;&#25903;&#25345;&#12290;&#25105;&#20204;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#21644;&#19982;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#20849;&#21516;&#35757;&#32451;&#30340;CNNs&#65292;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#23454;&#26102;&#21513;&#20182;&#20307;&#20987;&#25171;&#35782;&#21035;&#21644;&#23884;&#20837;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#22522;&#20110;&#25163;&#37096;&#37096;&#20301;&#21644;&#20301;&#32622;&#65292;&#24341;&#20837;&#20102;&#21513;&#20182;&#20307;&#20987;&#25171;&#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#36890;&#36807;&#25910;&#38598;&#24182;&#26681;&#25454;&#20998;&#31867;&#31995;&#32479;&#26631;&#35760;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#36328;&#25968;&#25454;&#38598;&#35780;&#20272;&#26041;&#27861;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time music information retrieval (RT-MIR) has much potential to augment the capabilities of traditional acoustic instruments. We develop RT-MIR techniques aimed at augmenting percussive fingerstyle, which blends acoustic guitar playing with guitar body percussion. We formulate several design objectives for RT-MIR systems for augmented instrument performance: (i) causal constraint, (ii) perceptually negligible action-to-sound latency, (iii) control intimacy support, (iv) synthesis control support. We present and evaluate real-time guitar body percussion recognition and embedding learning techniques based on convolutional neural networks (CNNs) and CNNs jointly trained with variational autoencoders (VAEs). We introduce a taxonomy of guitar body percussion based on hand part and location. We follow a cross-dataset evaluation approach by collecting three datasets labelled according to the taxonomy. The embedding quality of the models is assessed using KL-Divergence across distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;GPT&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;&#12290;&#20256;&#32479;&#30340;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#20174;&#32780;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#65292;&#23454;&#29616;&#26356;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07420</link><description>&lt;p&gt;
&#20351;&#29992;GPT&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition using GPT for identifying comparable companies. (arXiv:2307.07420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;GPT&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;&#12290;&#20256;&#32479;&#30340;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#20174;&#32780;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#65292;&#23454;&#29616;&#26356;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20844;&#20849;&#21644;&#31169;&#20154;&#20844;&#21496;&#65292;&#21487;&#27604;&#20844;&#21496;&#20998;&#26512;&#34987;&#24191;&#27867;&#29992;&#20316;&#20844;&#21496;&#20272;&#20540;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#31169;&#21215;&#32929;&#26435;&#20844;&#21496;&#30340;&#20272;&#20540;&#65292;&#35813;&#26041;&#27861;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#30340;&#20960;&#31181;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#36825;&#24448;&#24448;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#34892;&#19994;&#20998;&#31867;&#26041;&#26696;&#21644;/&#25110;&#20998;&#26512;&#24072;&#30340;&#30452;&#35273;&#21644;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#21644;&#31169;&#21215;&#32929;&#26435;&#34892;&#19994;&#24320;&#22987;&#20351;&#29992;&#26356;&#22810;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#32858;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#23545;&#20110;NLP&#26041;&#27861;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#20174;&#20844;&#21496;&#30340;&#32593;&#31449;&#25110;&#26469;&#33258;&#26576;&#20123;&#37329;&#34701;&#25968;&#25454;&#24211;&#31995;&#32479;&#30340;&#20844;&#21496;&#25551;&#36848;&#20013;&#25552;&#21462;&#20135;&#21697;&#23454;&#20307;&#65292;&#28982;&#21518;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#20844;&#21496;&#32500;&#22522;&#30334;&#31185;&#32593;&#31449;&#30340;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;GPT
&lt;/p&gt;
&lt;p&gt;
For both public and private firms, comparable companies analysis is widely used as a method for company valuation. In particular, the method is of great value for valuation of private equity companies. The several approaches to the comparable companies method usually rely on a qualitative approach to identifying similar peer companies, which tends to use established industry classification schemes and/or analyst intuition and knowledge. However, more quantitative methods have started being used in the literature and in the private equity industry, in particular, machine learning clustering, and natural language processing (NLP). For NLP methods, the process consists of extracting product entities from e.g., the company's website or company descriptions from some financial database system and then to perform similarity analysis. Here, using companies descriptions/summaries from publicly available companies' Wikipedia websites, we show that using large language models (LLMs), such as GPT
&lt;/p&gt;</description></item><item><title>RoPDA&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36830;&#32493;&#25552;&#31034;&#36827;&#34892;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#25216;&#26415;&#20197;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.07417</link><description>&lt;p&gt;
RoPDA&#65306;&#29992;&#20110;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#40065;&#26834;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07417
&lt;/p&gt;
&lt;p&gt;
RoPDA&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36830;&#32493;&#25552;&#31034;&#36827;&#34892;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#25216;&#26415;&#20197;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;NER&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#23384;&#22312;&#30772;&#22351;&#21477;&#27861;&#32467;&#26500;&#12289;&#26631;&#35760;-&#26631;&#31614;&#19981;&#21305;&#37197;&#21644;&#23545;&#22806;&#37096;&#30693;&#35782;&#25110;&#25163;&#21160;&#24037;&#20316;&#30340;&#38656;&#27714;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RoPDA: &#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;NER&#30340;&#40065;&#26834;&#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#36830;&#32493;&#25552;&#31034;&#65292;RoPDA&#36890;&#36807;&#20116;&#20010;&#22522;&#26412;&#30340;&#22686;&#24378;&#25805;&#20316;&#36827;&#34892;&#23454;&#20307;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#29983;&#25104;&#26631;&#31614;&#32763;&#36716;&#21644;&#20445;&#30041;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#20248;&#21270;&#22686;&#24378;&#26679;&#26412;&#30340;&#21033;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#65306;&#33258;&#19968;&#33268;&#24615;&#36807;&#28388;&#21644;&#28151;&#21512;&#12290;&#21069;&#32773;&#26377;&#25928;&#22320;&#28040;&#38500;&#20302;&#36136;&#37327;&#26679;&#26412;&#65292;&#21518;&#32773;&#38450;&#27490;&#30452;&#25509;&#21033;&#29992;&#26631;&#31614;&#32763;&#36716;&#26679;&#26412;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Data augmentation has been widely used in low-resource NER tasks to tackle the problem of data sparsity. However, previous data augmentation methods have the disadvantages of disrupted syntactic structures, token-label mismatch, and requirement for external knowledge or manual effort. To address these issues, we propose \textbf{Ro}bust \textbf{P}rompt-based \textbf{D}ata \textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained language models (PLMs) with continuous prompt, RoPDA performs entity augmentation and context augmentation through five fundamental augmentation operations to generate label-flipping and label-preserving examples. To optimize the utilization of the augmented samples, we present two techniques: Self-Consistency Filtering and mixup. The former effectively eliminates low-quality samples, while the latter prevents performance degradation arising from the direct utilization of label-flipping samples. Extensive experiments on three benchmarks from diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#29983;&#25104;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20248;&#21270;&#21407;&#22987;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.07415</link><description>&lt;p&gt;
AutoHint: &#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#19982;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoHint: Automatic Prompt Optimization with Hint Generation. (arXiv:2307.07415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#29983;&#25104;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20248;&#21270;&#21407;&#22987;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#34429;&#28982;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27880;&#37322;&#33021;&#21147;&#65292;&#20294;&#23558;&#27492;&#33021;&#21147;&#24212;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#20851;&#38190;&#22312;&#20110;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#27966;&#29983;&#30340;&#20016;&#23500;&#25351;&#23548;&#32435;&#20837;&#21407;&#22987;&#25552;&#31034;&#65292;&#20197;&#32487;&#25215;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#20016;&#23500;&#31216;&#20026;&#8220;&#25552;&#31034;&#8221;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20174;&#19968;&#20010;&#21021;&#22987;&#25552;&#31034;&#24320;&#22987;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#25351;&#23548;LLM&#20174;&#38169;&#35823;&#39044;&#27979;&#20013;&#25512;&#26029;&#20986;&#36873;&#23450;&#26679;&#26412;&#30340;&#26032;&#25552;&#31034;&#65292;&#28982;&#21518;&#20174;&#27599;&#20010;&#26679;&#26412;&#30340;&#25552;&#31034;&#20013;&#36827;&#34892;&#24635;&#32467;&#65292;&#24182;&#23558;&#32467;&#26524;&#28155;&#21152;&#22238;&#21021;&#22987;&#25552;&#31034;&#65292;&#24418;&#25104;&#19968;&#20010;&#26032;&#30340;&#20016;&#23500;&#25351;&#23548;&#12290;&#35813;&#26041;&#27861;&#22312;BIG-Bench&#25351;&#20196;&#25512;&#23548;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents AutoHint, a novel framework for automatic prompt engineering and optimization for Large Language Models (LLM). While LLMs have demonstrated remarkable ability in achieving high-quality annotation in various tasks, the key to applying this ability to specific tasks lies in developing high-quality prompts. Thus we propose a framework to inherit the merits of both in-context learning and zero-shot learning by incorporating enriched instructions derived from input-output demonstrations to optimize original prompt. We refer to the enrichment as the hint and propose a framework to automatically generate the hint from labeled data. More concretely, starting from an initial prompt, our method first instructs a LLM to deduce new hints for selected samples from incorrect predictions, and then summarizes from per-sample hints and adds the results back to the initial prompt to form a new, enriched instruction. The proposed method is evaluated on the BIG-Bench Instruction Induct
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#24102;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;ALPL&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#21644;&#22686;&#24378;&#36873;&#25321;&#20195;&#34920;&#24615;&#26679;&#26412;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#20363;&#26500;&#36896;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340; WorseNet &#26469;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07413</link><description>&lt;p&gt;
&#21033;&#29992;&#21453;&#20363;&#23545;&#24102;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploiting Counter-Examples for Active Learning with Partial labels. (arXiv:2307.07413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#24102;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;ALPL&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#21644;&#22686;&#24378;&#36873;&#25321;&#20195;&#34920;&#24615;&#26679;&#26412;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#20363;&#26500;&#36896;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340; WorseNet &#26469;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#24102;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;ALPL&#65289;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#19968;&#20010; oracle &#29992;&#37096;&#20998;&#26631;&#31614;&#23545;&#26597;&#35810;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#25918;&#23485;&#20102;&#23545;&#20934;&#30830;&#26631;&#27880;&#36807;&#31243;&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915; ALPL&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#22522;&#32447;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#29616;&#26377;&#30340; AL &#26694;&#26550;&#20013;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#36825;&#20010;&#22522;&#32447;&#20173;&#28982;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#65292;&#24182;&#19988;&#22312;&#26597;&#35810;&#36807;&#31243;&#20013;&#32570;&#20047;&#20195;&#34920;&#24615;&#30340;&#22522;&#20110;&#37096;&#20998;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#20013;&#20154;&#31867;&#25512;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#36825;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#23398;&#20064;&#27169;&#24335;&#26469;&#35299;&#20915;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#21516;&#26102;&#22686;&#24378; ALPL &#20013;&#36873;&#25321;&#20195;&#34920;&#24615;&#26679;&#26412;&#30340;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21453;&#36716;&#27599;&#20010;&#23454;&#20363;&#30340;&#37096;&#20998;&#26631;&#31614;&#26500;&#36896;&#21453;&#20363;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340; WorseNet &#26469;&#30452;&#25509;&#20174;&#36825;&#20123;&#21453;&#20363;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a new problem, \emph{active learning with partial labels} (ALPL). In this setting, an oracle annotates the query samples with partial labels, relaxing the oracle from the demanding accurate labeling process. To address ALPL, we first build an intuitive baseline that can be seamlessly incorporated into existing AL frameworks. Though effective, this baseline is still susceptible to the \emph{overfitting}, and falls short of the representative partial-label-based samples during the query process. Drawing inspiration from human inference in cognitive science, where accurate inferences can be explicitly derived from \emph{counter-examples} (CEs), our objective is to leverage this human-like learning pattern to tackle the \emph{overfitting} while enhancing the process of selecting representative samples in ALPL. Specifically, we construct CEs by reversing the partial labels for each instance, and then we propose a simple but effective WorseNet to directly learn from this c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;CheXOFA&#65292;&#36890;&#36807;&#22312;&#19968;&#33324;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36716;&#31227;&#21040;&#33016;&#37096;X&#23556;&#32447;&#39046;&#22495;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#38656;&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#65292;&#24182;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;RadSum23&#27979;&#35797;&#38598;&#30340;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2307.07409</link><description>&lt;p&gt;
KU-DMIS-MSRA&#22312;RadSum23&#20013;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization. (arXiv:2307.07409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;CheXOFA&#65292;&#36890;&#36807;&#22312;&#19968;&#33324;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36716;&#31227;&#21040;&#33016;&#37096;X&#23556;&#32447;&#39046;&#22495;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#38656;&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#65292;&#24182;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;RadSum23&#27979;&#35797;&#38598;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CheXOFA&#65292;&#19968;&#31181;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#39046;&#22495;&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLM)&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#22312;&#19968;&#33324;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20877;&#36716;&#31227;&#21040;&#33016;&#37096;X&#23556;&#32447;&#39046;&#22495;&#12290;&#22312;&#19968;&#20010;&#33879;&#21517;&#30340;VLM&#20013;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#32479;&#19968;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#27169;&#24335;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20174;&#26377;&#38480;&#30340;&#39046;&#22495;&#36164;&#28304;&#20013;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#38656;&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#36890;&#36807;&#22312;BioNLP&#20849;&#20139;&#20219;&#21153;&#25552;&#20379;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21463;&#30410;&#20110;&#36328;&#22810;&#20010;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#38598;&#25104;&#21644;&#20107;&#23454;&#26657;&#20934;&#31561;&#24494;&#22937;&#30340;&#25216;&#24039;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;RadSum23&#30340;&#38544;&#34255;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema. It enables the model to effectively learn the required knowledge and skills from limited resources in the domain. Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task, our model benefits from its training across multiple tasks and domains. With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#23391;&#21152;&#25289;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#30830;&#23450;&#32473;&#23450;&#25991;&#26412;&#30340;&#26368;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2307.07392</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#26041;&#27861;&#22686;&#24378;&#23391;&#21152;&#25289;&#25991;&#26412;&#25688;&#35201;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach. (arXiv:2307.07392v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#23391;&#21152;&#25289;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#30830;&#23450;&#32473;&#23450;&#25991;&#26412;&#30340;&#26368;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#26082;&#39640;&#25928;&#21448;&#20934;&#30830;&#30340;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#65292;&#25506;&#32034;&#33021;&#22815;&#22686;&#24378;&#19987;&#20026;&#23391;&#21152;&#25289;&#25991;&#26412;&#25688;&#35201;&#32780;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#31934;&#30830;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;&#20154;&#20204;&#21487;&#20197;&#20351;&#29992;&#20247;&#22810;&#30340;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#25688;&#35201;&#27169;&#22411;&#29983;&#25104;&#30340;&#21508;&#31181;&#36873;&#39033;&#20013;&#65292;&#30830;&#23450;&#32473;&#23450;&#25991;&#26412;&#30340;&#26368;&#20855;&#20449;&#24687;&#37327;&#21644;&#30456;&#20851;&#24615;&#30340;&#25688;&#35201;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#22235;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#23391;&#21152;&#25289;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#30830;&#23450;&#32473;&#23450;&#25991;&#26412;&#30340;&#26368;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#12290;&#35813;&#36807;&#31243;&#39318;&#20808;&#23545;&#36755;&#20837;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#21253;&#25324;&#21435;&#38500;&#29305;&#27530;&#23383;&#31526;&#21644;&#26631;&#28857;&#31526;&#21495;&#31561;&#19981;&#24517;&#35201;&#30340;&#20803;&#32032;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#22235;&#20010;&#39044;&#35757;&#32451;&#30340;&#25688;&#35201;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing need for text summarization techniques that are both efficient and accurate, it becomes crucial to explore avenues that enhance the quality and precision of pre-trained models specifically tailored for summarizing Bengali texts. When it comes to text summarization tasks, there are numerous pre-trained transformer models at one's disposal. Consequently, it becomes quite a challenge to discern the most informative and relevant summary for a given text among the various options generated by these pre-trained summarization models. This paper aims to identify the most accurate and informative summary for a given text by utilizing a simple but effective ranking-based approach that compares the output of four different pre-trained Bengali text summarization models. The process begins by carrying out preprocessing of the input text that involves eliminating unnecessary elements such as special characters and punctuation marks. Next, we utilize four pre-trained summarization
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#31070;&#32463;&#32593;&#32476;AIC-AB NET&#23558;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#25991;&#26412;&#23646;&#24615;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#36755;&#20837;&#25991;&#26412;&#23646;&#24615;&#20449;&#24687;&#65292;&#25552;&#21319;&#22270;&#20687;&#23383;&#24149;&#30340;&#24615;&#33021;&#21644;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07370</link><description>&lt;p&gt;
AIC-AB NET&#65306;&#19968;&#31181;&#20855;&#26377;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#22270;&#20687;&#23383;&#24149;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention and Text Attributes. (arXiv:2307.07370v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07370
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#31070;&#32463;&#32593;&#32476;AIC-AB NET&#23558;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#25991;&#26412;&#23646;&#24615;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#36755;&#20837;&#25991;&#26412;&#23646;&#24615;&#20449;&#24687;&#65292;&#25552;&#21319;&#22270;&#20687;&#23383;&#24149;&#30340;&#24615;&#33021;&#21644;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#23646;&#24615;-&#20449;&#24687;-&#32452;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;AIC-AB NET&#65292;&#35813;&#32593;&#32476;&#23558;&#31354;&#38388;&#27880;&#24847;&#21147;&#26550;&#26500;&#21644;&#25991;&#26412;&#23646;&#24615;&#32467;&#21512;&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#20013;&#12290;&#23545;&#20110;&#23383;&#24149;&#29983;&#25104;&#65292;&#33258;&#36866;&#24212;&#31354;&#38388;&#27880;&#24847;&#21147;&#30830;&#23450;&#26368;&#33021;&#20195;&#34920;&#22270;&#20687;&#30340;&#22270;&#20687;&#21306;&#22495;&#65292;&#24182;&#20915;&#23450;&#26159;&#20391;&#37325;&#20110;&#35270;&#35273;&#29305;&#24449;&#36824;&#26159;&#35270;&#35273;&#26631;&#35760;&#12290;&#25991;&#26412;&#23646;&#24615;&#20449;&#24687;&#21516;&#27493;&#36755;&#20837;&#35299;&#30721;&#22120;&#65292;&#20197;&#24110;&#21161;&#22270;&#20687;&#35782;&#21035;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;MS COCO&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#25552;&#20986;&#30340;&#26102;&#23578;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23545;AICAB NET&#30340;&#27979;&#35797;&#21644;&#35780;&#20272;&#12290;&#35813;&#26102;&#23578;&#25968;&#25454;&#38598;&#34987;&#29992;&#20316;&#21333;&#29289;&#20307;&#22270;&#20687;&#30340;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#21644;&#21435;&#38500;&#20102;&#37096;&#20998;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;MSCOCO&#22270;&#20687;&#21644;&#25105;&#20204;&#30340;&#21333;&#29289;&#20307;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;AIC-AB NET&#20248;&#20110;&#22522;&#32447;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image captioning is a significant field across computer vision and natural language processing. We propose and present AIC-AB NET, a novel Attribute-Information-Combined Attention-Based Network that combines spatial attention architecture and text attributes in an encoder-decoder. For caption generation, adaptive spatial attention determines which image region best represents the image and whether to attend to the visual features or the visual sentinel. Text attribute information is synchronously fed into the decoder to help image recognition and reduce uncertainty. We have tested and evaluated our AICAB NET on the MS COCO dataset and a new proposed Fashion dataset. The Fashion dataset is employed as a benchmark of single-object images. The results show the superior performance of the proposed model compared to the state-of-the-art baseline and ablated models on both the images from MSCOCO and our single-object images. Our AIC-AB NET outperforms the baseline adaptive attention network 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21487;&#33021;&#23545;&#20844;&#24320;&#30340;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#21644;&#30693;&#35782;&#36164;&#28304;&#24102;&#26469;&#37325;&#22823;&#24433;&#21709;&#65292;&#23588;&#20854;&#22312;Stack Overflow&#36825;&#26679;&#30340;&#32534;&#31243;&#38382;&#31572;&#24179;&#21488;&#19978;&#65292;ChatGPT&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#27963;&#21160;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2307.07367</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#23383;&#20844;&#20849;&#36164;&#28304;&#26500;&#25104;&#23041;&#32961;&#21527;&#65311;&#26469;&#33258;Stack Overflow&#27963;&#21160;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models a Threat to Digital Public Goods? Evidence from Activity on Stack Overflow. (arXiv:2307.07367v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07367
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21487;&#33021;&#23545;&#20844;&#24320;&#30340;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#21644;&#30693;&#35782;&#36164;&#28304;&#24102;&#26469;&#37325;&#22823;&#24433;&#21709;&#65292;&#23588;&#20854;&#22312;Stack Overflow&#36825;&#26679;&#30340;&#32534;&#31243;&#38382;&#31572;&#24179;&#21488;&#19978;&#65292;ChatGPT&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#27963;&#21160;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;ChatGPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#20026;&#29992;&#25143;&#25552;&#20379;&#21508;&#31181;&#20027;&#39064;&#30340;&#20449;&#24687;&#65292;&#25104;&#20026;&#25628;&#32034;&#32593;&#32476;&#21644;&#22312;&#32447;&#27714;&#21161;&#30340;&#28508;&#22312;&#26367;&#20195;&#21697;&#12290;&#20294;&#30001;&#20110;&#29992;&#25143;&#19982;&#27169;&#22411;&#36827;&#34892;&#31169;&#19979;&#20132;&#20114;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#22823;&#24133;&#20943;&#23569;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#21644;&#30693;&#35782;&#36164;&#28304;&#12290;&#36825;&#31181;&#26367;&#20195;&#21487;&#33021;&#32473;&#26410;&#26469;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#33719;&#21462;&#24102;&#26469;&#37325;&#22823;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;Stack Overflow&#30340;&#27963;&#21160;&#24773;&#20917;&#65288;&#36825;&#26159;&#35745;&#31639;&#26426;&#32534;&#31243;&#39046;&#22495;&#39046;&#20808;&#30340;&#22312;&#32447;&#38382;&#31572;&#24179;&#21488;&#65289;&#65292;&#30740;&#31350;&#20102;ChatGPT&#21457;&#24067;&#22914;&#20309;&#25913;&#21464;&#20102;&#32593;&#32476;&#19978;&#30340;&#20154;&#24037;&#29983;&#25104;&#24320;&#25918;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#20420;&#32599;&#26031;&#21644;&#20013;&#22269;&#30340;&#31867;&#20284;&#24179;&#21488;&#65288;&#22240;&#20026;ChatGPT&#30340;&#35775;&#38382;&#21463;&#38480;&#65289;&#20197;&#21450;&#25968;&#23398;&#30456;&#20851;&#35770;&#22363;&#65288;ChatGPT&#33021;&#21147;&#36739;&#24369;&#65289;&#65292;Stack Overflow&#30340;&#27963;&#21160;&#26174;&#33879;&#20943;&#23569;&#12290;&#24046;&#20998;&#20013;&#24046;&#20998;&#27169;&#22411;&#20272;&#35745;Stack Overflow&#30340;&#27599;&#21608;&#24086;&#23376;&#25968;&#37327;&#20943;&#23569;&#20102;16&#65285;&#12290;&#36825;&#20010;&#25928;&#24212;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Large language models like ChatGPT efficiently provide users with information about various topics, presenting a potential substitute for searching the web and asking people for help online. But since users interact privately with the model, these models may drastically reduce the amount of publicly available human-generated data and knowledge resources. This substitution can present a significant problem in securing training data for future models. In this work, we investigate how the release of ChatGPT changed human-generated open data on the web by analyzing the activity on Stack Overflow, the leading online Q\&amp;A platform for computer programming. We find that relative to its Russian and Chinese counterparts, where access to ChatGPT is limited, and to similar forums for mathematics, where ChatGPT is less capable, activity on Stack Overflow significantly decreased. A difference-in-differences model estimates a 16\% decrease in weekly posts on Stack Overflow. This effect increases in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#26694;&#26550;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#25991;&#26412;&#36164;&#28304;&#19979;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12290;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#23618;&#23545;&#38899;&#39057;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#26694;&#26550;&#23558;&#36825;&#20123;&#34920;&#31034;&#24402;&#31867;&#20026;&#23569;&#37327;&#31867;&#20284;&#38899;&#32032;&#30340;&#21333;&#20803;&#65292;&#29992;&#20110;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07325</link><description>&lt;p&gt;
&#26080;&#36164;&#28304;&#35821;&#38899;&#24212;&#29992;&#30340;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications. (arXiv:2307.07325v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#26694;&#26550;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#25991;&#26412;&#36164;&#28304;&#19979;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12290;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#23618;&#23545;&#38899;&#39057;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#26694;&#26550;&#23558;&#36825;&#20123;&#34920;&#31034;&#24402;&#31867;&#20026;&#23569;&#37327;&#31867;&#20284;&#38899;&#32032;&#30340;&#21333;&#20803;&#65292;&#29992;&#20110;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25991;&#26412;&#36164;&#28304;&#19979;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26159;&#35768;&#22810;&#20302;&#36164;&#28304;&#35821;&#38899;&#24212;&#29992;&#20013;&#19968;&#20010;&#38750;&#24120;&#24863;&#20852;&#36259;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#65288;HUC&#65289;&#26694;&#26550;&#20174;&#21407;&#22987;&#38899;&#39057;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#27169;&#22411;&#30340;&#36755;&#20837;&#21253;&#25324;&#34987;&#31383;&#21475;&#21270;&#24182;&#32463;&#36807;1-D&#21367;&#31215;&#23618;&#22788;&#29702;&#30340;&#38899;&#39057;&#26679;&#26412;&#12290;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22359;&#23398;&#20064;&#21040;&#30340;"&#26102;&#39057;"&#34920;&#31034;&#32463;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#23618;&#36827;&#19968;&#27493;&#22788;&#29702;&#65292;&#20026;&#27599;&#20010;&#31383;&#21475;&#21270;&#29255;&#27573;&#29983;&#25104;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#12290;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#26694;&#26550;&#23545;&#36825;&#20123;&#34920;&#31034;&#36827;&#34892;&#20998;&#31867;&#65292;&#23558;&#20854;&#24402;&#31867;&#20026;&#23569;&#37327;&#31867;&#20284;&#38899;&#32032;&#30340;&#21333;&#20803;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290;&#30446;&#26631;&#21253;&#25324;&#27599;&#20010;&#38899;&#39057;&#29255;&#27573;&#30340;&#31867;&#20284;&#38899;&#32032;&#20266;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#26159;&#20351;&#29992;&#36845;&#20195;k-means&#31639;&#27861;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20123;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation learning of speech, without textual resources, is an area of significant interest for many low resource speech applications. In this paper, we describe an approach to self-supervised representation learning from raw audio using a hidden unit clustering (HUC) framework. The input to the model consists of audio samples that are windowed and processed with 1-D convolutional layers. The learned "time-frequency" representations from the convolutional neural network (CNN) module are further processed with long short term memory (LSTM) layers which generate a contextual vector representation for every windowed segment. The HUC framework, allowing the categorization of the representations into a small number of phoneme-like units, is used to train the model for learning semantically rich speech representations. The targets consist of phoneme-like pseudo labels for each audio segment and these are generated with an iterative k-means algorithm. We explore techniques that impro
&lt;/p&gt;</description></item><item><title>C3&#26159;&#22522;&#20110;ChatGPT&#30340;&#38646;-shot Text-to-SQL&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#25552;&#20379;&#31995;&#32479;&#24615;&#30340;&#22788;&#29702;&#26041;&#27861;&#65292;&#22312;Spider Challenge&#19978;&#21462;&#24471;&#20102;82.3%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07306</link><description>&lt;p&gt;
C3: &#20351;&#29992;ChatGPT&#36827;&#34892;&#38646;-shot Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
C3: Zero-shot Text-to-SQL with ChatGPT. (arXiv:2307.07306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07306
&lt;/p&gt;
&lt;p&gt;
C3&#26159;&#22522;&#20110;ChatGPT&#30340;&#38646;-shot Text-to-SQL&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#25552;&#20379;&#31995;&#32479;&#24615;&#30340;&#22788;&#29702;&#26041;&#27861;&#65292;&#22312;Spider Challenge&#19978;&#21462;&#24471;&#20102;82.3%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#38646;-shot Text-to-SQL&#26041;&#27861;&#65292;&#21517;&#20026;C3&#65292;&#20854;&#22312;Spider&#30340;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;82.3%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#24182;&#25104;&#20026;Spider Challenge&#20013;&#26368;&#20808;&#36827;&#30340;&#38646;-shot Text-to-SQL&#26041;&#27861;&#12290;C3&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;Clear Prompting (CP)&#65292;Calibration with Hints (CH)&#21644;Consistent Output (CO)&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#27169;&#22411;&#36755;&#20837;&#65292;&#27169;&#22411;&#20559;&#24046;&#21644;&#27169;&#22411;&#36755;&#20986;&#12290;&#23427;&#20026;&#38646;-shot Text-to-SQL&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3, which achieves 82.3\% in terms of execution accuracy on the holdout test set of Spider and becomes the state-of-the-art zero-shot Text-to-SQL method on the Spider Challenge. C3 consists of three key components: Clear Prompting (CP), Calibration with Hints (CH), and Consistent Output (CO), which are corresponding to the model input, model bias and model output respectively. It provides a systematic treatment for zero-shot Text-to-SQL. Extensive experiments have been conducted to verify the effectiveness and efficiency of our proposed method.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#23610;&#24230;&#26102;&#31354;&#39592;&#26550;&#21305;&#37197;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#27425;&#24615;&#21160;&#20316;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#26102;&#38388;&#39034;&#24207;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#29305;&#24449;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.07286</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#22810;&#23610;&#24230;&#26102;&#31354;&#39592;&#26550;&#21305;&#37197;&#30340;&#19968;&#27425;&#24615;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching. (arXiv:2307.07286v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07286
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#23610;&#24230;&#26102;&#31354;&#39592;&#26550;&#21305;&#37197;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#27425;&#24615;&#21160;&#20316;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#26102;&#38388;&#39034;&#24207;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#29305;&#24449;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27425;&#24615;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#26088;&#22312;&#36890;&#36807;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#27169;&#22411;&#65292;&#30001;&#20110;&#25910;&#38598;&#21644;&#26631;&#27880;&#22823;&#35268;&#27169;&#39592;&#26550;&#21160;&#20316;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#23427;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#36890;&#36807;&#30452;&#25509;&#27604;&#36739;&#29305;&#24449;&#21521;&#37327;&#26469;&#21305;&#37197;&#39592;&#26550;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#39592;&#26550;&#25968;&#25454;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#26102;&#38388;&#39034;&#24207;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#27425;&#24615;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#26102;&#31354;&#29305;&#24449;&#21305;&#37197;&#26469;&#22788;&#29702;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#34920;&#31034;&#39592;&#26550;&#25968;&#25454;&#65292;&#24182;&#20174;&#20004;&#20010;&#35282;&#24230;&#23454;&#29616;&#26368;&#20248;&#29305;&#24449;&#21305;&#37197;&#12290;&#31532;&#19968;&#31181;&#26159;&#22810;&#23610;&#24230;&#21305;&#37197;&#65292;&#21516;&#26102;&#25429;&#25417;&#22810;&#20010;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#39592;&#26550;&#25968;&#25454;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#31532;&#20108;&#31181;&#26159;&#36328;&#23610;&#24230;&#21305;&#37197;&#65292;&#36890;&#36807;&#25429;&#25417;&#26679;&#26412;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#36816;&#21160;&#24133;&#24230;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
One-shot skeleton action recognition, which aims to learn a skeleton action recognition model with a single training sample, has attracted increasing interest due to the challenge of collecting and annotating large-scale skeleton action data. However, most existing studies match skeleton sequences by comparing their feature vectors directly which neglects spatial structures and temporal orders of skeleton data. This paper presents a novel one-shot skeleton action recognition technique that handles skeleton action recognition via multi-scale spatial-temporal feature matching. We represent skeleton data at multiple spatial and temporal scales and achieve optimal feature matching from two perspectives. The first is multi-scale matching which captures the scale-wise semantic relevance of skeleton data at multiple spatial and temporal scales simultaneously. The second is cross-scale matching which handles different motion magnitudes and speeds by capturing sample-wise relevance across multi
&lt;/p&gt;</description></item><item><title>AudioInceptionNeXt&#26159;&#19968;&#20010;&#22522;&#20110;&#21333;&#20010;&#27969;&#30340;CNN&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;-&#39057;&#29575;log-mel-spectrogram&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#32467;&#21512;&#24182;&#34892;&#30340;&#22810;&#23610;&#24230;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#26680;&#26469;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#26102;&#38388;&#21644;&#39057;&#29575;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;2023&#24180;EPIC-SOUND&#38899;&#39057;&#20132;&#20114;&#35782;&#21035;&#25361;&#25112;&#36187;&#19978;55.43%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.07265</link><description>&lt;p&gt;
AudioInceptionNeXt&#65306;TCL AI LAB&#23545;EPIC-SOUND&#38899;&#39057;&#20132;&#20114;&#35782;&#21035;&#25361;&#25112;&#36187;2023&#30340;&#25237;&#31295;
&lt;/p&gt;
&lt;p&gt;
AudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND Audio-Based-Interaction-Recognition Challenge 2023. (arXiv:2307.07265v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07265
&lt;/p&gt;
&lt;p&gt;
AudioInceptionNeXt&#26159;&#19968;&#20010;&#22522;&#20110;&#21333;&#20010;&#27969;&#30340;CNN&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;-&#39057;&#29575;log-mel-spectrogram&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#32467;&#21512;&#24182;&#34892;&#30340;&#22810;&#23610;&#24230;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#26680;&#26469;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#26102;&#38388;&#21644;&#39057;&#29575;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;2023&#24180;EPIC-SOUND&#38899;&#39057;&#20132;&#20114;&#35782;&#21035;&#25361;&#25112;&#36187;&#19978;55.43%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#25105;&#20204;&#23545;2023&#24180;Epic-Kitchen EPIC-SOUNDS&#38899;&#39057;&#20132;&#20114;&#35782;&#21035;&#25361;&#25112;&#36187;&#30340;&#25552;&#20132;&#30340;&#25216;&#26415;&#32454;&#33410;&#12290;&#20219;&#21153;&#26159;&#23398;&#20064;&#20174;&#38899;&#39057;&#26679;&#26412;&#21040;&#30456;&#24212;&#21160;&#20316;&#26631;&#31614;&#30340;&#26144;&#23556;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioInceptionNeXt&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#21333;&#20010;&#27969;&#30340;CNN&#26550;&#26500;&#65292;&#23427;&#22522;&#20110;&#38899;&#39057;&#26679;&#26412;&#30340;&#26102;&#38388;-&#39057;&#29575;&#23545;&#25968;-&#26757;&#23572;&#35889;&#22270;&#36827;&#34892;&#25805;&#20316;&#12290;&#21463;InceptionNeXt&#35774;&#35745;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;AudioInceptionNeXt&#22359;&#20013;&#25552;&#20986;&#20102;&#24182;&#34892;&#30340;&#22810;&#23610;&#24230;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#26680;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#26102;&#38388;&#21644;&#39057;&#29575;&#20449;&#24687;&#12290;&#22823;&#23610;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#26680;&#25429;&#25417;&#27963;&#21160;&#30340;&#38271;&#26102;&#38388;&#25345;&#32493;&#24615;&#21644;&#20840;&#23616;&#39057;&#29575;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#23567;&#23610;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#26680;&#25429;&#25417;&#27963;&#21160;&#30340;&#30701;&#26102;&#38388;&#25345;&#32493;&#24615;&#21644;&#39057;&#29575;&#20449;&#24687;&#30340;&#23616;&#37096;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25361;&#25112;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;55.43%&#30340;top-1&#20934;&#30830;&#29575;&#65292;&#25490;&#21517;&#20026;...
&lt;/p&gt;
&lt;p&gt;
This report presents the technical details of our submission to the 2023 Epic-Kitchen EPIC-SOUNDS Audio-Based Interaction Recognition Challenge. The task is to learn the mapping from audio samples to their corresponding action labels. To achieve this goal, we propose a simple yet effective single-stream CNN-based architecture called AudioInceptionNeXt that operates on the time-frequency log-mel-spectrogram of the audio samples. Motivated by the design of the InceptionNeXt, we propose parallel multi-scale depthwise separable convolutional kernels in the AudioInceptionNeXt block, which enable the model to learn the time and frequency information more effectively. The large-scale separable kernels capture the long duration of activities and the global frequency semantic information, while the small-scale separable kernels capture the short duration of activities and local details of frequency information. Our approach achieved 55.43% of top-1 accuracy on the challenge test set, ranked as 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#25193;&#23637;&#30340;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22320;&#22270;&#20013;&#21160;&#24577;&#28857;&#21435;&#38500;&#25216;&#26415;&#65292;&#37325;&#26500;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#20998;&#26512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;&#35813;&#22522;&#20934;&#20351;&#29992;&#22810;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20844;&#24320;&#33719;&#21462;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.07260</link><description>&lt;p&gt;
&#19968;&#31181;&#28857;&#20113;&#22320;&#22270;&#20013;&#21160;&#24577;&#28857;&#21435;&#38500;&#30340;&#21160;&#24577;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Dynamic Points Removal Benchmark in Point Cloud Maps. (arXiv:2307.07260v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#25193;&#23637;&#30340;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22320;&#22270;&#20013;&#21160;&#24577;&#28857;&#21435;&#38500;&#25216;&#26415;&#65292;&#37325;&#26500;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#20998;&#26512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;&#35813;&#22522;&#20934;&#20351;&#29992;&#22810;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20844;&#24320;&#33719;&#21462;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#28857;&#20113;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#22320;&#22270;&#34920;&#31034;&#26041;&#24335;&#12290;&#20174;&#23450;&#20301;&#21644;&#20840;&#23616;&#36335;&#24452;&#35268;&#21010;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#19982;&#21160;&#24577;&#29289;&#20307;&#23545;&#24212;&#30340;&#28857;&#20250;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#28857;&#20113;&#20013;&#21160;&#24577;&#28857;&#21435;&#38500;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#28165;&#26224;&#30340;&#27604;&#36739;&#35780;&#20272;&#21644;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#25193;&#23637;&#30340;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22320;&#22270;&#20013;&#21160;&#24577;&#28857;&#21435;&#38500;&#25216;&#26415;&#12290;&#23427;&#21253;&#25324;&#37325;&#26500;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#23616;&#38480;&#24615;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;&#35813;&#22522;&#20934;&#20351;&#29992;&#22810;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#19982;&#25105;&#20204;&#30740;&#31350;&#30456;&#20851;&#30340;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#37117;&#21487;&#20197;&#20844;&#24320;&#33719;&#24471;&#65292;&#20197;&#20415;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#24320;&#21457;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of robotics, the point cloud has become an essential map representation. From the perspective of downstream tasks like localization and global path planning, points corresponding to dynamic objects will adversely affect their performance. Existing methods for removing dynamic points in point clouds often lack clarity in comparative evaluations and comprehensive analysis. Therefore, we propose an easy-to-extend unified benchmarking framework for evaluating techniques for removing dynamic points in maps. It includes refactored state-of-art methods and novel metrics to analyze the limitations of these approaches. This enables researchers to dive deep into the underlying reasons behind these limitations. The benchmark makes use of several datasets with different sensor types. All the code and datasets related to our study are publicly available for further development and utilization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#35774;&#35745;&#30340;&#30456;&#20851;&#35201;&#32032;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.07255</link><description>&lt;p&gt;
&#23545;&#35805;&#20195;&#29702;101&#65306;&#35774;&#35745;&#26377;&#25928;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#32032;&#21021;&#23398;&#32773;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems. (arXiv:2307.07255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#35774;&#35745;&#30340;&#30456;&#20851;&#35201;&#32032;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;&#21516;&#34892;&#36827;&#34892;&#20132;&#27969;&#26469;&#20998;&#20139;&#24819;&#27861;&#26159;&#20154;&#31867;&#20114;&#21160;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#22312;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#23548;&#33268;&#23545;&#35805;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#20010;&#20219;&#21153;&#21516;&#26102;&#25506;&#32034;&#65292;&#24403;&#21069;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#29616;&#29366;&#21464;&#24471;&#20998;&#25955;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24110;&#21161;&#20174;&#38646;&#24320;&#22987;&#35774;&#35745;&#23545;&#35805;&#20195;&#29702;&#30340;&#20174;&#19994;&#32773;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#29305;&#24449;&#12289;&#25903;&#25345;&#20219;&#21153;&#12289;&#30456;&#24212;&#30340;&#24320;&#25918;&#39046;&#22495;&#25968;&#25454;&#38598;&#20197;&#21450;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#30340;&#23545;&#35805;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26159;&#26114;&#36149;&#19988;&#20887;&#20313;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharing ideas through communication with peers is the primary mode of human interaction. Consequently, extensive research has been conducted in the area of conversational AI, leading to an increase in the availability and diversity of conversational tasks, datasets, and methods. However, with numerous tasks being explored simultaneously, the current landscape of conversational AI becomes fragmented. Therefore, initiating a well-thought-out model for a dialogue agent can pose significant challenges for a practitioner. Towards highlighting the critical ingredients needed for a practitioner to design a dialogue agent from scratch, the current study provides a comprehensive overview of the primary characteristics of a dialogue agent, the supporting tasks, their corresponding open-domain datasets, and the methods used to benchmark these datasets. We observe that different methods have been used to tackle distinct dialogue tasks. However, building separate models for each task is costly and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#37327;&#21270;&#21644;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#25932;&#23545;&#36755;&#20837;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07250</link><description>&lt;p&gt;
&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#30340;&#22240;&#26524;&#21442;&#25968;&#20272;&#35745;&#26469;&#32531;&#35299;&#25932;&#23545;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07250
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#37327;&#21270;&#21644;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#25932;&#23545;&#36755;&#20837;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#34893;&#29983;&#20986;&#30340;&#25932;&#23545;&#20363;&#23376;&#21487;&#20197;&#36731;&#26494;&#22320;&#25439;&#23475;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#38450;&#27490;&#28508;&#22312;&#30340;&#23041;&#32961;&#65292;&#21508;&#31181;&#22522;&#20110;&#25932;&#23545;&#35757;&#32451;&#30340;&#38450;&#24481;&#26041;&#27861;&#36805;&#36895;&#22686;&#38271;&#65292;&#24182;&#25104;&#20026;&#31283;&#20581;&#24615;&#30340;&#20107;&#23454;&#19978;&#26631;&#20934;&#26041;&#27861;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#25104;&#23601;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25932;&#23545;&#33030;&#24369;&#24615;&#22312;&#19981;&#21516;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#26576;&#20123;&#33030;&#24369;&#24615;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#28145;&#23618;&#27425;&#30340;&#26550;&#26500;&#21644;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36825;&#31181;&#22855;&#29305;&#30340;&#29616;&#35937;&#20173;&#28982;&#26080;&#27861;&#32531;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;ADML&#65289;&#30340;&#22240;&#26524;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#37327;&#21270;&#32593;&#32476;&#39044;&#27979;&#30340;&#25932;&#23545;&#33030;&#24369;&#24615;&#31243;&#24230;&#65292;&#24182;&#25429;&#25417;&#23545;&#32467;&#26524;&#30340;&#22788;&#29702;&#25928;&#26524;&#12290;ADML&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#25932;&#23545;&#25200;&#21160;&#26412;&#36523;&#30340;&#22240;&#26524;&#21442;&#25968;&#65292;&#24182;&#20943;&#36731;&#21487;&#33021;&#25439;&#23475;&#31283;&#20581;&#24615;&#30340;&#36127;&#38754;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision process of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across targets and certain vulnerabilities remain prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods. To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adversarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial perturbations per se and mitigate negative effects that can potentially damage robustness, bridgi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36827;&#34892;&#20102;&#23545;OneMinMax&#38382;&#39064;&#19978;&#20351;&#29992;GSEMO&#31639;&#27861;&#36827;&#34892;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#20005;&#26684;&#36816;&#34892;&#26102;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#33021;&#22312;&#26399;&#26395;&#26102;&#38388;$O(n^2)$&#20869;&#25214;&#21040;&#20855;&#26377;&#26368;&#20339;&#22810;&#26679;&#24615;&#30340;&#31181;&#32676;&#12290;</title><link>http://arxiv.org/abs/2307.07248</link><description>&lt;p&gt;
&#23545;OneMinMax&#19978;&#20351;&#29992;GSEMO&#36827;&#34892;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#20005;&#26684;&#36816;&#34892;&#26102;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Rigorous Runtime Analysis of Diversity Optimization with GSEMO on OneMinMax. (arXiv:2307.07248v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#23545;OneMinMax&#38382;&#39064;&#19978;&#20351;&#29992;GSEMO&#31639;&#27861;&#36827;&#34892;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#20005;&#26684;&#36816;&#34892;&#26102;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#33021;&#22312;&#26399;&#26395;&#26102;&#38388;$O(n^2)$&#20869;&#25214;&#21040;&#20855;&#26377;&#26368;&#20339;&#22810;&#26679;&#24615;&#30340;&#31181;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#22810;&#26679;&#24615;&#20248;&#21270;&#26088;&#22312;&#23547;&#25214;&#28385;&#36275;&#20854;&#36866;&#24212;&#24230;&#32422;&#26463;&#30340;&#22810;&#26679;&#24615;&#35299;&#38598;&#12290;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#35813;&#32422;&#26463;&#21487;&#33021;&#35201;&#27714;&#35299;&#20915;&#26041;&#26696;&#20026;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;GSEMO&#31639;&#27861;&#22312;&#19968;&#20010;&#21452;&#30446;&#26631;&#22522;&#20934;&#38382;&#39064;OneMinMax&#19978;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#22810;&#26679;&#24615;&#30340;&#21551;&#21457;&#24335;&#20248;&#21270;&#20854;&#31181;&#32676;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23545;&#20248;&#21270;&#30340;&#26368;&#21518;&#19968;&#27493;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#36816;&#34892;&#26102;&#20998;&#26512;&#65292;&#21363;&#31639;&#27861;&#20174;&#20855;&#26377;&#27425;&#20248;&#22810;&#26679;&#24615;&#30340;&#31181;&#32676;&#24320;&#22987;&#65292;&#24182;&#35777;&#26126;&#22312;&#38382;&#39064;&#35268;&#27169;$n$&#20026;&#22855;&#25968;&#26102;&#65292;&#23427;&#20197;&#26399;&#26395;&#26102;&#38388;$O(n^2)$&#25214;&#21040;&#20855;&#26377;&#26368;&#20339;&#22810;&#26679;&#24615;&#30340;&#31181;&#32676;&#12290;&#20026;&#20102;&#36798;&#21040;&#25105;&#20204;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#31181;&#32676;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#36825;&#21453;&#26144;&#20102;&#31181;&#32676;&#20013;&#21464;&#21270;&#30340;&#39057;&#29575;&#21450;&#20854;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolutionary diversity optimization aims at finding a diverse set of solutions which satisfy some constraint on their fitness. In the context of multi-objective optimization this constraint can require solutions to be Pareto-optimal. In this paper we study how the GSEMO algorithm with additional diversity-enhancing heuristic optimizes a diversity of its population on a bi-objective benchmark problem OneMinMax, for which all solutions are Pareto-optimal.  We provide a rigorous runtime analysis of the last step of the optimization, when the algorithm starts with a population with a second-best diversity, and prove that it finds a population with optimal diversity in expected time $O(n^2)$, when the problem size $n$ is odd. For reaching our goal, we analyse the random walk of the population, which reflects the frequency of changes in the population and their outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20056;&#27861;&#26356;&#26032;&#35268;&#21017;&#65292;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07189</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#21644;&#25552;&#39640;&#40065;&#26834;&#24615;&#30340;&#20056;&#27861;&#26356;&#26032;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Multiplicative update rules for accelerating deep learning training and increasing robustness. (arXiv:2307.07189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20056;&#27861;&#26356;&#26032;&#35268;&#21017;&#65292;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#24050;&#32463;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#21152;&#36895;&#35757;&#32451;&#21644;&#26500;&#24314;&#40065;&#26834;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#26367;&#20195;&#30340;&#21442;&#25968;&#26356;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#21152;&#24555;&#35757;&#32451;&#21644;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even nowadays, where Deep Learning (DL) has achieved state-of-the-art performance in a wide range of research domains, accelerating training and building robust DL models remains a challenging task. To this end, generations of researchers have pursued to develop robust methods for training DL architectures that can be less sensitive to weight distributions, model architectures and loss landscapes. However, such methods are limited to adaptive learning rate optimizers, initialization schemes, and clipping gradients without investigating the fundamental rule of parameters update. Although multiplicative updates have contributed significantly to the early development of machine learning and hold strong theoretical claims, to best of our knowledge, this is the first work that investigate them in context of DL training acceleration and robustness. In this work, we propose an optimization framework that fits to a wide range of optimization algorithms and enables one to apply alternative upda
&lt;/p&gt;</description></item><item><title>TriFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#36731;&#24230;&#35748;&#30693;&#25439;&#23475;&#36716;&#21270;&#20026;&#31283;&#23450;MCI&#25110;&#36827;&#23637;MCI&#30340;&#24773;&#20917;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#22270;&#20687;Transformer&#12289;&#20020;&#24202;Transformer&#21644;&#27169;&#24577;&#34701;&#21512;Transformer&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#20808;&#21069;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07177</link><description>&lt;p&gt;
TriFormer:&#19968;&#31181;&#29992;&#20110;&#36731;&#24230;&#35748;&#30693;&#25439;&#23475;&#36716;&#21270;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;Transformer&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TriFormer: A Multi-modal Transformer Framework For Mild Cognitive Impairment Conversion Prediction. (arXiv:2307.07177v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07177
&lt;/p&gt;
&lt;p&gt;
TriFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#36731;&#24230;&#35748;&#30693;&#25439;&#23475;&#36716;&#21270;&#20026;&#31283;&#23450;MCI&#25110;&#36827;&#23637;MCI&#30340;&#24773;&#20917;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#22270;&#20687;Transformer&#12289;&#20020;&#24202;Transformer&#21644;&#27169;&#24577;&#34701;&#21512;Transformer&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#20808;&#21069;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26089;&#26399;&#27835;&#30103;&#20197;&#38450;&#27490;&#25110;&#20943;&#32531;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#36827;&#23637;&#65292;&#36731;&#24230;&#35748;&#30693;&#25439;&#23475;&#65288;MCI&#65289;&#36716;&#21270;&#20026;AD&#30340;&#39044;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#20934;&#30830;&#39044;&#27979;MCI&#36716;&#21270;&#20026;&#31283;&#23450;MCI&#25110;&#36827;&#23637;MCI&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;TriFormer&#65292;&#23427;&#21253;&#25324;&#19977;&#20010;&#19987;&#38376;&#30340;Transformer&#26469;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;TriFormer&#20351;&#29992;I&#65289;&#22270;&#20687;Transformer&#20174;&#21307;&#23398;&#25195;&#25551;&#20013;&#25552;&#21462;&#22810;&#35270;&#22270;&#22270;&#20687;&#29305;&#24449;&#65292;II&#65289;&#20020;&#24202;Transformer&#23884;&#20837;&#21644;&#30456;&#20851;&#22810;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#65292; III&#65289;&#27169;&#24577;&#34701;&#21512;Transformer&#26681;&#25454;&#22270;&#20687;&#21644;&#20020;&#24202;Transformer&#30340;&#36755;&#20986;&#34701;&#21512;&#20135;&#29983;&#20934;&#30830;&#39044;&#27979;&#12290;TriFormer&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#35745;&#21010;&#65288;ANDI&#65289;1&#21644;ADNI2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of mild cognitive impairment (MCI) conversion to Alzheimer's disease (AD) is important for early treatment to prevent or slow the progression of AD. To accurately predict the MCI conversion to stable MCI or progressive MCI, we propose Triformer, a novel transformer-based framework with three specialized transformers to incorporate multi-model data. Triformer uses I) an image transformer to extract multi-view image features from medical scans, II) a clinical transformer to embed and correlate multi-modal clinical data, and III) a modality fusion transformer that produces an accurate prediction based on fusing the outputs from the image and clinical transformers. Triformer is evaluated on the Alzheimer's Disease Neuroimaging Initiative (ANDI)1 and ADNI2 datasets and outperforms previous state-of-the-art single and multi-modal methods.
&lt;/p&gt;</description></item><item><title>Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07176</link><description>&lt;p&gt;
Safe DreamerV3&#65306;&#24102;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3: Safe Reinforcement Learning with World Models. (arXiv:2307.07176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07176
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#36824;&#27809;&#26377;&#23454;&#29616;, &#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#26410;&#33021;&#28385;&#36275;&#36825;&#20123;&#31995;&#32479;&#30340;&#22522;&#26412;&#23433;&#20840;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#26469;&#22686;&#24378;&#23433;&#20840;&#24615;&#65292;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#65292;&#21253;&#25324;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#36827;&#34892;&#20840;&#38754;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#35757;&#32451;&#65292;&#20063;&#26080;&#27861;&#23454;&#29616;&#38646;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Safe DreamerV3&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#22312;SafeRL&#20013;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#65292;&#26159;&#31532;&#19968;&#20010;&#22312;Safety-Gymnasium&#22522;&#20934;&#20013;&#23454;&#29616;&#36817;&#20046;&#38646;&#25104;&#26412;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#31449;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://sites.google.com/view/safedreamerv3&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread application of Reinforcement Learning (RL) in real-world situations is yet to come to fruition, largely as a result of its failure to satisfy the essential safety demands of such systems. Existing safe reinforcement learning (SafeRL) methods, employing cost functions to enhance safety, fail to achieve zero-cost in complex scenarios, including vision-only tasks, even with comprehensive data sampling and training. To address this, we introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model. Our methodology represents a significant advancement in SafeRL as the first algorithm to achieve nearly zero-cost in both low-dimensional and vision-only tasks within the Safety-Gymnasium benchmark. Our project website can be found in: https://sites.google.com/view/safedreamerv3.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#21035;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#33258;&#28982;&#31034;&#20363;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#21644;&#23545;&#25239;&#25915;&#20987;&#23548;&#33268;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#26469;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07167</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#23454;&#20363;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Vulnerability-Aware Instance Reweighting For Adversarial Training. (arXiv:2307.07167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07167
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#21035;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#33258;&#28982;&#31034;&#20363;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#21644;&#23545;&#25239;&#25915;&#20987;&#23548;&#33268;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#26469;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#19968;&#30452;&#34987;&#21457;&#29616;&#33021;&#22823;&#22823;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#23545;&#25239;&#35757;&#32451;&#36890;&#36807;&#22312;&#35757;&#32451;&#20998;&#31867;&#22120;&#26102;&#21152;&#20837;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#33719;&#24471;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#21464;&#20307;&#23558;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#24179;&#31561;&#23545;&#24453;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#36890;&#36807;&#23558;&#23427;&#20204;&#19981;&#24179;&#31561;&#22320;&#23545;&#24453;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23545;&#25239;&#35757;&#32451;&#23545;&#35757;&#32451;&#38598;&#20013;&#30340;&#19981;&#21516;&#31867;&#21035;&#20135;&#29983;&#20102;&#19981;&#22343;&#34913;&#30340;&#24433;&#21709;&#65292;&#24182;&#19981;&#20844;&#24179;&#22320;&#25439;&#23475;&#20102;&#19982;&#26412;&#36136;&#19978;&#26356;&#38590;&#20998;&#31867;&#30340;&#31867;&#21035;&#30456;&#23545;&#24212;&#30340;&#31034;&#20363;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#20026;&#35757;&#32451;&#38598;&#20013;&#21508;&#20010;&#31034;&#20363;&#30340;&#40065;&#26834;&#25439;&#22833;&#20998;&#37197;&#19981;&#24179;&#31561;&#30340;&#26435;&#37325;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#21035;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#12290;&#23427;&#32771;&#34385;&#20102;&#27599;&#20010;&#33258;&#28982;&#31034;&#20363;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#21644;&#30001;&#20110;&#23545;&#25239;&#25915;&#20987;&#32780;&#23548;&#33268;&#30340;&#20854;&#23545;&#25163;&#31034;&#20363;&#19978;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training (AT) has been found to substantially improve the robustness of deep learning classifiers against adversarial attacks. AT involves obtaining robustness by including adversarial examples in training a classifier. Most variants of AT algorithms treat every training example equally. However, recent works have shown that better performance is achievable by treating them unequally. In addition, it has been observed that AT exerts an uneven influence on different classes in a training set and unfairly hurts examples corresponding to classes that are inherently harder to classify. Consequently, various reweighting schemes have been proposed that assign unequal weights to robust losses of individual examples in a training set. In this work, we propose a novel instance-wise reweighting scheme. It considers the vulnerability of each natural example and the resulting information loss on its adversarial counterpart occasioned by adversarial attacks. Through extensive experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#26469;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20197;&#25552;&#20379;&#22810;&#26679;&#21270;&#12289;&#20010;&#24615;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2307.07146</link><description>&lt;p&gt;
&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Federated Learning-Empowered AI-Generated Content in Wireless Networks. (arXiv:2307.07146v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#26469;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20197;&#25552;&#20379;&#22810;&#26679;&#21270;&#12289;&#20010;&#24615;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#20869;&#23481;&#21019;&#24314;&#36807;&#31243;&#30340;&#25928;&#29575;&#12289;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#37096;&#32626;AIGC&#26381;&#21153;&#34987;&#26399;&#26395;&#21487;&#20197;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AIGC&#26381;&#21153;&#22312;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#23454;&#29616;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#27169;&#22411;&#35757;&#32451;&#20998;&#24067;&#21040;&#21327;&#20316;&#25968;&#25454;&#25152;&#26377;&#32773;&#65292;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#65292;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#24182;&#23454;&#29616;&#23545;AIGC&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;FL&#30340;&#25216;&#26415;&#65292;&#20197;&#22686;&#24378;AIGC&#65292;&#24182;&#26088;&#22312;&#20351;&#29992;&#25143;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#20010;&#24615;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence generated content (AIGC) has emerged as a promising technology to improve the efficiency, quality, diversity and flexibility of the content creation process by adopting a variety of generative AI models. Deploying AIGC services in wireless networks has been expected to enhance the user experience. However, the existing AIGC service provision suffers from several limitations, e.g., the centralized training in the pre-training, fine-tuning and inference processes, especially their implementations in wireless networks with privacy preservation. Federated learning (FL), as a collaborative learning framework where the model training is distributed to cooperative data owners without the need for data sharing, can be leveraged to simultaneously improve learning efficiency and achieve privacy protection for AIGC. To this end, we present FL-based techniques for empowering AIGC, and aim to enable users to generate diverse, personalized, and high-quality content. Furthermo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20219;&#21153;&#26080;&#20851;&#30340;&#35780;&#20272;&#26694;&#26550;Camilla&#65292;&#36890;&#36807;&#23450;&#20041;&#22810;&#32500;&#24230;&#30340;&#35786;&#26029;&#24230;&#37327;Ability&#26469;&#21327;&#21516;&#27979;&#37327;&#27599;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#22810;&#38754;&#20307;&#24378;&#24230;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#23454;&#38469;&#24615;&#33021;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07134</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#22810;&#32500;&#33021;&#21147;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-Dimensional Ability Diagnosis for Machine Learning Algorithms. (arXiv:2307.07134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20219;&#21153;&#26080;&#20851;&#30340;&#35780;&#20272;&#26694;&#26550;Camilla&#65292;&#36890;&#36807;&#23450;&#20041;&#22810;&#32500;&#24230;&#30340;&#35786;&#26029;&#24230;&#37327;Ability&#26469;&#21327;&#21516;&#27979;&#37327;&#27599;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#22810;&#38754;&#20307;&#24378;&#24230;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#23454;&#38469;&#24615;&#33021;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#65288;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#32479;&#25351;&#26631;&#65288;&#20363;&#22914;&#27599;&#20010;&#20998;&#31867;&#22120;&#30340;&#31895;&#31890;&#24230;&#20934;&#30830;&#24230;&#65289;&#30340;&#27979;&#37327;&#19981;&#36275;&#65292;&#36890;&#24120;&#22312;&#36825;&#20123;&#31639;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#20013;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#20154;&#31867;&#27979;&#37327;&#20013;&#30340;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20219;&#21153;&#26080;&#20851;&#30340;&#35780;&#20272;&#26694;&#26550;Camilla&#65292;&#20854;&#20013;&#23450;&#20041;&#20102;&#19968;&#20010;&#22810;&#32500;&#35786;&#26029;&#24230;&#37327;Ability&#26469;&#21327;&#21516;&#27979;&#37327;&#27599;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#22810;&#38754;&#20307;&#24378;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#19981;&#21516;&#31639;&#27861;&#23545;&#25968;&#25454;&#26679;&#26412;&#30340;&#21709;&#24212;&#26085;&#24535;&#65292;&#25105;&#20204;&#21033;&#29992;&#35748;&#30693;&#35786;&#26029;&#20551;&#35774;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#31639;&#27861;&#12289;&#26679;&#26412;&#20197;&#21450;&#27599;&#20010;&#26679;&#26412;&#30340;&#25216;&#33021;&#65288;&#26174;&#24335;&#25110;&#38544;&#24335;&#39044;&#23450;&#20041;&#65289;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#21516;&#26102;&#35780;&#20272;&#27599;&#20010;&#31639;&#27861;&#22312;&#22810;&#20010;&#25216;&#33021;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have become ubiquitous in a number of applications (e.g. image classification). However, due to the insufficient measurement of traditional metrics (e.g. the coarse-grained Accuracy of each classifier), substantial gaps are usually observed between the real-world performance of these algorithms and their scores in standardized evaluations. In this paper, inspired by the psychometric theories from human measurement, we propose a task-agnostic evaluation framework Camilla, where a multi-dimensional diagnostic metric Ability is defined for collaboratively measuring the multifaceted strength of each machine learning algorithm. Specifically, given the response logs from different algorithms to data samples, we leverage cognitive diagnosis assumptions and neural networks to learn the complex interactions among algorithms, samples and the skills (explicitly or implicitly pre-defined) of each sample. In this way, both the abilities of each algorithm on multiple skil
&lt;/p&gt;</description></item><item><title>DataAssist&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#33410;&#30465;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07119</link><description>&lt;p&gt;
DataAssist:&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DataAssist: A Machine Learning Approach to Data Cleaning and Preparation. (arXiv:2307.07119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07119
&lt;/p&gt;
&lt;p&gt;
DataAssist&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#33410;&#30465;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20027;&#35201;&#20851;&#27880;&#20110;&#27169;&#22411;&#36873;&#25321;&#21644;&#21442;&#25968;&#20248;&#21270;&#65292;&#24573;&#30053;&#20102;&#25968;&#25454;&#28165;&#27927;&#21644;&#25972;&#29702;&#25152;&#21344;&#25454;&#30340;&#22823;&#37096;&#20998;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DataAssist&#30340;&#33258;&#21160;&#21270;&#25968;&#25454;&#20934;&#22791;&#21644;&#28165;&#27927;&#24179;&#21488;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DataAssist&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#21644;&#25968;&#25454;&#28165;&#27927;&#30340;&#31649;&#36947;&#65292;&#21253;&#25324;&#20026;&#29992;&#25143;&#36873;&#25321;&#30340;&#21464;&#37327;&#29983;&#25104;&#21487;&#35270;&#21270;&#65292;&#32479;&#19968;&#25968;&#25454;&#27880;&#37322;&#65292;&#25552;&#20379;&#24322;&#24120;&#20540;&#21024;&#38500;&#24314;&#35758;&#20197;&#21450;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#23548;&#20986;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36731;&#26494;&#19982;&#20854;&#20182;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#25110;&#29992;&#25143;&#25351;&#23450;&#30340;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#20197;&#36827;&#34892;&#21518;&#32493;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#21270;&#24037;&#20855;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32463;&#27982;&#23398;&#12289;&#21830;&#19994;&#21644;&#39044;&#27979;&#24212;&#29992;&#65292;&#21487;&#33410;&#30465;&#36229;&#36807;50\%&#30340;&#25968;&#25454;&#28165;&#29702;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current automated machine learning (ML) tools are model-centric, focusing on model selection and parameter optimization. However, the majority of the time in data analysis is devoted to data cleaning and wrangling, for which limited tools are available. Here we present DataAssist, an automated data preparation and cleaning platform that enhances dataset quality using ML-informed methods. We show that DataAssist provides a pipeline for exploratory data analysis and data cleaning, including generating visualization for user-selected variables, unifying data annotation, suggesting anomaly removal, and preprocessing data. The exported dataset can be readily integrated with other autoML tools or user-specified model for downstream analysis. Our data-centric tool is applicable to a variety of fields, including economics, business, and forecasting applications saving over 50\% time of the time spent on data cleansing and preparation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#20102;&#22235;&#20010;&#21253;&#21547;256&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#30001;&#24615;&#33021;&#19981;&#21516;&#30340;&#20195;&#29702;&#37319;&#38598;&#65292;&#21253;&#21547;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;</title><link>http://arxiv.org/abs/2307.07091</link><description>&lt;p&gt;
&#31163;&#32447;&#32452;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning. (arXiv:2307.07091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#20102;&#22235;&#20010;&#21253;&#21547;256&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#30001;&#24615;&#33021;&#19981;&#21516;&#30340;&#20195;&#29702;&#37319;&#38598;&#65292;&#21253;&#21547;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21487;&#20197;&#35753;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#26114;&#36149;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#30340;&#37325;&#22797;&#12290;&#20026;&#20102;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#23545;&#20110;&#29983;&#25104;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#23588;&#20026;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20801;&#35768;&#20174;&#23569;&#37327;&#32452;&#20214;&#20013;&#21019;&#24314;&#22810;&#20010;&#20219;&#21153;&#65292;2&#65289;&#20219;&#21153;&#32467;&#26500;&#21487;&#20197;&#35753;&#35757;&#32451;&#22909;&#30340;&#20195;&#29702;&#36890;&#36807;&#32452;&#21512;&#30456;&#20851;&#30340;&#23398;&#20064;&#32452;&#20214;&#26469;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#24182;&#19988;3&#65289;&#32452;&#21512;&#32500;&#24230;&#25552;&#20379;&#20102;&#20219;&#21153;&#20851;&#32852;&#24615;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#22235;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20351;&#29992;&#20102;&#26469;&#33258;CompoSuite [Mendez et al., 2022a]&#30340;256&#20010;&#20219;&#21153;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#24615;&#33021;&#31561;&#32423;&#30340;&#20195;&#29702;&#25910;&#38598;&#30340;&#65292;&#21253;&#21547;&#20102;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#23398;&#20064;&#32452;&#21512;&#20219;&#21153;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22522;&#20934;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is a promising direction that allows RL agents to pre-train on large datasets, avoiding the recurrence of expensive data collection. To advance the field, it is crucial to generate large-scale datasets. Compositional RL is particularly appealing for generating such large datasets, since 1) it permits creating many tasks from few components, 2) the task structure may enable trained agents to solve new tasks by combining relevant learned components, and 3) the compositional dimensions provide a notion of task relatedness. This paper provides four offline RL datasets for simulated robotic manipulation created using the 256 tasks from CompoSuite [Mendez et al., 2022a]. Each dataset is collected from an agent with a different degree of performance, and consists of 256 million transitions. We provide training and evaluation settings for assessing an agent's ability to learn compositional task policies. Our benchmarking experiments on each setting show that
&lt;/p&gt;</description></item><item><title>Espaloma-0.3.0&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#65292;&#36890;&#36807;&#33021;&#37327;&#21644;&#21147;&#30340;&#25311;&#21512;&#32435;&#20837;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07085</link><description>&lt;p&gt;
Espaloma-0.3.0: &#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#21450;&#20854;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Espaloma-0.3.0: Machine-learned molecular mechanics force field for the simulation of protein-ligand systems and beyond. (arXiv:2307.07085v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07085
&lt;/p&gt;
&lt;p&gt;
Espaloma-0.3.0&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#65292;&#36890;&#36807;&#33021;&#37327;&#21644;&#21147;&#30340;&#25311;&#21512;&#32435;&#20837;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21147;&#23398;&#65288;MM&#65289;&#21147;&#22330;&#26159;&#36890;&#36807;&#31616;&#21333;&#30340;&#19968;&#23545;&#19968;&#21644;&#22810;&#39033;&#24335;&#39033;&#26469;&#34920;&#24449;&#20998;&#23376;&#31995;&#32479;&#33021;&#37327;&#26223;&#35266;&#30340;&#27169;&#22411;&#12290;&#20256;&#32479;&#19978;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#20154;&#24037;&#19987;&#23478;&#31574;&#21010;&#12289;&#19981;&#28789;&#27963;&#19988;&#38590;&#20197;&#25193;&#23637;&#30340;&#31163;&#25955;&#21270;&#21270;&#23398;&#21442;&#25968;&#36171;&#20540;&#35268;&#21017;&#65292;&#21363;&#21407;&#23376;&#25110;&#20215;&#24577;&#31867;&#22411;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27492;&#36807;&#31243;&#65292;&#24182;&#20351;&#21442;&#25968;&#21270;&#26041;&#26696;&#33021;&#22815;&#30452;&#25509;&#20174;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#25110;&#20957;&#32858;&#30456;&#25968;&#25454;&#20013;&#20197;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#20998;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#34920;&#31034;&#20986;&#20102;&#24040;&#22823;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#33021;&#37327;&#21644;&#21147;&#30340;&#36866;&#24212;&#24615;&#25311;&#21512;&#30452;&#25509;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#65292;&#25193;&#23637;&#20102;Espaloma&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#21147;&#22330;&#26500;&#24314;&#26041;&#27861;&#12290;&#22522;&#20110;OpenMM SPICE&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#19982;&#29983;&#29289;&#20998;&#23376;&#24314;&#27169;&#24191;&#27867;&#30456;&#20851;&#30340;&#21270;&#23398;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#23567;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;RNA&#12290;&#26368;&#32456;&#24471;&#21040;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular mechanics (MM) force fields -- the models that characterize the energy landscape of molecular systems via simple pairwise and polynomial terms -- have traditionally relied on human expert-curated, inflexible, and poorly extensible discrete chemical parameter assignment rules, namely atom or valence types. Recently, there has been significant interest in using graph neural networks to replace this process, while enabling the parametrization scheme to be learned in an end-to-end differentiable manner directly from quantum chemical calculations or condensed-phase data. In this paper, we extend the Espaloma end-to-end differentiable force field construction approach by incorporating both energy and force fitting directly to quantum chemical data into the training process. Building on the OpenMM SPICE dataset, we curate a dataset containing chemical spaces highly relevant to the broad interest of biomolecular modeling, covering small molecules, proteins, and RNA. The resulting for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.07084</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;Wasserstein&#21464;&#20998;&#25512;&#29702;&#65306;&#21487;&#35299;&#37322;&#24615;&#30340;&#24418;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25110;&#26368;&#20248;&#25511;&#21046;&#21487;&#20197;&#20026;&#20855;&#26377;&#21487;&#21464;&#21160;&#24577;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#23454;&#26045;&#20013;&#65292;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#23558;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#22240;&#20026;&#27010;&#29575;&#25512;&#29702;&#21407;&#21017;&#19978;&#25552;&#20379;&#20102;&#22810;&#26679;&#19988;&#24378;&#22823;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#25512;&#26029;&#38543;&#26426;&#21160;&#24577;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35299;&#37322;&#22870;&#21169;&#35774;&#35745;&#65292;&#36879;&#26126;&#22320;&#35757;&#32451;&#25910;&#25947;&#65292;&#20197;&#21450;&#23545;&#39034;&#24207;&#20915;&#31574;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#20026;&#20102;&#35777;&#26126;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25910;&#25947;&#35757;&#32451;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39030;&#28857;&#30340;&#32593;&#32476;&#21152;&#36895;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#19987;&#27880;&#26368;&#20248;&#36335;&#24452;&#19978;&#30340;&#20851;&#38190;&#39030;&#28857;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#29983;&#25104;&#30340;&#22320;&#22270;&#19978;&#65292;&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#20102;&#36229;&#36807;400%&#30340;&#36895;&#24230;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.07059</link><description>&lt;p&gt;
&#22522;&#20110;&#39030;&#28857;&#30340;&#32593;&#32476;&#21152;&#36895;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vertex-based Networks to Accelerate Path Planning Algorithms. (arXiv:2307.07059v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39030;&#28857;&#30340;&#32593;&#32476;&#21152;&#36895;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#19987;&#27880;&#26368;&#20248;&#36335;&#24452;&#19978;&#30340;&#20851;&#38190;&#39030;&#28857;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#29983;&#25104;&#30340;&#22320;&#22270;&#19978;&#65292;&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#20102;&#36229;&#36807;400%&#30340;&#36895;&#24230;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#24452;&#35268;&#21010;&#22312;&#33258;&#20027;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;RRT*&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#39030;&#28857;&#30340;&#32593;&#32476;&#26469;&#22686;&#24378;RRT*&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#26368;&#20248;&#36335;&#24452;&#19978;&#30340;&#20851;&#38190;&#39030;&#28857;&#65292;&#25552;&#20379;&#20102;&#20851;&#38190;&#20294;&#26356;&#31232;&#30095;&#30340;&#36335;&#24452;&#25277;&#35937;&#12290;&#25105;&#20204;&#37319;&#29992;&#32858;&#28966;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#30456;&#20851;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#30340;&#25513;&#30721;&#37197;&#32622;&#20197;&#30830;&#23450;&#31995;&#32479;&#24615;&#33021;&#30340;&#23454;&#38469;&#26435;&#34913;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#29983;&#25104;&#30340;&#22320;&#22270;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;400%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path planning plays a crucial role in various autonomy applications, and RRT* is one of the leading solutions in this field. In this paper, we propose the utilization of vertex-based networks to enhance the sampling process of RRT*, leading to more efficient path planning. Our approach focuses on critical vertices along the optimal paths, which provide essential yet sparser abstractions of the paths. We employ focal loss to address the associated data imbalance issue, and explore different masking configurations to determine practical tradeoffs in system performance. Through experiments conducted on randomly generated floor maps, our solutions demonstrate significant speed improvements, achieving over a 400% enhancement compared to the baseline model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20869;&#31397;&#38236;&#32958;&#32467;&#30707;&#35782;&#21035;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#26679;&#26412;&#31232;&#32570;&#31867;&#21035;&#65292;&#24182;&#19988;&#22312;&#25512;&#24191;&#21040;&#26032;&#26679;&#26412;&#20197;&#21450;&#28155;&#21152;&#26032;&#31867;&#21035;&#21040;&#25968;&#25454;&#24211;&#26102;&#33021;&#26377;&#26356;&#22909;&#30340;&#24212;&#23545;&#33021;&#21147;</title><link>http://arxiv.org/abs/2307.07046</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20869;&#31397;&#38236;&#32958;&#32467;&#30707;&#35782;&#21035;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A metric learning approach for endoscopic kidney stone identification. (arXiv:2307.07046v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20869;&#31397;&#38236;&#32958;&#32467;&#30707;&#35782;&#21035;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#26679;&#26412;&#31232;&#32570;&#31867;&#21035;&#65292;&#24182;&#19988;&#22312;&#25512;&#24191;&#21040;&#26032;&#26679;&#26412;&#20197;&#21450;&#28155;&#21152;&#26032;&#31867;&#21035;&#21040;&#25968;&#25454;&#24211;&#26102;&#33021;&#26377;&#26356;&#22909;&#30340;&#24212;&#23545;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#29992;&#20110;&#22312;&#36755;&#23615;&#31649;&#38236;&#26816;&#26597;&#26399;&#38388;&#33258;&#21160;&#35782;&#21035;&#32958;&#32467;&#30707;&#65292;&#20197;&#20415;&#24555;&#36895;&#36827;&#34892;&#27835;&#30103;&#20915;&#31574;&#12290;&#23613;&#31649;&#36825;&#20123;DL&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#36866;&#29992;&#20110;&#26377;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#32958;&#32467;&#30707;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20123;&#32597;&#35265;&#30340;&#32958;&#32467;&#30707;&#31867;&#22411;&#65292;&#21482;&#26377;&#24456;&#23569;&#30340;&#26631;&#35760;&#22270;&#20687;&#21487;&#29992;&#12290;&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;DML&#65289;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20123;&#26679;&#26412;&#31232;&#32570;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#33021;&#22815;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20998;&#24067;&#26679;&#26412;&#20043;&#22806;&#65292;&#24182;&#19988;&#23545;&#20110;&#28155;&#21152;&#21040;&#25968;&#25454;&#24211;&#30340;&#26032;&#31867;&#21035;&#33021;&#22815;&#26377;&#26356;&#22909;&#30340;&#24212;&#23545;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#24341;&#23548;&#24335;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26356;&#22909;&#22320;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#30340;&#26041;&#24335;&#12290;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#30340;&#21551;&#21457;&#65292;&#24182;&#37319;&#29992;&#20102;&#24072;&#29983;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#24072;&#27169;&#22411;&#65288;GEMINI&#65289;&#22522;&#20110;&#24050;&#30693;&#26631;&#31614;&#30340;&#20808;&#39564;&#30693;&#35782;&#29983;&#25104;&#20102;&#19968;&#20010;&#32553;&#23567;&#30340;&#20551;&#35774;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Several Deep Learning (DL) methods have recently been proposed for an automated identification of kidney stones during an ureteroscopy to enable rapid therapeutic decisions. Even if these DL approaches led to promising results, they are mainly appropriate for kidney stone types for which numerous labelled data are available. However, only few labelled images are available for some rare kidney stone types. This contribution exploits Deep Metric Learning (DML) methods i) to handle such classes with few samples, ii) to generalize well to out of distribution samples, and iii) to cope better with new classes which are added to the database. The proposed Guided Deep Metric Learning approach is based on a novel architecture which was designed to learn data representations in an improved way. The solution was inspired by Few-Shot Learning (FSL) and makes use of a teacher-student approach. The teacher model (GEMINI) generates a reduced hypothesis space based on prior knowledge from the labeled 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#35299;&#30340;&#21160;&#20316;&#31354;&#38388;&#26469;&#20943;&#36731;&#28041;&#21450;&#22823;&#22411;&#32452;&#21512;&#21160;&#20316;&#31354;&#38388;&#38382;&#39064;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#21160;&#20316;&#31354;&#38388;&#30340;&#26032;&#22411;&#8220;&#20998;&#35299;&#8221;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#20272;&#35745;&#22120;&#31995;&#21015;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#35299;IS&#20272;&#35745;&#22120;&#20855;&#26377;&#27604;&#38750;&#20998;&#35299;&#29256;&#26412;&#26356;&#23567;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#20855;&#26377;&#38646;&#20559;&#24046;&#30340;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07014</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#35299;&#30340;&#21160;&#20316;&#31354;&#38388;&#36827;&#34892;&#38750;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Leveraging Factored Action Spaces for Off-Policy Evaluation. (arXiv:2307.07014v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#35299;&#30340;&#21160;&#20316;&#31354;&#38388;&#26469;&#20943;&#36731;&#28041;&#21450;&#22823;&#22411;&#32452;&#21512;&#21160;&#20316;&#31354;&#38388;&#38382;&#39064;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#21160;&#20316;&#31354;&#38388;&#30340;&#26032;&#22411;&#8220;&#20998;&#35299;&#8221;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#20272;&#35745;&#22120;&#31995;&#21015;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#35299;IS&#20272;&#35745;&#22120;&#20855;&#26377;&#27604;&#38750;&#20998;&#35299;&#29256;&#26412;&#26356;&#23567;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#20855;&#26377;&#38646;&#20559;&#24046;&#30340;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#26088;&#22312;&#20272;&#35745;&#26681;&#25454;&#25191;&#34892;&#24207;&#21015;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#36981;&#24490;&#21453;&#20107;&#23454;&#30340;&#19968;&#31995;&#21015;&#21160;&#20316;&#30340;&#25928;&#30410;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;OPE&#20272;&#35745;&#22120;&#22312;&#28041;&#21450;&#22823;&#22411;&#32452;&#21512;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#20013;&#32463;&#24120;&#34920;&#29616;&#20986;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#20998;&#35299;&#30340;&#21160;&#20316;&#31354;&#38388;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#23558;&#27599;&#20010;&#21160;&#20316;&#34920;&#31034;&#20026;&#26469;&#33258;&#36739;&#23567;&#21160;&#20316;&#31354;&#38388;&#30340;&#29420;&#31435;&#23376;&#21160;&#20316;&#30340;&#32452;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#23545;&#21160;&#20316;&#22312;&#20854;&#25928;&#26524;&#19978;&#30340;&#24046;&#24322;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#21160;&#20316;&#31354;&#38388;&#30340;&#26032;&#22411;&#8220;&#20998;&#35299;&#8221;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#20272;&#35745;&#22120;&#31995;&#21015;&#12290;&#22312;&#23545;&#24213;&#23618;&#38382;&#39064;&#32467;&#26500;&#36827;&#34892;&#19968;&#23450;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#35299;IS&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#27604;&#20854;&#21407;&#22987;&#38750;&#20998;&#35299;&#29256;&#26412;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#38646;&#20559;&#24046;&#30340;&#24615;&#36136;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#26816;&#39564;&#20102;&#21508;&#31181;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy evaluation (OPE) aims to estimate the benefit of following a counterfactual sequence of actions, given data collected from executed sequences. However, existing OPE estimators often exhibit high bias and high variance in problems involving large, combinatorial action spaces. We investigate how to mitigate this issue using factored action spaces i.e. expressing each action as a combination of independent sub-actions from smaller action spaces. This approach facilitates a finer-grained analysis of how actions differ in their effects. In this work, we propose a new family of "decomposed" importance sampling (IS) estimators based on factored action spaces. Given certain assumptions on the underlying problem structure, we prove that the decomposed IS estimators have less variance than their original non-decomposed versions, while preserving the property of zero bias. Through simulations, we empirically verify our theoretical results, probing the validity of various assumptions. P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20843;&#31181;&#26131;&#20110;&#38598;&#25104;&#21040;&#29616;&#26377;NLP&#31995;&#32479;&#20013;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#24102;&#22806;&#25968;&#25454;&#25110;&#27169;&#22411;&#20462;&#25913;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#23454;&#39564;&#32467;&#26524;&#30340;&#30740;&#31350;&#29615;&#22659;&#12290;&#20998;&#26512;&#34920;&#26126;&#29616;&#26377;&#30340;NLP&#20219;&#21153;&#20013;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#23545;&#20110;&#25429;&#25417;&#25152;&#26377;&#30001;&#19981;&#21516;&#31867;&#22411;&#20998;&#24067;&#36716;&#25442;&#29305;&#24449;&#30340;&#26679;&#26412;&#23578;&#19981;&#22815;&#25935;&#24863;&#65292;&#36825;&#38656;&#35201;&#26410;&#26469;&#30340;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07002</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#32463;&#20856;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks. (arXiv:2307.07002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20843;&#31181;&#26131;&#20110;&#38598;&#25104;&#21040;&#29616;&#26377;NLP&#31995;&#32479;&#20013;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#24102;&#22806;&#25968;&#25454;&#25110;&#27169;&#22411;&#20462;&#25913;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#23454;&#39564;&#32467;&#26524;&#30340;&#30740;&#31350;&#29615;&#22659;&#12290;&#20998;&#26512;&#34920;&#26126;&#29616;&#26377;&#30340;NLP&#20219;&#21153;&#20013;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#23545;&#20110;&#25429;&#25417;&#25152;&#26377;&#30001;&#19981;&#21516;&#31867;&#22411;&#20998;&#24067;&#36716;&#25442;&#29305;&#24449;&#30340;&#26679;&#26412;&#23578;&#19981;&#22815;&#25935;&#24863;&#65292;&#36825;&#38656;&#35201;&#26410;&#26469;&#30340;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#21463;&#25511;&#29615;&#22659;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38754;&#23545;&#24102;&#22806;&#20998;&#24067;&#30340;&#20363;&#23376;&#26102;&#24448;&#24448;&#34920;&#29616;&#20986;&#22256;&#38590;&#65292;&#22240;&#27492;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#25104;&#20026;NLP&#31995;&#32479;&#20013;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#30528;&#37325;&#24378;&#35843;&#20102;&#29616;&#26377;NLP&#39046;&#22495;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20843;&#31181;&#26131;&#20110;&#38598;&#25104;&#21040;&#29616;&#26377;NLP&#31995;&#32479;&#20013;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#24102;&#22806;&#25968;&#25454;&#25110;&#27169;&#22411;&#20462;&#25913;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20043;&#19968;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#33391;&#22909;&#30340;&#30740;&#31350;&#29615;&#22659;&#65292;&#21487;&#20197;&#23436;&#20840;&#22797;&#29616;&#23454;&#39564;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#29616;&#26377;&#30340;NLP&#20219;&#21153;&#20013;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#23545;&#20110;&#25429;&#25417;&#25152;&#26377;&#30001;&#19981;&#21516;&#31867;&#22411;&#20998;&#24067;&#36716;&#25442;&#29305;&#24449;&#30340;&#26679;&#26412;&#23578;&#19981;&#22815;&#25935;&#24863;&#12290;&#22312;&#39046;&#22495;&#32972;&#26223;&#36716;&#21464;&#21644;&#21333;&#35789;&#38543;&#26426;&#25490;&#21015;&#30340;&#24773;&#20917;&#19979;&#65292;&#27979;&#35797;&#24773;&#26223;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31361;&#26174;&#20102;&#26410;&#26469;&#38656;&#35201;&#24320;&#23637;&#26356;&#26377;&#25928;&#30340;&#24037;&#20316;&#26469;&#21457;&#23637;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art models can perform well in controlled environments, but they often struggle when presented with out-of-distribution (OOD) examples, making OOD detection a critical component of NLP systems. In this paper, we focus on highlighting the limitations of existing approaches to OOD detection in NLP. Specifically, we evaluated eight OOD detection methods that are easily integrable into existing NLP systems and require no additional OOD data or model modifications. One of our contributions is providing a well-structured research environment that allows for full reproducibility of the results. Additionally, our analysis shows that existing OOD detection methods for NLP tasks are not yet sufficiently sensitive to capture all samples characterized by various types of distributional shifts. Particularly challenging testing scenarios arise in cases of background shift and randomly shuffled word order within in domain texts. This highlights the need for future work to develop more ef
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29305;&#23450;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;IR&#35774;&#35745;&#65292;&#21487;&#23558;&#20132;&#36890;&#25968;&#25454;&#32479;&#19968;&#22788;&#29702;&#25104;&#22270;&#24418;&#25968;&#25454;&#26684;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25968;&#25454;&#22788;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06983</link><description>&lt;p&gt;
&#38754;&#21521;&#29305;&#23450;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;IR&#35774;&#35745;:&#20197;&#20132;&#36890;&#25968;&#25454;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
IR Design for Application-Specific Natural Language: A Case Study on Traffic Data. (arXiv:2307.06983v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29305;&#23450;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;IR&#35774;&#35745;&#65292;&#21487;&#23558;&#20132;&#36890;&#25968;&#25454;&#32479;&#19968;&#22788;&#29702;&#25104;&#22270;&#24418;&#25968;&#25454;&#26684;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25968;&#25454;&#22788;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#36890;&#34892;&#19994;&#30340;&#36719;&#20214;&#24212;&#29992;&#39046;&#22495;&#65292;&#30001;&#20110;&#26131;&#20110;&#20351;&#29992;&#21644;&#20854;&#20182;&#21508;&#31181;&#22909;&#22788;&#65292;&#39046;&#22495;&#19987;&#29992;&#35821;&#35328;(DSLs)&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#24615;&#33021;&#30340;&#19981;&#26029;&#25552;&#21319;&#21644;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#32534;&#31243;&#30340;&#21487;&#33021;&#24615;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#24050;&#32463;&#20986;&#29616;&#65292;&#34987;&#31216;&#20026;&#24212;&#29992;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;(ASNL)&#12290;ASNL&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#33258;&#30001;&#24230;&#65292;&#20174;&#32780;&#23548;&#33268;&#35299;&#26512;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#65292;&#22788;&#29702;&#24615;&#33021;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;ASNL&#30340;&#20013;&#38388;&#34920;&#31034;(IR)&#30340;&#35774;&#35745;&#65292;&#21487;&#20197;&#23558;&#20132;&#36890;&#25968;&#25454;&#32479;&#19968;&#22788;&#29702;&#25104;&#22270;&#24418;&#25968;&#25454;&#26684;&#24335;&#65292;&#25552;&#39640;&#25968;&#25454;&#22788;&#29702;&#24615;&#33021;&#12290;&#23454;&#39564;&#27604;&#36739;&#34920;&#26126;&#65292;&#22312;&#26631;&#20934;&#25968;&#25454;&#26597;&#35810;&#25805;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;IR&#35774;&#35745;&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;&#22235;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#19982;&#30452;&#25509;&#20351;&#29992;ASNL&#36827;&#34892;&#22788;&#29702;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of software applications in the transportation industry, Domain-Specific Languages (DSLs) have enjoyed widespread adoption due to their ease of use and various other benefits. With the ceaseless progress in computer performance and the rapid development of large-scale models, the possibility of programming using natural language in specified applications - referred to as Application-Specific Natural Language (ASNL) - has emerged. ASNL exhibits greater flexibility and freedom, which, in turn, leads to an increase in computational complexity for parsing and a decrease in processing performance. To tackle this issue, our paper advances a design for an intermediate representation (IR) that caters to ASNL and can uniformly process transportation data into graph data format, improving data processing performance. Experimental comparisons reveal that in standard data query operations, our proposed IR design can achieve a speed improvement of over forty times compared to direct us
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#20803;&#20851;&#31995;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#30701;&#24067;&#23572;&#20844;&#24335;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#23545;&#26399;&#26395;&#38169;&#35823;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19977;&#20010;&#20855;&#20307;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#38480;&#21046;&#20844;&#24335;&#38271;&#24230;&#65292;&#21487;&#20197;&#33719;&#24471;&#36991;&#20813;&#36807;&#25311;&#21512;&#19988;&#20934;&#30830;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.06971</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;&#30701;&#24067;&#23572;&#20844;&#24335;&#20316;&#20026;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Short Boolean Formulas as Explanations in Practice. (arXiv:2307.06971v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#20803;&#20851;&#31995;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#30701;&#24067;&#23572;&#20844;&#24335;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#23545;&#26399;&#26395;&#38169;&#35823;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19977;&#20010;&#20855;&#20307;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#38480;&#21046;&#20844;&#24335;&#38271;&#24230;&#65292;&#21487;&#20197;&#33719;&#24471;&#36991;&#20813;&#36807;&#25311;&#21512;&#19988;&#20934;&#30830;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#20803;&#20851;&#31995;&#30340;&#25968;&#25454;&#27169;&#22411;&#20013;&#36890;&#36807;&#30701;&#24067;&#23572;&#20844;&#24335;&#36827;&#34892;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;&#12290;&#20316;&#20026;&#38271;&#24230;&#20026;k&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#38271;&#24230;&#20026;k&#30340;&#24067;&#23572;&#20844;&#24335;&#65292;&#35813;&#20844;&#24335;&#22312;&#35299;&#37322;&#30446;&#26631;&#23646;&#24615;&#26041;&#38754;&#30340;&#38169;&#35823;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#36825;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26399;&#26395;&#38169;&#35823;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19977;&#20010;&#20855;&#20307;&#30340;&#25968;&#25454;&#38598;&#26469;&#28436;&#31034;&#35813;&#35774;&#32622;&#22312;&#23454;&#36341;&#20013;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;Answer Set Programming&#20013;&#30340;&#32534;&#30721;&#35745;&#31639;&#19981;&#21516;&#38271;&#24230;&#30340;&#35299;&#37322;&#20844;&#24335;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#26368;&#20934;&#30830;&#30340;&#20844;&#24335;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#31867;&#20284;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#30340;&#21407;&#22240;&#65292;&#36825;&#20123;&#20844;&#24335;&#19981;&#19968;&#23450;&#26159;&#29702;&#24819;&#30340;&#35299;&#37322;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#35299;&#37322;&#38271;&#24230;&#12290;&#36890;&#36807;&#38480;&#21046;&#20026;&#26356;&#30701;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;&#35299;&#37322;&#19981;&#20165;&#36991;&#20813;&#20102;&#36807;&#25311;&#21512;&#65292;&#32780;&#19988;&#20381;&#28982;&#30456;&#24403;&#20934;&#30830;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#65292;&#26131;&#20110;&#20154;&#31867;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate explainability via short Boolean formulas in the data model based on unary relations. As an explanation of length k, we take a Boolean formula of length k that minimizes the error with respect to the target attribute to be explained. We first provide novel quantitative bounds for the expected error in this scenario. We then also demonstrate how the setting works in practice by studying three concrete data sets. In each case, we calculate explanation formulas of different lengths using an encoding in Answer Set Programming. The most accurate formulas we obtain achieve errors similar to other methods on the same data sets. However, due to overfitting, these formulas are not necessarily ideal explanations, so we use cross validation to identify a suitable length for explanations. By limiting to shorter formulas, we obtain explanations that avoid overfitting but are still reasonably accurate and also, importantly, human interpretable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20272;&#35745;&#20102;&#37319;&#29992;FDM&#24037;&#33402;&#21046;&#36896;&#30340;PLA&#35797;&#26679;&#30340;&#26497;&#38480;&#25239;&#25289;&#24378;&#24230;&#65288;UTS&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;KNN&#31639;&#27861;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#22312;&#21306;&#20998;&#19981;&#21516;UTS&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06970</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#27169;&#24335;&#35782;&#21035;&#31639;&#27861;&#29992;&#20110;&#20272;&#35745;&#29076;&#34701;&#27785;&#31215;&#27169;&#24335;&#32858;&#20083;&#37240;&#37240;&#65288;PLA&#65289;&#35797;&#26679;&#30340;&#26497;&#38480;&#25239;&#25289;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Assisted Pattern Recognition Algorithms for Estimating Ultimate Tensile Strength in Fused Deposition Modeled Polylactic Acid Specimens. (arXiv:2307.06970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20272;&#35745;&#20102;&#37319;&#29992;FDM&#24037;&#33402;&#21046;&#36896;&#30340;PLA&#35797;&#26679;&#30340;&#26497;&#38480;&#25239;&#25289;&#24378;&#24230;&#65288;UTS&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;KNN&#31639;&#27861;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#22312;&#21306;&#20998;&#19981;&#21516;UTS&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#20272;&#35745;&#37319;&#29992;&#29076;&#34701;&#27785;&#31215;&#27169;&#22411;&#65288;FDM&#65289;&#24037;&#33402;&#21046;&#36896;&#30340;&#32858;&#20083;&#37240;&#37240;&#65288;PLA&#65289;&#35797;&#26679;&#30340;&#26497;&#38480;&#25239;&#25289;&#24378;&#24230;&#65288;UTS&#65289;&#12290;&#24635;&#20849;&#21046;&#22791;&#20102;31&#20010;PLA&#35797;&#26679;&#65292;&#20854;&#20013;&#22635;&#20805;&#30334;&#20998;&#27604;&#12289;&#23618;&#39640;&#12289;&#25171;&#21360;&#36895;&#24230;&#21644;&#25380;&#20986;&#28201;&#24230;&#20316;&#20026;&#36755;&#20837;&#21442;&#25968;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#22235;&#31181;&#19981;&#21516;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#12289;&#26799;&#24230;&#25552;&#21319;&#20998;&#31867;&#12289;&#20915;&#31574;&#26641;&#21644;K&#36817;&#37051;&#65292;&#22312;&#39044;&#27979;&#35797;&#26679;UTS&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20915;&#31574;&#26641;&#21644;K&#36817;&#37051;&#31639;&#27861;&#30340;F1&#20998;&#25968;&#22343;&#20026;0.71&#65292;&#20294;KNN&#31639;&#27861;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20998;&#25968;&#26356;&#39640;&#65292;&#36798;&#21040;0.79&#65292;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;&#36825;&#34920;&#26126;KNN&#31639;&#27861;&#22312;&#21306;&#20998;&#20004;&#31867;&#26497;&#38480;&#25239;&#25289;&#24378;&#24230;&#26041;&#38754;&#20855;&#26377;&#36739;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate the application of supervised machine learning algorithms for estimating the Ultimate Tensile Strength (UTS) of Polylactic Acid (PLA) specimens fabricated using the Fused Deposition Modeling (FDM) process. A total of 31 PLA specimens were prepared, with Infill Percentage, Layer Height, Print Speed, and Extrusion Temperature serving as input parameters. The primary objective was to assess the accuracy and effectiveness of four distinct supervised classification algorithms, namely Logistic Classification, Gradient Boosting Classification, Decision Tree, and K-Nearest Neighbor, in predicting the UTS of the specimens. The results revealed that while the Decision Tree and K-Nearest Neighbor algorithms both achieved an F1 score of 0.71, the KNN algorithm exhibited a higher Area Under the Curve (AUC) score of 0.79, outperforming the other algorithms. This demonstrates the superior ability of the KNN algorithm in differentiating between the two classes of ultimate
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#32479;&#19968;&#24403;&#20195;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;&#34429;&#28982;XAI&#26041;&#27861;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36755;&#20986;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#24212;&#27880;&#24847;&#23427;&#20204;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#22312;&#35813;&#39046;&#22495;&#26377;&#19968;&#20010;&#27010;&#24565;&#31361;&#30772;&#20197;&#35299;&#20915;XAI&#26041;&#27861;&#21644;&#24212;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.06963</link><description>&lt;p&gt;
&#20219;&#21153;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26159;&#20010;&#31070;&#35805;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Task-Agnostic Explainable AI a Myth?. (arXiv:2307.06963v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06963
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#32479;&#19968;&#24403;&#20195;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;&#34429;&#28982;XAI&#26041;&#27861;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36755;&#20986;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#24212;&#27880;&#24847;&#23427;&#20204;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#22312;&#35813;&#39046;&#22495;&#26377;&#19968;&#20010;&#27010;&#24565;&#31361;&#30772;&#20197;&#35299;&#20915;XAI&#26041;&#27861;&#21644;&#24212;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;&#24403;&#20195;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#34429;&#28982;XAI&#26041;&#27861;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#34917;&#20805;&#21644;&#28508;&#22312;&#26377;&#29992;&#30340;&#36755;&#20986;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#24212;&#35813;&#27880;&#24847;&#23427;&#20204;&#30340;&#27010;&#24565;&#21644;&#25216;&#26415;&#38480;&#21046;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#36825;&#20123;&#26041;&#27861;&#26412;&#36523;&#25104;&#20026;&#40657;&#30418;&#23376;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;XAI&#30740;&#31350;&#26041;&#21521;&#65292;&#28085;&#30422;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#22270;&#24418;&#25968;&#25454;&#65292;&#21253;&#25324;&#26174;&#33879;&#24615;&#12289;&#27880;&#24847;&#21147;&#21644;&#22270;&#24418;&#35299;&#37322;&#22120;&#12290;&#23613;&#31649;&#25152;&#25552;&#21450;&#30340;&#26696;&#20363;&#30340;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#36328;&#24230;&#21508;&#19981;&#30456;&#21516;&#65292;&#20294;&#21516;&#26679;&#30340;&#25345;&#32493;&#38459;&#30861;&#20986;&#29616;&#20102;&#65292;&#31361;&#26174;&#20986;&#22312;&#35813;&#39046;&#22495;&#20013;&#35299;&#20915;XAI&#26041;&#27861;&#21644;&#24212;&#29992;&#20219;&#21153;&#20043;&#38388;&#20860;&#23481;&#24615;&#25361;&#25112;&#30340;&#27010;&#24565;&#31361;&#30772;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work serves as a framework for unifying the challenges of contemporary explainable AI (XAI). We demonstrate that while XAI methods provide supplementary and potentially useful output for machine learning models, researchers and decision-makers should be mindful of their conceptual and technical limitations, which frequently result in these methods themselves becoming black boxes. We examine three XAI research avenues spanning image, textual, and graph data, covering saliency, attention, and graph-type explainers. Despite the varying contexts and timeframes of the mentioned cases, the same persistent roadblocks emerge, highlighting the need for a conceptual breakthrough in the field to address the challenge of compatibility between XAI methods and application tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#25991;&#26412;&#29983;&#25104;&#23450;&#20041;&#20026;&#20174;&#29616;&#26377;&#25991;&#26412;&#38598;&#21512;&#20013;&#36880;&#27493;&#22797;&#21046;&#25991;&#26412;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#22797;&#21046;&#21644;&#31896;&#36148;&#25805;&#20316;&#26469;&#23454;&#29616;&#29983;&#25104;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#39034;&#24207;&#36873;&#25321;&#21333;&#35789;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#25512;&#29702;&#25928;&#29575;&#19982;&#22522;&#20110;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2307.06962</link><description>&lt;p&gt;
&#22797;&#21046;&#23601;&#26159;&#20320;&#25152;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copy Is All You Need. (arXiv:2307.06962v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25991;&#26412;&#29983;&#25104;&#23450;&#20041;&#20026;&#20174;&#29616;&#26377;&#25991;&#26412;&#38598;&#21512;&#20013;&#36880;&#27493;&#22797;&#21046;&#25991;&#26412;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#22797;&#21046;&#21644;&#31896;&#36148;&#25805;&#20316;&#26469;&#23454;&#29616;&#29983;&#25104;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#39034;&#24207;&#36873;&#25321;&#21333;&#35789;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#25512;&#29702;&#25928;&#29575;&#19982;&#22522;&#20110;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#23548;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#39034;&#24207;&#36873;&#25321;&#26469;&#33258;&#22266;&#23450;&#35789;&#27719;&#34920;&#30340;&#21333;&#35789;&#26469;&#32452;&#25104;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#29983;&#25104;&#23450;&#20041;&#20026;&#20174;&#29616;&#26377;&#25991;&#26412;&#38598;&#21512;&#20013;&#36880;&#27493;&#22797;&#21046;&#25991;&#26412;&#29255;&#27573;&#65288;&#20363;&#22914;&#21333;&#35789;&#25110;&#30701;&#35821;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#25928;&#30340;&#21521;&#37327;&#25628;&#32034;&#24037;&#20855;&#21253;&#35745;&#31639;&#26377;&#24847;&#20041;&#30340;&#25991;&#26412;&#29255;&#27573;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#24182;&#23545;&#20854;&#36827;&#34892;&#32034;&#24341;&#12290;&#28982;&#21518;&#65292;&#25991;&#26412;&#29983;&#25104;&#30340;&#20219;&#21153;&#34987;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#30340;&#22797;&#21046;&#21644;&#31896;&#36148;&#25805;&#20316;&#65306;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#65292;&#25105;&#20204;&#20174;&#25991;&#26412;&#38598;&#21512;&#20013;&#23547;&#25214;&#21512;&#36866;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#32780;&#19981;&#26159;&#20174;&#29420;&#31435;&#30340;&#35789;&#27719;&#34920;&#20013;&#36873;&#25321;&#12290;&#22312;&#26631;&#20934;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#27979;&#35797;&#65288;WikiText-103&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#37117;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20943;&#23569;&#20102;&#35299;&#30721;&#27493;&#39588;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25512;&#29702;&#25928;&#29575;&#19982;&#22522;&#20110;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#22320;&#20999;&#25442;&#21040;&#19981;&#21516;&#39046;&#22495;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary. In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection. We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits. The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary. Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations. Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps. We also show that our approach allows for effective domain adaptation by simply switching to d
&lt;/p&gt;</description></item><item><title>ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.06954</link><description>&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#32508;&#36848;&#65306;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06954
&lt;/p&gt;
&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#26159;Evalita 2023&#39318;&#27425;&#25552;&#20986;&#30340;&#26032;&#20849;&#20139;&#20219;&#21153;&#12290;ACTI&#25361;&#25112;&#20165;&#22522;&#20110;Telegram&#19978;&#30340;&#38452;&#35851;&#39057;&#36947;&#35780;&#35770;&#65292;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(i) &#38452;&#35851;&#20869;&#23481;&#20998;&#31867;&#65306;&#36776;&#35782;&#38452;&#35851;&#20869;&#23481;&#21644;(ii) &#38452;&#35851;&#31867;&#21035;&#20998;&#31867;&#65306;&#38024;&#23545;&#29305;&#23450;&#38452;&#35851;&#29702;&#35770;&#20998;&#31867;&#12290;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#20102;&#35813;&#20219;&#21153;&#65292;&#24635;&#20849;&#25552;&#20132;&#20102;81&#20010;&#32467;&#26524;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conspiracy Theory Identication task is a new shared task proposed for the first time at the Evalita 2023. The ACTI challenge, based exclusively on comments published on conspiratorial channels of telegram, is divided into two subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial content and (ii) Conspiratorial Category Classification about specific conspiracy theory classification. A total of fifteen teams participated in the task for a total of 81 submissions. We illustrate the best performing approaches were based on the utilization of large language models. We finally draw conclusions about the utilization of these models for counteracting the spreading of misinformation in online platforms.
&lt;/p&gt;</description></item><item><title>&#22269;&#38469;&#31038;&#20250;&#24517;&#39035;&#21512;&#20316;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#36890;&#36807;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#27668;&#20505;&#32463;&#27982;&#27169;&#25311;&#65292;&#35774;&#35745;&#20419;&#36827;&#21644;&#28608;&#21169;&#21512;&#20316;&#30340;&#22269;&#38469;&#26694;&#26550;&#65292;&#35299;&#20915;&#21327;&#35758;&#36981;&#23432;&#12289;&#25919;&#31574;&#30446;&#26631;&#23454;&#29616;&#21644;&#25345;&#32493;&#25215;&#35834;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.06951</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#20840;&#29699;&#27668;&#20505;&#21512;&#20316;2023&#31454;&#36187;&#35770;&#25991;&#38598;
&lt;/p&gt;
&lt;p&gt;
AI For Global Climate Cooperation 2023 Competition Proceedings. (arXiv:2307.06951v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06951
&lt;/p&gt;
&lt;p&gt;
&#22269;&#38469;&#31038;&#20250;&#24517;&#39035;&#21512;&#20316;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#36890;&#36807;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#27668;&#20505;&#32463;&#27982;&#27169;&#25311;&#65292;&#35774;&#35745;&#20419;&#36827;&#21644;&#28608;&#21169;&#21512;&#20316;&#30340;&#22269;&#38469;&#26694;&#26550;&#65292;&#35299;&#20915;&#21327;&#35758;&#36981;&#23432;&#12289;&#25919;&#31574;&#30446;&#26631;&#23454;&#29616;&#21644;&#25345;&#32493;&#25215;&#35834;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22269;&#38469;&#31038;&#20250;&#24517;&#39035;&#21512;&#20316;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#24182;&#20445;&#25345;&#32463;&#27982;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27809;&#26377;&#20840;&#29699;&#26435;&#23041;&#26426;&#26500;&#33021;&#30830;&#20445;&#22269;&#38469;&#27668;&#20505;&#21327;&#35758;&#30340;&#36981;&#23432;&#65292;&#21512;&#20316;&#24456;&#38590;&#23454;&#29616;&#12290;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#27668;&#20505;&#32463;&#27982;&#27169;&#25311;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#20419;&#36827;&#21644;&#28608;&#21169;&#21512;&#20316;&#30340;&#22269;&#38469;&#26694;&#26550;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#35848;&#21028;&#21327;&#35758;&#21644;&#27668;&#20505;&#21327;&#35758;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26694;&#26550;&#36824;&#24212;&#32771;&#34385;&#21040;&#27668;&#20505;&#32463;&#27982;&#21160;&#24577;&#21644;&#25112;&#30053;&#34892;&#20026;&#65292;&#20197;&#23454;&#29616;&#25919;&#31574;&#30446;&#26631;&#30340;&#23454;&#29616;&#21644;&#25345;&#32493;&#25215;&#35834;&#12290;&#36825;&#20123;&#25361;&#25112;&#38656;&#35201;&#36328;&#26426;&#22120;&#23398;&#20064;&#12289;&#32463;&#27982;&#23398;&#12289;&#27668;&#20505;&#31185;&#23398;&#12289;&#27861;&#24459;&#12289;&#25919;&#31574;&#12289;&#20262;&#29702;&#23398;&#31561;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#32452;&#32455;&#20102;AI for Global Climate Cooperation&#65288;&#20840;&#29699;&#27668;&#20505;&#21512;&#20316;&#20154;&#24037;&#26234;&#33021;&#65289;&#31454;&#36187;&#65292;&#21442;&#36187;&#22242;&#38431;&#25552;&#20132;&#20102;&#22522;&#20110;&#65288;&#20462;&#25913;&#30340;&#65289;RICE-N&#30340;&#22269;&#38469;&#26694;&#26550;&#30340;&#25552;&#26696;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The international community must collaborate to mitigate climate change and sustain economic growth. However, collaboration is hard to achieve, partly because no global authority can ensure compliance with international climate agreements. Combining AI with climate-economic simulations offers a promising solution to design international frameworks, including negotiation protocols and climate agreements, that promote and incentivize collaboration. In addition, these frameworks should also have policy goals fulfillment, and sustained commitment, taking into account climate-economic dynamics and strategic behaviors. These challenges require an interdisciplinary approach across machine learning, economics, climate science, law, policy, ethics, and other fields.  Towards this objective, we organized AI for Global Climate Cooperation, a Mila competition in which teams submitted proposals and analyses of international frameworks, based on (modifications of) RICE-N, an AI-driven integrated ass
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20808;&#39564;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#26816;&#26597;&#65292;&#24182;&#25506;&#32034;&#20102;&#20197;&#19977;&#23618;&#30693;&#35782;&#25972;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#30693;&#35782;&#20998;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#24179;&#34913;&#20102;&#25972;&#20307;&#35770;&#21644;&#36824;&#21407;&#35770;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.06950</link><description>&lt;p&gt;
&#36808;&#21521;&#24037;&#31243;&#20013;&#20808;&#39564;&#30693;&#35782;&#25972;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Pathway toward prior knowledge-integrated machine learning in engineering. (arXiv:2307.06950v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20808;&#39564;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#26816;&#26597;&#65292;&#24182;&#25506;&#32034;&#20102;&#20197;&#19977;&#23618;&#30693;&#35782;&#25972;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#30693;&#35782;&#20998;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#24179;&#34913;&#20102;&#25972;&#20307;&#35770;&#21644;&#36824;&#21407;&#35770;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#23383;&#21270;&#36235;&#21183;&#21644;&#25968;&#25454;&#37327;&#28608;&#22686;&#65292;&#20294;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#27169;&#22411;&#65288;&#20063;&#31216;&#20026;&#36923;&#36753;&#39537;&#21160;&#12289;&#22522;&#20110;&#29289;&#29702;&#12289;&#22522;&#20110;&#35268;&#21017;&#25110;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#65289;&#19982;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23384;&#22312;&#24182;&#34892;&#23384;&#22312;&#65292;&#21453;&#26144;&#20102;&#20851;&#20110;&#31526;&#21495;&#20027;&#20041;&#19982;&#36830;&#25509;&#20027;&#20041;&#30340;&#25345;&#32493;AI&#36777;&#35770;&#12290;&#22312;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#20013;&#20256;&#36755;&#21644;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#30340;&#36807;&#31243;&#24320;&#21457;&#30740;&#31350;&#24456;&#23569;&#35265;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#23558;&#22810;&#23398;&#31185;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#25972;&#21512;&#21040;&#26426;&#22120;&#30693;&#35782;&#21270;&#30340;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#20013;&#30340;&#21162;&#21147;&#21644;&#20027;&#27969;&#36235;&#21183;&#65292;&#20197;&#20004;&#31181;&#26041;&#24335;&#32452;&#32455;&#65306;&#26816;&#26597;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#65292;&#24182;&#25506;&#32034;&#20197;&#19977;&#23618;&#30693;&#35782;&#25972;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#30693;&#35782;&#20998;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#24179;&#34913;&#20102;&#24037;&#31243;&#39046;&#22495;&#30340;&#25972;&#20307;&#35770;&#21644;&#36824;&#21407;&#35770;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the digitalization trend and data volume surge, first-principles models (also known as logic-driven, physics-based, rule-based, or knowledge-based models) and data-driven approaches have existed in parallel, mirroring the ongoing AI debate on symbolism versus connectionism. Research for process development to integrate both sides to transfer and utilize domain knowledge in the data-driven process is rare. This study emphasizes efforts and prevailing trends to integrate multidisciplinary domain professions into machine acknowledgeable, data-driven processes in a two-fold organization: examining information uncertainty sources in knowledge representation and exploring knowledge decomposition with a three-tier knowledge-integrated machine learning paradigm. This approach balances holist and reductionist perspectives in the engineering domain.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27010;&#24565;&#21521;&#37327;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#19988;&#20855;&#26377;&#35821;&#20041;&#30340;&#29420;&#29305;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#36825;&#20123;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26131;&#20110;&#29702;&#35299;&#21644;&#19982;&#20219;&#21153;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.06913</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#25581;&#31034;&#29420;&#29305;&#30340;&#27010;&#24565;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Uncovering Unique Concept Vectors through Latent Space Decomposition. (arXiv:2307.06913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06913
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27010;&#24565;&#21521;&#37327;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#19988;&#20855;&#26377;&#35821;&#20041;&#30340;&#29420;&#29305;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#36825;&#20123;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26131;&#20110;&#29702;&#35299;&#21644;&#19982;&#20219;&#21153;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#23545;&#20110;&#24314;&#31435;&#20449;&#20219;&#21644;&#30830;&#20445;&#27169;&#22411;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26356;&#26131;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#27604;&#22914;&#20687;&#32032;&#26174;&#33879;&#24615;&#31561;&#29305;&#24449;&#24402;&#22240;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#23450;&#20041;&#35299;&#37322;&#20998;&#26512;&#30340;&#27010;&#24565;&#20250;&#21463;&#21040;&#29992;&#25143;&#23545;&#27010;&#24565;&#26399;&#26395;&#30340;&#20559;&#24046;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#27169;&#22411;&#22312;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20998;&#35299;&#19968;&#20010;&#23618;&#30340;&#28508;&#22312;&#31354;&#38388;&#25104;&#22855;&#24322;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#23545;&#20854;&#36827;&#34892;&#31934;&#28860;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#39640;&#26041;&#24046;&#26041;&#21521;&#19978;&#30340;&#27010;&#24565;&#21521;&#37327;&#65292;&#24182;&#25351;&#21521;&#35821;&#20041;&#19978;&#29420;&#29305;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#22823;&#37096;&#20998;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26131;&#20110;&#29702;&#35299;&#30340;&#65292;&#20855;&#26377;&#19968;&#33268;&#24615;&#65292;&#24182;&#19982;&#25152;&#38656;&#20219;&#21153;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCPAP&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#33008;&#33146;&#30340;&#22797;&#26434;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#34880;&#31958;&#27979;&#37327;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.06501</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#23454;&#29616;&#20154;&#24037;&#33008;&#33146;
&lt;/p&gt;
&lt;p&gt;
Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning. (arXiv:2307.06501v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCPAP&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#33008;&#33146;&#30340;&#22797;&#26434;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#34880;&#31958;&#27979;&#37327;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20154;&#24037;&#33008;&#33146;(AP)&#22312;&#23454;&#29616;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#38381;&#29615;&#34880;&#31958;&#25511;&#21046;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#30340;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#30340;&#34880;&#31958;&#27979;&#37327;&#65292;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;AP&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#36890;&#36807;&#21160;&#24577;&#27169;&#22411;&#21644;&#23433;&#20840;&#32422;&#26463;&#25552;&#20379;&#20102;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20294;&#20854;&#32570;&#20047;&#20010;&#24615;&#21270;&#65292;&#24182;&#19988;&#21463;&#21040;&#26410;&#23459;&#24067;&#30340;&#39278;&#39135;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#20294;&#38754;&#20020;&#20998;&#24067;&#20559;&#31227;&#21644;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#21363;HyCPAP&#65292;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;HyCPAP&#23558;MPC&#31574;&#30053;&#19982;&#38598;&#25104;DRL&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#24357;&#34917;&#21508;&#33258;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The artificial pancreas (AP) has shown promising potential in achieving closed-loop glucose control for individuals with type 1 diabetes mellitus (T1DM). However, designing an effective control policy for the AP remains challenging due to the complex physiological processes, delayed insulin response, and inaccurate glucose measurements. While model predictive control (MPC) offers safety and stability through the dynamic model and safety constraints, it lacks individualization and is adversely affected by unannounced meals. Conversely, deep reinforcement learning (DRL) provides personalized and adaptive strategies but faces challenges with distribution shifts and substantial data requirements. Methods: We propose a hybrid control policy for the artificial pancreas (HyCPAP) to address the above challenges. HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the strengths of both policies while compensating for their respective limitations. To facilitate faste
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#35266;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#22270;&#34955;&#65288;Bag-of-Views&#65289;&#27169;&#22411;&#26469;&#23545;&#25429;&#33719;&#30340;&#35270;&#22270;&#36827;&#34892;&#31163;&#32447;&#25968;&#25454;&#38598;&#32454;&#21270;&#21644;&#22312;&#32447;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#65288;NBV&#65289;&#35268;&#21010;&#24212;&#29992;&#20998;&#37197;&#25928;&#29992;&#65292;&#20197;&#23454;&#29616;3D&#37325;&#24314;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#35270;&#22270;&#35268;&#21010;&#24037;&#20855;&#31665;&#65288;VPT&#65289;&#12290;</title><link>http://arxiv.org/abs/2307.05832</link><description>&lt;p&gt;
&#35270;&#22270;&#34955;&#65306;&#19968;&#31181;&#22522;&#20110;&#22806;&#35266;&#30340;&#29992;&#20110;3D&#37325;&#24314;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bag of Views: An Appearance-based Approach to Next-Best-View Planning for 3D Reconstruction. (arXiv:2307.05832v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05832
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#35266;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#22270;&#34955;&#65288;Bag-of-Views&#65289;&#27169;&#22411;&#26469;&#23545;&#25429;&#33719;&#30340;&#35270;&#22270;&#36827;&#34892;&#31163;&#32447;&#25968;&#25454;&#38598;&#32454;&#21270;&#21644;&#22312;&#32447;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#65288;NBV&#65289;&#35268;&#21010;&#24212;&#29992;&#20998;&#37197;&#25928;&#29992;&#65292;&#20197;&#23454;&#29616;3D&#37325;&#24314;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#35270;&#22270;&#35268;&#21010;&#24037;&#20855;&#31665;&#65288;VPT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#26234;&#33021;&#25968;&#25454;&#37319;&#38598;&#29992;&#20110;3D&#37325;&#24314;&#21644;&#22522;&#30784;&#35774;&#26045;&#30417;&#27979;&#65292;&#30001;&#20110;&#22270;&#20687;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#27491;&#32463;&#21382;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#35270;&#22270;&#35268;&#21010;&#26159;&#36825;&#20010;&#20219;&#21153;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#23427;&#20915;&#23450;&#20102;&#20449;&#24687;&#25429;&#33719;&#31574;&#30053;&#65292;&#24182;&#19988;&#20005;&#37325;&#24433;&#21709;&#20174;&#25429;&#33719;&#30340;&#25968;&#25454;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#25110;&#30446;&#26631;&#30340;&#37096;&#20998;&#37325;&#24314;&#26469;&#23454;&#29616;&#20027;&#21160;&#37325;&#24314;&#30340;&#35270;&#22270;&#35268;&#21010;&#65307;&#21069;&#19968;&#31181;&#26041;&#27861;&#23545;&#20110;&#22797;&#26434;&#25110;&#26032;&#35782;&#21035;&#30340;&#30446;&#26631;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#32780;&#21518;&#32773;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35270;&#22270;&#34955;&#65288;BoV&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#22806;&#35266;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20026;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#32454;&#21270;&#21644;&#22312;&#32447;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#65288;NBV&#65289;&#35268;&#21010;&#24212;&#29992;&#20998;&#37197;&#25928;&#29992;&#65292;&#20197;&#23454;&#29616;3D&#37325;&#24314;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#20010;&#24037;&#20316;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#35270;&#22270;&#35268;&#21010;&#24037;&#20855;&#31665;&#65288;VPT&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
UAV-based intelligent data acquisition for 3D reconstruction and monitoring of infrastructure has been experiencing an increasing surge of interest due to the recent advancements in image processing and deep learning-based techniques. View planning is an essential part of this task that dictates the information capture strategy and heavily impacts the quality of the 3D model generated from the captured data. Recent methods have used prior knowledge or partial reconstruction of the target to accomplish view planning for active reconstruction; the former approach poses a challenge for complex or newly identified targets while the latter is computationally expensive. In this work, we present Bag-of-Views (BoV), a fully appearance-based model used to assign utility to the captured views for both offline dataset refinement and online next-best-view (NBV) planning applications targeting the task of 3D reconstruction. With this contribution, we also developed the View Planning Toolbox (VPT), 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05300</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#35748;&#30693;&#21327;&#21516;&#65306;&#36890;&#36807;&#22810;&#20154;&#26684;&#33258;&#25105;&#21327;&#20316;&#23454;&#29616;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#20381;&#36182;&#20110;&#35748;&#30693;&#21327;&#21516;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;&#19981;&#21516;&#35748;&#30693;&#36807;&#31243;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#21644;&#20449;&#24687;&#25972;&#21512;&#65292;&#20197;&#33719;&#24471;&#27604;&#20010;&#20307;&#35748;&#30693;&#36807;&#31243;&#26356;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#38656;&#35201;&#20016;&#23500;&#39046;&#22495;&#30693;&#35782;&#21644;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;LLM&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#12290;&#35748;&#30693;&#21327;&#21516;&#32773;&#25351;&#30340;&#26159;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#19982;&#22810;&#20010;&#26234;&#24935;&#21512;&#20316;&#65292;&#32467;&#21512;&#20182;&#20204;&#30340;&#20010;&#20307;&#20248;&#21183;&#21644;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#36755;&#20837;&#21160;&#24577;&#35782;&#21035;&#21644;&#27169;&#25311;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;SPP&#37322;&#25918;&#20102;LLM&#20013;&#35748;&#30693;&#21327;&#21516;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assi
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.04962</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#32593;&#32476;&#29702;&#35770;&#36827;&#34892;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04962
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#39537;&#21160;&#30340;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#29992;&#36884;&#65292;&#21363;&#20351;&#27809;&#26377;&#39069;&#22806;&#30340;&#22806;&#22312;&#22870;&#21169;&#12290;&#24403;&#29615;&#22659;&#33258;&#28982;&#34920;&#31034;&#20026;&#22270;&#26102;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#24341;&#23548;&#25506;&#32034;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65306;&#20449;&#24687;&#24046;&#29702;&#35770;&#21644;&#21387;&#32553;&#36827;&#23637;&#29702;&#35770;&#65292;&#26469;&#28608;&#21169;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#25506;&#32034;&#12290;&#36825;&#20123;&#29702;&#35770;&#23558;&#22909;&#22855;&#24515;&#35270;&#20026;&#23545;&#29615;&#22659;&#20013;&#35775;&#38382;&#33410;&#28857;&#25152;&#24341;&#21457;&#30340;&#23376;&#22270;&#30340;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#20248;&#21270;&#30340;&#20869;&#22312;&#21160;&#26426;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25552;&#20986;&#30340;&#29305;&#24449;&#20316;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;&#31867;&#21035;&#30340;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#20195;&#29702;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#21644;&#27604;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#38271;&#30340;&#25506;&#32034;&#24615;&#27493;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#20110;&#30456;&#20851;&#25299;&#25169;&#23646;&#24615;&#30340;&#36138;&#23146;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#20869;&#22312;&#21160;&#26426;&#20135;&#29983;&#30340;&#22870;&#21169;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#29983;&#25104;&#19978;&#25512;&#24191;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03380</link><description>&lt;p&gt;
&#20851;&#20110;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;ML&#27169;&#22411;&#33030;&#24369;&#24615;&#65292;&#20844;&#24179;&#24615;&#20197;&#21450;&#35299;&#37322;&#24615;&#30340;&#32570;&#20047;&#31561;&#37325;&#35201;&#38382;&#39064;&#38656;&#35201;&#31215;&#26497;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#24418;&#24335;&#21270;&#30340;ML&#27169;&#22411;&#39564;&#35777;&#12290;XAI&#30340;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;Anchors&#65289;&#21644;&#29305;&#24449;&#24402;&#22240;&#25216;&#26415;&#65288;&#20363;&#22914;&#65292;LIME&#21644;SHAP&#65289;&#12290;&#23613;&#31649;&#26377;&#24076;&#26395;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#37117;&#23481;&#26131;&#20986;&#29616;&#19968;&#31995;&#21015;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#35299;&#37322;&#19981;&#27491;&#30830;&#21644;&#36229;&#20986;&#20998;&#24067;&#37319;&#26679;&#12290;&#36817;&#26399;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#65288;FXAI&#65289;&#34429;&#28982;&#20316;&#20026;&#20197;&#19978;&#26041;&#27861;&#30340;&#26367;&#20195;&#21697;&#24182;&#36991;&#20813;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20363;&#22914;&#65292;&#38500;&#20102;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#22806;&#65292;&#36825;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#29305;&#24449;&#24402;&#22240;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;FLat-tO-WidE AppRoach (FLOWER)&#65292;&#36890;&#36807;&#23547;&#25214;&#25153;&#24179;&#21270;&#23485;&#21270;&#26497;&#23567;&#20540;&#30340;&#36807;&#31243;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#21033;&#29992;&#29699;&#20307;&#29983;&#25104;&#22120;&#27010;&#24565;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#22312;&#23567;&#22411;&#22522;&#30784;&#20219;&#21153;&#20013;&#65292;FLOWER&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14369</link><description>&lt;p&gt;
&#21033;&#29992;&#25153;&#24179;&#21270;&#21040;&#23485;&#21270;&#26041;&#27861;&#36827;&#34892;&#23569;&#26679;&#26412;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Continual Learning via Flat-to-Wide Approaches. (arXiv:2306.14369v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;FLat-tO-WidE AppRoach (FLOWER)&#65292;&#36890;&#36807;&#23547;&#25214;&#25153;&#24179;&#21270;&#23485;&#21270;&#26497;&#23567;&#20540;&#30340;&#36807;&#31243;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#21033;&#29992;&#29699;&#20307;&#29983;&#25104;&#22120;&#27010;&#24565;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#22312;&#23567;&#22411;&#22522;&#30784;&#20219;&#21153;&#20013;&#65292;FLOWER&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#12290;&#30001;&#20110;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25317;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;FLat-tO-WidE AppRoach (FLOWER)&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#23547;&#25214;&#25153;&#24179;&#21270;&#23485;&#21270;&#26497;&#23567;&#20540;&#30340;&#25153;&#24179;&#21270;&#21040;&#23485;&#21270;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#29699;&#20307;&#29983;&#25104;&#22120;&#27010;&#24565;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#23558;&#37319;&#26679;&#31354;&#38388;&#38480;&#21046;&#22312;&#26368;&#23567;&#22806;&#25509;&#29699;&#20869;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;FLOWER&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#23567;&#22411;&#22522;&#30784;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;FLOWER&#30340;&#28304;&#20195;&#30721;&#65292;&#31454;&#20105;&#31639;&#27861;&#21644;&#23454;&#39564;&#26085;&#24535;&#20844;&#24320;&#20849;&#20139;&#22312;\url{https://github.com/anwarmaxsum/FLOWER}&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches on continual learning call for a lot of samples in their training processes. Such approaches are impractical for many real-world problems having limited samples because of the overfitting problem. This paper proposes a few-shot continual learning approach, termed FLat-tO-WidE AppRoach (FLOWER), where a flat-to-wide learning process finding the flat-wide minima is proposed to address the catastrophic forgetting problem. The issue of data scarcity is overcome with a data augmentation approach making use of a ball generator concept to restrict the sampling space into the smallest enclosing ball. Our numerical studies demonstrate the advantage of FLOWER achieving significantly improved performances over prior arts notably in the small base tasks. For further study, source codes of FLOWER, competitor algorithms and experimental logs are shared publicly in \url{https://github.com/anwarmaxsum/FLOWER}.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#24247;&#22797;&#35757;&#32451;&#30340;&#31649;&#29702;&#12290;&#24739;&#32773;&#21487;&#20197;&#36890;&#36807;&#31995;&#32479;&#36827;&#34892;&#24212;&#29992;&#31243;&#24207;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27835;&#30103;&#24072;&#21487;&#20197;&#36890;&#36807;&#26381;&#21153;&#22120;&#31471;&#36827;&#34892;&#36827;&#24230;&#30340;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.10756</link><description>&lt;p&gt;
&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A HRNet-based Rehabilitation Monitoring System. (arXiv:2306.10756v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#24247;&#22797;&#35757;&#32451;&#30340;&#31649;&#29702;&#12290;&#24739;&#32773;&#21487;&#20197;&#36890;&#36807;&#31995;&#32479;&#36827;&#34892;&#24212;&#29992;&#31243;&#24207;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27835;&#30103;&#24072;&#21487;&#20197;&#36890;&#36807;&#26381;&#21153;&#22120;&#31471;&#36827;&#34892;&#36827;&#24230;&#30340;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24247;&#22797;&#27835;&#30103;&#26377;&#21161;&#20110;&#27835;&#24840;&#36731;&#24494;&#30340;&#20307;&#32946;&#21644;&#32844;&#19994;&#20260;&#23475;&#12290;&#20256;&#32479;&#30340;&#24247;&#22797;&#36807;&#31243;&#20013;&#65292;&#27835;&#30103;&#24072;&#20250;&#25351;&#23450;&#26576;&#20123;&#21160;&#20316;&#20379;&#24739;&#32773;&#22312;&#21307;&#38498;&#35775;&#38382;&#20043;&#38388;&#25191;&#34892;&#65292;&#36825;&#23558;&#20381;&#36182;&#20110;&#24739;&#32773;&#27491;&#30830;&#22320;&#35760;&#20303;&#21160;&#20316;&#21644;&#25191;&#34892;&#35745;&#21010;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#24739;&#32773;&#20250;&#24536;&#35760;&#25191;&#34892;&#21160;&#20316;&#25110;&#26080;&#27861;&#35814;&#32454;&#22238;&#24819;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#24247;&#22797;&#27835;&#30103;&#21463;&#21040;&#38459;&#30861;&#65292;&#25110;&#32773;&#22312;&#26368;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#24739;&#32773;&#21487;&#33021;&#20250;&#22240;&#25191;&#34892;&#19981;&#27491;&#30830;&#30340;&#21160;&#20316;&#32780;&#36973;&#21463;&#39069;&#22806;&#30340;&#20260;&#23475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#24739;&#32773;&#30340;&#26234;&#33021;&#25163;&#26426;&#25552;&#37266;&#24739;&#32773;&#20309;&#26102;&#25191;&#34892;&#21160;&#20316;&#24182;&#23637;&#31034;&#21160;&#20316;&#20379;&#24739;&#32773;&#36319;&#38543;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#24110;&#21161;&#27835;&#30103;&#24072;&#30417;&#27979;&#24739;&#32773;&#30340;&#24247;&#22797;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#30001;&#19968;&#27454;iOS&#24212;&#29992;&#31243;&#24207;&#21644;&#20960;&#20010;&#26381;&#21153;&#22120;&#32452;&#20214;&#32452;&#25104;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#36127;&#36131;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
The rehabilitation treatment helps to heal minor sports and occupational injuries. In a traditional rehabilitation process, a therapist will assign certain actions to a patient to perform in between hospital visits, and it will rely on the patient to remember actions correctly and the schedule to perform them. Unfortunately, many patients forget to perform actions or fail to recall actions in detail. As a consequence, the rehabilitation treatment is hampered or, in the worst case, the patient may suffer from additional injury caused by performing incorrect actions. To resolve these issues, we propose a HRNet-based rehabilitation monitoring system, which can remind a patient when to perform the actions and display the actions for the patient to follow via the patient's smartphone. In addition, it helps the therapist to monitor the progress of the rehabilitation for the patient. Our system consists of an iOS app and several components at the server side. The app is in charge of displayin
&lt;/p&gt;</description></item><item><title>Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18405</link><description>&lt;p&gt;
Dink-Net: &#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18405
&lt;/p&gt;
&lt;p&gt;
Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#22270;&#32858;&#31867;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#24418;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#32452;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;Dink-Net&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21306;&#20998;&#24102;&#22686;&#24378;&#30340;&#36319;&#19981;&#24102;&#22686;&#24378;&#30340;&#33410;&#28857;&#65292;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#24418;&#24335;&#12290;&#21516;&#26102;&#65292;&#23558;&#32858;&#31867;&#20013;&#24515;&#21021;&#22987;&#21270;&#20026;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#26041;&#24335;&#26368;&#23567;&#21270;&#25552;&#20986;&#30340;&#38598;&#32676;&#33192;&#32960;&#25439;&#22833;&#21644;&#38598;&#32676;&#25910;&#32553;&#25439;&#22833;&#65292;&#20248;&#21270;&#32858;&#31867;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#20123;&#35774;&#32622;&#65292;&#25105;&#20204;&#23558;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#20004;&#20010;&#27493;&#39588;&#32479;&#19968;&#20026;&#19968;&#20010;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#24341;&#23548;&#32593;&#32476;&#23398;&#20064;&#32858;&#31867;&#21451;&#22909;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;Dink-Net&#33021;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#22270;&#24418;&#19978;&#65292;&#22240;&#20026;&#35774;&#35745;&#30340;&#33192;&#32960;&#25910;&#32553;&#25805;&#20316;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Dink-Net&#22312;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#22270;&#24418;&#30340;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#22270;&#32858;&#31867;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#12290;&#20316;&#32773;&#21457;&#29616;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#65292;&#36825;&#19968;&#29702;&#35770;&#20026;&#24212;&#29992;&#19982;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.17375</link><description>&lt;p&gt;
&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Attention Schema in Neural Agents. (arXiv:2305.17375v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#12290;&#20316;&#32773;&#21457;&#29616;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#65292;&#36825;&#19968;&#29702;&#35770;&#20026;&#24212;&#29992;&#19982;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#24120;&#35265;&#35201;&#32032;&#12290;&#23427;&#36890;&#36807;&#21152;&#20837;&#21160;&#24577;&#20449;&#24687;&#36873;&#25321;&#65292;&#25903;&#25345;&#38745;&#24577;&#30340;&#26435;&#37325;&#36873;&#25321;&#12290;&#21516;&#26679;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#24819;&#35937;&#22312;&#27880;&#24847;&#21147;&#20043;&#19978;&#26500;&#24314;&#19968;&#20010;&#26356;&#39640;&#38454;&#30340;&#20449;&#24687;&#36807;&#28388;&#22120;&#65306;&#27880;&#24847;&#21147;&#27169;&#24335;&#65288;AS&#65289;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#19968;&#20010;&#25551;&#36848;&#24615;&#21644;&#39044;&#27979;&#24615;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#25903;&#25345;&#36825;&#31181;&#21306;&#20998;&#27880;&#24847;&#21147;&#21644;AS&#30340;&#24819;&#27861;&#12290;&#35813;&#29702;&#35770;&#30340;&#19968;&#20010;&#37325;&#35201;&#39044;&#27979;&#26159;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#20351;&#29992;&#33258;&#24049;&#30340;AS&#26469;&#25512;&#26029;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#27880;&#24847;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#23454;&#39564;&#27979;&#35797;AST&#26377;&#25928;&#24615;&#30340;&#29702;&#24819;&#22330;&#26223;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;AS&#30456;&#20114;&#20316;&#29992;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#23454;&#39564;&#20026;&#29702;&#35299;&#20197;&#21450;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention has become a common ingredient in deep learning architectures. It adds a dynamical selection of information on top of the static selection of information supported by weights. In the same way, we can imagine a higher-order informational filter built on top of attention: an Attention Schema (AS), namely, a descriptive and predictive model of attention. In cognitive neuroscience, Attention Schema Theory (AST) supports this idea of distinguishing attention from AS. A strong prediction of this theory is that an agent can use its own AS to also infer the states of other agents' attention and consequently enhance coordination with other agents. As such, multi-agent reinforcement learning would be an ideal setting to experimentally test the validity of AST. We explore different ways in which attention and AS interact with each other. Our preliminary results indicate that agents that implement the AS as a recurrent internal control achieve the best performance. In general, these expl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#65292;&#24182;&#19988;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#20849;&#21019;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;</title><link>http://arxiv.org/abs/2305.14724</link><description>&lt;p&gt;
&#25105;&#23547;&#35269;&#19968;&#20010;&#38544;&#21947;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20849;&#21019;&#35270;&#35273;&#38544;&#21947;
&lt;/p&gt;
&lt;p&gt;
I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#65292;&#24182;&#19988;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#20849;&#21019;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38544;&#21947;&#26159;&#36890;&#36807;&#22270;&#20687;&#26469;&#35828;&#26381;&#25110;&#20256;&#36798;&#21019;&#24847;&#24819;&#27861;&#30340;&#24378;&#22823;&#20462;&#36766;&#25163;&#27861;&#12290;&#19982;&#35821;&#35328;&#38544;&#21947;&#31867;&#20284;&#65292;&#23427;&#20204;&#36890;&#36807;&#31526;&#21495;&#20027;&#20041;&#21644;&#31526;&#21495;&#30340;&#24182;&#32622;&#38544;&#21547;&#22320;&#20256;&#36798;&#21547;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#35821;&#35328;&#38544;&#21947;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#30340;&#26032;&#20219;&#21153;&#12290;&#36825;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65288;&#22914;DALL $\cdot$ E 2&#65289;&#26469;&#35828;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#27169;&#25311;&#38544;&#21547;&#21547;&#20041;&#21644;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#37319;&#29992;&#20197;&#8220;&#20018;&#32852;&#24605;&#32500;&#8221;&#20026;&#25552;&#31034;&#30340;Instruct GPT-3&#65288;davinci-002&#65289;&#29983;&#25104;&#20195;&#34920;&#35821;&#35328;&#38544;&#21947;&#30340;&#35270;&#35273;&#38416;&#36848;&#30340;&#25991;&#26412;&#65292;&#20854;&#20013;&#21253;&#21547;&#38544;&#21547;&#21547;&#20041;&#21644;&#30456;&#20851;&#23545;&#35937;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#65292;&#20154;&#20204;&#19982;LLM&#21644;&#34920;&#29616;&#26368;&#20339;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#21019;&#24314;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#38544;&#21947;&#21644;&#23427;&#20204;&#30340;&#35270;&#35273;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#21487;&#20197;&#20849;&#21516;&#21019;&#36896;&#20986;&#20855;&#26377;&#35270;&#35273;&#20914;&#20987;&#21147;&#21644;&#35821;&#20041;&#21547;&#20041;&#30340;&#38544;&#21947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models.Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-qu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01486</link><description>&lt;p&gt;
ARBEx&#65306;&#29992;&#20110;&#40065;&#26834;&#24615;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#30340;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#19982;&#21487;&#38752;&#24615;&#24179;&#34913;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning. (arXiv:2305.01486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#26159;&#30001;Vision Transformer&#39537;&#21160;&#30340;&#26032;&#22411;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#24102;&#26377;&#21487;&#38752;&#24615;&#24179;&#34913;&#65292;&#20197;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#36739;&#24046;&#31867;&#20998;&#24067;&#12289;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#31934;&#21270;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#31383;&#21475;&#30340;&#20132;&#21449;&#20851;&#27880;ViT&#26469;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#65292;&#21152;&#19978;&#26631;&#31614;&#20998;&#24067;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#36890;&#36807;&#21487;&#38752;&#24615;&#24179;&#34913;&#20248;&#21270;&#23545;&#24369;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#39640;&#26631;&#31614;&#39044;&#27979;&#38887;&#24615;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#30830;&#20445;&#27491;&#30830;&#30340;&#26631;&#31614;&#20998;&#31867;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38170;&#25439;&#22833;&#65292;&#40723;&#21169;&#38170;&#28857;&#20043;&#38388;&#30340;&#22823;&#38388;&#38548;&#12290;&#21478;&#22806;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#20063;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#25552;&#21319;&#22312;FEL&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;ARBEx&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a framework ARBEx, a novel attentive feature extraction framework driven by Vision Transformer with reliability balancing to cope against poor class distributions, bias, and uncertainty in the facial expression learning (FEL) task. We reinforce several data pre-processing and refinement methods along with a window-based cross-attention ViT to squeeze the best of the data. We also employ learnable anchor points in the embedding space with label distributions and multi-head self-attention mechanism to optimize performance against weak predictions with reliability balancing, which is a strategy that leverages anchor points, attention scores, and confidence values to enhance the resilience of label predictions. To ensure correct label classification and improve the models' discriminative power, we introduce anchor loss, which encourages large margins between anchor points. Additionally, the multi-head self-attention mechanism, which is also trainable, plays an i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21487;&#35299;&#37322;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#21644;&#31526;&#21495;&#19990;&#30028;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#21487;&#35299;&#37322;&#24615;&#21644;&#22788;&#29702;&#29615;&#22659;&#32467;&#26500;&#21464;&#21270;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2304.08349</link><description>&lt;p&gt;
&#28145;&#24230;&#21487;&#35299;&#37322;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach. (arXiv:2304.08349v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21487;&#35299;&#37322;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#21644;&#31526;&#21495;&#19990;&#30028;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#21487;&#35299;&#37322;&#24615;&#21644;&#22788;&#29702;&#29615;&#22659;&#32467;&#26500;&#21464;&#21270;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#20173;&#28982;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;DRL&#27809;&#26377;&#21033;&#29992;&#31526;&#21495;&#20851;&#31995;&#34920;&#31034;&#65292;&#23427;&#22312;&#22788;&#29702;&#29615;&#22659;&#32467;&#26500;&#21464;&#21270;&#65288;&#22914;&#23545;&#35937;&#25968;&#37327;&#22686;&#21152;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#20174;&#31526;&#21495;&#35745;&#21010;&#32487;&#25215;&#20102;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#26080;&#27861;&#25193;&#23637;&#24182;&#20805;&#20998;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21487;&#35299;&#37322;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#65288;DERRL&#65289;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#31070;&#32463;&#21644;&#31526;&#21495;&#19990;&#30028;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#37319;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;DERRL&#23558;&#31526;&#21495;&#35745;&#21010;&#20013;&#30340;&#20851;&#31995;&#34920;&#31034;&#21644;&#32422;&#26463;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#21462;&#20986;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#20197;&#36923;&#36753;&#35268;&#21017;&#30340;&#24418;&#24335;&#35299;&#37322;&#20102;&#27599;&#20010;&#20915;&#31574;&#65288;&#25110;&#21160;&#20316;&#65289;&#30340;&#20135;&#29983;&#36807;&#31243;&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DERRL&#22312;&#22788;&#29702;&#29615;&#22659;&#32467;&#26500;&#21464;&#21270;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite numerous successes in Deep Reinforcement Learning (DRL), the learned policies are not interpretable. Moreover, since DRL does not exploit symbolic relational representations, it has difficulties in coping with structural changes in its environment (such as increasing the number of objects). Relational Reinforcement Learning, on the other hand, inherits the relational representations from symbolic planning to learn reusable policies. However, it has so far been unable to scale up and exploit the power of deep neural networks. We propose Deep Explainable Relational Reinforcement Learning (DERRL), a framework that exploits the best of both -- neural and symbolic worlds. By resorting to a neuro-symbolic approach, DERRL combines relational representations and constraints from symbolic planning with deep learning to extract interpretable policies. These policies are in the form of logical rules that explain how each decision (or action) is arrived at. Through several experiments, in 
&lt;/p&gt;</description></item><item><title>DiffTAD&#26159;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;Proposal Denoising Diffusion&#30340;&#29983;&#25104;&#24314;&#27169;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;/&#22122;&#22768;&#36807;&#31243;&#21644;&#21453;&#21521;/&#21435;&#22122;&#36807;&#31243;&#23454;&#29616;&#20934;&#30830;&#30340;&#21160;&#20316;&#25552;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.14863</link><description>&lt;p&gt;
DiffTAD: &#20351;&#29992;Proposal Denoising Diffusion&#30340;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion. (arXiv:2303.14863v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14863
&lt;/p&gt;
&lt;p&gt;
DiffTAD&#26159;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;Proposal Denoising Diffusion&#30340;&#29983;&#25104;&#24314;&#27169;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;/&#22122;&#22768;&#36807;&#31243;&#21644;&#21453;&#21521;/&#21435;&#22122;&#36807;&#31243;&#23454;&#29616;&#20934;&#30830;&#30340;&#21160;&#20316;&#25552;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#65288;TAD&#65289;&#30340;&#34920;&#36798;&#24418;&#24335;&#65292;&#21363;DiffTAD&#12290;&#23427;&#37319;&#29992;&#38543;&#26426;&#26102;&#38388;&#25552;&#35758;&#20316;&#20026;&#36755;&#20837;&#65292;&#22312;&#32473;&#23450;&#26410;&#20462;&#21098;&#30340;&#38271;&#26102;&#38388;&#35270;&#39057;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#20135;&#29983;&#21160;&#20316;&#25552;&#35758;&#12290;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#24314;&#27169;&#30340;&#35270;&#35282;&#65292;&#19982;&#20808;&#21069;&#30340;&#21028;&#21035;&#23398;&#20064;&#26041;&#24335;&#30456;&#23545;&#31435;&#12290;&#36890;&#36807;&#39318;&#20808;&#23558;&#22320;&#38754;&#30495;&#23454;&#25552;&#35758;&#25193;&#25955;&#21040;&#38543;&#26426;&#25552;&#35758;&#65288;&#21363;&#27491;&#21521;/&#22122;&#22768;&#36807;&#31243;&#65289;&#65292;&#28982;&#21518;&#23398;&#20064;&#36870;&#36716;&#22122;&#22768;&#36807;&#31243;&#65288;&#21363;&#21453;&#21521;/&#21435;&#22122;&#36807;&#31243;&#65289;&#26469;&#23454;&#29616;&#36825;&#31181;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;Transformer&#35299;&#30721;&#22120;&#65288;&#22914;DETR&#65289;&#20013;&#24341;&#20837;&#20855;&#26377;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#30340;&#26102;&#38388;&#20301;&#32622;&#26597;&#35810;&#35774;&#35745;&#26469;&#24314;&#31435;&#21435;&#22122;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#21152;&#36895;&#30340;&#20132;&#21449;&#27493;&#36873;&#25321;&#24615;&#35843;&#33410;&#31639;&#27861;&#12290;&#23545;ActivityNet&#21644;THUMOS&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;DiffTAD&#30456;&#27604;&#20808;&#21069;&#30340;&#26367;&#20195;&#26041;&#26696;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/sauradip/Di&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new formulation of temporal action detection (TAD) with denoising diffusion, DiffTAD in short. Taking as input random temporal proposals, it can yield action proposals accurately given an untrimmed long video. This presents a generative modeling perspective, against previous discriminative learning manners. This capability is achieved by first diffusing the ground-truth proposals to random ones (i.e., the forward/noising process) and then learning to reverse the noising process (i.e., the backward/denoising process). Concretely, we establish the denoising process in the Transformer decoder (e.g., DETR) by introducing a temporal location query design with faster convergence in training. We further propose a cross-step selective conditioning algorithm for inference acceleration. Extensive evaluations on ActivityNet and THUMOS show that our DiffTAD achieves top performance compared to previous art alternatives. The code will be made available at https://github.com/sauradip/Di
&lt;/p&gt;</description></item><item><title>WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07274</link><description>&lt;p&gt;
&#25171;&#30772;&#24120;&#35782;&#65306;WHOOPS&#65281;&#19968;&#20010;&#22522;&#20110;&#21512;&#25104;&#21644;&#32452;&#21512;&#22270;&#20687;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07274
&lt;/p&gt;
&lt;p&gt;
WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22855;&#24618;&#12289;&#24322;&#24120;&#21644;&#31070;&#31192;&#30340;&#22270;&#20687;&#20250;&#24341;&#36215;&#35266;&#23519;&#32773;&#30340;&#22909;&#22855;&#24515;&#65292;&#22240;&#20026;&#23427;&#20204;&#25361;&#25112;&#20102;&#24120;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;WHOOPS&#65281;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#35774;&#35745;&#24072;&#20351;&#29992;Midjourney&#31561;&#20844;&#20849;&#21487;&#29992;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#21046;&#20316;&#65292;&#24182;&#21253;&#21547;&#33509;&#24178;&#20010;&#20219;&#21153;&#12290;&#38500;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#24182;&#35299;&#37322;&#32473;&#23450;&#22270;&#20687;&#30340;&#24322;&#24120;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weird, unusual, and uncanny images pique the curiosity of observers because they challenge commonsense. For example, an image released during the 2022 world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo playing chess, which playfully violates our expectation that their competition should occur on the football field. Humans can easily recognize and interpret these unconventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explanation generation task, where models must identify and explain why a given image is unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;3D&#28857;&#20113;&#20013;&#36827;&#34892;&#26080;&#38480;&#25968;&#37327;&#25903;&#25745;&#26816;&#27979;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#26469;&#21033;&#29992;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#35774;&#32622;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.02401</link><description>&lt;p&gt;
&#22312;3D&#28857;&#20113;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Affordance Detection in 3D Point Clouds. (arXiv:2303.02401v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;3D&#28857;&#20113;&#20013;&#36827;&#34892;&#26080;&#38480;&#25968;&#37327;&#25903;&#25745;&#26816;&#27979;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#26469;&#21033;&#29992;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#35774;&#32622;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25745;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#23616;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#25903;&#25745;&#26631;&#31614;&#65292;&#21487;&#33021;&#38480;&#21046;&#20102;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#65288;OpenAD&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#28857;&#20113;&#20013;&#26816;&#27979;&#26080;&#38480;&#25968;&#37327;&#30340;&#25903;&#25745;&#12290;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#65292;OpenAD&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#24182;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#20309;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#25903;&#25745;&#26816;&#27979;&#35774;&#32622;&#19978;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;OpenAD&#22312;&#23454;&#38469;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affordance detection is a challenging problem with a wide variety of robotic applications. Traditional affordance detection methods are limited to a predefined set of affordance labels, hence potentially restricting the adaptability of intelligent robots in complex and dynamic environments. In this paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method, which is capable of detecting an unbounded number of affordances in 3D point clouds. By simultaneously learning the affordance text and the point feature, OpenAD successfully exploits the semantic relationships between affordances. Therefore, our proposed method enables zero-shot detection and can be able to detect previously unseen affordances without a single annotation example. Intensive experimental results show that OpenAD works effectively on a wide range of affordance detection setups and outperforms other baselines by a large margin. Additionally, we demonstrate the practicality of the proposed OpenAD in real
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DoCoFL&#65292;&#19968;&#31181;&#29992;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#30340;&#19979;&#34892;&#21387;&#32553;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#38477;&#20302;&#21452;&#21521;&#24102;&#23485;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00543</link><description>&lt;p&gt;
DoCoFL&#65306;&#29992;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#30340;&#19979;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
DoCoFL: Downlink Compression for Cross-Device Federated Learning. (arXiv:2302.00543v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DoCoFL&#65292;&#19968;&#31181;&#29992;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#30340;&#19979;&#34892;&#21387;&#32553;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#38477;&#20302;&#21452;&#21521;&#24102;&#23485;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21387;&#32553;&#25216;&#26415;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#20943;&#23569;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#65292;&#32780;&#27169;&#22411;&#26356;&#26032;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#36880;&#28176;&#20943;&#23569;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36328;&#35774;&#22791;&#30340;&#19979;&#34892;&#65288;&#21363;&#20174;&#26381;&#21153;&#22120;&#21040;&#23458;&#25143;&#31471;&#65289;&#21387;&#32553;&#65292;&#22312;&#36825;&#31181;&#22330;&#26223;&#19979;&#65292;&#24322;&#26500;&#23458;&#25143;&#31471;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#33021;&#21482;&#20986;&#29616;&#19968;&#27425;&#65292;&#22240;&#27492;&#24517;&#39035;&#19979;&#36733;&#27169;&#22411;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DoCoFL&#8212;&#8212;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#36328;&#35774;&#22791;&#19979;&#34892;&#21387;&#32553;&#30340;&#26694;&#26550;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;DoCoFL&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#35768;&#22810;&#19978;&#34892;&#21387;&#32553;&#26041;&#26696;&#32467;&#21512;&#20351;&#29992;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#21452;&#21521;&#21387;&#32553;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;DoCoFL&#22312;&#26174;&#33879;&#38477;&#20302;&#21452;&#21521;&#24102;&#23485;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#19982;&#27809;&#26377;&#20219;&#20309;&#21387;&#32553;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many compression techniques have been proposed to reduce the communication overhead of Federated Learning training procedures. However, these are typically designed for compressing model updates, which are expected to decay throughout training. As a result, such methods are inapplicable to downlink (i.e., from the parameter server to clients) compression in the cross-device setting, where heterogeneous clients $\textit{may appear only once}$ during training and thus must download the model parameters. Accordingly, we propose $\textsf{DoCoFL}$ -- a new framework for downlink compression in the cross-device setting. Importantly, $\textsf{DoCoFL}$ can be seamlessly combined with many uplink compression schemes, rendering it suitable for bi-directional compression. Through extensive evaluation, we show that $\textsf{DoCoFL}$ offers significant bi-directional bandwidth reduction while achieving competitive accuracy to that of a baseline without any compression.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;Transformer&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#19968;&#32452;&#22270;&#20687;&#25551;&#36848;&#20026;&#19968;&#20010;&#25925;&#20107;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;ViT&#25552;&#21462;&#36755;&#20837;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;LSTM&#25429;&#25417;&#22270;&#20687;&#34917;&#19969;&#30340;&#36807;&#21435;&#21644;&#26410;&#26469;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#21152;&#26435;&#35745;&#31639;&#24471;&#21040;&#26368;&#32456;&#30340;&#25925;&#20107;&#25551;&#36848;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.02762</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;Transformer&#30340;&#27169;&#22411;&#29992;&#20110;&#23558;&#19968;&#32452;&#22270;&#20687;&#25551;&#36848;&#20026;&#19968;&#20010;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;Transformer&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#19968;&#32452;&#22270;&#20687;&#25551;&#36848;&#20026;&#19968;&#20010;&#25925;&#20107;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;ViT&#25552;&#21462;&#36755;&#20837;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;LSTM&#25429;&#25417;&#22270;&#20687;&#34917;&#19969;&#30340;&#36807;&#21435;&#21644;&#26410;&#26469;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#21152;&#26435;&#35745;&#31639;&#24471;&#21040;&#26368;&#32456;&#30340;&#25925;&#20107;&#25551;&#36848;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25925;&#20107;&#35762;&#36848;&#26159;&#23558;&#19968;&#32452;&#22270;&#20687;&#24418;&#25104;&#22810;&#21477;&#25925;&#20107;&#30340;&#36807;&#31243;&#12290;&#24688;&#24403;&#22320;&#21253;&#21547;&#36755;&#20837;&#22270;&#20687;&#20869;&#25429;&#25417;&#21040;&#30340;&#35270;&#35273;&#21464;&#21270;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#35270;&#35273;&#25925;&#20107;&#35762;&#36848;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#30001;&#19968;&#32452;&#22270;&#20687;&#24320;&#21457;&#30340;&#25925;&#20107;&#32463;&#24120;&#32570;&#20047;&#20957;&#32858;&#21147;&#12289;&#30456;&#20851;&#24615;&#21644;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35270;&#35273;Transformer&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#19968;&#32452;&#22270;&#20687;&#25551;&#36848;&#20026;&#19968;&#20010;&#25925;&#20107;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#25552;&#21462;&#36755;&#20837;&#22270;&#20687;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#25104;16X16&#30340;&#34917;&#19969;&#65292;&#24182;&#25414;&#32465;&#21040;&#25153;&#24179;&#21270;&#34917;&#19969;&#30340;&#32447;&#24615;&#25237;&#24433;&#20013;&#12290;&#20174;&#21333;&#20010;&#22270;&#20687;&#21040;&#22810;&#20010;&#22270;&#20687;&#34917;&#19969;&#30340;&#36716;&#25442;&#25429;&#25417;&#21040;&#20102;&#36755;&#20837;&#35270;&#35273;&#27169;&#24335;&#30340;&#35270;&#35273;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#21452;&#21521;LSTM&#65292;&#23427;&#26159;&#24207;&#21015;&#32534;&#30721;&#22120;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#26679;&#21487;&#20197;&#25429;&#25417;&#21040;&#25152;&#26377;&#22270;&#20687;&#34917;&#19969;&#30340;&#36807;&#21435;&#21644;&#26410;&#26469;&#22270;&#20687;&#19978;&#19979;&#25991;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#21152;&#26435;&#35745;&#31639;&#22270;&#20687;&#34917;&#19969;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#20135;&#29983;&#26368;&#32456;&#30340;&#25925;&#20107;&#25551;&#36848;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Story-Telling is the process of forming a multi-sentence story from a set of images. Appropriately including visual variation and contextual information captured inside the input images is one of the most challenging aspects of visual storytelling. Consequently, stories developed from a set of images often lack cohesiveness, relevance, and semantic relationship. In this paper, we propose a novel Vision Transformer Based Model for describing a set of images as a story. The proposed method extracts the distinct features of the input images using a Vision Transformer (ViT). Firstly, input images are divided into 16X16 patches and bundled into a linear projection of flattened patches. The transformation from a single image to multiple image patches captures the visual variety of the input visual patterns. These features are used as input to a Bidirectional-LSTM which is part of the sequence encoder. This captures the past and future image context of all image patches. Then, an atten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#30830;&#20445;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2202.12319</link><description>&lt;p&gt;
&#20445;&#25252;&#38544;&#31169;&#30340;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving machine learning with tensor networks. (arXiv:2202.12319v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#30830;&#20445;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#34987;&#24191;&#27867;&#29992;&#20110;&#25552;&#20379;&#20302;&#33021;&#37327;&#24577;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#21307;&#30103;&#35760;&#24405;&#31561;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#30340;&#26032;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30830;&#20445;&#23545;&#36825;&#31181;&#28431;&#27934;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#65292;&#36825;&#28041;&#21450;&#21040;&#22312;&#35268;&#33539;&#23545;&#31216;&#24615;&#19979;&#31561;&#20215;&#30340;&#27169;&#22411;&#30340;&#21051;&#30011;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;&#30340;&#35268;&#33539;&#24418;&#24335;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#35268;&#24459;&#24615;&#24182;&#20462;&#27491;&#20102;&#27531;&#20313;&#35268;&#33539;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor networks, widely used for providing efficient representations of low-energy states of local quantum many-body systems, have been recently proposed as machine learning architectures which could present advantages with respect to traditional ones. In this work we show that tensor network architectures have especially prospective properties for privacy-preserving machine learning, which is important in tasks such as the processing of medical records. First, we describe a new privacy vulnerability that is present in feedforward neural networks, illustrating it in synthetic and real-world datasets. Then, we develop well-defined conditions to guarantee robustness to such vulnerability, which involve the characterization of models equivalent under gauge symmetry. We rigorously prove that such conditions are satisfied by tensor-network architectures. In doing so, we define a novel canonical form for matrix product states, which has a high degree of regularity and fixes the residual gaug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#31639;&#27861;&#20915;&#31574;&#36827;&#34892;&#26368;&#20248;&#35843;&#25511;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38480;&#21046;&#20195;&#29702;&#20351;&#29992;&#36879;&#26126;&#24230;&#36275;&#22815;&#39640;&#30340;&#39044;&#27979;&#20989;&#25968;&#26159;&#20302;&#25928;&#30340;&#65292;&#32780;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#30340;&#30446;&#26631;&#21270;&#24037;&#20855;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25913;&#21892;&#31119;&#21033;&#12290;</title><link>http://arxiv.org/abs/2110.03443</link><description>&lt;p&gt;
&#25581;&#24320;&#40657;&#30418;&#23376;&#65306;&#35843;&#25511;&#31639;&#27861;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Unpacking the Black Box: Regulating Algorithmic Decisions. (arXiv:2110.03443v2 [econ.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#31639;&#27861;&#20915;&#31574;&#36827;&#34892;&#26368;&#20248;&#35843;&#25511;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38480;&#21046;&#20195;&#29702;&#20351;&#29992;&#36879;&#26126;&#24230;&#36275;&#22815;&#39640;&#30340;&#39044;&#27979;&#20989;&#25968;&#26159;&#20302;&#25928;&#30340;&#65292;&#32780;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#30340;&#30446;&#26631;&#21270;&#24037;&#20855;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25913;&#21892;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#65288;&#22914;&#36151;&#27454;&#12289;&#21307;&#30103;&#27979;&#35797;&#25110;&#25307;&#32856;&#65289;&#19988;&#22996;&#25176;&#20154;&#22312;&#20102;&#35299;&#20195;&#29702;&#30340;&#40657;&#30418;&#27169;&#22411;&#26041;&#38754;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#22320;&#35843;&#25511;&#39044;&#27979;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#35825;&#23548;&#19981;&#36275;&#65292;&#19988;&#26368;&#20248;&#39044;&#27979;&#20989;&#25968;&#36275;&#22815;&#22797;&#26434;&#65292;&#23558;&#20195;&#29702;&#38480;&#21046;&#22312;&#36275;&#22815;&#36879;&#26126;&#30340;&#39044;&#27979;&#20989;&#25968;&#20013;&#26159;&#20302;&#25928;&#30340;&#12290;&#31639;&#27861;&#23457;&#35745;&#26377;&#21161;&#20110;&#25552;&#39640;&#31119;&#21033;&#65292;&#20294;&#20854;&#25910;&#30410;&#21462;&#20915;&#20110;&#23457;&#35745;&#24037;&#20855;&#30340;&#35774;&#35745;&#12290;&#35768;&#22810;&#35299;&#37322;&#24037;&#20855;&#20542;&#21521;&#20110;&#26368;&#23567;&#21270;&#25972;&#20307;&#20449;&#24687;&#25439;&#22833;&#65292;&#20294;&#36825;&#36890;&#24120;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38598;&#20013;&#20110;&#35299;&#37322;&#39044;&#27979;&#20989;&#25968;&#30340;&#24179;&#22343;&#34892;&#20026;&#12290;&#38024;&#23545;&#24615;&#30340;&#24037;&#20855;&#65292;&#22914;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#65288;&#22914;&#36807;&#22810;&#30340;&#20551;&#38451;&#24615;&#25110;&#31181;&#26063;&#24046;&#24322;&#65289;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#29702;&#35770;&#30340;&#23454;&#35777;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how to optimally regulate prediction algorithms in a world where an agent uses complex 'black-box' prediction functions to make decisions such as lending, medical testing, or hiring, and where a principal is limited in how much she can learn about the agent's black-box model. We show that limiting agents to prediction functions that are simple enough to be fully transparent is inefficient as long as the misalignment is limited and first-best prediction functions are sufficiently complex. Algorithmic audits can improve welfare, but the gains depend on the design of the audit tools. Tools that focus on minimizing overall information loss, the focus of many explainer tools, will generally be inefficient since they focus on explaining the average behavior of the prediction function. Targeted tools that focus on the source of incentive misalignment, e.g., excess false positives or racial disparities, can provide second-best solutions. We provide empirical support for our theoretical
&lt;/p&gt;</description></item></channel></rss>