<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#33021;&#22815;&#20197;&#30495;&#23454;&#12289;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#30340;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.16668</link><description>&lt;p&gt;
RealFill&#65306;&#21442;&#32771;&#39537;&#21160;&#30340;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RealFill: Reference-Driven Generation for Authentic Image Completion. (arXiv:2309.16668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16668
&lt;/p&gt;
&lt;p&gt;
RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#33021;&#22815;&#20197;&#30495;&#23454;&#12289;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#22270;&#20687;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#33021;&#22815;&#22312;&#26410;&#30693;&#21306;&#22495;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#22270;&#20687;&#20869;&#23481;&#30340;&#22806;&#25299;&#21644;&#20462;&#22635;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#20869;&#23481;&#26159;&#19981;&#30495;&#23454;&#30340;&#65292;&#22240;&#20026;&#27169;&#22411;&#32570;&#20047;&#20851;&#20110;&#30495;&#23454;&#22330;&#26223;&#30340;&#36275;&#22815;&#32972;&#26223;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#29983;&#25104;&#26041;&#27861;RealFill&#65292;&#23427;&#36890;&#36807;&#22635;&#20805;&#22270;&#20687;&#20013;&#32570;&#22833;&#21306;&#22495;&#20351;&#20854;&#20869;&#23481;&#30495;&#27491;&#24212;&#22312;&#30340;&#20869;&#23481;&#12290;RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#20960;&#24352;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#20123;&#21442;&#32771;&#22270;&#20687;&#19981;&#38656;&#35201;&#19982;&#30446;&#26631;&#22270;&#20687;&#23545;&#40784;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#35270;&#35282;&#12289;&#20809;&#29031;&#26465;&#20214;&#12289;&#25668;&#20687;&#26426;&#20809;&#22280;&#25110;&#22270;&#20687;&#39118;&#26684;&#25293;&#25668;&#12290;&#20010;&#24615;&#21270;&#21518;&#65292;RealFill&#33021;&#22815;&#20197;&#35270;&#35273;&#19978;&#24341;&#20154;&#27880;&#30446;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#19988;&#24544;&#23454;&#20110;&#21407;&#22987;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#38754;&#19988;&#20855;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#20462;&#22797;&#22522;&#20934;&#19978;&#23545;RealFill&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions, but the content these models hallucinate is necessarily inauthentic, since the models lack sufficient context about the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging
&lt;/p&gt;</description></item><item><title>SA2-Net&#26159;&#19968;&#31181;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;&#22810;&#26679;&#32467;&#26500;&#12290;&#23427;&#32467;&#21512;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#23398;&#20064;&#21644;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2309.16661</link><description>&lt;p&gt;
SA2-Net: &#29992;&#20110;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#30340;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation. (arXiv:2309.16661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16661
&lt;/p&gt;
&lt;p&gt;
SA2-Net&#26159;&#19968;&#31181;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;&#22810;&#26679;&#32467;&#26500;&#12290;&#23427;&#32467;&#21512;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#23398;&#20064;&#21644;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#20026;&#32473;&#23450;&#30340;&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#37197;&#35821;&#20041;&#26631;&#31614;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#26159;&#35768;&#22810;&#29616;&#26377;&#26694;&#26550;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#26126;&#30830;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#21464;&#21387;&#22120;&#26368;&#21021;&#26159;&#20026;&#20102;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#26174;&#24494;&#22270;&#20687;&#20013;&#65292;&#21253;&#25324;&#24418;&#29366;&#12289;&#22823;&#23567;&#12289;&#22806;&#35266;&#21644;&#30446;&#26631;&#21306;&#22495;&#23494;&#24230;&#30340;&#21508;&#31181;&#25361;&#25112;&#20013;&#65292;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SA2-Net&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#23398;&#20064;&#26469;&#26377;&#25928;&#22788;&#29702;&#26174;&#24494;&#22270;&#20687;&#20013;&#22810;&#26679;&#32467;&#26500;&#30340;&#27880;&#24847;&#24341;&#23548;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#65288;SA2&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#25429;&#25417;&#26174;&#24494;&#21306;&#22495;&#65288;&#22914;&#32454;&#32990;&#65289;&#23610;&#24230;&#21644;&#24418;&#29366;&#30340;&#22266;&#26377;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;&#36825;&#20010;&#27169;&#22359;&#32467;&#21512;&#20102;&#23616;&#37096;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Microscopic image segmentation is a challenging task, wherein the objective is to assign semantic labels to each pixel in a given microscopic image. While convolutional neural networks (CNNs) form the foundation of many existing frameworks, they often struggle to explicitly capture long-range dependencies. Although transformers were initially devised to address this issue using self-attention, it has been proven that both local and global features are crucial for addressing diverse challenges in microscopic images, including variations in shape, size, appearance, and target region density. In this paper, we introduce SA2-Net, an attention-guided method that leverages multi-scale feature learning to effectively handle diverse structures within microscopic images. Specifically, we propose scale-aware attention (SA2) module designed to capture inherent variations in scales and shapes of microscopic regions, such as cells, for accurate segmentation. This module incorporates local attention
&lt;/p&gt;</description></item><item><title>MindShift&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#36866;&#24212;&#29992;&#25143;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#39640;&#36136;&#37327;&#35828;&#26381;&#20869;&#23481;&#26469;&#24110;&#21161;&#29992;&#25143;&#35299;&#20915;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2309.16639</link><description>&lt;p&gt;
MindShift: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention. (arXiv:2309.16639v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16639
&lt;/p&gt;
&lt;p&gt;
MindShift&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#36866;&#24212;&#29992;&#25143;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#39640;&#36136;&#37327;&#35828;&#26381;&#20869;&#23481;&#26469;&#24110;&#21161;&#29992;&#25143;&#35299;&#20915;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#23545;&#36523;&#20307;&#21644;&#24515;&#29702;&#20581;&#24247;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#20808;&#21069;&#30740;&#31350;&#65292;&#29616;&#26377;&#30340;&#35828;&#26381;&#25216;&#24039;&#19981;&#36275;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#36523;&#20307;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#25552;&#20379;&#21160;&#24577;&#35828;&#26381;&#20869;&#23481;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#20026;&#25805;&#20316;&#30740;&#31350;&#65288;N = 12&#65289;&#21644;&#19968;&#39033;&#35775;&#35848;&#30740;&#31350;&#65288;N = 10&#65289;&#65292;&#24635;&#32467;&#20102;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#32972;&#21518;&#30340;&#24515;&#24577;&#65306;&#26080;&#32842;&#12289;&#21387;&#21147;&#21644;&#24815;&#24615;&#12290;&#36825;&#20026;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#31181;&#35828;&#26381;&#31574;&#30053;&#65306;&#29702;&#35299;&#12289;&#23433;&#25242;&#12289;&#21796;&#36215;&#21644;&#25903;&#25345;&#20064;&#24815;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#20102;&#26377;&#25928;&#35828;&#26381;&#20869;&#23481;&#30340;&#33258;&#21160;&#21644;&#21160;&#24577;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#25216;&#26415;&#39537;&#21160;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#25216;&#26415;MindShift&#12290;MindShift&#26681;&#25454;&#29992;&#25143;&#24403;&#19979;&#30340;&#36523;&#20307;&#29615;&#22659;&#12289;&#24515;&#24577;&#12289;&#24212;&#29992;&#20351;&#29992;&#34892;&#20026;&#12289;&#29992;&#25143;&#30340;&#30446;&#26631;&#19982;&#20064;&#24815;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#36866;&#24403;&#35828;&#26381;&#31574;&#30053;&#30340;&#39640;&#36136;&#37327;&#21644;&#28789;&#27963;&#30340;&#35828;&#26381;&#20869;&#23481;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;5-
&lt;/p&gt;
&lt;p&gt;
Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users' physical contexts and mental states. We first conduct a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leverage large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We develop MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users' in-the-moment physical contexts, mental states, app usage behaviors, users' goals &amp; habits as input, and generates high-quality and flexible persuasive content with appropriate persuasion strategies. We conduct a 5-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16633</link><description>&lt;p&gt;
&#28151;&#21512;&#20320;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;
&lt;/p&gt;
&lt;p&gt;
Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#22238;&#24402;&#38382;&#39064;&#20256;&#32479;&#19978;&#27604;&#20998;&#31867;&#38382;&#39064;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#30452;&#25509;&#24212;&#29992;&#20026;&#20998;&#31867;&#35774;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21040;&#22238;&#24402;&#38382;&#39064;&#24448;&#24448;&#20250;&#23548;&#33268;&#28508;&#31354;&#38388;&#20013;&#30862;&#29255;&#21270;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#30001;&#20110;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#24207;&#24207;&#24863;&#30693;&#21644;&#38590;&#24230;&#65292;&#23545;&#20110;&#22238;&#24402;&#38382;&#39064;&#32780;&#35328;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#28508;&#33021;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#8220;&#28151;&#21512;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;&#36827;&#34892;&#30417;&#30563;&#24615;&#23545;&#27604;&#22238;&#24402;&#8221;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#38752;&#30495;&#23454;/&#22686;&#24378;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#24335;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#23398;&#20064;&#65288;SupReMix&#65289;&#12290;&#23427;&#22312;&#23884;&#20837;&#32423;&#21035;&#19978;&#20197;&#38170;&#28857;&#21253;&#21547;&#30340;&#28151;&#21512;&#65288;&#38170;&#28857;&#21644;&#19968;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#36127;&#23545;&#65292;&#20197;&#38170;&#28857;&#25490;&#38500;&#30340;&#28151;&#21512;&#65288;&#20004;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#27491;&#23545;&#12290;&#36825;&#19968;&#31574;&#30053;&#24418;&#25104;&#20102;&#22256;&#38590;&#26679;&#26412;&#23545;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In representation learning, regression has traditionally received less attention than classification. Directly applying representation learning techniques designed for classification to regression often results in fragmented representations in the latent space, yielding sub-optimal performance. In this paper, we argue that the potential of contrastive learning for regression has been overshadowed due to the neglect of two crucial aspects: ordinality-awareness and hardness. To address these challenges, we advocate "mixup your own contrastive pairs for supervised contrastive regression", instead of relying solely on real/augmented samples. Specifically, we propose Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample) as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct negative samples) as hard positive pairs at the embedding level. This strategy formulates harde
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21387;&#21147;&#27979;&#35797;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38142;&#24335;&#24605;&#36335;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#20934;&#30830;&#30340;CoT&#20540;&#23545;&#20110;&#39044;&#27979;&#27491;&#30830;&#31572;&#26696;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#38169;&#35823;&#30340;CoT&#25552;&#31034;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;CoT&#36816;&#31639;&#31526;&#25110;CoT&#39034;&#24207;&#38169;&#35823;&#30340;&#19981;&#27491;&#30830;&#28436;&#31034;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;&#36825;&#39033;&#30740;&#31350;&#21152;&#28145;&#20102;&#23545;CoT&#25552;&#31034;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;LLM&#23398;&#20064;&#19978;&#19979;&#25991;&#25512;&#29702;&#33021;&#21147;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38142;&#24335;&#24605;&#36335;&#25552;&#31034;&#30340;&#21387;&#21147;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Stress Testing Chain-of-Thought Prompting for Large Language Models. (arXiv:2309.16621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21387;&#21147;&#27979;&#35797;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38142;&#24335;&#24605;&#36335;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#20934;&#30830;&#30340;CoT&#20540;&#23545;&#20110;&#39044;&#27979;&#27491;&#30830;&#31572;&#26696;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#38169;&#35823;&#30340;CoT&#25552;&#31034;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;CoT&#36816;&#31639;&#31526;&#25110;CoT&#39034;&#24207;&#38169;&#35823;&#30340;&#19981;&#27491;&#30830;&#28436;&#31034;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;&#36825;&#39033;&#30740;&#31350;&#21152;&#28145;&#20102;&#23545;CoT&#25552;&#31034;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;LLM&#23398;&#20064;&#19978;&#19979;&#25991;&#25512;&#29702;&#33021;&#21147;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#30740;&#31350;&#20102;&#38142;&#24335;&#24605;&#36335;&#25552;&#31034;&#65288;CoT&#65289;&#22312;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#21463;&#21040;&#20043;&#21069;&#30340;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;CoT&#25552;&#31034;&#30340;&#19977;&#31181;&#31867;&#22411;&#30340;&#25200;&#21160;&#65288;&#21363;CoT&#39034;&#24207;&#65292;CoT&#20540;&#21644;CoT&#36816;&#31639;&#31526;&#65289;&#23545;GPT-3&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#38169;&#35823;&#30340;CoT&#25552;&#31034;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25351;&#26631;&#19979;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#27491;&#30830;&#30340;CoT&#20540;&#23545;&#20110;&#39044;&#27979;&#27491;&#30830;&#31572;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#24403;&#19982;&#22522;&#20110;&#20540;&#30340;&#25200;&#21160;&#30456;&#27604;&#26102;&#65292;CoT&#36816;&#31639;&#31526;&#25110;CoT&#39034;&#24207;&#38169;&#35823;&#30340;&#19981;&#27491;&#30830;&#28436;&#31034;&#24182;&#27809;&#26377;&#22914;&#27492;&#21095;&#28872;&#22320;&#24433;&#21709;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#21152;&#28145;&#20102;&#25105;&#20204;&#23545;CoT&#25552;&#31034;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#20110;LLM&#23398;&#20064;&#19978;&#19979;&#25991;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report examines the effectiveness of Chain-of-Thought (CoT) prompting in improving the multi-step reasoning abilities of large language models (LLMs). Inspired by previous studies \cite{Min2022RethinkingWork}, we analyze the impact of three types of CoT prompt perturbations, namely CoT order, CoT values, and CoT operators on the performance of GPT-3 on various tasks. Our findings show that incorrect CoT prompting leads to poor performance on accuracy metrics. Correct values in the CoT is crucial for predicting correct answers. Moreover, incorrect demonstrations, where the CoT operators or the CoT order are wrong, do not affect the performance as drastically when compared to the value based perturbations. This research deepens our understanding of CoT prompting and opens some new questions regarding the capability of LLMs to learn reasoning in context.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#30340;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2309.16620</link><description>&lt;p&gt;
&#27531;&#24046;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#36229;&#21442;&#25968;&#36716;&#31227;&#65306;&#21160;&#24577;&#21644;&#32553;&#25918;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit. (arXiv:2309.16620v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16620
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#30340;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#20419;&#20351;&#20174;&#19994;&#32773;&#23547;&#25214;&#20351;&#29992;&#36739;&#23567;&#32593;&#32476;&#30340;&#20195;&#29702;&#26041;&#27861;&#36827;&#34892;&#35843;&#25972;&#12290;&#20854;&#20013;&#19968;&#20010;&#24314;&#35758;&#20351;&#29992;$\mu$P&#21442;&#25968;&#21270;&#32593;&#32476;&#65292;&#20854;&#20013;&#23567;&#23485;&#24230;&#32593;&#32476;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#36716;&#31227;&#21040;&#20219;&#24847;&#23485;&#24230;&#30340;&#32593;&#32476;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26041;&#26696;&#20013;&#65292;&#36229;&#21442;&#25968;&#19981;&#20250;&#22312;&#19981;&#21516;&#28145;&#24230;&#20043;&#38388;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;$1/\sqrt{\text{depth}}$&#30340;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#21442;&#25968;&#21270;&#35757;&#32451;&#30340;&#27531;&#24046;&#32467;&#26500;&#65292;&#21253;&#25324;&#21367;&#31215;ResNet&#21644;Vision Transformer&#65292;&#22312;CIFAR-10&#21644;ImageNet&#19978;&#23637;&#31034;&#20102;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#21457;&#29616;&#24471;&#21040;&#20102;&#29702;&#35770;&#30340;&#25903;&#25345;&#21644;&#21160;&#26426;&#12290;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#25551;&#36848;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
The cost of hyperparameter tuning in deep learning has been rising with model sizes, prompting practitioners to find new tuning methods using a proxy of smaller networks. One such proposal uses $\mu$P parameterized networks, where the optimal hyperparameters for small width networks transfer to networks with arbitrarily large width. However, in this scheme, hyperparameters do not transfer across depths. As a remedy, we study residual networks with a residual branch scale of $1/\sqrt{\text{depth}}$ in combination with the $\mu$P parameterization. We provide experiments demonstrating that residual architectures including convolutional ResNets and Vision Transformers trained with this parameterization exhibit transfer of optimal hyperparameters across width and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findings are supported and motivated by theory. Using recent developments in the dynamical mean field theory (DMFT) description of neural network learning dynamics, we show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#31070;&#32463;&#31243;&#24207;&#24179;&#28369;&#65288;NPS&#65289;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#36827;&#34892;&#20102;&#26368;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#21407;&#22987;&#24615;&#33021;&#22768;&#26126;&#19981;&#25104;&#31435;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#23454;&#38469;&#38480;&#21046;&#38382;&#39064;&#30340;&#26032;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;Neuzz++&#12290;</title><link>http://arxiv.org/abs/2309.16618</link><description>&lt;p&gt;
&#37325;&#35775;&#31070;&#32463;&#31243;&#24207;&#24179;&#28369;&#25216;&#26415;&#29992;&#20110;&#27169;&#31946;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Revisiting Neural Program Smoothing for Fuzzing. (arXiv:2309.16618v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31070;&#32463;&#31243;&#24207;&#24179;&#28369;&#65288;NPS&#65289;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#36827;&#34892;&#20102;&#26368;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#21407;&#22987;&#24615;&#33021;&#22768;&#26126;&#19981;&#25104;&#31435;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#23454;&#38469;&#38480;&#21046;&#38382;&#39064;&#30340;&#26032;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;Neuzz++&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29983;&#25104;&#36755;&#20837;&#36827;&#34892;&#27979;&#35797;&#65288;&#27169;&#31946;&#27979;&#35797;&#65289;&#30001;&#20110;&#33021;&#22815;&#33258;&#21160;&#26292;&#38706;&#31243;&#24207;&#30340;&#28431;&#27934;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#27169;&#31946;&#27979;&#35797;&#29983;&#25104;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#31070;&#32463;&#31243;&#24207;&#24179;&#28369;&#65288;NPS&#65289;&#26159;&#19968;&#31181;&#29305;&#23450;&#30340;&#22522;&#20110;ML&#30340;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20316;&#31243;&#24207;&#30446;&#26631;&#30340;&#24179;&#28369;&#36817;&#20284;&#65292;&#20197;&#29983;&#25104;&#26032;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;NPS&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#36827;&#34892;&#20102;&#26368;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#24182;&#19982;&#26631;&#20934;&#30340;&#28784;&#30418;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65288;&gt;11&#20010;CPU&#24180;&#21644;&gt;5.5&#20010;GPU&#24180;&#65289;&#65292;&#24182;&#20570;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;(1)&#25105;&#20204;&#21457;&#29616;NPS&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#30340;&#21407;&#22987;&#24615;&#33021;&#22768;&#26126;&#19981;&#25104;&#31435;&#65307;&#25105;&#20204;&#23558;&#36825;&#20010;&#24046;&#36317;&#19982;&#20808;&#21069;&#24037;&#20316;&#30340;&#22522;&#26412;&#12289;&#23454;&#26045;&#21644;&#23454;&#39564;&#38480;&#21046;&#30456;&#20851;&#32852;&#12290;(2)&#25105;&#20204;&#39318;&#27425;&#28145;&#20837;&#20998;&#26512;&#20102;ML&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#21464;&#24322;&#22312;NPS&#20013;&#30340;&#36129;&#29486;&#12290;(3)&#25105;&#20204;&#23454;&#29616;&#20102;&#26032;&#30340;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;Neuzz++&#65292;&#23427;&#35299;&#20915;&#20102;&#23454;&#38469;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Testing with randomly generated inputs (fuzzing) has gained significant traction due to its capacity to expose program vulnerabilities automatically. Fuzz testing campaigns generate large amounts of data, making them ideal for the application of machine learning (ML). Neural program smoothing (NPS), a specific family of ML-guided fuzzers, aims to use a neural network as a smooth approximation of the program target for new test case generation.  In this paper, we conduct the most extensive evaluation of NPS fuzzers against standard gray-box fuzzers (&gt;11 CPU years and &gt;5.5 GPU years), and make the following contributions: (1) We find that the original performance claims for NPS fuzzers do not hold; a gap we relate to fundamental, implementation, and experimental limitations of prior works. (2) We contribute the first in-depth analysis of the contribution of machine learning and gradient-based mutations in NPS. (3) We implement Neuzz++, which shows that addressing the practical limitation
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#27491;&#38754;&#36824;&#26159;&#36127;&#38754;&#25551;&#36848;AI&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#37117;&#19981;&#23384;&#22312;AI&#26102;&#65292;&#21442;&#19982;&#32773;&#30340;&#26399;&#26395;&#20540;&#37117;&#24456;&#39640;&#65292;&#24182;&#19988;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;AI&#30340;&#34920;&#29616;&#26399;&#26395;&#22312;&#36127;&#38754;&#25551;&#36848;&#19979;&#26159;&#26377;&#20559;&#24046;&#19988;&#31283;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.16606</link><description>&lt;p&gt;
&#8220;AI&#22686;&#24378;&#25105;&#20204;&#30340;&#34920;&#29616;&#65292;&#25105;&#27627;&#19981;&#24576;&#30097;&#36825;&#19968;&#31687;&#35770;&#25991;&#20063;&#20250;&#20570;&#21040;&#21516;&#26679;&#30340;&#20107;&#24773;&#8221;&#65306;&#23433;&#24944;&#21058;&#25928;&#24212;&#23545;AI&#36127;&#38754;&#25551;&#36848;&#26377;&#19968;&#23450;&#30340;&#24433;&#21709;(arXiv:2309.16606v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
"AI enhances our performance, I have no doubt this one will do the same": The Placebo effect is robust to negative descriptions of AI. (arXiv:2309.16606v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#27491;&#38754;&#36824;&#26159;&#36127;&#38754;&#25551;&#36848;AI&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#37117;&#19981;&#23384;&#22312;AI&#26102;&#65292;&#21442;&#19982;&#32773;&#30340;&#26399;&#26395;&#20540;&#37117;&#24456;&#39640;&#65292;&#24182;&#19988;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;AI&#30340;&#34920;&#29616;&#26399;&#26395;&#22312;&#36127;&#38754;&#25551;&#36848;&#19979;&#26159;&#26377;&#20559;&#24046;&#19988;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#26356;&#39640;&#30340;&#26399;&#26395;&#36890;&#36807;&#23433;&#24944;&#21058;&#25928;&#24212;&#20419;&#36827;&#20102;&#20154;&#26426;&#20132;&#20114;&#30340;&#34920;&#29616;&#12290;&#38477;&#20302;&#26399;&#26395;&#26469;&#25511;&#21046;&#23433;&#24944;&#21058;&#25928;&#24212;&#26159;&#26126;&#26234;&#30340;&#65292;&#20294;&#36807;&#20110;&#36127;&#38754;&#30340;&#26399;&#26395;&#21487;&#33021;&#23548;&#33268;&#36127;&#23433;&#24944;&#21058;&#25928;&#24212;&#12290;&#22312;&#19968;&#20010;&#23383;&#27597;&#36776;&#35782;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21578;&#30693;&#21442;&#19982;&#32773;AI&#20250;&#36890;&#36807;&#35843;&#25972;&#30028;&#38754;&#26469;&#22686;&#24378;&#25110;&#38477;&#20302;&#20182;&#20204;&#30340;&#34920;&#29616;&#65292;&#20294;&#23454;&#38469;&#19978;&#65292;&#22312;&#20219;&#20309;&#26465;&#20214;&#19979;&#37117;&#27809;&#26377;AI&#23384;&#22312;&#12290;&#36125;&#21494;&#26031;&#20998;&#26512;&#26174;&#31034;&#65292;&#21442;&#19982;&#32773;&#26399;&#26395;&#20540;&#24456;&#39640;&#65292;&#24182;&#19988;&#19981;&#31649;AI&#30340;&#25551;&#36848;&#22914;&#20309;&#65292;&#24403;&#19968;&#20010;&#34394;&#20551;&#30340;AI&#23384;&#22312;&#26102;&#65292;&#20182;&#20204;&#30340;&#34920;&#29616;&#37117;&#26356;&#22909;&#12290;&#36890;&#36807;&#35748;&#30693;&#24314;&#27169;&#65292;&#25105;&#20204;&#21487;&#20197;&#36861;&#28335;&#21040;&#21442;&#19982;&#32773;&#25910;&#38598;&#26356;&#22810;&#20449;&#24687;&#20174;&#32780;&#33719;&#24471;&#20248;&#21183;&#12290;&#19968;&#39033;&#22797;&#21046;&#30740;&#31350;&#39564;&#35777;&#20102;&#36127;&#38754;AI&#25551;&#36848;&#19981;&#20250;&#25913;&#21464;&#26399;&#26395;&#20540;&#65292;&#36825;&#34920;&#26126;AI&#30340;&#34920;&#29616;&#26399;&#26395;&#22312;&#36127;&#38754;&#35328;&#35821;&#25551;&#36848;&#19979;&#26159;&#26377;&#20559;&#24046;&#19988;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#29992;&#25143;&#26399;&#26395;&#23545;&#20110;AI&#20132;&#20114;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#34892;&#20026;&#23433;&#24944;&#21058;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heightened AI expectations facilitate performance in human-AI interactions through placebo effects. While lowering expectations to control for placebo effects is advisable, overly negative expectations could induce nocebo effects. In a letter discrimination task, we informed participants that an AI would either increase or decrease their performance by adapting the interface, but in reality, no AI was present in any condition. A Bayesian analysis showed that participants had high expectations and performed descriptively better irrespective of the AI description when a sham-AI was present. Using cognitive modeling, we could trace this advantage back to participants gathering more information. A replication study verified that negative AI descriptions do not alter expectations, suggesting that performance expectations with AI are biased and robust to negative verbal descriptions. We discuss the impact of user expectations on AI interactions and evaluation and provide a behavioral placebo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MPHD&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#23454;&#29616;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16597</link><description>&lt;p&gt;
&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces. (arXiv:2309.16597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MPHD&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#23454;&#29616;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#65288;&#36890;&#24120;&#26159;&#39640;&#26031;&#36807;&#31243;&#65289;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#8220;&#35757;&#32451;&#8221;&#20989;&#25968;&#30340;&#35266;&#23519;&#32467;&#26524;&#26469;&#33258;&#21160;&#35774;&#35745;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#12290;&#36825;&#20123;&#35757;&#32451;&#20989;&#25968;&#36890;&#24120;&#38656;&#35201;&#19982;&#8220;&#27979;&#35797;&#8221;&#20989;&#25968;&#65288;&#24453;&#20248;&#21270;&#30340;&#40657;&#30418;&#20989;&#25968;&#65289;&#20855;&#26377;&#30456;&#21516;&#30340;&#23450;&#20041;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MPHD&#30340;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#26144;&#23556;&#21040;&#20998;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#35268;&#33539;&#12290;MPHD&#21487;&#20197;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#26080;&#32541;&#38598;&#25104;&#65292;&#23454;&#29616;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on "training" functions. These training functions are typically required to have the same domain as the "test" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.16595</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#26377;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#22270;&#23398;&#20064;&#65306;&#20309;&#26102;&#20309;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#22270;&#25968;&#25454;&#65289;&#19978;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;LLM&#25991;&#29486;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#37325;&#35201;&#25968;&#25454;&#24418;&#24577;&#12290;&#25105;&#20204;&#26088;&#22312;&#20102;&#35299;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20309;&#26102;&#20309;&#22320;&#24341;&#20837;&#22270;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#8220;&#20309;&#26102;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#35774;&#32622;&#20013;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#20016;&#23500;&#25110;&#31232;&#32570;&#12290;&#23545;&#20110;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#24615;&#33021;&#30340;&#20004;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#25968;&#25454;&#27844;&#38706;&#21644;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65307;&#65288;ii&#65289;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#35777;&#25454;&#34920;&#26126;LLM&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#26377;&#26174;&#33879;&#30456;&#20851;&#65307;&#65288;iii&#65289;LLM&#22312;&#30446;&#26631;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#19982;&#27491;&#21521;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively relat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#30693;&#35782;&#22270;&#35889;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24433;&#21709;&#20197;&#21450;&#24320;&#21457;&#21487;&#35299;&#37322;&#24615;AI&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#12290;&#24378;&#35843;&#20102;&#36890;&#36807;&#30693;&#35782;&#27880;&#20837;&#23398;&#20064;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.16593</link><description>&lt;p&gt;
&#23548;&#33322;&#21307;&#30103;&#27934;&#35265;&#65306;&#30693;&#35782;&#22270;&#35889;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#40479;&#30640;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Navigating Healthcare Insights: A Birds Eye View of Explainability with Knowledge Graphs. (arXiv:2309.16593v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#30693;&#35782;&#22270;&#35889;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24433;&#21709;&#20197;&#21450;&#24320;&#21457;&#21487;&#35299;&#37322;&#24615;AI&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#12290;&#24378;&#35843;&#20102;&#36890;&#36807;&#30693;&#35782;&#27880;&#20837;&#23398;&#20064;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#26085;&#30410;&#21463;&#21040;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#21046;&#33647;&#30740;&#31350;&#20013;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#26469;&#25972;&#21512;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#28304;&#65292;&#22686;&#24378;&#20102;AI&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#36825;&#31181;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#20449;&#20219;&#21644;&#36879;&#26126;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#25903;&#25345;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#20915;&#31574;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24433;&#21709;&#21450;&#20854;&#22312;&#24320;&#21457;&#21487;&#35299;&#37322;&#24615;AI&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#30340;&#26368;&#26032;&#25991;&#29486;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#26500;&#24314;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#25512;&#29702;&#20197;&#21450;&#23427;&#20204;&#22312;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#12289;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#12289;&#33647;&#29289;&#24320;&#21457;&#12289;&#33647;&#29289;&#19981;&#33391;&#21453;&#24212;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24378;&#35843;&#36890;&#36807;&#22312;&#21307;&#30103;&#20013;&#36827;&#34892;&#30693;&#35782;&#27880;&#20837;&#23398;&#20064;&#26469;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31361;&#20986;&#30740;&#31350;&#25361;&#25112;&#24182;&#25552;&#20379;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are gaining prominence in Healthcare AI, especially in drug discovery and pharmaceutical research as they provide a structured way to integrate diverse information sources, enhancing AI system interpretability. This interpretability is crucial in healthcare, where trust and transparency matter, and eXplainable AI (XAI) supports decision making for healthcare professionals. This overview summarizes recent literature on the impact of KGs in healthcare and their role in developing explainable AI models. We cover KG workflow, including construction, relationship extraction, reasoning, and their applications in areas like Drug-Drug Interactions (DDI), Drug Target Interactions (DTI), Drug Development (DD), Adverse Drug Reactions (ADR), and bioinformatics. We emphasize the importance of making KGs more interpretable through knowledge-infused learning in healthcare. Finally, we highlight research challenges and provide insights for future directions.
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#20316;&#20026;&#19987;&#26377;&#31995;&#32479;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#35780;&#20272;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;ARRT&#65288;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65289;&#25361;&#25112;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#20197;&#21450;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.16573</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#30340;ARRT: &#26032;&#33539;&#24335;&#21450;&#20854;&#25361;&#25112;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges. (arXiv:2309.16573v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16573
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#20316;&#20026;&#19987;&#26377;&#31995;&#32479;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#35780;&#20272;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;ARRT&#65288;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65289;&#25361;&#25112;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#20197;&#21450;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#19968;&#20123;&#26368;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#26159;&#19987;&#26377;&#31995;&#32479;&#65292;&#21482;&#33021;&#36890;&#36807;&#65288;&#36890;&#24120;&#26159;&#38480;&#21046;&#24615;&#30340;&#65289;&#32593;&#32476;&#25110;&#36719;&#20214;&#32534;&#31243;&#25509;&#21475;&#35775;&#38382;&#12290;&#36825;&#23601;&#26159;&#35821;&#35328;&#27169;&#22411;&#21363;&#26381;&#21153;&#65288;LMaaS&#65289;&#30340;&#33539;&#24335;&#12290;&#19982;&#21487;&#20197;&#23436;&#20840;&#35775;&#38382;&#27169;&#22411;&#30340;&#24773;&#20917;&#30456;&#21453;&#65292;&#22914;&#24320;&#25918;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#23553;&#38381;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35780;&#20272;&#12289;&#22522;&#20934;&#27979;&#35797;&#21644;&#27979;&#35797;&#36896;&#25104;&#20102;&#29305;&#23450;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30028;&#23450;&#21069;&#36848;&#25361;&#25112;&#22914;&#20309;&#20316;&#20026;&#23545;LMaaS&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65288;ARRT&#65289;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19982;&#27599;&#20010;&#36825;&#22235;&#20010;&#26041;&#38754;&#30340;&#35821;&#35328;&#27169;&#22411;&#20449;&#24687;&#19981;&#36275;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#24314;&#35758;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26412;&#25991;&#26159;&#24403;&#21069;&#20027;&#35201;LMaaS&#29616;&#26377;&#30693;&#35782;&#30340;&#19968;&#31449;&#24335;&#38598;&#38182;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some of the most powerful language models currently are proprietary systems, accessible only via (typically restrictive) web or software programming interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm. Contrasting with scenarios where full model access is available, as in the case of open-source models, such closed-off language models create specific challenges for evaluating, benchmarking, and testing them. This paper has two goals: on the one hand, we delineate how the aforementioned challenges act as impediments to the accessibility, replicability, reliability, and trustworthiness (ARRT) of LMaaS. We systematically examine the issues that arise from a lack of information about language models for each of these four aspects. We shed light on current solutions, provide some recommendations, and highlight the directions for future advancements. On the other hand, it serves as a one-stop-shop for the extant knowledge about current, major LMaaS, offering a synthesized o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#24182;&#21033;&#29992;&#20445;&#25345;&#35821;&#20041;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21019;&#24314;&#20102;&#35299;&#37322;&#24615;&#23884;&#20837;&#65292;&#24182;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#39046;&#22495;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.16564</link><description>&lt;p&gt;
&#22686;&#24378;&#35299;&#37322;&#24615;: &#26080;&#30417;&#30563;&#30340;&#21644;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings. (arXiv:2309.16564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#24182;&#21033;&#29992;&#20445;&#25345;&#35821;&#20041;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21019;&#24314;&#20102;&#35299;&#37322;&#24615;&#23884;&#20837;&#65292;&#24182;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#39046;&#22495;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#21487;&#29992;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#21019;&#24314;&#21487;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#20856;&#22411;&#32570;&#20047;&#35299;&#37322;&#24615;&#24050;&#25104;&#20026;&#26368;&#36817;&#36879;&#26126;&#20154;&#24037;&#26234;&#33021;&#27861;&#35268;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#20064;&#20445;&#25345;&#35821;&#20041;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21517;&#20026;INGENIOUS&#65292;&#21019;&#24314;&#20102;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#65292;&#24182;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#21518;&#32493;&#20998;&#26512;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#38024;&#23545;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#39046;&#22495;&#32570;&#20047;&#24418;&#24335;&#21270;&#21644;&#24230;&#37327;&#30340;&#39069;&#22806;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36890;&#36807;&#24212;&#29992;&#20110;&#22270;&#32423;&#21644;&#33410;&#28857;&#32423;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning allows us to leverage unlabelled data, which has become abundantly available, and to create embeddings that are usable on a variety of downstream tasks. However, the typical lack of interpretability of unsupervised representation learning has become a limiting factor with regard to recent transparent-AI regulations. In this paper, we study graph representation learning and we show that data augmentation that preserves semantics can be learned and used to produce interpretations. Our framework, which we named INGENIOUS, creates inherently interpretable embeddings and eliminates the need for costly additional post-hoc analysis. We also introduce additional metrics addressing the lack of formalism and metrics in the understudied area of unsupervised-representation learning interpretability. Our results are supported by an experimental study applied to both graph-level and node-level tasks and show that interpretable embeddings provide state-of-the-art performance on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25237;&#31080;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#39640;&#20998;&#36776;&#29575;&#33322;&#31354;&#24433;&#20687;&#20013;&#20998;&#21106;&#21644;&#20998;&#31867;&#31561;&#39640;&#22564;&#20892;&#30000;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#31080;&#26426;&#21046;&#26469;&#20943;&#23569;&#36793;&#30028;&#25197;&#26354;&#21644;&#31867;&#21035;&#28151;&#28102;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16561</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#32593;&#32476;&#30340;&#31561;&#39640;&#22564;&#20892;&#30000;&#20998;&#21106;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Voting Network for Contour Levee Farmland Segmentation and Classification. (arXiv:2309.16561v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25237;&#31080;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#39640;&#20998;&#36776;&#29575;&#33322;&#31354;&#24433;&#20687;&#20013;&#20998;&#21106;&#21644;&#20998;&#31867;&#31561;&#39640;&#22564;&#20892;&#30000;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#31080;&#26426;&#21046;&#26469;&#20943;&#23569;&#36793;&#30028;&#25197;&#26354;&#21644;&#31867;&#21035;&#28151;&#28102;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#33322;&#31354;&#24433;&#20687;&#20801;&#35768;&#22312;&#20892;&#30000;&#20998;&#21106;&#20013;&#33719;&#21462;&#32454;&#33410;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23567;&#29289;&#20307;&#21644;&#29305;&#24449;&#20250;&#23548;&#33268;&#29289;&#20307;&#36793;&#30028;&#30340;&#25197;&#26354;&#65292;&#38656;&#35201;&#26356;&#22823;&#30340;&#19978;&#19979;&#25991;&#35270;&#22270;&#26469;&#20943;&#23569;&#31867;&#21035;&#28151;&#28102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#39640;&#20998;&#36776;&#29575;&#33322;&#31354;&#24433;&#20687;&#20013;&#20998;&#21106;&#31561;&#39640;&#22564;&#20892;&#30000;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#34701;&#21512;&#22359;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#20010;&#25237;&#31080;&#22359;&#65292;&#20197;&#23454;&#29616;&#22270;&#20687;&#20998;&#21106;&#21644;&#20998;&#31867;&#12290;&#25105;&#20204;&#23558;&#34701;&#21512;&#22359;&#19982;&#39592;&#24178;&#32593;&#32476;&#32467;&#21512;&#65292;&#21516;&#26102;&#29983;&#25104;&#35821;&#20041;&#39044;&#27979;&#21644;&#20998;&#21106;&#29255;&#27573;&#12290;&#20998;&#21106;&#29255;&#27573;&#29992;&#20110;&#22312;&#39044;&#27979;&#19978;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#12290;&#32593;&#32476;&#34987;&#35757;&#32451;&#20026;&#23558;&#27573;&#33853;&#30340;&#26368;&#26377;&#21487;&#33021;&#31867;&#21035;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#20687;&#32032;&#65292;&#20174;&#32780;&#23398;&#20064;&#20892;&#30000;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20998;&#26512;&#20687;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-resolution aerial imagery allows fine details in the segmentation of farmlands. However, small objects and features introduce distortions to the delineation of object boundaries, and larger contextual views are needed to mitigate class confusion. In this work, we present an end-to-end trainable network for segmenting farmlands with contour levees from high-resolution aerial imagery. A fusion block is devised that includes multiple voting blocks to achieve image segmentation and classification. We integrate the fusion block with a backbone and produce both semantic predictions and segmentation slices. The segmentation slices are used to perform majority voting on the predictions. The network is trained to assign the most likely class label of a segment to its pixels, learning the concept of farmlands rather than analyzing constitutive pixels separately. We evaluate our method using images from the National Agriculture Imagery Program. Our method achieved an average accuracy of 94.3
&lt;/p&gt;</description></item><item><title>KLoB&#26159;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#20107;&#23454;&#30693;&#35782;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16535</link><description>&lt;p&gt;
KLoB: &#19968;&#31181;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models. (arXiv:2309.16535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16535
&lt;/p&gt;
&lt;p&gt;
KLoB&#26159;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#20107;&#23454;&#30693;&#35782;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23450;&#20301;&#28982;&#21518;&#32534;&#36753;&#30340;&#33539;&#24335;&#24050;&#32463;&#25104;&#20026;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23450;&#20301;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#22320;&#25214;&#21040;&#23884;&#20837;&#25152;&#38656;&#30693;&#35782;&#30340;&#30830;&#20999;&#21442;&#25968;&#36824;&#32570;&#20047;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#23545;&#20107;&#23454;&#30693;&#35782;&#30340;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#20294;&#27809;&#26377;&#25552;&#20379;&#19968;&#31181;&#27979;&#35797;&#20551;&#35774;&#30340;&#26041;&#27861;&#20197;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#35752;&#35770;&#21644;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KLoB&#65292;&#19968;&#20010;&#35780;&#20272;&#21487;&#38752;&#30340;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#24212;&#28385;&#36275;&#30340;&#19977;&#20010;&#22522;&#26412;&#23646;&#24615;&#30340;&#22522;&#20934;&#12290;KLoB&#21487;&#20316;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#24182;&#20026;&#37325;&#26032;&#35780;&#20272;&#20107;&#23454;&#30693;&#35782;&#30340;&#23616;&#37096;&#24615;&#20551;&#35774;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;\url{https://github.com/juyiming/KLoB}&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches in changing factual knowledge stored in the Language models. However, there is a lack of research on whether present locating methods can pinpoint the exact parameters embedding the desired knowledge. Moreover, although many researchers have questioned the validity of locality hypothesis of factual knowledge, no method is provided to test the a hypothesis for more in-depth discussion and research. Therefore, we introduce KLoB, a benchmark examining three essential properties that a reliable knowledge locating method should satisfy. KLoB can serve as a benchmark for evaluating existing locating methods in language models, and can contributes a method to reassessing the validity of locality hypothesis of factual knowledge. Our is publicly available at \url{https://github.com/juyiming/KLoB}.
&lt;/p&gt;</description></item><item><title>MotionLM&#27169;&#22411;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#38170;&#28857;&#25110;&#26174;&#24335;&#28508;&#22312;&#21464;&#37327;&#20248;&#21270;&#65292;&#33021;&#22815;&#29983;&#25104;&#20851;&#20110;&#20132;&#20114;&#26234;&#33021;&#20307;&#26410;&#26469;&#30340;&#32852;&#21512;&#20998;&#24067;&#21644;&#23454;&#29616;&#26102;&#38388;&#22240;&#26524;&#26465;&#20214;&#23637;&#24320;&#12290;</title><link>http://arxiv.org/abs/2309.16534</link><description>&lt;p&gt;
MotionLM: &#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MotionLM: Multi-Agent Motion Forecasting as Language Modeling. (arXiv:2309.16534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16534
&lt;/p&gt;
&lt;p&gt;
MotionLM&#27169;&#22411;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#38170;&#28857;&#25110;&#26174;&#24335;&#28508;&#22312;&#21464;&#37327;&#20248;&#21270;&#65292;&#33021;&#22815;&#29983;&#25104;&#20851;&#20110;&#20132;&#20114;&#26234;&#33021;&#20307;&#26410;&#26469;&#30340;&#32852;&#21512;&#20998;&#24067;&#21644;&#23454;&#29616;&#26102;&#38388;&#22240;&#26524;&#26465;&#20214;&#23637;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#35268;&#21010;&#20013;&#65292;&#21487;&#38752;&#22320;&#39044;&#27979;&#36947;&#36335;&#19978;&#26234;&#33021;&#20307;&#30340;&#26410;&#26469;&#34892;&#20026;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#36830;&#32493;&#36712;&#36857;&#34920;&#31034;&#20026;&#31163;&#25955;&#36816;&#21160;&#20196;&#29260;&#30340;&#24207;&#21015;&#65292;&#24182;&#23558;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#35270;&#20026;&#23545;&#35813;&#39046;&#22495;&#30340;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MotionLM&#25552;&#20379;&#20102;&#20960;&#20010;&#20248;&#21183;&#65306;&#39318;&#20808;&#65292;&#23427;&#19981;&#38656;&#35201;&#38170;&#28857;&#25110;&#26174;&#24335;&#28508;&#22312;&#21464;&#37327;&#20248;&#21270;&#26469;&#23398;&#20064;&#22810;&#27169;&#24577;&#20998;&#24067;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#21333;&#20010;&#26631;&#20934;&#30340;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#65292;&#26368;&#22823;&#21270;&#24207;&#21015;&#20196;&#29260;&#30340;&#24179;&#22343;&#23545;&#25968;&#27010;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32469;&#36807;&#20107;&#21518;&#20132;&#20114;&#21551;&#21457;&#24335;&#65292;&#20854;&#20013;&#22312;&#20132;&#20114;&#35780;&#20998;&#20043;&#21069;&#36827;&#34892;&#21333;&#20010;&#20195;&#29702;&#36712;&#36857;&#29983;&#25104;&#12290;&#30456;&#21453;&#65292;MotionLM&#22312;&#21333;&#20010;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#29983;&#25104;&#20851;&#20110;&#20132;&#20114;&#20195;&#29702;&#26410;&#26469;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#26102;&#24207;&#22240;&#23376;&#21270;&#20351;&#20854;&#33021;&#22815;&#23454;&#29616;&#26102;&#38388;&#22240;&#26524;&#26465;&#20214;&#23637;&#24320;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable forecasting of the future behavior of road agents is a critical component to safe planning in autonomous vehicles. Here, we represent continuous trajectories as sequences of discrete motion tokens and cast multi-agent motion prediction as a language modeling task over this domain. Our model, MotionLM, provides several advantages: First, it does not require anchors or explicit latent variable optimization to learn multimodal distributions. Instead, we leverage a single standard language modeling objective, maximizing the average log probability over sequence tokens. Second, our approach bypasses post-hoc interaction heuristics where individual agent trajectory generation is conducted prior to interactive scoring. Instead, MotionLM produces joint distributions over interactive agent futures in a single autoregressive decoding process. In addition, the model's sequential factorization enables temporally causal conditional rollouts. The proposed approach establishes new state-of-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16512</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#21040;&#28165;&#26224;&#65306;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#20998;&#26512;&#34920;&#36798;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#65288;Clifford&#65289;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#26631;&#20934;&#27491;&#21017;&#21270;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#30001;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#32473;&#20986;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#38382;&#39064;&#21487;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#22312;&#20854;&#20013;&#32534;&#30721;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#20197;&#25968;&#25454;&#21521;&#37327;&#29983;&#25104;&#30340;&#19977;&#35282;&#24418;&#21644;&#24179;&#34892;&#20307;&#30340;&#26377;&#31526;&#21495;&#20307;&#31215;&#34920;&#31034;&#12290;&#20984;&#38382;&#39064;&#36890;&#36807;$\ell_1$&#27491;&#21017;&#21270;&#25214;&#21040;&#26679;&#26412;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#20197;&#21457;&#29616;&#20165;&#30456;&#20851;&#30340;&#26964;&#31215;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#23618;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Toloka&#35270;&#35273;&#38382;&#31572;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#31454;&#36187;&#65292;&#21457;&#29616;&#30446;&#21069;&#27809;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#36229;&#36807;&#38750;&#19987;&#23478;&#20247;&#21253;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2309.16511</link><description>&lt;p&gt;
Toloka&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;. (arXiv:2309.16511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Toloka Visual Question Answering Benchmark. (arXiv:2309.16511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Toloka&#35270;&#35273;&#38382;&#31572;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#31454;&#36187;&#65292;&#21457;&#29616;&#30446;&#21069;&#27809;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#36229;&#36807;&#38750;&#19987;&#23478;&#20247;&#21253;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Toloka&#35270;&#35273;&#38382;&#31572;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#23545;&#27604;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#32473;&#23450;&#19968;&#24352;&#22270;&#20687;&#21644;&#19968;&#20010;&#25991;&#26412;&#38382;&#39064;&#65292;&#38656;&#35201;&#27491;&#30830;&#22320;&#32472;&#21046;&#20986;&#21253;&#22260;&#35813;&#38382;&#39064;&#22238;&#31572;&#30340;&#23545;&#35937;&#30340;&#36793;&#30028;&#26694;&#12290;&#27599;&#20010;&#22270;&#20687;-&#38382;&#39064;&#23545;&#37117;&#21253;&#21547;&#22238;&#31572;&#65292;&#27599;&#20010;&#22270;&#20687;&#21482;&#26377;&#19968;&#20010;&#27491;&#30830;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;45,199&#20010;&#22270;&#20687;&#21644;&#38382;&#39064;&#30340;&#23545;&#65292;&#20197;&#33521;&#25991;&#25552;&#20379;&#65292;&#24182;&#38468;&#24102;&#26377;&#30495;&#23454;&#30340;&#36793;&#30028;&#26694;&#65292;&#20998;&#20026;&#35757;&#32451;&#21644;&#20004;&#20010;&#27979;&#35797;&#23376;&#38598;&#12290;&#38500;&#20102;&#25551;&#36848;&#25968;&#25454;&#38598;&#24182;&#22312;CC BY&#35768;&#21487;&#19979;&#21457;&#24067;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#24320;&#28304;&#30340;&#38646;&#26679;&#26412;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#24182;&#32452;&#32455;&#20102;&#22312;WSDM Cup&#19978;&#21560;&#24341;&#20102;&#20840;&#29699;48&#20010;&#21442;&#19982;&#32773;&#30340;&#22810;&#38454;&#27573;&#31454;&#36187;&#12290;&#28982;&#32780;&#65292;&#22312;&#25552;&#20132;&#35770;&#25991;&#26102;&#65292;&#26681;&#25454;&#20132;&#21449;&#39564;&#35777;&#27809;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36229;&#36234;&#20102;&#38750;&#19987;&#23478;&#20247;&#21253;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Toloka Visual Question Answering, a new crowdsourced dataset allowing comparing performance of machine learning systems against human level of expertise in the grounding visual question answering task. In this task, given an image and a textual question, one has to draw the bounding box around the object correctly responding to that question. Every image-question pair contains the response, with only one correct response per image. Our dataset contains 45,199 pairs of images and questions in English, provided with ground truth bounding boxes, split into train and two test subsets. Besides describing the dataset and releasing it under a CC BY license, we conducted a series of experiments on open source zero-shot baseline models and organized a multi-phase competition at WSDM Cup that attracted 48 participants worldwide. However, by the time of paper submission, no machine learning model outperformed the non-expert crowdsourcing baseline according to the interse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#20135;&#25414;&#32465;-&#39044;&#27979;-&#35843;&#25972;&#65288;BPR&#65289;&#26694;&#26550;&#65292;&#23558;&#36164;&#20135;&#25414;&#32465;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#39044;&#27979;&#21327;&#35843;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#23545;&#39118;&#30005;&#21151;&#29575;&#30340;&#20934;&#30830;&#39044;&#27979;&#21644;&#19968;&#33268;&#24615;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2309.16492</link><description>&lt;p&gt;
&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340;&#36164;&#20135;&#25414;&#32465;
&lt;/p&gt;
&lt;p&gt;
Asset Bundling for Wind Power Forecasting. (arXiv:2309.16492v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#20135;&#25414;&#32465;-&#39044;&#27979;-&#35843;&#25972;&#65288;BPR&#65289;&#26694;&#26550;&#65292;&#23558;&#36164;&#20135;&#25414;&#32465;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#39044;&#27979;&#21327;&#35843;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#23545;&#39118;&#30005;&#21151;&#29575;&#30340;&#20934;&#30830;&#39044;&#27979;&#21644;&#19968;&#33268;&#24615;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#30005;&#32593;&#20013;&#38388;&#26029;&#24615;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#39118;&#33021;&#21644;&#22826;&#38451;&#33021;&#21457;&#30005;&#65292;&#23548;&#33268;&#36816;&#33829;&#19981;&#30830;&#23450;&#24615;&#22686;&#21152;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#20934;&#30830;&#30340;&#39044;&#27979;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39118;&#33021;&#21457;&#30005;&#65292;&#30001;&#20110;&#20854;&#21464;&#21270;&#24133;&#24230;&#22823;&#19988;&#21382;&#21490;&#19978;&#38590;&#20197;&#39044;&#27979;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#20135;&#25414;&#32465;-&#39044;&#27979;-&#35843;&#25972;&#65288;BPR&#65289;&#26694;&#26550;&#65292;&#23558;&#36164;&#20135;&#25414;&#32465;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#39044;&#27979;&#21327;&#35843;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;BPR&#26694;&#26550;&#39318;&#20808;&#23398;&#20064;&#20013;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#65288;&#25414;&#32465;&#65289;&#65292;&#28982;&#21518;&#39044;&#27979;&#36164;&#20135;&#12289;&#25414;&#32465;&#21644;&#25972;&#20010;&#39118;&#30005;&#22330;&#30340;&#39118;&#30005;&#21151;&#29575;&#65292;&#26368;&#21518;&#35843;&#25972;&#25152;&#26377;&#39044;&#27979;&#32467;&#26524;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#23398;&#20064;&#20219;&#21153;&#65288;&#39044;&#27979;&#25414;&#32465;&#23618;&#27425;&#30340;&#26102;&#38388;&#24207;&#21015;&#65289;&#65292;&#20197;&#24110;&#21161;&#20027;&#35201;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#33021;&#22815;&#25429;&#25417;&#39118;&#30005;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#31354;&#21160;&#24577;&#30340;&#26032;&#30340;&#36164;&#20135;&#25414;&#32465;&#26631;&#20934;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing penetration of intermittent, renewable generation in US power grids, especially wind and solar generation, results in increased operational uncertainty. In that context, accurate forecasts are critical, especially for wind generation, which exhibits large variability and is historically harder to predict. To overcome this challenge, this work proposes a novel Bundle-Predict-Reconcile (BPR) framework that integrates asset bundling, machine learning, and forecast reconciliation techniques. The BPR framework first learns an intermediate hierarchy level (the bundles), then predicts wind power at the asset, bundle, and fleet level, and finally reconciles all forecasts to ensure consistency. This approach effectively introduces an auxiliary learning task (predicting the bundle-level time series) to help the main learning tasks. The paper also introduces new asset-bundling criteria that capture the spatio-temporal dynamics of wind power time series. Extensive numerical experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#35752;&#35770;&#20102;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#30693;&#35782;&#28304;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35775;&#38382;&#21644;&#25805;&#20316;&#33021;&#21147;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16459</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22686;&#24378;LLM&#65306;&#20851;&#20110;&#24187;&#35273;&#39044;&#38450;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Augmenting LLMs with Knowledge: A survey on hallucination prevention. (arXiv:2309.16459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#35752;&#35770;&#20102;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#30693;&#35782;&#28304;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35775;&#38382;&#21644;&#25805;&#20316;&#33021;&#21147;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23384;&#20648;&#20107;&#23454;&#30693;&#35782;&#21644;&#22312;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#32467;&#26524;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#20934;&#30830;&#35775;&#38382;&#21644;&#25805;&#20316;&#30693;&#35782;&#30340;&#33021;&#21147;&#20173;&#28982;&#21463;&#38480;&#65292;&#23548;&#33268;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;&#26550;&#26500;&#30456;&#27604;&#23384;&#22312;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#27169;&#22411;&#20915;&#31574;&#30340;&#26469;&#28304;&#21644;&#20445;&#25345;&#26368;&#26032;&#19990;&#30028;&#30693;&#35782;&#30340;&#25361;&#25112;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21487;&#24494;&#20998;&#35775;&#38382;&#26426;&#21046;&#38598;&#25104;&#21040;&#26174;&#24335;&#30340;&#38750;&#21442;&#25968;&#35760;&#24518;&#20013;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#35843;&#30740;&#25506;&#35752;&#20102;&#22686;&#24378;&#20102;&#19982;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#65288;&#21253;&#25324;&#22806;&#37096;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#65289;&#30456;&#36830;&#25509;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine-tuned for downstream natural language processing tasks. Nonetheless, their capacity to access and manipulate knowledge with precision remains constrained, resulting in performance disparities on knowledge-intensive tasks when compared to task-specific architectures. Additionally, the challenges of providing provenance for model decisions and maintaining up-to-date world knowledge persist as open research frontiers. To address these limitations, the integration of pre-trained models with differentiable access mechanisms to explicit non-parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources, including external knowledge bases and search engines. While adhering to the standard objective of predicting missin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21487;&#28385;&#36275;&#24615;&#27714;&#35299;&#36827;&#34892;&#21453;&#20363;&#24341;&#23548;&#24402;&#32435;&#21512;&#25104;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#34394;&#20551;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24418;&#24335;&#21270;&#24037;&#20214;&#21512;&#25104;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.16436</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#29992;&#20110;&#35745;&#21010;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21487;&#28385;&#36275;&#24615;&#27714;&#35299;&#36827;&#34892;&#21453;&#20363;&#24341;&#23548;&#24402;&#32435;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving. (arXiv:2309.16436v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16436
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21487;&#28385;&#36275;&#24615;&#27714;&#35299;&#36827;&#34892;&#21453;&#20363;&#24341;&#23548;&#24402;&#32435;&#21512;&#25104;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#34394;&#20551;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24418;&#24335;&#21270;&#24037;&#20214;&#21512;&#25104;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#35757;&#32451;&#30340;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;GPT-4&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#20154;&#24037;&#25552;&#20379;&#30340;&#25351;&#20196;&#25552;&#31034;&#26469;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#22238;&#22797;&#12290;&#38500;&#20102;&#33258;&#28982;&#35821;&#35328;&#22238;&#22797;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36824;&#34987;&#21457;&#29616;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;&#20195;&#30721;&#12289;&#35745;&#21010;&#21644;&#36923;&#36753;&#35268;&#33539;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#26174;&#30528;&#25913;&#21892;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#21487;&#33021;&#20135;&#29983;&#20107;&#23454;&#19981;&#27491;&#30830;&#25110;&#19978;&#19979;&#25991;&#19981;&#24688;&#24403;&#30340;&#32467;&#26524;&#65292;&#36825;&#34987;&#31216;&#20026;&#24187;&#35273;&#29616;&#35937;&#12290;&#36825;&#31181;&#38480;&#21046;&#20351;&#24471;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#21512;&#25104;&#24418;&#24335;&#21270;&#24037;&#20214;&#21464;&#24471;&#22256;&#38590;&#12290;&#19982;&#25991;&#26412;&#25688;&#35201;&#21644;&#38382;&#31572;&#31561;&#20219;&#21153;&#19981;&#21516;&#65292;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#12289;&#35745;&#21010;&#21644;&#20854;&#20182;&#24418;&#24335;&#21270;&#24037;&#20214;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#20855;&#26377;&#28798;&#38590;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#21487;&#20197;&#20351;&#29992;&#21487;&#28385;&#36275;&#24615;&#27169;&#22411;&#26816;&#27979;&#65288;SMT&#65289;&#27714;&#35299;&#22120;&#20316;&#20026;&#28436;&#32462;&#25512;&#29702;&#24341;&#25806;&#26469;&#20998;&#26512;&#29983;&#25104;&#30340;&#24037;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative large language models (LLMs) with instruct training such as GPT-4 can follow human-provided instruction prompts and generate human-like responses to these prompts. Apart from natural language responses, they have also been found to be effective at generating formal artifacts such as code, plans, and logical specifications from natural language prompts. Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan, and other formal artifacts produced by LLMs can be catastrophic. We posit that we can use the satisfiability modulo theory (SMT) solvers as deductive reasoning engines to analyze the generat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#32593;&#32476;&#23558;&#38899;&#39057;&#30340;&#34920;&#31034;&#26144;&#23556;&#21040;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#36755;&#20837;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;&#20840;&#23616;&#21644;&#26102;&#38388;&#19978;&#19982;&#36755;&#20837;&#38899;&#39057;&#23545;&#40784;&#30340;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.16429</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#33258;&#36866;&#24212;&#23454;&#29616;&#22810;&#26679;&#19988;&#23545;&#40784;&#30340;&#38899;&#39057;&#21040;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation. (arXiv:2309.16429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16429
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#32593;&#32476;&#23558;&#38899;&#39057;&#30340;&#34920;&#31034;&#26144;&#23556;&#21040;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#36755;&#20837;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;&#20840;&#23616;&#21644;&#26102;&#38388;&#19978;&#19982;&#36755;&#20837;&#38899;&#39057;&#23545;&#40784;&#30340;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#26469;&#33258;&#21508;&#31181;&#35821;&#20041;&#31867;&#21035;&#30340;&#33258;&#28982;&#38899;&#39057;&#26679;&#26412;&#26469;&#24341;&#23548;&#29983;&#25104;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#35270;&#39057;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#35270;&#39057;&#38656;&#35201;&#22312;&#20840;&#23616;&#21644;&#26102;&#38388;&#19978;&#19982;&#36755;&#20837;&#38899;&#39057;&#36827;&#34892;&#23545;&#40784;&#65306;&#20840;&#23616;&#19978;&#65292;&#36755;&#20837;&#38899;&#39057;&#19982;&#25972;&#20010;&#36755;&#20986;&#35270;&#39057;&#26377;&#35821;&#20041;&#20851;&#32852;&#65307;&#26102;&#38388;&#19978;&#65292;&#36755;&#20837;&#38899;&#39057;&#30340;&#27599;&#20010;&#29255;&#27573;&#37117;&#19982;&#30456;&#24212;&#30340;&#35270;&#39057;&#29255;&#27573;&#20851;&#32852;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#39537;&#21160;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#36866;&#37197;&#22120;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#23398;&#20064;&#23558;&#22522;&#20110;&#38899;&#39057;&#30340;&#34920;&#31034;&#26144;&#23556;&#21040;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#36755;&#20837;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#23427;&#20063;&#21487;&#20197;&#23454;&#29616;&#22522;&#20110;&#25991;&#26412;&#12289;&#38899;&#39057;&#20197;&#21450;&#25991;&#26412;&#21644;&#38899;&#39057;&#30340;&#35270;&#39057;&#29983;&#25104;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#38899;&#39057;-&#35270;&#39057;&#26679;&#26412;&#30340;&#26174;&#33879;&#35821;&#20041;&#22810;&#26679;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#20132;&#35821;&#22659;&#25299;&#25169;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16424</link><description>&lt;p&gt;
Prompt-and-Align: &#22522;&#20110;&#25552;&#31034;&#30340;&#31038;&#20132;&#35843;&#25972;&#29992;&#20110;&#23569;&#26679;&#26412;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection. (arXiv:2309.16424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#20132;&#35821;&#22659;&#25299;&#25169;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#26816;&#27979;&#34394;&#20551;&#26032;&#38395;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#26032;&#38395;&#30340;&#21450;&#26102;&#24615;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#26681;&#25454;&#26377;&#38480;&#30340;&#20107;&#23454;&#26680;&#23545;&#20449;&#24687;&#26469;&#39044;&#27979;&#26032;&#38395;&#25991;&#31456;&#30340;&#30495;&#23454;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36981;&#24490;&#8220;&#20174;&#22836;&#35757;&#32451;&#8221;&#30340;&#33539;&#20363;&#65292;&#36825;&#22312;&#26681;&#26412;&#19978;&#21463;&#21040;&#22823;&#35268;&#27169;&#27880;&#37322;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#12290;&#23613;&#31649;&#24050;&#32463;&#20197;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#30340;&#26041;&#24335;&#36866;&#24212;&#20102;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#20294;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#20063;&#38656;&#35201;&#26114;&#36149;&#30340;&#20219;&#21153;&#29305;&#23450;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;Prompt-and-Align&#8221;&#65288;P&amp;A&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#26032;&#33539;&#20363;&#65292;&#32852;&#21512;&#21033;&#29992;&#20102;PLM&#20013;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#31038;&#20132;&#35821;&#22659;&#25299;&#25169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#26032;&#38395;&#25991;&#31456;&#21253;&#35065;&#22312;&#19968;&#20010;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25991;&#26412;&#25552;&#31034;&#20013;&#26469;&#32531;&#35299;&#26631;&#31614;&#31232;&#32570;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;PLM&#22788;&#29702;&#26469;&#30452;&#25509;&#24341;&#20986;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite considerable advances in automated fake news detection, due to the timely nature of news, it remains a critical open question how to effectively predict the veracity of news articles based on limited fact-checks. Existing approaches typically follow a "Train-from-Scratch" paradigm, which is fundamentally bounded by the availability of large-scale annotated data. While expressive pre-trained language models (PLMs) have been adapted in a "Pre-Train-and-Fine-Tune" manner, the inconsistency between pre-training and downstream objectives also requires costly task-specific supervision. In this paper, we propose "Prompt-and-Align" (P&amp;A), a novel prompt-based paradigm for few-shot fake news detection that jointly leverages the pre-trained knowledge in PLMs and the social context topology. Our approach mitigates label scarcity by wrapping the news article in a task-related textual prompt, which is then processed by the PLM to directly elicit task-specific knowledge. To supplement the PL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.16414</link><description>&lt;p&gt;
AutoCLIP: &#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26500;&#24314;&#30340;&#20998;&#31867;&#22120;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#26681;&#25454;&#25552;&#31034;&#27169;&#26495;&#33258;&#21160;&#21019;&#24314;&#27599;&#20010;&#31867;&#21035;&#30340;&#25551;&#36848;&#31526;&#38598;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#21253;&#25324;&#25163;&#24037;&#35774;&#35745;&#30340;&#27169;&#26495;&#12289;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#30340;&#27169;&#26495;&#20197;&#21450;&#20174;&#38543;&#26426;&#21333;&#35789;&#21644;&#23383;&#31526;&#26500;&#24314;&#30340;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#20174;&#30456;&#24212;&#30340;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#23548;&#20986;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65306;&#23558;&#22270;&#20687;&#30340;&#24179;&#22343;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#19982;&#32534;&#30721;&#22270;&#20687;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26368;&#22823;&#21270;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#25551;&#36848;&#31526;&#27604;&#20854;&#20182;&#25551;&#36848;&#31526;&#26356;&#22909;&#22320;&#21305;&#37197;&#32473;&#23450;&#22270;&#20687;&#19978;&#30340;&#35270;&#35273;&#32447;&#32034;&#26102;&#65292;&#23558;&#25152;&#26377;&#31867;&#21035;&#25551;&#36848;&#31526;&#31561;&#26435;&#37325;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35843;&#35856;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;AutoCLIP&#12290;AutoCLIP&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#20102;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#20174;s
&lt;/p&gt;
&lt;p&gt;
Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. In contrast, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, that is: classify to the class that maximizes the cosine similarity between its averaged encoded class descriptors and the encoded image. However, weighting all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP assigns to each prompt template per-image weights, which are derived from s
&lt;/p&gt;</description></item><item><title>&#36951;&#20256;&#24037;&#31243;&#31639;&#27861;&#65288;GEA&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#39640;&#25928;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#35774;&#35745;&#20256;&#32479;&#36951;&#20256;&#31639;&#27861;&#24182;&#24341;&#20837;&#26032;&#30340;&#25628;&#32034;&#26041;&#27861;&#65292;GEA&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#26089;&#29087;&#25910;&#25947;&#12289;&#32570;&#20047;&#38382;&#39064;&#29305;&#23450;&#30693;&#35782;&#21644;&#38543;&#26426;&#24615;&#31561;&#38480;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#36951;&#20256;&#31639;&#27861;&#21644;&#20854;&#20182;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16413</link><description>&lt;p&gt;
&#36951;&#20256;&#24037;&#31243;&#31639;&#27861;&#65288;GEA&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#39640;&#25928;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Genetic Engineering Algorithm (GEA): An Efficient Metaheuristic Algorithm for Solving Combinatorial Optimization Problems. (arXiv:2309.16413v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16413
&lt;/p&gt;
&lt;p&gt;
&#36951;&#20256;&#24037;&#31243;&#31639;&#27861;&#65288;GEA&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#39640;&#25928;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#35774;&#35745;&#20256;&#32479;&#36951;&#20256;&#31639;&#27861;&#24182;&#24341;&#20837;&#26032;&#30340;&#25628;&#32034;&#26041;&#27861;&#65292;GEA&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#26089;&#29087;&#25910;&#25947;&#12289;&#32570;&#20047;&#38382;&#39064;&#29305;&#23450;&#30693;&#35782;&#21644;&#38543;&#26426;&#24615;&#31561;&#38480;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#36951;&#20256;&#31639;&#27861;&#21644;&#20854;&#20182;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#20256;&#31639;&#27861;&#65288;GAs&#65289;&#20197;&#20854;&#22312;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#29575;&#32780;&#38395;&#21517;&#65292;&#25317;&#26377;&#25506;&#32034;&#22810;&#26679;&#21270;&#35299;&#31354;&#38388;&#30340;&#33021;&#21147;&#12289;&#22788;&#29702;&#21508;&#31181;&#34920;&#31034;&#12289;&#21033;&#29992;&#24182;&#34892;&#24615;&#12289;&#20445;&#30041;&#33391;&#22909;&#35299;&#12289;&#36866;&#24212;&#21464;&#21270;&#21160;&#24577;&#12289;&#22788;&#29702;&#32452;&#21512;&#22810;&#26679;&#24615;&#21644;&#25552;&#20379;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;GAs&#23384;&#22312;&#26089;&#29087;&#25910;&#25947;&#12289;&#32570;&#20047;&#38382;&#39064;&#29305;&#23450;&#30693;&#35782;&#21644;&#20132;&#21449;&#21644;&#21464;&#24322;&#25805;&#20316;&#30340;&#38543;&#26426;&#24615;&#31561;&#38480;&#21046;&#65292;&#20351;&#20854;&#36890;&#24120;&#22312;&#25214;&#21040;&#26368;&#20248;&#35299;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36951;&#20256;&#24037;&#31243;&#31639;&#27861;&#65288;GEA&#65289;&#30340;&#26032;&#22411;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20174;&#22522;&#22240;&#24037;&#31243;&#27010;&#24565;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;GEA&#37325;&#26032;&#35774;&#35745;&#20102;&#20256;&#32479;&#36951;&#20256;&#31639;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#38548;&#31163;&#12289;&#32431;&#21270;&#12289;&#25554;&#20837;&#21644;&#34920;&#36798;&#26032;&#22522;&#22240;&#65292;&#22522;&#20110;&#24050;&#26377;&#22522;&#22240;&#20135;&#29983;&#29305;&#23450;&#26579;&#33394;&#20307;&#65292;&#20174;&#32780;&#23454;&#29616;&#25152;&#38656;&#29305;&#24449;&#30340;&#20986;&#29616;&#21644;&#29305;&#23450;&#26579;&#33394;&#20307;&#30340;&#29983;&#25104;&#12290;&#27604;&#36739;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;GEA&#22312;&#22810;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#36951;&#20256;&#31639;&#27861;&#21644;&#20854;&#20182;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Genetic Algorithms (GAs) are known for their efficiency in solving combinatorial optimization problems, thanks to their ability to explore diverse solution spaces, handle various representations, exploit parallelism, preserve good solutions, adapt to changing dynamics, handle combinatorial diversity, and provide heuristic search. However, limitations such as premature convergence, lack of problem-specific knowledge, and randomness of crossover and mutation operators make GAs generally inefficient in finding an optimal solution. To address these limitations, this paper proposes a new metaheuristic algorithm called the Genetic Engineering Algorithm (GEA) that draws inspiration from genetic engineering concepts. GEA redesigns the traditional GA while incorporating new search methods to isolate, purify, insert, and express new genes based on existing ones, leading to the emergence of desired traits and the production of specific chromosomes based on the selected genes. Comparative evaluati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#28065;&#27169;&#25311;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#24335;&#22320;&#20445;&#30041;&#28237;&#27969;&#31995;&#32479;&#30340;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25551;&#36848;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#22823;&#24133;&#24230;&#20943;&#23569;&#35745;&#31639;&#37327;&#24182;&#20445;&#25345;&#28237;&#27969;&#31995;&#32479;&#30340;&#32479;&#35745;&#29289;&#29702;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2309.16400</link><description>&lt;p&gt;
&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#30340;&#29289;&#29702;&#20445;&#25345;&#30340;AI&#21152;&#36895;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Physics-Preserving AI-Accelerated Simulations of Plasma Turbulence. (arXiv:2309.16400v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#28065;&#27169;&#25311;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#24335;&#22320;&#20445;&#30041;&#28237;&#27969;&#31995;&#32479;&#30340;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25551;&#36848;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#22823;&#24133;&#24230;&#20943;&#23569;&#35745;&#31639;&#37327;&#24182;&#20445;&#25345;&#28237;&#27969;&#31995;&#32479;&#30340;&#32479;&#35745;&#29289;&#29702;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#20307;&#12289;&#27668;&#20307;&#21644;&#31561;&#31163;&#23376;&#20307;&#20013;&#30340;&#28237;&#27969;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#26082;&#37325;&#35201;&#21448;&#27809;&#26377;&#35299;&#31572;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#20854;&#26080;&#27861;&#31616;&#21333;&#22320;&#36890;&#36807;&#35745;&#31639;&#22788;&#29702;&#20854;&#19981;&#21487;&#31616;&#21270;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#22823;&#28065;&#27169;&#25311; (LES) &#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064; (ML) &#25216;&#26415;&#65292;&#20165;&#20445;&#30041;&#26368;&#22823;&#30340;&#21160;&#21147;&#23398;&#26174;&#24335;&#22320;&#23637;&#31034;&#65292;&#32780;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#21017;&#36890;&#36807;&#22522;&#20110;ML&#30340;&#23376;&#32593;&#26684;&#27169;&#22411;&#26469;&#25551;&#36848;&#12290;&#23558;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#39537;&#21160;&#30340;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#65292;&#33021;&#22815;&#21435;&#38500;&#22823;&#37096;&#20998;&#24815;&#24615;&#33539;&#22260;&#65292;&#23558;&#35745;&#31639;&#37327;&#38477;&#20302;&#20102;&#22823;&#32422;&#19977;&#20010;&#25968;&#37327;&#32423;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#28237;&#27969;&#31995;&#32479;&#30340;&#32479;&#35745;&#29289;&#29702;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turbulence in fluids, gases, and plasmas remains an open problem of both practical and fundamental importance. Its irreducible complexity usually cannot be tackled computationally in a brute-force style. Here, we combine Large Eddy Simulation (LES) techniques with Machine Learning (ML) to retain only the largest dynamics explicitly, while small-scale dynamics are described by an ML-based sub-grid-scale model. Applying this novel approach to self-driven plasma turbulence allows us to remove large parts of the inertial range, reducing the computational effort by about three orders of magnitude, while retaining the statistical physical properties of the turbulent system.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.16397</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#20027;&#21160;&#20132;&#20114;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#26694;&#26550;&#65292;&#22240;&#27492;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#23588;&#20854;&#21560;&#24341;&#20154;&#12290;&#26368;&#36817;Transformers&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#23558;&#31163;&#32447;RL&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#36825;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#36807;&#20110;&#20048;&#35266;&#65292;&#38169;&#35823;&#22320;&#20551;&#35774;&#30456;&#21516;&#30340;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#30456;&#21516;&#30340;&#21160;&#20316;&#19968;&#33268;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#36716;&#25442;&#27169;&#22411;&#25110;&#22797;&#26434;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UNREST&#36890;&#36807;&#36716;&#25442;&#19982;&#22238;&#25253;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#26469;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#12290;&#36890;&#36807;&#21457;&#29616;&#39550;&#39542;&#29615;&#22659;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#32047;&#31215;&#8221;&#21644;&#8220;&#26102;&#38388;&#23616;&#37096;&#24615;&#8221;&#29305;&#24615;&#65292;UNREST&#23558;&#20915;&#31574;Transformer&#20013;&#30340;&#20840;&#23616;&#22238;&#25253;&#26367;&#25442;&#20026;&#36739;&#23569;&#30340;&#37096;&#20998;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) has emerged as a promising framework for learning policies without active interactions, making it especially appealing for autonomous driving tasks. Recent successes of Transformers inspire casting offline RL as sequence modeling, which performs well in long-horizon tasks. However, they are overly optimistic in stochastic environments with incorrect assumptions that the same goal can be consistently achieved by identical actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in stochastic driving environments without introducing additional transition or complex generative models. Specifically, UNREST estimates state uncertainties by the conditional mutual information between transitions and returns, and segments sequences accordingly. Discovering the `uncertainty accumulation' and `temporal locality' properties of driving environments, UNREST replaces the global returns in decision transformers with less 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16391</link><description>&lt;p&gt;
&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;&#20108;&#32500;Copula&#36924;&#36817;&#21464;&#25442;&#65306;2-Cats&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copula&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#24212;&#29992;Copula&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#29420;&#31435;&#30340;&#36793;&#38469;&#20998;&#24067;&#65288;&#19968;&#20010;&#31616;&#21333;&#20219;&#21153;&#65289;&#65292;&#28982;&#21518;&#20272;&#35745;&#36830;&#25509;&#36793;&#38469;&#30340;&#21333;&#20010;Copula&#20989;&#25968;C&#65288;&#19968;&#20010;&#22256;&#38590;&#20219;&#21153;&#65289;&#26469;&#20272;&#35745;&#22810;&#20803;&#20998;&#24067;&#20989;&#25968;&#12290;&#23545;&#20110;&#20108;&#32500;&#25968;&#25454;&#65292;Copula&#26159;&#19968;&#20010;&#24418;&#22914;C&#65306;(u&#65292;v)&#8712;\mathbf{I}^2\rightarrow \mathbf{I}&#30340;&#20108;&#27425;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;\mathbf{I}=[0&#65292;1]&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22914;&#20309;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;2-Cats&#65292;&#21463;&#21040;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;Sobolev&#35757;&#32451;&#25991;&#29486;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19981;&#20165;&#35777;&#26126;&#20102;&#25105;&#20204;&#33021;&#22815;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#22320;&#20272;&#35745;2D Copula&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19988;&#31526;&#21512;Copula C&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
&lt;/p&gt;</description></item><item><title>RLLTE&#26159;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#39044;&#35745;&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#24182;&#21050;&#28608;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#12290;</title><link>http://arxiv.org/abs/2309.16382</link><description>&lt;p&gt;
RLLTE&#65306;&#24378;&#21270;&#23398;&#20064;&#30340;&#38271;&#26399;&#28436;&#36827;&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
RLLTE: Long-Term Evolution Project of Reinforcement Learning. (arXiv:2309.16382v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16382
&lt;/p&gt;
&lt;p&gt;
RLLTE&#26159;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#39044;&#35745;&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#24182;&#21050;&#28608;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RLLTE&#65306;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#19982;&#24212;&#29992;&#26694;&#26550;&#12290;&#38500;&#20102;&#25552;&#20379;&#19968;&#27969;&#30340;&#31639;&#27861;&#23454;&#29616;&#20043;&#22806;&#65292;RLLTE&#36824;&#20316;&#20026;&#19968;&#20010;&#31639;&#27861;&#24320;&#21457;&#24037;&#20855;&#21253;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;RLLTE&#23436;&#20840;&#35299;&#32806;&#20102;RL&#31639;&#27861;&#19982;&#24320;&#21457;&#31639;&#27861;&#30340;&#23454;&#36341;&#35282;&#24230;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#32452;&#20214;&#26469;&#21152;&#36895;&#31639;&#27861;&#30340;&#21457;&#23637;&#21644;&#28436;&#36827;&#12290;&#29305;&#21035;&#22320;&#65292;RLLTE&#26159;&#31532;&#19968;&#20010;&#26500;&#24314;&#20102;&#23436;&#25972;&#20016;&#23500;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;RL&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#22411;&#35757;&#32451;&#12289;&#35780;&#20272;&#12289;&#37096;&#32626;&#12289;&#22522;&#20934;&#27979;&#35797;&#20013;&#24515;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22686;&#24378;&#30340;&#21103;&#39550;&#39542;&#12290;&#39044;&#26399;RLLTE&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#65292;&#24182;&#23545;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#20855;&#26377;&#39640;&#24230;&#21050;&#28608;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RLLTE: a long-term evolution, extremely modular, and open-source framework for reinforcement learning (RL) research and application. Beyond delivering top-notch algorithm implementations, RLLTE also serves as a toolkit for developing algorithms. More specifically, RLLTE decouples the RL algorithms completely from the exploitation-exploration perspective, providing a large number of components to accelerate algorithm development and evolution. In particular, RLLTE is the first RL framework to build a complete and luxuriant ecosystem, which includes model training, evaluation, deployment, benchmark hub, and large language model (LLM)-empowered copilot. RLLTE is expected to set standards for RL engineering practice and be highly stimulative for industry and academia.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#27491;&#24577;&#27969;&#23545;IceCube&#20013;&#24494;&#23376;&#35266;&#27979;&#31449;&#30340;&#20107;&#20214;&#37325;&#26500;&#36827;&#34892;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26465;&#20214;&#27491;&#24577;&#27969;&#21487;&#20197;&#27491;&#30830;&#22320;&#32771;&#34385;&#21335;&#26497;&#20912;&#30340;&#20809;&#23398;&#29305;&#24615;&#21644;&#20854;&#19982;&#25506;&#27979;&#22120;&#30340;&#20851;&#31995;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#19979;&#19968;&#20195;&#37325;&#26500;&#24037;&#20316;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16380</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;IceCube&#20107;&#20214;&#37325;&#26500;&#30340;&#26465;&#20214;&#27491;&#24577;&#27969;
&lt;/p&gt;
&lt;p&gt;
Conditional normalizing flows for IceCube event reconstruction. (arXiv:2309.16380v1 [astro-ph.HE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#27491;&#24577;&#27969;&#23545;IceCube&#20013;&#24494;&#23376;&#35266;&#27979;&#31449;&#30340;&#20107;&#20214;&#37325;&#26500;&#36827;&#34892;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26465;&#20214;&#27491;&#24577;&#27969;&#21487;&#20197;&#27491;&#30830;&#22320;&#32771;&#34385;&#21335;&#26497;&#20912;&#30340;&#20809;&#23398;&#29305;&#24615;&#21644;&#20854;&#19982;&#25506;&#27979;&#22120;&#30340;&#20851;&#31995;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#19979;&#19968;&#20195;&#37325;&#26500;&#24037;&#20316;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
IceCube&#20013;&#24494;&#23376;&#35266;&#27979;&#31449;&#26159;&#37096;&#32626;&#22312;&#21335;&#26497;&#20912;&#20013;&#30340;&#19968;&#20010;&#31435;&#26041;&#21315;&#31859;&#32423;&#30340;&#39640;&#33021;&#20013;&#24494;&#23376;&#25506;&#27979;&#22120;&#12290;&#20004;&#20010;&#20027;&#35201;&#30340;&#20107;&#20214;&#31867;&#21035;&#26159;&#24102;&#30005;&#30005;&#23376;&#21644;&#24102;&#30005;&#956;&#20013;&#24494;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;&#26465;&#20214;&#27491;&#24577;&#27969;&#23545;&#36825;&#20123;&#31867;&#21035;&#30340;&#26041;&#21521;&#21644;&#33021;&#37327;&#36827;&#34892;&#25512;&#26029;&#12290;&#23427;&#20204;&#20801;&#35768;&#22522;&#20110;&#21407;&#22987;&#25968;&#25454;&#23545;&#27599;&#20010;&#21333;&#29420;&#20107;&#20214;&#25512;&#23548;&#20986;&#19968;&#20010;&#21518;&#39564;&#20998;&#24067;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#21253;&#25324;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38750;&#24120;&#36866;&#29992;&#20110;&#19979;&#19968;&#20195;&#37325;&#26500;&#24037;&#20316;&#12290;&#23545;&#20110;&#27599;&#20010;&#27491;&#24577;&#27969;&#65292;&#25105;&#20204;&#20351;&#29992;&#24494;&#20998;&#29109;&#21644;KL&#25955;&#24230;&#21040;&#20854;&#26368;&#22823;&#29109;&#36817;&#20284;&#26469;&#35299;&#37322;&#32467;&#26524;&#12290;&#27491;&#24577;&#27969;&#27491;&#30830;&#22320;&#21253;&#21547;&#20102;&#21335;&#26497;&#20912;&#30340;&#22797;&#26434;&#20809;&#23398;&#29305;&#24615;&#21450;&#20854;&#19982;&#23884;&#20837;&#24335;&#25506;&#27979;&#22120;&#30340;&#20851;&#31995;&#12290;&#23545;&#20110;&#28107;&#28020;&#20107;&#20214;&#65292;&#24494;&#20998;&#29109;&#22312;&#39640;&#20809;&#23376;&#21560;&#25910;&#21306;&#22495;&#22686;&#21152;&#65292;&#22312;&#28165;&#26224;&#20912;&#20013;&#20943;&#23569;&#12290;&#23545;&#20110;&#956;&#23376;&#65292;&#24494;&#20998;&#29109;&#19982;&#25152;&#21253;&#21547;&#30340;&#21306;&#22495;&#24378;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The IceCube Neutrino Observatory is a cubic-kilometer high-energy neutrino detector deployed in the Antarctic ice. Two major event classes are charged-current electron and muon neutrino interactions. In this contribution, we discuss the inference of direction and energy for these classes using conditional normalizing flows. They allow to derive a posterior distribution for each individual event based on the raw data that can include systematic uncertainties, which makes them very promising for next-generation reconstructions. For each normalizing flow we use the differential entropy and the KL-divergence to its maximum entropy approximation to interpret the results. The normalizing flows correctly incorporate complex optical properties of the Antarctic ice and their relation to the embedded detector. For showers, the differential entropy increases in regions of high photon absorption and decreases in clear ice. For muons, the differential entropy strongly correlates with the contained 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35748;&#30693;&#36923;&#36753;&#31243;&#24207;&#65288;ELPs&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#35748;&#30693;&#36816;&#31639;&#31526;&#21644;&#19990;&#30028;&#35266;&#30340;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#20256;&#32479;ASP&#30340;&#33258;&#19979;&#32780;&#19978;&#27169;&#22359;&#21270;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#20215;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#22810;&#31181;&#35821;&#20041;&#65292;&#24182;&#19988;&#22312;&#20219;&#20309;&#35821;&#20041;&#19979;&#37117;&#26159;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.16344</link><description>&lt;p&gt;
&#35748;&#30693;&#36923;&#36753;&#31243;&#24207;&#65306;&#19968;&#20123;&#24615;&#36136;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Epistemic Logic Programs: a study of some properties. (arXiv:2309.16344v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35748;&#30693;&#36923;&#36753;&#31243;&#24207;&#65288;ELPs&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#35748;&#30693;&#36816;&#31639;&#31526;&#21644;&#19990;&#30028;&#35266;&#30340;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#20256;&#32479;ASP&#30340;&#33258;&#19979;&#32780;&#19978;&#27169;&#22359;&#21270;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#20215;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#22810;&#31181;&#35821;&#20041;&#65292;&#24182;&#19988;&#22312;&#20219;&#20309;&#35821;&#20041;&#19979;&#37117;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#36923;&#36753;&#31243;&#24207;&#65288;ELPs&#65289;&#22312;&#31572;&#26696;&#38598;&#32534;&#31243;&#65288;ASP&#65289;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#35748;&#30693;&#36816;&#31639;&#31526;&#12290;&#36825;&#31867;&#31243;&#24207;&#30340;&#35821;&#20041;&#26159;&#36890;&#36807;&#19990;&#30028;&#35266;&#26469;&#25552;&#20379;&#30340;&#65292;&#19990;&#30028;&#35266;&#26159;&#19968;&#32452;&#20449;&#24565;&#38598;&#21512;&#65292;&#21363;&#21477;&#27861;&#19978;&#30340;&#21407;&#23376;&#38598;&#21512;&#30340;&#38598;&#21512;&#12290;&#19981;&#21516;&#30340;&#35821;&#20041;&#26041;&#27861;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#19990;&#30028;&#35266;&#21051;&#30011;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20123;&#23545;ELPs&#30340;&#35821;&#20041;&#28385;&#36275;&#30340;&#24615;&#36136;&#65292;&#20363;&#22914;&#35748;&#30693;&#20998;&#21106;&#24615;&#36136;&#65292;&#22914;&#26524;&#28385;&#36275;&#35813;&#24615;&#36136;&#65292;&#21487;&#20197;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;ASP&#20197;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#24335;&#27169;&#22359;&#21270;&#22320;&#35745;&#31639;&#19990;&#30028;&#35266;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25913;&#21464;&#35266;&#23519;&#35282;&#24230;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#33258;&#19979;&#32780;&#19978;&#30340;&#20998;&#35010;&#26041;&#27861;&#36716;&#21521;&#33258;&#19978;&#32780;&#19979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#19982;&#33258;&#19979;&#32780;&#19978;&#26041;&#27861;&#31561;&#20215;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#26032;&#23450;&#20041;&#65306;&#65288;i&#65289;&#21487;&#20197;&#35777;&#26126;&#36866;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#35821;&#20041;&#65307;&#65288;ii&#65289;&#19982;&#8220;&#20256;&#32479;&#8221;&#30340;ASP&#31867;&#20284;&#22320;&#25805;&#20316;&#65307;&#65288;iii&#65289;&#22312;&#20219;&#20309;&#35821;&#20041;&#19979;&#37117;&#21487;&#20197;&#35777;&#26126;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epistemic Logic Programs (ELPs), extend Answer Set Programming (ASP) with epistemic operators. The semantics of such programs is provided in terms of world views, which are sets of belief sets, i.e., syntactically, sets of sets of atoms. Different semantic approaches propose different characterizations of world views. Recent work has introduced semantic properties that should be met by any semantics for ELPs, like the Epistemic Splitting Property, that, if satisfied, allows to modularly compute world views in a bottom-up fashion, analogously to ``traditional'' ASP. We analyze the possibility of changing the perspective, shifting from a bottom-up to a top-down approach to splitting. We propose a basic top-down approach, which we prove to be equivalent to the bottom-up one. We then propose an extended approach, where our new definition: (i) is provably applicable to many of the existing semantics; (ii) operates similarly to ``traditional'' ASP; (iii) provably coincides under any semantic
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20013;&#39044;&#27979;&#25151;&#39076;&#30340;&#39118;&#38505;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#30446;&#21069;&#30340;&#24515;&#30005;&#22270;&#20013;&#35782;&#21035;&#20986;&#26410;&#26469;&#20250;&#21457;&#23637;&#25151;&#39076;&#30340;&#24739;&#32773;&#65292;&#24182;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#30340;&#39118;&#38505;&#31561;&#32423;&#35780;&#20272;&#20182;&#20204;&#22312;&#19968;&#23450;&#26102;&#38388;&#20869;&#21457;&#23637;&#25151;&#39076;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16335</link><description>&lt;p&gt;
&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20013;&#36827;&#34892;&#30340;&#25151;&#39076;&#39118;&#38505;&#39044;&#27979;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Risk Prediction of Atrial Fibrillation from the 12-Lead ECG by Deep Neural Networks. (arXiv:2309.16335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20013;&#39044;&#27979;&#25151;&#39076;&#30340;&#39118;&#38505;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#30446;&#21069;&#30340;&#24515;&#30005;&#22270;&#20013;&#35782;&#21035;&#20986;&#26410;&#26469;&#20250;&#21457;&#23637;&#25151;&#39076;&#30340;&#24739;&#32773;&#65292;&#24182;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#30340;&#39118;&#38505;&#31561;&#32423;&#35780;&#20272;&#20182;&#20204;&#22312;&#19968;&#23450;&#26102;&#38388;&#20869;&#21457;&#23637;&#25151;&#39076;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#25151;&#39076;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#24459;&#22833;&#24120;&#20043;&#19968;&#65292;&#27599;&#24180;&#24433;&#21709;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#65292;&#19982;&#20013;&#39118;&#21644;&#24515;&#21147;&#34928;&#31469;&#31561;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#39118;&#38505;&#23494;&#20999;&#30456;&#20851;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35780;&#20272;&#20174;&#24515;&#30005;&#22270;&#20013;&#21457;&#23637;&#25151;&#39076;&#30340;&#39118;&#38505;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#24052;&#35199;&#25910;&#38598;&#30340;&#22823;&#22411;CODE&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#21644;&#35780;&#20272;&#36825;&#26679;&#30340;&#31639;&#27861;&#12290;&#32467;&#26524;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35782;&#21035;&#20986;&#22312;&#30446;&#21069;&#30340;&#24515;&#30005;&#22270;&#20013;&#27809;&#26377;&#25151;&#39076;&#36857;&#35937;&#65292;&#20294;&#23558;&#26469;&#20250;&#21457;&#23637;&#25151;&#39076;&#30340;&#24739;&#32773;&#65292;AUC&#24471;&#20998;&#20026;0.845&#12290;&#20174;&#25105;&#20204;&#30340;&#29983;&#23384;&#27169;&#22411;&#20013;&#24471;&#30693;&#65292;&#39640;&#39118;&#38505;&#32452;&#65288;&#21363;&#26410;&#26469;&#25151;&#39076;&#21457;&#30149;&#27010;&#29575;&#22823;&#20110;0.7&#65289;&#30340;&#24739;&#32773;&#22312;40&#21608;&#20869;&#21457;&#23637;&#25151;&#39076;&#30340;&#21487;&#33021;&#24615;&#35201;&#39640;&#20986;50%&#65292;&#32780;&#23646;&#20110;&#26368;&#20302;&#39118;&#38505;&#32452;&#65288;&#21363;&#26410;&#26469;&#25151;&#39076;&#21457;&#30149;&#27010;&#29575;&#23567;&#20110;&#25110;&#31561;&#20110;0.1&#65289;&#30340;&#24739;&#32773;&#26377;&#36229;&#36807;85%&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Atrial fibrillation (AF) is one of the most common cardiac arrhythmias that affects millions of people each year worldwide and it is closely linked to increased risk of cardiovascular diseases such as stroke and heart failure. Machine learning methods have shown promising results in evaluating the risk of developing atrial fibrillation from the electrocardiogram. We aim to develop and evaluate one such algorithm on a large CODE dataset collected in Brazil.  Results: The deep neural network model identified patients without indication of AF in the presented ECG but who will develop AF in the future with an AUC score of 0.845. From our survival model, we obtain that patients in the high-risk group (i.e. with the probability of a future AF case being greater than 0.7) are 50% more likely to develop AF within 40 weeks, while patients belonging to the minimal-risk group (i.e. with the probability of a future AF case being less than or equal to 0.1) have more than 85% chance of r
&lt;/p&gt;</description></item><item><title>ReCAT&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;Transformer&#27169;&#22411;&#65292;&#20351;&#29992;&#36882;&#24402;&#32452;&#21512;&#21644;&#19978;&#19979;&#25991;&#20869;&#22806;&#23618;&#33021;&#22815;&#27169;&#25311;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.16319</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#32452;&#21512;&#22810;&#31890;&#24230;&#34920;&#31034;&#30340;Transformer&#22686;&#24378;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Augmenting transformers with recursively composed multi-grained representations. (arXiv:2309.16319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16319
&lt;/p&gt;
&lt;p&gt;
ReCAT&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;Transformer&#27169;&#22411;&#65292;&#20351;&#29992;&#36882;&#24402;&#32452;&#21512;&#21644;&#19978;&#19979;&#25991;&#20869;&#22806;&#23618;&#33021;&#22815;&#27169;&#25311;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReCAT&#30340;&#36882;&#24402;&#32452;&#21512;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#23398;&#20064;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#26126;&#30830;&#24314;&#27169;&#21407;&#22987;&#25991;&#26412;&#30340;&#23618;&#32423;&#21477;&#27861;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#12290;&#29616;&#26377;&#30740;&#31350;&#38480;&#21046;&#25968;&#25454;&#36981;&#24490;&#23618;&#32423;&#26641;&#32467;&#26500;&#65292;&#22240;&#27492;&#32570;&#20047;&#36328;&#36317;&#36890;&#20449;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#20869;&#22806;(CIO)&#23618;&#65292;&#36890;&#36807;&#33258;&#24213;&#21521;&#19978;&#21644;&#33258;&#39030;&#21521;&#19979;&#30340;&#20256;&#36882;&#23398;&#20064;&#36328;&#24230;&#30340;&#19978;&#19979;&#25991;&#21270;&#34920;&#31034;&#65292;&#20854;&#20013;&#33258;&#24213;&#21521;&#19978;&#20256;&#36882;&#36890;&#36807;&#32452;&#21512;&#20302;&#32423;&#36328;&#24230;&#24418;&#25104;&#39640;&#32423;&#36328;&#24230;&#30340;&#34920;&#31034;&#65292;&#32780;&#33258;&#39030;&#21521;&#19979;&#20256;&#36882;&#21017;&#32467;&#21512;&#20102;&#36328;&#24230;&#20869;&#37096;&#21644;&#22806;&#37096;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;&#23884;&#20837;&#23618;&#21644;&#27880;&#24847;&#21147;&#23618;&#20043;&#38388;&#21472;&#21152;&#22810;&#20010;CIO&#23618;&#65292;ReCAT&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#36328;&#36317;&#20869;&#37096;&#21644;&#36328;&#36317;&#38388;&#30340;&#28145;&#23618;&#20132;&#20114;&#65292;&#20174;&#32780;&#29983;&#25104;&#19982;&#20854;&#20182;&#36328;&#24230;&#23436;&#20840;&#19978;&#19979;&#25991;&#21270;&#30340;&#22810;&#31890;&#24230;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;CIO&#23618;&#21487;&#20197;&#36827;&#34892;&#32852;&#21512;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, the ReCAT model can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. Moreover, the CIO layers can be jointly pre-trained
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#23384;&#22312;&#26681;&#26412;&#30340;&#38480;&#21046;&#65292;&#20294;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#26041;&#27861;&#21644;&#26500;&#24314;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#31639;&#27861;&#19981;&#21463;&#35813;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.16291</link><description>&lt;p&gt;
RL&#26041;&#27861;&#30340;&#25928;&#29575;&#20998;&#31163;&#65306;&#26080;&#27169;&#22411;&#12289;&#26377;&#27169;&#22411;&#21644;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#25928;&#29575;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Efficiency Separation between RL Methods: Model-Free, Model-Based and Goal-Conditioned. (arXiv:2309.16291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#23384;&#22312;&#26681;&#26412;&#30340;&#38480;&#21046;&#65292;&#20294;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#26041;&#27861;&#21644;&#26500;&#24314;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#31639;&#27861;&#19981;&#21463;&#35813;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#25928;&#29575;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#36825;&#20010;&#38480;&#21046;&#36866;&#29992;&#20110;&#26080;&#27169;&#22411;&#30340;RL&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#30340;&#26377;&#27169;&#22411;&#26041;&#27861;&#65292;&#22914;&#26641;&#25628;&#32034;&#30340;&#35268;&#21010;&#12290;&#22312;&#23545;&#36825;&#31867;&#38382;&#39064;&#30340;&#25277;&#35937;&#23450;&#20041;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;RL&#38382;&#39064;&#65292;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#26469;&#35828;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#23547;&#25214;&#26368;&#20248;&#34892;&#20026;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#30028;&#26159;&#25351;&#25968;&#32423;&#30340;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#19968;&#31181;&#26041;&#27861;&#65292;&#19981;&#38024;&#23545;&#36825;&#20010;&#29305;&#23450;&#30340;&#38382;&#39064;&#23478;&#26063;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23478;&#26063;&#20013;&#30340;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#38480;&#21046;&#19981;&#36866;&#29992;&#20110;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20960;&#31181;&#26041;&#27861;&#65292;&#20363;&#22914;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#26041;&#27861;&#25110;&#26500;&#24314;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a fundamental limitation on the efficiency of a wide class of Reinforcement Learning (RL) algorithms. This limitation applies to model-free RL methods as well as a broad range of model-based methods, such as planning with tree search.  Under an abstract definition of this class, we provide a family of RL problems for which these methods suffer a lower bound exponential in the horizon for their interactions with the environment to find an optimal behavior. However, there exists a method, not tailored to this specific family of problems, which can efficiently solve the problems in the family.  In contrast, our limitation does not apply to several types of methods proposed in the literature, for instance, goal-conditioned methods or other algorithms that construct an inverse dynamics model.
&lt;/p&gt;</description></item><item><title>LawBench&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27861;&#24459;&#30693;&#35782;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#20174;&#35760;&#24518;&#65292;&#29702;&#35299;&#21644;&#24212;&#29992;&#19977;&#20010;&#23618;&#38754;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#27861;&#24459;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16289</link><description>&lt;p&gt;
LawBench: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27861;&#24459;&#30693;&#35782;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LawBench: Benchmarking Legal Knowledge of Large Language Models. (arXiv:2309.16289v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16289
&lt;/p&gt;
&lt;p&gt;
LawBench&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27861;&#24459;&#30693;&#35782;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#20174;&#35760;&#24518;&#65292;&#29702;&#35299;&#21644;&#24212;&#29992;&#19977;&#20010;&#23618;&#38754;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#27861;&#24459;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#39640;&#24230;&#19987;&#19994;&#21270;&#12289;&#23433;&#20840;&#20851;&#38190;&#30340;&#27861;&#24459;&#39046;&#22495;&#26102;&#65292;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#25152;&#20855;&#22791;&#30340;&#27861;&#24459;&#30693;&#35782;&#37327;&#20197;&#21450;&#23427;&#20204;&#26159;&#21542;&#33021;&#21487;&#38752;&#22320;&#25191;&#34892;&#19982;&#27861;&#24459;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;LawBench&#12290;LawBench&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#20174;&#19977;&#20010;&#35748;&#30693;&#23618;&#38754;&#23545;LLMs&#30340;&#27861;&#24459;&#33021;&#21147;&#36827;&#34892;&#31934;&#30830;&#35780;&#20272;&#65306;&#65288;1&#65289;&#27861;&#24459;&#30693;&#35782;&#35760;&#24518;&#65306;LLMs&#26159;&#21542;&#33021;&#22815;&#35760;&#20303;&#25152;&#38656;&#30340;&#27861;&#24459;&#27010;&#24565;&#12289;&#26465;&#27454;&#21644;&#20107;&#23454;&#65307;&#65288;2&#65289;&#27861;&#24459;&#30693;&#35782;&#29702;&#35299;&#65306;LLMs&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#12289;&#20107;&#20214;&#21644;&#20851;&#31995;&#65307;&#65288;3&#65289;&#27861;&#24459;&#30693;&#35782;&#24212;&#29992;&#65306;LLMs&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#36816;&#29992;&#33258;&#24049;&#30340;&#27861;&#24459;&#30693;&#35782;&#24182;&#36827;&#34892;&#24517;&#35201;&#30340;&#25512;&#29702;&#27493;&#39588;&#26469;&#35299;&#20915;&#29616;&#23454;&#30340;&#27861;&#24459;&#20219;&#21153;&#12290;LawBench&#21253;&#21547;20&#20010;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;5&#31181;&#20219;&#21153;&#31867;&#22411;&#65306;&#21333;&#26631;&#31614;&#20998;&#31867;&#65288;SLC&#65289;&#12289;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;MLC&#65289;&#12289;&#22238;&#24402;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated strong capabilities in various aspects. However, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. To address this gap, we propose a comprehensive evaluation benchmark LawBench. LawBench has been meticulously crafted to have precise assessment of the LLMs' legal capabilities from three cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal knowledge understanding: whether LLMs can comprehend entities, events and relationships within legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label classification (MLC), regression, e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#26500;&#32852;&#37030;&#20132;&#21449;&#30456;&#20851;&#21644;&#23454;&#20363;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38750;&#30446;&#26631;&#33976;&#39311;&#26469;&#35299;&#20915;&#27169;&#22411;&#24322;&#36136;&#24615;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24212;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16286</link><description>&lt;p&gt;
&#36890;&#29992;&#30340;&#24322;&#26500;&#32852;&#37030;&#20132;&#21449;&#30456;&#20851;&#21644;&#23454;&#20363;&#30456;&#20284;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning. (arXiv:2309.16286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#26500;&#32852;&#37030;&#20132;&#21449;&#30456;&#20851;&#21644;&#23454;&#20363;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38750;&#30446;&#26631;&#33976;&#39311;&#26469;&#35299;&#20915;&#27169;&#22411;&#24322;&#36136;&#24615;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24212;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#22810;&#26041;&#23398;&#20064;&#33539;&#24335;&#65292;&#28041;&#21450;&#19982;&#20182;&#20154;&#30340;&#21512;&#20316;&#23398;&#20064;&#21644;&#23545;&#31169;&#26377;&#25968;&#25454;&#30340;&#26412;&#22320;&#26356;&#26032;&#12290;&#27169;&#22411;&#24322;&#36136;&#24615;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#20004;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#24212;&#29992;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FCCL+&#26041;&#27861;&#65292;&#21363;&#32852;&#37030;&#30456;&#20851;&#24615;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#19982;&#38750;&#30446;&#26631;&#33976;&#39311;&#65292;&#20419;&#36827;&#20102;&#22495;&#20869;&#21306;&#20998;&#33021;&#21147;&#21644;&#22495;&#38388;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#20110;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#26080;&#20851;&#30340;&#26410;&#26631;&#35760;&#20844;&#20849;&#25968;&#25454;&#26469;&#36827;&#34892;&#24322;&#26500;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#26500;&#24314;&#20132;&#21449;&#30456;&#20851;&#30697;&#38453;&#65292;&#24182;&#22312;&#26631;&#24535;&#21644;&#29305;&#24449;&#27700;&#24179;&#19978;&#23545;&#23454;&#20363;&#30456;&#20284;&#24615;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#36890;&#20449;&#38556;&#30861;&#24182;&#25552;&#39640;&#20102;&#24191;&#27867;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#26412;&#22320;&#26356;&#26032;&#38454;&#27573;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;FCCL+&#24341;&#20837;&#20102;&#32852;&#37030;&#38750;&#30446;&#26631;&#33976;&#39311;&#65292;&#26082;&#20445;&#30041;&#20102;&#22495;&#38388;&#30693;&#35782;&#21448;&#36991;&#20813;&#20102;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an important privacy-preserving multi-party learning paradigm, involving collaborative learning with others and local updating on private data. Model heterogeneity and catastrophic forgetting are two crucial challenges, which greatly limit the applicability and generalizability. This paper presents a novel FCCL+, federated correlation and similarity learning with non-target distillation, facilitating the both intra-domain discriminability and inter-domain generalization. For heterogeneity issue, we leverage irrelevant unlabeled public data for communication between the heterogeneous participants. We construct cross-correlation matrix and align instance similarity distribution on both logits and feature levels, which effectively overcomes the communication barrier and improves the generalizable ability. For catastrophic forgetting in local updating stage, FCCL+ introduces Federated Non Target Distillation, which retains inter-domain knowledge while avoiding the opt
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#21477;&#23376;Transformer&#27169;&#22411;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#22312;&#38452;&#35851;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25104;&#32489;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.16275</link><description>&lt;p&gt;
UPB @ ACTI: &#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#21477;&#23376;Transformer&#26469;&#26816;&#27979;&#38452;&#35851;&#35770;
&lt;/p&gt;
&lt;p&gt;
UPB @ ACTI: Detecting Conspiracies using fine tuned Sentence Transformers. (arXiv:2309.16275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16275
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#21477;&#23376;Transformer&#27169;&#22411;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#22312;&#38452;&#35851;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25104;&#32489;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38452;&#35851;&#35770;&#24050;&#25104;&#20026;&#22312;&#32447;&#35752;&#35770;&#30340;&#37325;&#35201;&#21644;&#20196;&#20154;&#25285;&#24551;&#30340;&#26041;&#38754;&#65292;&#23545;&#20449;&#24687;&#23436;&#25972;&#24615;&#21644;&#31038;&#20250;&#20449;&#20219;&#26500;&#25104;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#38452;&#35851;&#35770;&#26816;&#27979;&#20316;&#20026;ACTI @ EVALITA 2023&#20849;&#20139;&#20219;&#21153;&#30340;&#25552;&#35758;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;Transformer&#27169;&#22411;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#32452;&#21512;&#20351;&#25105;&#20204;&#22312;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#26368;&#32456;&#25490;&#34892;&#27036;&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#33719;&#24471;&#20102;85.71%&#30340;F1&#20998;&#25968;&#65292;&#22312;&#31934;&#32454;&#38452;&#35851;&#20027;&#39064;&#20998;&#31867;&#20013;&#33719;&#24471;&#20102;91.23%&#30340;F1&#20998;&#25968;&#65292;&#36229;&#36807;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conspiracy theories have become a prominent and concerning aspect of online discourse, posing challenges to information integrity and societal trust. As such, we address conspiracy theory detection as proposed by the ACTI @ EVALITA 2023 shared task. The combination of pre-trained sentence Transformer models and data augmentation techniques enabled us to secure first place in the final leaderboard of both sub-tasks. Our methodology attained F1 scores of 85.71% in the binary classification and 91.23% for the fine-grained conspiracy topic classification, surpassing other competing systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#28608;&#21457;&#21512;&#20316;&#30340;&#31574;&#30053;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#21512;&#20316;&#31574;&#30053;&#21644;&#24341;&#20837;&#40723;&#21169;&#22242;&#38431;&#22238;&#25253;&#30340;&#20462;&#25913;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#29616;&#23454;&#22256;&#22659;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#22343;&#20540;&#22330;&#21338;&#24328;&#29702;&#35770;&#65292;&#24314;&#31435;&#20102;&#26080;&#38480;&#22823;&#26234;&#33021;&#20307;&#38598;&#21512;&#20013;&#30340;&#24179;&#34913;&#35299;&#21644;&#22870;&#21169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16263</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#21160;&#21147;&#23398;&#65306;&#25506;&#32034;&#20855;&#26377;&#22343;&#20540;&#22330;&#22343;&#34913;&#30340;&#21338;&#24328;&#29702;&#35770;&#24773;&#26223;
&lt;/p&gt;
&lt;p&gt;
Cooperation Dynamics in Multi-Agent Systems: Exploring Game-Theoretic Scenarios with Mean-Field Equilibria. (arXiv:2309.16263v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#28608;&#21457;&#21512;&#20316;&#30340;&#31574;&#30053;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#21512;&#20316;&#31574;&#30053;&#21644;&#24341;&#20837;&#40723;&#21169;&#22242;&#38431;&#22238;&#25253;&#30340;&#20462;&#25913;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#29616;&#23454;&#22256;&#22659;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#22343;&#20540;&#22330;&#21338;&#24328;&#29702;&#35770;&#65292;&#24314;&#31435;&#20102;&#26080;&#38480;&#22823;&#26234;&#33021;&#20307;&#38598;&#21512;&#20013;&#30340;&#24179;&#34913;&#35299;&#21644;&#22870;&#21169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#26159;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#36890;&#24120;&#35201;&#27714;&#26234;&#33021;&#20307;&#22312;&#20010;&#20307;&#25910;&#30410;&#21644;&#38598;&#20307;&#22238;&#25253;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22312;&#21338;&#24328;&#29702;&#35770;&#24773;&#26223;&#20013;&#28608;&#21457;&#21512;&#20316;&#30340;&#31574;&#30053;&#65292;&#20363;&#22914;&#36845;&#20195;&#22234;&#24466;&#22256;&#22659;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#24517;&#39035;&#20248;&#21270;&#20010;&#20307;&#21644;&#22242;&#38431;&#30340;&#32467;&#26524;&#12290;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#21512;&#20316;&#31574;&#30053;&#23545;&#20110;&#20419;&#36827;&#37325;&#22797;&#21338;&#24328;&#20013;&#22242;&#38431;&#23548;&#21521;&#34892;&#20026;&#30340;&#26377;&#25928;&#24615;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#65292;&#21363;&#40723;&#21169;&#22242;&#38431;&#22238;&#25253;&#20063;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#20010;&#20307;&#25910;&#30410;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#29616;&#23454;&#22256;&#22659;&#12290;&#30740;&#31350;&#36824;&#25193;&#23637;&#21040;&#26234;&#33021;&#20307;&#20154;&#21475;&#25351;&#25968;&#22686;&#38271;&#30340;&#24773;&#26223;&#65288;$N \longrightarrow +\infty$&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#35745;&#31639;&#21644;&#24179;&#34913;&#30830;&#23450;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21033;&#29992;&#22343;&#20540;&#22330;&#21338;&#24328;&#29702;&#35770;&#65292;&#24314;&#31435;&#20102;&#26080;&#38480;&#22823;&#26234;&#33021;&#20307;&#38598;&#21512;&#20013;&#30340;&#24179;&#34913;&#35299;&#21644;&#22870;&#21169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperation is fundamental in Multi-Agent Systems (MAS) and Multi-Agent Reinforcement Learning (MARL), often requiring agents to balance individual gains with collective rewards. In this regard, this paper aims to investigate strategies to invoke cooperation in game-theoretic scenarios, namely the Iterated Prisoner's Dilemma, where agents must optimize both individual and group outcomes. Existing cooperative strategies are analyzed for their effectiveness in promoting group-oriented behavior in repeated games. Modifications are proposed where encouraging group rewards will also result in a higher individual gain, addressing real-world dilemmas seen in distributed systems. The study extends to scenarios with exponentially growing agent populations ($N \longrightarrow +\infty$), where traditional computation and equilibrium determination are challenging. Leveraging mean-field game theory, equilibrium solutions and reward structures are established for infinitely large agent sets in repea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#38750;&#21442;&#25968;&#37327;&#23376;&#30005;&#36335;&#20026;&#22522;&#30784;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#65292;&#24182;&#23558;&#37327;&#23376;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#32435;&#20837;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#31526;&#21512;&#35201;&#27714;&#30340;&#38543;&#26426;&#21464;&#37327;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#37327;&#22823;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.16258</link><description>&lt;p&gt;
QonFusion -- &#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#30340;&#37327;&#23376;&#26041;&#27861;&#65306;&#22312;&#31283;&#23450;&#24615;&#25193;&#25955;&#21644;&#24067;&#26391;&#36816;&#21160;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
QonFusion -- Quantum Approaches to Gaussian Random Variables: Applications in Stable Diffusion and Brownian Motion. (arXiv:2309.16258v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#38750;&#21442;&#25968;&#37327;&#23376;&#30005;&#36335;&#20026;&#22522;&#30784;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#65292;&#24182;&#23558;&#37327;&#23376;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#32435;&#20837;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#31526;&#21512;&#35201;&#27714;&#30340;&#38543;&#26426;&#21464;&#37327;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#37327;&#22823;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20197;&#38750;&#21442;&#25968;&#37327;&#23376;&#30005;&#36335;&#20026;&#37325;&#28857;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#65288;GRVs&#65289;&#12290;&#36825;&#31181;&#20197;&#37327;&#23376;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#20316;&#20026;&#20256;&#32479;&#20266;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#65288;PRNGs&#65289;&#30340;&#26367;&#20195;&#21697;&#65292;&#22914;PyTorch&#20013;&#30340;\textbf{torch.rand}&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#39064;&#26159;&#23558;&#37327;&#23376;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#65288;QRNGs&#65289;&#32435;&#20837;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#37327;&#23376;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#29983;&#25104;&#22120;&#22312;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#21644;&#24067;&#26391;&#36816;&#21160;&#65288;BM&#65289;&#20013;&#21457;&#25381;&#20102;&#21452;&#37325;&#20316;&#29992;&#12290;&#36825;&#19982;&#20351;&#29992;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65288;PQCs&#65289;&#20197;&#21450;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#35299;&#31639;&#22120;&#65288;VQEs&#65289;&#30340;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#12290;&#23613;&#31649;&#20256;&#32479;&#25216;&#26415;&#21487;&#20197;&#20934;&#30830;&#22320;&#36817;&#20284;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#22522;&#24577;&#25110;&#24314;&#27169;&#31934;&#32454;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#35745;&#31639;&#37327;&#22823;&#30340;&#20248;&#21270;&#36807;&#31243;&#26469;&#35843;&#25972;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#38750;&#21442;&#25968;&#37327;&#23376;&#30005;&#36335;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#31526;&#21512;&#35201;&#27714;&#30340;&#38543;&#26426;&#21464;&#37327;&#65292;&#36991;&#20813;&#20102;&#36825;&#19968;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present study, we delineate a strategy focused on non-parametric quantum circuits for the generation of Gaussian random variables (GRVs). This quantum-centric approach serves as a substitute for conventional pseudorandom number generators (PRNGs), such as the \textbf{torch.rand} function in PyTorch. The principal theme of our research is the incorporation of Quantum Random Number Generators (QRNGs) into classical models of diffusion. Notably, our Quantum Gaussian Random Variable Generator fulfills dual roles, facilitating simulations in both Stable Diffusion (SD) and Brownian Motion (BM). This diverges markedly from prevailing methods that utilize parametric quantum circuits (PQCs), often in conjunction with variational quantum eigensolvers (VQEs). Although conventional techniques can accurately approximate ground states in complex systems or model elaborate probability distributions, they require a computationally demanding optimization process to tune parameters. Our non-param
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;CNN&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#23545;&#40481;&#34507;&#21463;&#31934;&#36827;&#34892;&#20102;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#23478;&#31165;&#23413;&#21270;&#22330;&#23454;&#36341;&#65307;&#36890;&#36807;&#23545;&#22235;&#31181;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;InceptionNet&#22312;&#20934;&#30830;&#24230;&#21644;&#24615;&#33021;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#22312;&#27979;&#35797;&#38598;&#20013;&#36798;&#21040;&#20102;0.98&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#23545;&#21487;&#23413;&#21270;&#21644;&#19981;&#21487;&#23413;&#21270;&#30340;&#40481;&#34507;&#37117;&#26377;&#39640;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16257</link><description>&lt;p&gt;
&#20351;&#29992;CNN&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#30340;&#38750;&#30772;&#22351;&#24615;&#40481;&#34507;&#21463;&#31934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Nondestructive chicken egg fertility detection using CNN-transfer learning algorithms. (arXiv:2309.16257v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;CNN&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#23545;&#40481;&#34507;&#21463;&#31934;&#36827;&#34892;&#20102;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#23478;&#31165;&#23413;&#21270;&#22330;&#23454;&#36341;&#65307;&#36890;&#36807;&#23545;&#22235;&#31181;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;InceptionNet&#22312;&#20934;&#30830;&#24230;&#21644;&#24615;&#33021;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#22312;&#27979;&#35797;&#38598;&#20013;&#36798;&#21040;&#20102;0.98&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#23545;&#21487;&#23413;&#21270;&#21644;&#19981;&#21487;&#23413;&#21270;&#30340;&#40481;&#34507;&#37117;&#26377;&#39640;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;CNN&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#23545;&#40481;&#34507;&#21463;&#31934;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#23478;&#31165;&#23413;&#21270;&#22330;&#23454;&#36341;&#12290;&#20351;&#29992;&#22686;&#24191;&#22270;&#20687;&#65288;&#26059;&#36716;&#12289;&#32763;&#36716;&#12289;&#32553;&#25918;&#12289;&#24179;&#31227;&#21644;&#21453;&#23556;&#65289;&#23545;&#22235;&#31181;&#27169;&#22411;&#65288;VGG16&#12289;ResNet50&#12289;InceptionNet&#21644;MobileNet&#65289;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#25968;&#25454;&#38598;&#21253;&#25324;200&#20010;&#21333;&#19968;&#40481;&#34507;&#22270;&#20687;&#12290;&#34429;&#28982;&#35757;&#32451;&#32467;&#26524;&#26174;&#31034;&#25152;&#26377;&#27169;&#22411;&#22343;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#34920;&#26126;&#23427;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#23398;&#20064;&#21644;&#20998;&#31867;&#40481;&#34507;&#30340;&#21463;&#31934;&#29366;&#24577;&#65292;&#20294;&#22312;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#26102;&#65292;&#20934;&#30830;&#24230;&#21644;&#24615;&#33021;&#23384;&#22312;&#24046;&#24322;&#12290;InceptionNet&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#31867;&#21487;&#23413;&#21270;&#21644;&#19981;&#21487;&#23413;&#21270;&#30340;&#40481;&#34507;&#12290;&#22312;&#35780;&#20272;&#25351;&#26631;&#30340;&#25152;&#26377;&#21442;&#25968;&#19978;&#65292;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#22312;&#27979;&#35797;&#38598;&#20013;&#65292;&#23427;&#23454;&#29616;&#20102;0.98&#30340;&#20934;&#30830;&#24230;&#65292;1&#30340;&#25935;&#24863;&#24615;&#26469;&#26816;&#27979;&#21487;&#23413;&#21270;&#30340;&#40481;&#34507;&#65292;&#24182;&#20197;0.96&#30340;&#29305;&#24322;&#24615;&#26469;&#35782;&#21035;&#19981;&#21487;&#23413;&#21270;&#30340;&#40481;&#34507;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explored the application of CNN-Transfer Learning for nondestructive chicken egg fertility detection for precision poultry hatchery practices. Four models, VGG16, ResNet50, InceptionNet, and MobileNet, were trained and evaluated on a dataset (200 single egg images) using augmented images (rotation, flip, scale, translation, and reflection). Although the training results demonstrated that all models achieved high accuracy, indicating their ability to accurately learn and classify chicken eggs' fertility state, when evaluated on the testing set, variations in accuracy and performance were observed. InceptionNet exhibited the best overall performance, accurately classifying fertile and non-fertile eggs. It demonstrated excellent performance in both training and testing sets in all parameters of the evaluation metrics. In testing set, it achieved an accuracy of 0.98, a sensitivity of 1 for detecting fertile eggs, and a specificity of 0.96 for identifying non-fertile eggs. The hi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#24046;&#24322;&#32422;&#26463;&#25512;&#24191;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#20272;&#35745;&#26041;&#27861;&#30340;&#38656;&#35201;&#24182;&#31616;&#21270;&#20102;&#22870;&#21169;&#21644;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.16240</link><description>&lt;p&gt;
&#36229;&#36234;&#36870;KL&#65306;&#36890;&#36807;&#22810;&#26679;&#30340;&#24046;&#24322;&#32422;&#26463;&#25512;&#24191;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints. (arXiv:2309.16240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#24046;&#24322;&#32422;&#26463;&#25512;&#24191;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#20272;&#35745;&#26041;&#27861;&#30340;&#38656;&#35201;&#24182;&#31616;&#21270;&#20102;&#22870;&#21169;&#21644;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19981;&#26029;&#22686;&#24378;&#33021;&#21147;&#20026;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20294;&#21516;&#26102;&#20063;&#25918;&#22823;&#20102;&#23433;&#20840;&#38382;&#39064;&#65292;&#22914;AI&#31995;&#32479;&#30340;&#28508;&#22312;&#28389;&#29992;&#65292;&#36825;&#38656;&#35201;&#26377;&#25928;&#30340;AI&#23545;&#40784;&#12290;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;AI&#23545;&#40784;&#30340;&#19968;&#26465;&#26377;&#24076;&#26395;&#30340;&#36335;&#24452;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#21644;&#23545;&#29420;&#31435;&#22870;&#21169;&#27169;&#22411;&#30340;&#20381;&#36182;&#24615;&#32780;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#22312;&#36870;KL&#27491;&#21017;&#21270;&#32422;&#26463;&#19979;&#31561;&#21516;&#20110;RLHF&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;f-DPO&#65292;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#22810;&#26679;&#30340;&#24046;&#24322;&#32422;&#26463;&#26469;&#25512;&#24191;DPO&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;f-&#25955;&#24230;&#19979;&#65292;&#21253;&#25324;Jensen-Shannon&#25955;&#24230;&#12289;&#27491;&#21521;KL&#25955;&#24230;&#21644;&#945;-&#25955;&#24230;&#65292;&#22870;&#21169;&#21644;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20063;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;Karush-Kuhn-Tucker&#26465;&#20214;&#26469;&#31616;&#21270;&#12290;&#36825;&#28040;&#38500;&#20102;&#23545;&#20272;&#35745;&#26041;&#27861;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative, and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the need for estimat
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#20026;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#21644;&#20998;&#23376;&#35774;&#35745;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#22836;&#35774;&#35745;&#33647;&#29289;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#21453;&#24212;&#21270;&#23398;&#12290;&#21516;&#26102;&#65292;&#24320;&#28304;&#36719;&#20214;&#36164;&#28304;&#38477;&#20302;&#20102;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#30340;&#38376;&#27099;&#65292;&#26410;&#26469;&#23558;&#32467;&#21512;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#35745;&#31639;&#21270;&#23398;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.16235</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Language models in molecular discovery. (arXiv:2309.16235v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16235
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#20026;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#21644;&#20998;&#23376;&#35774;&#35745;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#22836;&#35774;&#35745;&#33647;&#29289;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#21453;&#24212;&#21270;&#23398;&#12290;&#21516;&#26102;&#65292;&#24320;&#28304;&#36719;&#20214;&#36164;&#28304;&#38477;&#20302;&#20102;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#30340;&#38376;&#27099;&#65292;&#26410;&#26469;&#23558;&#32467;&#21512;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#35745;&#31639;&#21270;&#23398;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24050;&#32463;&#28183;&#36879;&#21040;&#20854;&#20182;&#39046;&#22495;&#65292;&#20986;&#29616;&#20102;&#22312;&#23567;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#25110;&#32858;&#21512;&#29289;&#19978;&#36816;&#20316;&#30340;&#8220;&#31185;&#23398;&#35821;&#35328;&#27169;&#22411;&#8221;&#12290;&#22312;&#21270;&#23398;&#39046;&#22495;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#21152;&#36895;&#20998;&#23376;&#21457;&#29616;&#21608;&#26399;&#26041;&#38754;&#21457;&#25381;&#20102;&#20316;&#29992;&#65292;&#26368;&#36817;&#22312;&#26089;&#26399;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#26377;&#20102;&#26377;&#24076;&#26395;&#30340;&#21457;&#29616;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#35282;&#33394;&#65292;&#24378;&#35843;&#20102;&#20174;&#22836;&#35774;&#35745;&#33647;&#29289;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#21453;&#24212;&#21270;&#23398;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#26377;&#20215;&#20540;&#30340;&#24320;&#28304;&#36719;&#20214;&#36164;&#28304;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#30340;&#38376;&#27099;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21246;&#30011;&#20102;&#26410;&#26469;&#20998;&#23376;&#35774;&#35745;&#30340;&#24895;&#26223;&#65292;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#30028;&#38754;&#19982;&#35745;&#31639;&#21270;&#23398;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#21270;&#23398;&#23478;&#21644;&#23545;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21152;&#36895;&#21270;&#23398;&#21457;&#29616;&#24863;&#20852;&#36259;&#30340;&#20154;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of language models, especially transformer-based architectures, has trickled into other domains giving rise to "scientific language models" that operate on small molecules, proteins or polymers. In chemistry, language models contribute to accelerating the molecule discovery cycle as evidenced by promising recent findings in early-stage drug discovery. Here, we review the role of language models in molecular discovery, underlining their strength in de novo drug design, property prediction and reaction chemistry. We highlight valuable open-source software assets thus lowering the entry barrier to the field of scientific language modeling. Last, we sketch a vision for future molecular design that combines a chatbot interface with access to computational chemistry tools. Our contribution serves as a valuable resource for researchers, chemists, and AI enthusiasts interested in understanding how language models can and will be used to accelerate chemical discovery.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;GInX-Eval&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35780;&#20272;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.16223</link><description>&lt;p&gt;
GInX-Eval: &#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations. (arXiv:2309.16223v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;GInX-Eval&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35780;&#20272;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20026;&#20102;&#31361;&#20986;&#22270;&#20013;&#23545;&#27169;&#22411;&#39044;&#27979;&#26368;&#26377;&#36129;&#29486;&#30340;&#36793;&#21644;&#33410;&#28857;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#21508;&#31181;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20174;&#20154;&#31867;&#25110;&#27169;&#22411;&#30340;&#35282;&#24230;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#27491;&#30830;&#24615;&#12290;&#24403;&#21069;&#35780;&#20272;&#36807;&#31243;&#20013;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#29942;&#39048;&#38382;&#39064;&#26159;&#35299;&#37322;&#30340;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#20250;&#24433;&#21709;&#21040;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#27969;&#34892;&#30340;&#24544;&#23454;&#24230;&#25110;&#20445;&#30495;&#24230;&#24471;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24544;&#23454;&#24230;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GInX-Eval&#65288;&#22270;&#20869;&#20998;&#24067;&#35299;&#37322;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#35299;&#37322;&#30340;&#36807;&#31243;&#65292;&#20811;&#26381;&#20102;&#24544;&#23454;&#24230;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35299;&#37322;&#26041;&#27861;&#30340;&#26032;&#35265;&#35299;&#12290;&#20351;&#29992;&#37325;&#26032;&#35757;&#32451;&#31574;&#30053;&#65292;GInX&#24471;&#20998;&#21487;&#34913;&#37327;&#24050;&#31227;&#38500;&#36793;&#23545;&#27169;&#22411;&#30340;&#20449;&#24687;&#37327;&#20197;&#21450;&#36793;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diverse explainability methods of graph neural networks (GNN) have recently been developed to highlight the edges and nodes in the graph that contribute the most to the model predictions. However, it is not clear yet how to evaluate the correctness of those explanations, whether it is from a human or a model perspective. One unaddressed bottleneck in the current evaluation procedure is the problem of out-of-distribution explanations, whose distribution differs from those of the training data. This important issue affects existing evaluation metrics such as the popular faithfulness or fidelity score. In this paper, we show the limitations of faithfulness metrics. We propose GInX-Eval (Graph In-distribution eXplanation Evaluation), an evaluation procedure of graph explanations that overcomes the pitfalls of faithfulness and offers new insights on explainability methods. Using a retraining strategy, the GInX score measures how informative removed edges are for the model and the EdgeRank s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;ODD&#26816;&#27979;&#65292;&#20026;&#20102;&#23454;&#29616;&#22312;&#23454;&#38469;&#21307;&#30103;&#31995;&#32479;&#20013;&#21487;&#38752;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#36991;&#20813;&#23545;ODD&#25968;&#25454;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#35813;&#22522;&#20934;&#21033;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;ICU&#24739;&#32773;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#22810;&#31181;&#26041;&#27861;&#21644;&#39044;&#27979;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16220</link><description>&lt;p&gt;
&#25581;&#31034;&#21464;&#33394;&#40857;&#65306;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;ODD&#26816;&#27979;&#30340;&#22522;&#20934;&#12290; (arXiv:2309.16220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in Medical Tabular Data. (arXiv:2309.16220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;ODD&#26816;&#27979;&#65292;&#20026;&#20102;&#23454;&#29616;&#22312;&#23454;&#38469;&#21307;&#30103;&#31995;&#32479;&#20013;&#21487;&#38752;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#36991;&#20813;&#23545;ODD&#25968;&#25454;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#35813;&#22522;&#20934;&#21033;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;ICU&#24739;&#32773;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#22810;&#31181;&#26041;&#27861;&#21644;&#39044;&#27979;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#26469;&#33258;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#19978;&#27809;&#26377;&#26377;&#25928;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#21487;&#38752;&#22320;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21307;&#30103;&#31995;&#32479;&#20013;&#20351;&#29992;ML&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#23545;ODD&#25968;&#25454;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#26816;&#27979;ODD&#26679;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;&#20854;&#20182;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;ODD&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#24403;&#22788;&#29702;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#26159;&#21542;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#19968;&#36843;&#20999;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#21487;&#22797;&#21046;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#20010;&#27979;&#35797;&#26469;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#36817;&#21644;&#36828;ODDs&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21033;&#29992;&#20102;&#26368;&#26032;&#29256;&#26412;&#30340;eICU&#21644;MIMIC-IV&#65292;&#36825;&#26159;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#25968;&#19975;&#21517;ICU&#24739;&#32773;&#22312;&#22810;&#23478;&#21307;&#38498;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21508;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#26041;&#27861;&#21644;SOTA&#21518;&#32622;&#26816;&#27979;&#22120;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#39044;&#27979;&#26550;&#26500;&#65292;&#21253;&#25324;MLP&#12289;ResNet&#21644;Transformer&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their success, Machine Learning (ML) models do not generalize effectively to data not originating from the training distribution. To reliably employ ML models in real-world healthcare systems and avoid inaccurate predictions on out-of-distribution (OOD) data, it is crucial to detect OOD samples. Numerous OOD detection approaches have been suggested in other fields - especially in computer vision - but it remains unclear whether the challenge is resolved when dealing with medical tabular data. To answer this pressing need, we propose an extensive reproducible benchmark to compare different methods across a suite of tests including both near and far OODs. Our benchmark leverages the latest versions of eICU and MIMIC-IV, two public datasets encompassing tens of thousands of ICU patients in several hospitals. We consider a wide array of density-based methods and SOTA post-hoc detectors across diverse predictive architectures, including MLP, ResNet, and Transformer. Our findings sho
&lt;/p&gt;</description></item><item><title>VDC&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#25968;&#25454;&#28165;&#27905;&#22120;&#65292;&#36890;&#36807;&#26816;&#27979;&#22270;&#20687;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#19981;&#19968;&#33268;&#24615;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#30340;&#33039;&#26679;&#26412;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16211</link><description>&lt;p&gt;
VDC: &#36890;&#36807;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#33039;&#26679;&#26412;&#30340;&#22810;&#21151;&#33021;&#25968;&#25454;&#28165;&#27905;&#22120;
&lt;/p&gt;
&lt;p&gt;
VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency. (arXiv:2309.16211v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16211
&lt;/p&gt;
&lt;p&gt;
VDC&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#25968;&#25454;&#28165;&#27905;&#22120;&#65292;&#36890;&#36807;&#26816;&#27979;&#22270;&#20687;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#19981;&#19968;&#33268;&#24615;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#30340;&#33039;&#26679;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#27010;&#24565;&#24378;&#35843;&#20102;&#25968;&#25454;&#22312;&#26500;&#24314;AI&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#21253;&#21547;&#33039;&#26679;&#26412;&#65292;&#20363;&#22914;&#26469;&#33258;&#21518;&#38376;&#25915;&#20987;&#30340;&#27602;&#26679;&#26412;&#65292;&#20247;&#21253;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#65292;&#29978;&#33267;&#26159;&#23427;&#20204;&#30340;&#28151;&#21512;&#20307;&#12290;&#36825;&#20123;&#33039;&#26679;&#26412;&#30340;&#23384;&#22312;&#20351;&#24471;DNNs&#21464;&#24471;&#33030;&#24369;&#21644;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#33039;&#26679;&#26412;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#21482;&#20851;&#27880;&#26816;&#27979;&#27602;&#26679;&#26412;&#25110;&#22122;&#22768;&#26631;&#31614;&#65292;&#24403;&#22788;&#29702;&#26469;&#33258;&#20854;&#20182;&#39046;&#22495;&#30340;&#33039;&#26679;&#26412;&#26102;&#65292;&#24448;&#24448;&#23481;&#26131;&#20986;&#29616;&#24369;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21508;&#31181;&#33039;&#26679;&#26412;&#20043;&#38388;&#30340;&#19968;&#20010;&#20849;&#21516;&#24615;&#26159;&#22270;&#20687;&#21644;&#30456;&#20851;&#26631;&#31614;&#20043;&#38388;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#25968;&#25454;&#28165;&#27905;&#22120;(VDC)&#65292;&#21033;&#29992;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLM)&#30340;&#36229;&#36807;&#33021;&#21147;&#12290;&#23427;&#30001;...&#65288;&#27492;&#22788;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22522;&#26412;&#21407;&#29702;&#20986;&#21457;&#30340;&#26356;&#19968;&#33324;&#30340;&#35786;&#26029;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#23558;&#27169;&#22411;&#35786;&#26029;&#25512;&#24191;&#20026;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#31995;&#32479;&#21644;&#35786;&#26029;&#37117;&#36866;&#29992;&#30340;&#26041;&#27861;&#12290;&#22312;&#30456;&#23545;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#35745;&#31639;&#20986;&#39318;&#36873;&#35786;&#26029;&#20505;&#36873;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.16180</link><description>&lt;p&gt;
&#20174;&#22522;&#26412;&#21407;&#29702;&#20986;&#21457;&#30340;&#26356;&#19968;&#33324;&#30340;&#35786;&#26029;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A More General Theory of Diagnosis from First Principles. (arXiv:2309.16180v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22522;&#26412;&#21407;&#29702;&#20986;&#21457;&#30340;&#26356;&#19968;&#33324;&#30340;&#35786;&#26029;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#23558;&#27169;&#22411;&#35786;&#26029;&#25512;&#24191;&#20026;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#31995;&#32479;&#21644;&#35786;&#26029;&#37117;&#36866;&#29992;&#30340;&#26041;&#27861;&#12290;&#22312;&#30456;&#23545;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#35745;&#31639;&#20986;&#39318;&#36873;&#35786;&#26029;&#20505;&#36873;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#35786;&#26029;&#26159;&#20154;&#24037;&#26234;&#33021;&#12289;&#24418;&#24335;&#26041;&#27861;&#21644;&#25511;&#21046;&#31561;&#19981;&#21516;&#31038;&#21306;&#20013;&#30340;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#35805;&#39064;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#31995;&#32479;&#21644;&#23547;&#25214;&#19981;&#21516;&#24418;&#24335;&#30340;&#35786;&#26029;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;Reiter&#30340;&#29702;&#35770;&#25512;&#24191;&#20026;&#23545;&#31995;&#32479;&#21644;&#35786;&#26029;&#31867;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#24046;&#24322;&#12290;&#36825;&#20010;&#26356;&#19968;&#33324;&#30340;&#22522;&#20110;&#22522;&#26412;&#21407;&#29702;&#30340;&#35786;&#26029;&#29702;&#35770;&#23558;&#26368;&#23567;&#35786;&#26029;&#23450;&#20041;&#20026;&#22312;&#20551;&#35774;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#39318;&#36873;&#35786;&#26029;&#20505;&#36873;&#38598;&#21512;&#12290;&#35745;&#31639;&#26368;&#23567;&#35786;&#26029;&#26159;&#36890;&#36807;&#25506;&#32034;&#35786;&#26029;&#20551;&#35774;&#31354;&#38388;&#12289;&#27979;&#35797;&#19968;&#32452;&#20551;&#35774;&#19982;&#31995;&#32479;&#27169;&#22411;&#21644;&#35266;&#23519;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#29983;&#25104;&#25490;&#38500;&#32487;&#25215;&#32773;&#21644;&#20854;&#20182;&#37096;&#20998;&#25628;&#32034;&#31354;&#38388;&#30340;&#20914;&#31361;&#26469;&#23454;&#29616;&#30340;&#12290;&#22312;&#30456;&#23545;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#35745;&#31639;&#20986;&#39318;&#36873;&#35786;&#26029;&#20505;&#36873;&#38598;&#21512;&#12290;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;s
&lt;/p&gt;
&lt;p&gt;
Model-based diagnosis has been an active research topic in different communities including artificial intelligence, formal methods, and control. This has led to a set of disparate approaches addressing different classes of systems and seeking different forms of diagnoses. In this paper, we resolve such disparities by generalising Reiter's theory to be agnostic to the types of systems and diagnoses considered. This more general theory of diagnosis from first principles defines the minimal diagnosis as the set of preferred diagnosis candidates in a search space of hypotheses. Computing the minimal diagnosis is achieved by exploring the space of diagnosis hypotheses, testing sets of hypotheses for consistency with the system's model and the observation, and generating conflicts that rule out successors and other portions of the search space. Under relatively mild assumptions, our algorithms correctly compute the set of preferred diagnosis candidates. The main difficulty here is that the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;ACE&#20195;&#29702;&#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#20013;&#30340;CoinRun&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#20027;&#20195;&#29702;&#22312;&#26032;&#29615;&#22659;&#19979;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26032;&#22870;&#21169;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20851;&#38190;&#24773;&#20917;&#19979;&#21463;&#20154;&#20449;&#20219;&#22320;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2309.16166</link><description>&lt;p&gt;
CoinRun: &#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CoinRun: Solving Goal Misgeneralisation. (arXiv:2309.16166v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;ACE&#20195;&#29702;&#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#20013;&#30340;CoinRun&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#20027;&#20195;&#29702;&#22312;&#26032;&#29615;&#22659;&#19979;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26032;&#22870;&#21169;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20851;&#38190;&#24773;&#20917;&#19979;&#21463;&#20154;&#20449;&#20219;&#22320;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#26159;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#20351;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#23558;&#20854;&#30446;&#26631;&#19982;&#20154;&#31867;&#24847;&#22270;&#21644;&#36947;&#24503;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ACE&#65288;&#27010;&#24565;&#25193;&#23637;&#31639;&#27861;&#65289;&#20195;&#29702;&#22914;&#20309;&#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#30340;&#19968;&#39033;&#20851;&#38190;&#26631;&#20934;&#25361;&#25112;&#65306;CoinRun&#25361;&#25112;&#12290;&#35813;&#20195;&#29702;&#22312;&#26032;&#29615;&#22659;&#20013;&#19981;&#20351;&#29992;&#20219;&#20309;&#26032;&#30340;&#22870;&#21169;&#20449;&#24687;&#12290;&#36825;&#34920;&#26126;&#33258;&#20027;&#20195;&#29702;&#21487;&#20197;&#22312;&#26032;&#39062;&#21644;&#20851;&#38190;&#30340;&#24773;&#20917;&#19979;&#21463;&#20154;&#20449;&#20219;&#22320;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal misgeneralisation is a key challenge in AI alignment -- the task of getting powerful Artificial Intelligences to align their goals with human intentions and human morality. In this paper, we show how the ACE (Algorithm for Concept Extrapolation) agent can solve one of the key standard challenges in goal misgeneralisation: the CoinRun challenge. It uses no new reward information in the new environment. This points to how autonomous agents could be trusted to act in human interests, even in novel and critical situations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#21487;&#39044;&#27979;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#19981;&#21487;&#20449;&#30340;&#22806;&#37096;&#21629;&#20196;&#36827;&#34892;&#22810;&#26426;&#22120;&#20154;&#21327;&#21516;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;Meta Bandit Sequential Greedy (MetaBSG)&#65292;&#33021;&#22815;&#22312;&#22806;&#37096;&#21629;&#20196;&#20219;&#24847;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.16161</link><description>&lt;p&gt;
&#22312;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#21033;&#29992;&#19981;&#21487;&#20449;&#21629;&#20196;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#21327;&#21516;: &#19968;&#31181;&#36172;&#21338;&#27425;&#27169;&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Untrustworthy Commands for Multi-Robot Coordination in Unpredictable Environments: A Bandit Submodular Maximization Approach. (arXiv:2309.16161v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#21487;&#39044;&#27979;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#19981;&#21487;&#20449;&#30340;&#22806;&#37096;&#21629;&#20196;&#36827;&#34892;&#22810;&#26426;&#22120;&#20154;&#21327;&#21516;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;Meta Bandit Sequential Greedy (MetaBSG)&#65292;&#33021;&#22815;&#22312;&#22806;&#37096;&#21629;&#20196;&#20219;&#24847;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#21487;&#39044;&#27979;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#19981;&#21487;&#20449;&#30340;&#22806;&#37096;&#21629;&#20196;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#21629;&#20196;&#26159;&#23545;&#26426;&#22120;&#20154;&#24314;&#35758;&#30340;&#21160;&#20316;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#20445;&#35777;&#26410;&#30693;&#65292;&#22240;&#27492;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#20449;&#30340;&#12290;&#36825;&#20123;&#21629;&#20196;&#21487;&#20197;&#30001;&#20154;&#24037;&#25805;&#20316;&#21592;&#25110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29983;&#25104;&#65292;&#24182;&#19988;&#23613;&#31649;&#19981;&#21487;&#20449;&#65292;&#20294;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#22797;&#26434;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#26469;&#28304;&#20110;&#22797;&#26434;&#30340;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#22914;&#30446;&#26631;&#36319;&#36394;&#12289;&#29615;&#22659;&#24314;&#27169;&#21644;&#21306;&#22495;&#30417;&#27979;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20449;&#24687;&#37325;&#21472;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#27425;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20803;&#36172;&#21338;&#39034;&#36138;&#24515;&#65288;MetaBSG&#65289;&#65292;&#21363;&#20351;&#22806;&#37096;&#21629;&#20196;&#20219;&#24847;&#31967;&#31957;&#65292;&#23427;&#20063;&#33021;&#22815;&#25552;&#20379;&#24615;&#33021;&#20445;&#35777;&#12290;MetaBSG&#21033;&#29992;&#20803;&#31639;&#27861;&#23398;&#20064;&#26426;&#22120;&#20154;&#26159;&#21542;&#24212;&#35813;&#36981;&#24490;&#21629;&#20196;&#65292;&#25110;&#32773;&#24212;&#35813;&#37319;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#27425;&#27169;&#21327;&#21516;&#31639;&#27861;&#65292;&#36172;&#21338;&#39034;&#21033;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of multi-agent coordination in unpredictable and partially-observable environments with untrustworthy external commands. The commands are actions suggested to the robots, and are untrustworthy in that their performance guarantees, if any, are unknown. Such commands may be generated by human operators or machine learning algorithms and, although untrustworthy, can often increase the robots' performance in complex multi-robot tasks. We are motivated by complex multi-robot tasks such as target tracking, environmental mapping, and area monitoring. Such tasks are often modeled as submodular maximization problems due to the information overlap among the robots. We provide an algorithm, Meta Bandit Sequential Greedy (MetaBSG), which enjoys performance guarantees even when the external commands are arbitrarily bad. MetaBSG leverages a meta-algorithm to learn whether the robots should follow the commands or a recently developed submodular coordination algorithm, Bandit Sequ
&lt;/p&gt;</description></item><item><title>AE-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#30417;&#27979;&#25253;&#21578;&#20013;&#25552;&#21462;&#19981;&#33391;&#20107;&#20214;&#65292;&#35813;&#30740;&#31350;&#20197;&#27969;&#24863;&#30123;&#33495;&#19981;&#33391;&#20107;&#20214;&#20026;&#20363;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#32463;&#36807;&#24494;&#35843;&#30340;GPT 3.5&#27169;&#22411;&#65288;AE-GPT&#65289;&#22312;&#20005;&#26684;&#21305;&#37197;&#21644;&#28789;&#27963;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#26174;&#31034;&#20102;LLMs&#22312;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21487;&#25512;&#24191;&#21040;&#20854;&#20182;&#19981;&#33391;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.16150</link><description>&lt;p&gt;
AE-GPT:&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30417;&#27979;&#25253;&#21578;&#20013;&#25552;&#21462;&#19981;&#33391;&#20107;&#20214;-&#20197;&#27969;&#24863;&#30123;&#33495;&#19981;&#33391;&#20107;&#20214;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events. (arXiv:2309.16150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16150
&lt;/p&gt;
&lt;p&gt;
AE-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#30417;&#27979;&#25253;&#21578;&#20013;&#25552;&#21462;&#19981;&#33391;&#20107;&#20214;&#65292;&#35813;&#30740;&#31350;&#20197;&#27969;&#24863;&#30123;&#33495;&#19981;&#33391;&#20107;&#20214;&#20026;&#20363;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#32463;&#36807;&#24494;&#35843;&#30340;GPT 3.5&#27169;&#22411;&#65288;AE-GPT&#65289;&#22312;&#20005;&#26684;&#21305;&#37197;&#21644;&#28789;&#27963;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#26174;&#31034;&#20102;LLMs&#22312;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21487;&#25512;&#24191;&#21040;&#20854;&#20182;&#19981;&#33391;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30123;&#33495;&#22312;&#20840;&#29699;&#20581;&#24247;&#12289;&#20943;&#36731;&#20256;&#26579;&#30149;&#21644;&#22823;&#27969;&#34892;&#29190;&#21457;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20598;&#23572;&#20063;&#20250;&#23548;&#33268;&#19981;&#33391;&#20107;&#20214;&#65288;AEs&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26377;&#25928;&#35782;&#21035;&#21644;&#32534;&#30446;&#20020;&#24202;&#25253;&#21578;&#20013;&#30340;&#19981;&#33391;&#20107;&#20214;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;1990&#24180;&#33267;2016&#24180;&#26469;&#33258;&#30123;&#33495;&#19981;&#33391;&#20107;&#20214;&#25253;&#21578;&#31995;&#32479;&#65288;VAERS&#65289;&#30340;&#25968;&#25454;&#65292;&#29305;&#21035;&#20851;&#27880;&#35780;&#20272;LLMs&#22312;&#19981;&#33391;&#20107;&#20214;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20351;&#29992;&#27969;&#24863;&#30123;&#33495;&#20316;&#20026;&#20351;&#29992;&#26696;&#20363;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#20027;&#27969;&#30340;LLMs&#65292;&#21253;&#25324;GPT-2&#12289;GPT-3&#21464;&#20307;&#12289;GPT-4&#21644;Llama 2&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;GPT 3.5&#27169;&#22411;&#65288;AE-GPT&#65289;&#22312;&#20005;&#26684;&#21305;&#37197;&#26041;&#38754;&#30340;&#24179;&#22343;&#24494; F1 &#20998;&#25968;&#20026;0.704&#65292;&#28789;&#27963;&#21305;&#37197;&#26041;&#38754;&#20026;0.816&#12290;AE-GPT&#30340;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#31361;&#26174;&#20102;LLMs&#22312;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#34920;&#26126;&#22312;&#20808;&#36827;&#30340;AE&#26816;&#27979;&#26041;&#38754;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#22240;&#27492;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;AE&#25552;&#21462;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though Vaccines are instrumental in global health, mitigating infectious diseases and pandemic outbreaks, they can occasionally lead to adverse events (AEs). Recently, Large Language Models (LLMs) have shown promise in effectively identifying and cataloging AEs within clinical reports. Utilizing data from the Vaccine Adverse Event Reporting System (VAERS) from 1990 to 2016, this study particularly focuses on AEs to evaluate LLMs' capability for AE extraction. A variety of prevalent LLMs, including GPT-2, GPT-3 variants, GPT-4, and Llama 2, were evaluated using Influenza vaccine as a use case. The fine-tuned GPT 3.5 model (AE-GPT) stood out with a 0.704 averaged micro F1 score for strict match and 0.816 for relaxed match. The encouraging performance of the AE-GPT underscores LLMs' potential in processing medical data, indicating a significant stride towards advanced AE detection, thus presumably generalizable to other AE extraction tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;T-COL&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#19981;&#20165;&#33021;&#22815;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#30340;&#21407;&#22240;&#65292;&#36824;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#24314;&#35758;&#32473;&#29992;&#25143;&#12290;&#36890;&#36807;&#23558;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#26144;&#23556;&#21040;CEs&#30340;&#23646;&#24615;&#19978;&#65292;&#20197;&#21450;&#37319;&#29992;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;T-COL&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#25361;&#25112;&#24182;&#20445;&#25345;&#20581;&#22766;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16146</link><description>&lt;p&gt;
T-COL: &#20026;&#21487;&#21464;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#29983;&#25104;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
T-COL: Generating Counterfactual Explanations for General User Preferences on Variable Machine Learning Systems. (arXiv:2309.16146v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;T-COL&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#19981;&#20165;&#33021;&#22815;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#30340;&#21407;&#22240;&#65292;&#36824;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#24314;&#35758;&#32473;&#29992;&#25143;&#12290;&#36890;&#36807;&#23558;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#26144;&#23556;&#21040;CEs&#30340;&#23646;&#24615;&#19978;&#65292;&#20197;&#21450;&#37319;&#29992;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;T-COL&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#25361;&#25112;&#24182;&#20445;&#25345;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#12290;CEs&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#23427;&#20204;&#19981;&#20165;&#35299;&#37322;&#20026;&#20160;&#20040;&#20250;&#39044;&#27979;&#26576;&#20010;&#29305;&#23450;&#32467;&#26524;&#65292;&#36824;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#24314;&#35758;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;CEs&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#38480;&#21046;&#65292;&#21363;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#21644;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#29305;&#21035;&#26159;&#65292;&#29992;&#25143;&#20559;&#22909;&#24448;&#24448;&#26159;&#19968;&#33324;&#24615;&#30340;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#29305;&#24449;&#20540;&#12290;&#27492;&#22806;&#65292;CEs&#38656;&#35201;&#26681;&#25454;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#21464;&#24615;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#19988;&#22312;&#36825;&#20123;&#39564;&#35777;&#27169;&#22411;&#21457;&#29983;&#21464;&#21270;&#26102;&#20173;&#28982;&#20445;&#25345;&#20581;&#22766;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21487;&#33021;&#39564;&#35777;&#30340;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;CEs&#30340;&#23646;&#24615;&#19978;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;T-COL&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#20004;&#31181;&#21487;&#36873;&#32467;&#26500;&#21644;&#20960;&#32452;&#21327;&#21516;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) based systems have been suffering a lack of interpretability. To address this problem, counterfactual explanations (CEs) have been proposed. CEs are unique as they provide workable suggestions to users, in addition to explaining why a certain outcome was predicted. However, the application of CEs has been hindered by two main challenges, namely general user preferences and variable ML systems. User preferences, in particular, tend to be general rather than specific feature values. Additionally, CEs need to be customized to suit the variability of ML models, while also maintaining robustness even when these validation models change. To overcome these challenges, we propose several possible general user preferences that have been validated by user research and map them to the properties of CEs. We also introduce a new method called \uline{T}ree-based \uline{C}onditions \uline{O}ptional \uline{L}inks (T-COL), which has two optional structures and several groups of co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#26679;&#26412;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#20013;&#26080;&#27861;&#33719;&#21462;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16143</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#20248;&#21270;&#21512;&#25104;&#26679;&#26412;&#30340;&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples. (arXiv:2309.16143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#26679;&#26412;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#20013;&#26080;&#27861;&#33719;&#21462;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#20351;&#29992;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#30001;&#20110;&#27861;&#24459;&#38480;&#21046;&#65288;&#20363;&#22914;&#65292;GDPR&#65289;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#26412;&#25991;&#30740;&#31350;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#22312;&#27809;&#26377;&#23454;&#38469;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#21322;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20174;&#21253;&#21547;&#25968;&#30334;&#19975;&#26679;&#26412;&#30340;&#22810;&#26679;&#39046;&#22495;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;ImageNet&#65289;&#35757;&#32451;&#30340;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#35782;&#21035;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#20013;&#20223;&#30495;&#26080;&#26631;&#31614;&#26679;&#26412;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#26469;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#65306;&#65288;i&#65289;&#20803;&#23398;&#20064;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#21644;&#65288;ii&#65289;&#20351;&#29992;&#30495;&#23454;&#26631;&#35760;&#26679;&#26412;&#21644;&#21512;&#25104;&#26679;&#26412;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) is a promising approach for training deep classification models using labeled and unlabeled datasets. However, existing SSL methods rely on a large unlabeled dataset, which may not always be available in many real-world applications due to legal constraints (e.g., GDPR). In this paper, we investigate the research question: Can we train SSL models without real unlabeled datasets? Instead of using real unlabeled datasets, we propose an SSL method using synthetic datasets generated from generative foundation models trained on datasets containing millions of samples in diverse domains (e.g., ImageNet). Our main concepts are identifying synthetic samples that emulate unlabeled samples from generative foundation models and training classifiers using these synthetic samples. To achieve this, our method is formulated as an alternating optimization problem: (i) meta-learning of generative foundation models and (ii) SSL of classifiers using real labeled and synthet
&lt;/p&gt;</description></item><item><title>ModuLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#12289;&#33021;&#22815;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#25903;&#25345;3&#27604;&#29305;LLMs&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#30340;&#38598;&#25104;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.16119</link><description>&lt;p&gt;
ModuLoRA:&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#38598;&#25104;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#23545;3 Bit LLMs&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers. (arXiv:2309.16119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16119
&lt;/p&gt;
&lt;p&gt;
ModuLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#12289;&#33021;&#22815;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#25903;&#25345;3&#27604;&#29305;LLMs&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#30340;&#38598;&#25104;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#31639;&#27861;&#65292;&#21487;&#25903;&#25345;&#22312;&#20165;&#20351;&#29992;1&#20010;48GB GPU&#19978;&#20197;3&#27604;&#29305;&#25110;4&#27604;&#29305;&#31934;&#24230;&#24494;&#35843;&#20855;&#26377;65B&#21442;&#25968;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#27169;&#22359;&#21270;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;ModuLoRA&#65289;&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#23558;&#20219;&#20309;&#29992;&#25143;&#25351;&#23450;&#30340;&#26435;&#37325;&#37327;&#21270;&#22120;&#19982;&#24494;&#35843;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#37327;&#21270;&#26080;&#20851;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#33258;&#23450;&#20041;&#30340;&#40657;&#30418;&#37327;&#21270;&#27169;&#22359;&#20174;&#20302;&#31934;&#24230;LLM&#26435;&#37325;&#20013;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#39318;&#27425;&#33021;&#22815;&#36827;&#34892;3&#27604;&#29305;LLMs&#30340;&#24494;&#35843;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;3&#27604;&#29305;OPTQ&#37327;&#21270;&#24448;&#24448;&#20248;&#20110;&#20381;&#36182;&#20110;&#36739;&#19981;&#22797;&#26434;&#30340;4&#27604;&#29305;&#21644;8&#27604;&#29305;&#26041;&#27861;&#30340;&#24494;&#35843;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;ModuLoRA&#22312;&#25991;&#26412;&#20998;&#31867;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#30340;&#20869;&#23384;&#27604;&#29616;&#26377;&#26041;&#27861;&#23569;&#24456;&#22810;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#25688;&#35201;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;ROUGE&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit precision on as little as one 48GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 3-bit LLMs for the first time--leveraging state-of-the-art 3-bit OPTQ quantization often outperforms finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. W
&lt;/p&gt;</description></item><item><title>E2Net&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#23376;&#32593;&#33976;&#39311;&#21644;&#31934;&#30830;&#30340;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#36739;&#23567;&#30340;&#36951;&#24536;&#65292;&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38480;&#21046;&#19979;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#22788;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.16117</link><description>&lt;p&gt;
E2Net: &#24377;&#24615;&#25193;&#23637;&#32593;&#32476;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E2Net: Resource-Efficient Continual Learning with Elastic Expansion Network. (arXiv:2309.16117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16117
&lt;/p&gt;
&lt;p&gt;
E2Net&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#23376;&#32593;&#33976;&#39311;&#21644;&#31934;&#30830;&#30340;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#36739;&#23567;&#30340;&#36951;&#24536;&#65292;&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38480;&#21046;&#19979;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#22788;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#28040;&#38500;&#20197;&#21069;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#25345;&#32493;&#23398;&#20064;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#23384;&#20648;&#23481;&#37327;&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24377;&#24615;&#25193;&#23637;&#32593;&#32476;&#65288;E2Net&#65289;&#12290;&#36890;&#36807;&#26680;&#24515;&#23376;&#32593;&#33976;&#39311;&#21644;&#31934;&#30830;&#30340;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#65292;E2Net&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38480;&#21046;&#19979;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#21644;&#36739;&#23567;&#30340;&#36951;&#24536;&#65292;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#22788;&#29702;&#26102;&#38388;&#12290;&#22312;E2Net&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20195;&#34920;&#24615;&#32593;&#32476;&#33976;&#39311;&#65292;&#36890;&#36807;&#35780;&#20272;&#21442;&#25968;&#25968;&#37327;&#21644;&#19982;&#24037;&#20316;&#32593;&#32476;&#30340;&#36755;&#20986;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#20195;&#34920;&#24615;&#30340;&#26680;&#24515;&#23376;&#32593;&#65292;&#33976;&#39311;&#24037;&#20316;&#32593;&#32476;&#20869;&#30340;&#31867;&#20284;&#23376;&#32593;&#20197;&#20943;&#36731;&#23545;&#37325;&#28436;&#32531;&#20914;&#21306;&#30340;&#20381;&#36182;&#65292;&#24182;&#20419;&#36827;&#36328;&#20808;&#21069;&#20219;&#21153;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#20026;&#20102;&#25552;&#39640;&#23384;&#20648;&#36164;&#28304;&#21033;&#29992;&#29575;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23376;&#32593;&#32422;&#26463;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning methods are designed to learn new tasks without erasing previous knowledge. However, Continual Learning often requires massive computational power and storage capacity for satisfactory performance. In this paper, we propose a resource-efficient continual learning method called the Elastic Expansion Network (E2Net). Leveraging core subnet distillation and precise replay sample selection, E2Net achieves superior average accuracy and diminished forgetting within the same computational and storage constraints, all while minimizing processing time. In E2Net, we propose Representative Network Distillation to identify the representative core subnet by assessing parameter quantity and output similarity with the working network, distilling analogous subnets within the working network to mitigate reliance on rehearsal buffers and facilitating knowledge transfer across previous tasks. To enhance storage resource utilization, we then propose Subnet Constraint Experience Replay t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.16108</link><description>&lt;p&gt;
&#39057;&#36947;&#35270;&#35273;Transformer&#65306;&#19968;&#24352;&#22270;&#20540;C x 16 x 16&#20010;&#35789;
&lt;/p&gt;
&lt;p&gt;
Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26576;&#20123;&#22270;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#22914;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#65292;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#36890;&#36947;&#65292;&#27599;&#20010;&#36890;&#36947;&#37117;&#25658;&#24102;&#30528;&#35821;&#20041;&#19978;&#19981;&#21516;&#21644;&#29420;&#31435;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#36755;&#20837;&#36890;&#36947;&#30340;&#31232;&#30095;&#24615;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#36807;&#31243;&#20013;&#21487;&#33021;&#27809;&#26377;&#23494;&#38598;&#21487;&#29992;&#30340;&#36890;&#36947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#65292;&#22686;&#24378;&#20102;&#23545;&#36755;&#20837;&#36890;&#36947;&#20043;&#38388;&#30340;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;(HCS)&#20316;&#20026;&#19968;&#31181;&#38468;&#21152;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20165;&#20986;&#29616;&#37096;&#20998;&#36890;&#36947;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;ChannelViT&#29420;&#31435;&#22320;&#26500;&#24314;&#34917;&#19969;&#20196;&#29260;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36890;&#36947;&#23884;&#20837;&#23558;&#20854;&#28155;&#21152;&#21040;&#34917;&#19969;&#20196;&#29260;&#20013;&#65292;&#31867;&#20284;&#20110;&#20301;&#32622;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25928;&#29992;&#30340;&#21306;&#38388;&#35268;&#21017;&#25366;&#25496;&#31639;&#27861;&#65288;UIRMiner&#65289;&#65292;&#21487;&#20197;&#20174;&#21306;&#38388;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#25152;&#26377;&#22522;&#20110;&#25928;&#29992;&#30340;&#21306;&#38388;&#35268;&#21017;&#65288;UIRs&#65289;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16102</link><description>&lt;p&gt;
&#21457;&#29616;&#22522;&#20110;&#25928;&#29992;&#30340;&#21306;&#38388;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Discovering Utility-driven Interval Rules. (arXiv:2309.16102v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25928;&#29992;&#30340;&#21306;&#38388;&#35268;&#21017;&#25366;&#25496;&#31639;&#27861;&#65288;UIRMiner&#65289;&#65292;&#21487;&#20197;&#20174;&#21306;&#38388;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#25152;&#26377;&#22522;&#20110;&#25928;&#29992;&#30340;&#21306;&#38388;&#35268;&#21017;&#65288;UIRs&#65289;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#65292;&#39640;&#25928;&#29992;&#24207;&#21015;&#35268;&#21017;&#25366;&#25496;&#65288;HUSRM&#65289;&#26159;&#19968;&#31181;&#33021;&#22815;&#25581;&#31034;&#24207;&#21015;&#20013;&#20107;&#20214;&#20043;&#38388;&#20851;&#32852;&#30340;&#30693;&#35782;&#21457;&#29616;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20016;&#23500;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#39640;&#25928;&#29992;&#24207;&#21015;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#19982;&#22522;&#20110;&#28857;&#30340;&#24207;&#21015;&#30456;&#20851;&#12290;&#23384;&#22312;&#19968;&#20123;&#25345;&#32493;&#19968;&#27573;&#26102;&#38388;&#30340;&#21306;&#38388;&#20107;&#20214;&#12290;&#20256;&#32479;&#30340;&#21306;&#38388;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#21457;&#29616;&#20219;&#21153;&#20027;&#35201;&#20851;&#27880;&#27169;&#24335;&#21457;&#29616;&#65292;&#20294;&#26159;&#27169;&#24335;&#26080;&#27861;&#24456;&#22909;&#22320;&#25581;&#31034;&#21306;&#38388;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;HUSRM&#31639;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#21306;&#38388;&#20107;&#20214;&#24207;&#21015;&#65292;&#22240;&#20026;&#21306;&#38388;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20851;&#32852;&#35201;&#27604;&#22522;&#20110;&#28857;&#30340;&#24207;&#21015;&#22797;&#26434;&#24471;&#22810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25928;&#29992;&#30340;&#21306;&#38388;&#35268;&#21017;&#25366;&#25496;&#65288;UIRMiner&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#21306;&#38388;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#25152;&#26377;&#22522;&#20110;&#25928;&#29992;&#30340;&#21306;&#38388;&#35268;&#21017;&#65288;UIRs&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
For artificial intelligence, high-utility sequential rule mining (HUSRM) is a knowledge discovery method that can reveal the associations between events in the sequences. Recently, abundant methods have been proposed to discover high-utility sequence rules. However, the existing methods are all related to point-based sequences. Interval events that persist for some time are common. Traditional interval-event sequence knowledge discovery tasks mainly focus on pattern discovery, but patterns cannot reveal the correlation between interval events well. Moreover, the existing HUSRM algorithms cannot be directly applied to interval-event sequences since the relation in interval-event sequences is much more intricate than those in point-based sequences. In this work, we propose a utility-driven interval rule mining (UIRMiner) algorithm that can extract all utility-driven interval rules (UIRs) from the interval-event sequence database to solve the problem. In UIRMiner, we first introduce a num
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35770;&#35777;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#38598;&#20013;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#19982;&#21542;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16096</link><description>&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#21487;&#33021;&#26159;&#21487;&#20197;&#36991;&#20813;&#30340;&#65306;&#25968;&#25454;&#38598;&#20013;&#24615;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness. (arXiv:2309.16096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#35777;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#38598;&#20013;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#19982;&#21542;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#20110;&#23545;&#25239;&#26679;&#26412;&#30340;&#25935;&#24863;&#24615;&#24341;&#21457;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#26263;&#31034;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#21487;&#33021;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#21487;&#33021;&#36807;&#20110;&#19968;&#33324;&#21270;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#25968;&#25454;&#20998;&#24067;&#12290;&#20107;&#23454;&#19978;&#65292;&#20154;&#31867;&#22312;&#28041;&#21450;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#30456;&#24403;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#26126;&#26174;&#30340;&#30683;&#30462;&#25512;&#21160;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#25506;&#32034;&#19968;&#20010;&#38382;&#39064;&#65306;&#23545;&#25239;&#26679;&#26412;&#26159;&#21542;&#30495;&#30340;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#20010;&#20851;&#38190;&#23646;&#24615;&#8212;&#8212;&#23545;&#36755;&#20837;&#31354;&#38388;&#30340;&#23567;&#23481;&#31215;&#23376;&#38598;&#30340;&#38598;&#20013;&#31243;&#24230;&#65292;&#20915;&#23450;&#20102;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#40065;&#26834;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#33258;&#28982;&#22320;&#24471;&#21040;&#20139;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#22312;&#29305;&#23450;&#33539;&#22260;&#20869;&#21487;&#35777;&#26126;&#35748;&#35777;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The susceptibility of modern machine learning classifiers to adversarial examples has motivated theoretical results suggesting that these might be unavoidable. However, these results can be too general to be applicable to natural data distributions. Indeed, humans are quite robust for tasks involving vision. This apparent conflict motivates a deeper dive into the question: Are adversarial examples truly unavoidable? In this work, we theoretically demonstrate that a key property of the data distribution -- concentration on small-volume subsets of the input space -- determines whether a robust classifier exists. We further demonstrate that, for a data distribution concentrated on a union of low-dimensional linear subspaces, exploiting data structure naturally leads to classifiers that enjoy good robustness guarantees, improving upon methods for provable certification in certain regimes.
&lt;/p&gt;</description></item><item><title>TPE&#26159;&#19968;&#31181;&#38754;&#21521;&#27010;&#24565;&#24037;&#20855;&#30340;&#22810;&#20154;&#29289;&#21327;&#20316;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16090</link><description>&lt;p&gt;
TPE: &#38754;&#21521;&#22810;&#20154;&#29289;&#21327;&#20316;&#30340;&#27010;&#24565;&#24037;&#20855;&#26356;&#22909;&#21512;&#25104;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration. (arXiv:2309.16090v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16090
&lt;/p&gt;
&lt;p&gt;
TPE&#26159;&#19968;&#31181;&#38754;&#21521;&#27010;&#24565;&#24037;&#20855;&#30340;&#22810;&#20154;&#29289;&#21327;&#20316;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35268;&#21010;&#20351;&#29992;&#21508;&#31181;&#21151;&#33021;&#24037;&#20855;&#65292;&#22914;&#35745;&#31639;&#22120;&#21644;&#26816;&#32034;&#22120;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#25193;&#23637;&#20102;&#36825;&#20123;&#24037;&#20855;&#30340;&#23450;&#20041;&#65292;&#37325;&#28857;&#26159;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#27010;&#24565;&#24037;&#20855;&#12290;&#27010;&#24565;&#24037;&#20855;&#25351;&#23450;&#20102;&#19968;&#31181;&#26377;&#21161;&#20110;&#31995;&#32479;&#24615;&#25110;&#35843;&#26597;&#24615;&#24605;&#32771;&#30340;&#35748;&#30693;&#27010;&#24565;&#12290;&#36825;&#20123;&#27010;&#24565;&#24037;&#20855;&#22312;&#23454;&#36341;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20363;&#22914;&#22810;&#37325;&#24515;&#29702;&#25110;&#36741;&#23548;&#31574;&#30053;&#21160;&#24577;&#24212;&#29992;&#20110;&#21333;&#20010;&#22238;&#21512;&#20197;&#32452;&#21512;&#26377;&#29992;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#22312;&#36825;&#20123;&#27010;&#24565;&#24037;&#20855;&#19978;&#30340;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#20154;&#29289;&#21327;&#20316;&#26694;&#26550;: Think-Plan-Execute (TPE)&#12290;&#35813;&#26694;&#26550;&#23558;&#21709;&#24212;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#33394;: &#24605;&#32771;&#32773;&#65292;&#35268;&#21010;&#32773;&#21644;&#25191;&#34892;&#32773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24605;&#32771;&#32773;&#20998;&#26512;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#34920;&#29616;&#20986;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20363;&#22914;&#29992;&#25143;&#30340;&#24773;&#32490;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated exceptional performance in planning the use of various functional tools, such as calculators and retrievers, particularly in question-answering tasks. In this paper, we expand the definition of these tools, centering on conceptual tools within the context of dialogue systems. A conceptual tool specifies a cognitive concept that aids systematic or investigative thought. These conceptual tools play important roles in practice, such as multiple psychological or tutoring strategies being dynamically applied in a single turn to compose helpful responses. To further enhance the reasoning and planning capability of LLMs with these conceptual tools, we introduce a multi-persona collaboration framework: Think-Plan-Execute (TPE). This framework decouples the response generation process into three distinct roles: Thinker, Planner, and Executor. Specifically, the Thinker analyzes the internal status exhibited in the dialogue context, such as user emot
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#21644;&#25490;&#38500;&#27861;&#38598;&#25104;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25991;&#26412;&#24207;&#21015;&#26102;&#33021;&#22815;&#21462;&#24471;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16082</link><description>&lt;p&gt;
&#36890;&#36807;&#25490;&#38500;&#27861;&#38598;&#25104;&#27169;&#22411;&#26469;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25991;&#26412;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Forgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble. (arXiv:2309.16082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#21644;&#25490;&#38500;&#27861;&#38598;&#25104;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25991;&#26412;&#24207;&#21015;&#26102;&#33021;&#22815;&#21462;&#24471;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#26377;&#20542;&#21521;&#20110;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#35760;&#20303;&#32597;&#35265;&#25110;&#29420;&#29305;&#30340;&#20196;&#29260;&#24207;&#21015;&#12290;&#22312;&#37096;&#32626;&#27169;&#22411;&#21518;&#65292;&#26681;&#25454;&#20010;&#20154;&#35201;&#27714;&#65292;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#20219;&#20309;&#20010;&#20154;&#20449;&#24687;&#21487;&#33021;&#20250;&#34987;&#25552;&#20986;&#12290;&#27599;&#27425;&#20010;&#20154;&#24819;&#35201;&#34892;&#20351;&#34987;&#36951;&#24536;&#26435;&#21033;&#26102;&#37325;&#26032;&#35757;&#32451;&#24213;&#23618;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#20351;&#29992;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#38500;&#27861;&#38598;&#25104;&#26041;&#27861;&#26469;&#20174;&#27169;&#22411;&#20013;&#36951;&#24536;&#38656;&#35201;&#36951;&#24536;&#30340;&#25991;&#26412;&#24207;&#21015;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#22810;&#20010;&#25945;&#24072;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65307;&#23545;&#20110;&#27599;&#20010;&#38656;&#35201;&#21024;&#38500;&#30340;&#30446;&#26631;&#24207;&#21015;&#65292;&#25105;&#20204;&#25490;&#38500;&#22312;&#21253;&#21547;&#35813;&#24207;&#21015;&#30340;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#20174;&#21097;&#20313;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#32858;&#21512;&#39044;&#27979;&#32467;&#26524;&#65292;&#20197;&#25552;&#20379;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#30417;&#30563;&#12290;&#22312;LibriSpeech&#21644;WikiText-103&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that language models have a tendency to memorize rare or unique token sequences in the training corpus. After deploying a model, practitioners might be asked to delete any personal information from the model by individuals' requests. Re-training the underlying model every time individuals would like to practice their rights to be forgotten is computationally expensive. We employ a teacher-student framework and propose a novel leave-one-out ensemble method to unlearn the targeted textual sequences that need to be forgotten from the model. In our approach, multiple teachers are trained on disjoint sets; for each targeted sequence to be removed, we exclude the teacher trained on the set containing this sequence and aggregate the predictions from remaining teachers to provide supervision during fine-tuning. Experiments on LibriSpeech and WikiText-103 datasets show that the proposed method achieves superior privacy-utility trade-offs than other counterparts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#25512;&#26029;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.16064</link><description>&lt;p&gt;
&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#32454;&#32990;&#24418;&#24577;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Masked autoencoders are scalable learners of cellular morphology. (arXiv:2309.16064v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#25512;&#26029;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#20869;&#23481;&#26174;&#24494;&#38236;&#26816;&#26597;&#20013;&#20174;&#32454;&#32990;&#34920;&#22411;&#20013;&#25512;&#26029;&#29983;&#29289;&#20851;&#31995;&#22312;&#29983;&#29289;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#27604;&#25163;&#24037;&#35774;&#35745;&#30340;&#29305;&#24449;&#26356;&#33021;&#25429;&#25417;&#29983;&#29289;&#20449;&#21495;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#26368;&#39640;&#23610;&#24230;&#19978;&#65292;&#19968;&#20010;&#22312;&#20844;&#20849;&#25968;&#25454;&#24211;&#20013;&#26500;&#24314;&#30340;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#35206;&#30422;&#36229;&#36807;35&#20159;&#20010;&#21807;&#19968;&#21098;&#35009;&#22270;&#20687;&#30340;ViT-L/8&#27169;&#22411;&#65292;&#22312;&#25512;&#26029;&#24050;&#30693;&#29983;&#29289;&#20851;&#31995;&#26102;&#30456;&#23545;&#25913;&#36827;&#39640;&#36798;28%&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring biological relationships from cellular phenotypes in high-content microscopy screens provides significant opportunity and challenge in biological research. Prior results have shown that deep vision models can capture biological signal better than hand-crafted features. This work explores how weakly supervised and self-supervised deep learning approaches scale when training larger models on larger datasets. Our results show that both CNN- and ViT-based masked autoencoders significantly outperform weakly supervised models. At the high-end of our scale, a ViT-L/8 trained on over 3.5-billion unique crops sampled from 95-million microscopy images achieves relative improvements as high as 28% over our best weakly supervised models at inferring known biological relationships curated from public databases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.16042</link><description>&lt;p&gt;
&#12298;&#35821;&#35328;&#27169;&#22411;&#20013;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#65306;&#24230;&#37327;&#21644;&#26041;&#27861;&#12299;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#35299;&#37322;&#24615;&#26088;&#22312;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20854;&#20013;&#23450;&#20301;-&#35782;&#21035;&#37325;&#35201;&#30340;&#27169;&#22411;&#32452;&#20214;&#26159;&#20851;&#38190;&#27493;&#39588;&#12290;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#65292;&#20063;&#31216;&#20026;&#22240;&#26524;&#36861;&#36394;&#25110;&#20132;&#25442;&#24178;&#39044;&#65292;&#26159;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#20294;&#25991;&#29486;&#20013;&#23384;&#22312;&#35768;&#22810;&#21464;&#20307;&#65292;&#23545;&#36229;&#21442;&#25968;&#25110;&#26041;&#27861;&#36873;&#25321;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#21644;&#25439;&#22351;&#26041;&#27861;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#20301;&#21644;&#30005;&#36335;&#21457;&#29616;&#30340;&#20960;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#36890;&#36807;&#32463;&#39564;&#35266;&#23519;&#25903;&#25345;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#25351;&#26631;&#25110;&#26041;&#27861;&#21487;&#33021;&#26356;&#21463;&#27426;&#36814;&#30340;&#27010;&#24565;&#24615;&#35770;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16035</link><description>&lt;p&gt;
MedEdit&#65306;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases. (arXiv:2309.16035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#23398;&#38382;&#31572;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34429;&#28982;&#22312;&#19968;&#33324;&#39046;&#22495;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24448;&#24448;&#20316;&#20026;&#8220;&#40657;&#30418;&#8221;&#36816;&#20316;&#65292;&#38590;&#20197;&#20462;&#25913;&#20854;&#34892;&#20026;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#26088;&#22312;&#25913;&#36827;LLM&#30340;&#21709;&#24212;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26816;&#32034;&#31574;&#30053;&#65292;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#21307;&#23398;&#20107;&#23454;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;LLM&#30340;&#26597;&#35810;&#25552;&#31034;&#20013;&#12290;&#36890;&#36807;&#23545;MedQA-SMILE&#25968;&#25454;&#38598;&#36827;&#34892;&#21307;&#23398;QA&#30340;&#37325;&#28857;&#30740;&#31350;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#26816;&#32034;&#27169;&#22411;&#21644;&#21521;LLM&#25552;&#20379;&#30340;&#20107;&#23454;&#25968;&#37327;&#23545;&#20854;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#32534;&#36753;&#21518;&#30340;Vicuna&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20174;44.46&#65285;&#25552;&#39640;&#21040;48.54&#65285;&#12290;&#36825;&#39033;&#24037;&#20316;&#20984;&#26174;&#20102;&#27169;&#22411;&#32534;&#36753;&#25913;&#21892;LLM&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#32531;&#35299;&#40657;&#30418;LLM&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks like medical question answering (QA). Moreover, they tend to function as "black-boxes," making it challenging to modify their behavior. Addressing this, our study delves into model editing utilizing in-context learning, aiming to improve LLM responses without the need for fine-tuning or retraining. Specifically, we propose a comprehensive retrieval strategy to extract medical facts from an external knowledge base, and then we incorporate them into the query prompt for the LLM. Focusing on medical QA using the MedQA-SMILE dataset, we evaluate the impact of different retrieval models and the number of facts provided to the LLM. Notably, our edited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%. This work underscores the potential of model editing to enhance LLM performance, offering a practical approach to mitigate the challenges of black-box LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#26469;&#23398;&#20064;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25913;&#36827;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16025</link><description>&lt;p&gt;
&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65306;&#20174;&#40657;&#30418;&#21040;&#21487;&#35299;&#37322;&#30340;&#39550;&#39542;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Symbolic Imitation Learning: From Black-Box to Explainable Driving Policies. (arXiv:2309.16025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#26469;&#23398;&#20064;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25913;&#36827;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#33719;&#21462;&#39550;&#39542;&#31574;&#30053;&#30340;&#26377;&#25928;&#25163;&#27573;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#32570;&#28857;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#23588;&#20026;&#20196;&#20154;&#25285;&#24551;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#23398;&#20064;&#20174;&#21487;&#29992;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;highD&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#19982;&#24403;&#21069;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#39550;&#39542;&#31574;&#30053;&#25171;&#24320;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods of imitation learning (IL), primarily based on deep neural networks, offer efficient means for obtaining driving policies from real-world data but suffer from significant limitations in interpretability and generalizability. These shortcomings are particularly concerning in safety-critical applications like autonomous driving. In this paper, we address these limitations by introducing Symbolic Imitation Learning (SIL), a groundbreaking method that employs Inductive Logic Programming (ILP) to learn driving policies which are transparent, explainable and generalisable from available datasets. Utilizing the real-world highD dataset, we subject our method to a rigorous comparative analysis against prevailing neural-network-based IL methods. Our results demonstrate that SIL not only enhances the interpretability of driving policies but also significantly improves their applicability across varied driving situations. Hence, this work offers a novel pathway to more reliable an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#20020;&#24202;&#35797;&#39564;&#25512;&#33616;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#20013;&#23454;&#29616;&#39640;&#36798;70%-83%&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#19988;&#26368;&#30456;&#20851;&#30340;&#25512;&#33616;&#20301;&#20110;&#25512;&#33616;&#21015;&#34920;&#30340;&#21069;&#37096;&#12290;</title><link>http://arxiv.org/abs/2309.15979</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35821;&#20041;&#30340;&#24402;&#32435;&#25512;&#29702;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#20020;&#24202;&#35797;&#39564;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Clinical Trial Recommendations Using Semantics-Based Inductive Inference and Knowledge Graph Embeddings. (arXiv:2309.15979v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#20020;&#24202;&#35797;&#39564;&#25512;&#33616;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#20013;&#23454;&#29616;&#39640;&#36798;70%-83%&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#19988;&#26368;&#30456;&#20851;&#30340;&#25512;&#33616;&#20301;&#20110;&#25512;&#33616;&#21015;&#34920;&#30340;&#21069;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26032;&#30340;&#20020;&#24202;&#35797;&#39564;&#28041;&#21450;&#21040;&#35832;&#22810;&#20915;&#31574;&#65292;&#27604;&#22914;&#30830;&#23450;&#38431;&#21015;&#21644;&#35774;&#23450;&#30740;&#31350;&#30446;&#26631;&#31561;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23545;&#36807;&#21435;&#20020;&#24202;&#35797;&#39564;&#35760;&#24405;&#30340;&#20840;&#38754;&#25366;&#25496;&#26469;&#33719;&#24471;&#25512;&#33616;&#24314;&#35758;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#35797;&#39564;&#30693;&#35782;&#22270;&#30340;&#31070;&#32463;&#23884;&#20837;&#35757;&#32451;&#30340;&#25512;&#33616;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21253;&#25324;&#35774;&#35745;&#20020;&#24202;&#35797;&#39564;&#30693;&#35782;&#22270;&#65292;&#21508;&#31181;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;&#23884;&#20837;&#36827;&#34892;&#24402;&#32435;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#20013;&#29983;&#25104;&#25512;&#33616;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;clinicaltrials.gov&#30340;&#20844;&#24320;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25512;&#33616;&#26041;&#27861;&#23454;&#29616;&#20102;70&#65285;-83&#65285;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#20197;&#23454;&#38469;&#20020;&#24202;&#35797;&#39564;&#20803;&#32032;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#34913;&#37327;&#65292;&#32780;&#26368;&#30456;&#20851;&#30340;&#25512;&#33616;&#21487;&#20197;&#22312;&#21015;&#34920;&#30340;&#21069;&#37096;&#25214;&#21040;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#24314;&#35758;&#20102;&#20854;&#20182;&#26041;&#27861;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing a new clinical trial entails many decisions, such as defining a cohort and setting the study objectives to name a few, and therefore can benefit from recommendations based on exhaustive mining of past clinical trial records. Here, we propose a novel recommendation methodology, based on neural embeddings trained on a first-of-a-kind knowledge graph of clinical trials. We addressed several important research questions in this context, including designing a knowledge graph (KG) for clinical trial data, effectiveness of various KG embedding (KGE) methods for it, a novel inductive inference using KGE, and its use in generating recommendations for clinical trial design. We used publicly available data from clinicaltrials.gov for the study. Results show that our recommendations approach achieves relevance scores of 70%-83%, measured as the text similarity to actual clinical trial elements, and the most relevant recommendation can be found near the top of list. Our study also suggest
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#38271;&#26399;&#26102;&#24207;&#39044;&#27979;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#32463;&#20856;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#20998;&#26512;&#65292;&#21457;&#29616;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#24615;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.15946</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#38271;&#26399;&#26102;&#24207;&#39044;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Unified Long-Term Time-Series Forecasting Benchmark. (arXiv:2309.15946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#38271;&#26399;&#26102;&#24207;&#39044;&#27979;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#32463;&#20856;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#20998;&#26512;&#65292;&#21457;&#29616;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#24615;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#38271;&#26399;&#26102;&#24207;&#39044;&#27979;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#38598;&#20102;&#26469;&#33258;&#21508;&#31181;&#19981;&#21516;&#12289;&#21160;&#24577;&#31995;&#32479;&#21644;&#30495;&#23454;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#32463;&#36807;&#26631;&#20934;&#21270;&#22788;&#29702;&#65292;&#20998;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#36712;&#36857;&#65292;&#24182;&#39044;&#20808;&#30830;&#23450;&#20102;&#22238;&#28335;&#38271;&#24230;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#38271;&#24230;&#20026;2000&#30340;&#36712;&#36857;&#65292;&#20197;&#30830;&#20445;&#21487;&#38752;&#22320;&#35780;&#20272;&#38271;&#26399;&#39044;&#27979;&#33021;&#21147;&#12290;&#20026;&#20102;&#30830;&#23450;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#20856;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65288;&#21253;&#25324;LSTM&#12289;DeepAR&#12289;NLinear&#12289;N-Hits&#12289;PatchTST&#21644;LatentODE&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#26377;&#36259;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#26377;&#25928;&#24615;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#24615;&#30340;&#29305;&#28857;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#28508;&#22312;NLinear&#27169;&#22411;&#65292;&#24182;&#22312;DeepAR&#20013;&#22686;&#21152;&#20102;&#19968;&#20010;&#35838;&#31243;&#23398;&#20064;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to support the advancement of machine learning methods for predicting time-series data, we present a comprehensive dataset designed explicitly for long-term time-series forecasting. We incorporate a collection of datasets obtained from diverse, dynamic systems and real-life records. Each dataset is standardized by dividing it into training and test trajectories with predetermined lookback lengths. We include trajectories of length up to $2000$ to ensure a reliable evaluation of long-term forecasting capabilities. To determine the most effective model in diverse scenarios, we conduct an extensive benchmarking analysis using classical and state-of-the-art models, namely LSTM, DeepAR, NLinear, N-Hits, PatchTST, and LatentODE. Our findings reveal intriguing performance comparisons among these models, highlighting the dataset-dependent nature of model effectiveness. Notably, we introduce a custom latent NLinear model and enhance DeepAR with a curriculum learning phase. Both consist
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30828;&#20214;-&#31639;&#27861;-&#36890;&#20449;&#21327;&#21516;&#35774;&#35745;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#39640;&#25928;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#32467;&#21512;&#29289;&#29702;&#27934;&#35265;&#12289;&#39640;&#25928;&#20449;&#24687;&#22788;&#29702;&#21407;&#21017;&#12289;&#26368;&#20248;&#19981;&#30830;&#23450;&#24230;&#37327;&#32467;&#26524;&#21644;&#20998;&#24067;&#24335;&#22788;&#29702;&#20934;&#21017;&#65292;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#30340;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.15942</link><description>&lt;p&gt;
&#36890;&#36807;&#30828;&#20214;-&#31639;&#27861;-&#36890;&#20449;&#21327;&#21516;&#35774;&#35745;&#23454;&#29616;&#39640;&#25928;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Trustworthy AI Through Hardware-Algorithm-Communication Co-Design. (arXiv:2309.15942v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15942
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30828;&#20214;-&#31639;&#27861;-&#36890;&#20449;&#21327;&#21516;&#35774;&#35745;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#39640;&#25928;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#32467;&#21512;&#29289;&#29702;&#27934;&#35265;&#12289;&#39640;&#25928;&#20449;&#24687;&#22788;&#29702;&#21407;&#21017;&#12289;&#26368;&#20248;&#19981;&#30830;&#23450;&#24230;&#37327;&#32467;&#26524;&#21644;&#20998;&#24067;&#24335;&#22788;&#29702;&#20934;&#21017;&#65292;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#30340;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#24050;&#32463;&#34987;&#35774;&#35745;&#20102;&#20960;&#21313;&#24180;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26576;&#31181;&#20934;&#30830;&#24615;&#24230;&#37327;&#12290;&#36825;&#23548;&#33268;&#20102;&#20004;&#20010;&#19981;&#24076;&#26395;&#20986;&#29616;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#20197;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#20026;&#34913;&#37327;&#26631;&#20934;&#65292;&#27169;&#22411;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#31532;&#20108;&#65292;&#26368;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#25552;&#20379;&#21487;&#20449;&#24230;&#37327;&#26041;&#38754;&#24456;&#38590;&#23454;&#29616;&#65292;&#21487;&#33021;&#20986;&#29616;"&#24187;&#35273;"&#38382;&#39064;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#20854;&#22312;&#25935;&#24863;&#24212;&#29992;&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#26412;&#25991;&#24378;&#35843;&#30828;&#20214;&#21644;&#36719;&#20214;&#35774;&#35745;&#20132;&#21449;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#23558;&#29289;&#29702;&#27934;&#35265;&#19982;&#35745;&#31639;&#22522;&#30784;&#12289;&#31070;&#32463;&#31185;&#23398;&#26377;&#20851;&#30340;&#39640;&#25928;&#20449;&#24687;&#22788;&#29702;&#21407;&#21017;&#12289;&#20449;&#24687;&#35770;&#20013;&#20851;&#20110;&#26368;&#20248;&#19981;&#30830;&#23450;&#24230;&#37327;&#30340;&#32467;&#26524;&#20197;&#21450;&#36890;&#20449;&#29702;&#35770;&#20013;&#20851;&#20110;&#20998;&#24067;&#24335;&#22788;&#29702;&#30340;&#20934;&#21017;&#30456;&#32467;&#21512;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20513;&#26032;&#30340;&#30828;&#20214;-&#31639;&#27861;-&#36890;&#20449;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) algorithms based on neural networks have been designed for decades with the goal of maximising some measure of accuracy. This has led to two undesired effects. First, model complexity has risen exponentially when measured in terms of computation and memory requirements. Second, state-of-the-art AI models are largely incapable of providing trustworthy measures of their uncertainty, possibly `hallucinating' their answers and discouraging their adoption for decision-making in sensitive applications.  With the goal of realising efficient and trustworthy AI, in this paper we highlight research directions at the intersection of hardware and software design that integrate physical insights into computational substrates, neuroscientific principles concerning efficient information processing, information-theoretic results on optimal uncertainty quantification, and communication-theoretic guidelines for distributed processing. Overall, the paper advocates for novel d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#23884;&#20837;&#35757;&#32451;&#65288;MLET&#65289;&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#36328;&#31867;&#21035;&#23398;&#20064;&#20135;&#29983;&#20248;&#31168;&#30340;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23884;&#20837;&#23618;&#30340;&#20998;&#35299;&#35757;&#32451;&#23884;&#20837;&#65292;&#20869;&#37096;&#32500;&#24230;&#39640;&#20110;&#30446;&#26631;&#23884;&#20837;&#32500;&#24230;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#35813;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#20196;&#20154;&#24778;&#35766;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#35299;&#37322;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15881</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#23618;&#23884;&#20837;&#35757;&#32451;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#36328;&#31867;&#21035;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training. (arXiv:2309.15881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15881
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#23884;&#20837;&#35757;&#32451;&#65288;MLET&#65289;&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#36328;&#31867;&#21035;&#23398;&#20064;&#20135;&#29983;&#20248;&#31168;&#30340;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23884;&#20837;&#23618;&#30340;&#20998;&#35299;&#35757;&#32451;&#23884;&#20837;&#65292;&#20869;&#37096;&#32500;&#24230;&#39640;&#20110;&#30446;&#26631;&#23884;&#20837;&#32500;&#24230;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#35813;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#20196;&#20154;&#24778;&#35766;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#35299;&#37322;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22522;&#20110;DNN&#30340;&#25512;&#33616;&#31995;&#32479;&#20381;&#36182;&#20110;&#23545;&#31232;&#30095;&#29305;&#24449;&#36827;&#34892;&#35757;&#32451;&#24471;&#21040;&#30340;&#23884;&#20837;&#12290;&#36755;&#20837;&#31232;&#30095;&#24615;&#20351;&#24471;&#24456;&#38590;&#33719;&#24471;&#23569;&#20986;&#29616;&#31867;&#21035;&#30340;&#39640;&#36136;&#37327;&#23884;&#20837;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#34920;&#31034;&#24456;&#23569;&#26356;&#26032;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#35757;&#32451;&#26102;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#36328;&#31867;&#21035;&#23398;&#20064;&#20135;&#29983;&#20248;&#31168;&#30340;&#23884;&#20837;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20854;&#20196;&#20154;&#24778;&#35766;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#26696;&#34987;&#31216;&#20026;&#22810;&#23618;&#23884;&#20837;&#35757;&#32451;&#65288;MLET&#65289;&#65292;&#36890;&#36807;&#23884;&#20837;&#23618;&#30340;&#20998;&#35299;&#35757;&#32451;&#23884;&#20837;&#65292;&#20869;&#37096;&#32500;&#24230;&#39640;&#20110;&#30446;&#26631;&#23884;&#20837;&#32500;&#24230;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;MLET&#23558;&#35757;&#32451;&#24471;&#21040;&#30340;&#21452;&#23618;&#23884;&#20837;&#36716;&#25442;&#20026;&#21333;&#23618;&#23884;&#20837;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#25512;&#29702;&#26102;&#30340;&#27169;&#22411;&#22823;&#23567;&#19981;&#21464;&#12290;MLET&#30340;&#23454;&#39564;&#20248;&#36234;&#24615;&#20196;&#20154;&#22256;&#24785;&#65292;&#22240;&#20026;&#20854;&#25628;&#32034;&#31354;&#38388;&#24182;&#19981;&#27604;&#21333;&#23618;&#23884;&#20837;&#26356;&#22823;&#12290;MLET&#23545;&#20869;&#37096;&#32500;&#24230;&#30340;&#24378;&#20381;&#36182;&#29978;&#33267;&#26356;&#20196;&#20154;&#24778;&#35766;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#20010;&#29702;&#35770;&#26469;&#35299;&#37322;&#36825;&#20004;&#31181;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern DNN-based recommendation systems rely on training-derived embeddings of sparse features. Input sparsity makes obtaining high-quality embeddings for rarely-occurring categories harder as their representations are updated infrequently. We demonstrate a training-time technique to produce superior embeddings via effective cross-category learning and theoretically explain its surprising effectiveness. The scheme, termed the multi-layer embeddings training (MLET), trains embeddings using factorization of the embedding layer, with an inner dimension higher than the target embedding dimension. For inference efficiency, MLET converts the trained two-layer embedding into a single-layer one thus keeping inference-time model size unchanged.  Empirical superiority of MLET is puzzling as its search space is not larger than that of the single-layer embedding. The strong dependence of MLET on the inner dimension is even more surprising. We develop a theory that explains both of these behaviors 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2309.15877</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Hierarchical Multimodal Learning. (arXiv:2309.15877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15877
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#21644;&#22788;&#29702;&#26469;&#33258;&#22810;&#31181;&#20449;&#24687;&#28304;&#25110;&#27169;&#24577;&#23545;&#20110;&#33719;&#24471;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20449;&#24687;&#35770;&#20998;&#23618;&#24863;&#30693;(ITHP)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#27010;&#24565;&#12290;&#19982;&#22823;&#22810;&#25968;&#26088;&#22312;&#23558;&#25152;&#26377;&#27169;&#24577;&#32435;&#20837;&#36755;&#20837;&#30340;&#20256;&#32479;&#34701;&#21512;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#20027;&#35201;&#27169;&#24577;&#25351;&#23450;&#20026;&#36755;&#20837;&#65292;&#32780;&#20854;&#20313;&#27169;&#24577;&#21017;&#20316;&#20026;&#20449;&#24687;&#36335;&#24452;&#20013;&#30340;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24863;&#30693;&#27169;&#22411;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#36755;&#20837;&#27169;&#24577;&#29366;&#24577;&#20043;&#38388;&#26368;&#23567;&#21270;&#30456;&#20114;&#20449;&#24687;&#24182;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#20854;&#20313;&#27169;&#24577;&#20043;&#38388;&#26368;&#22823;&#21270;&#30456;&#20114;&#20449;&#24687;&#30340;&#24179;&#34913;&#65292;&#26500;&#24314;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#24182;&#26368;&#23567;&#21270;&#20887;&#20313;&#30340;&#32039;&#20945;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Distinct from most traditional fusion models that aim to incorporate all modalities as input, our model designates the prime modality as input, while the remaining modalities act as detectors in the information pathway. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby sub
&lt;/p&gt;</description></item><item><title>STAG&#26159;&#19968;&#20010;GNN&#26381;&#21153;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22270;&#20013;&#22522;&#20110;GNN&#30340;&#26381;&#21153;&#20013;&#30340;&#20302;&#24310;&#36831;&#21644;&#20302;&#38472;&#26087;&#24230;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#21327;&#21516;&#26381;&#21153;&#26426;&#21046;&#21644;&#22686;&#37327;&#20256;&#25773;&#31574;&#30053;&#26469;&#20248;&#21270;&#33410;&#28857;&#34920;&#31034;&#30340;&#26356;&#26032;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.15875</link><description>&lt;p&gt;
STAG: &#23454;&#29616;&#21160;&#24577;&#22270;&#20013;&#22522;&#20110;GNN&#30340;&#26381;&#21153;&#20302;&#24310;&#36831;&#21644;&#20302;&#38472;&#26087;&#24230;
&lt;/p&gt;
&lt;p&gt;
STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs. (arXiv:2309.15875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15875
&lt;/p&gt;
&lt;p&gt;
STAG&#26159;&#19968;&#20010;GNN&#26381;&#21153;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22270;&#20013;&#22522;&#20110;GNN&#30340;&#26381;&#21153;&#20013;&#30340;&#20302;&#24310;&#36831;&#21644;&#20302;&#38472;&#26087;&#24230;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#21327;&#21516;&#26381;&#21153;&#26426;&#21046;&#21644;&#22686;&#37327;&#20256;&#25773;&#31574;&#30053;&#26469;&#20248;&#21270;&#33410;&#28857;&#34920;&#31034;&#30340;&#26356;&#26032;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26032;&#20852;&#30340;&#29992;&#25143;&#38754;&#21521;&#26381;&#21153;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#25552;&#39640;&#26381;&#21153;&#20934;&#30830;&#24615;&#12290;&#24403;GNN&#27169;&#22411;&#20351;&#29992;&#30340;&#22270;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#22270;&#20013;&#33410;&#28857;&#30340;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#24212;&#30456;&#24212;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#33410;&#28857;&#34920;&#31034;&#30340;&#26356;&#26032;&#36895;&#24230;&#36807;&#24930;&#65292;&#23548;&#33268;&#29992;&#25143;&#26597;&#35810;&#30340;&#21709;&#24212;&#24310;&#36831;&#36739;&#38271;&#65288;&#26356;&#26032;&#23436;&#25104;&#21518;&#36827;&#34892;&#25512;&#29702;&#65289;&#25110;&#23384;&#22312;&#36739;&#39640;&#30340;&#38472;&#26087;&#24230;&#38382;&#39064;&#65288;&#22522;&#20110;&#38472;&#26087;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#65289;&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#26356;&#26032;&#36807;&#24930;&#20027;&#35201;&#26159;&#30001;&#20110;&#22270;&#20013;&#30340;&#37051;&#23621;&#29190;&#28856;&#38382;&#39064;&#21644;&#37325;&#22797;&#35745;&#31639;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAG&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#22522;&#20110;GNN&#30340;&#26381;&#21153;&#20302;&#24310;&#36831;&#21644;&#20302;&#38472;&#26087;&#24230;&#30340;GNN&#26381;&#21153;&#26694;&#26550;&#12290;&#23427;&#21253;&#25324;&#21327;&#21516;&#26381;&#21153;&#26426;&#21046;&#21644;&#22522;&#20110;&#21487;&#21152;&#24615;&#30340;&#22686;&#37327;&#20256;&#25773;&#31574;&#30053;&#12290;&#36890;&#36807;&#21327;&#21516;&#26381;&#21153;&#26426;&#21046;&#65292;&#21482;&#26377;&#37096;&#20998;&#33410;&#28857;&#34920;&#31034;&#22312;&#26356;&#26032;&#38454;&#27573;&#36827;&#34892;&#26356;&#26032;&#65292;&#26368;&#32456;&#30340;&#34920;&#31034;&#26159;&#22312;&#22686;&#37327;&#20256;&#25773;&#31574;&#30053;&#20013;&#35745;&#31639;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many emerging user-facing services adopt Graph Neural Networks (GNNs) to improve serving accuracy. When the graph used by a GNN model changes, representations (embedding) of nodes in the graph should be updated accordingly. However, the node representation update is too slow, resulting in either long response latency of user queries (the inference is performed after the update completes) or high staleness problem (the inference is performed based on stale data). Our in-depth analysis shows that the slow update is mainly due to neighbor explosion problem in graphs and duplicated computation. Based on such findings, we propose STAG, a GNN serving framework that enables low latency and low staleness of GNN-based services. It comprises a collaborative serving mechanism and an additivity-based incremental propagation strategy. With the collaborative serving mechanism, only part of node representations are updated during the update phase, and the final representations are calculated in the i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;ChatGPT&#22312;&#26426;&#26800;&#24037;&#31243;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20184;&#36153;&#35746;&#38405;&#27169;&#22411;&#65288;GPT-4&#65289;&#22312;&#22238;&#31572;&#26426;&#26800;&#24037;&#31243;&#32771;&#35797;&#21644;FE&#32771;&#35797;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20813;&#36153;&#29256;&#26412;&#65288;GPT-3.5&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.15866</link><description>&lt;p&gt;
ChatGPT&#19982;&#26426;&#26800;&#24037;&#31243;&#65306;&#23545;FE&#26426;&#26800;&#24037;&#31243;&#21644;&#26412;&#31185;&#32771;&#35797;&#30340;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ChatGPT &amp; Mechanical Engineering: Examining performance on the FE Mechanical Engineering and Undergraduate Exams. (arXiv:2309.15866v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15866
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;ChatGPT&#22312;&#26426;&#26800;&#24037;&#31243;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20184;&#36153;&#35746;&#38405;&#27169;&#22411;&#65288;GPT-4&#65289;&#22312;&#22238;&#31572;&#26426;&#26800;&#24037;&#31243;&#32771;&#35797;&#21644;FE&#32771;&#35797;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20813;&#36153;&#29256;&#26412;&#65288;GPT-3.5&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;2022&#24180;&#24213;&#30340;&#21457;&#24067;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#22312;STEM&#25945;&#32946;&#21644;STEM&#19987;&#19994;&#20013;&#21487;&#33021;&#24212;&#29992;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;&#35838;&#22530;&#20869;&#22806;&#29983;&#25104;AI&#24037;&#20855;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#35768;&#22810;&#38382;&#39064;&#65292;&#24182;&#24320;&#22987;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;ChatGPT&#22312;&#26426;&#26800;&#24037;&#31243;&#23398;&#31185;&#20013;&#30340;&#33021;&#21147;&#12290;&#23427;&#26088;&#22312;&#30740;&#31350;&#35813;&#25216;&#26415;&#22312;&#35838;&#22530;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#26696;&#20363;&#21644;&#28508;&#22312;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#20123;&#26469;&#33258;&#22823;&#22411;&#31169;&#31435;&#22823;&#23398;&#30340;&#21021;&#32423;&#21644;&#39640;&#32423;&#26426;&#26800;&#24037;&#31243;&#32771;&#35797;&#39064;&#30446;&#65292;&#20197;&#21450;&#19968;&#32452;&#29992;&#20110;&#26426;&#26800;&#24037;&#31243;&#22522;&#30784;&#24037;&#31243;&#32771;&#35797;&#65288;FE&#65289;&#30340;&#32451;&#20064;&#39064;&#12290;&#20316;&#32773;&#20998;&#26512;&#20102;&#20004;&#20010;ChatGPT&#27169;&#22411;&#65288;&#19968;&#20010;&#20813;&#36153;&#20351;&#29992;&#65292;&#19968;&#20010;&#20184;&#36153;&#35746;&#38405;&#65289;&#30340;&#22238;&#31572;&#12290;&#30740;&#31350;&#21457;&#29616;&#20184;&#36153;&#35746;&#38405;&#27169;&#22411;&#65288;GPT-4&#65289;&#30340;&#34920;&#29616;&#36828;&#36828;&#36229;&#36807;&#20102;&#20813;&#36153;&#29256;&#26412;&#65288;GPT-3.5&#65289;&#65292;&#36798;&#21040;&#20102;76&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The launch of ChatGPT at the end of 2022 generated large interest into possible applications of artificial intelligence in STEM education and among STEM professions. As a result many questions surrounding the capabilities of generative AI tools inside and outside of the classroom have been raised and are starting to be explored. This study examines the capabilities of ChatGPT within the discipline of mechanical engineering. It aims to examine use cases and pitfalls of such a technology in the classroom and professional settings. ChatGPT was presented with a set of questions from junior and senior level mechanical engineering exams provided at a large private university, as well as a set of practice questions for the Fundamentals of Engineering Exam (FE) in Mechanical Engineering. The responses of two ChatGPT models, one free to use and one paid subscription, were analyzed. The paper found that the subscription model (GPT-4) greatly outperformed the free version (GPT-3.5), achieving 76%
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.15857</link><description>&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15857
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#25104;&#20026;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#23548;&#33268;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#20215;&#20540;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#21457;&#23637;&#37324;&#31243;&#30865;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#23427;&#20204;&#30340;&#21457;&#23637;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#22522;&#20110;&#23427;&#20204;&#34987;&#24341;&#20837;&#30340;&#26102;&#38388;&#21644;&#23545;&#23398;&#31185;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20219;&#21153;&#22312;&#23398;&#26415;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#26222;&#21450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#19982;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30456;&#20851;&#30340;&#20219;&#21153;&#21010;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#31867;&#22411;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#27599;&#20010;&#31867;&#21035;&#20869;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#20173;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amidst the evolving landscape of artificial intelligence, the convergence of visual and textual information has surfaced as a crucial frontier, leading to the advent of image-text multimodal models. This paper provides a comprehensive review of the evolution and current state of image-text multimodal models, exploring their application value, challenges, and potential research trajectories. Initially, we revisit the basic concepts and developmental milestones of these models, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline. Furthermore, based on the tasks' significance and prevalence in the academic landscape, we propose a categorization of the tasks associated with image-text multimodal models into five major types, elucidating the recent progress and key technologies within each category. Despite the remarkable accomplishments of these models, numerous challenges a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15757</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data. (arXiv:2309.15757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#65288;&#26377;&#65289;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#23454;&#20363;&#38388;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25512;&#26029;&#25429;&#25417;&#20869;&#22312;&#25968;&#25454;&#20851;&#31995;&#30340;&#28508;&#22312;&#22270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#20449;&#24687;&#22312;&#25972;&#20010;&#22270;&#20013;&#30340;&#26080;&#32541;&#20256;&#25773;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#24403;&#20195;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#21457;&#29616;&#23454;&#20363;&#38388;&#20851;&#31995;&#20316;&#20026;&#26500;&#24314;&#24378;&#21270;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#40065;&#26834;&#28508;&#22312;&#22270;&#30340;&#23454;&#38469;&#25163;&#27573;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of semi-supervised learning, the current approaches insufficiently exploit the potential of considering inter-instance relationships among (un)labeled data. In this work, we address this limitation by providing an approach for inferring latent graphs that capture the intrinsic data relationships. By leveraging graph-based representations, our approach facilitates the seamless propagation of information throughout the graph, enabling the effective incorporation of global and local knowledge. Through evaluations on biomedical tabular datasets, we compare the capabilities of our approach to other contemporary methods. Our work demonstrates the significance of inter-instance relationship discovery as practical means for constructing robust latent graphs to enhance semi-supervised learning techniques. Our method achieves state-of-the-art results on three biomedical datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;WavLabLM&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#65292;&#20351;&#20854;&#22312;ML-SUPERB&#19978;&#36798;&#21040;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.15317</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
joint prediction and denoising for large-scale multilingual self-supervised learning. (arXiv:2309.15317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15317
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;WavLabLM&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#65292;&#20351;&#20854;&#22312;ML-SUPERB&#19978;&#36798;&#21040;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#30001;&#20110;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#25152;&#38656;&#30340;&#36153;&#29992;&#21644;&#22797;&#26434;&#24615;&#32780;&#32463;&#24120;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#36825;&#36827;&#19968;&#27493;&#24433;&#21709;&#20102;SSL&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#30001;&#20110;&#36164;&#28304;&#20351;&#29992;&#30340;&#38480;&#21046;&#65292;SSL&#24050;&#32463;&#20165;&#38480;&#20110;&#23569;&#25968;&#30740;&#31350;&#22242;&#38431;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#24378;&#22823;&#30340;&#25216;&#26415;&#23454;&#38469;&#19978;&#21487;&#20197;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#26356;&#22810;&#30340;&#30740;&#31350;&#22242;&#38431;&#33021;&#22815;&#21152;&#20837;SSL&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23558;WavLM&#30340;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#25193;&#23637;&#21040;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#12290;&#20026;&#20102;&#26500;&#24314;WavLabLM&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#12290;WavLabLM&#22312;ML-SUPERB&#19978;&#23454;&#29616;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;vanilla HuBERT Base&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#25928;&#29575;&#25552;&#21319;&#65292;&#20165;&#20351;&#29992;3%&#30340;&#25968;&#25454;&#12289;4&#20010;GPU&#21644;&#26377;&#38480;&#30340;&#35797;&#39564;&#27425;&#25968;&#65292;&#23601;&#33021;&#20445;&#25345;94%&#30340;XLS-R&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM's joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R's performance with only 3% of the data, 4 GPUs, and limited trials. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#25968;&#25454;&#30340;&#20132;&#36890;&#28783;-&#36710;&#36947;&#20998;&#37197;&#35821;&#20041;&#22320;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#33258;&#21160;&#25512;&#23548;&#20132;&#36890;&#28783;&#21040;&#36710;&#36947;&#30340;&#20998;&#37197;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#23433;&#20840;&#32771;&#34385;&#21644;&#25968;&#25454;&#38598;&#36716;&#25442;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14793</link><description>&lt;p&gt;
&#22522;&#20110;&#36816;&#21160;&#25968;&#25454;&#30340;&#20132;&#36890;&#28783;-&#36710;&#36947;&#20998;&#37197;&#35821;&#20041;&#22320;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic Map Learning of Traffic Light to Lane Assignment based on Motion Data. (arXiv:2309.14793v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#25968;&#25454;&#30340;&#20132;&#36890;&#28783;-&#36710;&#36947;&#20998;&#37197;&#35821;&#20041;&#22320;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#33258;&#21160;&#25512;&#23548;&#20132;&#36890;&#28783;&#21040;&#36710;&#36947;&#30340;&#20998;&#37197;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#23433;&#20840;&#32771;&#34385;&#21644;&#25968;&#25454;&#38598;&#36716;&#25442;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21738;&#20010;&#20132;&#36890;&#28783;&#25511;&#21046;&#21738;&#20010;&#36710;&#36947;&#23545;&#20110;&#23433;&#20840;&#36890;&#36807;&#36335;&#21475;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36890;&#24120;&#20381;&#36182;&#21253;&#21547;&#20132;&#36890;&#28783;-&#36710;&#36947;&#20998;&#37197;&#20449;&#24687;&#30340;&#39640;&#28165;&#22320;&#22270;&#12290;&#25163;&#21160;&#25552;&#20379;&#36825;&#20123;&#20449;&#24687;&#26082;&#36153;&#26102;&#21448;&#26114;&#36149;&#65292;&#32780;&#19988;&#19981;&#21487;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20132;&#36890;&#28783;&#29366;&#24577;&#21644;&#36710;&#36742;&#36816;&#21160;&#27169;&#24335;&#25512;&#23548;&#20986;&#20998;&#37197;&#26041;&#24335;&#12290;&#35813;&#26041;&#27861;&#33258;&#21160;&#21270;&#24182;&#19988;&#19981;&#20381;&#36182;&#20960;&#20309;&#25490;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#21644;&#35780;&#20272;&#22522;&#20110;&#27169;&#24335;&#30340;&#36129;&#29486;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#26412;&#32479;&#35745;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#25298;&#32477;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#32771;&#34385;&#20102;&#23433;&#20840;&#22240;&#32032;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#36716;&#25442;&#26041;&#27861;&#65292;&#20197;&#37325;&#26032;&#21033;&#29992;&#29616;&#26377;&#30340;&#36816;&#21160;&#39044;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#35821;&#20041;&#22320;&#22270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;Lyft Level 5&#25968;&#25454;&#38598;&#30340;&#20844;&#24320;API&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding which traffic light controls which lane is crucial to navigate intersections safely. Autonomous vehicles commonly rely on High Definition (HD) maps that contain information about the assignment of traffic lights to lanes. The manual provisioning of this information is tedious, expensive, and not scalable. To remedy these issues, our novel approach derives the assignments from traffic light states and the corresponding motion patterns of vehicle traffic. This works in an automated way and independently of the geometric arrangement. We show the effectiveness of basic statistical approaches for this task by implementing and evaluating a pattern-based contribution method. In addition, our novel rejection method includes accompanying safety considerations by leveraging statistical hypothesis testing. Finally, we propose a dataset transformation to re-purpose available motion prediction datasets for semantic map learning. Our publicly available API for the Lyft Level 5 dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#29943;&#30742;&#12290;</title><link>http://arxiv.org/abs/2309.14564</link><description>&lt;p&gt;
&#29983;&#25104;&#33406;&#33293;&#23572;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Generative Escher Meshes. (arXiv:2309.14564v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#29943;&#30742;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#12289;&#20197;&#25991;&#26412;&#20026;&#23548;&#21521;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21608;&#26399;&#24615;&#30340;&#12289;&#21487;&#37325;&#22797;&#30340;&#20108;&#32500;&#33402;&#26415;&#20316;&#21697;&#65292;&#22914;&#22320;&#26495;&#12289;&#39532;&#36187;&#20811;&#12289;&#38518;&#29943;&#21644;&#33406;&#33293;&#23572;&#30340;&#20316;&#21697;&#12290;&#19982;&#20256;&#32479;&#30340;&#26080;&#32541;&#32441;&#29702;&#27010;&#24565;&#19981;&#21516;&#65292;&#21363;&#24179;&#38138;&#26080;&#32541;&#30340;&#27491;&#26041;&#24418;&#22270;&#20687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#26159;&#30001;&#37325;&#22797;&#30340;&#30456;&#21516;&#23545;&#35937;&#32452;&#25104;&#30340;&#38750;&#27491;&#26041;&#24418;&#38262;&#23884;&#22270;&#26696;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#20108;&#32500;&#32593;&#26684;&#30340;&#20960;&#20309;&#21644;&#39068;&#33394;&#26469;&#29983;&#25104;&#19982;&#25152;&#38656;&#23545;&#35937;&#24418;&#29366;&#21644;&#22806;&#35266;&#30456;&#20284;&#30340;&#38750;&#27491;&#26041;&#24418;&#29943;&#30742;&#65292;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#32972;&#26223;&#32454;&#33410;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#36129;&#29486;&#23454;&#29616;&#20102;&#38262;&#23884;&#22270;&#26696;&#30340;&#20960;&#20309;&#20248;&#21270;&#65306;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#12289;&#21487;&#24494;&#20998;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#32473;&#23450;&#23545;&#31216;&#32676;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#21487;&#38138;&#30742;&#24418;&#29366;&#31354;&#38388;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20462;&#25913;&#20108;&#32500;&#32593;&#26684;&#26144;&#23556;&#25216;&#26415;Orbifold Tutte Embedding&#20013;&#20351;&#29992;&#30340;Laplacian&#31639;&#23376;&#21487;&#20197;&#23454;&#29616;&#25152;&#36873;&#24179;&#38754;&#23545;&#31216;&#32676;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#38138;&#30742;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a fully-automatic, text-guided generative method for producing periodic, repeating, tile-able 2D art, such as the one seen on floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the standard concept of a seamless texture, i.e., square images that are seamless when tiled, our method generates non-square tilings which comprise solely of repeating copies of the same object. It achieves this by optimizing both geometry and color of a 2D mesh, in order to generate a non-square tile in the shape and appearance of the desired object, with close to no additional background details. We enable geometric optimization of tilings by our key technical contribution: an unconstrained, differentiable parameterization of the space of all possible tileable shapes for a given symmetry group. Namely, we prove that modifying the laplacian used in a 2D mesh-mapping technique Orbifold Tutte Embedding - can achieve all possible tiling configurations for a chosen planar 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AIGC&#19982;&#25968;&#23383;&#21465;&#20107;&#30340;&#25972;&#21512;&#29366;&#24577;&#65292;&#21457;&#29616;&#34429;&#28982;AIGC&#22312;&#26576;&#20123;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#30340;&#21019;&#36896;&#21147;&#21644;&#23457;&#32654;&#24863;&#31561;&#22240;&#32032;&#65292;&#20173;&#26080;&#27861;&#26367;&#20195;&#20154;&#31867;&#22312;&#22797;&#26434;&#20154;&#29289;&#21160;&#30011;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#38899;&#25928;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.14329</link><description>&lt;p&gt;
AIGC&#30340;&#21019;&#26032;&#25968;&#23383;&#21465;&#20107;&#25506;&#32034;&#19982;&#35752;&#35770;&#65306;&#23545;&#26368;&#26032;&#36827;&#23637;&#30340;&#25506;&#32034;&#19982;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Innovative Digital Storytelling with AIGC: Exploration and Discussion of Recent Advances. (arXiv:2309.14329v1 [cs.HC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AIGC&#19982;&#25968;&#23383;&#21465;&#20107;&#30340;&#25972;&#21512;&#29366;&#24577;&#65292;&#21457;&#29616;&#34429;&#28982;AIGC&#22312;&#26576;&#20123;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#30340;&#21019;&#36896;&#21147;&#21644;&#23457;&#32654;&#24863;&#31561;&#22240;&#32032;&#65292;&#20173;&#26080;&#27861;&#26367;&#20195;&#20154;&#31867;&#22312;&#22797;&#26434;&#20154;&#29289;&#21160;&#30011;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#38899;&#25928;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21465;&#20107;&#20316;&#20026;&#19968;&#31181;&#33402;&#26415;&#24418;&#24335;&#65292;&#19968;&#30452;&#22312;&#36153;&#29992;&#19982;&#36136;&#37327;&#20043;&#38388;&#25379;&#25166;&#12290;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#20986;&#29616;&#34987;&#35270;&#20026;&#39640;&#25928;&#25968;&#23383;&#21465;&#20107;&#21046;&#20316;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#34701;&#21512;&#30340;&#20855;&#20307;&#24418;&#24335;&#12289;&#25928;&#26524;&#21644;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#65292;&#20351;&#24471;AIGC&#19982;&#21465;&#20107;&#30340;&#36793;&#30028;&#27169;&#31946;&#19981;&#28165;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AIGC&#19982;&#25968;&#23383;&#21465;&#20107;&#30340;&#24403;&#21069;&#25972;&#21512;&#29366;&#24577;&#65292;&#22312;&#26679;&#26412;&#39033;&#30446;&#20013;&#30740;&#31350;&#20102;&#20004;&#32773;&#34701;&#21512;&#30340;&#33402;&#26415;&#20215;&#20540;&#65292;&#24182;&#36890;&#36807;&#35775;&#35848;&#35299;&#20915;&#24120;&#35265;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;AIGC&#22312;&#22270;&#20687;&#21019;&#20316;&#12289;&#37197;&#38899;&#21046;&#20316;&#21644;&#38899;&#20048;&#21019;&#20316;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#23457;&#32654;&#24863;&#30340;&#19981;&#21487;&#26367;&#20195;&#20803;&#32032;&#65292;&#23588;&#20854;&#26159;&#22312;&#22797;&#26434;&#20154;&#29289;&#21160;&#30011;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#38899;&#25928;&#26041;&#38754;&#65292;AIGC&#36824;&#26080;&#27861;&#21462;&#20195;&#20154;&#31867;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22686;&#24378;&#20844;&#20247;&#23545;&#24403;&#21069;&#29366;&#24577;&#12289;&#38480;&#21046;&#21644;&#25361;&#25112;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital storytelling, as an art form, has struggled with cost-quality balance. The emergence of AI-generated Content (AIGC) is considered as a potential solution for efficient digital storytelling production. However, the specific form, effects, and impacts of this fusion remain unclear, leaving the boundaries of AIGC combined with storytelling undefined. This work explores the current integration state of AIGC and digital storytelling, investigates the artistic value of their fusion in a sample project, and addresses common issues through interviews. Through our study, we conclude that AIGC, while proficient in image creation, voiceover production, and music composition, falls short of replacing humans due to the irreplaceable elements of human creativity and aesthetic sensibilities at present, especially in complex character animations, facial expressions, and sound effects. The research objective is to increase public awareness of the current state, limitations, and challenges arisi
&lt;/p&gt;</description></item><item><title>Q-Bench&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#32423;&#21035;&#35270;&#35273;&#24863;&#30693;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14181</link><description>&lt;p&gt;
Q-Bench: &#19968;&#31181;&#29992;&#20110;&#20302;&#32423;&#21035;&#35270;&#35273;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision. (arXiv:2309.14181v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14181
&lt;/p&gt;
&lt;p&gt;
Q-Bench&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#32423;&#21035;&#35270;&#35273;&#24863;&#30693;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20174;&#19987;&#38376;&#27169;&#22411;&#21521;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLMs&#22312;&#20302;&#32423;&#21035;&#35270;&#35273;&#24863;&#30693;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Q-Bench&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;MLLMs&#22312;&#19977;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#33021;&#21147;&#65306;&#20302;&#32423;&#21035;&#35270;&#35273;&#24863;&#30693;&#12289;&#20302;&#32423;&#21035;&#35270;&#35273;&#25551;&#36848;&#21644;&#25972;&#20307;&#35270;&#35273;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions o
&lt;/p&gt;</description></item><item><title>AgriSORT&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22312;&#32447;&#23454;&#26102;&#26816;&#27979;&#36319;&#36394;&#26694;&#26550;&#65292;&#20027;&#35201;&#35299;&#20915;&#31934;&#20934;&#20892;&#19994;&#20013;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#38382;&#39064;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#36816;&#21160;&#20449;&#24687;&#36827;&#34892;&#20851;&#32852;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#36319;&#36394;&#36712;&#36857;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2309.13393</link><description>&lt;p&gt;
AgriSORT: &#19968;&#31181;&#29992;&#20110;&#20892;&#19994;&#26426;&#22120;&#20154;&#31934;&#20934;&#20892;&#19994;&#30340;&#31616;&#21333;&#22312;&#32447;&#23454;&#26102;&#26816;&#27979;&#36319;&#36394;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AgriSORT: A Simple Online Real-time Tracking-by-Detection framework for robotics in precision agriculture. (arXiv:2309.13393v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13393
&lt;/p&gt;
&lt;p&gt;
AgriSORT&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22312;&#32447;&#23454;&#26102;&#26816;&#27979;&#36319;&#36394;&#26694;&#26550;&#65292;&#20027;&#35201;&#35299;&#20915;&#31934;&#20934;&#20892;&#19994;&#20013;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#38382;&#39064;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#36816;&#21160;&#20449;&#24687;&#36827;&#34892;&#20851;&#32852;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#36319;&#36394;&#36712;&#36857;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#38382;&#39064;&#26159;&#25351;&#22312;&#35270;&#39057;&#24207;&#21015;&#20013;&#26816;&#27979;&#21644;&#36319;&#36394;&#25152;&#26377;&#29289;&#20307;&#65292;&#24182;&#20026;&#27599;&#20010;&#29289;&#20307;&#20445;&#25345;&#21807;&#19968;&#30340;&#26631;&#35782;&#31526;&#12290;&#36825;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22522;&#30784;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#31934;&#20934;&#20892;&#19994;&#20013;&#65292;&#35201;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30528;&#25668;&#20687;&#26426;&#36816;&#21160;&#21095;&#28872;&#12289;&#20809;&#29031;&#31361;&#21464;&#21644;&#20005;&#37325;&#36974;&#25377;&#31561;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#20195;&#36319;&#36394;&#22120;&#20381;&#36182;&#20110;&#29289;&#20307;&#30340;&#22806;&#35266;&#32780;&#19981;&#26159;&#36816;&#21160;&#26469;&#36827;&#34892;&#20851;&#32852;&#65292;&#22312;&#20892;&#19994;&#39046;&#22495;&#20013;&#38745;&#27490;&#29289;&#20307;&#20855;&#26377;&#30456;&#21516;&#22806;&#35266;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#19981;&#22826;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#22312;SORT&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AgriSORT&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#12289;&#22312;&#32447;&#30340;&#12289;&#23454;&#26102;&#30340;&#22522;&#20110;&#36816;&#21160;&#20449;&#24687;&#30340;&#31934;&#20934;&#20892;&#19994;&#26816;&#27979;&#36319;&#36394;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#22312;&#24103;&#38388;&#20934;&#30830;&#19988;&#24555;&#36895;&#22320;&#20256;&#25773;&#36319;&#36394;&#36712;&#36857;&#12290;AgriSORT&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#25928;&#29575;&#12289;&#28789;&#27963;&#24615;&#12289;&#26368;&#23567;&#20381;&#36182;&#24615;&#20197;&#21450;&#22312;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#30340;&#26131;&#37096;&#32626;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of multi-object tracking (MOT) consists in detecting and tracking all the objects in a video sequence while keeping a unique identifier for each object. It is a challenging and fundamental problem for robotics. In precision agriculture the challenge of achieving a satisfactory solution is amplified by extreme camera motion, sudden illumination changes, and strong occlusions. Most modern trackers rely on the appearance of objects rather than motion for association, which can be ineffective when most targets are static objects with the same appearance, as in the agricultural case. To this end, on the trail of SORT [5], we propose AgriSORT, a simple, online, real-time tracking-by-detection pipeline for precision agriculture based only on motion information that allows for accurate and fast propagation of tracks between frames. The main focuses of AgriSORT are efficiency, flexibility, minimal dependencies, and ease of deployment on robotic platforms. We test the proposed pipeli
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;It-LLMs&#65289;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#30340;&#40065;&#26834;&#24615;&#33021;&#21147;&#65292;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12481</link><description>&lt;p&gt;
HANS&#65292;&#20320;&#32874;&#26126;&#21527;&#65311;&#31070;&#32463;&#31995;&#32479;&#30340;Clever Hans&#25928;&#24212;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
HANS, are you clever? Clever Hans Effect Analysis of Neural Systems. (arXiv:2309.12481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;It-LLMs&#65289;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQ&#65289;&#30340;&#40065;&#26834;&#24615;&#33021;&#21147;&#65292;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(It-LLMs)&#23637;&#31034;&#20986;&#20102;&#22312;&#35748;&#30693;&#29366;&#24577;&#12289;&#24847;&#22270;&#21644;&#21453;&#24212;&#26041;&#38754;&#25512;&#29702;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#21487;&#20197;&#35753;&#20154;&#20204;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#29702;&#35299;&#26085;&#24120;&#31038;&#20132;&#20114;&#21160;&#12290;&#20107;&#23454;&#19978;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;(MCQ)&#22522;&#20934;&#26469;&#26500;&#24314;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#30830;&#20999;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;It-LLMs&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#8220;&#39034;&#24207;&#20559;&#35265;&#8221;&#65292;&#32473;&#36866;&#24403;&#30340;&#35780;&#20272;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;MCQ&#22522;&#20934;&#23545;It-LLMs&#30340;&#25269;&#25239;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#39034;&#24207;&#21464;&#21160;&#26102;&#65292;&#25581;&#31034;&#20102;&#36873;&#25321;&#20559;&#35265;&#24182;&#24341;&#21457;&#20102;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#35752;&#35770;&#12290;&#36890;&#36807;&#31532;&#19968;&#20301;&#32622;&#21644;&#27169;&#22411;&#36873;&#25321;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#27169;&#22411;&#20013;&#23384;&#22312;&#32467;&#26500;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (It-LLMs) have been exhibiting outstanding abilities to reason around cognitive states, intentions, and reactions of all people involved, letting humans guide and comprehend day-to-day social interactions effectively. In fact, several multiple-choice questions (MCQ) benchmarks have been proposed to construct solid assessments of the models' abilities. However, earlier works are demonstrating the presence of inherent "order bias" in It-LLMs, posing challenges to the appropriate evaluation. In this paper, we investigate It-LLMs' resilience abilities towards a series of probing tests using four MCQ benchmarks. Introducing adversarial examples, we show a significant performance gap, mainly when varying the order of the choices, which reveals a selection bias and brings into discussion reasoning abilities. Following a correlation between first positions and model choices due to positional bias, we hypothesized the presence of structural heuristics in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#35270;&#35273;&#21644;&#35302;&#35273;&#24863;&#30693;&#23454;&#29616;&#25163;&#20013;&#29289;&#20307;&#36890;&#29992;&#26059;&#36716;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#27169;&#25311;&#25512;&#26029;&#29289;&#20307;&#30340;&#24418;&#29366;&#21644;&#29289;&#29702;&#23646;&#24615;&#65292;&#24182;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.09979</link><description>&lt;p&gt;
&#20855;&#22791;&#35270;&#35273;&#21644;&#35302;&#35273;&#30340;&#25163;&#20013;&#29289;&#20307;&#36890;&#29992;&#26059;&#36716;
&lt;/p&gt;
&lt;p&gt;
General In-Hand Object Rotation with Vision and Touch. (arXiv:2309.09979v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#35270;&#35273;&#21644;&#35302;&#35273;&#24863;&#30693;&#23454;&#29616;&#25163;&#20013;&#29289;&#20307;&#36890;&#29992;&#26059;&#36716;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#27169;&#25311;&#25512;&#26029;&#29289;&#20307;&#30340;&#24418;&#29366;&#21644;&#29289;&#29702;&#23646;&#24615;&#65292;&#24182;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RotateIt&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#24863;&#30693;&#36755;&#20837;&#65292;&#20351;&#25351;&#23574;&#33021;&#22815;&#22312;&#22810;&#20010;&#36724;&#19978;&#36827;&#34892;&#29289;&#20307;&#26059;&#36716;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21487;&#20197;&#33719;&#21462;&#29289;&#20307;&#30340;&#30495;&#23454;&#24418;&#29366;&#21644;&#29289;&#29702;&#23646;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#20854;&#31616;&#21270;&#20026;&#22312;&#30495;&#23454;&#20294;&#22122;&#22768;&#24178;&#25200;&#19979;&#30340;&#27169;&#25311;&#35302;&#35273;&#21644;&#26412;&#20307;&#24863;&#30693;&#36755;&#20837;&#12290;&#36825;&#20123;&#22810;&#27169;&#24577;&#36755;&#20837;&#36890;&#36807;&#35270;&#35273;&#35302;&#35273;&#21464;&#25442;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20351;&#24471;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#21487;&#20197;&#36827;&#34892;&#29289;&#20307;&#24418;&#29366;&#21644;&#29289;&#29702;&#23646;&#24615;&#30340;&#22312;&#32447;&#25512;&#26029;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#30340;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35270;&#35273;&#21644;&#35302;&#35273;&#24863;&#30693;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce RotateIt, a system that enables fingertip-based object rotation along multiple axes by leveraging multimodal sensory inputs. Our system is trained in simulation, where it has access to ground-truth object shapes and physical properties. Then we distill it to operate on realistic yet noisy simulated visuotactile and proprioceptive sensory inputs. These multimodal inputs are fused via a visuotactile transformer, enabling online inference of object shapes and physical properties during deployment. We show significant performance improvements over prior methods and the importance of visual and tactile sensing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#38750;&#31283;&#24577;&#28418;&#31227;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#36827;&#34892;&#20998;&#31867;&#26694;&#26550;&#35774;&#35745;&#65292;&#37319;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20845;&#20010;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#20102;&#19971;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#20004;&#31181;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.09175</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#36827;&#34892;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Imbalanced Data Stream Classification using Dynamic Ensemble Selection. (arXiv:2309.09175v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#38750;&#31283;&#24577;&#28418;&#31227;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#36827;&#34892;&#20998;&#31867;&#26694;&#26550;&#35774;&#35745;&#65292;&#37319;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20845;&#20010;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#20102;&#19971;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#20004;&#31181;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#27969;&#25968;&#25454;&#20998;&#31867;&#38754;&#20020;&#30528;&#27010;&#24565;&#28418;&#31227;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#23545;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#20998;&#31867;&#19981;&#27491;&#30830;&#12290;&#27492;&#22806;&#65292;&#22810;&#20010;&#31867;&#21035;&#30340;&#37325;&#21472;&#31561;&#20854;&#20182;&#22240;&#32032;&#38480;&#21046;&#20102;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#31243;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#38750;&#31283;&#24577;&#28418;&#31227;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#36827;&#34892;&#20998;&#31867;&#26694;&#26550;&#35774;&#35745;&#65292;&#37319;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20845;&#20010;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#65292;&#36825;&#20123;&#25968;&#25454;&#27969;&#20855;&#26377;&#19981;&#21516;&#30340;&#19981;&#24179;&#34913;&#27604;&#20363;&#65292;&#24182;&#19988;&#21253;&#21547;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27010;&#24565;&#28418;&#31227;&#12290;&#27599;&#20010;&#25968;&#25454;&#27969;&#30001;200&#20010;500&#20010;&#23545;&#35937;&#30340;&#22359;&#32452;&#25104;&#65292;&#27599;&#20010;&#23545;&#35937;&#30001;&#20843;&#20010;&#29305;&#24449;&#25551;&#36848;&#65292;&#24182;&#19988;&#21253;&#21547;&#20116;&#20010;&#27010;&#24565;&#28418;&#31227;&#12290;&#32771;&#34385;&#20102;&#19971;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#20004;&#31181;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern streaming data categorization faces significant challenges from concept drift and class imbalanced data. This negatively impacts the output of the classifier, leading to improper classification. Furthermore, other factors such as the overlapping of multiple classes limit the extent of the correctness of the output. This work proposes a novel framework for integrating data pre-processing and dynamic ensemble selection, by formulating the classification framework for the nonstationary drifting imbalanced data stream, which employs the data pre-processing and dynamic ensemble selection techniques. The proposed framework was evaluated using six artificially generated data streams with differing imbalance ratios in combination with two different types of concept drifts. Each stream is composed of 200 chunks of 500 objects described by eight features and contains five concept drifts. Seven pre-processing techniques and two dynamic ensemble selection methods were considered. According 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#29289;&#32852;&#32593;&#29615;&#22659;&#23450;&#21046;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#22534;&#21472;&#23376;&#32858;&#31867;&#25216;&#26415;&#26469;&#35782;&#21035;&#26410;&#30693;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.07461</link><description>&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#26816;&#27979;&#26410;&#30693;&#25915;&#20987;: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07461
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#29289;&#32852;&#32593;&#29615;&#22659;&#23450;&#21046;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#22534;&#21472;&#23376;&#32858;&#31867;&#25216;&#26415;&#26469;&#35782;&#21035;&#26410;&#30693;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#35774;&#22791;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#36825;&#24341;&#20837;&#20102;&#20114;&#32852;&#30340;&#26102;&#20195;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24378;&#22823;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23433;&#20840;&#31995;&#32479;&#26159;&#22522;&#20110;&#23553;&#38381;&#19990;&#30028;&#35270;&#35282;&#35774;&#35745;&#30340;&#65292;&#24448;&#24448;&#38754;&#20020;&#19982;&#19981;&#26029;&#21457;&#23637;&#30340;&#23041;&#32961;&#29615;&#22659;&#20013;&#26032;&#30340;&#12289;&#38476;&#29983;&#30340;&#25915;&#20987;&#30456;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29289;&#32852;&#32593;&#29615;&#22659;&#19979;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#20013;&#30340;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20174;&#32593;&#32476;&#27969;&#37327;&#20013;&#25552;&#21462;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38598;&#25104;&#20102;&#22534;&#21472;&#21644;&#23376;&#32858;&#31867;&#25216;&#26415;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#33391;&#24615;&#34892;&#20026;&#65292;&#23454;&#29616;&#23545;&#26410;&#30693;&#25915;&#20987;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread integration of Internet of Things (IoT) devices across all facets of life has ushered in an era of interconnectedness, creating new avenues for cybersecurity challenges and underscoring the need for robust intrusion detection systems. However, traditional security systems are designed with a closed-world perspective and often face challenges in dealing with the ever-evolving threat landscape, where new and unfamiliar attacks are constantly emerging. In this paper, we introduce a framework aimed at mitigating the open set recognition (OSR) problem in the realm of Network Intrusion Detection Systems (NIDS) tailored for IoT environments. Our framework capitalizes on image-based representations of packet-level data, extracting spatial and temporal patterns from network traffic. Additionally, we integrate stacking and sub-clustering techniques, enabling the identification of unknown attacks by effectively modeling the complex and diverse nature of benign behavior. The empiric
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEyes&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#30524;&#37096;&#22270;&#20687;&#21464;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06129</link><description>&lt;p&gt;
LEyes&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images. (arXiv:2309.06129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEyes&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#30524;&#37096;&#22270;&#20687;&#21464;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21152;&#24378;&#20102;&#20957;&#35270;&#20272;&#35745;&#25216;&#26415;&#65292;&#20294;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#19981;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#30524;&#37096;&#22270;&#20687;&#30340;&#30828;&#20214;&#24341;&#36215;&#30340;&#21464;&#24322;&#20197;&#21450;&#35760;&#24405;&#30340;&#21442;&#19982;&#32773;&#20043;&#38388;&#22266;&#26377;&#30340;&#29983;&#29289;&#24046;&#24322;&#20250;&#23548;&#33268;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#24046;&#24322;&#65292;&#38459;&#30861;&#20102;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#34394;&#25311;&#25968;&#25454;&#38598;&#21487;&#20197;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21019;&#24314;&#34394;&#25311;&#25968;&#25454;&#38598;&#26082;&#38656;&#35201;&#26102;&#38388;&#21448;&#38656;&#35201;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Light Eyes or "LEyes"&#30340;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#30340;&#36924;&#30495;&#26041;&#27861;&#19981;&#21516;&#65292;LEyes&#20165;&#27169;&#25311;&#35270;&#39057;&#30524;&#21160;&#36319;&#36394;&#25152;&#38656;&#30340;&#20851;&#38190;&#22270;&#20687;&#29305;&#24449;&#12290;LEyes&#20415;&#20110;&#22312;&#22810;&#26679;&#21270;&#30340;&#20957;&#35270;&#20272;&#35745;&#20219;&#21153;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30524;&#30555;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has bolstered gaze estimation techniques, but real-world deployment has been impeded by inadequate training datasets. This problem is exacerbated by both hardware-induced variations in eye images and inherent biological differences across the recorded participants, leading to both feature and pixel-level variance that hinders the generalizability of models trained on specific datasets. While synthetic datasets can be a solution, their creation is both time and resource-intensive. To address this problem, we present a framework called Light Eyes or "LEyes" which, unlike conventional photorealistic methods, only models key image features required for video-based eye tracking using simple light distributions. LEyes facilitates easy configuration for training neural networks across diverse gaze-estimation tasks. We demonstrate that models trained using LEyes outperform other state-of-the-art algorithms in terms of pupil and CR localization across well-known datasets. In addit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24773;&#24863;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.14359</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks. (arXiv:2308.14359v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#23884;&#20837;&#23545;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24773;&#24863;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#20013;&#65292;&#20154;&#31867;&#24773;&#24863;&#29702;&#35299;&#22312;&#20351;&#23545;&#35805;&#25216;&#26415;&#25104;&#20026;&#20027;&#27969;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#35821;&#38899;&#24773;&#24863;&#29702;&#35299;&#35270;&#20026;&#19968;&#31181;&#30693;&#35273;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#29616;&#23454;&#30340;&#24773;&#26223;&#12290;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#65288;&#35821;&#35328;&#65292;&#20154;&#21475;&#32479;&#35745;&#23398;&#31561;&#65289;&#65292;&#19981;&#21516;&#27604;&#20363;&#30340;&#20154;&#20250;&#23558;&#30456;&#21516;&#30340;&#35821;&#38899;&#29255;&#27573;&#35270;&#20026;&#38750;&#19968;&#33268;&#30340;&#24773;&#24863;&#12290;&#20316;&#20026;ACM&#22810;&#23186;&#20307;2023&#35745;&#31639;&#35821;&#38899;&#32852;&#26426;&#25361;&#25112;&#65288;ComParE&#65289;&#30340;&#19968;&#37096;&#20998;&#65292;&#22312;EMotion Share&#36712;&#36947;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#20182;&#20204;&#20016;&#23500;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#21644;&#22810;&#26631;&#31614;&#22238;&#24402;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#8220;&#24773;&#24863;&#20998;&#20139;&#8221;&#25110;&#23545;&#35813;&#24773;&#24863;&#30340;&#24863;&#30693;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#26696;&#20915;&#23450;&#20102;&#23427;&#20204;&#22312;&#36229;&#36234;&#35821;&#38899;&#35782;&#21035;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24773;&#24863;&#29702;&#35299;&#31561;&#38750;&#35821;&#20041;&#35821;&#38899;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#65292;&#30446;&#26631;&#26631;&#31614;&#30340;&#21464;&#21270;&#20197;&#21450;&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#30340;&#22266;&#26377;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36731;&#37327;&#32423;HuBERT-Large&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human emotion understanding is pivotal in making conversational technology mainstream. We view speech emotion understanding as a perception task which is a more realistic setting. With varying contexts (languages, demographics, etc.) different share of people perceive the same speech segment as a non-unanimous emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset of multilingual speakers and multi-label regression target of 'emotion share' or perception of that emotion. We demonstrate that the training scheme of different foundation models dictates their effectiveness for tasks beyond speech recognition, especially for non-semantic speech tasks like emotion understanding. This is a very complex task due to multilingual speakers, variability in the target labels, and inherent imbalance in the regression dataset. Our results show that HuBERT-Large with a self-attention-based light-weight se
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22270;&#19978;&#22522;&#20110;QUBO&#20844;&#24335;&#30340;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#20844;&#24335;&#24418;&#24335;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;...</title><link>http://arxiv.org/abs/2308.13978</link><description>&lt;p&gt;
&#29702;&#35299;&#22522;&#20110;QUBO&#30340;&#21704;&#23494;&#39039;&#20989;&#25968;&#22312;&#22270;&#19978;&#30340;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#20351;&#29992;&#65306;&#20197;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20026;&#20363;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Understanding the Usage of QUBO-based Hamiltonian Function in Combinatorial Optimization over Graphs: A Discussion Using Max Cut (MC) Problem. (arXiv:2308.13978v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13978
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22270;&#19978;&#22522;&#20110;QUBO&#20844;&#24335;&#30340;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#20844;&#24335;&#24418;&#24335;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#65288;QUBO&#65289;&#26159;&#19968;&#31181;&#24191;&#20041;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#21508;&#31181;NP&#22256;&#38590;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#24314;&#27169;&#20026;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#24418;&#24335;&#12290;&#21704;&#23494;&#39039;&#20989;&#25968;&#32463;&#24120;&#29992;&#20110;&#24418;&#25104;QUBO&#38382;&#39064;&#65292;&#20854;&#20013;&#23427;&#22312;&#20248;&#21270;&#30340;&#19978;&#19979;&#25991;&#20013;&#34987;&#29992;&#20316;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#35299;&#20915;QUBO&#20844;&#24335;&#20013;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#22312;&#33410;&#28857;&#20043;&#38388;&#20256;&#36882;&#20449;&#24687;&#12290;&#25105;&#20204;&#20027;&#35201;&#30740;&#31350;&#20102;&#19977;&#31181;&#20844;&#24335;&#65292;Monty-Carlo Tree Search with GNN-based RL&#65288;MCTS-GNN&#65289;&#12289;DQN with GNN-based RL&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#30340;&#36890;&#29992;GNN&#65288;GRL&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to model various NP-hard combinatorial optimization problems in the form of binary variables. The Hamiltonian function is often used to formulate QUBO problems where it is used as the objective function in the context of optimization. In this study, we investigate how reinforcement learning-based (RL) paradigms with the presence of the Hamiltonian function can address combinatorial optimization problems over graphs in QUBO formulations. We use Graph Neural Network (GNN) as the message-passing architecture to convey the information among the nodes. We have centered our discussion on QUBO formulated Max-Cut problem but the intuitions can be extended to any QUBO supported canonical NP-Hard combinatorial optimization problems. We mainly investigate three formulations, Monty-Carlo Tree Search with GNN-based RL (MCTS-GNN), DQN with GNN-based RL, and a generic GNN with attention-based RL (GRL). Our findings state that i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#24605;&#24819;&#31639;&#27861;"&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#24819;&#25506;&#32034;&#65292;&#20197;&#20302;&#25104;&#26412;&#12289;&#20302;&#23384;&#20648;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#12290;</title><link>http://arxiv.org/abs/2308.10379</link><description>&lt;p&gt;
&#24605;&#24819;&#31639;&#27861;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#24819;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models. (arXiv:2308.10379v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#24605;&#24819;&#31639;&#27861;"&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#24819;&#25506;&#32034;&#65292;&#20197;&#20302;&#25104;&#26412;&#12289;&#20302;&#23384;&#20648;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#29486;&#26088;&#22312;&#36229;&#36234;&#8220;&#36830;&#32493;&#24605;&#32500;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#37319;&#29992;&#22806;&#37096;&#25805;&#20316;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20572;&#27490;&#12289;&#20462;&#25913;&#65292;&#28982;&#21518;&#24674;&#22797;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#27169;&#24335;&#22686;&#21152;&#20102;&#26597;&#35810;&#35831;&#27714;&#30340;&#25968;&#37327;&#65292;&#22686;&#21152;&#20102;&#25104;&#26412;&#12289;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#24819;&#31639;&#27861;&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;LLM&#65292;&#24320;&#21019;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#24335;&#12290;&#36890;&#36807;&#20351;&#29992;&#31639;&#27861;&#31034;&#20363;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#30340;&#22266;&#26377;&#24490;&#29615;&#21160;&#21147;&#23398;&#65292;&#20165;&#20351;&#29992;&#19968;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#26597;&#35810;&#25193;&#23637;&#20854;&#24605;&#24819;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20248;&#20110;&#26089;&#26399;&#30340;&#21333;&#27425;&#26597;&#35810;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#36817;&#37319;&#29992;&#24191;&#27867;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#30340;&#22810;&#27425;&#26597;&#35810;&#31574;&#30053;&#19981;&#30456;&#19978;&#19979;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;LLM&#21487;&#20197;&#20351;&#24615;&#33021;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#65292;&#36825;&#26263;&#31034;&#30528;
&lt;/p&gt;
&lt;p&gt;
Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#30456;&#32467;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21327;&#21516;&#26041;&#27861;&#20114;&#34917;&#21508;&#33258;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#21644;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09830</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#30340;&#21327;&#21516;&#38598;&#25104;&#23545;&#20110;&#31283;&#20581;&#20154;&#24037;&#26234;&#33021;&#30340;&#25506;&#32034;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis. (arXiv:2308.09830v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#30456;&#32467;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21327;&#21516;&#26041;&#27861;&#20114;&#34917;&#21508;&#33258;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#21644;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#26500;&#24314;&#34920;&#29616;&#20986;&#26234;&#33021;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26102;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#35748;&#30693;&#26550;&#26500;(CAs) &#36827;&#34892;&#38598;&#25104;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#29702;&#35770;&#27169;&#22411;&#30340;&#25351;&#23548;&#19979;&#65292;&#36890;&#36807;&#21021;&#27493;&#30340;&#32463;&#39564;&#25968;&#25454;&#25903;&#25345;&#65292;&#25105;&#20204;&#20551;&#35774;&#19981;&#21516;&#30340;&#21327;&#21516;&#26041;&#27861;&#21487;&#20197;&#20114;&#34917;&#23427;&#20204;&#21508;&#33258;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#20174;&#32780;&#22521;&#32946;&#20986;&#26356;&#31283;&#20581;&#21644;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#25152;&#28041;&#21450;&#30340;&#26435;&#34913;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores alternatives for integrating two subdisciplines of AI in the construction of artificial agents that exhibit intelligent behavior: Large Language Models (LLMs) and Cognitive Architectures (CAs). Guided by theoretical models and supported by preliminary empirical data, we hypothesize how diverse synergistic approaches can mutually compensate for their respective weaknesses and limitations, ultimately fostering more robust and sophisticated artificial intelligence systems. Additionally, we discuss the tradeoffs and challenges associated with each approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;LLM&#29983;&#25104;&#30340;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#34920;&#31034;&#20026;&#25512;&#29702;&#22270;&#65292;&#20174;&#32780;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.09267</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#39564;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Reasoning Capabilities of Large Language Models: A Graph-Based Verification Approach. (arXiv:2308.09267v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;LLM&#29983;&#25104;&#30340;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#34920;&#31034;&#20026;&#25512;&#29702;&#22270;&#65292;&#20174;&#32780;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#31561;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#21463;&#29305;&#23450;&#35774;&#35745;&#30340;&#25552;&#31034;&#25351;&#23548;&#19979;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#36825;&#19981;&#20165;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36824;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#38382;&#39064;&#27714;&#35299;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#25552;&#21319;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;LLM&#36755;&#20986;&#39564;&#35777;&#22120;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#36825;&#20123;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#24819;&#19968;&#20010;&#25512;&#29702;&#20219;&#21153;&#30340;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#30001;LLM&#29983;&#25104;&#65292;&#21487;&#20197;&#30001;&#25512;&#29702;&#22270;&#34920;&#31034;&#65292;&#22240;&#20026;&#19981;&#21516;&#25512;&#29702;&#36335;&#24452;&#30340;&#20013;&#38388;&#27493;&#39588;&#20043;&#38388;&#23384;&#22312;&#36923;&#36753;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25512;&#29702;&#22270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased impressive reasoning capabilities, particularly when guided by specifically designed prompts in complex reasoning tasks such as math word problems. These models typically solve tasks using a chain-of-thought approach, which not only bolsters their reasoning abilities but also provides valuable insights into their problem-solving process. However, there is still significant room for enhancing the reasoning abilities of LLMs. Some studies suggest that the integration of an LLM output verifier can boost reasoning accuracy without necessitating additional model training. In this paper, we follow these studies and introduce a novel graph-based method to further augment the reasoning capabilities of LLMs. We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths. Therefore, we propose the Reasoning Graph 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#24615;&#20381;&#36182;&#35268;&#21010;&#30340;HTN&#35268;&#21010;&#22120;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35745;&#21010;&#12290;&#36890;&#36807;&#25193;&#23637;HTN&#35268;&#21010;&#30340;&#24418;&#24335;&#21270;&#21644;&#24341;&#20837;&#26032;&#39062;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#35813;&#35268;&#21010;&#22120;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29615;&#22659;&#20013;&#20135;&#29983;&#28789;&#27963;&#19988;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06922</link><description>&lt;p&gt;
&#22522;&#20110;HTN&#30340;&#27010;&#29575;&#24615;&#20381;&#36182;&#35268;&#21010;&#29992;&#20110;&#39640;&#36136;&#37327;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Probabilistic contingent planning based on HTN for high-quality plans. (arXiv:2308.06922v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06922
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#24615;&#20381;&#36182;&#35268;&#21010;&#30340;HTN&#35268;&#21010;&#22120;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35745;&#21010;&#12290;&#36890;&#36807;&#25193;&#23637;HTN&#35268;&#21010;&#30340;&#24418;&#24335;&#21270;&#21644;&#24341;&#20837;&#26032;&#39062;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#35813;&#35268;&#21010;&#22120;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29615;&#22659;&#20013;&#20135;&#29983;&#28789;&#27963;&#19988;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#24615;&#35268;&#21010;&#20551;&#35774;&#35268;&#21010;&#27839;&#30528;&#23436;&#20840;&#21487;&#39044;&#27979;&#30340;&#36335;&#24452;&#21457;&#23637;&#65292;&#22240;&#27492;&#22312;&#22823;&#22810;&#25968;&#30495;&#23454;&#30340;&#24773;&#26223;&#20013;&#22833;&#21435;&#20102;&#23454;&#38469;&#20215;&#20540;&#12290;&#26356;&#29616;&#23454;&#30340;&#35266;&#28857;&#26159;&#35268;&#21010;&#24212;&#35813;&#20107;&#20808;&#32771;&#34385;&#21040;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#24182;&#20026;&#26356;&#28789;&#27963;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#32780;&#21162;&#21147;&#12290;&#32780;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#35268;&#21010;&#30340;&#36136;&#37327;&#19981;&#21487;&#36991;&#20813;&#22320;&#21464;&#21270;&#36739;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#24615;&#20381;&#36182;Hierarchical Task Network (HTN) &#35268;&#21010;&#22120;&#65292;&#31216;&#20026;High-Quality Contingent Planner (HQCP)&#65292;&#20197;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35745;&#21010;&#12290;&#25105;&#20204;&#23558;HTN&#35268;&#21010;&#20013;&#30340;&#24418;&#24335;&#21270;&#25193;&#23637;&#21040;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#24182;&#23545;&#20854;&#25104;&#26412;&#36827;&#34892;&#35780;&#20272;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#36136;&#37327;&#35745;&#21010;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#24182;&#24320;&#21457;&#20102;&#38598;&#25104;&#35268;&#21010;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#39564;&#35777;&#20102;&#35268;&#21010;&#22120;&#22312;&#27010;&#29575;&#24615;&#20381;&#36182;&#35268;&#21010;&#21644;&#39640;&#36136;&#37327;&#35745;&#21010;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deterministic planning assumes that the planning evolves along a fully predictable path, and therefore it loses the practical value in most real projections. A more realistic view is that planning ought to take into consideration partial observability beforehand and aim for a more flexible and robust solution. What is more significant, it is inevitable that the quality of plan varies dramatically in the partially observable environment. In this paper we propose a probabilistic contingent Hierarchical Task Network (HTN) planner, named High-Quality Contingent Planner (HQCP), to generate high-quality plans in the partially observable environment. The formalisms in HTN planning are extended into partial observability and are evaluated regarding the cost. Next, we explore a novel heuristic for high-quality plans and develop the integrated planning algorithm. Finally, an empirical study verifies the effectiveness and efficiency of the planner both in probabilistic contingent planning and for
&lt;/p&gt;</description></item><item><title>VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.06595</link><description>&lt;p&gt;
VisIT-Bench: &#19968;&#20010;&#21463;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#21551;&#21457;&#30340;&#35270;&#35273;&#35821;&#35328;&#25351;&#31034;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06595
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VisIT-Bench&#65288;Visual InsTruction Benchmark&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20215;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#31034;&#36981;&#24490;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;&#31574;&#21010;&#20102;70&#20010;&#8220;&#25351;&#31034;&#23478;&#26063;&#8221;&#65292;&#25105;&#20204;&#35748;&#20026;&#25351;&#31034;&#35843;&#20248;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#23478;&#26063;&#12290;&#20219;&#21153;&#19981;&#20165;&#38480;&#20110;VQAv2&#21644;COCO&#31561;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20174;&#22522;&#26412;&#35782;&#21035;&#21040;&#28216;&#25103;&#29609;&#27861;&#21644;&#21019;&#36896;&#24615;&#29983;&#25104;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#22312;&#31574;&#21010;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;592&#20010;&#27979;&#35797;&#26597;&#35810;&#65292;&#27599;&#20010;&#26597;&#35810;&#37117;&#24102;&#26377;&#19968;&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#31034;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#12290;&#36825;&#20123;&#25551;&#36848;&#23637;&#29616;&#20102;&#29305;&#23450;&#25351;&#31034;&#22240;&#32032;&#65292;&#20363;&#22914;&#23545;&#20110;&#35810;&#38382;&#24215;&#38754;&#23545;&#20110;&#36718;&#26885;&#29992;&#25143;&#30340;&#26131;&#35775;&#38382;&#24615;&#30340;&#25351;&#31034;&#65292;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#25551;&#36848;&#20102;&#26012;&#22369;/&#28508;&#22312;&#38556;&#30861;&#29289;&#12290;&#36825;&#20123;&#25551;&#36848;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#65306;1&#65289;&#25910;&#38598;&#27599;&#20010;&#23454;&#20363;&#30340;&#20154;&#24037;&#39564;&#35777;&#30340;&#21442;&#32771;&#36755;&#20986;&#65307;2&#65289;&#20351;&#29992;&#20165;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20505;&#36873;&#22810;&#27169;&#24577;&#29983;&#25104;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#65292;&#35774;&#35745;&#20102;&#33021;&#21516;&#26102;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04412</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Invariant Learning with Randomized Linear Classifiers. (arXiv:2308.04412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#65292;&#35774;&#35745;&#20102;&#33021;&#21516;&#26102;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26082;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21448;&#33021;&#20445;&#25345;&#20219;&#21153;&#24050;&#30693;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22312;&#19981;&#21464;&#24615;&#21644;&#35745;&#31639;&#25110;&#20869;&#23384;&#36164;&#28304;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38543;&#26426;&#24615;&#35774;&#35745;&#26082;&#20855;&#34920;&#36798;&#33021;&#21147;&#21448;&#20855;&#19981;&#21464;&#24615;&#20294;&#20351;&#29992;&#26356;&#23569;&#36164;&#28304;&#30340;&#27169;&#22411;&#12290;&#21463;&#38543;&#26426;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#21487;&#20197;&#20943;&#23569;&#36164;&#28304;&#38656;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#31216;&#20026;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120; (RLCs) &#30340;&#20108;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#21442;&#25968;&#21644;&#26679;&#26412;&#22823;&#23567;&#30340;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;RLCs &#21487;&#20197;&#20197;&#39640;&#27010;&#29575;&#36924;&#36817;&#20219;&#20309;&#65288;&#24179;&#28369;&#65289;&#20989;&#25968;&#65292;&#24182;&#20445;&#25345;&#23545;&#32039;&#33268;&#32676;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#21487;&#39564;&#35777;&#22320;&#27010;&#29575;&#19981;&#21464;&#30340; RLCs&#65292;&#29992;&#20110;&#38598;&#21512;&#12289;&#22270;&#21644;&#29699;&#24418;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#23454;&#29616;&#27010;&#29575;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing models that are both expressive and preserve known invariances of tasks is an increasingly hard problem. Existing solutions tradeoff invariance for computational or memory resources. In this work, we show how to leverage randomness and design models that are both expressive and invariant but use less resources. Inspired by randomized algorithms, our key insight is that accepting probabilistic notions of universal approximation and invariance can reduce our resource requirements. More specifically, we propose a class of binary classification models called Randomized Linear Classifiers (RLCs). We give parameter and sample size conditions in which RLCs can, with high probability, approximate any (smooth) function while preserving invariance to compact group transformations. Leveraging this result, we design three RLCs that are provably probabilistic invariant for classification tasks over sets, graphs, and spherical data. We show how these models can achieve probabilistic invari
&lt;/p&gt;</description></item><item><title>TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2307.06822</link><description>&lt;p&gt;
TinyMetaFed: &#39640;&#25928;&#30340;&#29992;&#20110;TinyML&#30340;&#32852;&#37030;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed: Efficient Federated Meta-Learning for TinyML. (arXiv:2307.06822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06822
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning (TinyML)&#39046;&#22495;&#22312;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#65288;&#22914;&#24494;&#25511;&#21046;&#22120;&#65289;&#19978;&#23454;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#24494;&#22411;&#35774;&#22791;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#32858;&#21512;&#23427;&#20204;&#30340;&#30693;&#35782;&#26159;&#21542;&#33021;&#22815;&#20351;TinyML&#24212;&#29992;&#21463;&#30410;&#12290;&#32852;&#37030;&#20803;&#23398;&#20064;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#31572;&#26696;&#65292;&#22240;&#20026;&#23427;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35774;&#22791;&#20043;&#38388;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;TinyML&#30828;&#20214;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#36164;&#28304;&#38480;&#21046;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#33021;&#28304;&#12289;&#38544;&#31169;&#21644;&#36890;&#20449;&#38480;&#21046;&#32780;&#19981;&#23454;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TinyMetaFed&#65292;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;TinyMetaFed&#20419;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#26032;&#35774;&#22791;&#19978;&#24555;&#36895;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#37096;&#20998;&#26412;&#22320;&#37325;&#26500;&#21644;Top-P%&#36873;&#25321;&#24615;&#36890;&#20449;&#25552;&#20379;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#26524;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers. The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications. Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world. However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations. We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML. TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices. It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational eff
&lt;/p&gt;</description></item><item><title>SayPlan&#26159;&#19968;&#31181;&#21033;&#29992;3D&#22330;&#26223;&#22270;&#20026;&#22522;&#30784;&#30340;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#20219;&#21153;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;3D&#22330;&#26223;&#22270;&#30340;&#23618;&#27425;&#32467;&#26500;&#12289;&#38598;&#25104;&#32463;&#20856;&#36335;&#24452;&#35268;&#21010;&#22120;&#20197;&#21450;&#24341;&#20837;&#36845;&#20195;&#30340;&#37325;&#35268;&#21010;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#24222;&#22823;&#22797;&#26434;&#29615;&#22659;&#20013;&#36827;&#34892;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.06135</link><description>&lt;p&gt;
SayPlan: &#20351;&#29992;3D&#22330;&#26223;&#22270;&#20026;&#21487;&#25193;&#23637;&#20219;&#21153;&#35268;&#21010;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#30784;&#21270;
&lt;/p&gt;
&lt;p&gt;
SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning. (arXiv:2307.06135v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06135
&lt;/p&gt;
&lt;p&gt;
SayPlan&#26159;&#19968;&#31181;&#21033;&#29992;3D&#22330;&#26223;&#22270;&#20026;&#22522;&#30784;&#30340;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#20219;&#21153;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;3D&#22330;&#26223;&#22270;&#30340;&#23618;&#27425;&#32467;&#26500;&#12289;&#38598;&#25104;&#32463;&#20856;&#36335;&#24452;&#35268;&#21010;&#22120;&#20197;&#21450;&#24341;&#20837;&#36845;&#20195;&#30340;&#37325;&#35268;&#21010;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#24222;&#22823;&#22797;&#26434;&#29615;&#22659;&#20013;&#36827;&#34892;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24320;&#21457;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#36890;&#29992;&#35268;&#21010;&#26234;&#33021;&#20307;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#35268;&#21010;&#24212;&#29992;&#20110;&#24222;&#22823;&#12289;&#22810;&#23618;&#27004;&#12289;&#22810;&#25151;&#38388;&#30340;&#29615;&#22659;&#20013;&#23545;&#26426;&#22120;&#20154;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SayPlan&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#21033;&#29992;3D&#22330;&#26223;&#22270;&#65288;3DSG&#65289;&#34920;&#31034;&#36827;&#34892;&#22522;&#20110;LLM&#30340;&#22823;&#35268;&#27169;&#20219;&#21153;&#35268;&#21010;&#12290;&#20026;&#20102;&#30830;&#20445;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#65306;&#65288;1&#65289;&#21033;&#29992;3DSG&#30340;&#23618;&#27425;&#32467;&#26500;&#20801;&#35768;LLMs&#20174;&#36739;&#23567;&#30340;&#12289;&#25240;&#21472;&#30340;&#23436;&#25972;&#22270;&#34920;&#31034;&#20013;&#36827;&#34892;&#35821;&#20041;&#25628;&#32034;&#65292;&#23547;&#25214;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#22270;&#65307;&#65288;2&#65289;&#36890;&#36807;&#38598;&#25104;&#32463;&#20856;&#36335;&#24452;&#35268;&#21010;&#22120;&#20943;&#23569;LLM&#30340;&#35268;&#21010;&#35270;&#37326;&#65307;&#65288;3&#65289;&#24341;&#20837;&#19968;&#20010;&#36845;&#20195;&#30340;&#37325;&#35268;&#21010;&#27969;&#31243;&#65292;&#36890;&#36807;&#19982;&#22330;&#26223;&#22270;&#27169;&#25311;&#22120;&#30340;&#21453;&#39304;&#26469;&#20462;&#27491;&#19981;&#21487;&#34892;&#30340;&#21160;&#20316;&#24182;&#36991;&#20813;&#35268;&#21010;&#22833;&#36133;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#28085;&#30422;3&#23618;&#12289;36&#20010;&#25151;&#38388;&#21644;140&#20010;&#23545;&#35937;&#30340;&#22823;&#35268;&#27169;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors, 36 rooms and 140 objects
&lt;/p&gt;</description></item><item><title>TGB&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#26102;&#24577;&#22270;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22810;&#26679;&#21270;&#65292;&#28085;&#30422;&#20102;&#33410;&#28857;&#21644;&#36793;&#32423;&#39044;&#27979;&#20219;&#21153;&#65292;&#23545;&#22810;&#31181;&#39046;&#22495;&#30340;&#26102;&#24577;&#22270;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#26377;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#21160;&#24577;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#26041;&#27861;&#21487;&#33021;&#27604;&#29616;&#26377;&#30340;&#26102;&#24577;&#22270;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01026</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26102;&#24577;&#22270;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Temporal Graph Benchmark for Machine Learning on Temporal Graphs. (arXiv:2307.01026v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01026
&lt;/p&gt;
&lt;p&gt;
TGB&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#26102;&#24577;&#22270;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22810;&#26679;&#21270;&#65292;&#28085;&#30422;&#20102;&#33410;&#28857;&#21644;&#36793;&#32423;&#39044;&#27979;&#20219;&#21153;&#65292;&#23545;&#22810;&#31181;&#39046;&#22495;&#30340;&#26102;&#24577;&#22270;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#26377;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#21160;&#24577;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#26041;&#27861;&#21487;&#33021;&#27604;&#29616;&#26377;&#30340;&#26102;&#24577;&#22270;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Temporal Graph Benchmark&#65288;TGB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#26102;&#24577;&#22270;&#19978;&#36827;&#34892;&#30495;&#23454;&#12289;&#21487;&#37325;&#29616;&#21644;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#24615;&#21644;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#38598;&#21512;&#12290;TGB&#25968;&#25454;&#38598;&#20855;&#26377;&#22823;&#35268;&#27169;&#12289;&#36328;&#24180;&#30340;&#26102;&#38271;&#65292;&#21253;&#25324;&#33410;&#28857;&#21644;&#36793;&#32423;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#28085;&#30422;&#20102;&#31038;&#20132;&#12289;&#36152;&#26131;&#12289;&#20132;&#26131;&#21644;&#20132;&#36890;&#32593;&#32476;&#31561;&#22810;&#31181;&#39046;&#22495;&#12290;&#38024;&#23545;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#25105;&#20204;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#27169;&#22411;&#30340;&#24615;&#33021;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#22312;&#21160;&#24577;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#26041;&#27861;&#24120;&#24120;&#27604;&#29616;&#26377;&#30340;&#26102;&#24577;&#22270;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#26102;&#24577;&#22270;&#30740;&#31350;&#24320;&#36767;&#20102;&#26426;&#20250;&#12290;&#26368;&#21518;&#65292;TGB&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#21487;&#37325;&#29616;&#21644;&#21487;&#35775;&#38382;&#30340;&#26102;&#24577;&#22270;&#30740;&#31350;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
We present the Temporal Graph Benchmark (TGB), a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evaluation of machine learning models on temporal graphs. TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks. For both tasks, we design evaluation protocols based on realistic use-cases. We extensively benchmark each dataset and find that the performance of common models can vary drastically across datasets. In addition, on dynamic node property prediction tasks, we show that simple methods often achieve superior performance compared to existing temporal graph models. We believe that these findings open up opportunities for future research on temporal graphs. Finally, TGB provides an automated machine learning pipeline for reproducible and accessible temporal graph research, inclu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;CoLAC&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#20302;&#20110;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#36328;&#35821;&#35328;&#36716;&#31227;&#21487;&#34892;&#65292;&#24182;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2305.14091</link><description>&lt;p&gt;
&#37325;&#28201;&#25509;&#21463;&#24615;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revisiting Acceptability Judgements. (arXiv:2305.14091v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;CoLAC&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#20302;&#20110;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#36328;&#35821;&#35328;&#36716;&#31227;&#21487;&#34892;&#65292;&#24182;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33267;NLP&#31038;&#21306;&#26368;&#21518;&#19968;&#27425;&#20851;&#27880;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#24050;&#32463;&#36807;&#21435;&#22810;&#24180;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#37325;&#28201;&#36825;&#20010;&#35805;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; CoLAC-&#20013;&#25991;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30001;&#27597;&#35821;&#35762;&#32773;&#39564;&#35777;&#24182;&#24102;&#26377;&#20004;&#32452;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#38750;&#33521;&#35821;&#21487;&#25509;&#21463;&#24615;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#22823;&#30340;InstructGPT&#27169;&#22411;&#22312;CoLAC&#19978;&#20063;&#21482;&#33021;&#34920;&#29616;&#38543;&#26426;&#27700;&#24179;&#65292;&#32780;ChatGPT&#30340;&#24615;&#33021;&#65288;48.30 MCC&#65289;&#20063;&#36828;&#20302;&#20110;&#30417;&#30563;&#27169;&#22411;&#65288;59.03 MCC&#65289;&#21644;&#20154;&#31867;&#65288;65.11 MCC&#65289;&#12290;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#23454;&#39564;&#21644;&#32454;&#31890;&#24230;&#30340;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#30693;&#35782;&#21487;&#20197;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#20043;&#38388;&#36716;&#31227;&#65292;&#32780;&#19988;&#21487;&#20197;&#36861;&#28335;&#21040;&#39044;&#35757;&#32451;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Years have passed since the NLP community has last focused on linguistic acceptability. In this work, we revisit this topic in the context of large language models. We introduce CoLAC - Corpus of Linguistic Acceptability in Chinese, the first large-scale non-English acceptability dataset that is verified by native speakers and comes with two sets of labels. Our experiments show that even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also way below supervised models (59.03 MCC) and human (65.11 MCC). Through cross-lingual transfer experiments and fine-grained linguistic analysis, we demonstrate for the first time that knowledge of linguistic acceptability can be transferred across typologically distinct languages, as well as be traced back to pre-training.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#23618;&#21487;&#35270;&#21270;&#26469;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11364</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models. (arXiv:2305.11364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#23618;&#21487;&#35270;&#21270;&#26469;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#29983;&#25104;&#26356;&#31934;&#32454;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#12289;&#24494;&#35843;&#25110;&#20854;&#20182;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#21644;&#35780;&#20272;&#36825;&#20123;&#25968;&#25454;&#38598;&#24456;&#22256;&#38590;&#65292;&#32780;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#22833;&#36133;&#27169;&#24335;&#20173;&#19981;&#20026;&#20154;&#20204;&#25152;&#29702;&#35299;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25968;&#25454;&#21487;&#33021;&#20197;&#24847;&#22806;&#30340;&#26041;&#24335;&#21464;&#24471;&#37325;&#22797;&#65292;&#19981;&#20165;&#35821;&#20041;&#19978;&#22914;&#27492;&#65292;&#32780;&#19988;&#22312;&#21477;&#27861;&#12289;&#35789;&#27719;&#26041;&#38754;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LinguisticLens&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#20998;&#26512;LLM&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#12290; LinguisticLens&#27839;&#30528;&#21477;&#27861;&#12289;&#35789;&#27719;&#21644;&#35821;&#20041;&#36724;&#23558;&#25991;&#26412;&#32858;&#31867;&#12290;&#23427;&#25903;&#25345;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#20998;&#23618;&#21487;&#35270;&#21270;&#65292;&#20801;&#35768;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;&#23454;&#26102;&#28436;&#31034;&#21487;&#22312;shorturl.at/zHOUV&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can be used to generate smaller, more refined datasets via few-shot prompting for benchmarking, fine-tuning or other use cases. However, understanding and evaluating these datasets is difficult, and the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically. We present LinguisticLens, a novel inter-active visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets. LinguisticLens clusters text along syntactic, lexical, and semantic axes. It supports hierarchical visualization of a text dataset, allowing users to quickly scan for an overview and inspect individual examples. The live demo is available at shorturl.at/zHOUV.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.04866</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#20840;&#36523;&#25805;&#20316;&#30340;&#22240;&#26524;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Causal Policy Gradient for Whole-Body Mobile Manipulation. (arXiv:2305.04866v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19979;&#19968;&#20195;&#23478;&#24237;&#26426;&#22120;&#20154;&#21161;&#25163;&#38656;&#35201;&#32467;&#21512;&#26426;&#21160;&#21644;&#20132;&#20114;&#33021;&#21147;&#65292;&#21363;&#36890;&#24120;&#25152;&#35828;&#30340;&#31227;&#21160;&#25805;&#20316;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#21644;&#20219;&#21153;&#24120;&#35265;&#30340;&#22810;&#30446;&#26631;&#24615;&#36136;&#65292;&#20363;&#22914;&#33021;&#22815;&#26377;&#25928;&#22320;&#36798;&#21040;&#30446;&#26631;&#19988;&#36991;&#20813;&#38556;&#30861;&#65292;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#24456;&#38590;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#26681;&#25454;&#20154;&#24037;&#21305;&#37197;&#21160;&#20316;&#31354;&#38388;&#30340;&#37096;&#20998;&#21040;&#31227;&#21160;&#25805;&#20316;&#23376;&#30446;&#26631;&#65288;&#20363;&#22914;&#29992;&#20110;&#31227;&#21160;&#30446;&#26631;&#30340;&#22522;&#30784;&#21160;&#20316;&#21644;&#29992;&#20110;&#25805;&#20316;&#30340;&#25163;&#33218;&#21160;&#20316;&#65289;&#23558;&#20219;&#21153;&#20998;&#20026;&#19981;&#24102;&#25805;&#20316;&#30340;&#23548;&#33322;&#21644;&#19981;&#24102;&#26426;&#21160;&#30340;&#22266;&#23450;&#25805;&#20316;&#12290;&#27492;&#35299;&#20915;&#26041;&#26696;&#38450;&#27490;&#20102;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#30340;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#24182;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#35813;&#26694;&#26550;&#29992;&#20110;&#35757;&#32451;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion by manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subsp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19978;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20197;&#21450;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.04811</link><description>&lt;p&gt;
&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;: 2020-2022&#24180;&#30340;&#26368;&#26032;&#36827;&#23637;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques for financial time series forecasting: A review of recent advancements: 2020-2022. (arXiv:2305.04811v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19978;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20197;&#21450;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#20851;&#27880;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#37117;&#34987;&#25506;&#32034;&#29992;&#20110;&#24320;&#21457;&#26377;&#25928;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#26032;&#36827;&#23637;&#24448;&#24448;&#38590;&#20197;&#36319;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36825;&#31687;&#25991;&#29486;&#32508;&#36848;&#65292;&#23545;2020&#24180;&#33267;2022&#24180;&#38388;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#37329;&#34701;&#20215;&#26684;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#26469;&#28304;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#23454;&#29616;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#20445;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#20415;&#20110;&#26681;&#25454;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#36873;&#25321;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting financial time series has long been a challenging problem that has attracted attention from both researchers and practitioners. Statistical and machine learning techniques have both been explored to develop effective forecasting models in the past few decades. With recent developments in deep learning models, financial time series forecasting models have advanced significantly, and these developments are often difficult to keep up with. Hence, we have conducted this literature review to provide a comprehensive assessment of recent research from 2020 to 2022 on deep learning models used to predict prices based on financial time series. Our review presents different data sources and neural network structures, as well as their implementation details. Our goals are to ensure that interested researchers remain up-to-date on recent developments in the field and facilitate the selection of baselines based on models used in prior studies. Additionally, we provide suggestions for fu
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03942</link><description>&lt;p&gt;
&#23398;&#20064;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#28151;&#21512;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03942
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#28789;&#24039;&#24615;&#20013;&#65292;&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#26159;&#25805;&#20316;&#29289;&#20307;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38750;&#25235;&#21462;&#24335;&#25805;&#32437;&#21487;&#20197;&#20351;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#20063;&#22312;&#25512;&#29702;&#20132;&#20114;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;HACMan&#30340;&#28151;&#21512;&#28436;&#21592;&#35780;&#35770;&#21592;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;6D&#38750;&#25235;&#21462;&#24335;&#29289;&#20307;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;HACMan&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#25277;&#35937;&#21644;&#31354;&#38388;&#22522;&#30784;&#30340;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;RL&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#28151;&#21512;&#30340;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;HACMan&#36827;&#34892;&#20102;6D&#29289;&#20307;&#23039;&#24577;&#23545;&#40784;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#29256;&#26412;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#29289;&#20307;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;HACMan&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#31181;&#32534;&#30721;&#65292;&#23558;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#20316;&#20026;&#32463;&#20856;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26368;&#20248;&#30340;&#32463;&#20856;&#35268;&#21010;&#22120;&#26469;&#32508;&#21512;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20248;&#24067;&#23616;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#22823;&#35268;&#27169;&#37327;&#23376;&#27604;&#29305;&#20248;&#21270;&#24067;&#23616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12014</link><description>&lt;p&gt;
&#20316;&#20026;&#32463;&#20856;&#35745;&#21010;&#30340;&#37327;&#23376;&#30005;&#36335;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Optimal Layout Synthesis for Quantum Circuits as Classical Planning. (arXiv:2304.12014v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#31181;&#32534;&#30721;&#65292;&#23558;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#20316;&#20026;&#32463;&#20856;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26368;&#20248;&#30340;&#32463;&#20856;&#35268;&#21010;&#22120;&#26469;&#32508;&#21512;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20248;&#24067;&#23616;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#22823;&#35268;&#27169;&#37327;&#23376;&#27604;&#29305;&#20248;&#21270;&#24067;&#23616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24067;&#23616;&#32508;&#21512;&#20013;&#65292;&#23558;&#37327;&#23376;&#30005;&#36335;&#30340;&#36923;&#36753;&#37327;&#23376;&#27604;&#29305;&#26144;&#23556;&#21040;&#32473;&#23450;&#37327;&#23376;&#30828;&#20214;&#24179;&#21488;&#30340;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#65292;&#32771;&#34385;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#30340;&#36830;&#25509;&#12290;&#36825;&#28041;&#21450;&#22312;&#24212;&#29992;&#20110;&#36828;&#36317;&#31163;&#37327;&#23376;&#27604;&#29305;&#30340;&#25805;&#20316;&#20043;&#21069;&#25554;&#20837;SWAP&#38376;&#12290;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#23545;&#20110;&#24403;&#21069;&#35823;&#24046;&#29575;&#36739;&#39640;&#30340;&#30828;&#20214;&#19978;&#23454;&#29992;&#30340;&#37327;&#23376;&#35745;&#31639;&#38750;&#24120;&#37325;&#35201;&#65306;&#26368;&#23567;&#21270;SWAP&#38376;&#25968;&#37327;&#30452;&#25509;&#20943;&#36731;&#20102;&#36816;&#34892;&#37327;&#23376;&#30005;&#36335;&#26102;&#30340;&#38169;&#35823;&#29575;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;SWAP&#25554;&#20837;&#27425;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#31934;&#30830;&#26041;&#27861;&#21482;&#33021;&#25193;&#23637;&#21040;&#23569;&#37327;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#35777;&#26126;&#25152;&#38656;&#30340;&#20132;&#25442;&#25554;&#20837;&#27425;&#25968;&#26159;&#26368;&#20248;&#30340;&#35201;&#27604;&#29983;&#25104;&#36817;&#20284;&#26368;&#20248;&#30340;&#26144;&#23556;&#22256;&#38590;&#24471;&#22810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#32534;&#30721;&#65292;&#23558;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#20316;&#20026;&#32463;&#20856;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20248;&#30340;&#32463;&#20856;&#35268;&#21010;&#22120;&#26469;&#32508;&#21512;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20248;&#24067;&#23616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Layout Synthesis, the logical qubits of a quantum circuit are mapped to the physical qubits of a given quantum hardware platform, taking into account the connectivity of physical qubits. This involves inserting SWAP gates before an operation is applied on distant qubits. Optimal Layout Synthesis is crucial for practical Quantum Computing on current error-prone hardware: Minimizing the number of SWAP gates directly mitigates the error rates when running quantum circuits.  In recent years, several approaches have been proposed for minimizing the required SWAP insertions. The proposed exact approaches can only scale to a small number of qubits. Proving that a number of swap insertions is optimal is much harder than producing near optimal mappings.  In this paper, we provide two encodings for Optimal Layout Synthesis as a classical planning problem. We use optimal classical planners to synthesize the optimal layout for a standard set of benchmarks. Our results show the scalability of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#32467;&#26500;&#12290;MSE-NAS&#21487;&#20197;&#24110;&#21161;SNN&#23454;&#29616;&#22810;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#24615;&#30340;&#36328;&#27169;&#24335;&#36830;&#25509;&#26469;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10749</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#28145;&#24230;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural Networks. (arXiv:2304.10749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#32467;&#26500;&#12290;MSE-NAS&#21487;&#20197;&#24110;&#21161;SNN&#23454;&#29616;&#22810;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#24615;&#30340;&#36328;&#27169;&#24335;&#36830;&#25509;&#26469;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19981;&#20165;&#22240;&#20854;&#31163;&#25955;&#20449;&#21495;&#22788;&#29702;&#30340;&#33021;&#28304;&#25928;&#29575;&#21331;&#36234;&#65292;&#32780;&#19988;&#22240;&#20854;&#22825;&#28982;&#36866;&#21512;&#20110;&#38598;&#25104;&#22810;&#23610;&#24230;&#29983;&#29289;&#21487;&#22609;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;SNN&#30452;&#25509;&#37319;&#29992;&#25104;&#29087;&#30340;DNN&#32467;&#26500;&#65292;&#24456;&#23569;&#33258;&#21160;&#35774;&#35745;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#29992;&#20110;SNN&#12290;&#20154;&#31867;&#22823;&#33041;&#31070;&#32463;&#27169;&#24335;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#27169;&#22359;&#21270;&#30340;&#21306;&#22495;&#32467;&#26500;&#21644;&#20840;&#23616;&#24615;&#30340;&#36328;&#33041;&#21306;&#36830;&#25509;&#26159;&#33258;&#28982;&#36827;&#21270;&#30340;&#20135;&#29289;&#65292;&#21487;&#20197;&#20316;&#20026;&#35774;&#35745;&#22522;&#20110;&#33041;&#30340;SNN&#26550;&#26500;&#30340;&#23436;&#32654;&#21442;&#32771;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;MSE-NAS&#65289;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#20316;&#20026;&#36827;&#21270;&#25628;&#32034;&#31354;&#38388;&#12290; MSE-NAS&#36890;&#36807;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#38388;&#25509;&#26041;&#24335;&#65292;&#36827;&#21270;&#21333;&#20010;&#31070;&#32463;&#20803;&#25805;&#20316;&#65292;&#22810;&#20010;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#20197;&#21450;&#36328;&#27169;&#24335;&#30340;&#20840;&#23616;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have received considerable attention not only for their superiority in energy efficient with discrete signal processing, but also for their natural suitability to integrate multi-scale biological plasticity. However, most SNNs directly adopt the structure of the well-established DNN, rarely automatically design Neural Architecture Search (NAS) for SNNs. The neural motifs topology, modular regional structure and global cross-brain region connection of the human brain are the product of natural evolution and can serve as a perfect reference for designing brain-inspired SNN architecture. In this paper, we propose a Multi-Scale Evolutionary Neural Architecture Search (MSE-NAS) for SNN, simultaneously considering micro-, meso- and macro-scale brain topologies as the evolutionary search space. MSE-NAS evolves individual neuron operation, self-organized integration of multiple circuit motifs, and global connectivity across motifs through a brain-inspired indirec
&lt;/p&gt;</description></item><item><title>Generative Disco&#26159;&#19968;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24110;&#21161;&#29983;&#25104;&#38899;&#20048;&#21487;&#35270;&#21270;&#65307;&#29992;&#25143;&#36890;&#36807;&#23450;&#20041;&#24320;&#22987;&#21644;&#32467;&#26463;&#25552;&#31034;&#26469;&#21442;&#25968;&#21270;&#21487;&#35270;&#21270;&#65292;&#21487;&#29983;&#25104;&#21453;&#24212;&#38899;&#39057;&#30340;&#35270;&#39057;&#65292;&#24341;&#20837;&#20102;&#8220;&#36807;&#28193;&#8221;&#21644;&#8220;&#20445;&#25345;&#8221;&#30340;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#34920;&#29616;&#21147;&#24182;&#36866;&#29992;&#20110;&#19987;&#19994;&#20154;&#21592;&#12290;</title><link>http://arxiv.org/abs/2304.08551</link><description>&lt;p&gt;
&#21019;&#20316;&#36842;&#26031;&#31185;&#65306;&#38899;&#20048;&#21487;&#35270;&#21270;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generative Disco: Text-to-Video Generation for Music Visualization. (arXiv:2304.08551v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08551
&lt;/p&gt;
&lt;p&gt;
Generative Disco&#26159;&#19968;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24110;&#21161;&#29983;&#25104;&#38899;&#20048;&#21487;&#35270;&#21270;&#65307;&#29992;&#25143;&#36890;&#36807;&#23450;&#20041;&#24320;&#22987;&#21644;&#32467;&#26463;&#25552;&#31034;&#26469;&#21442;&#25968;&#21270;&#21487;&#35270;&#21270;&#65292;&#21487;&#29983;&#25104;&#21453;&#24212;&#38899;&#39057;&#30340;&#35270;&#39057;&#65292;&#24341;&#20837;&#20102;&#8220;&#36807;&#28193;&#8221;&#21644;&#8220;&#20445;&#25345;&#8221;&#30340;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#34920;&#29616;&#21147;&#24182;&#36866;&#29992;&#20110;&#19987;&#19994;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#26159;&#38899;&#20048;&#20307;&#39564;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25918;&#22823;&#38899;&#20048;&#20256;&#36798;&#30340;&#24773;&#24863;&#21644;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#21019;&#36896;&#38899;&#20048;&#21487;&#35270;&#21270;&#26159;&#19968;&#20010;&#22797;&#26434;&#12289;&#32791;&#26102;&#21644;&#36164;&#28304;&#23494;&#38598;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Generative Disco&#65292;&#19968;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24110;&#21161;&#29983;&#25104;&#38899;&#20048;&#21487;&#35270;&#21270;&#12290;&#29992;&#25143;&#36873;&#25321;&#35201;&#21487;&#35270;&#21270;&#30340;&#38899;&#20048;&#38388;&#38548;&#65292;&#28982;&#21518;&#36890;&#36807;&#23450;&#20041;&#24320;&#22987;&#21644;&#32467;&#26463;&#25552;&#31034;&#26469;&#21442;&#25968;&#21270;&#35813;&#21487;&#35270;&#21270;&#12290;&#36825;&#20123;&#25552;&#31034;&#26681;&#25454;&#38899;&#20048;&#30340;&#33410;&#22863;&#36827;&#34892;&#21464;&#24418;&#21644;&#29983;&#25104;&#65292;&#20197;&#20135;&#29983;&#21453;&#24212;&#38899;&#39057;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25913;&#36827;&#29983;&#25104;&#35270;&#39057;&#30340;&#35774;&#35745;&#27169;&#24335;&#65306;&#8220;&#36807;&#28193;&#8221;&#65292;&#34920;&#36798;&#39068;&#33394;&#12289;&#26102;&#38388;&#12289;&#20027;&#39064;&#25110;&#39118;&#26684;&#30340;&#21464;&#21270;&#65292;&#8220;&#20445;&#25345;&#8221;&#65292;&#40723;&#21169;&#35270;&#35273;&#37325;&#28857;&#21644;&#19968;&#33268;&#24615;&#12290;&#19987;&#19994;&#20154;&#21592;&#21442;&#19982;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#24841;&#24742;&#24615;&#12289;&#26131;&#20110;&#25506;&#32034;&#21644;&#39640;&#24230;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;Generative Disco&#22312;&#19987;&#19994;&#20154;&#22763;&#20013;&#30340;&#24212;&#29992;&#26696;&#20363;&#20197;&#21450;AI&#29983;&#25104;&#20869;&#23481;&#19982;&#29256;&#26435;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visuals are a core part of our experience of music, owing to the way they can amplify the emotions and messages conveyed through the music. However, creating music visualization is a complex, time-consuming, and resource-intensive process. We introduce Generative Disco, a generative AI system that helps generate music visualizations with large language models and text-to-image models. Users select intervals of music to visualize and then parameterize that visualization by defining start and end prompts. These prompts are warped between and generated according to the beat of the music for audioreactive video. We introduce design patterns for improving generated videos: "transitions", which express shifts in color, time, subject, or style, and "holds", which encourage visual emphasis and consistency. A study with professionals showed that the system was enjoyable, easy to explore, and highly expressive. We conclude on use cases of Generative Disco for professionals and how AI-generated c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.16296</link><description>&lt;p&gt;
Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65306;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Dice&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#30340;&#35768;&#22810;&#33258;&#21160;&#20998;&#21106;&#26041;&#26696;&#20013;&#65292;&#36719;Dice&#25439;&#22833;&#65288;SDL&#65289;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#25581;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#32972;&#21518;&#30340;&#19968;&#20123;&#21407;&#22240;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20854;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23454;&#29616;&#25903;&#25345;&#30452;&#25509;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#23427;&#30340;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22312;&#20351;&#29992;SDL&#21644;&#30740;&#31350;&#21033;&#29992;&#36719;&#26631;&#31614;&#30340;&#21516;&#26102;&#36827;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21327;&#21516;&#20316;&#29992;&#20173;&#28982;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65288;DMLs&#65289;&#65292;&#23427;&#20204;&#65288;i&#65289;&#22312;&#30828;&#26631;&#31614;&#30340;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;SDL&#30456;&#21516;&#65292;&#20294;&#65288;ii&#65289;&#20063;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#30340;QUBIQ&#12289;LiTS&#21644;KiTS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DMLs&#19982;&#36719;&#26631;&#31614;&#65288;&#22914;&#24179;&#22343;&#12289;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#65289;&#30340;&#28508;&#22312;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;DMLs&#19982;&#30828;&#26631;&#31614;&#65288;&#22914;&#22823;&#22810;&#25968;&#25237;&#31080;&#21644;&#38543;&#26426;&#36873;&#25321;&#65289;&#30456;&#27604;&#65292;&#20135;&#29983;&#20102;&#26356;&#20248;&#31168;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calib
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Tubelet-&#23545;&#27604;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23616;&#37096;&#36816;&#21160;&#21160;&#24577;&#30456;&#21516;&#20294;&#22806;&#35266;&#19981;&#21516;&#30340;&#35270;&#39057;&#20043;&#38388;&#23398;&#20064;&#30456;&#20284;&#24615;&#65292;&#24341;&#20837;&#36229;&#20986;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#36816;&#21160;&#27169;&#24335;&#26469;&#23398;&#20064;&#39640;&#25928;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#20165;25&#65285;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#26102;&#20173;&#33021;&#20445;&#25345;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#21644;&#19968;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11003</link><description>&lt;p&gt;
Tubelet-&#23545;&#27604;&#33258;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#35270;&#39057;&#39640;&#25928;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization. (arXiv:2303.11003v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Tubelet-&#23545;&#27604;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23616;&#37096;&#36816;&#21160;&#21160;&#24577;&#30456;&#21516;&#20294;&#22806;&#35266;&#19981;&#21516;&#30340;&#35270;&#39057;&#20043;&#38388;&#23398;&#20064;&#30456;&#20284;&#24615;&#65292;&#24341;&#20837;&#36229;&#20986;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#36816;&#21160;&#27169;&#24335;&#26469;&#23398;&#20064;&#39640;&#25928;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#20165;25&#65285;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#26102;&#20173;&#33021;&#20445;&#25345;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#21644;&#19968;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#23398;&#20064;&#20197;&#36816;&#21160;&#20026;&#37325;&#28857;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#26102;&#38388;&#22686;&#24378;&#30340;&#35270;&#39057;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#20445;&#25345;&#39640;&#31354;&#38388;&#30456;&#20284;&#24615;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22312;&#23616;&#37096;&#36816;&#21160;&#21160;&#24577;&#30456;&#21516;&#20294;&#22806;&#35266;&#19981;&#21516;&#30340;&#35270;&#39057;&#20043;&#38388;&#23398;&#20064;&#30456;&#20284;&#24615;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#35270;&#39057;&#20013;&#28155;&#21152;&#20102;&#21512;&#25104;&#30340;&#36816;&#21160;&#36712;&#36857;&#65292;&#31216;&#20043;&#20026;tubelets&#65292;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#30340;tubelet&#36816;&#21160;&#24182;&#24212;&#29992;&#32553;&#25918;&#21644;&#26059;&#36716;&#31561;&#21464;&#25442;&#65292;&#24341;&#20837;&#20102;&#36229;&#20986;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#36816;&#21160;&#27169;&#24335;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#20986;&#19968;&#31181;&#25968;&#25454;&#25928;&#29575;&#26174;&#33879;&#30340;&#35270;&#39057;&#34920;&#31034;&#65306;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#20165;25&#65285;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#26102;&#20173;&#33021;&#20445;&#25345;&#24615;&#33021;&#12290;&#22312;10&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#35774;&#32622;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#31454;&#20105;&#21147;&#24378;&#30340;&#24615;&#33021;&#21644;&#23545;&#26032;&#39046;&#22495;&#21644;&#32454;&#31890;&#24230;&#21160;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a self-supervised method for learning motion-focused video representations. Existing approaches minimize distances between temporally augmented videos, which maintain high spatial similarity. We instead propose to learn similarities between videos with identical local motion dynamics but an otherwise different appearance. We do so by adding synthetic motion trajectories to videos which we refer to as tubelets. By simulating different tubelet motions and applying transformations, such as scaling and rotation, we introduce motion patterns beyond what is present in the pretraining data. This allows us to learn a video representation that is remarkably data efficient: our approach maintains performance when using only 25\% of the pretraining videos. Experiments on 10 diverse downstream settings demonstrate our competitive performance and generalizability to new domains and fine-grained actions.
&lt;/p&gt;</description></item><item><title>EvCenterNet&#26159;&#19968;&#31181;&#20351;&#29992;&#35777;&#25454;&#23398;&#20064;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#26368;&#19981;&#30830;&#23450;&#30340;&#28857;&#26469;&#25913;&#21892;&#26816;&#27979;&#24615;&#33021;&#12290;&#27169;&#22411;&#22312;KITTI&#25968;&#25454;&#38598;&#21644;&#20854;&#20182;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.03037</link><description>&lt;p&gt;
EvCenterNet: &#20351;&#29992;&#35777;&#25454;&#23398;&#20064;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
EvCenterNet: Uncertainty Estimation for Object Detection using Evidential Learning. (arXiv:2303.03037v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03037
&lt;/p&gt;
&lt;p&gt;
EvCenterNet&#26159;&#19968;&#31181;&#20351;&#29992;&#35777;&#25454;&#23398;&#20064;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#26368;&#19981;&#30830;&#23450;&#30340;&#28857;&#26469;&#25913;&#21892;&#26816;&#27979;&#24615;&#33021;&#12290;&#27169;&#22411;&#22312;KITTI&#25968;&#25454;&#38598;&#21644;&#20854;&#20182;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#22330;&#26223;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20026;&#39640;&#32423;&#20915;&#31574;&#21644;&#36335;&#24452;&#35268;&#21010;&#31561;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvCenterNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20108;&#32500;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#35777;&#25454;&#23398;&#20064;&#30452;&#25509;&#20272;&#35745;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#24212;&#29992;&#35777;&#25454;&#23398;&#20064;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35777;&#25454;&#21644;&#28966;&#28857;&#25439;&#22833;&#20989;&#25968;&#30340;&#32452;&#21512;&#65292;&#29992;&#20110;&#31232;&#30095;&#28909;&#22270;&#36755;&#20837;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31867;&#24179;&#34913;&#26435;&#37325;&#26469;&#22788;&#29702;&#35777;&#25454;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#28857;&#20851;&#27880;&#26368;&#19981;&#30830;&#23450;&#30340;&#28857;&#26469;&#21033;&#29992;&#39044;&#27979;&#30340;&#28909;&#22270;&#19981;&#30830;&#23450;&#24615;&#26469;&#25913;&#21892;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#21253;&#25324;BDD100K&#21644;nuImages&#22312;&#20869;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#24067;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is crucial in safety-critical settings such as automated driving as it provides valuable information for several downstream tasks including high-level decision making and path planning. In this work, we propose EvCenterNet, a novel uncertainty-aware 2D object detection framework using evidential learning to directly estimate both classification and regression uncertainties. To employ evidential learning for object detection, we devise a combination of evidential and focal loss functions for the sparse heatmap inputs. We introduce class-balanced weighting for regression and heatmap prediction to tackle the class imbalance encountered by evidential learning. Moreover, we propose a learning scheme to actively utilize the predicted heatmap uncertainties to improve the detection performance by focusing on the most uncertain points. We train our model on the KITTI dataset and evaluate it on challenging out-of-distribution datasets including BDD100K and nuImages. Our ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24418;&#24577;&#20449;&#24687;&#22312;&#20845;&#31181;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23545;&#35789;&#24418;&#36824;&#21407;&#22120;&#36827;&#34892;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.00407</link><description>&lt;p&gt;
&#20851;&#20110;&#24418;&#24577;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Morphological Information for Contextual Lemmatization. (arXiv:2302.00407v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24418;&#24577;&#20449;&#24687;&#22312;&#20845;&#31181;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23545;&#35789;&#24418;&#36824;&#21407;&#22120;&#36827;&#34892;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#24418;&#36824;&#21407;&#26159;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#23427;&#21253;&#25324;&#20174;&#32473;&#23450;&#30340;&#23624;&#25240;&#35789;&#29983;&#25104;&#20854;&#35268;&#33539;&#24418;&#24335;&#25110;&#35789;&#24418;&#36824;&#21407;&#12290;&#35789;&#24418;&#36824;&#21407;&#26159;&#31616;&#21270;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#23545;&#20110;&#39640;&#24230;&#23624;&#25240;&#35821;&#35328;&#23588;&#20026;&#37325;&#35201;&#12290;&#34429;&#28982;&#26681;&#25454;&#23624;&#25240;&#35789;&#33719;&#24471;&#35789;&#24418;&#36824;&#21407;&#24418;&#24335;&#30340;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#20854;&#24418;&#24577;&#21477;&#27861;&#31867;&#21035;&#26469;&#35299;&#37322;&#65292;&#20294;&#22312;&#35757;&#32451;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#26102;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#24120;&#35265;&#20570;&#27861;&#65292;&#32780;&#26080;&#35270;&#19979;&#28216;&#32489;&#25928;&#26159;&#21542;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#20845;&#31181;&#35821;&#35328;&#20013;&#32771;&#23519;&#24418;&#24577;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#36825;&#20845;&#31181;&#35821;&#35328;&#30340;&#24418;&#24577;&#22797;&#26434;&#24615;&#21508;&#19981;&#30456;&#21516;&#65292;&#21253;&#25324;&#24052;&#26031;&#20811;&#35821;&#12289;&#22303;&#32819;&#20854;&#35821;&#12289;&#20420;&#35821;&#12289;&#25463;&#20811;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#12290;&#27492;&#22806;&#65292;&#19982;&#32477;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#36824;&#22312;&#39046;&#22495;&#22806;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35789;&#24418;&#36824;&#21407;&#22120;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lemmatization is a natural language processing (NLP) task which consists of producing, from a given inflected word, its canonical form or lemma. Lemmatization is one of the basic tasks that facilitate downstream NLP applications, and is of particular importance for high-inflected languages. Given that the process to obtain a lemma from an inflected word can be explained by looking at its morphosyntactic category, including fine-grained morphosyntactic information to train contextual lemmatizers has become common practice, without considering whether that is the optimum in terms of downstream performance. In order to address this issue, in this paper we empirically investigate the role of morphological information to develop contextual lemmatizers in six languages within a varied spectrum of morphological complexity: Basque, Turkish, Russian, Czech, Spanish and English. Furthermore, and unlike the vast majority of previous work, we also evaluate lemmatizers in out-of-domain settings, wh
&lt;/p&gt;</description></item><item><title>&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11562</link><description>&lt;p&gt;
&#39044;&#27979;&#26159;&#21542;&#38543;&#24847;&#65311;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#35780;&#20272;&#33258;&#27965;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290; &#23454;&#35777;&#34920;&#26126;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30340;&#26041;&#24046;&#24046;&#24322;&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#20915;&#31574;&#23454;&#38469;&#19978;&#26159;&#38543;&#24847;&#30340;&#12290; &#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#20570;&#20986;&#20102;&#22235;&#20010;&#24635;&#20307;&#36129;&#29486;&#65306;&#25105;&#20204;1&#65289;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#33258;&#27965;&#24615;&#65292;&#22312;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#26102;&#20351;&#29992;&#65307; 2&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#24403;&#39044;&#27979;&#26080;&#27861;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#21487;&#20197;&#25918;&#24323;&#20998;&#31867;&#65307; 3&#65289;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26377;&#20851;&#20844;&#24179;&#20998;&#31867;&#20013;&#26041;&#24046;&#65288;&#30456;&#23545;&#20110;&#33258;&#27965;&#24615;&#21644;&#38543;&#24847;&#24615;&#65289;&#20316;&#29992;&#30340;&#26368;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65307; 4&#65289;&#25512;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#20351;&#32654;&#22269;&#20303;&#25151;&#25269;&#25276;&#36151;&#27454;&#25259;&#38706;&#27861;&#26696;&#65288;HMDA&#65289;&#25968;&#25454;&#38598;&#26131;&#20110;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#12290; &#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#25581;&#31034;&#20102;&#20851;&#20110;&#21487;&#37325;&#22797;&#24615;&#30340;&#20196;&#20154;&#38663;&#24778;&#30340;&#35265;&#35299;&#12290;&#24403;&#32771;&#34385;&#21040;&#26041;&#24046;&#21644;&#38543;&#24847;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#26102;&#65292;&#22823;&#22810;&#25968;&#20844;&#24179;&#20998;&#31867;&#22522;&#20934;&#25509;&#36817;&#20844;&#24179;&#12290; &#20294;&#26159;&#65292;&#19968;&#23567;&#37096;&#20998;&#23454;&#20363;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#38543;&#24847;&#24615;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
&lt;/p&gt;</description></item><item><title>&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;VFL&#27010;&#24565;&#12289;&#31639;&#27861;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#32508;&#21512;&#22238;&#39038;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#20998;&#31867;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#32479;&#19968;&#26694;&#26550;VFLow&#12290;&#27492;&#22806;&#65292;&#36824;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;VFL&#38754;&#20020;&#30340;&#26410;&#26469;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2211.12814</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65306;&#27010;&#24565;&#12289;&#36827;&#23637;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning: Concepts, Advances and Challenges. (arXiv:2211.12814v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12814
&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;VFL&#27010;&#24565;&#12289;&#31639;&#27861;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#32508;&#21512;&#22238;&#39038;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#20998;&#31867;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#32479;&#19968;&#26694;&#26550;VFLow&#12290;&#27492;&#22806;&#65292;&#36824;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;VFL&#38754;&#20020;&#30340;&#26410;&#26469;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#21463;VFL&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#30340;&#25512;&#21160;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;VFL&#30340;&#27010;&#24565;&#21644;&#31639;&#27861;&#65292;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#24403;&#21069;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;VFL&#35774;&#32622;&#21644;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#35814;&#23613;&#20998;&#31867;&#65292;&#24182;&#23545;&#27599;&#20010;&#21327;&#35758;&#30340;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;VFLow&#65292;&#23427;&#32771;&#34385;&#20102;VFL&#38382;&#39064;&#22312;&#36890;&#20449;&#12289;&#35745;&#31639;&#12289;&#38544;&#31169;&#20197;&#21450;&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#26368;&#26032;&#30340;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;VFL&#38754;&#20020;&#30340;&#24320;&#25918;&#24615;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36127;&#36131;&#20219;&#30340;AI&#27169;&#24335;&#30446;&#24405;&#65292;&#26088;&#22312;&#20174;&#31995;&#32479;&#35282;&#24230;&#23454;&#29616;&#36127;&#36131;&#20219;&#30340;AI&#12290;&#19982;&#29616;&#26377;&#30340;&#20262;&#29702;&#21407;&#21017;&#26694;&#26550;&#19981;&#21516;&#65292;&#35813;&#30446;&#24405;&#25552;&#20379;&#20102;&#23454;&#36341;&#20013;&#21487;&#37319;&#21462;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#28085;&#30422;&#20102;AI&#31995;&#32479;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2209.04963</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;&#30340;AI&#27169;&#24335;&#30446;&#24405;&#65306;AI&#27835;&#29702;&#19982;&#24037;&#31243;&#30340;&#26368;&#20339;&#23454;&#36341;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. (arXiv:2209.04963v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36127;&#36131;&#20219;&#30340;AI&#27169;&#24335;&#30446;&#24405;&#65292;&#26088;&#22312;&#20174;&#31995;&#32479;&#35282;&#24230;&#23454;&#29616;&#36127;&#36131;&#20219;&#30340;AI&#12290;&#19982;&#29616;&#26377;&#30340;&#20262;&#29702;&#21407;&#21017;&#26694;&#26550;&#19981;&#21516;&#65292;&#35813;&#30446;&#24405;&#25552;&#20379;&#20102;&#23454;&#36341;&#20013;&#21487;&#37319;&#21462;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#28085;&#30422;&#20102;AI&#31995;&#32479;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#36131;&#20219;&#30340;AI&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#25105;&#20204;&#26102;&#20195;&#26368;&#37325;&#35201;&#30340;&#31185;&#23398;&#25361;&#25112;&#20043;&#19968;&#65292;&#20063;&#26159;&#22686;&#21152;AI&#37319;&#29992;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;AI&#20262;&#29702;&#21407;&#21017;&#26694;&#26550;&#24050;&#32463;&#21457;&#24067;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#36827;&#19968;&#27493;&#30340;&#26368;&#20339;&#23454;&#36341;&#25351;&#23548;&#65292;&#20174;&#19994;&#32773;&#20204;&#38500;&#20102;&#31354;&#27934;&#30340;&#35789;&#21477;&#20043;&#22806;&#27809;&#26377;&#22826;&#22810;&#19996;&#35199;&#21487;&#20381;&#38752;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#21162;&#21147;&#26356;&#22810;&#22320;&#38598;&#20013;&#22312;&#31639;&#27861;&#23618;&#38754;&#19978;&#65292;&#32780;&#19981;&#26159;&#31995;&#32479;&#23618;&#38754;&#19978;&#65292;&#20027;&#35201;&#20851;&#27880;&#19968;&#20123;&#31526;&#21512;&#25968;&#23398;&#21407;&#21017;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#22914;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#20262;&#29702;&#38382;&#39064;&#21487;&#33021;&#22312;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#20219;&#20309;&#38454;&#27573;&#20986;&#29616;&#65292;&#28041;&#21450;&#22810;&#20010;AI&#21644;&#38750;AI&#31995;&#32479;&#32452;&#20214;&#12290;&#20026;&#20102;&#20174;&#31995;&#32479;&#35282;&#24230;&#23454;&#29616;&#36127;&#36131;&#20219;&#30340;AI&#65292;&#26412;&#25991;&#22522;&#20110;&#22810;&#22768;&#38899;&#25991;&#29486;&#32508;&#36848;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36127;&#36131;&#20219;&#30340;AI&#27169;&#24335;&#30446;&#24405;&#12290;&#25105;&#20204;&#30340;&#20851;&#27880;&#28857;&#19981;&#20165;&#20165;&#20572;&#30041;&#22312;&#21407;&#21017;&#21644;&#31639;&#27861;&#23618;&#38754;&#19978;&#65292;&#32780;&#26159;&#32858;&#28966;&#20110;AI&#31995;&#32479;&#21033;&#30410;&#30456;&#20851;&#32773;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#37319;&#21462;&#30340;&#27169;&#24335;&#65292;&#20197;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
Responsible AI is widely considered as one of the greatest scientific challenges of our time and is key to increase the adoption of AI. Recently, a number of AI ethics principles frameworks have been published. However, without further guidance on best practices, practitioners are left with nothing much beyond truisms. Also, significant efforts have been placed at algorithm-level rather than system-level, mainly focusing on a subset of mathematics-amenable ethical principles, such as fairness. Nevertheless, ethical issues can arise at any step of the development lifecycle, cutting across many AI and non-AI components of systems beyond AI algorithms and models. To operationalize responsible AI from a system perspective, in this paper, we present a Responsible AI Pattern Catalogue based on the results of a Multivocal Literature Review (MLR). Rather than staying at the principle or algorithm level, we focus on patterns that AI system stakeholders can undertake in practice to ensure that t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65288;QSANN&#65289;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#26426;&#21046;&#24341;&#20837;&#21040;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#39640;&#26031;&#25237;&#24433;&#30340;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;QSANN&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2205.05625</link><description>&lt;p&gt;
&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Quantum Self-Attention Neural Networks for Text Classification. (arXiv:2205.05625v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65288;QSANN&#65289;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#26426;&#21046;&#24341;&#20837;&#21040;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#39640;&#26031;&#25237;&#24433;&#30340;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;QSANN&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#30340;&#19968;&#20010;&#26032;&#20852;&#26041;&#21521;&#26159;&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20869;&#30340;&#20154;&#24037;&#26234;&#33021;&#21508;&#20010;&#39046;&#22495;&#24314;&#31435;&#26377;&#24847;&#20041;&#30340;&#37327;&#23376;&#24212;&#29992;&#12290;&#23613;&#31649;&#22522;&#20110;&#21477;&#27861;&#20998;&#26512;&#30340;&#19968;&#20123;&#24037;&#20316;&#20026;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#20294;&#26159;&#35832;&#22914;&#32321;&#37325;&#30340;&#21477;&#27861;&#39044;&#22788;&#29702;&#21644;&#21477;&#27861;&#30456;&#20851;&#30340;&#32593;&#32476;&#32467;&#26500;&#31561;&#38480;&#21046;&#20351;&#24471;&#23427;&#20204;&#22312;&#26356;&#22823;&#35268;&#27169;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65288;QSANN&#65289;&#65292;&#21487;&#20197;&#24357;&#34917;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#33258;&#27880;&#24847;&#26426;&#21046;&#24341;&#20837;&#21040;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#28982;&#21518;&#21033;&#29992;&#39640;&#26031;&#25237;&#24433;&#30340;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#20316;&#20026;&#33258;&#27880;&#24847;&#21147;&#30340;&#37327;&#23376;&#29256;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;QSANN&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#21487;&#23454;&#29616;&#30340;&#29702;&#24819;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;QSANN&#20248;&#20110;&#26368;&#20339;&#30340;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging direction of quantum computing is to establish meaningful quantum applications in various fields of artificial intelligence, including natural language processing (NLP). Although some efforts based on syntactic analysis have opened the door to research in Quantum NLP (QNLP), limitations such as heavy syntactic preprocessing and syntax-dependent network architecture make them impracticable on larger and real-world data sets. In this paper, we propose a new simple network architecture, called the quantum self-attention neural network (QSANN), which can compensate for these limitations. Specifically, we introduce the self-attention mechanism into quantum neural networks and then utilize a Gaussian projected quantum self-attention serving as a sensible quantum version of self-attention. As a result, QSANN is effective and scalable on larger data sets and has the desirable property of being implementable on near-term quantum devices. In particular, our QSANN outperforms the best
&lt;/p&gt;</description></item></channel></rss>