<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01261</link><description>&lt;p&gt;
TEDDY: &#22522;&#20110;&#24230;&#37327;&#21028;&#21035;&#31574;&#30053;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TEDDY: Trimming Edges with Degree-based Discrimination strategY
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01261
&lt;/p&gt;
&lt;p&gt;
TEDDY&#26159;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#36793;&#32536;&#20462;&#21098;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#24615;&#25805;&#20316;&#23454;&#29616;&#36793;&#32536;&#31232;&#30095;&#21270;&#65292;&#36827;&#32780;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#25928;&#26524;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Chen&#31561;&#20154;&#22312;2021&#24180;&#25552;&#20986;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25277;&#22870;&#31080;&#20551;&#35774;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20197;&#26469;&#65292;&#23547;&#25214;&#22270;&#25277;&#22870;&#31080;&#65288;GLT&#65289;&#30340;&#30740;&#31350;&#24050;&#25104;&#20026;GNN&#31038;&#21306;&#30340;&#37325;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#65292;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#29616;&#19982;&#21407;&#22987;&#23494;&#38598;&#32593;&#32476;&#30456;&#24403;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#26356;&#31232;&#30095;&#30340;GLT&#12290;&#21516;&#26102;&#65292;&#22270;&#32467;&#26500;&#20316;&#20026;GNN&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#20063;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#36817;&#20960;&#39033;&#30740;&#31350;&#30340;&#38416;&#26126;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20851;&#20110;GLT&#30340;&#30740;&#31350;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22270;&#32467;&#26500;&#20013;&#30340;&#20869;&#22312;&#36335;&#24452;&#65292;&#24182;&#20197;&#36845;&#20195;&#26041;&#24335;&#35782;&#21035;&#31080;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#32791;&#26102;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;TEDDY&#65292;&#19968;&#31181;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#24182;&#25972;&#21512;&#36793;&#32536;&#24230;&#37327;&#20449;&#24687;&#30340;&#19968;&#27425;&#24615;&#36793;&#32536;&#31232;&#30095;&#21270;&#26694;&#26550;&#12290;&#22312;&#36827;&#34892;&#36793;&#32536;&#31232;&#30095;&#21270;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#40723;&#21169;&#21442;&#25968;&#31232;&#30095;&#21270;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient desc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#31995;&#32479;VideoAgent&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20013;&#22830;&#20195;&#29702;&#65292;&#37319;&#29992;&#20114;&#21160;&#25512;&#29702;&#21644;&#35745;&#21010;&#26469;&#22788;&#29702;&#38271;&#35270;&#39057;&#29702;&#35299;&#38382;&#39064;&#65292;&#22312;&#25361;&#25112;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10517</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#39057;&#20195;&#29702;&#65306;&#38271;&#35270;&#39057;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
VideoAgent: Long-form Video Understanding with Large Language Model as Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10517
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#31995;&#32479;VideoAgent&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20013;&#22830;&#20195;&#29702;&#65292;&#37319;&#29992;&#20114;&#21160;&#25512;&#29702;&#21644;&#35745;&#21010;&#26469;&#22788;&#29702;&#38271;&#35270;&#39057;&#29702;&#35299;&#38382;&#39064;&#65292;&#22312;&#25361;&#25112;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#35270;&#39057;&#29702;&#35299;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20195;&#34920;&#30528;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#25512;&#29702;&#38271;&#26102;&#38388;&#22810;&#27169;&#24577;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#38271;&#35270;&#39057;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24378;&#35843;&#20114;&#21160;&#25512;&#29702;&#21644;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#22788;&#29702;&#38271;&#31687;&#35270;&#35273;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#31995;&#32479;VideoAgent&#65292;&#23427;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20013;&#22830;&#20195;&#29702;&#65292;&#36845;&#20195;&#22320;&#35782;&#21035;&#21644;&#25972;&#29702;&#20851;&#38190;&#20449;&#24687;&#20197;&#22238;&#31572;&#38382;&#39064;&#65292;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#24037;&#20855;&#26469;&#32763;&#35793;&#21644;&#26816;&#32034;&#35270;&#35273;&#20449;&#24687;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;EgoSchema&#21644;NExT-QA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;VideoAgent&#22312;&#24179;&#22343;&#20165;&#20351;&#29992;8.4&#21644;8.2&#24103;&#30340;&#24773;&#20917;&#19979;&#20998;&#21035;&#23454;&#29616;&#20102;54.1%&#21644;71.3%&#30340;&#38646;-shot&#20934;&#30830;&#29575;&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30456;&#23545;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#21331;&#36234;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#31361;&#20986;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10517v1 Announce Type: cross  Abstract: Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-base
&lt;/p&gt;</description></item><item><title>FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.10516</link><description>&lt;p&gt;
FeatUp: &#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29305;&#24449;&#20219;&#24847;&#20998;&#36776;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FeatUp: A Model-Agnostic Framework for Features at Any Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10516
&lt;/p&gt;
&lt;p&gt;
FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29305;&#24449;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#22522;&#30707;&#65292;&#25429;&#25417;&#22270;&#20687;&#35821;&#20041;&#24182;&#20351;&#31038;&#21306;&#33021;&#22815;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#20351;&#22312;&#38646;&#25110;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#32570;&#20047;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#20687;&#20998;&#21106;&#21644;&#28145;&#24230;&#39044;&#27979;&#36825;&#26679;&#30340;&#31264;&#23494;&#39044;&#27979;&#20219;&#21153;&#65292;&#22240;&#20026;&#27169;&#22411;&#20250;&#36807;&#20110;&#32858;&#21512;&#22823;&#33539;&#22260;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#65292;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;&#28145;&#24230;&#29305;&#24449;&#20013;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#19968;&#20010;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#24341;&#23548;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#20449;&#21495;&#30340;&#29305;&#24449;&#65292;&#21478;&#19968;&#20010;&#36866;&#24212;&#21333;&#20010;&#22270;&#20687;&#24182;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#26500;&#29305;&#24449;&#30340;&#38544;&#24335;&#27169;&#22411;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#19982; NeRF &#31867;&#20284;&#30340;&#28145;&#24230;&#31867;&#27604;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#29305;&#24449;&#20445;&#30041;&#20854;&#21407;&#22987;&#35821;&#20041;&#65292;&#24182;&#21487;&#20197;&#26367;&#25442;&#29616;&#26377;&#24212;&#29992;&#31243;&#24207;&#65292;&#21363;&#20351;&#19981;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10516v1 Announce Type: cross  Abstract: Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;HumanoidBench&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.10506</link><description>&lt;p&gt;
HumanoidBench&#65306;&#29992;&#20110;&#20840;&#36523;&#36816;&#21160;&#21644;&#25805;&#20316;&#30340;&#20223;&#30495;&#20154;&#22411;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10506
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;HumanoidBench&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#22411;&#26426;&#22120;&#20154;&#22312;&#21327;&#21161;&#20154;&#31867;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#31867;&#20154;&#24418;&#24577;&#12290;&#28982;&#32780;&#65292;&#20154;&#22411;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#24120;&#24120;&#21463;&#21040;&#26114;&#36149;&#19988;&#26131;&#25439;&#30340;&#30828;&#20214;&#35774;&#32622;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#21152;&#36895;&#20154;&#22411;&#26426;&#22120;&#20154;&#31639;&#27861;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;HumanoidBench&#65292;&#35813;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#37197;&#22791;&#28789;&#24039;&#25163;&#37096;&#21644;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20840;&#36523;&#25805;&#20316;&#21644;&#36816;&#21160;&#20219;&#21153;&#30340;&#20154;&#22411;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#30340;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#22312;&#34892;&#36208;&#25110;&#21040;&#36798;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#20511;&#21161;HumanoidBench&#65292;&#25105;&#20204;&#20026;&#26426;&#22120;&#20154;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#29992;&#20110;&#35782;&#21035;&#35299;&#20915;&#20154;&#22411;&#26426;&#22120;&#20154;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20419;&#36827;&#31639;&#27861;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10506v1 Announce Type: cross  Abstract: Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitatin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#24230;&#37327;&#30340;&#20840;&#26032;&#20449;&#24565;&#21464;&#21270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#36890;&#29992;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;&#12289;&#28385;&#36275;AGM&#20844;&#29702;&#30340;KM-based BC&#31639;&#23376;&#20197;&#21450;&#23558;&#28385;&#36275;AGM&#20844;&#29702;&#30340;&#20219;&#20309;BC&#31639;&#23376;&#29305;&#24449;&#21270;&#20026;&#22522;&#20110;KM&#30340;BC&#31639;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.10502</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#24230;&#37327;&#30340;&#20449;&#24565;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Belief Change based on Knowledge Measures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10502
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#24230;&#37327;&#30340;&#20840;&#26032;&#20449;&#24565;&#21464;&#21270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#36890;&#29992;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;&#12289;&#28385;&#36275;AGM&#20844;&#29702;&#30340;KM-based BC&#31639;&#23376;&#20197;&#21450;&#23558;&#28385;&#36275;AGM&#20844;&#29702;&#30340;&#20219;&#20309;BC&#31639;&#23376;&#29305;&#24449;&#21270;&#20026;&#22522;&#20110;KM&#30340;BC&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24230;&#37327;&#65288;KMs&#65289;&#26088;&#22312;&#37327;&#21270;&#30693;&#35782;&#24211;&#25152;&#25658;&#24102;&#30340;&#30693;&#35782;/&#20449;&#24687;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20449;&#24565;&#21464;&#21270;&#65288;BC&#65289;&#26159;&#25913;&#21464;&#20449;&#24565;&#30340;&#36807;&#31243;&#65288;&#22312;&#25105;&#20204;&#30340;&#26696;&#20363;&#20013;&#65292;&#26159;&#25351;&#32553;&#20943;&#12289;&#25193;&#23637;&#21644;&#20462;&#27491;&#65289;&#65292;&#32771;&#34385;&#21040;&#19968;&#26465;&#21487;&#33021;&#19982;&#24403;&#21069;&#20449;&#24565;&#30456;&#30683;&#30462;&#30340;&#26032;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;KMs&#30340;&#20840;&#26032;&#37327;&#21270;BC&#26694;&#26550;&#65292;&#36890;&#36807;&#23450;&#20041;&#20449;&#24565;&#21464;&#21270;&#31639;&#23376;&#26469;&#35797;&#22270;&#20174;&#20449;&#24687;&#35770;&#35282;&#24230;&#26368;&#23567;&#21270;&#25913;&#21464;&#21518;&#30340;&#20449;&#24565;&#25152;&#25658;&#24102;&#30340;&#24847;&#22806;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#23567;&#24847;&#22806;&#21407;&#21017;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#20010;&#36890;&#29992;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#22788;&#29702;KMs&#65292;&#20854;&#20013;[1]&#26159;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65307;&#65288;ii&#65289;&#22522;&#20110;KM&#30340;BC&#31639;&#23376;&#65292;&#28385;&#36275;&#25152;&#35859;&#30340;AGM&#20844;&#29702;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#23558;&#28385;&#36275;AGM&#20844;&#29702;&#30340;&#20219;&#20309;BC&#31639;&#23376;&#30340;&#29305;&#24449;&#21270;&#20026;&#22522;&#20110;KM&#30340;BC&#31639;&#23376;&#65292;&#21363;&#20219;&#20309;BC&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10502v1 Announce Type: new  Abstract: Knowledge Measures (KMs) aim at quantifying the amount of knowledge/information that a knowledge base carries. On the other hand, Belief Change (BC) is the process of changing beliefs (in our case, in terms of contraction, expansion and revision) taking into account a new piece of knowledge, which possibly may be in contradiction with the current belief. We propose a new quantitative BC framework that is based on KMs by defining belief change operators that try to minimise, from an information-theoretic point of view, the surprise that the changed belief carries. To this end, we introduce the principle of minimal surprise. In particular, our contributions are (i) a general information-theoretic approach to KMs for which [1] is a special case; (ii) KM-based BC operators that satisfy the so-called AGM postulates; and (iii) a characterisation of any BC operator that satisfies the AGM postulates as a KM-based BC operator, i.e., any BC operat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;CLIP&#36827;&#34892;&#22823;&#35268;&#27169;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#28085;&#30422;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#12289;&#21512;&#25104;&#20998;&#24067;&#20559;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;&#31561;&#22810;&#20010;&#26041;&#38754;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.10499</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;CLIP&#36827;&#34892;&#22823;&#35268;&#27169;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#28085;&#30422;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#12289;&#21512;&#25104;&#20998;&#24067;&#20559;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;&#31561;&#22810;&#20010;&#26041;&#38754;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#20851;&#20110;&#22270;&#20687;&#30340;&#21407;&#22987;&#25991;&#26412;&#20013;&#39044;&#35757;&#32451;&#22270;&#20687;&#34920;&#31034;&#65292;&#20351;&#24471;&#38646;&#26679;&#26412;&#35270;&#35273;&#20256;&#36755;&#33267;&#19979;&#28216;&#20219;&#21153;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#22312;&#20114;&#32852;&#32593;&#19978;&#37319;&#38598;&#30340;&#25968;&#30334;&#19975;&#26679;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22914;CLIP&#20043;&#31867;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#65292;&#36890;&#24120;&#22312;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;&#38500;&#20102;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#40723;&#33310;&#20154;&#24515;&#20043;&#22806;&#65292;&#25253;&#36947;&#31216;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22312;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#19979;&#19982;&#22312;ImageNet&#19978;&#35757;&#32451;&#30340;&#30417;&#30563;&#27169;&#22411;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#26469;&#32553;&#23567;&#40065;&#26834;&#24615;&#24046;&#36317;&#12290;&#30001;&#20110;&#40065;&#26834;&#24615;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28085;&#30422;7&#31181;&#33258;&#28982;&#12289;3&#31181;&#21512;&#25104;&#20998;&#24067;&#20559;&#31227;&#21644;11&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#22823;&#35268;&#27169;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#20197;CLIP&#20316;&#20026;&#35797;&#28857;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CLIP&#23548;&#33268;&#20102;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10499v1 Announce Type: cross  Abstract: Pre-training image representations from the raw text about images enables zero-shot vision transfer to downstream tasks. Through pre-training on millions of samples collected from the internet, multimodal foundation models, such as CLIP, produce state-of-the-art zero-shot results that often reach competitiveness with fully supervised methods without the need for task-specific training. Besides the encouraging performance on classification accuracy, it is reported that these models close the robustness gap by matching the performance of supervised models trained on ImageNet under natural distribution shift. Because robustness is critical to real-world applications, especially safety-critical ones, in this paper, we present a comprehensive evaluation based on a large-scale robustness benchmark covering 7 natural, 3 synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a pilot study. We show that CLIP leads to a signif
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31454;&#20105;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#31454;&#20105;&#20449;&#24687;&#28608;&#21457;&#26426;&#22120;&#20154;&#30340;&#28508;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#26426;&#22120;&#20154;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.10487</link><description>&lt;p&gt;
&#36890;&#36807;&#31454;&#20105;&#28608;&#21457;&#26426;&#22120;&#20154;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Stimulate the Potential of Robots via Competition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31454;&#20105;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#31454;&#20105;&#20449;&#24687;&#28608;&#21457;&#26426;&#22120;&#20154;&#30340;&#28508;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#26426;&#22120;&#20154;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#24863;&#21463;&#21040;&#21387;&#21147;&#26159;&#24456;&#24120;&#35265;&#30340;&#65292;&#36825;&#31181;&#21387;&#21147;&#28304;&#20110;&#19982;&#20854;&#20182;&#20010;&#20307;&#25110;&#23545;&#25163;&#27604;&#36739;&#33719;&#24471;&#25104;&#21151;&#30340;&#24895;&#26395;&#12290;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#22312;&#21387;&#21147;&#19979;&#24863;&#21040;&#28966;&#34385;&#65292;&#20294;&#36825;&#20063;&#21487;&#20197;&#28608;&#21457;&#25105;&#20204;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#28508;&#21147;&#65292;&#20197;&#20351;&#33258;&#24049;&#19982;&#20182;&#20154;&#20445;&#25345;&#21516;&#27493;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31454;&#20105;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#20307;&#26426;&#22120;&#20154;&#20174;&#31454;&#20105;&#20013;&#33719;&#24471;&#30693;&#35782;&#65292;&#20805;&#20998;&#28608;&#21457;&#20854;&#22312;&#27604;&#36187;&#20013;&#30340;&#21160;&#24577;&#28508;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#31454;&#20105;&#32773;&#20043;&#38388;&#30340;&#31454;&#20105;&#20449;&#24687;&#34987;&#24341;&#20837;&#20026;&#39069;&#22806;&#30340;&#36741;&#21161;&#20449;&#21495;&#65292;&#20197;&#23398;&#20064;&#20986;&#20248;&#21183;&#34892;&#20026;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#31454;&#36187;&#29615;&#22659;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#26426;&#22120;&#20154;&#32988;&#36807;&#20102;&#22312;&#21333;&#20010;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#29992;SoTA&#31639;&#27861;&#35757;&#32451;&#30340;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10487v1 Announce Type: cross  Abstract: It is common for us to feel pressure in a competition environment, which arises from the desire to obtain success comparing with other individuals or opponents. Although we might get anxious under the pressure, it could also be a drive for us to stimulate our potentials to the best in order to keep up with others. Inspired by this, we propose a competitive learning framework which is able to help individual robot to acquire knowledge from the competition, fully stimulating its dynamics potential in the race. Specifically, the competition information among competitors is introduced as the additional auxiliary signal to learn advantaged actions. We further build a Multiagent-Race environment, and extensive experiments are conducted, demonstrating that robots trained in competitive environments outperform ones that are trained with SoTA algorithms in single robot environment.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;AI&#20195;&#29702;&#30340;&#25972;&#21512;&#22312;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#39046;&#22495;&#26631;&#24535;&#30528;&#19968;&#39033;&#24320;&#21019;&#24615;&#21457;&#23637;&#65292;&#33021;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;&#25237;&#36164;&#32452;&#21512;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.10482</link><description>&lt;p&gt;
GPT4&#21160;&#21147;AI&#20195;&#29702;&#33021;&#25104;&#20026;&#36275;&#22815;&#20248;&#31168;&#30340;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#24072;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10482
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;AI&#20195;&#29702;&#30340;&#25972;&#21512;&#22312;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#39046;&#22495;&#26631;&#24535;&#30528;&#19968;&#39033;&#24320;&#21019;&#24615;&#21457;&#23637;&#65292;&#33021;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;&#25237;&#36164;&#32452;&#21512;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#34987;&#23450;&#20041;&#20026;&#35299;&#37322;&#25237;&#36164;&#32452;&#21512;&#30456;&#23545;&#20110;&#22522;&#20934;&#30340;&#36229;&#39069;&#32489;&#25928;&#39537;&#21160;&#22240;&#32032;&#30340;&#36807;&#31243;&#65292;&#22312;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#20013;&#21344;&#25454;&#37325;&#35201;&#22320;&#20301;&#65292;&#22312;&#25237;&#36164;&#20915;&#31574;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#37329;&#31649;&#29702;&#34892;&#19994;&#12290;&#26681;&#26893;&#20110;&#29282;&#22266;&#30340;&#37329;&#34701;&#21644;&#25968;&#23398;&#26694;&#26550;&#20013;&#65292;&#36825;&#31181;&#20998;&#26512;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#21644;&#26041;&#27861;&#23398;&#24050;&#22312;&#20247;&#22810;&#23398;&#26415;&#30740;&#31350;&#35770;&#25991;&#21644;&#33879;&#20316;&#20013;&#24471;&#21040;&#24191;&#27867;&#35760;&#24405;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;AI&#20195;&#29702;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#35813;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#21457;&#23637;&#12290;&#36825;&#20123;&#20195;&#29702;&#26088;&#22312;&#36890;&#36807;&#20934;&#30830;&#35745;&#31639;&#21644;&#20998;&#26512;&#25237;&#36164;&#32452;&#21512;&#34920;&#29616;&#19982;&#22522;&#20934;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;AI&#20195;&#29702;&#24212;&#29992;&#20110;&#21508;&#31181;&#37325;&#35201;&#30340;&#32489;&#25928;&#24402;&#22240;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10482v1 Announce Type: cross  Abstract: Performance attribution analysis, defined as the process of explaining the drivers of the excess performance of an investment portfolio against a benchmark, stands as a significant aspect of portfolio management and plays a crucial role in the investment decision-making process, particularly within the fund management industry. Rooted in a solid financial and mathematical framework, the importance and methodologies of this analytical technique are extensively documented across numerous academic research papers and books. The integration of large language models (LLMs) and AI agents marks a groundbreaking development in this field. These agents are designed to automate and enhance the performance attribution analysis by accurately calculating and analyzing portfolio performances against benchmarks. In this study, we introduce the application of an AI Agent for a variety of essential performance attribution tasks, including the analysis 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#26696;&#20363;&#26694;&#26550;&#26469;&#32452;&#32455;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#35770;&#35777;&#65292;&#21253;&#25324;&#22235;&#31867;&#35770;&#28857;&#65306;&#23436;&#20840;&#26080;&#27861;&#36896;&#25104;&#28798;&#38590;&#65292;&#24378;&#22823;&#30340;&#25511;&#21046;&#25514;&#26045;&#65292;&#23613;&#31649;&#26377;&#21487;&#33021;&#36896;&#25104;&#20260;&#23475;&#65292;&#20294;&#20381;&#28982;&#21487;&#20449;&#36182;&#65292;&#20197;&#21450;&#23545;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#39038;&#38382;&#30340;&#23562;&#37325;&#12290;</title><link>https://arxiv.org/abs/2403.10462</link><description>&lt;p&gt;
&#23433;&#20840;&#26696;&#20363;&#65306;&#35777;&#26126;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Safety Cases: Justifying the Safety of Advanced AI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#26696;&#20363;&#26694;&#26550;&#26469;&#32452;&#32455;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#35770;&#35777;&#65292;&#21253;&#25324;&#22235;&#31867;&#35770;&#28857;&#65306;&#23436;&#20840;&#26080;&#27861;&#36896;&#25104;&#28798;&#38590;&#65292;&#24378;&#22823;&#30340;&#25511;&#21046;&#25514;&#26045;&#65292;&#23613;&#31649;&#26377;&#21487;&#33021;&#36896;&#25104;&#20260;&#23475;&#65292;&#20294;&#20381;&#28982;&#21487;&#20449;&#36182;&#65292;&#20197;&#21450;&#23545;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#39038;&#38382;&#30340;&#23562;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21464;&#24471;&#26356;&#21152;&#20808;&#36827;&#65292;&#20225;&#19994;&#21644;&#30417;&#31649;&#26426;&#26500;&#23558;&#38754;&#20020;&#20851;&#20110;&#26159;&#21542;&#23433;&#20840;&#36827;&#34892;&#35757;&#32451;&#21644;&#37096;&#32626;&#30340;&#22256;&#38590;&#20915;&#31574;&#12290;&#20026;&#20102;&#20026;&#36825;&#20123;&#20915;&#31574;&#20570;&#20934;&#22791;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24320;&#21457;&#20154;&#21592;&#22914;&#20309;&#21046;&#23450;&#19968;&#31181;"&#23433;&#20840;&#26696;&#20363;"&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#29702;&#30001;&#65292;&#35777;&#26126;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19981;&#22826;&#21487;&#33021;&#36896;&#25104;&#28798;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32452;&#32455;&#23433;&#20840;&#26696;&#20363;&#30340;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22235;&#31867;&#35770;&#35777;&#23433;&#20840;&#30340;&#35770;&#28857;&#65306;&#23436;&#20840;&#26080;&#27861;&#36896;&#25104;&#28798;&#38590;&#65292;&#36275;&#22815;&#24378;&#22823;&#30340;&#25511;&#21046;&#25514;&#26045;&#65292;&#23613;&#31649;&#33021;&#22815;&#36896;&#25104;&#20260;&#23475;&#20173;&#20540;&#24471;&#20449;&#36182;&#65292;&#20197;&#21450;&#23545;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#39038;&#38382;&#30340;&#23562;&#37325;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#20855;&#20307;&#35770;&#28857;&#31034;&#20363;&#65292;&#24182;&#27010;&#36848;&#20102;&#22914;&#20309;&#32452;&#21512;&#35770;&#28857;&#26469;&#35777;&#26126;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#23433;&#20840;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10462v1 Announce Type: cross  Abstract: As AI systems become more advanced, companies and regulators will make difficult decisions about whether it is safe to train and deploy them. To prepare for these decisions, we investigate how developers could make a 'safety case,' which is a structured rationale that AI systems are unlikely to cause a catastrophe. We propose a framework for organizing a safety case and discuss four categories of arguments to justify safety: total inability to cause a catastrophe, sufficiently strong control measures, trustworthiness despite capability to cause harm, and deference to credible AI advisors. We evaluate concrete examples of arguments in each category and outline how arguments could be combined to justify that AI systems are safe to deploy.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#22320;&#24179;&#32447;&#30340;&#38598;&#20013;&#24335;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#24182;&#21457;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.10460</link><description>&lt;p&gt;
&#22312;&#32447;&#24182;&#21457;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Online Concurrent Multi-Robot Coverage Path Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#22320;&#24179;&#32447;&#30340;&#38598;&#20013;&#24335;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#24182;&#21457;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#38598;&#20013;&#24335;&#36880;&#27493;&#22320;&#24179;&#32447;&#22312;&#32447;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#23637;&#29616;&#20986;&#22312;&#24443;&#24213;&#25506;&#32034;&#25317;&#26377;&#22823;&#37327;&#26426;&#22120;&#20154;&#30340;&#22823;&#22411;&#12289;&#22797;&#26434;&#12289;&#26410;&#30693;&#24037;&#20316;&#31354;&#38388;&#26041;&#38754;&#30340;&#20986;&#33394;&#21487;&#20280;&#32553;&#24615;&#12290;&#22312;&#19968;&#20010;&#26102;&#38388;&#27573;&#20869;&#65292;&#36335;&#24452;&#35268;&#21010;&#21644;&#36335;&#24452;&#25191;&#34892;&#20132;&#26367;&#36827;&#34892;&#65292;&#21363;&#24403;&#20026;&#27809;&#26377;&#36335;&#24452;&#30340;&#26426;&#22120;&#20154;&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#26102;&#65292;&#20855;&#26377;&#26410;&#23436;&#25104;&#36335;&#24452;&#30340;&#26426;&#22120;&#20154;&#19981;&#25191;&#34892;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#22522;&#20110;&#22320;&#24179;&#32447;&#30340;&#38598;&#20013;&#24335;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#38543;&#26102;&#20026;&#27809;&#26377;&#36335;&#24452;&#30340;&#26426;&#22120;&#20154;&#23376;&#38598;&#65288;&#21363;&#24050;&#36798;&#21040;&#20854;&#20808;&#21069;&#20998;&#37197;&#30446;&#26631;&#30340;&#26426;&#22120;&#20154;&#65289;&#35268;&#21010;&#36335;&#24452;&#65292;&#32780;&#20854;&#20313;&#26426;&#22120;&#20154;&#25191;&#34892;&#20854;&#26410;&#23436;&#25104;&#30340;&#36335;&#24452;&#65292;&#20174;&#32780;&#23454;&#29616;&#24182;&#21457;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#35813;&#25552;&#35758;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10460v1 Announce Type: cross  Abstract: Recently, centralized receding horizon online multi-robot coverage path planning algorithms have shown remarkable scalability in thoroughly exploring large, complex, unknown workspaces with many robots. In a horizon, the path planning and the path execution interleave, meaning when the path planning occurs for robots with no paths, the robots with outstanding paths do not execute, and subsequently, when the robots with new or outstanding paths execute to reach respective goals, path planning does not occur for those robots yet to get new paths, leading to wastage of both the robotic and the computation resources. As a remedy, we propose a centralized algorithm that is not horizon-based. It plans paths at any time for a subset of robots with no paths, i.e., who have reached their previously assigned goals, while the rest execute their outstanding paths, thereby enabling concurrent planning and execution. We formally prove that the propo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;TAMP&#31574;&#30053;&#65288;TAMPURA&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#21021;&#22987;&#29366;&#24577;&#21644;&#21160;&#20316;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#38271;&#26102;&#31243;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#20449;&#24687;&#25910;&#38598;&#21644;&#36991;&#20813;&#19981;&#33391;&#21644;&#19981;&#21487;&#36870;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10454</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10454
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;TAMP&#31574;&#30053;&#65288;TAMPURA&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#21021;&#22987;&#29366;&#24577;&#21644;&#21160;&#20316;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#38271;&#26102;&#31243;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#20449;&#24687;&#25910;&#38598;&#21644;&#36991;&#20813;&#19981;&#33391;&#21644;&#19981;&#21487;&#36870;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#30340;&#38271;&#26102;&#31243;&#26426;&#22120;&#20154;&#25805;&#32437;&#21644;&#23548;&#33322;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;TAMP&#38382;&#39064;&#20844;&#24335;&#21270;&#20551;&#35774;&#23436;&#20840;&#21487;&#35266;&#27979;&#21644;&#30830;&#23450;&#24615;&#21160;&#20316;&#25928;&#26524;&#12290;&#36825;&#20123;&#20551;&#35774;&#38480;&#21046;&#20102;&#35268;&#21010;&#32773;&#33719;&#21462;&#20449;&#24687;&#21644;&#20570;&#20986;&#20855;&#26377;&#39118;&#38505;&#24847;&#35782;&#30340;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;TAMP&#31574;&#30053;&#65288;TAMPURA&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#21021;&#22987;&#29366;&#24577;&#21644;&#21160;&#20316;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#38271;&#26102;&#31243;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#20449;&#24687;&#25910;&#38598;&#21644;&#36991;&#20813;&#19981;&#33391;&#21644;&#19981;&#21487;&#36870;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35268;&#21010;&#32773;&#22312;&#25277;&#35937;&#20219;&#21153;&#32423;&#21035;&#21644;&#36830;&#32493;&#25511;&#21046;&#22120;&#32423;&#21035;&#22343;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#36827;&#34892;&#25512;&#29702;&#12290;&#37492;&#20110;&#19968;&#32452;&#22312;&#22522;&#26412;&#21160;&#20316;&#31354;&#38388;&#20013;&#36816;&#34892;&#30340;&#38381;&#29615;&#30446;&#26631;&#39537;&#21160;&#25511;&#21046;&#22120;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#20204;&#30340;&#21069;&#25552;&#26465;&#20214;&#21644;&#28508;&#22312;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10454v1 Announce Type: cross  Abstract: Integrated task and motion planning (TAMP) has proven to be a valuable approach to generalizable long-horizon robotic manipulation and navigation problems. However, the typical TAMP problem formulation assumes full observability and deterministic action effects. These assumptions limit the ability of the planner to gather information and make decisions that are risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness (TAMPURA) that is capable of efficiently solving long-horizon planning problems with initial-state and action outcome uncertainty, including problems that require information gathering and avoiding undesirable and irreversible outcomes. Our planner reasons under uncertainty at both the abstract task level and continuous controller level. Given a set of closed-loop goal-conditioned controllers operating in the primitive action space and a description of their preconditions and potential capabilities, w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#20262;&#29702;&#32039;&#24613;&#28436;&#32451;&#65288;DEED&#65289;&#24037;&#20855;&#31665;&#65292;&#24110;&#21161;&#25968;&#25454;&#31185;&#23398;&#22242;&#38431;&#35752;&#35770;&#21644;&#21453;&#24605;&#24037;&#20316;&#20013;&#30340;&#20262;&#29702;&#24433;&#21709;&#65292;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#34394;&#26500;&#20262;&#29702;&#32039;&#24613;&#24773;&#26223;&#65292;&#24320;&#21551;&#20102;&#20851;&#20110;&#20262;&#29702;&#38382;&#39064;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.10438</link><description>&lt;p&gt;
&#25968;&#25454;&#20262;&#29702;&#32039;&#24613;&#28436;&#32451;&#65306;&#29992;&#20110;&#35752;&#35770;&#24037;&#19994;&#22242;&#38431;&#36127;&#36131;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
Data Ethics Emergency Drill: A Toolbox for Discussing Responsible AI for Industry Teams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#20262;&#29702;&#32039;&#24613;&#28436;&#32451;&#65288;DEED&#65289;&#24037;&#20855;&#31665;&#65292;&#24110;&#21161;&#25968;&#25454;&#31185;&#23398;&#22242;&#38431;&#35752;&#35770;&#21644;&#21453;&#24605;&#24037;&#20316;&#20013;&#30340;&#20262;&#29702;&#24433;&#21709;&#65292;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#34394;&#26500;&#20262;&#29702;&#32039;&#24613;&#24773;&#26223;&#65292;&#24320;&#21551;&#20102;&#20851;&#20110;&#20262;&#29702;&#38382;&#39064;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25958;&#20419;&#25968;&#25454;&#31185;&#23398;&#23478;&#31561;&#25216;&#26415;&#20174;&#19994;&#32773;&#32771;&#34385;&#31639;&#27861;&#20915;&#31574;&#30340;&#24433;&#21709;&#21644;&#20262;&#29702;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#19982;&#32534;&#31243;&#12289;&#32479;&#35745;&#21644;&#25968;&#25454;&#31649;&#29702;&#19981;&#21516;&#65292;&#23545;&#20262;&#29702;&#24433;&#21709;&#30340;&#35752;&#35770;&#24456;&#23569;&#21253;&#21547;&#22312;&#26631;&#20934;&#25968;&#25454;&#31185;&#23398;&#22521;&#35757;&#20013;&#12290;&#20026;&#20102;&#24320;&#22987;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#27979;&#35797;&#20102;&#19968;&#20010;&#21517;&#20026;&#25968;&#25454;&#20262;&#29702;&#32039;&#24613;&#28436;&#32451;&#65288;DEED&#65289;&#30340;&#24037;&#20855;&#31665;&#65292;&#20197;&#24110;&#21161;&#25968;&#25454;&#31185;&#23398;&#22242;&#38431;&#35752;&#35770;&#21644;&#21453;&#24605;&#20854;&#24037;&#20316;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290; DEED&#26159;&#19968;&#20010;&#34394;&#26500;&#30340;&#20262;&#29702;&#32039;&#24613;&#22330;&#26223;&#30340;&#35282;&#33394;&#25198;&#28436;&#65292;&#20854;&#24773;&#22659;&#22320;&#22788;&#20110;&#22242;&#38431;&#20855;&#20307;&#30340;&#24037;&#20316;&#22330;&#25152;&#21644;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;DEED&#24037;&#20855;&#31665;&#65292;&#24182;&#25551;&#36848;&#20102;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#31185;&#23398;&#22242;&#38431;&#36827;&#34892;&#30340;&#19977;&#39033;&#30740;&#31350;&#65292;&#36825;&#20123;&#30740;&#31350;&#22312;&#35774;&#35745;&#19978;&#36827;&#34892;&#20102;&#36845;&#20195;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#35282;&#33394;&#25198;&#28436;&#20013;&#23398;&#21040;&#30340;&#32463;&#39564;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;DEED&#22914;&#20309;&#24341;&#21457;&#20102;&#20851;&#20110;&#20262;&#29702;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10438v1 Announce Type: cross  Abstract: Researchers urge technology practitioners such as data scientists to consider the impacts and ethical implications of algorithmic decisions. However, unlike programming, statistics, and data management, discussion of ethical implications is rarely included in standard data science training. To begin to address this gap, we designed and tested a toolbox called the data ethics emergency drill (DEED) to help data science teams discuss and reflect on the ethical implications of their work. The DEED is a roleplay of a fictional ethical emergency scenario that is contextually situated in the team's specific workplace and applications. This paper outlines the DEED toolbox and describes three studies carried out with two different data science teams that iteratively shaped its design. Our findings show that practitioners can apply lessons learnt from the roleplay to real-life situations, and how the DEED opened up conversations around ethics a
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#24418;&#25104;&#30340;&#22810;&#23618;&#27425;&#38598;&#20307;&#26234;&#33021;&#32593;&#32476;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#20219;&#19968;&#21333;&#29420;&#23454;&#20307;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.10433</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#38598;&#20307;&#26234;&#33021;&#65306;&#29616;&#29366;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
AI-enhanced Collective Intelligence: The State of the Art and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10433
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#24418;&#25104;&#30340;&#22810;&#23618;&#27425;&#38598;&#20307;&#26234;&#33021;&#32593;&#32476;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#20219;&#19968;&#21333;&#29420;&#23454;&#20307;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#31038;&#20250;&#25361;&#25112;&#36229;&#20986;&#20102;&#20154;&#31867;&#20010;&#20307;&#25110;&#38598;&#20307;&#21162;&#21147;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#22312;&#20154;&#31867;&#38598;&#20307;&#20013;&#30340;&#35282;&#33394;&#23558;&#20174;&#36741;&#21161;&#24037;&#20855;&#36716;&#21464;&#20026;&#21442;&#19982;&#24335;&#25104;&#21592;&#12290;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#25317;&#26377;&#20114;&#34917;&#30340;&#33021;&#21147;&#65292;&#24403;&#20108;&#32773;&#21327;&#21516;&#20316;&#29992;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#19968;&#31181;&#36229;&#36234;&#21333;&#29420;&#20154;&#31867;&#25110;&#20154;&#24037;&#26234;&#33021;&#38598;&#20307;&#33021;&#21147;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#20132;&#20114;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#65292;&#28041;&#21450;&#22797;&#26434;&#30340;&#36807;&#31243;&#21644;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#32508;&#36848;&#20174;&#32593;&#32476;&#31185;&#23398;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26500;&#24819;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#20154;&#24037;&#26234;&#33021;&#38598;&#20307;&#26234;&#33021;&#34920;&#31034;&#65292;&#21253;&#25324;&#35748;&#30693;&#23618;&#12289;&#29289;&#29702;&#23618;&#21644;&#20449;&#24687;&#23618;&#12290;&#22312;&#36825;&#20010;&#22810;&#23618;&#32593;&#32476;&#20013;&#65292;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#29305;&#24449;&#65307;&#20154;&#31867;&#22312;&#22810;&#26679;&#24615;&#26041;&#38754;&#20174;&#34920;&#23618;&#21040;&#28145;&#23618;&#23646;&#24615;&#19981;&#21516;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#31243;&#24230;&#19978;&#20063;&#26377;&#25152;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10433v1 Announce Type: cross  Abstract: The current societal challenges exceed the capacity of human individual or collective effort alone. As AI evolves, its role within human collectives is poised to vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, when synergized, can achieve a level of collective intelligence that surpasses the collective capabilities of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising a cognition layer, a physical layer, and an information layer. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of f
&lt;/p&gt;</description></item><item><title>NeuFlow&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20809;&#27969;&#26550;&#26500;&#65292;&#36890;&#36807;&#20840;&#23616;&#21040;&#23616;&#37096;&#30340;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#20809;&#27969;&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#20102;&#35745;&#31639;&#25104;&#26412;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10425</link><description>&lt;p&gt;
NeuFlow: &#20351;&#29992;&#36793;&#32536;&#35774;&#22791;&#22312;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#23454;&#26102;&#39640;&#31934;&#24230;&#20809;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10425
&lt;/p&gt;
&lt;p&gt;
NeuFlow&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20809;&#27969;&#26550;&#26500;&#65292;&#36890;&#36807;&#20840;&#23616;&#21040;&#23616;&#37096;&#30340;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#20809;&#27969;&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#20102;&#35745;&#31639;&#25104;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#39640;&#31934;&#24230;&#20809;&#27969;&#20272;&#35745;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#23450;&#20301;&#21644;&#22320;&#22270;&#32472;&#21046;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#30446;&#26631;&#36319;&#36394;&#21644;&#27963;&#21160;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuFlow&#30340;&#39640;&#25928;&#20809;&#27969;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#31934;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#37319;&#29992;&#20102;&#19968;&#31181;&#20840;&#23616;&#21040;&#23616;&#37096;&#30340;&#35774;&#35745;&#26041;&#26696;&#65292;&#36890;&#36807;&#20840;&#23616;&#21305;&#37197;&#22312;1/16&#20998;&#36776;&#29575;&#19978;&#23545;&#21021;&#22987;&#20809;&#27969;&#36827;&#34892;&#20272;&#35745;&#65292;&#25429;&#25417;&#36739;&#22823;&#30340;&#20301;&#31227;&#65292;&#28982;&#21518;&#22312;1/8&#20998;&#36776;&#29575;&#19978;&#20351;&#29992;&#36731;&#37327;&#32423;CNN&#23618;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10425v1 Announce Type: cross  Abstract: Real-time high-accuracy optical flow estimation is a crucial component in various applications, including localization and mapping in robotics, object tracking, and activity recognition in computer vision. While recent learning-based optical flow methods have achieved high accuracy, they often come with heavy computation costs. In this paper, we propose a highly efficient optical flow architecture, called NeuFlow, that addresses both high accuracy and computational cost concerns. The architecture follows a global-to-local scheme. Given the features of the input images extracted at different spatial resolutions, global matching is employed to estimate an initial optical flow on the 1/16 resolution, capturing large displacement, which is then refined on the 1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our approach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency improvements across different compu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#25216;&#26415;&#35780;&#20272;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#23558;&#20854;&#24402;&#31867;&#20026;&#22235;&#20010;&#19981;&#21516;&#31867;&#21035;</title><link>https://arxiv.org/abs/2403.10415</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#29305;&#24449;&#24402;&#22240;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25216;&#26415;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Gradient based Feature Attribution in Explainable AI: A Technical Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10415
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#25216;&#26415;&#35780;&#20272;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#23558;&#20854;&#24402;&#31867;&#20026;&#22235;&#20010;&#19981;&#21516;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10415v1 &#20844;&#24067;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#40657;&#30418;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#28608;&#22686;&#20419;&#20351;&#20102;&#35299;&#37322;&#20869;&#37096;&#26426;&#21046;&#24182;&#35777;&#26126;&#20854;&#21487;&#38752;&#24615;&#30340;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#33258;&#21160;&#39550;&#39542;&#12290;&#30001;&#20110;&#32570;&#20047;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#20005;&#26684;&#23450;&#20041;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#22823;&#37327;&#19982;&#35299;&#37322;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#30456;&#20851;&#30340;&#30740;&#31350;&#65292;&#20197;&#20174;&#21508;&#31181;&#35282;&#24230;&#35299;&#37322;&#21644;&#20998;&#26512;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#35770;&#25991;&#65292;&#20840;&#38754;&#20102;&#35299;XAI&#30740;&#31350;&#30340;&#21508;&#20010;&#26041;&#38754;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32771;&#34385;&#21040;&#31070;&#32463;&#32593;&#32476;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#27969;&#34892;&#65292;&#25105;&#20204;&#23558;&#28966;&#28857;&#32553;&#23567;&#21040;XAI&#30740;&#31350;&#30340;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#65306;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#65292;&#36825;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#22312;&#26412;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#36804;&#20170;&#20026;&#27490;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#23558;&#20854;&#24402;&#31867;&#20026;&#22235;&#20010;&#19981;&#21516;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10415v1 Announce Type: new  Abstract: The surge in black-box AI models has prompted the need to explain the internal mechanism and justify their reliability, especially in high-stakes applications, such as healthcare and autonomous driving. Due to the lack of a rigorous definition of explainable AI (XAI), a plethora of research related to explainability, interpretability, and transparency has been developed to explain and analyze the model from various perspectives. Consequently, with an exhaustive list of papers, it becomes challenging to have a comprehensive overview of XAI research from all aspects. Considering the popularity of neural networks in AI research, we narrow our focus to a specific area of XAI research: gradient based explanations, which can be directly adopted for neural network models. In this review, we systematically explore gradient based explanation methods to date and introduce a novel taxonomy to categorize them into four distinct classes. Then, we pre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#20064;&#20869;&#37096;&#20998;&#24067;&#29305;&#24449;&#23494;&#24230;&#30340;&#33021;&#37327;&#26657;&#27491;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.10403</link><description>&lt;p&gt;
&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#33021;&#37327;&#26657;&#27491;&#27169;&#22411;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Energy Correction Model in the Feature Space for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10403
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#20064;&#20869;&#37096;&#20998;&#24067;&#29305;&#24449;&#23494;&#24230;&#30340;&#33021;&#37327;&#26657;&#27491;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#30740;&#31350;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#23398;&#20064;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#29305;&#24449;&#30340;&#23494;&#24230;&#65292;&#21457;&#29616;&#22312;EBM&#35757;&#32451;&#36807;&#31243;&#20013;MCMC&#37319;&#26679;&#30340;&#38750;&#28151;&#21512;&#24615;&#20250;&#21066;&#24369;&#20854;&#26816;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#28151;&#21512;&#31867;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#32452;&#25104;&#30340;&#33021;&#37327;&#26657;&#27491;&#27169;&#22411;&#65292;&#19982;CIFAR-10/CIFAR-100 OOD&#26816;&#27979;&#22522;&#20934;&#19978;&#30340;&#24378;&#22522;&#32447;KNN&#26816;&#27979;&#22120;&#30456;&#27604;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10403v1 Announce Type: cross  Abstract: In this work, we study the out-of-distribution (OOD) detection problem through the use of the feature space of a pre-trained deep classifier. We show that learning the density of in-distribution (ID) features with an energy-based models (EBM) leads to competitive detection results. However, we found that the non-mixing of MCMC sampling during the EBM's training undermines its detection performance. To overcome this an energy-based correction of a mixture of class-conditional Gaussian distributions. We obtains favorable results when compared to a strong baseline like the KNN detector on the CIFAR-10/CIFAR-100 OOD detection benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;SculptDiff&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;3D&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#25805;&#20316;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.10401</link><description>&lt;p&gt;
SculptDiff: &#20174;&#20154;&#31867;&#23398;&#20064;&#30446;&#26631;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#30340;&#26426;&#22120;&#20154;&#31896;&#22303;&#38613;&#22609;
&lt;/p&gt;
&lt;p&gt;
SculptDiff: Learning Robotic Clay Sculpting from Humans with Goal Conditioned Diffusion Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10401
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;SculptDiff&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;3D&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#25805;&#20316;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31896;&#22303;&#38613;&#22609;&#25919;&#31574;&#65292;&#21253;&#25324;&#28857;&#20113;&#29366;&#24577;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;SculptDiff&#65292;&#29992;&#20110;&#30452;&#25509;&#23398;&#20064;&#21508;&#31181;&#30446;&#26631;&#24418;&#29366;&#30340;&#25919;&#31574;&#12290;&#22312;&#25105;&#20204;&#30340;&#30693;&#35782;&#33539;&#22260;&#20869;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#25104;&#21151;&#23398;&#20064;3D&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#31574;&#30053;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;&#27442;&#35266;&#30475;&#38613;&#22609;&#35270;&#39057;&#12289;&#35775;&#38382;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#30828;&#20214;CAD&#27169;&#22411;&#65292;&#35831;&#21442;&#38405;&#39033;&#30446;&#32593;&#31449;&#65306;https://sites.google.com/andrew.cmu.edu/imitation-sculpting/home
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10401v1 Announce Type: cross  Abstract: Manipulating deformable objects remains a challenge within robotics due to the difficulties of state estimation, long-horizon planning, and predicting how the object will deform given an interaction. These challenges are the most pronounced with 3D deformable objects. We propose SculptDiff, a goal-conditioned diffusion-based imitation learning framework that works with point cloud state observations to directly learn clay sculpting policies for a variety of target shapes. To the best of our knowledge this is the first real-world method that successfully learns manipulation policies for 3D deformable objects. For sculpting videos and access to our dataset and hardware CAD models, see the project website: https://sites.google.com/andrew.cmu.edu/imitation-sculpting/home
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BirdSet&#22522;&#20934;&#65292;&#29992;&#20110;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#25972;&#21512;&#24320;&#28304;&#40479;&#31867;&#24405;&#38899;&#25968;&#25454;&#38598;&#21512;&#65292;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21644;&#35782;&#21035;&#28508;&#22312;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.10380</link><description>&lt;p&gt;
BirdSet&#65306;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#20998;&#31867;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10380
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BirdSet&#22522;&#20934;&#65292;&#29992;&#20110;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#25972;&#21512;&#24320;&#28304;&#40479;&#31867;&#24405;&#38899;&#25968;&#25454;&#38598;&#21512;&#65292;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#21644;&#35782;&#21035;&#28508;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#40479;&#31867;&#29983;&#29289;&#22768;&#23398;&#39046;&#22495;&#35786;&#26029;&#29615;&#22659;&#20581;&#24247;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#32473;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BirdSet&#22522;&#20934;&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32508;&#21512;&#30740;&#31350;&#21162;&#21147;&#65292;&#20197;&#20840;&#38754;&#20998;&#31867;&#40479;&#31867;&#40483;&#21483;&#22768;&#12290;BirdSet&#23558;&#24320;&#28304;&#40479;&#31867;&#24405;&#38899;&#25972;&#21512;&#21040;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#21512;&#20013;&#65292;&#25552;&#20379;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#35782;&#21035;&#36328;&#19981;&#21516;&#30740;&#31350;&#30340;&#28508;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10380v1 Announce Type: cross  Abstract: Deep learning (DL) models have emerged as a powerful tool in avian bioacoustics to diagnose environmental health and biodiversity. However, inconsistencies in research pose notable challenges hindering progress in this domain. Reliable DL models need to analyze bird calls flexibly across various species and environments to fully harness the potential of bioacoustics in a cost-effective passive acoustic monitoring scenario. Data fragmentation and opacity across studies complicate a comprehensive evaluation of general model performance. To overcome these challenges, we present the BirdSet benchmark, a unified framework consolidating research efforts with a holistic approach for classifying bird vocalizations in avian bioacoustics. BirdSet harmonizes open-source bird recordings into a curated dataset collection. This unified approach provides an in-depth understanding of model performance and identifies potential shortcomings across diffe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#30340;&#33410;&#33021;&#38598;&#25104;&#26041;&#27861;ENAMLE&#65292;&#26088;&#22312;&#35299;&#20915;SECOE&#30340;&#33021;&#28304;&#29942;&#39048;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.10371</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#30340;&#33410;&#33021;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Energy-Efficient Ensemble Approach for Mitigating Data Incompleteness in IoT Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10371
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#30340;&#33410;&#33021;&#38598;&#25104;&#26041;&#27861;ENAMLE&#65292;&#26088;&#22312;&#35299;&#20915;SECOE&#30340;&#33021;&#28304;&#29942;&#39048;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#30340;&#21160;&#24577;&#24615;&#21644;&#20020;&#26102;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#29420;&#29305;&#25361;&#25112;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#65292;&#21363;&#32570;&#22833;&#30340;&#20256;&#24863;&#22120;&#35835;&#25968;&#12290;&#35768;&#22810;&#22240;&#32032;&#65292;&#21253;&#25324;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;/&#25110;&#32593;&#32476;&#20013;&#26029;&#65292;&#37117;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29289;&#32852;&#32593;&#31995;&#32479;&#21463;&#21040;&#20005;&#37325;&#30340;&#30005;&#21147;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#26500;&#24314;&#38024;&#23545;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#40065;&#26834;&#19988;&#33410;&#33021;&#30340;&#29289;&#32852;&#32593;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#26412;&#25991;&#23545;SECOE&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;-&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#29289;&#32852;&#32593;&#20013;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20851;&#27880;&#20854;&#33021;&#28304;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;SECOE&#30340;&#33021;&#28304;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ENAMLE-&#19968;&#31181;&#20027;&#21160;&#30340;&#12289;&#33021;&#28304;&#24863;&#30693;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20943;&#36731;&#21516;&#26102;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;ENAMLE&#22312;&#36825;&#26679;&#30340;&#24847;&#20041;&#19978;&#26159;&#29420;&#29305;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10371v1 Announce Type: cross  Abstract: Machine Learning (ML) is becoming increasingly important for IoT-based applications. However, the dynamic and ad-hoc nature of many IoT ecosystems poses unique challenges to the efficacy of ML algorithms. One such challenge is data incompleteness, which is manifested as missing sensor readings. Many factors, including sensor failures and/or network disruption, can cause data incompleteness. Furthermore, most IoT systems are severely power-constrained. It is important that we build IoT-based ML systems that are robust against data incompleteness while simultaneously being energy efficient. This paper presents an empirical study of SECOE - a recent technique for alleviating data incompleteness in IoT - with respect to its energy bottlenecks. Towards addressing the energy bottlenecks of SECOE, we propose ENAMLE - a proactive, energy-aware technique for mitigating the impact of concurrent missing data. ENAMLE is unique in the sense that it
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20010;&#20154;&#20559;&#22909;&#31283;&#23450;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#33719;&#24471;&#20102; $O(\log n)$-IP &#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.10365</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20010;&#20154;&#20559;&#22909;&#31283;&#23450;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Algorithms for Individual Preference Stable Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10365
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20010;&#20154;&#20559;&#22909;&#31283;&#23450;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#33719;&#24471;&#20102; $O(\log n)$-IP &#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20010;&#20154;&#20559;&#22909;&#65288;IP&#65289;&#31283;&#23450;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#25429;&#25417;&#32858;&#31867;&#20013;&#20010;&#20154;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#36825;&#20010;&#35774;&#23450;&#20013;&#65292;&#24403;&#27599;&#20010;&#25968;&#25454;&#28857;&#21040;&#20854;&#31751;&#30340;&#24179;&#22343;&#36317;&#31163;&#19981;&#36229;&#36807;&#20854;&#21040;&#20219;&#20309;&#20854;&#20182;&#31751;&#30340;&#24179;&#22343;&#36317;&#31163;&#30340; $\alpha$ &#20493;&#26102;&#65292;&#19968;&#20010;&#32858;&#31867;&#26159; $\alpha$-IP &#31283;&#23450;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110; IP &#31283;&#23450;&#32858;&#31867;&#30340;&#33258;&#28982;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#23454;&#20102;&#27492;&#31639;&#27861;&#30340; $O(\log n)$-IP &#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#20854;&#20013; $n$ &#34920;&#31034;&#36755;&#20837;&#20013;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25913;&#36827;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#34920;&#26126;&#20854;&#36816;&#34892;&#26102;&#38388;&#20960;&#20046;&#26159;&#32447;&#24615;&#30340;&#65292;&#20026; $\tilde{O}(nk)$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10365v1 Announce Type: cross  Abstract: In this paper, we study the individual preference (IP) stability, which is an notion capturing individual fairness and stability in clustering. Within this setting, a clustering is $\alpha$-IP stable when each data point's average distance to its cluster is no more than $\alpha$ times its average distance to any other cluster. In this paper, we study the natural local search algorithm for IP stable clustering. Our analysis confirms a $O(\log n)$-IP stability guarantee for this algorithm, where $n$ denotes the number of points in the input. Furthermore, by refining the local search approach, we show it runs in an almost linear time, $\tilde{O}(nk)$.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;CBoTT&#26694;&#26550;&#29992;&#20110;&#26080;&#30417;&#30563;&#23041;&#32961;&#29454;&#26432;&#22312;SIEM&#26085;&#24535;&#20013;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#24322;&#24120;&#27963;&#21160;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#32593;&#32476;&#23433;&#20840;&#20998;&#26512;&#21592;&#36827;&#34892;&#23041;&#32961;&#29454;&#26432;&#12290;</title><link>https://arxiv.org/abs/2403.10327</link><description>&lt;p&gt;
&#20351;&#29992;&#36830;&#32493;&#35789;&#39033;&#19982;&#26102;&#38388;&#65288;CBoTT&#65289;&#36827;&#34892;&#26080;&#30417;&#30563;&#23041;&#32961;&#29454;&#26432;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Threat Hunting using Continuous Bag-of-Terms-and-Time (CBoTT)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10327
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;CBoTT&#26694;&#26550;&#29992;&#20110;&#26080;&#30417;&#30563;&#23041;&#32961;&#29454;&#26432;&#22312;SIEM&#26085;&#24535;&#20013;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#24322;&#24120;&#27963;&#21160;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#32593;&#32476;&#23433;&#20840;&#20998;&#26512;&#21592;&#36827;&#34892;&#23041;&#32961;&#29454;&#26432;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23041;&#32961;&#29454;&#26432;&#26159;&#25351;&#36890;&#36807;&#31579;&#26597;&#31995;&#32479;&#26085;&#24535;&#26469;&#26816;&#27979;&#21487;&#33021;&#32469;&#36807;&#29616;&#26377;&#23433;&#20840;&#25514;&#26045;&#30340;&#24694;&#24847;&#27963;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#31216;&#20026;&#36830;&#32493;&#35789;&#39033;&#19982;&#26102;&#38388;&#65288;CBoTT&#65289;&#65292;&#24182;&#21457;&#24067;&#20854;&#24212;&#29992;&#31243;&#24207;&#25509;&#21475;&#65288;API&#65289;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#32593;&#32476;&#23433;&#20840;&#20998;&#26512;&#21592;&#22312;&#38754;&#21521;&#32456;&#31471;&#35774;&#22791;&#19978;&#30340;&#36827;&#31243;&#23457;&#35745;&#30340;SIEM&#26085;&#24535;&#20013;&#36827;&#34892;&#22522;&#20110;&#24322;&#24120;&#30340;&#23041;&#32961;&#29454;&#26432;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10327v1 Announce Type: cross  Abstract: Threat hunting is sifting through system logs to detect malicious activities that might have bypassed existing security measures. It can be performed in several ways, one of which is based on detecting anomalies. We propose an unsupervised framework, called continuous bag-of-terms-and-time (CBoTT), and publish its application programming interface (API) to help researchers and cybersecurity analysts perform anomaly-based threat hunting among SIEM logs geared toward process auditing on endpoint devices. Analyses show that our framework consistently outperforms benchmark approaches. When logs are sorted by likelihood of being an anomaly (from most likely to least), our approach identifies anomalies at higher percentiles (between 1.82-6.46) while benchmark approaches identify the same anomalies at lower percentiles (between 3.25-80.92). This framework can be used by other researchers to conduct benchmark analyses and cybersecurity analyst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24212;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20505;&#36873;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#22635;&#31354;&#24178;&#25200;&#39033;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;PLM&#22686;&#24378;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10326</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#22635;&#31354;&#24178;&#25200;&#39033;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24212;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20505;&#36873;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#22635;&#31354;&#24178;&#25200;&#39033;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;PLM&#22686;&#24378;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#35774;&#35745;&#22635;&#31354;&#27979;&#35797;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#38169;&#35823;&#36873;&#39033;&#65288;&#24178;&#25200;&#39033;&#65289;&#30340;&#36873;&#25321;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#24178;&#25200;&#39033;&#25552;&#39640;&#20102;&#23398;&#20064;&#32773;&#33021;&#21147;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#29983;&#25104;&#22635;&#31354;&#24178;&#25200;&#39033;&#30340;&#24819;&#27861;&#24212;&#36816;&#32780;&#29983;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24212;&#29992;&#20316;&#20026;&#20505;&#36873;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#30740;&#31350;&#22635;&#31354;&#24178;&#25200;&#39033;&#29983;&#25104;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PLM&#22686;&#24378;&#27169;&#22411;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#23558;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#20174;14.94&#25552;&#21319;&#33267;34.17&#65288;NDCG@10&#20998;&#25968;&#65289;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/AndyChiangSH/CDGP &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10326v1 Announce Type: cross  Abstract: Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is motivated. In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation. Experiments show that the PLM-enhanced model brings a substantial performance improvement. Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our code and dataset is available at https://github.com/AndyChiangSH/CDGP.
&lt;/p&gt;</description></item><item><title>KIF&#26694;&#26550;&#21033;&#29992;Wikidata&#20316;&#20026;&#36890;&#29992;&#35821;&#35328;&#65292;&#32467;&#21512;&#29992;&#25143;&#23450;&#20041;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#24322;&#26500;&#30693;&#35782;&#24211;&#30340;&#34394;&#25311;&#38598;&#25104;&#65292;&#24418;&#25104;&#31867;&#20284;&#20110;&#25193;&#23637;Wikidata&#30340;&#34394;&#25311;&#30693;&#35782;&#24211;&#65292;&#21487;&#36890;&#36807;&#36807;&#28388;&#25509;&#21475;&#25110;SPARQL&#36827;&#34892;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2403.10304</link><description>&lt;p&gt;
KIF&#65306;&#20351;&#29992;Wikidata&#36827;&#34892;&#24322;&#26500;&#30693;&#35782;&#24211;&#34394;&#25311;&#38598;&#25104;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIF: A Framework for Virtual Integration of Heterogeneous Knowledge Bases using Wikidata
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10304
&lt;/p&gt;
&lt;p&gt;
KIF&#26694;&#26550;&#21033;&#29992;Wikidata&#20316;&#20026;&#36890;&#29992;&#35821;&#35328;&#65292;&#32467;&#21512;&#29992;&#25143;&#23450;&#20041;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#24322;&#26500;&#30693;&#35782;&#24211;&#30340;&#34394;&#25311;&#38598;&#25104;&#65292;&#24418;&#25104;&#31867;&#20284;&#20110;&#25193;&#23637;Wikidata&#30340;&#34394;&#25311;&#30693;&#35782;&#24211;&#65292;&#21487;&#36890;&#36807;&#36807;&#28388;&#25509;&#21475;&#25110;SPARQL&#36827;&#34892;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#38598;&#25104;&#26694;&#26550;&#65288;&#31216;&#20026;KIF&#65289;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;Wikidata&#20316;&#20026;&#36890;&#29992;&#35821;&#35328;&#26469;&#38598;&#25104;&#24322;&#26500;&#30693;&#35782;&#24211;&#12290;&#36825;&#20123;&#30693;&#35782;&#24211;&#21487;&#20197;&#26159;&#19977;&#20803;&#32452;&#23384;&#20648;&#12289;&#20851;&#31995;&#22411;&#25968;&#25454;&#24211;&#12289;CSV&#25991;&#20214;&#31561;&#65292;&#21487;&#20197;&#25110;&#19981;&#21487;&#20197;&#20351;&#29992;RDF&#30340;Wikidata&#26041;&#35328;&#12290;KIF&#21033;&#29992;Wikidata&#30340;&#25968;&#25454;&#27169;&#22411;&#21644;&#35789;&#27719;&#20197;&#21450;&#29992;&#25143;&#23450;&#20041;&#30340;&#26144;&#23556;&#26469;&#23637;&#31034;&#38598;&#25104;&#24211;&#30340;&#32479;&#19968;&#35270;&#22270;&#65292;&#21516;&#26102;&#36319;&#36394;&#20854;&#38472;&#36848;&#30340;&#19978;&#19979;&#25991;&#21644;&#20986;&#22788;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#34892;&#20026;&#31867;&#20284;&#20110;&#8220;&#25193;&#23637;Wikidata&#8221;&#30340;&#34394;&#25311;&#30693;&#35782;&#24211;&#65292;&#21487;&#20197;&#36890;&#36807;&#39640;&#25928;&#36807;&#28388;&#25509;&#21475;&#25110;&#20351;&#29992;SPARQL&#36827;&#34892;&#26597;&#35810;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;KIF&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#35752;&#35770;&#20102;&#25105;&#20204;&#22914;&#20309;&#22312;&#21270;&#23398;&#39046;&#22495;&#65288;&#28041;&#21450;Wikidata&#12289;PubChem&#21644;IBM CIRCA&#65289;&#20013;&#20351;&#29992;&#23427;&#35299;&#20915;&#23454;&#38469;&#38598;&#25104;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;KIF&#30340;&#24615;&#33021;&#21644;&#24320;&#38144;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10304v1 Announce Type: new  Abstract: We present a knowledge integration framework (called KIF) that uses Wikidata as a lingua franca to integrate heterogeneous knowledge bases. These can be triplestores, relational databases, CSV files, etc., which may or may not use the Wikidata dialect of RDF. KIF leverages Wikidata's data model and vocabulary plus user-defined mappings to expose a unified view of the integrated bases while keeping track of the context and provenance of their statements. The result is a virtual knowledge base which behaves like an "extended Wikidata" and which can be queried either through an efficient filter interface or using SPARQL. We present the design and implementation of KIF, discuss how we have used it to solve a real integration problem in the domain of chemistry (involving Wikidata, PubChem, and IBM CIRCA), and present experimental results on the performance and overhead of KIF.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#22810;&#32422;&#26463;&#22810;&#30446;&#26631;&#20998;&#37197;&#27169;&#22411;&#22312;&#32039;&#24613;&#25937;&#25588;&#22330;&#26223;&#20013;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.10299</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#32039;&#24613;&#25937;&#25588;&#30340;&#22810;&#32422;&#26463;&#22810;&#30446;&#26631;&#20998;&#37197;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Multi-constraint and Multi-objective Allocation Model for Emergency Rescue in IoT Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#22810;&#32422;&#26463;&#22810;&#30446;&#26631;&#20998;&#37197;&#27169;&#22411;&#22312;&#32039;&#24613;&#25937;&#25588;&#22330;&#26223;&#20013;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#24613;&#25937;&#25588;&#34892;&#21160;&#22312;&#28798;&#21518;&#33267;&#20851;&#37325;&#35201;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#36164;&#28304;&#20998;&#37197;&#20197;&#26368;&#23567;&#21270;&#36127;&#38754;&#24433;&#21709;&#24182;&#26368;&#22823;&#21270;&#25910;&#30410;&#12290;&#22312;&#38271;&#26102;&#38388;&#21361;&#26426;&#25110;&#22823;&#35268;&#27169;&#28798;&#23475;&#20013;&#65292;&#31995;&#32479;&#21270;&#30340;&#22810;&#21608;&#26399;&#26041;&#27861;&#23545;&#20110;&#21450;&#26102;&#21644;&#26126;&#26234;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20511;&#21161;&#29289;&#32852;&#32593;&#21644;&#26102;&#31354;&#25968;&#25454;&#20998;&#26512;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#30446;&#26631;&#27927;&#29260;&#28784;&#29436;&#38738;&#34521;&#36339;&#27169;&#22411;&#65288;MSGW-FLM&#65289;&#12290;&#36825;&#20010;&#22810;&#32422;&#26463;&#12289;&#22810;&#30446;&#26631;&#30340;&#36164;&#28304;&#20998;&#37197;&#27169;&#22411;&#24050;&#32463;&#32463;&#36807;&#20005;&#26684;&#27979;&#35797;&#65292;&#19982;NSGA-II&#12289;IBEA&#21644;MOEA/D&#31561;&#24050;&#24314;&#31435;&#30340;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;MSGW-FLM&#30340;&#26377;&#25928;&#24615;&#22312;&#22797;&#26434;&#30340;&#22810;&#21608;&#26399;&#32039;&#24613;&#25937;&#25588;&#22330;&#26223;&#20013;&#23588;&#20026;&#24341;&#20154;&#27880;&#30446;&#65292;&#36825;&#20123;&#22330;&#26223;&#28041;&#21450;&#20247;&#22810;&#32422;&#26463;&#21644;&#30446;&#26631;&#12290;&#36825;&#20010;&#27169;&#22411;&#22312;&#20248;&#21270;&#24212;&#24613;&#21709;&#24212;&#24773;&#22659;&#20013;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10299v1 Announce Type: new  Abstract: Emergency relief operations are essential in disaster aftermaths, necessitating effective resource allocation to minimize negative impacts and maximize benefits. In prolonged crises or extensive disasters, a systematic, multi-cycle approach is key for timely and informed decision-making. Leveraging advancements in IoT and spatio-temporal data analytics, we've developed the Multi-Objective Shuffled Gray-Wolf Frog Leaping Model (MSGW-FLM). This multi-constraint, multi-objective resource allocation model has been rigorously tested against 28 diverse challenges, showing superior performance in comparison to established models such as NSGA-II, IBEA, and MOEA/D. MSGW-FLM's effectiveness is particularly notable in complex, multi-cycle emergency rescue scenarios, which involve numerous constraints and objectives. This model represents a significant step forward in optimizing resource distribution in emergency response situations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31895;&#31961;Transformer&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#30340;&#36755;&#20837;&#24207;&#21015;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#23545;&#20110;&#22788;&#29702;&#21307;&#30103;&#24773;&#22659;&#20013;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.10288</link><description>&lt;p&gt;
&#29992;&#20110;&#36830;&#32493;&#21644;&#39640;&#25928;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#31895;&#31961;Transformer
&lt;/p&gt;
&lt;p&gt;
Rough Transformers for Continuous and Efficient Time-Series Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10288
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31895;&#31961;Transformer&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#30340;&#36755;&#20837;&#24207;&#21015;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#23545;&#20110;&#22788;&#29702;&#21307;&#30103;&#24773;&#22659;&#20013;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#30103;&#29615;&#22659;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#34920;&#29616;&#20986;&#38271;&#31243;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#20197;&#19981;&#22343;&#21248;&#38388;&#38548;&#35266;&#23519;&#21040;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#24207;&#21015;&#30340;&#24490;&#29615;&#27169;&#22411;&#24456;&#38590;&#22788;&#29702;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#29992;&#22522;&#20110;&#31070;&#32463;ODE&#30340;&#27169;&#22411;&#26367;&#25442;&#24490;&#29615;&#26550;&#26500;&#26469;&#24314;&#27169;&#38750;&#22343;&#21248;&#37319;&#26679;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;Transformer&#26550;&#26500;&#26469;&#32771;&#34385;&#38271;&#31243;&#20381;&#36182;&#12290;&#23613;&#31649;&#36825;&#20004;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#20013;&#31561;&#38271;&#24230;&#21450;&#26356;&#38271;&#36755;&#20837;&#24207;&#21015;&#65292;&#20004;&#32773;&#37117;&#38656;&#35201;&#38750;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31895;&#31961;Transformer&#65292;&#36825;&#26159;Transformer&#27169;&#22411;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20854;&#22312;&#36755;&#20837;&#24207;&#21015;&#30340;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#19978;&#36816;&#34892;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#23545;&#20110;&#22788;&#29702;&#21307;&#30103;&#24773;&#22659;&#20013;&#24120;&#35265;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;&#31614;&#21517;&#27880;&#24847;&#21147;&#65292;&#21033;&#29992;&#36335;&#24452;&#31614;&#21517;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10288v1 Announce Type: cross  Abstract: Time-series data in real-world medical settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In such contexts, traditional sequence-based recurrent models struggle. To overcome this, researchers replace recurrent architectures with Neural ODE-based models to model irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of moderate lengths and greater. To mitigate this, we introduce the Rough Transformer, a variation of the Transformer model which operates on continuous-time representations of input sequences and incurs significantly reduced computational costs, critical for addressing long-range dependencies common in medical contexts. In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attent
&lt;/p&gt;</description></item><item><title>Team Trifecta&#22312;Factify 5WQA&#19978;&#20197;Fine-Tuning&#21462;&#24471;&#20102;&#39318;&#35201;&#22320;&#20301;&#65292;&#25104;&#21151;&#36229;&#36234;&#22522;&#20934;&#20934;&#30830;&#29575;103&#65285;&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#31532;&#20108;&#21517;&#31454;&#20105;&#32773;&#30340;70%&#39046;&#20808;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.10281</link><description>&lt;p&gt;
Team Trifecta&#22312;Factify 5WQA&#19978;&#35774;&#23450;&#20102;&#32454;&#21270;&#35843;&#25972;&#20013;&#20107;&#23454;&#39564;&#35777;&#30340;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10281
&lt;/p&gt;
&lt;p&gt;
Team Trifecta&#22312;Factify 5WQA&#19978;&#20197;Fine-Tuning&#21462;&#24471;&#20102;&#39318;&#35201;&#22320;&#20301;&#65292;&#25104;&#21151;&#36229;&#36234;&#22522;&#20934;&#20934;&#30830;&#29575;103&#65285;&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#31532;&#20108;&#21517;&#31454;&#20105;&#32773;&#30340;70%&#39046;&#20808;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Pre-CoFactv3&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#38382;&#31572;&#21644;&#25991;&#26412;&#20998;&#31867;&#32452;&#20214;&#32452;&#25104;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#29992;&#20110;&#20107;&#23454;&#39564;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;FakeNet&#27169;&#22411;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20107;&#23454;&#39564;&#35777;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;LLMs&#65292;&#24341;&#20837;&#20102;FakeNet&#65292;&#24182;&#23454;&#26045;&#20102;&#21508;&#31181;&#38598;&#25104;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;Trifecta&#22312;AAAI-24 Factify 3.0&#30740;&#35752;&#20250;&#19978;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#27604;&#22522;&#20934;&#20934;&#30830;&#29575;&#39640;&#20986;103%&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#31532;&#20108;&#21517;&#31454;&#20105;&#23545;&#25163;&#30340;70%&#39046;&#20808;&#20248;&#21183;&#12290;&#36825;&#19968;&#25104;&#21151;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#23545;&#25512;&#36827;&#20107;&#23454;&#39564;&#35777;&#30740;&#31350;&#30340;&#28508;&#22312;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10281v1 Announce Type: cross  Abstract: In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#25361;&#25112;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31616;&#21333;&#32780;&#20016;&#23500;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#27169;&#22411;&#22312;&#20449;&#21495;&#20256;&#36882;&#26041;&#38754;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.10275</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#21644;&#22522;&#20110;&#35789;&#32423;&#21333;&#21464;&#37327;&#19968;&#38454;&#27010;&#29575;&#20551;&#35774;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#25361;&#25112;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31616;&#21333;&#32780;&#20016;&#23500;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#27169;&#22411;&#22312;&#20449;&#21495;&#20256;&#36882;&#26041;&#38754;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#23545;&#20854;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#38543;&#26426;&#24615;&#24456;&#25935;&#24863;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#36825;&#31181;&#25935;&#24863;&#24615;&#36827;&#34892;&#34920;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#31616;&#21333;&#21644;&#20449;&#24687;&#20016;&#23500;&#35299;&#37322;&#30340;&#34920;&#24449;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20026;&#35299;&#37322;&#30340;&#20449;&#21495;&#12289;&#22122;&#22768;&#21644;&#20449;&#22122;&#27604;&#32473;&#20986;&#20102;&#32479;&#35745;&#23450;&#20041;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#22312;&#19968;&#20010;&#20856;&#22411;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#19968;&#38454;&#32479;&#35745;&#24037;&#20855;&#20998;&#26512;&#22522;&#20110;&#21333;&#19968;&#29305;&#24449;&#30340;&#27169;&#22411;&#35299;&#37322;&#26102;&#65292;&#31616;&#21333;&#29305;&#24449;&#27169;&#22411;&#30340;&#35299;&#37322;&#20256;&#36882;&#26356;&#22810;&#20449;&#21495;&#24182;&#19988;&#22122;&#22768;&#26356;&#23569;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36890;&#36807;&#26367;&#20195;&#20449;&#21495;&#21644;&#22122;&#22768;&#30340;&#23450;&#20041;&#26469;&#25913;&#36827;&#36825;&#20123;&#32467;&#26524;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#26356;&#22797;&#26434;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#36136;&#30097;&#20102;&#19982;&#35835;&#32773;&#21487;&#20449;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10275v1 Announce Type: cross  Abstract: The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity. In this paper, we propose a characterization that questions the possibility to provide simple and informative explanations for such models. To this end, we give statistical definitions for the explanations' signal, noise and signal-to-noise ratio. We highlight that, in a typical case study where word-level univariate explanations are analyzed with first-order statistical tools, the explanations of simple feature-based models carry more signal and less noise than those of transformer ones. We then discuss the possibility to improve these results with alternative definitions of signal and noise that would capture more complex explanations and analysis methods, while also questioning the tradeoff with their plausibility for readers.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;&#20998;&#31867;&#27169;&#22411;&#21644;LSTM&#27169;&#22411;&#22312;&#24037;&#19994;&#20013;&#36827;&#34892;&#39044;&#27979;&#24615;&#32500;&#25252;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23454;&#29616;&#23545;&#26426;&#22120;&#25925;&#38556;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.10259</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#31867;&#27169;&#22411;&#21644;LSTM&#27169;&#22411;&#22312;&#24037;&#19994;&#20013;&#36827;&#34892;&#39044;&#27979;&#24615;&#32500;&#25252;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Study Of Predictive Maintenance In Industries Using Classification Models And LSTM Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;&#20998;&#31867;&#27169;&#22411;&#21644;LSTM&#27169;&#22411;&#22312;&#24037;&#19994;&#20013;&#36827;&#34892;&#39044;&#27979;&#24615;&#32500;&#25252;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23454;&#29616;&#23545;&#26426;&#22120;&#25925;&#38556;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25216;&#26415;&#39537;&#21160;&#30340;&#26102;&#20195;&#65292;&#23545;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#20808;&#36827;&#35786;&#26029;&#30340;&#36843;&#20999;&#38656;&#27714;&#19981;&#20165;&#20165;&#38480;&#20110;&#33322;&#31354;&#39046;&#22495;&#65292;&#36824;&#21253;&#25324;&#23545;&#26059;&#36716;&#21644;&#31227;&#21160;&#26426;&#22120;&#20013;&#25439;&#22351;&#12289;&#25925;&#38556;&#21644;&#25805;&#20316;&#32570;&#38519;&#30340;&#35782;&#21035;&#12290;&#23454;&#26045;&#36825;&#26679;&#30340;&#26381;&#21153;&#19981;&#20165;&#21487;&#20197;&#32553;&#20943;&#32500;&#25252;&#25104;&#26412;&#65292;&#36824;&#21487;&#20197;&#24310;&#38271;&#26426;&#22120;&#23551;&#21629;&#65292;&#30830;&#20445;&#21331;&#36234;&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36825;&#20063;&#26159;&#19968;&#31181;&#38450;&#33539;&#28508;&#22312;&#20107;&#25925;&#25110;&#28798;&#38590;&#24615;&#20107;&#20214;&#30340;&#39044;&#38450;&#25514;&#26045;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#21508;&#34892;&#19994;&#30340;&#32500;&#25252;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#22120;&#25925;&#38556;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#33410;&#32422;&#20102;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#25216;&#26415;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#36923;&#36753;&#22238;&#24402;&#20197;&#21450;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;LSTM&#30340;&#26426;&#22120;&#24615;&#33021;&#39044;&#27979;&#19982;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10259v1 Announce Type: cross  Abstract: In today's technology-driven era, the imperative for predictive maintenance and advanced diagnostics extends beyond aviation to encompass the identification of damages, failures, and operational defects in rotating and moving machines. Implementing such services not only curtails maintenance costs but also extends machine lifespan, ensuring heightened operational efficiency. Moreover, it serves as a preventive measure against potential accidents or catastrophic events. The advent of Artificial Intelligence (AI) has revolutionized maintenance across industries, enabling more accurate and efficient prediction and analysis of machine failures, thereby conserving time and resources. Our proposed study aims to delve into various machine learning classification techniques, including Support Vector Machine (SVM), Random Forest, Logistic Regression, and Convolutional Neural Network LSTM-Based, for predicting and analyzing machine performance. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#27169;&#22411;&#22312;&#22797;&#26434;&#28216;&#25103;&#22330;&#26223;&#20013;&#30340;&#20351;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;LM&#30340;&#28216;&#25103;&#20195;&#29702;&#30340;&#29616;&#26377;&#26550;&#26500;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.10249</link><description>&lt;p&gt;
&#23545;&#28216;&#25103;&#26234;&#33021;&#20195;&#29702;&#19982;&#22823;&#22411;&#27169;&#22411;&#30340;&#35843;&#26597;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#27169;&#22411;&#22312;&#22797;&#26434;&#28216;&#25103;&#22330;&#26223;&#20013;&#30340;&#20351;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;LM&#30340;&#28216;&#25103;&#20195;&#29702;&#30340;&#29616;&#26377;&#26550;&#26500;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#19981;&#35770;&#26159;&#20197;&#35821;&#35328;&#20026;&#37325;&#28857;&#36824;&#26159;&#22810;&#27169;&#24577;&#65292;&#36805;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#24341;&#21457;&#20102;&#26497;&#22823;&#20852;&#36259;&#65292;&#20294;&#23545;&#20854;&#22312;&#19981;&#21516;&#26377;&#37325;&#22823;&#24433;&#21709;&#30340;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#21644;&#28508;&#21147;&#20173;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#12290;&#26412;&#25991;&#21147;&#22270;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#23545;LM&#22312;&#22797;&#26434;&#28216;&#25103;&#22330;&#26223;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#26816;&#26597;&#24182;&#25506;&#35752;&#20102;&#20381;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35797;&#22270;&#31995;&#32479;&#22320;&#23457;&#26597;&#22522;&#20110;LM&#30340;&#28216;&#25103;&#20195;&#29702;&#65288;LMAs&#65289;&#30340;&#29616;&#26377;&#26550;&#26500;&#65292;&#24182;&#24635;&#32467;&#23427;&#20204;&#30340;&#20849;&#21516;&#20043;&#22788;&#12289;&#25361;&#25112;&#21644;&#20219;&#20309;&#20854;&#20182;&#27934;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;LM&#22312;&#28216;&#25103;&#20013;&#36827;&#23637;&#30340;&#26377;&#21069;&#26223;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#26356;&#28165;&#26224;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#65292;&#24182;&#28608;&#21457;&#23545;&#36825;&#19968;&#39640;&#24230;&#24433;&#21709;&#21147;&#30740;&#31350;&#26041;&#21521;&#30340;&#26356;&#22810;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10249v1 Announce Type: new  Abstract: The swift evolution of Large-scale Models (LMs), either language-focused or multi-modal, has garnered extensive attention in both academy and industry. But despite the surge in interest in this rapidly evolving area, there are scarce systematic reviews on their capabilities and potential in distinct impactful scenarios. This paper endeavours to help bridge this gap, offering a thorough examination of the current landscape of LM usage in regards to complex game playing scenarios and the challenges still open. Here, we seek to systematically review the existing architectures of LM-based Agents (LMAs) for games and summarize their commonalities, challenges, and any other insights. Furthermore, we present our perspective on promising future research avenues for the advancement of LMs in games. We hope to assist researchers in gaining a clear understanding of the field and to generate more interest in this highly impactful research direction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39640;&#25928;&#21644;&#33258;&#36866;&#24212;&#39044;&#27979;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#36807;&#31243;&#20998;&#35299;&#20026;&#20174;&#26597;&#35810;&#20013;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#24182;&#22312;&#35813;&#21333;&#20010;&#12289;&#26597;&#35810;&#30456;&#20851;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#30340;&#20004;&#20010;&#27493;&#39588;&#65292;&#21033;&#29992;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10231</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39640;&#25928;&#21644;&#33258;&#36866;&#24212;&#39044;&#27979;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#36807;&#31243;&#20998;&#35299;&#20026;&#20174;&#26597;&#35810;&#20013;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#24182;&#22312;&#35813;&#21333;&#20010;&#12289;&#26597;&#35810;&#30456;&#20851;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#30340;&#20004;&#20010;&#27493;&#39588;&#65292;&#21033;&#29992;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#25512;&#23548;&#26032;&#30340;&#20107;&#23454;&#65292;&#38142;&#25509;&#39044;&#27979;&#22120;&#20174;&#22270;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#25910;&#38598;&#23616;&#37096;&#35777;&#25454;&#20197;&#25214;&#21040;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#21033;&#29992;&#25972;&#20010;KG&#36827;&#34892;&#39044;&#27979;&#32780;&#23384;&#22312;&#20005;&#37325;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;KG&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#36890;&#36807;&#24120;&#35268;&#25277;&#26679;&#26041;&#27861;&#35299;&#20915;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#12290; &#35774;&#35745;&#21407;&#21017;&#26159;&#65292;&#39044;&#27979;&#36807;&#31243;&#19981;&#30452;&#25509;&#20316;&#29992;&#20110;&#25972;&#20010;KG&#65292;&#32780;&#26159;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#21363;&#65288;i&#65289;&#26681;&#25454;&#26597;&#35810;&#20165;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#21644;&#65288;ii&#65289;&#22312;&#36825;&#20010;&#21333;&#19968;&#30340;&#12289;&#26597;&#35810;&#30456;&#20851;&#30340;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290; &#25105;&#20204;&#21457;&#29616;&#65292;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20010;&#24615;&#21270;PageRank&#65288;PPR&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#28508;&#22312;&#31572;&#26696;&#21644;&#25903;&#25345;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10231v1 Announce Type: cross  Abstract: To deduce new facts on a knowledge graph (KG), a link predictor learns from the graph structure and collects local evidence to find the answer to a given query. However, existing methods suffer from a severe scalability problem due to the utilization of the whole KG for prediction, which hinders their promise on large scale KGs and cannot be directly addressed by vanilla sampling methods. In this work, we propose the one-shot-subgraph link prediction to achieve efficient and adaptive prediction. The design principle is that, instead of directly acting on the whole KG, the prediction procedure is decoupled into two steps, i.e., (i) extracting only one subgraph according to the query and (ii) predicting on this single, query dependent subgraph. We reveal that the non-parametric and computation-efficient heuristics Personalized PageRank (PPR) can effectively identify the potential answers and supporting evidence. With efficient subgraph-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HawkEye&#65292;&#19968;&#20010;&#21487;&#20197;&#20197;&#23436;&#20840;&#25991;&#26412;&#26041;&#24335;&#25191;&#34892;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#30340;&#35270;&#39057;&#25991;&#26412;LLMs&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#35270;&#39057;&#25991;&#26412;&#35821;&#26009;&#24211;InternVid-G&#20197;&#21450;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#38754;&#21521;&#26102;&#38388;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#31895;&#31890;&#24230;&#34920;&#31034;&#35270;&#39057;&#27573;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.10228</link><description>&lt;p&gt;
HawkEye: &#29992;&#20110;&#23558;&#25991;&#26412;&#19982;&#35270;&#39057;&#30456;&#20851;&#32852;&#30340;&#35757;&#32451;&#35270;&#39057;&#25991;&#26412;LLMs
&lt;/p&gt;
&lt;p&gt;
HawkEye: Training Video-Text LLMs for Grounding Text in Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HawkEye&#65292;&#19968;&#20010;&#21487;&#20197;&#20197;&#23436;&#20840;&#25991;&#26412;&#26041;&#24335;&#25191;&#34892;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#30340;&#35270;&#39057;&#25991;&#26412;LLMs&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#35270;&#39057;&#25991;&#26412;&#35821;&#26009;&#24211;InternVid-G&#20197;&#21450;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#38754;&#21521;&#26102;&#38388;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#31895;&#31890;&#24230;&#34920;&#31034;&#35270;&#39057;&#27573;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25991;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;video-text LLMs&#65289;&#22312;&#22238;&#31572;&#38382;&#39064;&#21644;&#36827;&#34892;&#31616;&#21333;&#35270;&#39057;&#23545;&#35805;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#32780;&#22797;&#26434;&#30340;&#35270;&#39057;&#20013;&#65292;&#23427;&#20204;&#22312;&#25991;&#26412;&#26597;&#35810;&#19978;&#30340;&#34920;&#29616;&#20960;&#20046;&#19982;&#38543;&#26426;&#30456;&#21516;&#65292;&#20960;&#20046;&#27809;&#26377;&#33021;&#21147;&#29702;&#35299;&#21644;&#25512;&#29702;&#20851;&#20110;&#26102;&#38388;&#20449;&#24687;&#30340;&#20869;&#23481;&#65292;&#36825;&#26159;&#35270;&#39057;&#21644;&#22270;&#20687;&#20043;&#38388;&#26368;&#22522;&#26412;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HawkEye&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#23436;&#20840;&#20197;&#25991;&#26412;&#26041;&#24335;&#25191;&#34892;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#30340;&#35270;&#39057;&#25991;&#26412;LLMs&#20043;&#19968;&#12290;&#20026;&#20102;&#25910;&#38598;&#36866;&#29992;&#20110;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;InternVid-G&#65292;&#19968;&#20010;&#20855;&#26377;&#20998;&#27573;&#32423;&#26631;&#39064;&#21644;&#36127;&#38388;&#36317;&#30340;&#22823;&#35268;&#27169;&#35270;&#39057;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#35813;&#35821;&#26009;&#24211;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#38754;&#21521;&#26102;&#38388;&#30340;&#35757;&#32451;&#30446;&#26631;&#20197;&#20379;&#35270;&#39057;&#25991;&#26412;LLMs&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#35270;&#39057;&#20013;&#27573;&#30340;&#31895;&#31890;&#24230;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;LLMs&#23398;&#20064;&#21644;&#36981;&#24490;&#30340;&#26041;&#27861;&#26356;&#31283;&#20581;&#19988;&#26356;&#26131;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10228v1 Announce Type: cross  Abstract: Video-text Large Language Models (video-text LLMs) have shown remarkable performance in answering questions and holding conversations on simple videos. However, they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images. In this paper, we propose HawkEye, one of the first video-text LLMs that can perform temporal video grounding in a fully text-to-text manner. To collect training data that is applicable for temporal video grounding, we construct InternVid-G, a large-scale video-text corpus with segment-level captions and negative spans, with which we introduce two new time-aware training objectives to video-text LLMs. We also propose a coarse-grained method of representing segments in videos, which is more robust and easier for LLMs to learn and follow than o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AERO&#65292;&#19968;&#20010;&#19987;&#20026;&#22825;&#25991;&#35266;&#27979;&#20013;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#25797;&#38271;&#22788;&#29702;&#22825;&#25991;&#35266;&#27979;&#20013;&#29420;&#31435;&#20294;&#21463;&#21040;&#38543;&#26426;&#24182;&#21457;&#22122;&#22768;&#24178;&#25200;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.10220</link><description>&lt;p&gt;
&#20174;&#28151;&#27788;&#21040;&#28165;&#26224;&#65306;&#22825;&#25991;&#35266;&#27979;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Chaos to Clarity: Time Series Anomaly Detection in Astronomical Observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AERO&#65292;&#19968;&#20010;&#19987;&#20026;&#22825;&#25991;&#35266;&#27979;&#20013;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#25797;&#38271;&#22788;&#29702;&#22825;&#25991;&#35266;&#27979;&#20013;&#29420;&#31435;&#20294;&#21463;&#21040;&#38543;&#26426;&#24182;&#21457;&#22122;&#22768;&#24178;&#25200;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22825;&#25991;&#35774;&#26045;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#35774;&#26045;&#35266;&#27979;&#21040;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34987;&#25910;&#38598;&#36215;&#26469;&#12290;&#20998;&#26512;&#36825;&#20123;&#22825;&#25991;&#35266;&#27979;&#20013;&#30340;&#24322;&#24120;&#23545;&#20110;&#25581;&#31034;&#28508;&#22312;&#30340;&#22825;&#20307;&#20107;&#20214;&#21644;&#29289;&#29702;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#32780;&#25512;&#21160;&#31185;&#23398;&#30740;&#31350;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#22312;&#22788;&#29702;&#22825;&#25991;&#35266;&#27979;&#30340;&#29420;&#29305;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#20854;&#20013;&#27599;&#39063;&#26143;&#20307;&#26412;&#36136;&#19978;&#26159;&#29420;&#31435;&#30340;&#65292;&#20294;&#21463;&#21040;&#38543;&#26426;&#24182;&#21457;&#22122;&#22768;&#30340;&#24178;&#25200;&#65292;&#23548;&#33268;&#20551;&#35686;&#25253;&#29575;&#36739;&#39640;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AERO&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#22825;&#25991;&#35266;&#27979;&#20013;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#23398;&#20064;&#27599;&#20010;&#21464;&#37327;&#65288;&#21363;&#26143;&#20307;&#65289;&#19978;&#30340;&#27491;&#24120;&#26102;&#38388;&#27169;&#24335;&#65292;&#20197;&#19982;&#21464;&#37327;&#29420;&#31435;&#24615;&#30340;&#29305;&#24449;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10220v1 Announce Type: cross  Abstract: With the development of astronomical facilities, large-scale time series data observed by these facilities is being collected. Analyzing anomalies in these astronomical observations is crucial for uncovering potential celestial events and physical phenomena, thus advancing the scientific research process. However, existing time series anomaly detection methods fall short in tackling the unique characteristics of astronomical observations where each star is inherently independent but interfered by random concurrent noise, resulting in a high rate of false alarms. To overcome the challenges, we propose AERO, a novel two-stage framework tailored for unsupervised anomaly detection in astronomical observations. In the first stage, we employ a Transformer-based encoder-decoder architecture to learn the normal temporal patterns on each variate (i.e., star) in alignment with the characteristic of variate independence. In the second stage, we e
&lt;/p&gt;</description></item><item><title>&#22312;nnU-Net&#26694;&#26550;&#20013;&#25506;&#32034;&#20809;&#27969;&#30340;&#24212;&#29992;&#20197;&#25552;&#39640;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#36136;&#37327;&#65292;&#21033;&#29992;&#20809;&#27969;&#22270;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.10216</link><description>&lt;p&gt;
&#22312;nnU-Net&#26694;&#26550;&#20013;&#25506;&#32034;&#20809;&#27969;&#30340;&#24212;&#29992;&#20197;&#25552;&#39640;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Exploring Optical Flow Inclusion into nnU-Net Framework for Surgical Instrument Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10216
&lt;/p&gt;
&lt;p&gt;
&#22312;nnU-Net&#26694;&#26550;&#20013;&#25506;&#32034;&#20809;&#27969;&#30340;&#24212;&#29992;&#20197;&#25552;&#39640;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#36136;&#37327;&#65292;&#21033;&#29992;&#20809;&#27969;&#22270;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33145;&#33108;&#38236;&#25163;&#26415;&#20013;&#65292;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#23545;&#35745;&#31639;&#36741;&#21161;&#22806;&#31185;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#33145;&#33108;&#38236;&#25163;&#26415;&#30340;&#21160;&#24577;&#29615;&#22659;&#20173;&#28982;&#23545;&#31934;&#30830;&#20998;&#21106;&#25552;&#20986;&#25361;&#25112;&#12290;nnU-Net&#26694;&#26550;&#22312;&#20998;&#26512;&#19981;&#24102;&#26102;&#38388;&#20449;&#24687;&#30340;&#21333;&#24103;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#20809;&#27969;&#65288;OF&#65289;&#22270;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#24341;&#20837;&#21040;nnU-Net&#26550;&#26500;&#20013;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20805;&#20998;&#21033;&#29992;&#22120;&#26800;&#26159;&#22806;&#31185;&#39046;&#22495;&#20027;&#35201;&#31227;&#21160;&#23545;&#35937;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10216v1 Announce Type: cross  Abstract: Surgical instrument segmentation in laparoscopy is essential for computer-assisted surgical systems. Despite the Deep Learning progress in recent years, the dynamic setting of laparoscopic surgery still presents challenges for precise segmentation. The nnU-Net framework excelled in semantic segmentation analyzing single frames without temporal information. The framework's ease of use, including its ability to be automatically configured, and its low expertise requirements, have made it a popular base framework for comparisons. Optical flow (OF) is a tool commonly used in video tasks to estimate motion and represent it in a single frame, containing temporal information. This work seeks to employ OF maps as an additional input to the nnU-Net architecture to improve its performance in the surgical instrument segmentation task, taking advantage of the fact that instruments are the main moving objects in the surgical field. With this new in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#20174; Git README &#25991;&#20214;&#20013;&#25552;&#21462;&#21151;&#33021;&#65292;&#30740;&#31350;&#21160;&#26426;&#28304;&#33258;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#24212;&#29992;&#30340;&#20852;&#36259;&#65292;&#36890;&#36807;&#24320;&#21457;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;70%&#21644;20%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.10205</link><description>&lt;p&gt;
&#20174; README &#20013;&#25552;&#21462;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Read between the lines -- Functionality Extraction From READMEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#20174; Git README &#25991;&#20214;&#20013;&#25552;&#21462;&#21151;&#33021;&#65292;&#30740;&#31350;&#21160;&#26426;&#28304;&#33258;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#24212;&#29992;&#30340;&#20852;&#36259;&#65292;&#36890;&#36807;&#24320;&#21457;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;70%&#21644;20%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25991;&#26412;&#25688;&#35201;&#26159;&#19968;&#39033;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20174; Git README &#25991;&#20214;&#20013;&#25552;&#21462;&#21151;&#33021;&#30340;&#26032;&#39062;&#32780;&#26377;&#29992;&#30340;&#21464;&#20307;&#12290;&#34429;&#28982;&#36825;&#20010;&#20219;&#21153;&#22312;&#25277;&#35937;&#23618;&#38754;&#19978;&#26159;&#19968;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#20294;&#23427;&#28041;&#21450;&#21040;&#33258;&#24049;&#30340;&#29305;&#27530;&#24615;&#21644;&#25361;&#25112;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#24182;&#19981;&#21313;&#20998;&#26377;&#29992;&#12290;&#36825;&#19968;&#20219;&#21153;&#30340;&#21160;&#26426;&#28304;&#33258;&#26368;&#36817;&#22260;&#32469;&#30528;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#65288;&#22914;&#20195;&#30721;&#37325;&#26500;&#12289;&#20195;&#30721;&#25688;&#35201;&#31561;&#65289;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#27963;&#21160;&#30340;&#28608;&#22686;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;FuncRead&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#36825;&#19968;&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#20987;&#36133;&#20102;&#21487;&#20197;&#20351;&#29992;&#27969;&#34892;&#30340;&#40657;&#30418;&#25110;&#30333;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65288;&#22914;ChatGPT&#21644;Bard&#65289;&#35774;&#35745;&#30340;&#20219;&#20309;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#24494;&#35843;&#30340;70&#20159;CodeLlama&#27169;&#22411;&#22312;F1&#19978;&#21462;&#24471;&#20102;70%&#21644;20%&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10205v1 Announce Type: cross  Abstract: While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;JPEG-LDPC&#21387;&#32553;&#22270;&#20687;&#19978;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LDPC&#30721;&#30340;&#20869;&#37096;&#20195;&#30721;&#32467;&#26500;&#65292;&#20351;&#29992;GRU&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22270;&#20687;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.10202</link><description>&lt;p&gt;
&#22312;JPEG-LDPC&#21387;&#32553;&#22270;&#20687;&#19978;&#23398;&#20064;&#65306;&#21033;&#29992;&#32508;&#21512;&#30151;&#29366;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning on JPEG-LDPC Compressed Images: Classifying with Syndromes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;JPEG-LDPC&#21387;&#32553;&#22270;&#20687;&#19978;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LDPC&#30721;&#30340;&#20869;&#37096;&#20195;&#30721;&#32467;&#26500;&#65292;&#20351;&#29992;GRU&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#21521;&#30446;&#26631;&#30340;&#36890;&#20449;&#20013;&#65292;&#25509;&#25910;&#32773;&#30340;&#30446;&#26631;&#36890;&#24120;&#26159;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#37325;&#24314;&#21407;&#22987;&#25968;&#25454;&#12290; &#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22312;&#25509;&#25910;&#22120;&#19978;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#20808;&#21069;&#35299;&#30721;&#21363;&#21487;&#30452;&#25509;&#23545;&#21387;&#32553;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#26377;&#26395;&#22686;&#24378;&#25512;&#26029;&#27169;&#22411;&#30340;&#26102;&#38388;&#25928;&#29575;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20854;&#20013;&#32463;&#29109;&#32534;&#30721;&#26159;&#29992;&#20302;&#23494;&#24230;&#22855;&#20598;&#26657;&#39564;&#65288;LDPC&#65289;&#30721;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#20551;&#35774;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;LDPC&#30721;&#30340;&#20869;&#37096;&#20195;&#30721;&#32467;&#26500;&#12290; &#22312;&#25509;&#25910;&#22120;&#31471;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#36827;&#34892;&#35757;&#32451;&#12290; &#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;LDPC&#30340;&#20998;&#31867;&#26377;&#26395;&#36890;&#36807;&#22522;&#20110;&#25968;&#25454;&#32508;&#21512;&#30151;&#29366;&#30340;&#22270;&#20687;&#20998;&#31867;&#26126;&#26174;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10202v1 Announce Type: cross  Abstract: In goal-oriented communications, the objective of the receiver is often to apply a Deep-Learning model, rather than reconstructing the original data. In this context, direct learning over compressed data, without any prior decoding, holds promise for enhancing the time-efficient execution of inference models at the receiver. However, conventional entropic-coding methods like Huffman and Arithmetic break data structure, rendering them unsuitable for learning without decoding. In this paper, we propose an alternative approach in which entropic coding is realized with Low-Density Parity Check (LDPC) codes. We hypothesize that Deep Learning models can more effectively exploit the internal code structure of LDPC codes. At the receiver, we leverage a specific class of Recurrent Neural Networks (RNNs), specifically Gated Recurrent Unit (GRU), trained for image classification. Our numerical results indicate that classification based on LDPC-co
&lt;/p&gt;</description></item><item><title>&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#29616;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#26080;&#27861;&#24212;&#23545;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#36136;&#37327;&#30340;&#35757;&#32451;&#26041;&#27861;&#20197;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;</title><link>https://arxiv.org/abs/2403.10190</link><description>&lt;p&gt;
&#22522;&#20110;&#24863;&#30693;&#36136;&#37327;&#30340;&#27169;&#22411;&#35757;&#32451;&#22312;&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#19979;
&lt;/p&gt;
&lt;p&gt;
Perceptual Quality-based Model Training under Annotator Label Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10190
&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#29616;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#26080;&#27861;&#24212;&#23545;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#36136;&#37327;&#30340;&#35757;&#32451;&#26041;&#27861;&#20197;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10190v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495;. &#26631;&#27880;&#32773;&#22312;&#25968;&#25454;&#26631;&#35760;&#36807;&#31243;&#20013;&#23384;&#22312;&#20998;&#27495;&#65292;&#21487;&#31216;&#20026;&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#34920;&#29616;&#20026;&#26631;&#35760;&#36136;&#37327;&#30340;&#21464;&#21270;&#12290;&#27599;&#20010;&#26679;&#26412;&#20351;&#29992;&#21333;&#20010;&#20302;&#36136;&#37327;&#26631;&#27880;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#21487;&#38752;&#24615;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#32771;&#23519;&#20102;&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20250;&#38543;&#30528;&#20302;&#36136;&#37327;&#30340;&#22024;&#26434;&#26631;&#31614;&#30340;&#23384;&#22312;&#32780;&#38477;&#20302;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35780;&#20272;&#29616;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#34920;&#26126;&#23427;&#20204;&#26080;&#27861;&#24212;&#23545;&#26631;&#27880;&#32773;&#26631;&#31614;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#24615;&#33021;&#19979;&#38477;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#34920;&#26126;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#29420;&#31435;&#26631;&#27880;&#32773;&#25910;&#38598;&#30340;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24863;&#30693;&#36136;&#37327;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10190v1 Announce Type: cross  Abstract: Annotators exhibit disagreement during data labeling, which can be termed as annotator label uncertainty. Annotator label uncertainty manifests in variations of labeling quality. Training with a single low-quality annotation per sample induces model reliability degradations. In this work, we first examine the effects of annotator label uncertainty in terms of the model's generalizability and prediction uncertainty. We observe that the model's generalizability and prediction uncertainty degrade with the presence of low-quality noisy labels. Meanwhile, our evaluation of existing uncertainty estimation algorithms indicates their incapability in response to annotator label uncertainty. To mitigate performance degradation, prior methods show that training models with labels collected from multiple independent annotators can enhance generalizability. However, they require massive annotations. Hence, we introduce a novel perceptual quality-ba
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;TAPG&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#31574;&#30053;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;&#22312;&#25235;&#21462;&#20219;&#24847;&#29289;&#20307;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.10187</link><description>&lt;p&gt;
&#25235;&#20303;&#19968;&#20999;&#65306;&#23558;&#25945;&#24072;&#22686;&#24378;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#19982;&#23454;&#20363;&#20998;&#21106;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#25235;&#21462;&#20219;&#24847;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10187
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;TAPG&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#31574;&#30053;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;&#22312;&#25235;&#21462;&#20219;&#24847;&#29289;&#20307;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#20174;&#28151;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#29289;&#20307;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#28789;&#24039;&#65292;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#23384;&#22312;&#24050;&#20037;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#25361;&#25112;&#28304;&#20110;&#35270;&#35273;&#24863;&#30693;&#30340;&#22797;&#26434;&#24615;&#65292;&#23545;&#31934;&#20934;&#36816;&#21160;&#25216;&#33021;&#30340;&#38656;&#27714;&#65292;&#20197;&#21450;&#20004;&#32773;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Teacher-Augmented Policy Gradient (TAPG)&#65292;&#35813;&#26694;&#26550;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#31574;&#30053;&#33976;&#39311;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#25945;&#24072;&#31574;&#30053;&#26469;&#25484;&#25569;&#22522;&#20110;&#29289;&#20307;&#20301;&#32622;&#20449;&#24687;&#30340;&#36816;&#21160;&#25511;&#21046;&#65292;TAPG&#20419;&#36827;&#20102;&#22522;&#20110;&#29289;&#20307;&#20998;&#21106;&#30340;&#24863;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#25351;&#23548;&#24615;&#20294;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;Segment Anything&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#20174;&#20223;&#30495;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#30340;&#20256;&#36755;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#29289;&#20307;&#30340;&#29087;&#32451;&#25235;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#22522;&#20110;&#20154;&#31867;&#21487;&#29702;&#35299;&#25552;&#31034;&#30340;&#28151;&#20081;&#22330;&#26223;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#33021;&#22815;&#29087;&#32451;&#22320;&#25235;&#21462;&#21508;&#31181;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10187v1 Announce Type: cross  Abstract: Interactive grasping from clutter, akin to human dexterity, is one of the longest-standing problems in robot learning. Challenges stem from the intricacies of visual perception, the demand for precise motor skills, and the complex interplay between the two. In this work, we present Teacher-Augmented Policy Gradient (TAPG), a novel two-stage learning framework that synergizes reinforcement learning and policy distillation. After training a teacher policy to master the motor control based on object pose information, TAPG facilitates guided, yet adaptive, learning of a sensorimotor policy, based on object segmentation. We zero-shot transfer from simulation to a real robot by using Segment Anything Model for promptable object segmentation. Our trained policies adeptly grasp a wide variety of objects from cluttered scenarios in simulation and the real world based on human-understandable prompts. Furthermore, we show robust zero-shot transfe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#20851;&#31995;&#39046;&#22495;&#20013;&#22914;&#20309;&#21033;&#29992;&#25260;&#21319;&#25216;&#26415;&#39640;&#25928;&#35745;&#31639;&#22240;&#26524;&#25928;&#24212;&#65292;&#24341;&#20837;&#20102;&#21442;&#25968;&#21270;&#22240;&#26524;&#22240;&#23376;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#25260;&#21319;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#22240;&#26524;&#25512;&#26029;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.10184</link><description>&lt;p&gt;
&#22312;&#20851;&#31995;&#39046;&#22495;&#20013;&#30340;&#25260;&#21319;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Lifted Causal Inference in Relational Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#20851;&#31995;&#39046;&#22495;&#20013;&#22914;&#20309;&#21033;&#29992;&#25260;&#21319;&#25216;&#26415;&#39640;&#25928;&#35745;&#31639;&#22240;&#26524;&#25928;&#24212;&#65292;&#24341;&#20837;&#20102;&#21442;&#25968;&#21270;&#22240;&#26524;&#22240;&#23376;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#25260;&#21319;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#22240;&#26524;&#25512;&#26029;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25260;&#21319;&#25512;&#26029;&#36890;&#36807;&#20351;&#29992;&#21487;&#21306;&#20998;&#19981;&#21516;&#23545;&#35937;&#30340;&#20195;&#34920;&#65292;&#22312;&#27010;&#29575;&#22270;&#27169;&#22411;&#20013;&#21033;&#29992;&#23545;&#31216;&#24615;&#65292;&#20197;&#21152;&#24555;&#26597;&#35810;&#22238;&#31572;&#36895;&#24230;&#21516;&#26102;&#20445;&#25345;&#31934;&#30830;&#31572;&#26696;&#12290;&#23613;&#31649;&#25260;&#21319;&#26159;&#19968;&#31181;&#22312;&#20851;&#31995;&#39046;&#22495;&#20013;&#27010;&#29575;&#25512;&#26029;&#20219;&#21153;&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#20294;&#23578;&#26410;&#24212;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25260;&#21319;&#24212;&#29992;&#20110;&#22312;&#20851;&#31995;&#39046;&#22495;&#20869;&#39640;&#25928;&#35745;&#31639;&#22240;&#26524;&#25928;&#24212;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#21442;&#25968;&#21270;&#22240;&#26524;&#22240;&#23376;&#22270;&#20316;&#20026;&#21253;&#21547;&#22240;&#26524;&#30693;&#35782;&#30340;&#21442;&#25968;&#21270;&#22240;&#23376;&#22270;&#30340;&#25193;&#23637;&#65292;&#24182;&#22312;&#20854;&#20013;&#32473;&#20986;&#24178;&#39044;&#30340;&#24418;&#24335;&#35821;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25260;&#21319;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#65292;&#20197;&#22312;&#25260;&#21319;&#32423;&#21035;&#35745;&#31639;&#22240;&#26524;&#25928;&#24212;&#65292;&#20174;&#32780;&#19982;&#21629;&#39064;&#25512;&#26029;&#65288;&#20363;&#22914;&#22312;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;&#65289;&#30456;&#27604;&#22823;&#24133;&#21152;&#24555;&#22240;&#26524;&#25512;&#26029;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10184v1 Announce Type: new  Abstract: Lifted inference exploits symmetries in probabilistic graphical models by using a representative for indistinguishable objects, thereby speeding up query answering while maintaining exact answers. Even though lifting is a well-established technique for the task of probabilistic inference in relational domains, it has not yet been applied to the task of causal inference. In this paper, we show how lifting can be applied to efficiently compute causal effects in relational domains. More specifically, we introduce parametric causal factor graphs as an extension of parametric factor graphs incorporating causal knowledge and give a formal semantics of interventions therein. We further present the lifted causal inference algorithm to compute causal effects on a lifted level, thereby drastically speeding up causal inference compared to propositional inference, e.g., in causal Bayesian networks. In our empirical evaluation, we demonstrate the eff
&lt;/p&gt;</description></item><item><title>&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#31243;&#24207;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#65292;&#21487;&#20197;&#20445;&#35777;&#30417;&#30563;&#23398;&#20064;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;</title><link>https://arxiv.org/abs/2403.10175</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Short Survey on Importance Weighting for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10175
&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#31243;&#24207;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#65292;&#21487;&#20197;&#20445;&#35777;&#30417;&#30563;&#23398;&#20064;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#31243;&#24207;&#65292;&#26681;&#25454;&#26576;&#31181;&#24847;&#20041;&#19978;&#23454;&#20363;&#30340;&#37325;&#35201;&#24615;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#12290;&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#29992;&#30340;&#24605;&#24819;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20102;&#35768;&#22810;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#25454;&#30693;&#65292;&#22312;&#20851;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#20551;&#35774;&#19979;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#23494;&#24230;&#27604;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#21487;&#20197;&#20445;&#35777;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;&#12290;&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#30456;&#20851;&#30740;&#31350;&#20013;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10175v1 Announce Type: cross  Abstract: Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense. The simplicity and usefulness of the idea has led to many applications of importance weighting. For example, it is known that supervised learning under an assumption about the difference between the training and test distributions, called distribution shift, can guarantee statistically desirable properties through importance weighting by their density ratio. This survey summarizes the broad applications of importance weighting in machine learning and related research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10173</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#25552;&#20379;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#21160;&#24577;&#33539;&#22260;&#65292;&#20960;&#20046;&#27809;&#26377;&#36816;&#21160;&#27169;&#31946;&#65292;&#38750;&#24120;&#36866;&#21512;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#12290;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20107;&#20214;&#39537;&#21160;&#24863;&#30693;&#25968;&#25454;&#22825;&#29983;&#21305;&#37197;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#33021;&#22815;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#21644;&#20302;&#24310;&#36831;&#25512;&#26029;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#21017;&#23637;&#31034;&#20986;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28151;&#21512;SNN-ANN&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#21033;&#29992;SNN&#21644;ANN&#20307;&#31995;&#32467;&#26500;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#39592;&#24178;&#32593;&#32476;&#65292;&#29992;&#20110;&#20351;&#29992;&#20107;&#20214;&#30456;&#26426;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#26725;&#25509;&#27169;&#22359;&#65292;&#20174;SNN&#23618;&#20013;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#23494;&#38598;&#29305;&#24449;&#22270;&#65292;&#20379;&#39592;&#24178;&#32593;&#32476;&#30340;ANN&#37096;&#20998;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10173v1 Announce Type: cross  Abstract: Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for object detection tasks. While Spiking Neural Networks (SNNs) are a natural match for event-based sensory data and enable ultra-energy efficient and low latency inference on neuromorphic hardware, Artificial Neural Networks (ANNs) tend to display more stable training dynamics and faster convergence resulting in greater task performance. Hybrid SNN-ANN approaches are a promising alternative, enabling to leverage the strengths of both SNN and ANN architectures. In this work, we introduce the first Hybrid Attention-based SNN-ANN backbone for object detection using event cameras. We propose a novel Attention-based SNN-ANN bridge module to capture sparse spatial and temporal relations from the SNN layer and convert them into dense feature maps for the ANN part of the backbone. Experimental results demonstrate that our proposed m
&lt;/p&gt;</description></item><item><title>AUTONODE&#26159;&#19968;&#31181;&#31070;&#32463;&#22270;&#33258;&#23398;&#20064;&#24341;&#25806;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#31070;&#32463;&#22270;&#25216;&#26415;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#26234;&#33021;&#20195;&#29702;&#20154;&#22312;&#32593;&#32476;&#30028;&#38754;&#19978;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10171</link><description>&lt;p&gt;
AUTONODE: &#19968;&#31181;&#29992;&#20110;&#35748;&#30693;GUI&#33258;&#21160;&#21270;&#30340;&#31070;&#32463;&#22270;&#33258;&#23398;&#20064;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10171
&lt;/p&gt;
&lt;p&gt;
AUTONODE&#26159;&#19968;&#31181;&#31070;&#32463;&#22270;&#33258;&#23398;&#20064;&#24341;&#25806;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#31070;&#32463;&#22270;&#25216;&#26415;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#26234;&#33021;&#20195;&#29702;&#20154;&#22312;&#32593;&#32476;&#30028;&#38754;&#19978;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#65292;&#20986;&#29616;&#20102;&#19968;&#20123;&#20855;&#26377;&#22686;&#24378;&#30340;&#35748;&#30693;&#33021;&#21147;&#21644;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#30340;&#33021;&#22815;&#35299;&#20915;&#26426;&#22120;&#20154;&#27969;&#31243;&#33258;&#21160;&#21270;&#65288;RPA&#65289;&#25361;&#25112;&#30340;&#20195;&#29702;&#20154;&#12290;&#36825;&#19968;&#21457;&#23637;&#26631;&#24535;&#30528;&#22312;&#30446;&#26631;&#23454;&#29616;&#26041;&#38754;&#20986;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#31867;&#20284;&#20154;&#31867;&#30340;&#36866;&#24212;&#24615;&#30340;&#26032;&#26102;&#20195;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AUTONODE&#65288;&#36890;&#36807;&#22312;&#32447;&#31070;&#32463;&#22270;&#25805;&#20316;&#21644;&#28145;&#24230;&#25506;&#32034;&#23454;&#29616;&#33258;&#20027;&#29992;&#25143;&#30028;&#38754;&#36716;&#25442;&#65289;&#12290;AUTONODE&#37319;&#29992;&#20808;&#36827;&#30340;&#31070;&#32463;&#22270;&#25216;&#26415;&#65292;&#20419;&#36827;&#20102;&#23545;&#32593;&#32476;&#30028;&#38754;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#39044;&#23450;&#20041;&#33050;&#26412;&#25110;&#25163;&#21160;&#24178;&#39044;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#24341;&#25806;&#36171;&#20104;&#20195;&#29702;&#20154;&#29702;&#35299;&#21644;&#23454;&#26045;&#22797;&#26434;&#24037;&#20316;&#27969;&#30340;&#33021;&#21147;&#65292;&#20197;&#26080;&#19982;&#20262;&#27604;&#30340;&#25928;&#29575;&#36866;&#24212;&#21160;&#24577;&#32593;&#32476;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35748;&#30693;&#21151;&#33021;&#19982;&#26426;&#22120;&#20154;&#33258;&#21160;&#21270;&#30456;&#32467;&#21512;&#65292;&#36171;&#20104;AUTONODE&#20174;abiliti&#21040;abi&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10171v1 Announce Type: new  Abstract: In recent advancements within the domain of Large Language Models (LLMs), there has been a notable emergence of agents capable of addressing Robotic Process Automation (RPA) challenges through enhanced cognitive capabilities and sophisticated reasoning. This development heralds a new era of scalability and human-like adaptability in goal attainment. In this context, we introduce AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic Operations and Deep Exploration). AUTONODE employs advanced neuro-graphical techniques to facilitate autonomous navigation and task execution on web interfaces, thereby obviating the necessity for predefined scripts or manual intervention. Our engine empowers agents to comprehend and implement complex workflows, adapting to dynamic web environments with unparalleled efficiency. Our methodology synergizes cognitive functionalities with robotic automation, endowing AUTONODE with the abi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#23376;&#22270;&#20013;&#39640;&#25928;&#26816;&#27979;&#21487;&#20132;&#25442;&#22240;&#23376;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35745;&#31639;&#24037;&#20316;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.10167</link><description>&lt;p&gt;
&#22240;&#23376;&#22270;&#20013;&#21487;&#20132;&#25442;&#22240;&#23376;&#30340;&#39640;&#25928;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Detection of Exchangeable Factors in Factor Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10167
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#23376;&#22270;&#20013;&#39640;&#25928;&#26816;&#27979;&#21487;&#20132;&#25442;&#22240;&#23376;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35745;&#31639;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#23545;&#39046;&#22495;&#22823;&#23567;&#30340;&#21487;&#35745;&#31639;&#27010;&#29575;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#25552;&#21319;&#24335;&#27010;&#29575;&#25512;&#26029;&#65292;&#21033;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#26816;&#26597;&#20004;&#20010;&#22240;&#23376;&#26159;&#21542;&#32534;&#30721;&#31561;&#25928;&#35821;&#20041;&#20174;&#32780;&#26159;&#21487;&#20132;&#25442;&#30340;&#65292;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#39640;&#25928;&#35299;&#20915;&#20102;&#22312;&#22240;&#23376;&#22270;&#20013;&#26816;&#27979;&#21487;&#20132;&#25442;&#22240;&#23376;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24341;&#20837;&#20102;&#26816;&#27979;&#21487;&#20132;&#25442;&#22240;&#23376;&#65288;DEFT&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#23454;&#36341;&#20013;&#26816;&#26597;&#20004;&#20010;&#22240;&#23376;&#26159;&#21542;&#21487;&#20132;&#25442;&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;DEFT&#26377;&#25928;&#22320;&#35782;&#21035;&#38480;&#21046;&#20197;&#22823;&#24133;&#20943;&#23569;&#25490;&#21015;&#25968;&#37327;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#39564;&#35777;&#20102;DEFT&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10167v1 Announce Type: new  Abstract: To allow for tractable probabilistic inference with respect to domain sizes, lifted probabilistic inference exploits symmetries in probabilistic graphical models. However, checking whether two factors encode equivalent semantics and hence are exchangeable is computationally expensive. In this paper, we efficiently solve the problem of detecting exchangeable factors in a factor graph. In particular, we introduce the detection of exchangeable factors (DEFT) algorithm, which allows us to drastically reduce the computational effort for checking whether two factors are exchangeable in practice. While previous approaches iterate all $O(n!)$ permutations of a factor's argument list in the worst case (where $n$ is the number of arguments of the factor), we prove that DEFT efficiently identifies restrictions to drastically reduce the number of permutations and validate the efficiency of DEFT in our empirical evaluation.
&lt;/p&gt;</description></item><item><title>CoReEcho&#25552;&#20986;&#20102;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.10164</link><description>&lt;p&gt;
CoReEcho: 2D+&#26102;&#38388;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#26512;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10164
&lt;/p&gt;
&lt;p&gt;
CoReEcho&#25552;&#20986;&#20102;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#30452;&#22312;&#19981;&#21516;&#27169;&#24577;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#65292;&#21253;&#25324;&#36229;&#22768;&#24515;&#21160;&#22270;&#65292;&#22312;&#25552;&#20379;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#21516;&#26102;&#12290;&#28982;&#32780;&#65292;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#27700;&#32447;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#38590;&#20197;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#25429;&#33719;&#36229;&#22768;&#24515;&#21160;&#22270;&#29255;&#27573;&#20043;&#38388;&#30340;&#36830;&#32493;&#20851;&#31995;&#65292;&#23548;&#33268;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#23545;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoReEcho&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#35843;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;CoReEcho&#65306;1&#65289;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#65288;EchoNet-Dynamic&#65289;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65288;SOTA&#65289;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;3.90&#21644;R2 o
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10164v1 Announce Type: cross  Abstract: Deep learning (DL) models have been advancing automatic medical image analysis on various modalities, including echocardiography, by offering a comprehensive end-to-end training pipeline. This approach enables DL models to regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting in superior performance. However, the end-to-end training pipeline makes the learned representations less explainable. The representations may also fail to capture the continuous relation among echocardiogram clips, indicating the existence of spurious correlations, which can negatively affect the generalization. To mitigate this issue, we propose CoReEcho, a novel training framework emphasizing continuous representations tailored for direct EF regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms the current state-of-the-art (SOTA) on the largest echocardiography dataset (EchoNet-Dynamic) with MAE of 3.90 &amp; R2 o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#22797;&#26434;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#36827;&#34892;&#25968;&#25454;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.10158</link><description>&lt;p&gt;
&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#20419;&#36827;&#20581;&#24247;&#21644;&#31038;&#20250;&#20851;&#24576;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#22797;&#26434;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#36827;&#34892;&#25968;&#25454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20989;&#25968;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;funGCN&#65289;&#26694;&#26550;&#65292;&#23558;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#25968;&#23383;&#20581;&#24247;&#21644;&#32437;&#21521;&#30740;&#31350;&#20013;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12290;&#38543;&#30528;&#20581;&#24247;&#35299;&#20915;&#26041;&#26696;&#23545;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#21644;&#31038;&#20250;&#25903;&#25345;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#38271;&#65292;&#30830;&#20445;&#21508;&#24180;&#40836;&#27573;&#30340;&#20581;&#24247;&#29983;&#27963;&#21644;&#20419;&#36827;&#24184;&#31119;&#24863;&#65292;funGCN&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22810;&#20010;&#23454;&#20307;&#30340;&#22810;&#20803;&#32437;&#21521;&#25968;&#25454;&#65292;&#24182;&#30830;&#20445;&#21363;&#20351;&#22312;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#20063;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#31649;&#29702;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#23884;&#20837;&#32452;&#20214;&#12289;&#25191;&#34892;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21019;&#24314;&#30693;&#35782;&#22270;&#20197;&#33719;&#21462;&#27934;&#23519;&#24615;&#25968;&#25454;&#35299;&#37322;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#39564;&#35777;&#20102;funGCN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10158v1 Announce Type: cross  Abstract: This paper introduces a novel Functional Graph Convolutional Network (funGCN) framework that combines Functional Data Analysis and Graph Convolutional Networks to address the complexities of multi-task and multi-modal learning in digital health and longitudinal studies. With the growing importance of health solutions to improve health care and social support, ensure healthy lives, and promote well-being at all ages, funGCN offers a unified approach to handle multivariate longitudinal data for multiple entities and ensures interpretability even with small sample sizes. Key innovations include task-specific embedding components that manage different data types, the ability to perform classification, regression, and forecasting, and the creation of a knowledge graph for insightful data interpretation. The efficacy of funGCN is validated through simulation experiments and a real-data application.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.10144</link><description>&lt;p&gt;
NLP&#39564;&#35777;&#65306;&#36208;&#21521;&#19968;&#31181;&#36890;&#29992;&#30340;&#29992;&#20110;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
NLP Verification: Towards a General Methodology for Certifying Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#23545;&#21464;&#21270;&#25110;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#33021;&#23545;&#20854;&#36755;&#20986;&#32473;&#20986;&#20445;&#35777;&#12290;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#19981;&#21516;&#65292;NLP&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26041;&#27861;&#35770;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#25991;&#29486;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;NLP&#39564;&#35777;&#30340;&#23454;&#29992;&#38382;&#39064;&#24120;&#24120;&#28041;&#21450;&#19981;&#28145;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#25552;&#28860;&#21644;&#35780;&#20272;&#19968;&#20010;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#35813;&#27969;&#31243;&#26469;&#28304;&#20110;&#36804;&#20170;&#20026;&#27490;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#35821;&#20041;&#27867;&#21270;&#25216;&#26415;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10144v1 Announce Type: cross  Abstract: Deep neural networks have exhibited substantial success in the field of Natural Language Processing (NLP) and ensuring their safety and reliability is crucial: there are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Unlike Computer Vision, NLP lacks a unified verification methodology and, despite recent advancements in literature, they are often light on the pragmatical issues of NLP verification. In this paper, we make an attempt to distil and evaluate general components of an NLP verification pipeline, that emerges from the progress in the field to date. Our contributions are two-fold. Firstly, we give a general characterisation of verifiable subspaces that result from embedding sentences into continuous spaces. We identify, and give an effective method to deal with, the technical challenge of semantic generalisability of verified subspaces; and propose it a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#22797;&#27979;&#37327;&#30340;&#35270;&#35273;&#27169;&#25311;&#37327;&#34920;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21709;&#24212;&#39118;&#26684;&#65288;RP&#65289;&#34920;&#24449;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;VAS&#20013;&#22788;&#29702;RP&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.10136</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;&#27169;&#25311;&#37327;&#34920;&#23545;&#37325;&#22797;&#27979;&#37327;&#30340;&#21709;&#24212;&#39118;&#26684;&#36827;&#34892;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Response Style Characterization for Repeated Measures Using the Visual Analogue Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#22797;&#27979;&#37327;&#30340;&#35270;&#35273;&#27169;&#25311;&#37327;&#34920;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21709;&#24212;&#39118;&#26684;&#65288;RP&#65289;&#34920;&#24449;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;VAS&#20013;&#22788;&#29702;RP&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#25253;&#21578;&#27979;&#37327;&#65288;&#20363;&#22914;&#65292;&#21033;&#20811;&#29305;&#37327;&#34920;&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#20027;&#35266;&#20581;&#24247;&#24863;&#30693;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#31934;&#30830;&#19988;&#20415;&#20110;&#35780;&#20272;&#20154;&#20204;&#24863;&#21463;&#30340;&#33021;&#21147;&#65292;&#35270;&#35273;&#27169;&#25311;&#37327;&#34920;&#65288;VAS&#65289;&#65292;&#19968;&#31181;&#28369;&#21160;&#26465;&#37327;&#34920;&#65292;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#20250;&#21463;&#21040;&#21709;&#24212;&#39118;&#26684;&#65288;RS&#65289;&#30340;&#24433;&#21709;&#65292;RS&#26159;&#19968;&#31181;&#29992;&#25143;&#20381;&#36182;&#30340;&#31995;&#32479;&#24615;&#20542;&#21521;&#65292;&#26080;&#35770;&#38382;&#21367;&#35828;&#26126;&#22914;&#20309;&#37117;&#20250;&#21457;&#29983;&#12290;&#23613;&#31649;&#22312;&#20010;&#20307;&#38388;&#20998;&#26512;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#20294;&#23545;VAS&#20013;RS&#65288;&#34920;&#31034;&#20026;&#21709;&#24212;&#21078;&#38754;&#65288;RP&#65289;&#65289;&#30340;&#22788;&#29702;&#24182;&#26410;&#21463;&#21040;&#36275;&#22815;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20027;&#35201;&#29992;&#20110;&#20010;&#20307;&#20869;&#30417;&#27979;&#19988;&#19981;&#22826;&#21463;RP&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;VAS&#27979;&#37327;&#36890;&#24120;&#38656;&#35201;&#23545;&#21516;&#19968;&#38382;&#21367;&#39033;&#30446;&#36827;&#34892;&#37325;&#22797;&#33258;&#25105;&#25253;&#21578;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#22312;&#21033;&#20811;&#29305;&#37327;&#34920;&#19978;&#24212;&#29992;&#20256;&#32479;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RP&#34920;&#24449;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#37325;&#22797;&#27979;&#37327;&#30340;VAS&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10136v1 Announce Type: cross  Abstract: Self-report measures (e.g., Likert scales) are widely used to evaluate subjective health perceptions. Recently, the visual analog scale (VAS), a slider-based scale, has become popular owing to its ability to precisely and easily assess how people feel. These data can be influenced by the response style (RS), a user-dependent systematic tendency that occurs regardless of questionnaire instructions. Despite its importance, especially in between-individual analysis, little attention has been paid to handling the RS in the VAS (denoted as response profile (RP)), as it is mainly used for within-individual monitoring and is less affected by RP. However, VAS measurements often require repeated self-reports of the same questionnaire items, making it difficult to apply conventional methods on a Likert scale. In this study, we developed a novel RP characterization method for various types of repeatedly measured VAS data. This approach involves t
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#21512;&#28436;&#31034;&#30340;&#26032;&#39062;&#26041;&#27861;LLMSRec-Syn&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10135</link><description>&lt;p&gt;
&#25972;&#20307;&#20248;&#20110;&#24635;&#21644;&#65306;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#32858;&#21512;&#28436;&#31034;&#36827;&#34892;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10135
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#21512;&#28436;&#31034;&#30340;&#26032;&#39062;&#26041;&#27861;LLMSRec-Syn&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;LLMs&#20316;&#20026;&#24378;&#22823;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#23548;&#26684;&#24335;&#12289;&#20219;&#21153;&#19968;&#33268;&#24615;&#12289;&#28436;&#31034;&#36873;&#25321;&#21644;&#28436;&#31034;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLMSRec-Syn&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#28436;&#31034;&#29992;&#25143;&#25972;&#21512;&#25104;&#19968;&#20010;&#32858;&#21512;&#28436;&#31034;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;LLMSRec-Syn&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;LLMSRec-Syn&#21487;&#20197;&#19982;&#29978;&#33267;&#20248;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/demoleiwang/LLMSRec_Syn&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10135v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAFT&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#29992;&#30456;&#20851;&#25991;&#26723;&#20013;&#33021;&#22815;&#24110;&#21161;&#22238;&#31572;&#38382;&#39064;&#30340;&#27491;&#30830;&#24207;&#21015;&#26469;&#25913;&#21892;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10131</link><description>&lt;p&gt;
RAFT&#65306;&#23558;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#21040;&#29305;&#23450;&#39046;&#22495;RAG
&lt;/p&gt;
&lt;p&gt;
RAFT: Adapting Language Model to Domain Specific RAG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAFT&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#29992;&#30456;&#20851;&#25991;&#26723;&#20013;&#33021;&#22815;&#24110;&#21161;&#22238;&#31572;&#38382;&#39064;&#30340;&#27491;&#30830;&#24207;&#21015;&#26469;&#25913;&#21892;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26631;&#20934;&#33539;&#24335;&#12290;&#22312;&#23558;&#36825;&#20123;LLMs&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#26102;&#65292;&#36890;&#24120;&#36824;&#20250;&#36890;&#36807;&#22522;&#20110;RAG&#30340;&#25552;&#31034;&#25110;&#24494;&#35843;&#65292;&#23558;&#26032;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#26102;&#25928;&#26032;&#38395;&#25110;&#31169;&#26377;&#39046;&#22495;&#30693;&#35782;&#65289;&#23884;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33719;&#24471;&#36825;&#20123;&#26032;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#65288;RAFT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#22312;"&#24320;&#25918;&#20070;&#31821;"&#30340;&#39046;&#22495;&#35774;&#32622;&#20013;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#22312;RAFT&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#38382;&#39064;&#21644;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#24573;&#30053;&#37027;&#20123;&#23545;&#22238;&#31572;&#38382;&#39064;&#27809;&#26377;&#24110;&#21161;&#30340;&#25991;&#26723;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#24178;&#25200;&#25991;&#26723;&#12290;RAFT&#36890;&#36807;&#21407;&#25991;&#24341;&#29992;&#30456;&#20851;&#25991;&#26723;&#20013;&#33021;&#22815;&#24110;&#21161;&#22238;&#31572;&#38382;&#39064;&#30340;&#27491;&#30830;&#24207;&#21015;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10131v1 Announce Type: cross  Abstract: Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a "open-book" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36827;&#21270;&#26041;&#27861;&#30340;&#21333;&#26234;&#33021;&#20307;&#19982;&#22810;&#26234;&#33021;&#20307;&#31169;&#23494;&#20027;&#21160;&#24863;&#30693;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#31034;&#20363;&#29992;&#20363;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10112</link><description>&lt;p&gt;
&#21333;&#26234;&#33021;&#20307;&#19982;&#22810;&#26234;&#33021;&#20307;&#30340;&#31169;&#23494;&#20027;&#21160;&#24863;&#30693;&#65306;&#28145;&#24230;&#31070;&#32463;&#36827;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Single- and Multi-Agent Private Active Sensing: A Deep Neuroevolution Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36827;&#21270;&#26041;&#27861;&#30340;&#21333;&#26234;&#33021;&#20307;&#19982;&#22810;&#26234;&#33021;&#20307;&#31169;&#23494;&#20027;&#21160;&#24863;&#30693;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#31034;&#20363;&#29992;&#20363;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#23384;&#22312;&#31397;&#35270;&#32773;&#24773;&#20917;&#19979;&#30340;&#20027;&#21160;&#20551;&#35774;&#27979;&#35797;&#20013;&#30340;&#19968;&#20010;&#38598;&#20013;&#24335;&#38382;&#39064;&#21644;&#19968;&#20010;&#20998;&#25955;&#24335;&#38382;&#39064;&#12290;&#38024;&#23545;&#21253;&#25324;&#21333;&#20010;&#21512;&#27861;&#26234;&#33021;&#20307;&#30340;&#38598;&#20013;&#24335;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#36827;&#21270;&#65288;NE&#65289;&#30340;&#26032;&#26694;&#26550;&#65307;&#32780;&#38024;&#23545;&#20998;&#25955;&#24335;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;NE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#36259;&#22320;&#20445;&#25345;&#20102;&#21333;&#19968;&#26234;&#33021;&#20307;NE&#30340;&#25152;&#26377;&#35745;&#31639;&#20248;&#21183;&#12290;&#36890;&#36807;&#23545;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#19978;&#24322;&#24120;&#26816;&#27979;&#31034;&#20363;&#29992;&#20363;&#20013;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;EAHT&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#20027;&#21160;&#20551;&#35774;&#27979;&#35797;&#31574;&#30053;&#20197;&#21450;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10112v1 Announce Type: new  Abstract: In this paper, we focus on one centralized and one decentralized problem of active hypothesis testing in the presence of an eavesdropper. For the centralized problem including a single legitimate agent, we present a new framework based on NeuroEvolution (NE), whereas, for the decentralized problem, we develop a novel NE-based method for solving collaborative multi-agent tasks, which interestingly maintains all computational benefits of single-agent NE. The superiority of the proposed EAHT approaches over conventional active hypothesis testing policies, as well as learning-based methods, is validated through numerical investigations in an example use case of anomaly detection over wireless sensor networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65292;&#36890;&#36807;&#23398;&#20064;&#20803;&#31639;&#23376;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#22797;&#26434;&#26597;&#35810;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.10110</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19978;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#30340;&#20803;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Meta Operator for Complex Query Answering on Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65292;&#36890;&#36807;&#23398;&#20064;&#20803;&#31639;&#23376;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#22797;&#26434;&#26597;&#35810;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#21253;&#21547;&#26377;&#20449;&#24687;&#24615;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#23436;&#25972;&#30340;&#12290;&#20026;&#20102;&#22312;&#19981;&#23436;&#25972;&#30693;&#35782;&#19979;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#65288;CQA&#65289;&#27169;&#22411;&#65292;&#30452;&#25509;&#20174;&#26597;&#35810;-&#31572;&#26696;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#36991;&#20813;&#30452;&#25509;&#36941;&#21382;&#19981;&#23436;&#25972;&#30340;&#22270;&#25968;&#25454;&#12290;&#29616;&#26377;&#30740;&#31350;&#23558;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#27169;&#22411;&#30340;&#35757;&#32451;&#21046;&#23450;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22797;&#26434;&#26597;&#35810;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#35748;&#20026;&#19981;&#21516;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#31867;&#22411;&#65292;&#32780;&#19981;&#26159;&#19981;&#21516;&#30340;&#22797;&#26434;&#26597;&#35810;&#31867;&#22411;&#65292;&#26159;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#23398;&#20064;&#20803;&#36816;&#31639;&#31526;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#22797;&#26434;&#26597;&#35810;&#19979;&#30340;&#19981;&#21516;&#36816;&#31639;&#31526;&#23454;&#20363;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#20803;&#31639;&#23376;&#27604;&#23398;&#20064;&#21407;&#22987;CQA&#25110;&#20803;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10110v1 Announce Type: cross  Abstract: Knowledge graphs contain informative factual knowledge but are considered incomplete. To answer complex queries under incomplete knowledge, learning-based Complex Query Answering (CQA) models are proposed to directly learn from the query-answer samples to avoid the direct traversal of incomplete graph data. Existing works formulate the training of complex query answering models as multi-task learning and require a large number of training samples. In this work, we explore the compositional structure of complex queries and argue that the different logical operator types, rather than the different complex query types, are the key to improving generalizability. Accordingly, we propose a meta-learning algorithm to learn the meta-operators with limited data and adapt them to different instances of operators under various complex queries. Empirical results show that learning meta-operators is more effective than learning original CQA or meta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#25512;&#29702;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;V-HOI Multi-LLMs Collaborated Reasoning&#65288;V-HOI MLCR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10107</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;LLM&#21512;&#20316;&#25512;&#29702;&#25552;&#21319;&#20154;&#31867;&#20013;&#24515;&#21160;&#24577;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10107
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#25512;&#29702;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;V-HOI Multi-LLMs Collaborated Reasoning&#65288;V-HOI MLCR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#30340;&#21160;&#24577;&#22330;&#26223;&#29702;&#35299;&#22312;&#22686;&#24378;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20854;&#20013;&#35270;&#39057;&#20154;-&#29289;&#20132;&#20114;&#65288;V-HOI&#65289;&#26816;&#27979;&#26159;&#35821;&#20041;&#22330;&#26223;&#29702;&#35299;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#20840;&#38754;&#29702;&#35299;&#35270;&#39057;&#20013;&#30340;HOI&#20851;&#31995;&#65292;&#20197;&#20351;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#34892;&#20026;&#20915;&#31574;&#21463;&#30410;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;V-HOI&#26816;&#27979;&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#20687;&#20154;&#31867;&#19968;&#26679;&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#65292;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;HOI&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;V-HOI&#22810;LLM&#21327;&#21516;&#25512;&#29702;&#65288;V-HOI MLCR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#30001;&#19968;&#31995;&#21015;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#29616;&#25104;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#20419;&#36827;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10107v1 Announce Type: cross  Abstract: Human-centered dynamic scene understanding plays a pivotal role in enhancing the capability of robotic and autonomous systems, in which Video-based Human-Object Interaction (V-HOI) detection is a crucial task in semantic scene understanding, aimed at comprehensively understanding HOI relationships within a video to benefit the behavioral decisions of mobile robots and autonomous driving systems. Although previous V-HOI detection models have made significant strides in accurate detection on specific datasets, they still lack the general reasoning ability like human beings to effectively induce HOI relationships. In this study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a novel framework consisting of a series of plug-and-play modules that could facilitate the performance of current V-HOI detection models by leveraging the strong reasoning ability of different off-the-shelf pre-trained large language models (LLMs). 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;BNBRL+&#31639;&#27861;&#65292;&#32467;&#21512;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21487;&#35266;&#23519;&#21306;&#22495;&#35780;&#20272;&#39118;&#38505;&#21644;&#21046;&#23450;&#31227;&#21160;&#31574;&#30053;&#30340;&#30446;&#30340;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21160;&#24577;&#20851;&#31995;&#21644;&#31038;&#20250;&#35268;&#33539;&#65292;&#23454;&#29616;&#20102;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.10105</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#20449;&#24565;&#36741;&#21161;&#23548;&#33322;&#20197;&#36991;&#20813;&#30450;&#28857;&#20013;&#30340;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
Belief Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10105
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;BNBRL+&#31639;&#27861;&#65292;&#32467;&#21512;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21487;&#35266;&#23519;&#21306;&#22495;&#35780;&#20272;&#39118;&#38505;&#21644;&#21046;&#23450;&#31227;&#21160;&#31574;&#30053;&#30340;&#30446;&#30340;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21160;&#24577;&#20851;&#31995;&#21644;&#31038;&#20250;&#35268;&#33539;&#65292;&#23454;&#29616;&#20102;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#26368;&#26032;&#30740;&#31350;&#38598;&#20013;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#35201;&#27714;&#26469;&#33258;&#20840;&#21521;&#20256;&#24863;&#22120;&#30340;&#20934;&#30830;&#20301;&#32622;&#20449;&#24687;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#38024;&#23545;&#36825;&#19968;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#31639;&#27861;BNBRL+&#65292;&#22522;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26694;&#26550;&#65292;&#35780;&#20272;&#19981;&#21487;&#35266;&#23519;&#21306;&#22495;&#30340;&#39118;&#38505;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#21046;&#23450;&#31227;&#21160;&#31574;&#30053;&#12290;BNBRL+&#23558;&#20449;&#24565;&#31639;&#27861;&#19982;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#65292;&#26681;&#25454;&#20154;&#31867;&#30340;&#20301;&#32622;&#25968;&#25454;&#27010;&#29575;&#25512;&#26029;&#20449;&#24565;&#12290;&#23427;&#36827;&#19968;&#27493;&#25972;&#21512;&#20102;&#26426;&#22120;&#20154;&#12289;&#20154;&#31867;&#21644;&#25512;&#26029;&#20449;&#24565;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;&#65292;&#30830;&#23450;&#23548;&#33322;&#36335;&#24452;&#65292;&#24182;&#22312;&#22870;&#21169;&#20989;&#25968;&#20869;&#23884;&#20837;&#31038;&#20250;&#35268;&#33539;&#65292;&#20174;&#32780;&#20419;&#36827;&#31038;&#20132;&#24863;&#30693;&#23548;&#33322;&#12290;&#36890;&#36807;&#19981;&#21516;&#39118;&#38505;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10105v1 Announce Type: cross  Abstract: Recent research on mobile robot navigation has focused on socially aware navigation in crowded environments. However, existing methods do not adequately account for human robot interactions and demand accurate location information from omnidirectional sensors, rendering them unsuitable for practical applications. In response to this need, this study introduces a novel algorithm, BNBRL+, predicated on the partially observable Markov decision process framework to assess risks in unobservable areas and formulate movement strategies under uncertainty. BNBRL+ consolidates belief algorithms with Bayesian neural networks to probabilistically infer beliefs based on the positional data of humans. It further integrates the dynamics between the robot, humans, and inferred beliefs to determine the navigation paths and embeds social norms within the reward function, thereby facilitating socially aware navigation. Through experiments in various risk
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaRand&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#29305;&#24449;&#21521;&#37327;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#38656;&#35201;&#36741;&#21161;&#28304;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.10097</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#33258;&#36866;&#24212;&#38543;&#26426;&#29305;&#24449;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaRand&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#29305;&#24449;&#21521;&#37327;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#38656;&#35201;&#36741;&#21161;&#28304;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24494;&#35843;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#20107;&#23454;&#26631;&#20934;&#26041;&#27861;&#65292;&#20294;&#22312;&#20351;&#29992;&#23567;&#22411;&#30446;&#26631;&#25968;&#25454;&#38598;&#26102;&#20173;&#28982;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#20445;&#25345;&#23545;&#28304;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#25110;&#24341;&#20837;&#35832;&#22914;&#23545;&#27604;&#25439;&#22833;&#20043;&#31867;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36741;&#21161;&#28304;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#28304;&#26631;&#31614;&#25110;&#25968;&#25454;&#38598;&#65289;&#25110;&#37325;&#22797;&#38468;&#21152;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#36866;&#24212;&#38543;&#26426;&#29305;&#24449;&#27491;&#21017;&#21270;&#65288;AdaRand&#65289;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;AdaRand&#21487;&#20197;&#24110;&#21161;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#22312;&#27809;&#26377;&#36741;&#21161;&#28304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36866;&#24230;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#24449;&#21521;&#37327;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;AdaRand&#36890;&#36807;&#26368;&#23567;&#21270;&#29305;&#24449;&#21521;&#37327;&#21644;&#20174;&#31867;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38543;&#26426;&#21442;&#32771;&#21521;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10097v1 Announce Type: cross  Abstract: While fine-tuning is a de facto standard method for training deep neural networks, it still suffers from overfitting when using small target datasets. Previous methods improve fine-tuning performance by maintaining knowledge of the source datasets or introducing regularization terms such as contrastive loss. However, these methods require auxiliary source information (e.g., source labels or datasets) or heavy additional computations. In this paper, we propose a simple method called adaptive random feature regularization (AdaRand). AdaRand helps the feature extractors of training models to adaptively change the distribution of feature vectors for downstream classification tasks without auxiliary source information and with reasonable computation costs. To this end, AdaRand minimizes the gap between feature vectors and random reference vectors that are sampled from class conditional Gaussian distributions. Furthermore, AdaRand dynamicall
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;CoARL&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#31038;&#20250;&#20559;&#35265;&#20013;&#30340;&#35821;&#29992;&#21551;&#31034;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35328;&#35770;&#29983;&#25104;&#65292;&#21033;&#29992;&#39034;&#24207;&#22810;&#25351;&#23548;&#35843;&#33410;&#21644;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#24847;&#22270;&#35843;&#33410;&#30340;&#23545;&#25239;&#24615;&#35328;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.10088</link><description>&lt;p&gt;
&#21033;&#29992;RLAIF&#36827;&#34892;&#24847;&#22270;&#35843;&#33410;&#21644;&#26080;&#27602;&#23545;&#25239;&#29983;&#25104;&#30340;&#22810;&#20219;&#21153;&#25351;&#23548;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;CoARL&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#31038;&#20250;&#20559;&#35265;&#20013;&#30340;&#35821;&#29992;&#21551;&#31034;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35328;&#35770;&#29983;&#25104;&#65292;&#21033;&#29992;&#39034;&#24207;&#22810;&#25351;&#23548;&#35843;&#33410;&#21644;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#24847;&#22270;&#35843;&#33410;&#30340;&#23545;&#25239;&#24615;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#35328;&#35770;&#34987;&#23450;&#20041;&#20026;&#20943;&#32531;&#32593;&#32476;&#20167;&#24680;&#35328;&#35770;&#30340;&#22238;&#24212;&#65292;&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#19968;&#31181;&#38750;&#23457;&#26597;&#35299;&#20915;&#26041;&#26696;&#12290;&#26377;&#25928;&#24212;&#23545;&#20167;&#24680;&#35328;&#35770;&#28041;&#21450;&#28040;&#38500;&#36890;&#24120;&#22312;&#31616;&#30701;&#30340;&#21333;&#21477;&#38472;&#36848;&#25110;&#34384;&#24453;&#20013;&#26263;&#31034;&#30340;&#21051;&#26495;&#21360;&#35937;&#12289;&#20559;&#35265;&#21644;&#20559;&#35265;&#12290;&#36825;&#20123;&#38544;&#21547;&#30340;&#34920;&#36798;&#25361;&#25112;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;seq2seq&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#27169;&#22411;&#24615;&#33021;&#36890;&#24120;&#22312;&#26356;&#38271;&#19978;&#19979;&#25991;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;CoARL&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#22312;&#20167;&#24680;&#35328;&#35770;&#20013;&#26263;&#21547;&#30340;&#31038;&#20250;&#20559;&#35265;&#30340;&#23454;&#29992;&#21547;&#20041;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35328;&#35770;&#29983;&#25104;&#12290;CoARL&#30340;&#21069;&#20004;&#20010;&#38454;&#27573;&#28041;&#21450;&#39034;&#24207;&#22810;&#25351;&#23548;&#35843;&#33410;&#65292;&#25945;&#23548;&#27169;&#22411;&#29702;&#35299;&#25915;&#20987;&#24615;&#38472;&#36848;&#30340;&#24847;&#22270;&#12289;&#21453;&#24212;&#21644;&#21361;&#23475;&#65292;&#28982;&#21518;&#23398;&#20064;&#29983;&#25104;&#24847;&#22270;&#35843;&#33410;&#30340;&#23545;&#25239;&#24615;&#35328;&#35770;&#30340;&#29305;&#23450;&#20219;&#21153;&#20302;&#31209;&#36866;&#37197;&#22120;&#26435;&#37325;&#12290;&#26368;&#21518;&#19968;&#20010;&#38454;&#27573;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23545;&#36755;&#20986;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#25928;&#26524;&#21644;&#26080;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10088v1 Announce Type: cross  Abstract: Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. Addressing hate speech effectively involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These implicit expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. CoARL's first two phases involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses reinforcement learning to fine-tune outputs for effectiveness and non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#27979;&#35797;&#31243;&#24207;&#65292;&#20197;&#20248;&#21270;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10086</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38024;&#23545;&#38750;&#21151;&#33021;&#23646;&#24615;&#30340;&#31995;&#32479;&#32423;&#27979;&#35797;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#27979;&#35797;&#31243;&#24207;&#65292;&#20197;&#20248;&#21270;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#32423;&#27979;&#35797;&#65288;SLT&#65289;&#24050;&#32463;&#25104;&#20026;&#38598;&#25104;&#30005;&#36335;&#27979;&#35797;&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#36229;&#36807;&#21313;&#24180;&#65292;&#24182;&#19988;&#20173;&#28982;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#38024;&#23545;&#27979;&#35797;&#31243;&#24207;&#29983;&#25104;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#34987;&#27979;&#35774;&#22791;&#65288;DUT&#65289;&#30340;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#27979;&#35797;&#31243;&#24207;&#12290;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20102;&#39044;&#35757;&#32451;&#30340;LLMs&#22312;&#27979;&#35797;&#31243;&#24207;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#65292;&#20197;&#20248;&#21270;DUT&#30340;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10086v1 Announce Type: cross  Abstract: System-Level Test (SLT) has been a part of the test flow for integrated circuits for over a decade and still gains importance. However, no systematic approaches exist for test program generation, especially targeting non-functional properties of the Device under Test (DUT). Currently, test engineers manually compose test suites from off-the-shelf software, approximating the end-user environment of the DUT. This is a challenging and tedious task that does not guarantee sufficient control over non-functional properties. This paper proposes Large Language Models (LLMs) to generate test programs. We take a first glance at how pre-trained LLMs perform in test program generation to optimize non-functional properties of the DUT. Therefore, we write a prompt to generate C code snippets that maximize the instructions per cycle of a super-scalar, out-of-order architecture in simulation. Additionally, we apply prompt and hyperparameter optimizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38754;&#21521;&#29289;&#20307;&#30340;&#35270;&#35273;&#39044;&#27979;&#20013;&#30340;&#29289;&#29702;&#21160;&#21147;&#23398;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#29289;&#20307;&#20043;&#38388;&#30340;&#35270;&#35273;&#21160;&#21147;&#23398;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#65292;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#24863;&#30693;&#27169;&#22359;&#21644;&#21160;&#21147;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2403.10079</link><description>&lt;p&gt;
&#23398;&#20064;&#38754;&#21521;&#29289;&#20307;&#30340;&#35270;&#35273;&#39044;&#27979;&#20013;&#30340;&#29289;&#29702;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning Physical Dynamics for Object-centric Visual Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#38754;&#21521;&#29289;&#20307;&#30340;&#35270;&#35273;&#39044;&#27979;&#20013;&#30340;&#29289;&#29702;&#21160;&#21147;&#23398;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#29289;&#20307;&#20043;&#38388;&#30340;&#35270;&#35273;&#21160;&#21147;&#23398;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#65292;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#24863;&#30693;&#27169;&#22359;&#21644;&#21160;&#21147;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10079v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#27169;&#25311;&#35270;&#35273;&#22330;&#26223;&#30340;&#28508;&#22312;&#21160;&#21147;&#23398;&#24182;&#25512;&#26029;&#26410;&#26469;&#24773;&#20917;&#23545;&#20110;&#20154;&#31867;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#23581;&#35797;&#36171;&#20104;&#26234;&#33021;&#31995;&#32479;&#36825;&#31181;&#29289;&#29702;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#20687;&#32032;&#21040;&#20687;&#32032;&#30340;&#39044;&#27979;&#65292;&#36825;&#31181;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#21516;&#26102;&#32570;&#20047;&#23545;&#35270;&#39057;&#32972;&#21518;&#29289;&#29702;&#21160;&#21147;&#23398;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#38754;&#21521;&#29289;&#20307;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#38754;&#21521;&#29289;&#20307;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#29289;&#20307;&#20043;&#38388;&#30340;&#35270;&#35273;&#21160;&#21147;&#23398;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#24863;&#30693;&#27169;&#22359;&#21644;&#21160;&#21147;&#27169;&#22359;&#12290;&#24863;&#30693;&#27169;&#22359;&#29992;&#20110;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#22810;&#20010;&#29289;&#20307;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#38754;&#21521;&#29289;&#20307;&#30340;&#34920;&#31034;&#21512;&#25104;&#22270;&#20687;&#12290;&#21160;&#21147;&#27169;&#22359;&#34701;&#21512;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#36827;&#34892;&#29615;&#22659;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10079v1 Announce Type: cross  Abstract: The ability to model the underlying dynamics of visual scenes and reason about the future is central to human intelligence. Many attempts have been made to empower intelligent systems with such physical understanding and prediction abilities. However, most existing methods focus on pixel-to-pixel prediction, which suffers from heavy computational costs while lacking a deep understanding of the physical dynamics behind videos. Recently, object-centric prediction methods have emerged and attracted increasing interest. Inspired by it, this paper proposes an unsupervised object-centric prediction model that makes future predictions by learning visual dynamics between objects. Our model consists of two modules, perceptual, and dynamic module. The perceptual module is utilized to decompose images into several objects and synthesize images with a set of object-centric representations. The dynamic module fuses contextual information, takes env
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#20027;&#21160;&#24494;&#35843;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#36873;&#25321;&#36807;&#31243;&#26469;&#36873;&#25321;&#27880;&#37322;&#26679;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#26680;&#24515;&#26679;&#26412;&#36873;&#25321;&#20197;&#30830;&#20445;&#22810;&#26679;&#24615;&#21644;&#36793;&#30028;&#26679;&#26412;&#36873;&#25321;&#20197;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10069</link><description>&lt;p&gt;
&#36793;&#30028;&#38382;&#39064;&#65306;&#19968;&#20010;&#21452;&#23618;&#20027;&#21160;&#24494;&#35843;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Boundary Matters: A Bi-Level Active Finetuning Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10069
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#20027;&#21160;&#24494;&#35843;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#36873;&#25321;&#36807;&#31243;&#26469;&#36873;&#25321;&#27880;&#37322;&#26679;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#26680;&#24515;&#26679;&#26412;&#36873;&#25321;&#20197;&#30830;&#20445;&#22810;&#26679;&#24615;&#21644;&#36793;&#30028;&#26679;&#26412;&#36873;&#25321;&#20197;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#35270;&#35273;&#20219;&#21153;&#21644;&#20854;&#20182;&#39046;&#22495;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#38754;&#20020;&#39640;&#26679;&#26412;&#27880;&#37322;&#25104;&#26412;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#25361;&#25112;&#65292;&#20986;&#29616;&#20102;&#20027;&#21160;&#24494;&#35843;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#26377;&#38480;&#39044;&#31639;&#20869;&#36873;&#25321;&#27169;&#22411;&#24494;&#35843;&#30340;&#26368;&#21512;&#36866;&#26679;&#26412;&#12290;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#24448;&#24448;&#38754;&#20020;&#25209;&#37327;&#36873;&#25321;&#20013;&#30340;&#22266;&#26377;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#20027;&#21160;&#24494;&#35843;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#25152;&#36873;&#25321;&#30340;&#23376;&#38598;&#20998;&#24067;&#19982;&#25972;&#20307;&#25968;&#25454;&#27744;&#19968;&#33268;&#65292;&#20165;&#20851;&#27880;&#22810;&#26679;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#20027;&#21160;&#24494;&#35843;&#26694;&#26550;&#65292;&#22312;&#19968;&#27425;&#36873;&#25321;&#36807;&#31243;&#20013;&#36873;&#25321;&#27880;&#37322;&#26679;&#26412;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#29992;&#20110;&#22810;&#26679;&#24615;&#30340;&#26680;&#24515;&#26679;&#26412;&#36873;&#25321;&#21644;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#36793;&#30028;&#26679;&#26412;&#36873;&#25321;&#12290;&#35813;&#36807;&#31243;&#20174;&#35782;&#21035;&#20266;&#31867;&#20013;&#24515;&#24320;&#22987;&#65292;&#28982;&#21518;&#36827;&#34892;&#21019;&#26032;&#24615;&#30340;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10069v1 Announce Type: cross  Abstract: The pretraining-finetuning paradigm has gained widespread adoption in vision tasks and other fields, yet it faces the significant challenge of high sample annotation costs. To mitigate this, the concept of active finetuning has emerged, aiming to select the most appropriate samples for model finetuning within a limited budget. Traditional active learning methods often struggle in this setting due to their inherent bias in batch selection. Furthermore, the recent active finetuning approach has primarily concentrated on aligning the distribution of selected subsets with the overall data pool, focusing solely on diversity. In this paper, we propose a Bi-Level Active Finetuning framework to select the samples for annotation in one shot, which includes two stages: core sample selection for diversity, and boundary sample selection for uncertainty. The process begins with the identification of pseudo-class centers, followed by an innovative d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;Frank-Wolfe&#31867;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#24182;&#22312;&#21333;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26080;&#25237;&#24433;&#31639;&#27861;&#30340;&#26368;&#26032;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.10063</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#29992;&#20110;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;Frank-Wolfe&#31867;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#24182;&#22312;&#21333;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26080;&#25237;&#24433;&#31639;&#27861;&#30340;&#26368;&#26032;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#30340;&#26080;&#25237;&#24433;Frank-Wolfe&#31867;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#24615;&#36830;&#32493;DR-&#27425;&#27169;&#20248;&#21270;&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#20840;&#20449;&#24687;&#21644;&#65288;&#21322;&#65289;&#24378;&#25932;&#21453;&#39304;&#12289;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#20989;&#25968;&#12289;&#19981;&#21516;&#32422;&#26463;&#20197;&#21450;&#31867;&#22411;&#30340;&#38543;&#26426;&#26597;&#35810;&#31561;&#22330;&#26223;&#12290;&#22312;&#38750;&#21333;&#35843;&#35774;&#32622;&#20013;&#32771;&#34385;&#30340;&#27599;&#20010;&#38382;&#39064;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#35201;&#20040;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#35777;&#26126;&#30340;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#30340;&#31639;&#27861;&#65292;&#35201;&#20040;&#20855;&#26377;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#20854;&#20013; $\alpha$ &#26159;&#31163;&#32447;&#35774;&#32622;&#20013;&#30340;&#30456;&#24212;&#36817;&#20284;&#19978;&#30028;&#12290;&#22312;&#21333;&#35843;&#35774;&#32622;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;8&#20010;&#32771;&#34385;&#30340;&#24773;&#20917;&#20013;&#30340;7&#31181;&#20013;&#26159;&#26080;&#25237;&#24433;&#31639;&#27861;&#30340;&#26368;&#26032;&#27425;&#32447;&#24615; $\alpha$-&#21518;&#24724;&#19978;&#30028;&#65292;&#21516;&#26102;&#19982;&#21097;&#20313;&#24773;&#20917;&#30340;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;DR-&#27425;&#27169;&#20248;&#21270;&#30340;&#21322;&#24378;&#25932;&#21644;&#24378;&#25932;&#21453;&#39304;&#65292;&#25512;&#36827;&#20102;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10063v1 Announce Type: cross  Abstract: This paper introduces unified projection-free Frank-Wolfe type algorithms for adversarial continuous DR-submodular optimization, spanning scenarios such as full information and (semi-)bandit feedback, monotone and non-monotone functions, different constraints, and types of stochastic queries. For every problem considered in the non-monotone setting, the proposed algorithms are either the first with proven sub-linear $\alpha$-regret bounds or have better $\alpha$-regret bounds than the state of the art, where $\alpha$ is a corresponding approximation bound in the offline setting. In the monotone setting, the proposed approach gives state-of-the-art sub-linear $\alpha$-regret bounds among projection-free algorithms in 7 of the 8 considered cases while matching the result of the remaining case. Additionally, this paper addresses semi-bandit and bandit feedback for adversarial DR-submodular optimization, advancing the understanding of this
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#21644;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#21644;&#20943;&#36731;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.10056</link><description>&lt;p&gt;
&#19981;&#35201;&#21322;&#24515;&#21322;&#24847;&#65306;&#25429;&#25417;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#20013;&#30340;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10056
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#21644;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#21644;&#20943;&#36731;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10056v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#39537;&#20351;&#23427;&#20204;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20013;&#20135;&#29983;&#31526;&#21512;&#20154;&#31867;&#30446;&#26631;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#65288;CIT&#65289;&#36807;&#31243;&#21487;&#33021;&#20250;&#24102;&#26469;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#38382;&#39064;&#65292;&#23548;&#33268;&#20808;&#21069;&#23398;&#21040;&#30340;&#33021;&#21147;&#36864;&#21270;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#20462;&#25913;&#27169;&#22411;&#25110;&#37325;&#25918;&#25968;&#25454;&#26469;&#32531;&#35299;CF&#38382;&#39064;&#65292;&#20294;&#36825;&#21487;&#33021;&#21482;&#35760;&#20303;&#25351;&#20196;&#30340;&#34920;&#38754;&#27169;&#24335;&#24182;&#22312;&#30041;&#23384;&#20219;&#21153;&#19978;&#24863;&#21040;&#22256;&#24785;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#65288;KPIG&#65289;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25513;&#30422;&#37096;&#20998;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#24182;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20174;&#32780;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#19982;&#27491;&#30830;&#21709;&#24212;&#30456;&#20851;&#30340;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#65292;&#24182;&#20943;&#36731;&#23545;&#25351;&#23548;&#20013;&#36890;&#29992;&#25551;&#36848;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25351;&#26631;&#65292;P&#20998;&#21644;V&#20998;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10056v1 Announce Type: cross  Abstract: Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score,
&lt;/p&gt;</description></item><item><title>PPM&#26159;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#39044;&#35757;&#32451;&#25554;&#20214;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#22312;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.10049</link><description>&lt;p&gt;
PPM&#65306;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#39044;&#35757;&#32451;&#25554;&#20214;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10049
&lt;/p&gt;
&lt;p&gt;
PPM&#26159;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#39044;&#35757;&#32451;&#25554;&#20214;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#22312;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10049v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#23398;&#31185; &#25688;&#35201;: &#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#65288;&#31616;&#31216;&#20026;IDRec&#65289;&#20381;&#36182;&#20110;&#21807;&#19968;&#36523;&#20221;&#26469;&#34920;&#31034;&#20960;&#21313;&#24180;&#26469;&#30427;&#34892;&#30340;&#19981;&#21516;&#29992;&#25143;&#21644;&#29289;&#21697;&#12290;&#19968;&#26041;&#38754;&#65292;IDRec&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#19978;&#32463;&#24120;&#38754;&#20020;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#36845;&#20195;&#25928;&#29575;&#30340;&#32422;&#26463;&#65292;IDRec&#26080;&#27861;&#20351;&#29992;&#26356;&#38271;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#39044;&#35757;&#32451;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#39044;&#35757;&#32451;&#29992;&#25143;&#27169;&#22411;&#25110;&#22810;&#27169;&#24577;&#23884;&#20837;&#65289;&#26469;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#32447;&#24310;&#36831;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21487;&#20197;&#24402;&#22240;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24040;&#22823;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#26080;&#27861;&#22312;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#19982;IDRec&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\textbf{P}$re-trained $\textbf{P}$lug-in CTR $\textbf{M}$odel&#65292;&#21363;PPM&#12290;PPM em
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10049v1 Announce Type: cross  Abstract: Click-through rate (CTR) prediction is a core task in recommender systems. Existing methods (IDRec for short) rely on unique identities to represent distinct users and items that have prevailed for decades. On one hand, IDRec often faces significant performance degradation on cold-start problem; on the other hand, IDRec cannot use longer training data due to constraints imposed by iteration efficiency. Most prior studies alleviate the above problems by introducing pre-trained knowledge(e.g. pre-trained user model or multi-modal embeddings). However, the explosive growth of online latency can be attributed to the huge parameters in the pre-trained model. Therefore, most of them cannot employ the unified model of end-to-end training with IDRec in industrial recommender systems, thus limiting the potential of the pre-trained model. To this end, we propose a $\textbf{P}$re-trained $\textbf{P}$lug-in CTR $\textbf{M}$odel, namely PPM. PPM em
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASK&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#36890;&#36807;&#38750;&#35328;&#35821;&#20114;&#21160;&#19982;&#35266;&#20247;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#29366;&#24577;&#26426;&#32467;&#26500;&#35843;&#25972;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#23454;&#29616;&#22810;&#31181;&#19981;&#21516;&#35282;&#33394;&#30340;&#21160;&#24577;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2403.10041</link><description>&lt;p&gt;
&#22312;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#20013;&#23884;&#20837;&#21160;&#24577;&#35282;&#33394;: &#20266;&#35013;&#21160;&#30011;&#31038;&#20132;&#36816;&#21160;&#23398;&#65288;MASK&#65289;
&lt;/p&gt;
&lt;p&gt;
Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematics (MASK)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10041
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASK&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#36890;&#36807;&#38750;&#35328;&#35821;&#20114;&#21160;&#19982;&#35266;&#20247;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#29366;&#24577;&#26426;&#32467;&#26500;&#35843;&#25972;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#23454;&#29616;&#22810;&#31181;&#19981;&#21516;&#35282;&#33394;&#30340;&#21160;&#24577;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#65292;&#20197;&#22686;&#24378;&#35266;&#20247;&#21442;&#19982;&#24230;&#65292;&#20351;&#29992;&#31867;&#20284;&#35282;&#33394;&#30340;&#20154;&#29289;&#24418;&#35937;&#12290;&#22522;&#20110;&#20197;&#35282;&#33394;&#20026;&#39537;&#21160;&#30340;&#23545;&#35805;&#20195;&#29702;&#31995;&#32479;&#65292;&#26412;&#30740;&#31350;&#23558;&#35813;&#20195;&#29702;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;&#29289;&#29702;&#39046;&#22495;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#25552;&#20379;&#26356;&#20855;&#27785;&#28024;&#24863;&#21644;&#20114;&#21160;&#20307;&#39564;&#12290;&#25552;&#20986;&#30340;&#31995;&#32479;&#21517;&#20026;Masquerading Animated Social Kinematics (MASK)&#65292;&#21033;&#29992;&#31867;&#20154;&#26426;&#22120;&#20154;&#36890;&#36807;&#38750;&#35328;&#35821;&#20114;&#21160;&#19982;&#23458;&#20154;&#20114;&#21160;&#65292;&#21253;&#25324;&#38754;&#37096;&#34920;&#24773;&#21644;&#25163;&#21183;&#12290;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#29366;&#24577;&#26426;&#32467;&#26500;&#30340;&#34892;&#20026;&#29983;&#25104;&#31995;&#32479;&#26377;&#25928;&#22320;&#35843;&#25972;&#26426;&#22120;&#20154;&#34892;&#20026;&#20197;&#20256;&#36798;&#19981;&#21516;&#30340;&#20154;&#29289;&#35282;&#33394;&#12290;MASK&#26694;&#26550;&#38598;&#25104;&#20102;&#24863;&#30693;&#24341;&#25806;&#12289;&#34892;&#20026;&#36873;&#25321;&#24341;&#25806;&#21644;&#32508;&#21512;&#21160;&#20316;&#24211;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#34892;&#20026;&#35774;&#35745;&#20013;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#24178;&#39044;&#22320;&#23454;&#29616;&#23454;&#26102;&#12289;&#21160;&#24577;&#20114;&#21160;&#12290;&#22312;&#29992;&#25143;&#23545;&#35937;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#25506;&#35752;&#20102;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#20197;&#28608;&#21457;&#26410;&#26469;&#30740;&#31350;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10041v1 Announce Type: cross  Abstract: This paper presents the design and development of an innovative interactive robotic system to enhance audience engagement using character-like personas. Built upon the foundations of persona-driven dialog agents, this work extends the agent application to the physical realm, employing robots to provide a more immersive and interactive experience. The proposed system, named the Masquerading Animated Social Kinematics (MASK), leverages an anthropomorphic robot which interacts with guests using non-verbal interactions, including facial expressions and gestures. A behavior generation system based upon a finite-state machine structure effectively conditions robotic behavior to convey distinct personas. The MASK framework integrates a perception engine, a behavior selection engine, and a comprehensive action library to enable real-time, dynamic interactions with minimal human intervention in behavior design. Throughout the user subject studi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#26080;&#30417;&#30563;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#30001;&#20110;&#20302;&#36136;&#37327;&#20809;&#27969;&#32780;&#24341;&#36215;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#37325;&#31574;&#30053;&#65306;&#30452;&#25509;&#20174;&#20809;&#27969;&#20013;&#25552;&#21462;&#36793;&#30028;&#12289;&#36873;&#25321;&#24615;&#20002;&#24323;&#36136;&#37327;&#36739;&#24046;&#30340;&#24103;&#12289;&#20197;&#21450;&#21033;&#29992;&#21487;&#21464;&#24103;&#29575;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#35780;&#20272;&#65292;&#23637;&#31034;&#20986;&#26377;&#21069;&#26223;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.10039</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26080;&#30417;&#30563;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#20013;&#20302;&#36136;&#37327;&#20809;&#27969;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Low-quality Optical Flow in Unsupervised Surgical Instrument Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#26080;&#30417;&#30563;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#30001;&#20110;&#20302;&#36136;&#37327;&#20809;&#27969;&#32780;&#24341;&#36215;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#37325;&#31574;&#30053;&#65306;&#30452;&#25509;&#20174;&#20809;&#27969;&#20013;&#25552;&#21462;&#36793;&#30028;&#12289;&#36873;&#25321;&#24615;&#20002;&#24323;&#36136;&#37327;&#36739;&#24046;&#30340;&#24103;&#12289;&#20197;&#21450;&#21033;&#29992;&#21487;&#21464;&#24103;&#29575;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#35780;&#20272;&#65292;&#23637;&#31034;&#20986;&#26377;&#21069;&#26223;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#30340;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#19982;&#30417;&#30563;&#35774;&#32622;&#19981;&#21516;&#65292;&#26080;&#30417;&#30563;&#20998;&#21106;&#20027;&#35201;&#20381;&#36182;&#20110;&#36816;&#21160;&#32447;&#32034;&#65292;&#28982;&#32780;&#30001;&#20110;&#25163;&#26415;&#38236;&#22836;&#20013;&#20809;&#27969;&#36890;&#24120;&#27604;&#33258;&#28982;&#22330;&#26223;&#20013;&#30340;&#35201;&#20302;&#36136;&#37327;&#65292;&#36825;&#20123;&#36816;&#21160;&#32447;&#32034;&#24456;&#38590;&#35782;&#21035;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#21363;&#20351;&#38754;&#23545;&#20302;&#36136;&#37327;&#20809;&#27969;&#22266;&#26377;&#38480;&#21046;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19977;&#20010;&#26041;&#38754;&#20837;&#25163;&#65306;&#30452;&#25509;&#20174;&#20809;&#27969;&#20013;&#25552;&#21462;&#36793;&#30028;&#12289;&#26377;&#36873;&#25321;&#22320;&#20002;&#24323;&#36136;&#37327;&#36739;&#24046;&#30340;&#24103;&#12289;&#20197;&#21450;&#21033;&#29992;&#21487;&#21464;&#24103;&#29575;&#30340;&#24494;&#35843;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;EndoVis2017 VOS&#25968;&#25454;&#38598;&#21644;Endovis2017&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#31574;&#30053;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#27169;&#22411;&#23637;&#29616;&#20986;&#26377;&#21069;&#26223;&#30340;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#22343;&#20540;&#20132;&#21449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10039v1 Announce Type: cross  Abstract: Video-based surgical instrument segmentation plays an important role in robot-assisted surgeries. Unlike supervised settings, unsupervised segmentation relies heavily on motion cues, which are challenging to discern due to the typically lower quality of optical flow in surgical footage compared to natural scenes. This presents a considerable burden for the advancement of unsupervised segmentation techniques. In our work, we address the challenge of enhancing model performance despite the inherent limitations of low-quality optical flow. Our methodology employs a three-pronged approach: extracting boundaries directly from the optical flow, selectively discarding frames with inferior flow quality, and employing a fine-tuning process with variable frame rates. We thoroughly evaluate our strategy on the EndoVis2017 VOS dataset and Endovis2017 Challenge dataset, where our model demonstrates promising results, achieving a mean Intersection-o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MR-MT3&#27169;&#22411;&#26469;&#20943;&#23569;&#20048;&#22120;&#27844;&#28431;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#23384;&#20648;&#20445;&#30041;&#26426;&#21046;&#12289;&#20808;&#21069;&#20196;&#29260;&#37319;&#26679;&#21644;&#20196;&#29260;&#28151;&#27927;&#31561;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;Slakh2100&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.10024</link><description>&lt;p&gt;
MR-MT3:&#23384;&#20648;&#20445;&#30041;&#30340;&#22810;&#36712;&#38899;&#20048;&#36716;&#24405;&#20197;&#20943;&#23569;&#20048;&#22120;&#27844;&#28431;
&lt;/p&gt;
&lt;p&gt;
MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate Instrument Leakage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10024
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MR-MT3&#27169;&#22411;&#26469;&#20943;&#23569;&#20048;&#22120;&#27844;&#28431;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#23384;&#20648;&#20445;&#30041;&#26426;&#21046;&#12289;&#20808;&#21069;&#20196;&#29260;&#37319;&#26679;&#21644;&#20196;&#29260;&#28151;&#27927;&#31561;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;Slakh2100&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MT3&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;MT3&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20196;&#29260;&#30340;&#22810;&#20048;&#22120;&#33258;&#21160;&#38899;&#20048;&#36716;&#24405;&#27169;&#22411;&#12290;&#23613;&#31649;MT3&#24615;&#33021;&#26368;&#20808;&#36827;&#65292;&#20294;&#23384;&#22312;&#20048;&#22120;&#27844;&#28431;&#38382;&#39064;&#65292;&#21363;&#36716;&#24405;&#22312;&#19981;&#21516;&#20048;&#22120;&#20043;&#38388;&#29255;&#27573;&#21270;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MR-MT3&#65292;&#20854;&#20013;&#21253;&#25324;&#23384;&#20648;&#20445;&#30041;&#26426;&#21046;&#12289;&#20808;&#21069;&#20196;&#29260;&#37319;&#26679;&#21644;&#20196;&#29260;&#28151;&#27927;&#31561;&#22686;&#24378;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;Slakh2100&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#36215;&#22987;F1&#20998;&#25968;&#21644;&#20943;&#23569;&#30340;&#20048;&#22120;&#27844;&#28431;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#22810;&#20048;&#22120;&#36716;&#24405;F1&#20998;&#25968;&#65292;&#36824;&#24341;&#20837;&#20102;&#35832;&#22914;&#20048;&#22120;&#27844;&#28431;&#27604;&#29575;&#21644;&#20048;&#22120;&#26816;&#27979;F1&#20998;&#25968;&#31561;&#26032;&#25351;&#26631;&#65292;&#20197;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#36716;&#24405;&#36136;&#37327;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#36890;&#36807;&#22312;&#21333;&#20048;&#22120;&#21333;&#22768;&#36947;&#25968;&#25454;&#38598;&#65288;&#22914;ComMU&#21644;NSynth&#65289;&#19978;&#35780;&#20272;MT3&#26469;&#35780;&#20272;&#22495;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10024v1 Announce Type: cross  Abstract: This paper presents enhancements to the MT3 model, a state-of-the-art (SOTA) token-based multi-instrument automatic music transcription (AMT) model. Despite SOTA performance, MT3 has the issue of instrument leakage, where transcriptions are fragmented across different instruments. To mitigate this, we propose MR-MT3, with enhancements including a memory retention mechanism, prior token sampling, and token shuffling are proposed. These methods are evaluated on the Slakh2100 dataset, demonstrating improved onset F1 scores and reduced instrument leakage. In addition to the conventional multi-instrument transcription F1 score, new metrics such as the instrument leakage ratio and the instrument detection F1 score are introduced for a more comprehensive assessment of transcription quality. The study also explores the issue of domain overfitting by evaluating MT3 on single-instrument monophonic datasets such as ComMU and NSynth. The findings,
&lt;/p&gt;</description></item><item><title>NNCTC &#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36328;&#25216;&#26415;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20449;&#21495;&#22788;&#29702;&#32452;&#20214;&#36716;&#25442;&#20026;&#31070;&#32463;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#33258;&#20027;&#25512;&#23548;&#20986;&#26368;&#20339;&#30340;&#36890;&#20449;&#26377;&#25928;&#36733;&#33655;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24320;&#21457;&#22797;&#26434;&#24615;&#24182;&#23637;&#31034;&#20102;&#21487;&#25193;&#23637;&#28508;&#21147;</title><link>https://arxiv.org/abs/2403.10014</link><description>&lt;p&gt;
NNCTC: &#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#29289;&#29702;&#23618;&#36328;&#25216;&#26415;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
NNCTC: Physical Layer Cross-Technology Communication via Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10014
&lt;/p&gt;
&lt;p&gt;
NNCTC &#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36328;&#25216;&#26415;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20449;&#21495;&#22788;&#29702;&#32452;&#20214;&#36716;&#25442;&#20026;&#31070;&#32463;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#33258;&#20027;&#25512;&#23548;&#20986;&#26368;&#20339;&#30340;&#36890;&#20449;&#26377;&#25928;&#36733;&#33655;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24320;&#21457;&#22797;&#26434;&#24615;&#24182;&#23637;&#31034;&#20102;&#21487;&#25193;&#23637;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#25216;&#26415;&#36890;&#20449;(CTC)&#23454;&#29616;&#20102;&#19981;&#21516;&#26080;&#32447;&#25216;&#26415;&#20043;&#38388;&#30340;&#26080;&#32541;&#20132;&#20114;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#22522;&#20110;&#36870;&#36716;&#20256;&#36755;&#36335;&#24452;&#65292;&#20197;&#35782;&#21035;&#36866;&#24403;&#30340;&#26377;&#25928;&#36733;&#33655;&#26469;&#29983;&#25104;&#30446;&#26631;&#35774;&#22791;&#33021;&#35782;&#21035;&#30340;&#27874;&#24418;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#35768;&#22810;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#20381;&#36182;&#29305;&#23450;&#25216;&#26415;&#21644;&#38656;&#35201;&#22797;&#26434;&#31639;&#27861;&#26469;&#20943;&#36731;&#22833;&#30495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NNCTC&#65292;&#19968;&#20010;&#21463;&#35757;&#32451;&#30340;&#31070;&#32463;&#27169;&#22411;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#36866;&#24212;&#24615;&#21551;&#21457;&#19979;&#35774;&#35745;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36328;&#25216;&#26415;&#36890;&#20449;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;CTC&#31649;&#36947;&#20869;&#30340;&#20449;&#21495;&#22788;&#29702;&#32452;&#20214;&#36716;&#25442;&#20026;&#31070;&#32463;&#27169;&#22411;&#65292;NNCTC&#34987;&#35774;&#35745;&#20026;&#31471;&#21040;&#31471;&#35757;&#32451;&#32780;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;NNCTC&#31995;&#32479;&#33021;&#22815;&#33258;&#20027;&#25512;&#23548;&#20986;&#26368;&#20339;&#30340;CTC&#26377;&#25928;&#36733;&#33655;&#65292;&#26174;&#33879;&#20943;&#36731;&#20102;&#24320;&#21457;&#22797;&#26434;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#25193;&#23637;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10014v1 Announce Type: cross  Abstract: Cross-technology communication(CTC) enables seamless interactions between diverse wireless technologies. Most existing work is based on reversing the transmission path to identify the appropriate payload to generate the waveform that the target devices can recognize. However, this method suffers from many limitations, including dependency on specific technologies and the necessity for intricate algorithms to mitigate distortion. In this work, we present NNCTC, a Neural-Network-based Cross-Technology Communication framework inspired by the adaptability of trainable neural models in wireless communications. By converting signal processing components within the CTC pipeline into neural models, the NNCTC is designed for end-to-end training without requiring labeled data. This enables the NNCTC system to autonomously derive the optimal CTC payload, which significantly eases the development complexity and showcases the scalability potential 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#23436;&#20840;&#20108;&#36827;&#21046;&#28857;&#20113;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21387;&#32553;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#20026;1&#20301;&#20108;&#36827;&#21046;&#20540;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#28857;&#20113;&#22788;&#29702;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.09998</link><description>&lt;p&gt;
FBPT&#65306;&#19968;&#20010;&#23436;&#20840;&#20108;&#36827;&#21046;&#28857;&#20113;Transformer
&lt;/p&gt;
&lt;p&gt;
FBPT: A Fully Binary Point Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#23436;&#20840;&#20108;&#36827;&#21046;&#28857;&#20113;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21387;&#32553;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#20026;1&#20301;&#20108;&#36827;&#21046;&#20540;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#28857;&#20113;&#22788;&#29702;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Fully Binary Point Cloud Transformer&#65288;FBPT&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#21644;&#31227;&#21160;&#35774;&#22791;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#25193;&#23637;&#28508;&#21147;&#12290;&#36890;&#36807;&#23558;32&#20301;&#20840;&#31934;&#24230;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#21387;&#32553;&#20026;1&#20301;&#20108;&#36827;&#21046;&#20540;&#65292;&#25152;&#25552;&#20986;&#30340;&#20108;&#36827;&#21046;&#28857;&#20113;Transformer&#32593;&#32476;&#26174;&#33879;&#38477;&#20302;&#20102;&#29992;&#20110;&#28857;&#20113;&#22788;&#29702;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23384;&#20648;&#21344;&#29992;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#30456;&#36739;&#20110;&#20840;&#31934;&#24230;&#28857;&#20113;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#23436;&#20840;&#30340;&#20108;&#36827;&#21046;&#28857;&#20113;Transformer&#32593;&#32476;&#65292;&#20854;&#20013;&#38500;&#20102;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22359;&#22806;&#20854;&#20182;&#25152;&#26377;&#37096;&#20998;&#37117;&#26159;&#20108;&#36827;&#21046;&#30340;&#65292;&#20250;&#22312;&#37327;&#21270;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#30340;Q&#12289;K&#12289;V&#21644;&#33258;&#27880;&#24847;&#21147;&#28608;&#27963;&#26102;&#38754;&#20020;&#25361;&#25112;&#21644;&#29942;&#39048;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#31526;&#21512;&#31616;&#21333;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#19988;&#21487;&#33021;&#38543;&#36755;&#20837;&#25968;&#25454;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#22312;&#25105;&#20204;&#30340;&#32593;&#32476;&#20013;&#65292;&#20108;&#36827;&#21046;&#27880;&#24847;&#21147;&#27169;&#22359;&#32463;&#21382;&#20102;&#34928;&#20943;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09998v1 Announce Type: cross  Abstract: This paper presents a novel Fully Binary Point Cloud Transformer (FBPT) model which has the potential to be widely applied and expanded in the fields of robotics and mobile devices. By compressing the weights and activations of a 32-bit full-precision network to 1-bit binary values, the proposed binary point cloud Transformer network significantly reduces the storage footprint and computational resource requirements of neural network models for point cloud processing tasks, compared to full-precision point cloud networks. However, achieving a fully binary point cloud Transformer network, where all parts except the modules specific to the task are binary, poses challenges and bottlenecks in quantizing the activations of Q, K, V and self-attention in the attention module, as they do not adhere to simple probability distributions and can vary with input data. Furthermore, in our network, the binary attention module undergoes a degradation
&lt;/p&gt;</description></item><item><title>EfficientVMamba&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#27169;&#22411;&#21464;&#20307;&#65292;&#36890;&#36807;atrous-based selective scan&#26041;&#27861;&#23454;&#29616;&#20102;&#36731;&#37327;&#32423;&#27169;&#22411;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09977</link><description>&lt;p&gt;
EfficientVMamba: &#19968;&#31181;&#19987;&#20026;&#36731;&#37327;&#32423;&#35270;&#35273;Mamba&#35774;&#35745;&#30340;Atrous&#36873;&#25321;&#25195;&#25551;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09977
&lt;/p&gt;
&lt;p&gt;
EfficientVMamba&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#27169;&#22411;&#21464;&#20307;&#65292;&#36890;&#36807;atrous-based selective scan&#26041;&#27861;&#23454;&#29616;&#20102;&#36731;&#37327;&#32423;&#27169;&#22411;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#36731;&#37327;&#32423;&#27169;&#22411;&#24320;&#21457;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#35774;&#35745;&#19978;&#65292;&#20294;&#20173;&#38754;&#20020;&#25345;&#32493;&#25361;&#25112;&#12290;CNN&#22312;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#26041;&#38754;&#25797;&#38271;&#65292;&#20294;&#20250;&#25439;&#23475;&#20998;&#36776;&#29575;&#65292;&#32780;Transformers&#25552;&#20379;&#20840;&#23616;&#33539;&#22260;&#65292;&#20294;&#20250;&#22686;&#21152;&#35745;&#31639;&#38656;&#27714;$\mathcal{O}(N^2)$&#12290;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#36825;&#31181;&#25345;&#32493;&#26435;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#22914;Mamba&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#23558;&#20840;&#23616;&#20449;&#24687;&#25552;&#21462;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;$\mathcal{O}(N)$&#12290;&#22312;&#27492;&#21551;&#21457;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#25506;&#32034;&#35270;&#35273;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#36731;&#37327;&#32423;&#27169;&#22411;&#35774;&#35745;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EfficientVMamba&#30340;&#20840;&#26032;&#39640;&#25928;&#27169;&#22411;&#21464;&#20307;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;EfficientVMamba&#36890;&#36807;&#39640;&#25928;&#30340;&#36339;&#36291;&#37319;&#26679;&#38598;&#25104;&#20102;&#22522;&#20110;atrous&#30340;&#36873;&#25321;&#25195;&#25551;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09977v1 Announce Type: cross  Abstract: Prior efforts in light-weight model development mainly centered on CNN and Transformer-based designs yet faced persistent challenges. CNNs adept at local feature extraction compromise resolution while Transformers offer global reach but escalate computational demands $\mathcal{O}(N^2)$. This ongoing trade-off between accuracy and efficiency remains a significant hurdle. Recently, state space models (SSMs), such as Mamba, have shown outstanding performance and competitiveness in various tasks such as language modeling and computer vision, while reducing the time complexity of global information extraction to $\mathcal{O}(N)$. Inspired by this, this work proposes to explore the potential of visual state space models in light-weight model design and introduce a novel efficient model variant dubbed EfficientVMamba. Concretely, our EfficientVMamba integrates a atrous-based selective scan approach by efficient skip sampling, constituting bui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09974</link><description>&lt;p&gt;
GET&#65306;&#35299;&#38145;CLIP&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#21253;&#21547;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#26088;&#22312;&#20934;&#30830;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#24182;&#27491;&#30830;&#20998;&#31867;&#26087;&#31867;&#21035;&#65292;&#21033;&#29992;&#20174;&#26377;&#26631;&#31614;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#31867;&#21035;&#27010;&#24565;&#12290;&#24403;&#21069;&#30340;GCD&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#20449;&#24687;&#27169;&#24577;&#65292;&#23548;&#33268;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#31867;&#21035;&#30340;&#20998;&#31867;&#25928;&#26524;&#19981;&#20339;&#12290;&#34429;&#28982;&#26576;&#20123;&#31867;&#21035;&#22312;&#35270;&#35273;&#19978;&#23481;&#26131;&#28151;&#28102;&#65292;&#20294;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#21516;&#30340;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#25991;&#26412;&#20449;&#24687;&#24341;&#20837;&#21040;GCD&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#32570;&#20047;&#31867;&#21035;&#21517;&#31216;&#65292;&#20351;&#24471;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;TES&#21033;&#29992;CLIP&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#36825;&#19968;&#29305;&#24615;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#36716;&#25442;&#20026;CLIP&#25991;&#26412;&#27169;&#22411;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09974v1 Announce Type: cross  Abstract: Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their text information might be distinct, motivating us to introduce text information into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize text information. To tackle this challenging problem, in this paper, we propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.09963</link><description>&lt;p&gt;
&#22788;&#29702;&#22909;&#24744;&#30340;&#25552;&#31034;&#20559;&#35265;&#65281;&#35843;&#26597;&#21644;&#20943;&#36731;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#30340;&#25552;&#31034;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#21363;&#25552;&#31034;&#24448;&#24448;&#20250;&#24341;&#20837;&#23545;&#29305;&#23450;&#26631;&#31614;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#20869;&#37096;&#25552;&#31034;&#20559;&#35265;&#30340;&#31243;&#24230;&#21644;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22238;&#24212;&#36825;&#19968;&#28857;&#65292;&#26412;&#25991;&#37327;&#21270;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#23454;&#39564;&#20013;&#30340;&#25152;&#26377;&#25552;&#31034;&#37117;&#34920;&#29616;&#20986;&#19981;&#21487;&#24573;&#35270;&#30340;&#20559;&#35265;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25552;&#31034;&#22914;AutoPrompt&#21644;OptiPrompt&#26174;&#31034;&#20986;&#26356;&#39640;&#27700;&#24179;&#30340;&#20559;&#35265;&#65307;2&#65289;&#25552;&#31034;&#20559;&#35265;&#21487;&#20197;&#36890;&#36807;&#36807;&#24230;&#25311;&#21512;&#27979;&#35797;&#25968;&#25454;&#38598;&#19981;&#21512;&#29702;&#22320;&#25918;&#22823;&#22522;&#20934;&#27979;&#35797;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;LAMA&#36825;&#26679;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#25552;&#31034;&#20559;&#35265;&#65292;&#22312;&#25512;&#26029;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20165;&#25552;&#31034;&#26597;&#35810;&#26469;&#20272;&#35745;&#26377;&#20559;&#24046;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20174;&#20013;&#21024;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09963v1 Announce Type: cross  Abstract: Recent research shows that pre-trained language models (PLMs) suffer from "prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce biases toward specific labels. However, the extent and impact of prompt bias within the model remain underexplored. In response, this paper quantifies the bias with various types of prompts and assesses their impact on different benchmarks. We show that: 1) all prompts in the experiments exhibit non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the prompt bias during inference time. Specifically, we first estimate the biased representation using prompt-only querying, and then remove it from the 
&lt;/p&gt;</description></item><item><title>RadCLIP&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#21253;&#21547;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#30340;&#26032;&#39062;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#24182;&#20351;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.09948</link><description>&lt;p&gt;
RadCLIP: &#36890;&#36807;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#22686;&#24378;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09948
&lt;/p&gt;
&lt;p&gt;
RadCLIP&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#21253;&#21547;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#30340;&#26032;&#39062;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#24182;&#20351;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09948v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#19982;&#25918;&#23556;&#23398;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#21307;&#23398;&#35786;&#26029;&#39046;&#22495;&#30340;&#21464;&#38761;&#26102;&#20195;&#12290;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#24050;&#34987;&#37319;&#29992;&#26469;&#22686;&#24378;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#25918;&#23556;&#23398;&#22270;&#20687;&#30340;&#29420;&#29305;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#23545;2D&#21644;3D&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#35299;&#35835;&#65292;&#24102;&#26469;&#20102;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#24212;&#23545;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#36890;&#29992;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#21307;&#23398;&#25104;&#20687;&#25152;&#38656;&#30340;&#35786;&#26029;&#31934;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RadCLIP&#65306;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26469;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;RadCLIP&#21253;&#21547;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#19987;&#20026;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#65292;&#20351;&#29992;&#20102;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RadCLIP&#33021;&#26377;&#25928;&#22320;&#23545;&#40784;&#25918;&#23556;&#23398;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09948v1 Announce Type: cross  Abstract: The integration of artificial intelligence (AI) with radiology has marked a transformative era in medical diagnostics. Vision foundation models have been adopted to enhance radiologic imaging analysis. However, the distinct complexities of radiological imaging, including the interpretation of 2D and 3D radiological data, pose unique challenges that existing models, trained on general non-medical images, fail to address adequately. To bridge this gap and capitalize on the diagnostic precision required in medical imaging, we introduce RadCLIP: a pioneering cross-modal foundational model that harnesses Contrastive Language-Image Pre-training (CLIP) to refine radiologic image analysis. RadCLIP incorporates a novel 3D slice pooling mechanism tailored for volumetric image analysis and is trained using a comprehensive and diverse dataset of radiologic image-text pairs. Our evaluations demonstrate that RadCLIP effectively aligns radiological i
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#23545;&#25163;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#20855;&#26377;&#23545;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09940</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25163;&#30340;&#32852;&#37030;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#23545;&#25163;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#20855;&#26377;&#23545;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Federated Reinforcement Learning (FRL)&#20801;&#35768;&#22810;&#20010;&#20195;&#29702;&#20849;&#21516;&#26500;&#24314;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#21407;&#22987;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#20195;&#29702;&#20013;&#21482;&#26377;&#23569;&#37096;&#20998;&#26159;&#23545;&#25163;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#23545;&#25163;&#20195;&#29702;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#21521;&#26381;&#21153;&#22120;&#21457;&#36865;&#20219;&#24847;&#20540;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24418;&#25104;&#20102;&#20855;&#26377;&#19968;&#33324;&#21442;&#25968;&#21270;&#30340;&#39318;&#20010;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;&#23545;&#25163;&#30340;&#24377;&#24615;&#65292;&#21516;&#26102;&#36798;&#21040;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;$\tilde{\mathcal{O}}\left( \frac{1}{\epsilon^2} \left( \frac{1}{N-f} + \frac{f^2}{(N-f)^2}\right)\right)$&#65292;&#20854;&#20013;$N$&#26159;&#20195;&#29702;&#30340;&#24635;&#25968;&#65292;$f$&#26159;&#23545;&#25163;&#20195;&#29702;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09940v1 Announce Type: cross  Abstract: Federated Reinforcement Learning (FRL) allows multiple agents to collaboratively build a decision making policy without sharing raw trajectories. However, if a small fraction of these agents are adversarial, it can lead to catastrophic results. We propose a policy gradient based approach that is robust to adversarial agents which can send arbitrary values to the server. Under this setting, our results form the first global convergence guarantees with general parametrization. These results demonstrate resilience with adversaries, while achieving sample complexity of order $\tilde{\mathcal{O}}\left( \frac{1}{\epsilon^2} \left( \frac{1}{N-f} + \frac{f^2}{(N-f)^2}\right)\right)$, where $N$ is the total number of agents and $f$ is the number of adversarial agents.
&lt;/p&gt;</description></item><item><title>QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.09930</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65306;&#36890;&#36807;&#20540;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09930
&lt;/p&gt;
&lt;p&gt;
QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#34920;&#29616;&#20986;&#36866;&#24212;&#24847;&#22806;&#24773;&#20917;&#30340;&#24191;&#27867;&#34892;&#20026;&#35889;&#12290;&#36807;&#21435;&#21313;&#24180;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#27493;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#23601;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#36820;&#22238;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;QDAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#28436;&#21592;&#36890;&#36807;&#21463;&#38480;&#20248;&#21270;&#26469;&#26368;&#22823;&#21270;&#22238;&#25253;&#24182;&#25191;&#34892;&#22810;&#26679;&#24615;&#25216;&#33021;&#30340;&#23458;&#35266;&#20989;&#25968;&#65292;&#26080;&#32541;&#32479;&#19968;&#20102;&#20004;&#20010;&#35780;&#35770;&#23478;&#12290;&#19982;&#20854;&#20182;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#30456;&#27604;&#65292;QDAC&#22312;&#20845;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#36816;&#21160;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#22810;&#26679;&#24615;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09930v1 Announce Type: cross  Abstract: A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#36741;&#21161;&#20195;&#29702;&#27169;&#22411;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#21487;&#20197;&#26356;&#24555;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#19982;&#19981;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#33021;&#22815;&#20445;&#25345;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.09925</link><description>&lt;p&gt;
&#36741;&#21161;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Surrogate Assisted Monte Carlo Tree Search in Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09925
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36741;&#21161;&#20195;&#29702;&#27169;&#22411;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#21487;&#20197;&#26356;&#24555;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#19982;&#19981;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#33021;&#22815;&#20445;&#25345;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#19994;&#32463;&#24120;&#36890;&#36807;&#22312;&#26377;&#21069;&#26223;&#30340;&#22320;&#21306;&#24320;&#35774;&#26032;&#20998;&#25903;&#26426;&#26500;&#24182;&#20851;&#38381;&#20182;&#20204;&#39044;&#26399;&#20302;&#21033;&#28070;&#30340;&#22320;&#21306;&#30340;&#20998;&#25903;&#26426;&#26500;&#26469;&#35843;&#25972;&#20182;&#20204;&#30340;&#35774;&#26045;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#29305;&#23450;&#30340;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#30001;&#20110;&#20851;&#38381;&#20960;&#23478;&#38646;&#21806;&#24215;&#32780;&#23548;&#33268;&#30340;&#38144;&#21806;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#20272;&#35745;&#38144;&#21806;&#39069;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#36741;&#21161;&#19968;&#20010;&#33021;&#26356;&#24555;&#35745;&#31639;&#35780;&#20272;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#24555;&#36895;&#20195;&#29702;&#20989;&#25968;&#25903;&#25345;&#30340;MCTS&#21487;&#20197;&#26356;&#24555;&#22320;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#19982;&#19981;&#21033;&#29992;&#20195;&#29702;&#20989;&#25968;&#30340;MCTS&#30456;&#27604;&#65292;&#33021;&#20445;&#25345;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09925v1 Announce Type: new  Abstract: Industries frequently adjust their facilities network by opening new branches in promising areas and closing branches in areas where they expect low profits. In this paper, we examine a particular class of facility location problems. Our objective is to minimize the loss of sales resulting from the removal of several retail stores. However, estimating sales accurately is expensive and time-consuming. To overcome this challenge, we leverage Monte Carlo Tree Search (MCTS) assisted by a surrogate model that computes evaluations faster. Results suggest that MCTS supported by a fast surrogate function can generate solutions faster while maintaining a consistent solution compared to MCTS that does not benefit from the surrogate function.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#8220;Masked Siamese Network&#8221;&#65288;MSN&#65289;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35782;&#21035;&#26032;&#29616;&#35937;&#65292;&#24182;&#39044;&#27979;&#32467;&#32928;&#38236;&#27169;&#22411;&#23545;&#26410;&#30693;&#25216;&#26415;&#21644;&#19981;&#21516;&#22269;&#23478;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09920</link><description>&lt;p&gt;
&#39044;&#27979;AI&#32467;&#32928;&#38236;&#27169;&#22411;&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Predicting Generalization of AI Colonoscopy Models to Unseen Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09920
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#8220;Masked Siamese Network&#8221;&#65288;MSN&#65289;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35782;&#21035;&#26032;&#29616;&#35937;&#65292;&#24182;&#39044;&#27979;&#32467;&#32928;&#38236;&#27169;&#22411;&#23545;&#26410;&#30693;&#25216;&#26415;&#21644;&#19981;&#21516;&#22269;&#23478;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#21644;&#30446;&#26631; AI&#32467;&#32928;&#38236;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35780;&#20272;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#25216;&#26415;&#38656;&#35201;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#8220;Masked Siamese Network&#8221;&#65288;MSN&#65289;&#22312;&#26410;&#30693;&#25968;&#25454;&#20013;&#35782;&#21035;&#26032;&#29616;&#35937;&#24182;&#39044;&#27979;&#24687;&#32905;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;MSN&#34987;&#35757;&#32451;&#26469;&#39044;&#27979;&#24687;&#32905;&#22270;&#20687;&#20013;&#34987;&#23631;&#34109;&#30340;&#21306;&#22495;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#31614;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;MSN&#20165;&#22312;&#20197;&#33394;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#22312;&#26085;&#26412;&#32467;&#32928;&#38236;&#65288;354&#20010;&#35270;&#39057;&#65292;128&#23567;&#26102;&#65289;&#19978;&#26816;&#27979;&#26410;&#30693;&#25216;&#26415;&#65306;&#31364;&#24102;&#25104;&#20687;&#65288;NBI&#65289;&#21644;&#33394;&#32454;&#32990;&#20869;&#38236;&#65288;CE&#65289;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;MSN&#39044;&#27979;&#36328;&#22269;&#32467;&#32928;&#38236;&#35270;&#39057;&#19978;&#30340;&#24687;&#32905;&#35745;&#31639;&#26426;&#36741;&#21161;&#26816;&#27979;&#65288;CADe&#65289;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;MSN&#26410;&#25509;&#21463;&#36807;&#26469;&#33258;&#26085;&#26412;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09920v1 Announce Type: cross  Abstract: Background and aims Generalizability of AI colonoscopy algorithms is important for wider adoption in clinical practice. However, current techniques for evaluating performance on unseen data require expensive and time-intensive labels.   Methods We use a "Masked Siamese Network" (MSN) to identify novel phenomena in unseen data and predict polyp detector performance. MSN is trained to predict masked out regions of polyp images, without any labels. We test MSN's ability to be trained on data only from Israel and detect unseen techniques, narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes from Japan (354 videos, 128 hours). We also test MSN's ability to predict performance of Computer Aided Detection (CADe) of polyps on colonoscopies from both countries, even though MSN is not trained on data from Japan.   Results MSN correctly identifies NBI and CE as less similar to Israel whitelight than Japan whitelight (bootstrapped z-t
&lt;/p&gt;</description></item><item><title>FedComLoc&#21033;&#29992;Scaffnew&#31639;&#27861;&#30340;&#22522;&#30784;&#65292;&#24341;&#20837;&#20102;&#21387;&#32553;&#21644;&#26412;&#22320;&#35757;&#32451;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2403.09904</link><description>&lt;p&gt;
FedComLoc: &#31232;&#30095;&#21644;&#37327;&#21270;&#27169;&#22411;&#30340;&#36890;&#20449;&#39640;&#25928;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09904
&lt;/p&gt;
&lt;p&gt;
FedComLoc&#21033;&#29992;Scaffnew&#31639;&#27861;&#30340;&#22522;&#30784;&#65292;&#24341;&#20837;&#20102;&#21387;&#32553;&#21644;&#26412;&#22320;&#35757;&#32451;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30001;&#20110;&#20854;&#20801;&#35768;&#24322;&#26500;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#22788;&#29702;&#20854;&#31169;&#26377;&#25968;&#25454;&#24182;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20114;&#21160;&#65292;&#21516;&#26102;&#23562;&#37325;&#38544;&#31169;&#30340;&#29420;&#29305;&#29305;&#28857;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21463;&#21040;&#20102;&#21019;&#26032;&#30340;Scaffnew&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#35813;&#31639;&#27861;&#22312;FL&#20013;&#22823;&#22823;&#25512;&#21160;&#20102;&#36890;&#20449;&#22797;&#26434;&#24615;&#30340;&#38477;&#20302;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FedComLoc&#65288;&#32852;&#37030;&#21387;&#32553;&#21644;&#26412;&#22320;&#35757;&#32451;&#65289;&#65292;&#23558;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#21387;&#32553;&#38598;&#25104;&#21040;Scaffnew&#20013;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#27969;&#34892;&#30340;TopK&#21387;&#32553;&#22120;&#21644;&#37327;&#21270;&#65292;&#23427;&#22312;&#22823;&#24133;&#20943;&#23569;&#24322;&#26500;&#20013;&#30340;&#36890;&#20449;&#24320;&#38144;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09904v1 Announce Type: cross  Abstract: Federated Learning (FL) has garnered increasing attention due to its unique characteristic of allowing heterogeneous clients to process their private data locally and interact with a central server, while being respectful of privacy. A critical bottleneck in FL is the communication cost. A pivotal strategy to mitigate this burden is \emph{Local Training}, which involves running multiple local stochastic gradient descent iterations between communication phases. Our work is inspired by the innovative \emph{Scaffnew} algorithm, which has considerably advanced the reduction of communication complexity in FL. We introduce FedComLoc (Federated Compressed and Local Training), integrating practical and effective compression into \emph{Scaffnew} to further enhance communication efficiency. Extensive experiments, using the popular TopK compressor and quantization, demonstrate its prowess in substantially reducing communication overheads in heter
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09891</link><description>&lt;p&gt;
Fisher Mask&#33410;&#28857;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Fisher Mask Nodes for Language Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09891
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#21450;&#20854;&#34893;&#29983;&#29289;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26222;&#36941;&#24615;&#20063;&#23548;&#33268;&#20102;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#30340;&#28608;&#22686;&#12290;&#22312;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#19968;&#39033;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#38598;&#25104;&#12290;&#27169;&#22411;&#21512;&#24182;&#36825;&#19968;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21512;&#24182;&#20026;&#21333;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;Transformers&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20808;&#21069;Fisher&#21152;&#26435;&#24179;&#22343;&#21644;Fisher&#20449;&#24687;&#22312;&#27169;&#22411;&#20462;&#21098;&#20013;&#30340;&#24212;&#29992;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;Transformer&#26550;&#26500;&#20869;&#30340;mask&#33410;&#28857;&#30340;Fisher&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21152;&#26435;&#24179;&#22343;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09891v1 Announce Type: cross  Abstract: Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance
&lt;/p&gt;</description></item><item><title>Sabi'a-2&#26159;&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;Sabi'a-2 Medium&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;GPT-4&#65292;&#19988;&#22312;&#22823;&#22810;&#25968;&#32771;&#35797;&#20013;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#21516;&#26102;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#21487;&#22312;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#30340;&#24773;&#20917;&#19979;&#20197;&#27604;GPT-4&#20415;&#23452;10&#20493;&#30340;&#20215;&#26684;&#25552;&#20379;&#12290;</title><link>https://arxiv.org/abs/2403.09887</link><description>&lt;p&gt;
Sabi\'a-2:&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sabi\'a-2: A New Generation of Portuguese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09887
&lt;/p&gt;
&lt;p&gt;
Sabi'a-2&#26159;&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;Sabi'a-2 Medium&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;GPT-4&#65292;&#19988;&#22312;&#22823;&#22810;&#25968;&#32771;&#35797;&#20013;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#21516;&#26102;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#21487;&#22312;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#30340;&#24773;&#20917;&#19979;&#20197;&#27604;GPT-4&#20415;&#23452;10&#20493;&#30340;&#20215;&#26684;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Sabi'a-2&#65292;&#36825;&#26159;&#19968;&#26063;&#22312;&#33889;&#33796;&#29273;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#24052;&#35199;&#22823;&#23398;&#30340;&#20837;&#23398;&#32771;&#35797;&#12289;&#19987;&#19994;&#35748;&#35777;&#32771;&#35797;&#20197;&#21450;&#21508;&#31181;&#23398;&#31185;&#65288;&#22914;&#20250;&#35745;&#12289;&#32463;&#27982;&#23398;&#12289;&#24037;&#31243;&#23398;&#12289;&#27861;&#24459;&#21644;&#21307;&#23398;&#65289;&#30340;&#30740;&#31350;&#29983;&#20837;&#23398;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#25105;&#20204;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;Sabi'a-2 Medium&#65292;&#22312;64&#22330;&#32771;&#35797;&#20013;&#26377;23&#22330;&#19982;GPT-4&#30340;&#34920;&#29616;&#30456;&#21305;&#25932;&#25110;&#36229;&#36234;&#65292;&#24182;&#19988;&#22312;64&#22330;&#32771;&#35797;&#20013;&#26377;58&#22330;&#36229;&#36807;&#20102;GPT-3.5&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;Sabi'a-2 Medium&#65292;&#27599;&#20010;&#35760;&#21495;&#30340;&#20215;&#26684;&#27604;GPT-4&#20415;&#23452;10&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#23398;&#21644;&#32534;&#30721;&#26159;&#38656;&#35201;&#25913;&#36827;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09887v1 Announce Type: cross  Abstract: We introduce Sabi\'a-2, a family of large language models trained on Portuguese texts. The models are evaluated on a diverse range of exams, including entry-level tests for Brazilian universities, professional certification exams, and graduate-level exams for various disciplines such as accounting, economics, engineering, law and medicine. Our results reveal that our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64 exams. Notably, specialization has a significant impact on a model's performance without the need to increase its size, allowing us to offer Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4. Finally, we identified that math and coding are key abilities that need improvement.
&lt;/p&gt;</description></item><item><title>ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09871</link><description>&lt;p&gt;
ThermoHands&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20027;&#35266;&#35270;&#35282;&#28909;&#22270;&#20013;&#20272;&#35745;3D&#25163;&#37096;&#23039;&#21183;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09871
&lt;/p&gt;
&lt;p&gt;
ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ThermoHands&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#28909;&#22270;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#20811;&#26381;&#35832;&#22914;&#20809;&#29031;&#21464;&#21270;&#21644;&#36974;&#25377;&#65288;&#20363;&#22914;&#25163;&#37096;&#31359;&#25140;&#29289;&#65289;&#31561;&#25361;&#25112;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;28&#21517;&#20027;&#20307;&#36827;&#34892;&#25163;-&#29289;&#20307;&#21644;&#25163;-&#34394;&#25311;&#20132;&#20114;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#20934;&#30830;&#26631;&#27880;&#20102;3D&#25163;&#37096;&#23039;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#21033;&#29992;&#21452;transformer&#27169;&#22359;&#22312;&#28909;&#22270;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;TheFormer&#30340;&#39046;&#20808;&#24615;&#33021;&#65292;&#24182;&#30830;&#35748;&#20102;&#28909;&#25104;&#20687;&#22312;&#23454;&#29616;&#24694;&#21155;&#26465;&#20214;&#19979;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09871v1 Announce Type: cross  Abstract: In this work, we present ThermoHands, a new benchmark for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting and obstructions (e.g., handwear). The benchmark includes a diverse dataset from 28 subjects performing hand-object and hand-virtual interactions, accurately annotated with 3D hand poses through an automated process. We introduce a bespoken baseline method, TheFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TheFormer's leading performance and affirm thermal imaging's effectiveness in enabling robust 3D hand pose estimation in adverse conditions.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#26063;&#32452;&#24863;&#30693;&#20808;&#39564;&#20998;&#24067;&#65292;&#21487;&#20197;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#32676;&#20307;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#37325;&#26032;&#35757;&#32451;&#38750;&#40065;&#26834;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#65292;&#20351;&#29992;&#36825;&#31181;&#20808;&#39564;&#36827;&#34892;&#35757;&#32451;&#20063;&#33021;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09869</link><description>&lt;p&gt;
&#23545;&#20122;&#32676;&#20307;&#20559;&#31227;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#65306;&#20351;&#29992;&#32452;&#24863;&#30693;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Mind the GAP: Improving Robustness to Subpopulation Shifts with Group-Aware Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09869
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#26063;&#32452;&#24863;&#30693;&#20808;&#39564;&#20998;&#24067;&#65292;&#21487;&#20197;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#32676;&#20307;&#20559;&#31227;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#37325;&#26032;&#35757;&#32451;&#38750;&#40065;&#26834;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#65292;&#20351;&#29992;&#36825;&#31181;&#20808;&#39564;&#36827;&#34892;&#35757;&#32451;&#20063;&#33021;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#32676;&#20307;&#20559;&#31227;&#19979;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#24320;&#21457;&#33021;&#22815;&#35753;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#36825;&#31181;&#20559;&#31227;&#30340;&#26041;&#27861;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26063;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#32452;&#24863;&#30693;&#20808;&#39564;&#65288;GAP&#65289;&#20998;&#24067;&#65292;&#26126;&#30830;&#25903;&#25345;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#32676;&#20307;&#20559;&#31227;&#19979;&#27867;&#21270;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32452;&#24863;&#30693;&#20808;&#39564;&#65292;&#21482;&#38656;&#35201;&#35775;&#38382;&#19968;&#23567;&#37096;&#20998;&#21253;&#21547;&#32452;&#20449;&#24687;&#30340;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#22312;&#27492;&#20808;&#39564;&#19979;&#35757;&#32451;&#20250;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#8212;&#8212;&#21363;&#20351;&#21482;&#37325;&#26032;&#35757;&#32451;&#20808;&#21069;&#35757;&#32451;&#30340;&#38750;&#40065;&#26834;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#12290;&#32452;&#24863;&#30693;&#20808;&#39564;&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#23646;&#24615;&#20266;&#26631;&#35760;&#21644;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#65289;&#20114;&#34917;&#65292;&#20026;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#20197;&#23454;&#29616;&#23545;&#20122;&#32676;&#20307;&#20559;&#31227;&#30340;&#40065;&#26834;&#24615;&#24320;&#36767;&#20102;&#26377;&#21069;&#26223;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09869v1 Announce Type: cross  Abstract: Machine learning models often perform poorly under subpopulation shifts in the data distribution. Developing methods that allow machine learning models to better generalize to such shifts is crucial for safe deployment in real-world settings. In this paper, we develop a family of group-aware prior (GAP) distributions over neural network parameters that explicitly favor models that generalize well under subpopulation shifts. We design a simple group-aware prior that only requires access to a small set of data with group information and demonstrate that training with this prior yields state-of-the-art performance -- even when only retraining the final layer of a previously trained non-robust model. Group aware-priors are conceptually simple, complementary to existing approaches, such as attribute pseudo labeling and data reweighting, and open up promising new avenues for harnessing Bayesian inference to enable robustness to subpopulation
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09863</link><description>&lt;p&gt;
&#19968;&#20010;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Conceptual Framework For White Box Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09863
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#23436;&#20840;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#12290;&#19968;&#20010;&#20805;&#20998;&#21160;&#26426;&#30340;MNIST&#30456;&#20851;&#23376;&#38382;&#39064;&#30340;&#27010;&#24565;&#39564;&#35777;&#27169;&#22411;&#21253;&#25324;4&#20010;&#36825;&#26679;&#30340;&#23618;&#65292;&#24635;&#20849;4800&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#26131;&#20110;&#35299;&#37322;&#65292;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#23545;&#25239;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23545;&#25239;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#19978;&#24555;&#36895;&#35757;&#32451;&#12290;&#35813;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#25215;&#35834;&#20026;&#24443;&#24213;&#27665;&#20027;&#21270;&#21644;&#30495;&#27491;&#36890;&#29992;&#30340;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/314-Foundation/white-box-nn&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;IoT&#32593;&#20851;&#35774;&#22791;&#20013;&#29289;&#29702;&#23618;&#35843;&#21046;&#22120;&#30340;&#25277;&#35937;&#23618;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#30828;&#20214;&#21152;&#36895;&#21644;&#24322;&#26500;&#24179;&#21488;&#31227;&#26893;&#30340;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.09861</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#21046;&#22120;&#65306;IoT&#32593;&#20851;&#19978;&#21487;&#37325;&#26500;&#21644;&#21487;&#31227;&#26893;&#30340;&#36719;&#20214;&#35843;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
NN-Defined Modulator: Reconfigurable and Portable Software Modulator on IoT Gateways
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09861
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;IoT&#32593;&#20851;&#35774;&#22791;&#20013;&#29289;&#29702;&#23618;&#35843;&#21046;&#22120;&#30340;&#25277;&#35937;&#23618;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#30828;&#20214;&#21152;&#36895;&#21644;&#24322;&#26500;&#24179;&#21488;&#31227;&#26893;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#32593;&#20851;&#30340;&#29289;&#29702;&#23618;&#35843;&#21046;&#22120;&#26159;&#23558;&#31526;&#21495;&#26144;&#23556;&#21040;&#20449;&#21495;&#30340;&#37325;&#35201;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#20851;&#20027;&#26495;&#19978;&#28938;&#25509;&#30340;&#30828;&#20214;&#33455;&#29255;&#32452;&#20214;&#25110;&#19981;&#21516;&#24179;&#21488;&#19978;&#36719;&#20214;&#26080;&#32447;&#30005;&#30340;&#22810;&#26679;&#21270;&#24037;&#20855;&#21253;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20855;&#26377;&#26377;&#38480;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#35201;&#20040;&#29305;&#23450;&#20110;&#24179;&#21488;&#12290;&#22312;&#35843;&#21046;&#26041;&#26696;&#21644;&#30828;&#20214;&#24179;&#21488;&#21464;&#24471;&#26497;&#20026;&#22810;&#26679;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#38480;&#21046;&#24456;&#38590;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21363;&#22312;IoT&#32593;&#20851;&#35774;&#22791;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#29289;&#29702;&#23618;&#35843;&#21046;&#22120;&#30340;&#25277;&#35937;&#23618;&#65292;&#31216;&#20026;NN-defined modulators&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22312;&#21508;&#31181;&#30828;&#20214;&#24179;&#21488;&#19978;&#23454;&#29616;&#22810;&#31181;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;NN-defined modulator&#20351;&#29992;&#26681;&#26893;&#20110;&#22362;&#23454;&#25968;&#23398;&#22522;&#30784;&#30340;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#35770;&#65292;&#21516;&#26102;&#20855;&#26377;&#23545;&#30828;&#20214;&#21152;&#36895;&#30340;&#26412;&#22320;&#25903;&#25345;&#65292;&#21487;&#31227;&#26893;&#21040;&#24322;&#26500;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09861v1 Announce Type: cross  Abstract: A physical-layer modulator is a vital component for an IoT gateway to map the symbols to signals. However, due to the soldered hardware chipsets on the gateway's motherboards or the diverse toolkits on different platforms for the software radio, the existing solutions either have limited extensibility or are platform-specific. Such limitation is hard to ignore when modulation schemes and hardware platforms have become extremely diverse. This paper presents a new paradigm of using neural networks as an abstraction layer for physical layer modulators in IoT gateway devices, referred to as NN-defined modulators. Our approach addresses the challenges of extensibility and portability for multiple technologies on various hardware platforms. The proposed NN-defined modulator uses a model-driven methodology rooted in solid mathematical foundations while having native support for hardware acceleration and portability to heterogeneous platforms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.09857</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09857
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#27169;&#22411;&#26088;&#22312;&#22312;&#20445;&#30041;&#26087;&#31867;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#36880;&#27493;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#31232;&#32570;&#26679;&#26412;&#12290;&#29616;&#26377;&#30340;FSCIL&#26041;&#27861;&#36890;&#24120;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#24494;&#35843;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38459;&#30861;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#28508;&#21147;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#36817;&#22522;&#20110;&#25552;&#31034;&#30340;CIL&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#29992;&#36275;&#22815;&#30340;&#25968;&#25454;&#35757;&#32451;&#25552;&#31034;&#26469;&#20943;&#36731;&#36951;&#24536;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#65288;ASP&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;ASP&#36890;&#36807;&#20174;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;ASP&#20013;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#25552;&#20379;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;&#24635;&#20043;&#65292;ASP&#38450;&#27490;&#20102;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#30340;&#36807;&#25311;&#21512;&#65292;&#24182;&#19981;&#38656;&#35201;&#22312;&#23569;&#26679;&#26412;&#22686;&#37327;&#20219;&#21153;&#20013;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09857v1 Announce Type: cross  Abstract: Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensi
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33258;&#27965;&#24615;&#30340;&#26657;&#20934;&#26041;&#27861;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#31435;&#27169;&#22411;&#20449;&#24515;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2403.09849</link><description>&lt;p&gt;
&#33258;&#27965;&#24615;&#25552;&#21319;&#25968;&#23398;&#25512;&#29702;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Self-Consistency Boosts Calibration for Math Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09849
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#27965;&#24615;&#30340;&#26657;&#20934;&#26041;&#27861;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#31435;&#27169;&#22411;&#20449;&#24515;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26657;&#20934;&#65292;&#24314;&#31435;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#20449;&#24515;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#23545;LLM&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#22522;&#20110;&#33258;&#27965;&#24615;&#35774;&#35745;&#20102;&#19977;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26657;&#20934;&#26041;&#27861;&#65288;Wang&#31561;&#65292;2022&#24180;&#65289;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#65288;GSM8K&#21644;MathQA&#65289;&#19978;&#35780;&#20272;&#20351;&#29992;&#24378;&#22823;&#30340;&#24320;&#28304;LLMs&#65288;Mistral&#21644;LLaMA2&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#22522;&#20110;p(True)&#30340;&#29616;&#26377;&#26041;&#27861;&#65288;Kadavath&#31561;&#20154;&#65292;2022&#24180;&#65289;&#25110;logit&#65288;Kadavath&#31561;&#20154;&#65292;2022&#24180;&#65289;&#26356;&#22909;&#22320;&#36830;&#25509;&#27169;&#22411;&#20449;&#24515;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09849v1 Announce Type: cross  Abstract: Calibration, which establishes the correlation between accuracy and model confidence, is important for LLM development. We design three off-the-shelf calibration methods based on self-consistency (Wang et al., 2022) for math reasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using strong open-source LLMs (Mistral and LLaMA2), our methods better bridge model confidence and accuracy than existing methods based on p(True) (Kadavath et al., 2022) or logit (Kadavath et al., 2022).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#22320;&#30913;&#25200;&#21160;&#65292;&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#22826;&#38451;&#39118;&#25968;&#25454;&#65292;&#38024;&#23545;&#24378;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#35774;&#35745;&#36866;&#24403;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;&#20215;&#20540;&#21152;&#26435;&#25216;&#33021;&#35780;&#20998;&#35780;&#20272;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09847</link><description>&lt;p&gt;
&#21033;&#29992;&#22826;&#38451;&#39118;&#25968;&#25454;&#39044;&#27979;&#22320;&#30913;&#27963;&#36291;&#20107;&#20214;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35780;&#20272;&#26368;&#20855;&#39044;&#27979;&#24615;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Forecasting Geoffective Events from Solar Wind Data and Evaluating the Most Predictive Features through Machine Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#22320;&#30913;&#25200;&#21160;&#65292;&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#22826;&#38451;&#39118;&#25968;&#25454;&#65292;&#38024;&#23545;&#24378;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#35774;&#35745;&#36866;&#24403;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;&#20215;&#20540;&#21152;&#26435;&#25216;&#33021;&#35780;&#20998;&#35780;&#20272;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#22320;&#30913;&#25200;&#21160;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#36880;&#28857;&#37319;&#38598;&#30340;&#22826;&#38451;&#39118;&#31561;&#31163;&#23376;&#20307;&#21644;&#30913;&#22330;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#26102;&#38388;&#36328;&#24230;&#36229;&#36807;&#19968;&#20010;&#22826;&#38451;&#21608;&#26399;&#65292;&#20174;2005&#24180;&#21040;2019&#24180;&#65292;&#22312;&#25289;&#26684;&#26391;&#26085;&#28857;L1&#22788;&#33719;&#21462;&#25968;&#25454;&#12290;&#35813;&#38382;&#39064;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#39044;&#27979;SYC-H&#22320;&#30913;&#27963;&#21160;&#25351;&#25968;&#22312;&#19968;&#23567;&#26102;&#20869;&#19979;&#38477;&#21040;-50 nT&#20197;&#19979;&#30340;&#24773;&#20917;&#65292;&#36825;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#30913;&#23618;&#25200;&#21160;&#30340;&#25351;&#26631;&#12290;&#24378;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#36890;&#36807;&#20351;&#29992;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#21152;&#20197;&#24212;&#23545;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38454;&#27573;&#30340;&#36866;&#24403;&#25216;&#33021;&#35780;&#20998;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#25216;&#33021;&#35780;&#20998;&#65292;&#36824;&#20351;&#29992;&#20102;&#20215;&#20540;&#21152;&#26435;&#25216;&#33021;&#35780;&#20998;&#26469;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09847v1 Announce Type: cross  Abstract: This study addresses the prediction of geomagnetic disturbances by exploiting machine learning techniques. Specifically, the Long-Short Term Memory recurrent neural network, which is particularly suited for application over long time series, is employed in the analysis of in-situ measurements of solar wind plasma and magnetic field acquired over more than one solar cycle, from $2005$ to $2019$, at the Lagrangian point L$1$. The problem is approached as a binary classification aiming to predict one hour in advance a decrease in the SYM-H geomagnetic activity index below the threshold of $-50$ nT, which is generally regarded as indicative of magnetospheric perturbations. The strong class imbalance issue is tackled by using an appropriate loss function tailored to optimize appropriate skill scores in the training phase of the neural network. Beside classical skill scores, value-weighted skill scores are then employed to evaluate predictio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;DECAF&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#22312;&#22810;&#20010;&#30456;&#20851;&#29615;&#22659;&#20013;&#36827;&#34892;&#32452;&#21512;&#65292;&#36890;&#36807;&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;CRL&#26041;&#27861;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#33719;&#24471;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.09830</link><description>&lt;p&gt;
&#26088;&#22312;&#23454;&#29616;&#22240;&#26524;&#34920;&#31034;&#30340;&#21487;&#37325;&#29992;&#24615;&#21644;&#21487;&#32452;&#21512;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards the Reusability and Compositionality of Causal Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09830
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;DECAF&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#22312;&#22810;&#20010;&#30456;&#20851;&#29615;&#22659;&#20013;&#36827;&#34892;&#32452;&#21512;&#65292;&#36890;&#36807;&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;CRL&#26041;&#27861;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#33719;&#24471;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Causal Representation Learning&#65288;CRL&#65289;&#26088;&#22312;&#20174;&#39640;&#32500;&#35266;&#27979;&#20013;&#35782;&#21035;&#39640;&#32423;&#22240;&#26524;&#22240;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#20363;&#22914;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;CRL&#20316;&#21697;&#20391;&#37325;&#20110;&#22312;&#21333;&#20010;&#29615;&#22659;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#21521;&#65292;&#21363;&#20174;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#35843;&#25972;&#65292;&#25110;&#32773;&#36328;&#22810;&#20010;&#30456;&#20851;&#29615;&#22659;&#36827;&#34892;&#32452;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DECAF&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21738;&#20123;&#22240;&#26524;&#22240;&#32032;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#65292;&#21738;&#20123;&#38656;&#35201;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#22240;&#26524;&#34920;&#31034;&#20013;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#24178;&#39044;&#30446;&#26631;&#30340;&#21487;&#29992;&#24615;&#65292;&#36825;&#20123;&#30446;&#26631;&#25351;&#31034;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#34987;&#25200;&#21160;&#30340;&#21464;&#37327;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;CRL&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#20165;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#21363;&#21487;&#33719;&#24471;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09830v1 Announce Type: cross  Abstract: Causal Representation Learning (CRL) aims at identifying high-level causal factors and their relationships from high-dimensional observations, e.g., images. While most CRL works focus on learning causal representations in a single environment, in this work we instead propose a first step towards learning causal representations from temporal sequences of images that can be adapted in a new environment, or composed across multiple related environments. In particular, we introduce DECAF, a framework that detects which causal factors can be reused and which need to be adapted from previously learned causal representations. Our approach is based on the availability of intervention targets, that indicate which variables are perturbed at each time step. Experiments on three benchmark datasets show that integrating our framework with four state-of-the-art CRL approaches leads to accurate representations in a new environment with only a few sam
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#21363;&#26102;AI&#24178;&#39044;&#26469;&#25552;&#21319;&#20247;&#21253;&#24179;&#21488;&#20013;&#20154;&#31867;&#26631;&#27880;&#36136;&#37327;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#20171;&#32461;&#20102;LabelAId&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38169;&#35823;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09810</link><description>&lt;p&gt;
LabelAId: &#29992;&#20110;&#25913;&#21892;&#20247;&#21253;&#31995;&#32479;&#20013;&#20154;&#31867;&#26631;&#27880;&#36136;&#37327;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#21450;&#26102;AI&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
LabelAId: Just-in-time AI Interventions for Improving Human Labeling Quality and Domain Knowledge in Crowdsourcing Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#21363;&#26102;AI&#24178;&#39044;&#26469;&#25552;&#21319;&#20247;&#21253;&#24179;&#21488;&#20013;&#20154;&#31867;&#26631;&#27880;&#36136;&#37327;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#20171;&#32461;&#20102;LabelAId&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38169;&#35823;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#24179;&#21488;&#24050;&#32463;&#25913;&#21464;&#20102;&#20998;&#24067;&#24335;&#38382;&#39064;&#35299;&#20915;&#65292;&#20294;&#36136;&#37327;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#36136;&#37327;&#25511;&#21046;&#25514;&#26045;&#65292;&#22914;&#23545;&#24037;&#20316;&#32773;&#36827;&#34892;&#39044;&#31579;&#36873;&#21644;&#23436;&#21892;&#35828;&#26126;&#65292;&#36890;&#24120;&#21482;&#19987;&#27880;&#20110;&#20248;&#21270;&#32463;&#27982;&#20135;&#20986;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21363;&#26102;AI&#24178;&#39044;&#65292;&#20197;&#22686;&#24378;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#26631;&#27880;&#36136;&#37327;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LabelAId&#65292;&#19968;&#31181;&#20808;&#36827;&#30340;&#25512;&#26029;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#31243;&#24207;&#21270;&#24369;&#30417;&#30563;&#65288;PWS&#65289;&#21644;FT-Transformers&#65292;&#26681;&#25454;&#29992;&#25143;&#34892;&#20026;&#21644;&#39046;&#22495;&#30693;&#35782;&#25512;&#26029;&#26631;&#31614;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;LabelAId&#31649;&#36947;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;ML&#22522;&#32447;&#65292;&#20351;&#29992;50&#20010;&#19979;&#28216;&#26679;&#26412;&#25552;&#39640;&#20102;36.7%&#30340;&#38169;&#35823;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;LabelAId&#23454;&#29616;&#21040;Project Sidewalk&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#22478;&#24066;&#21487;&#35775;&#38382;&#24615;&#30340;&#24320;&#28304;&#20247;&#21253;&#24179;&#21488;&#12290;&#19968;&#39033;&#28041;&#21450;34&#21517;&#21442;&#19982;&#32773;&#30340;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09810v1 Announce Type: cross  Abstract: Crowdsourcing platforms have transformed distributed problem-solving, yet quality control remains a persistent challenge. Traditional quality control measures, such as prescreening workers and refining instructions, often focus solely on optimizing economic output. This paper explores just-in-time AI interventions to enhance both labeling quality and domain-specific knowledge among crowdworkers. We introduce LabelAId, an advanced inference model combining Programmatic Weak Supervision (PWS) with FT-Transformers to infer label correctness based on user behavior and domain knowledge. Our technical evaluation shows that our LabelAId pipeline consistently outperforms state-of-the-art ML baselines, improving mistake inference accuracy by 36.7% with 50 downstream samples. We then implemented LabelAId into Project Sidewalk, an open-source crowdsourcing platform for urban accessibility. A between-subjects study with 34 participants demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23545;&#27604;&#21644;&#29983;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#27934;&#23519;&#65292;&#24182;&#20026;&#36873;&#25321;&#21512;&#36866;&#30340;SSL&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.09809</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#23545;&#27604;&#25110;&#29983;&#25104;&#65311;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Time Series: Contrastive or Generative?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23545;&#27604;&#21644;&#29983;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#27934;&#23519;&#65292;&#24182;&#20026;&#36873;&#25321;&#21512;&#36866;&#30340;SSL&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#20174;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#20027;&#27969;&#65306;&#23545;&#27604;&#21644;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23545;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23545;&#27604;&#21644;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#21035;&#20171;&#32461;&#20102;&#23545;&#27604;&#21644;&#29983;&#25104;SSL&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#33719;&#24471;&#25351;&#23548;&#27169;&#22411;&#20248;&#21270;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#21035;&#20026;&#27599;&#31181;&#31867;&#22411;&#23454;&#29616;&#20102;&#32463;&#20856;&#31639;&#27861;&#65288;SimCLR vs. MAE&#65289;&#65292;&#24182;&#22312;&#20844;&#24179;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#20026;&#36873;&#25321;&#21512;&#36866;&#30340;SSL&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#26356;&#24191;&#27867;&#30340;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09809v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has recently emerged as a powerful approach to learning representations from large-scale unlabeled data, showing promising results in time series analysis. The self-supervised representation learning can be categorized into two mainstream: contrastive and generative. In this paper, we will present a comprehensive comparative study between contrastive and generative methods in time series. We first introduce the basic frameworks for contrastive and generative SSL, respectively, and discuss how to obtain the supervision signal that guides the model optimization. We then implement classical algorithms (SimCLR vs. MAE) for each type and conduct a comparative analysis in fair settings. Our results provide insights into the strengths and weaknesses of each approach and offer practical recommendations for choosing suitable SSL methods. We also discuss the implications of our findings for the broader field of rep
&lt;/p&gt;</description></item><item><title>&#20027;&#25968;&#25454;&#31649;&#29702;&#39046;&#22495;&#30340;xLP&#39033;&#30446;&#36890;&#36807;&#32467;&#21512;&#35299;&#37322;&#24615;&#12289;&#20107;&#23454;&#39564;&#35777;&#12289;&#36335;&#24452;&#25490;&#21517;&#12289;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#21644;&#33258;&#35299;&#37322;AI&#31561;&#30740;&#31350;&#25104;&#26524;&#65292;&#21019;&#36896;&#24615;&#22320;&#25552;&#20379;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#35299;&#37322;&#65292;&#24182;&#35753;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#26356;&#31526;&#21512;&#20854;&#38656;&#27714;&#30340;&#35299;&#37322;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.09806</link><description>&lt;p&gt;
xLP&#65306;&#20027;&#25968;&#25454;&#31649;&#29702;&#30340;&#21487;&#35299;&#37322;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
xLP: Explainable Link Prediction for Master Data Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09806
&lt;/p&gt;
&lt;p&gt;
&#20027;&#25968;&#25454;&#31649;&#29702;&#39046;&#22495;&#30340;xLP&#39033;&#30446;&#36890;&#36807;&#32467;&#21512;&#35299;&#37322;&#24615;&#12289;&#20107;&#23454;&#39564;&#35777;&#12289;&#36335;&#24452;&#25490;&#21517;&#12289;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#21644;&#33258;&#35299;&#37322;AI&#31561;&#30740;&#31350;&#25104;&#26524;&#65292;&#21019;&#36896;&#24615;&#22320;&#25552;&#20379;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#35299;&#37322;&#65292;&#24182;&#35753;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#26356;&#31526;&#21512;&#20854;&#38656;&#27714;&#30340;&#35299;&#37322;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09806v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#21521;&#29992;&#25143;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#30340;&#39044;&#27979;&#38656;&#35201;&#21019;&#36896;&#21147;&#12290;&#23588;&#20854;&#26159;&#22312;&#20225;&#19994;&#24212;&#29992;&#20013;&#65292;&#29992;&#25143;&#30340;&#26102;&#38388;&#25104;&#26412;&#24456;&#39640;&#65292;&#20182;&#20204;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#20449;&#20219;&#23545;&#20110;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#20027;&#25968;&#25454;&#31649;&#29702;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#27762;&#21462;&#20102;&#35299;&#37322;&#24615;&#12289;&#20107;&#23454;&#39564;&#35777;&#12289;&#36335;&#24452;&#25490;&#21517;&#12289;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#21644;&#33258;&#35299;&#37322;AI&#31561;&#30740;&#31350;&#25104;&#26524;&#12290;&#22312;&#36825;&#20010;&#28436;&#31034;&#20013;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#23500;&#26377;&#21019;&#24847;&#30340;&#26041;&#24335;&#21576;&#29616;&#38142;&#25509;&#39044;&#27979;&#30340;&#35299;&#37322;&#65292;&#35753;&#29992;&#25143;&#36873;&#25321;&#20182;&#20204;&#26356;&#33298;&#36866;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09806v1 Announce Type: new  Abstract: Explaining neural model predictions to users requires creativity. Especially in enterprise applications, where there are costs associated with users' time, and their trust in the model predictions is critical for adoption. For link prediction in master data management, we have built a number of explainability solutions drawing from research in interpretability, fact verification, path ranking, neuro-symbolic reasoning and self-explaining AI. In this demo, we present explanations for link prediction in a creative way, to allow users to choose explanations they are more comfortable with.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#35825;&#25296;&#39044;&#38450;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#27809;&#26377;&#27169;&#22411;&#26126;&#26174;&#36866;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#23384;&#22312;&#28508;&#22312;&#30340;&#26377;&#23475;&#31572;&#26696;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.09795</link><description>&lt;p&gt;
&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#35825;&#25296;&#39044;&#38450;&#20013;&#30340;&#25928;&#21147;&#65306;&#26377;&#30410;&#36824;&#26159;&#26377;&#23475;&#65311;
&lt;/p&gt;
&lt;p&gt;
Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09795
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#35825;&#25296;&#39044;&#38450;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#27809;&#26377;&#27169;&#22411;&#26126;&#26174;&#36866;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#23384;&#22312;&#28508;&#22312;&#30340;&#26377;&#23475;&#31572;&#26696;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38382;&#31572;&#31995;&#32479;&#27491;&#22312;&#25104;&#20026;&#26222;&#36890;&#22823;&#20247;&#21644;&#33030;&#24369;&#32676;&#20307;&#65288;&#22914;&#20799;&#31461;&#65289;&#20043;&#38388;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#38543;&#30528;&#20799;&#31461;&#19982;&#36825;&#20123;&#24037;&#20855;&#30340;&#20114;&#21160;&#26085;&#30410;&#22686;&#22810;&#65292;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#23457;&#35270;LLMs&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#30340;&#24212;&#29992;&#65292;&#27604;&#22914;&#22312;&#32447;&#20799;&#31461;&#23433;&#20840;&#26597;&#35810;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#22312;&#32447;&#35825;&#25296;&#39044;&#38450;&#20013;&#30340;&#25928;&#21147;&#65292;&#26082;&#21253;&#25324;&#36890;&#36807;&#24314;&#35758;&#29983;&#25104;&#26469;&#35782;&#21035;&#21644;&#36991;&#20813;&#35825;&#25296;&#65292;&#20063;&#36890;&#36807;&#25913;&#21464;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#21644;&#25552;&#31034;&#29305;&#23450;&#24615;&#26469;&#35843;&#26597;&#25552;&#31034;&#35774;&#35745;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#36229;&#36807;6,000&#27425;LLMs&#20114;&#21160;&#30340;&#32467;&#26524;&#21453;&#26144;&#65292;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#27169;&#22411;&#26126;&#26174;&#36866;&#29992;&#20110;&#22312;&#32447;&#35825;&#25296;&#39044;&#38450;&#65292;&#22312;&#34892;&#20026;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#32570;&#20047;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#23384;&#22312;&#26377;&#23475;&#31572;&#26696;&#29983;&#25104;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#24320;&#28304;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09795v1 Announce Type: cross  Abstract: Powerful generative Large Language Models (LLMs) are becoming popular tools amongst the general public as question-answering systems, and are being utilised by vulnerable groups such as children. With children increasingly interacting with these tools, it is imperative for researchers to scrutinise the safety of LLMs, especially for applications that could lead to serious outcomes, such as online child safety queries. In this paper, the efficacy of LLMs for online grooming prevention is explored both for identifying and avoiding grooming through advice generation, and the impact of prompt design on model performance is investigated by varying the provided context and prompt specificity. In results reflecting over 6,000 LLM interactions, we find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-sour
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;</title><link>https://arxiv.org/abs/2403.09793</link><description>&lt;p&gt;
&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#65306;&#20855;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#34892;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#27491;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25317;&#25380;&#22330;&#26223;&#65292;&#24182;&#25104;&#20026;&#25105;&#20204;&#31038;&#20250;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#20010;&#20855;&#26377;&#20010;&#20307;&#20154;&#31867;&#32771;&#34385;&#30340;&#31038;&#20250;&#21487;&#25509;&#21463;&#30340;&#23548;&#33322;&#34892;&#20026;&#23545;&#20110;&#21487;&#25193;&#23637;&#30340;&#24212;&#29992;&#21644;&#20154;&#31867;&#25509;&#21463;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#31574;&#30053;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#24314;&#35758;&#26681;&#25454;&#26426;&#22120;&#20154;&#23637;&#31034;&#30340;&#31038;&#20132;&#34892;&#20026;&#23558;&#29616;&#26377;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20998;&#20026;&#20855;&#26377;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#21644;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#32780;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26500;&#24335;&#28304;&#33258;&#31038;&#20250;&#23398;&#23450;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09793v1 Announce Type: cross  Abstract: Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot's navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot's exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot's social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32771;&#23519;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#25991;&#26412;&#20013;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#31934;&#24230;&#12289;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#39044;&#27979;&#20197;&#21450;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.09762</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#24773;&#32490;&#26234;&#33021;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#25991;&#26412;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Emotional Intelligence Through Artificial Intelligence : NLP and Deep Learning in the Analysis of Healthcare Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32771;&#23519;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#25991;&#26412;&#20013;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#31934;&#24230;&#12289;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#39044;&#27979;&#20197;&#21450;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35780;&#20272;&#19982;&#21307;&#30103;&#30456;&#20851;&#25991;&#26412;&#20013;&#24773;&#32490;&#26041;&#38754;&#30340;&#24212;&#29992;&#30340;&#26041;&#27861;&#35770;&#26816;&#26597;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#34701;&#21512;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#35768;&#22810;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#24773;&#24863;&#20998;&#26512;&#12289;&#23545;&#24773;&#32490;&#36827;&#34892;&#20998;&#31867;&#20197;&#21450;&#22522;&#20110;&#20020;&#24202;&#21465;&#20107;&#12289;&#24739;&#32773;&#23545;&#33647;&#29289;&#30340;&#21453;&#39304;&#21644;&#22312;&#32447;&#20581;&#24247;&#35752;&#35770;&#25152;&#33719;&#24471;&#30340;&#25991;&#26412;&#20449;&#24687;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#30340;&#30740;&#31350;&#12290;&#32508;&#36848;&#23637;&#31034;&#20102;&#29992;&#20110;&#24773;&#24863;&#20998;&#31867;&#30340;&#31639;&#27861;&#31934;&#24230;&#12289;&#29992;&#20110;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;AI&#27169;&#22411;&#39044;&#27979;&#33021;&#21147;&#20197;&#21450;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;AI&#31995;&#32479;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#21033;&#29992;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#35745;&#21010;&#65292;&#36890;&#36807;&#25972;&#21512;&#24739;&#32773;&#24773;&#32490;&#24182;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09762v1 Announce Type: cross  Abstract: This manuscript presents a methodical examination of the utilization of Artificial Intelligence in the assessment of emotions in texts related to healthcare, with a particular focus on the incorporation of Natural Language Processing and deep learning technologies. We scrutinize numerous research studies that employ AI to augment sentiment analysis, categorize emotions, and forecast patient outcomes based on textual information derived from clinical narratives, patient feedback on medications, and online health discussions. The review demonstrates noteworthy progress in the precision of algorithms used for sentiment classification, the prognostic capabilities of AI models for neurodegenerative diseases, and the creation of AI-powered systems that offer support in clinical decision-making. Remarkably, the utilization of AI applications has exhibited an enhancement in personalized therapy plans by integrating patient sentiment and contri
&lt;/p&gt;</description></item><item><title>SpokeN-100&#26159;&#19968;&#20010;&#36328;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;32&#20301;&#19981;&#21516;&#35828;&#35805;&#32773;&#35828;&#20986;&#30340;0&#21040;99&#30340;&#21475;&#35821;&#25968;&#23383;&#65292;&#26088;&#22312;&#20026;&#24494;&#22411;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#25552;&#20379;&#25361;&#25112;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#29992;&#20110;&#20998;&#31867;&#35821;&#35328;&#21644;&#21475;&#35821;&#25968;&#23383;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.09753</link><description>&lt;p&gt;
SpokeN-100&#65306;&#29992;&#20110;&#19981;&#21516;&#35821;&#35328;&#20013;&#21475;&#35821;&#25968;&#23383;&#20998;&#31867;&#30340;&#36328;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09753
&lt;/p&gt;
&lt;p&gt;
SpokeN-100&#26159;&#19968;&#20010;&#36328;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;32&#20301;&#19981;&#21516;&#35828;&#35805;&#32773;&#35828;&#20986;&#30340;0&#21040;99&#30340;&#21475;&#35821;&#25968;&#23383;&#65292;&#26088;&#22312;&#20026;&#24494;&#22411;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#25552;&#20379;&#25361;&#25112;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#29992;&#20110;&#20998;&#31867;&#35821;&#35328;&#21644;&#21475;&#35821;&#25968;&#23383;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09753v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#35821;&#35328;&#12290;&#22522;&#20934;&#27979;&#35797;&#22312;&#35780;&#20272;&#21644;&#25552;&#21319;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#25191;&#34892;&#32780;&#35774;&#35745;&#30340;&#32039;&#20945;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20363;&#22914;&#24494;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#23436;&#20840;&#20154;&#24037;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#19987;&#20026;&#35821;&#38899;&#35782;&#21035;&#32780;&#35774;&#35745;&#65292;&#20195;&#34920;&#20102;&#24494;&#22411;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;SpokeN-100&#21253;&#25324;&#26469;&#33258;32&#20301;&#19981;&#21516;&#35828;&#35805;&#32773;&#22312;&#22235;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#65288;&#33521;&#35821;&#12289;&#26222;&#36890;&#35805;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#65289;&#35828;&#20986;&#30340;0&#21040;99&#30340;&#21475;&#35821;&#25968;&#23383;&#65292;&#20849;&#35745;12,800&#20010;&#38899;&#39057;&#26679;&#26412;&#12290;&#25105;&#20204;&#30830;&#23450;&#21548;&#35273;&#29305;&#24449;&#24182;&#20351;&#29992;UMAP&#65288;Uniform Manifold Approximation and Projection for Dimension Reduction&#65289;&#20316;&#20026;&#38477;&#32500;&#26041;&#27861;&#65292;&#23637;&#31034;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#20016;&#23500;&#24615;&#12290;&#20026;&#20102;&#31361;&#20986;&#25968;&#25454;&#38598;&#30340;&#29992;&#20363;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#22522;&#20934;&#20219;&#21153;&#65306;&#32473;&#23450;&#19968;&#20010;&#38899;&#39057;&#26679;&#26412;&#65292;&#23545;&#25152;&#29992;&#35821;&#35328;&#36827;&#34892;&#20998;&#31867;&#65288;i&#65289;&#21644;/&#25110;&#23545;&#21475;&#35821;&#25968;&#23383;&#36827;&#34892;&#20998;&#31867;&#65288;ii&#65289;&#12290;&#25105;&#20204;&#20248;&#21270;&#20102;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09753v1 Announce Type: cross  Abstract: Benchmarking plays a pivotal role in assessing and enhancing the performance of compact deep learning models designed for execution on resource-constrained devices, such as microcontrollers. Our study introduces a novel, entirely artificially generated benchmarking dataset tailored for speech recognition, representing a core challenge in the field of tiny deep learning. SpokeN-100 consists of spoken numbers from 0 to 99 spoken by 32 different speakers in four different languages, namely English, Mandarin, German and French, resulting in 12,800 audio samples. We determine auditory features and use UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction) as a dimensionality reduction method to show the diversity and richness of the dataset. To highlight the use case of the dataset, we introduce two benchmark tasks: given an audio sample, classify (i) the used language and/or (ii) the spoken number. We optimized state-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#20114;&#32852;&#32593;&#21307;&#30103;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#19982;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;IoMT&#31995;&#32479;&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#25935;&#24863;&#24615;&#12289;&#24694;&#24847;&#25915;&#20987;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.09752</link><description>&lt;p&gt;
&#38754;&#21521;IoMT&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#19982;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Explainable Machine Learning-Based Security and Privacy Protection Framework for Internet of Medical Things Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#20114;&#32852;&#32593;&#21307;&#30103;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#19982;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;IoMT&#31995;&#32479;&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#25935;&#24863;&#24615;&#12289;&#24694;&#24847;&#25915;&#20987;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#21307;&#30103;&#29289;&#32852;&#32593;&#65288;IoMT&#65289;&#36328;&#36234;&#20102;&#20256;&#32479;&#21307;&#30103;&#36793;&#30028;&#65292;&#23454;&#29616;&#20102;&#20174;&#34987;&#21160;&#27835;&#30103;&#21521;&#20027;&#21160;&#39044;&#38450;&#30340;&#36807;&#28193;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#36890;&#36807;&#23454;&#26102;&#20581;&#24247;&#25968;&#25454;&#25910;&#38598;&#23454;&#29616;&#26089;&#26399;&#30142;&#30149;&#26816;&#27979;&#21644;&#20010;&#24615;&#21270;&#25252;&#29702;&#65292;&#29305;&#21035;&#22312;&#24930;&#24615;&#30149;&#31649;&#29702;&#26041;&#38754;&#65292;IoMT&#21487;&#20197;&#33258;&#21160;&#21270;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22788;&#29702;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#21644;&#20215;&#20540;&#65292;IoMT&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#36825;&#20250;&#23041;&#32961;&#21040;&#20854;&#29992;&#25143;&#30340;&#29983;&#21629;&#65292;&#22240;&#27492;&#21560;&#24341;&#20102;&#24694;&#24847;&#21033;&#30410;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#20250;&#20351;&#21307;&#30103;&#25968;&#25454;&#26292;&#38706;&#20110;&#34987;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#25130;&#33719;&#21644;&#31713;&#25913;&#30340;&#39118;&#38505;&#20043;&#19979;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20154;&#20026;&#38169;&#35823;&#12289;&#32593;&#32476;&#24178;&#25200;&#25110;&#30828;&#20214;&#25925;&#38556;&#65292;&#21487;&#33021;&#20250;&#20986;&#29616;&#24322;&#24120;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20877;&#27425;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09752v1 Announce Type: cross  Abstract: The Internet of Medical Things (IoMT) transcends traditional medical boundaries, enabling a transition from reactive treatment to proactive prevention. This innovative method revolutionizes healthcare by facilitating early disease detection and tailored care, particularly in chronic disease management, where IoMT automates treatments based on real-time health data collection. Nonetheless, its benefits are countered by significant security challenges that endanger the lives of its users due to the sensitivity and value of the processed data, thereby attracting malicious interests. Moreover, the utilization of wireless communication for data transmission exposes medical data to interception and tampering by cybercriminals. Additionally, anomalies may arise due to human errors, network interference, or hardware malfunctions. In this context, anomaly detection based on Machine Learning (ML) is an interesting solution, but it comes up again
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#20197;&#29992;&#20110;&#35835;&#21462;AI&#21161;&#25163;&#21152;&#23494;&#21709;&#24212;&#30340;&#26032;&#22411;&#26049;&#36335;&#25915;&#20987;&#8212;&#8212;&#20196;&#29260;&#38271;&#24230;&#26049;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#21477;&#23376;&#38388;&#19978;&#19979;&#25991;&#24182;&#36827;&#34892;&#24050;&#30693;&#26126;&#25991;&#25915;&#20987;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09751</link><description>&lt;p&gt;
&#20320;&#30340;&#25552;&#31034;&#26159;&#20160;&#20040;&#65311;&#19968;&#31181;&#38024;&#23545;AI&#21161;&#25163;&#30340;&#36828;&#31243;&#38190;&#30424;&#35760;&#24405;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
What Was Your Prompt? A Remote Keylogging Attack on AI Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#20197;&#29992;&#20110;&#35835;&#21462;AI&#21161;&#25163;&#21152;&#23494;&#21709;&#24212;&#30340;&#26032;&#22411;&#26049;&#36335;&#25915;&#20987;&#8212;&#8212;&#20196;&#29260;&#38271;&#24230;&#26049;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#21477;&#23376;&#38388;&#19978;&#19979;&#25991;&#24182;&#36827;&#34892;&#24050;&#30693;&#26126;&#25991;&#25915;&#20987;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21161;&#25163;&#27491;&#36880;&#28176;&#25104;&#20026;&#31038;&#20250;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#29992;&#20110;&#23547;&#27714;&#20010;&#20154;&#21644;&#26426;&#23494;&#38382;&#39064;&#30340;&#24314;&#35758;&#25110;&#24110;&#21161;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26049;&#36335;&#25915;&#20987;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#32593;&#32476;&#35835;&#21462;AI&#21161;&#25163;&#30340;&#21152;&#23494;&#21709;&#24212;&#65306;&#20196;&#29260;&#38271;&#24230;&#26049;&#36335;&#12290;&#25105;&#20204;&#21457;&#29616;&#21253;&#25324;OpenAI&#21644;Microsoft&#22312;&#20869;&#30340;&#35768;&#22810;&#21378;&#21830;&#21463;&#21040;&#36825;&#19968;&#26049;&#36335;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20165;&#20174;&#20196;&#29260;&#38271;&#24230;&#24207;&#21015;&#25512;&#26029;&#21709;&#24212;&#20869;&#23481;&#21364;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#20196;&#29260;&#31867;&#20284;&#20110;&#21333;&#35789;&#65292;&#21709;&#24212;&#21487;&#20197;&#26159;&#20960;&#21477;&#35805;&#38271;&#65292;&#23548;&#33268;&#26377;&#25104;&#21315;&#19978;&#19975;&#20010;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09751v1 Announce Type: cross  Abstract: AI assistants are becoming an integral part of society, used for asking advice or help in personal and confidential issues. In this paper, we unveil a novel side-channel that can be used to read encrypted responses from AI Assistants over the web: the token-length side-channel. We found that many vendors, including OpenAI and Microsoft, have this side-channel.   However, inferring the content of a response from a token-length sequence alone proves challenging. This is because tokens are akin to words, and responses can be several sentences long leading to millions of grammatically correct sentences. In this paper, we show how this can be overcome by (1) utilizing the power of a large language model (LLM) to translate these sequences, (2) providing the LLM with inter-sentence context to narrow the search space and (3) performing a known-plaintext attack by fine-tuning the model on the target model's writing style.   Using these methods,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#25506;&#32034;&#20102;LLMs&#20013;&#30340;&#38472;&#36848;&#24615;&#30693;&#35782;&#21644;&#31243;&#24207;&#24615;&#30693;&#35782;&#23545;&#21508;&#31181;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38472;&#36848;&#24615;&#30693;&#35782;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#30410;&#22788;&#22823;&#20110;&#31243;&#24207;&#24615;&#30693;&#35782;&#65292;&#22312;&#31616;&#21333;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#21453;&#20043;&#65307;&#38543;&#30528;&#39044;&#35757;&#32451;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#21033;&#29992;&#20004;&#31181;&#30693;&#35782;&#30340;&#33021;&#21147;&#22343;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2403.09750</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#20998;&#26512;&#65306;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38472;&#36848;&#24615;&#21644;&#31243;&#24207;&#24615;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09750
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#25506;&#32034;&#20102;LLMs&#20013;&#30340;&#38472;&#36848;&#24615;&#30693;&#35782;&#21644;&#31243;&#24207;&#24615;&#30693;&#35782;&#23545;&#21508;&#31181;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38472;&#36848;&#24615;&#30693;&#35782;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#30410;&#22788;&#22823;&#20110;&#31243;&#24207;&#24615;&#30693;&#35782;&#65292;&#22312;&#31616;&#21333;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#21453;&#20043;&#65307;&#38543;&#30528;&#39044;&#35757;&#32451;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#21033;&#29992;&#20004;&#31181;&#30693;&#35782;&#30340;&#33021;&#21147;&#22343;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#35748;&#30693;&#29702;&#35770;&#20013;&#30340;&#38472;&#36848;&#24615;&#30693;&#35782;&#21644;&#31243;&#24207;&#24615;&#30693;&#35782;&#26159;&#20004;&#20010;&#20851;&#38190;&#37096;&#20998;&#65292;&#22312;LLM&#30340;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#36825;&#20004;&#31181;&#30693;&#35782;&#30340;&#23450;&#20041;&#12289;&#25506;&#31350;&#21644;&#23450;&#37327;&#35780;&#20272;&#23384;&#22312;&#25361;&#25112;&#65292;&#32570;&#20047;&#23545;&#36825;&#20004;&#31181;&#30693;&#35782;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;LLMs&#30340;&#22320;&#38754;&#30495;&#30693;&#65292;&#24182;&#35780;&#20272;&#20102;&#26377;&#25928;&#24471;&#20998;&#12290;&#36890;&#36807;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;(1) &#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#26469;&#33258;&#38472;&#36848;&#24615;&#30693;&#35782;&#30340;&#30410;&#22788;&#22823;&#20110;&#26469;&#33258;&#31243;&#24207;&#24615;&#30693;&#35782;&#30340;&#30410;&#22788;&#12290;(2) &#20165;&#22312;&#20855;&#26377;&#31616;&#21333;&#36923;&#36753;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#65292;&#31243;&#24207;&#24615;&#30693;&#35782;&#30340;&#21033;&#28070;&#22823;&#20110;&#38472;&#36848;&#24615;&#30693;&#35782;&#12290;(3) &#38543;&#30528;&#39044;&#35757;&#32451;&#30340;&#36827;&#34892;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#21033;&#29992;&#20004;&#31181;&#30693;&#35782;&#30340;&#33021;&#21147;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#36895;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09750v1 Announce Type: cross  Abstract: Declarative knowledge and procedural knowledge are two key parts in meta-cognitive theory, and these two hold significant importance in pre-training and inference of LLMs. However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment. In this paper, we explore from a new perspective by providing ground-truth knowledge for LLMs and evaluating the effective score. Through extensive experiments with widely-used datasets and models, we get conclusions: (1) In most tasks, benefits from declarative knowledge are greater than those from procedural knowledge. (2) Profits of procedural knowledge are larger than declarative knowledge only in reasoning tasks with simple logic. (3) As pre-training progresses and size increases, model ability to utilize both kinds of knowledge significantly improves, but in different speed. We do detailed analysis 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#27744;&#21270;&#26041;&#27861;SoM-TP&#65292;&#36890;&#36807;&#22810;&#20803;&#35270;&#35282;&#23398;&#20064;&#23454;&#29616;&#21160;&#24577;&#36873;&#25321;&#26368;&#20339;&#26102;&#38388;&#27744;&#21270;&#65292;&#22312;&#21333;&#20010;&#20998;&#31867;&#22120;&#20869;&#23454;&#29616;&#38750;&#36845;&#20195;&#27744;&#21270;&#38598;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.09749</link><description>&lt;p&gt;
&#23454;&#29616;&#22810;&#20803;&#35270;&#35282;&#23398;&#20064;&#65306;&#22522;&#20110;&#22810;&#20010;&#26102;&#38388;&#27744;&#21270;&#30340;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Towards Diverse Perspective Learning with Selection over Multiple Temporal Poolings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09749
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#27744;&#21270;&#26041;&#27861;SoM-TP&#65292;&#36890;&#36807;&#22810;&#20803;&#35270;&#35282;&#23398;&#20064;&#23454;&#29616;&#21160;&#24577;&#36873;&#25321;&#26368;&#20339;&#26102;&#38388;&#27744;&#21270;&#65292;&#22312;&#21333;&#20010;&#20998;&#31867;&#22120;&#20869;&#23454;&#29616;&#38750;&#36845;&#20195;&#27744;&#21270;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#39034;&#24207;&#20449;&#24687;&#30340;&#26102;&#38388;&#27744;&#21270;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#27599;&#20010;&#26102;&#38388;&#27744;&#21270;&#20855;&#26377;&#19981;&#21516;&#30340;&#26426;&#21046;&#65292;&#24182;&#19988;&#26681;&#25454;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#19981;&#21516;&#24773;&#20917;&#21487;&#33021;&#25928;&#26524;&#22909;&#22351;&#19981;&#19968;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#22266;&#23450;&#27744;&#21270;&#26426;&#21046;&#31216;&#20026;&#21333;&#19968;&#35270;&#35282;&#30340;&#26102;&#38388;&#27744;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22810;&#20803;&#35270;&#35282;&#23398;&#20064;&#30340;&#26032;&#22411;&#26102;&#38388;&#27744;&#21270;&#26041;&#27861;&#65306;&#36873;&#25321;&#22810;&#20010;&#26102;&#38388;&#27744;&#21270;&#65288;SoM-TP&#65289;&#12290;SoM-TP&#36890;&#36807;&#27880;&#24847;&#21147;&#21160;&#24577;&#36873;&#25321;&#22810;&#31181;&#26041;&#27861;&#20013;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#30340;&#26368;&#20339;&#26102;&#38388;&#27744;&#21270;&#12290;SoM-TP&#30340;&#21160;&#24577;&#27744;&#21270;&#36873;&#25321;&#21463;&#21040;&#22810;&#36873;&#23398;&#20064;&#65288;MCL&#65289;&#38598;&#25104;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#35813;&#27010;&#24565;&#20174;&#22810;&#20010;&#36755;&#20986;&#20013;&#36873;&#25321;&#26368;&#20339;&#36755;&#20986;&#12290;SoM-TP&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#36873;&#25321;&#23454;&#29616;&#20102;&#21333;&#20010;&#20998;&#31867;&#22120;&#20869;&#30340;&#38750;&#36845;&#20195;&#27744;&#21270;&#38598;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35270;&#35282;&#25439;&#22833;&#21644;&#22810;&#20803;&#35270;&#35282;&#23398;&#20064;&#32593;&#32476;&#65288;DPLN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09749v1 Announce Type: cross  Abstract: In Time Series Classification (TSC), temporal pooling methods that consider sequential information have been proposed. However, we found that each temporal pooling has a distinct mechanism, and can perform better or worse depending on time series data. We term this fixed pooling mechanism a single perspective of temporal poolings. In this paper, we propose a novel temporal pooling method with diverse perspective learning: Selection over Multiple Temporal Poolings (SoM-TP). SoM-TP dynamically selects the optimal temporal pooling among multiple methods for each data by attention. The dynamic pooling selection is motivated by the ensemble concept of Multiple Choice Learning (MCL), which selects the best among multiple outputs. The pooling selection by SoM-TP's attention enables a non-iterative pooling ensemble within a single classifier. Additionally, we define a perspective loss and Diverse Perspective Learning Network (DPLN). The loss w
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#24341;&#20837;&#26032;&#30340;&#21069;&#27839;&#65292;&#20294;&#20173;&#38656;&#20811;&#26381;&#35299;&#20915;&#36807;&#26102;&#30693;&#35782;&#21644;&#20302;&#36136;&#37327;&#35777;&#25454;&#26816;&#32034;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.09747</link><description>&lt;p&gt;
&#37325;&#26032;&#25506;&#23547;&#30495;&#30456;&#65306;&#22810;&#36718;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09747
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#24341;&#20837;&#26032;&#30340;&#21069;&#27839;&#65292;&#20294;&#20173;&#38656;&#20811;&#26381;&#35299;&#20915;&#36807;&#26102;&#30693;&#35782;&#21644;&#20302;&#36136;&#37327;&#35777;&#25454;&#26816;&#32034;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#30340;&#27867;&#28389;&#23545;&#25919;&#27835;&#12289;&#32463;&#27982;&#21644;&#31038;&#20250;&#24102;&#26469;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#24050;&#32463;&#37319;&#29992;&#20102;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#20004;&#20010;&#22522;&#26412;&#35201;&#32032;&#65306;&#35777;&#25454;&#30340;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#39044;&#27979;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20174;&#32500;&#22522;&#30334;&#31185;&#31561;&#38745;&#24577;&#30693;&#35782;&#24211;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#20294;&#21463;&#38480;&#20110;&#36807;&#26102;&#25110;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#26032;&#20852;&#25110;&#32597;&#35265;&#30340;&#35201;&#27714;&#12290;&#20197;&#20854;&#21331;&#36234;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#32780;&#38395;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20026;&#20551;&#26032;&#38395;&#26816;&#27979;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19968;&#26679;&#65292;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#20063;&#20250;&#38754;&#20020;&#36807;&#26102;&#21644;&#38271;&#23614;&#30693;&#35782;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;LLMs&#32463;&#24120;&#36935;&#21040;&#20302;&#36136;&#37327;&#35777;&#25454;&#26816;&#32034;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09747v1 Announce Type: cross  Abstract: The proliferation of fake news has had far-reaching implications on politics, the economy, and society at large. While Fake news detection methods have been employed to mitigate this issue, they primarily depend on two essential elements: the quality and relevance of the evidence, and the effectiveness of the verdict prediction mechanism. Traditional methods, which often source information from static repositories like Wikipedia, are limited by outdated or incomplete data, particularly for emerging or rare claims. Large Language Models (LLMs), known for their remarkable reasoning and generative capabilities, introduce a new frontier for fake news detection. However, like traditional methods, LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints. To address these ch
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#29983;&#25104;&#21453;&#39304;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22810;&#25968;&#21453;&#39304;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#30721;&#38169;&#35823;&#65292;&#20294;&#23384;&#22312;&#19981;&#27491;&#30830;&#24314;&#35758;&#21644;&#34394;&#26500;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.09744</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#29983;&#25104;&#21453;&#39304;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Application of Large Language Models to Generate Feedback in Programming Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09744
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#29983;&#25104;&#21453;&#39304;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22810;&#25968;&#21453;&#39304;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#30721;&#38169;&#35823;&#65292;&#20294;&#23384;&#22312;&#19981;&#27491;&#30830;&#24314;&#35758;&#21644;&#34394;&#26500;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#25552;&#21319;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#27010;&#36848;&#20102;&#19968;&#20010;&#21033;&#29992;GPT-4&#25552;&#20379;&#32534;&#31243;&#20219;&#21153;&#21453;&#39304;&#20294;&#19981;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#30340;&#32593;&#39029;&#24212;&#29992;&#30340;&#35774;&#35745;&#12290;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#36827;&#34892;&#32534;&#31243;&#20219;&#21153;&#30340;&#32593;&#39029;&#24212;&#29992;&#65292;&#24182;&#22312;&#19968;&#20010;&#23398;&#26399;&#20869;&#23545;51&#21517;&#23398;&#29983;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#30001;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#30721;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#19981;&#27491;&#30830;&#24314;&#35758;&#21644;&#34394;&#26500;&#38382;&#39064;&#30340;&#25361;&#25112;&#34920;&#26126;&#26377;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24517;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09744v1 Announce Type: cross  Abstract: This study investigates the application of large language models, specifically GPT-4, to enhance programming education. The research outlines the design of a web application that uses GPT-4 to provide feedback on programming tasks, without giving away the solution. A web application for working on programming tasks was developed for the study and evaluated with 51 students over the course of one semester. The results show that most of the feedback generated by GPT-4 effectively addressed code errors. However, challenges with incorrect suggestions and hallucinated issues indicate the need for further improvements.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20154;&#31867;&#22240;&#32032;&#22312;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#20013;&#30340;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#20854;&#22312;&#19987;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#26102;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.09743</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#20026;&#22240;&#32032;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#19982;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09743
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20154;&#31867;&#22240;&#32032;&#22312;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#20013;&#30340;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#20854;&#22312;&#19987;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#26102;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI&#22312;2022&#24180;11&#26376;&#25512;&#20986;&#30340;ChatGPT&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#26102;&#21051;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#20837;&#20027;&#27969;&#65292;&#24182;&#22312;&#29992;&#25143;&#37319;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#35760;&#24405;&#12290;&#23588;&#20854;&#26159;ChatGPT&#65292;&#32463;&#36807;&#24191;&#27867;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#35757;&#32451;&#65292;&#23637;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#26263;&#31034;&#23545;&#21171;&#21160;&#21147;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;-&#8220;&#24187;&#35273;&#8221;&#21644;&#36951;&#28431;&#65292;&#20135;&#29983;&#19981;&#27491;&#30830;&#25110;&#19981;&#23436;&#25972;&#30340;&#20449;&#24687;&#12290;&#36825;&#22312;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#29615;&#22659;&#20013;&#23588;&#20026;&#21361;&#38505;&#65292;&#27604;&#22914;&#27861;&#24459;&#21512;&#35268;&#12289;&#21307;&#23398;&#25110;&#31934;&#32454;&#30340;&#27969;&#31243;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09743v1 Announce Type: cross  Abstract: The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for Artificial Intelligence, introducing Large Language Models (LLMs) to the mainstream and setting new records in user adoption. LLMs, particularly ChatGPT, trained on extensive internet data, demonstrate remarkable conversational capabilities across various domains, suggesting a significant impact on the workforce. However, these models are susceptible to errors - "hallucinations" and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.   There are both technical and human solutions to cope with this isse. This paper explores the human factors that enable users to detect errors in LLM outputs, a critical component in mitigating risks associated with their use in professional settings. Understanding these factors is essen
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#22238;&#39038;&#20102;&#35299;&#20915;&#26368;&#22823;&#22242;&#38382;&#39064;&#30340;&#32463;&#20856;&#31639;&#27861;&#65292;&#21516;&#26102;&#20063;&#28085;&#30422;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#27979;&#35797;&#36825;&#20123;&#31639;&#27861;&#30340;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.09742</link><description>&lt;p&gt;
&#26368;&#22823;&#22242;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#31616;&#35201;&#22238;&#39038;&#65306;&#20174;&#32463;&#20856;&#31639;&#27861;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Short Review on Novel Approaches for Maximum Clique Problem: from Classical algorithms to Graph Neural Networks and Quantum algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#22238;&#39038;&#20102;&#35299;&#20915;&#26368;&#22823;&#22242;&#38382;&#39064;&#30340;&#32463;&#20856;&#31639;&#27861;&#65292;&#21516;&#26102;&#20063;&#28085;&#30422;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#27979;&#35797;&#36825;&#20123;&#31639;&#27861;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25163;&#31295;&#20840;&#38754;&#22238;&#39038;&#20102;&#26368;&#22823;&#22242;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#28041;&#21450;&#22312;&#22270;&#20013;&#25214;&#21040;&#25152;&#26377;&#20004;&#20004;&#30456;&#37051;&#30340;&#39030;&#28857;&#23376;&#38598;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#25163;&#31295;&#20197;&#31616;&#21333;&#30340;&#26041;&#24335;&#28085;&#30422;&#20102;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#32463;&#20856;&#31639;&#27861;&#65292;&#24182;&#21253;&#25324;&#20102;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#31639;&#27861;&#26368;&#36817;&#21457;&#23637;&#30340;&#23457;&#26597;&#12290;&#35813;&#32508;&#36848;&#20197;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#32463;&#20856;&#20197;&#21450;&#26032;&#30340;&#23398;&#20064;&#21644;&#37327;&#23376;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09742v1 Announce Type: new  Abstract: This manuscript provides a comprehensive review of the Maximum Clique Problem, a computational problem that involves finding subsets of vertices in a graph that are all pairwise adjacent to each other. The manuscript covers in a simple way classical algorithms for solving the problem and includes a review of recent developments in graph neural networks and quantum algorithms. The review concludes with benchmarks for testing classical as well as new learning, and quantum algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#26041;&#27861;SolMover&#65292;&#21033;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;LLMs&#21327;&#21516;&#20316;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20197;&#23558;&#20195;&#30721;&#32763;&#35793;&#20026;&#19968;&#20010;&#38476;&#29983;&#30340;&#35821;&#35328;</title><link>https://arxiv.org/abs/2403.09740</link><description>&lt;p&gt;
&#25945;&#26426;&#22120;&#32534;&#20889;&#20195;&#30721;&#65306;LLM&#26234;&#33021;&#21512;&#32422;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Teaching Machines to Code: Smart Contract Translation with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#26041;&#27861;SolMover&#65292;&#21033;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;LLMs&#21327;&#21516;&#20316;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20197;&#23558;&#20195;&#30721;&#32763;&#35793;&#20026;&#19968;&#20010;&#38476;&#29983;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26631;&#24535;&#30528;&#19968;&#20010;&#37325;&#22823;&#37324;&#31243;&#30865;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#24120;&#24120;&#21487;&#20197;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#36825;&#20123;&#25104;&#23601;&#20013;&#65292;&#23427;&#20204;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#28789;&#27963;&#24615;&#31361;&#20986;&#65292;&#23494;&#20999;&#27169;&#20223;&#20154;&#31867;&#32763;&#35793;&#32773;&#36827;&#34892;&#30340;&#22797;&#26434;&#21644;&#21021;&#27493;&#27969;&#31243;&#65292;&#20197;&#30830;&#20445;&#32763;&#35793;&#20869;&#23481;&#30340;&#24544;&#23454;&#24615;&#21644;&#36136;&#37327;&#12290;&#23613;&#31649;&#22312;&#21033;&#29992;LLMs&#36328;&#19981;&#21516;&#35821;&#35328;&#32763;&#35793;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26234;&#33021;&#21512;&#32422;&#32763;&#35793;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36827;&#20837;LLM&#20043;&#21069;&#26410;&#26366;&#36935;&#21040;&#36807;&#30340;&#35821;&#35328;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#39046;&#22495;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#26041;&#27861;SolMover&#65292;&#23427;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#20805;&#20998;&#21033;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;LLMs&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#36825;&#20010;&#26694;&#26550;&#26088;&#22312;&#29702;&#35299;&#32534;&#30721;&#21407;&#21017;&#65292;&#24182;&#23558;&#36825;&#31181;&#29702;&#35299;&#24212;&#29992;&#20110;&#23558;&#20195;&#30721;&#32763;&#35793;&#20026;&#19968;&#20010;&#38476;&#29983;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09740v1 Announce Type: cross  Abstract: The advent of large language models (LLMs) has marked a significant milestone in the realm of artificial intelligence, with their capabilities often matching or surpassing human expertise in various domains. Among these achievements, their adeptness in translation tasks stands out, closely mimicking the intricate and preliminary processes undertaken by human translators to ensure the fidelity and quality of the translated content. Despite the advancements in utilizing LLMs for translating programming code across different languages, the domain of smart contract translation, particularly into languages not previously encountered by the LLM, remains largely unexplored. In our research, we present a pioneering approach, SolMover, which harnesses the synergy of two distinct LLMs within a unified framework. This framework is designed to grasp coding principles and apply this understanding to the translation of code into an unfamiliar langua
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.09738</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#25512;&#33616;&#20013;&#29983;&#25104;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09738
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29992;&#25143;&#26159;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#30495;&#23454;&#29992;&#25143;&#20195;&#29702;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#27169;&#25311;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#23427;&#20204;&#33021;&#21542;&#20195;&#34920;&#22810;&#26679;&#21270;&#29992;&#25143;&#32676;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#23545;&#35805;&#25512;&#33616;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#35813;&#21327;&#35758;&#30001;&#20116;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#21512;&#25104;&#29992;&#25143;&#24212;&#35813;&#34920;&#29616;&#20986;&#30340;&#20851;&#38190;&#29305;&#24615;&#65306;&#36873;&#25321;&#35201;&#35848;&#35770;&#30340;&#29289;&#21697;&#65292;&#34920;&#36798;&#20108;&#36827;&#21046;&#20559;&#22909;&#65292;&#34920;&#36798;&#24320;&#25918;&#24335;&#20559;&#22909;&#65292;&#35831;&#27714;&#25512;&#33616;&#20197;&#21450;&#25552;&#20379;&#21453;&#39304;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20934;&#30830;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#30340;&#32508;&#21512;&#26041;&#27861;&#35770;&#65292;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#38035;&#40060;&#32593;&#31449;&#21644;&#21512;&#27861;&#32593;&#31449;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09735</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20934;&#30830;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#30340;&#22797;&#26434;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Sophisticated Framework for the Accurate Detection of Phishing Websites
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09735
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20934;&#30830;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#30340;&#32508;&#21512;&#26041;&#27861;&#35770;&#65292;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#38035;&#40060;&#32593;&#31449;&#21644;&#21512;&#27861;&#32593;&#31449;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#38035;&#40060;&#26159;&#19968;&#31181;&#26085;&#30410;&#22797;&#26434;&#30340;&#32593;&#32476;&#25915;&#20987;&#24418;&#24335;&#65292;&#32473;&#20840;&#29699;&#20225;&#19994;&#24102;&#26469;&#24040;&#22823;&#36130;&#21153;&#25439;&#22833;&#30340;&#21516;&#26102;&#65292;&#20063;&#21361;&#21450;&#20010;&#20154;&#38544;&#31169;&#12290;&#25915;&#20987;&#32773;&#19981;&#26029;&#35774;&#35745;&#26032;&#30340;&#21457;&#21160;&#25915;&#20987;&#26041;&#27861;&#65292;&#26816;&#27979;&#36825;&#20123;&#25915;&#20987;&#21464;&#24471;&#21313;&#20998;&#22256;&#38590;&#12290;&#35768;&#22810;&#19981;&#21516;&#30340;&#25216;&#26415;&#24050;&#34987;&#25552;&#20986;&#65292;&#21508;&#26377;&#21033;&#24330;&#12290;&#34429;&#28982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25216;&#26415;&#22312;&#35782;&#21035;&#27492;&#31867;&#25915;&#20987;&#26041;&#38754;&#26368;&#20026;&#25104;&#21151;&#65292;&#20294;&#22312;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#35770;&#26469;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#12290;&#30446;&#26631;&#26159;&#35774;&#35745;&#20986;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#38035;&#40060;&#32593;&#31449;&#21644;&#21512;&#27861;&#32593;&#31449;&#30340;&#31995;&#32479;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#27867;&#21270;&#24615;&#33021;&#12290;&#37319;&#29992;&#20102;&#29305;&#24449;&#36873;&#25321;&#12289;&#36138;&#23146;&#31639;&#27861;&#12289;&#20132;&#21449;&#39564;&#35777;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#21512;&#32452;&#21512;&#26469;&#26500;&#24314;&#35813;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09735v1 Announce Type: cross  Abstract: Phishing is an increasingly sophisticated form of cyberattack that is inflicting huge financial damage to corporations throughout the globe while also jeopardizing individuals' privacy. Attackers are constantly devising new methods of launching such assaults and detecting them has become a daunting task. Many different techniques have been suggested, each with its own pros and cons. While machine learning-based techniques have been most successful in identifying such attacks, they continue to fall short in terms of performance and generalizability. This paper proposes a comprehensive methodology for detecting phishing websites. The goal is to design a system that is capable of accurately distinguishing phishing websites from legitimate ones and provides generalized performance over a broad variety of datasets. A combination of feature selection, greedy algorithm, cross-validation, and deep learning methods have been utilized to constru
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;ARC&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#21644;&#25104;&#24180;&#20154;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23545;LLMs&#21644;&#24180;&#24188;&#20799;&#31461;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;&#31867;&#20284;&#30340;&#35299;&#20915;&#31574;&#30053;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#20026;&#25105;&#20204;&#29702;&#35299;LLMs&#22914;&#20309;&#35299;&#20915;&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.09734</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#19968;&#26679;&#35299;&#20915;ARC&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Solve ARC Visual Analogies Like People Do?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;ARC&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#21644;&#25104;&#24180;&#20154;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23545;LLMs&#21644;&#24180;&#24188;&#20799;&#31461;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;&#31867;&#20284;&#30340;&#35299;&#20915;&#31574;&#30053;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#20026;&#25105;&#20204;&#29702;&#35299;LLMs&#22914;&#20309;&#35299;&#20915;&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#21046;&#35770;&#25991;&#65288;Chollet, 2019&#65289;&#24418;&#24335;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20799;&#31461;&#21451;&#22909;&#30340;ARC&#39033;&#30446;&#19978;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#20799;&#31461;&#36824;&#26159;&#25104;&#24180;&#20154;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#37117;&#32988;&#36807;&#22823;&#22810;&#25968;LLMs&#12290;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#21644;&#24180;&#24188;&#20799;&#31461;&#20043;&#38388;&#31867;&#20284;&#30340;&#8220;&#20498;&#36864;&#8221;&#35299;&#20915;&#31574;&#30053;&#65292;&#20854;&#20013;&#31867;&#27604;&#30340;&#19968;&#37096;&#20998;&#34987;&#31616;&#21333;&#22797;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#20182;&#20004;&#31181;&#38169;&#35823;&#31867;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#34920;&#38754;&#25484;&#25569;&#20851;&#38190;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#20869;&#22806;&#20851;&#31995;&#65289;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#36755;&#20837;&#30697;&#38453;&#30340;&#31616;&#21333;&#32452;&#21512;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#8220;&#27010;&#24565;&#8221;&#38169;&#35823;&#22312;&#20154;&#31867;&#20013;&#26356;&#24120;&#35265;&#65292;&#8220;&#30697;&#38453;&#8221;&#38169;&#35823;&#22312;LLMs&#20013;&#26356;&#24120;&#35265;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#38169;&#35823;&#20998;&#26512;&#20197;&#21450;&#19982;&#20154;&#31867;&#21457;&#23637;&#30340;&#27604;&#36739;&#26469;&#29702;&#35299;LLMs&#22914;&#20309;&#35299;&#20915;&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09734v1 Announce Type: cross  Abstract: The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test designed for humans and machines (Chollet, 2019). We compared human and large language model (LLM) performance on a new child-friendly set of ARC items. Results show that both children and adults outperform most LLMs on these tasks. Error analysis revealed a similar "fallback" solution strategy in LLMs and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole, "concept" errors were more common in humans, and "matrix" errors were more common in LLMs. This study sheds new light on LLM reasoning ability and the extent to which we can use error analyses and comparisons with human development to understand how LLMs solve visual analogies.
&lt;/p&gt;</description></item><item><title>OverleafCopilot&#26159;&#31532;&#19968;&#20010;&#26080;&#32541;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;Overleaf&#30340;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#22312;&#25776;&#20889;&#35770;&#25991;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09733</link><description>&lt;p&gt;
OverleafCopilot&#65306;&#22312;Overleaf&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23398;&#26415;&#20889;&#20316;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09733
&lt;/p&gt;
&lt;p&gt;
OverleafCopilot&#26159;&#31532;&#19968;&#20010;&#26080;&#32541;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;Overleaf&#30340;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#22312;&#25776;&#20889;&#35770;&#25991;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36805;&#36895;&#21457;&#23637;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#21508;&#31181;&#24212;&#29992;&#30340;&#23454;&#29616;&#12290;&#22312;&#26412;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;LLMs&#21644;&#27969;&#34892;&#30340;&#23398;&#26415;&#20889;&#20316;&#24037;&#20855;Overleaf&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#23398;&#26415;&#20889;&#20316;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#20026;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#65292;&#25105;&#20204;&#38754;&#20020;&#19977;&#20010;&#25361;&#25112;&#65306;i&#65289;&#22312;Overleaf&#21644;LLMs&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;ii&#65289;&#19982;LLM&#25552;&#20379;&#32773;&#24314;&#31435;&#21487;&#38752;&#36890;&#20449;&#65292;iii&#65289;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OverleafCopilot&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26080;&#32541;&#38598;&#25104;LLMs&#21644;Overleaf&#30340;&#24037;&#20855;&#65288;&#21363;&#27983;&#35272;&#22120;&#25193;&#23637;&#31243;&#24207;&#65289;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#22312;&#25776;&#20889;&#35770;&#25991;&#26102;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#26694;&#26550;&#26469;&#36830;&#25509;LLMs&#21644;Overleaf&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;PromptGenius&#32593;&#31449;&#65292;&#20379;&#30740;&#31350;&#20154;&#21592;&#36731;&#26494;&#26597;&#25214;&#21644;&#20849;&#20139;&#39640;&#36136;&#37327;&#30340;&#26368;&#26032;&#25552;&#31034;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;ag
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09733v1 Announce Type: cross  Abstract: The rapid development of Large Language Models (LLMs) has facilitated a variety of applications from different domains. In this technical report, we explore the integration of LLMs and the popular academic writing tool, Overleaf, to enhance the efficiency and quality of academic writing. To achieve the above goal, there are three challenges: i) including seamless interaction between Overleaf and LLMs, ii) establishing reliable communication with the LLM provider, and iii) ensuring user privacy. To address these challenges, we present OverleafCopilot, the first-ever tool (i.e., a browser extension) that seamlessly integrates LLMs and Overleaf, enabling researchers to leverage the power of LLMs while writing papers. Specifically, we first propose an effective framework to bridge LLMs and Overleaf. Then, we developed PromptGenius, a website for researchers to easily find and share high-quality up-to-date prompts. Thirdly, we propose an ag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09732</link><description>&lt;p&gt;
PET-SQL&#65306;&#19968;&#20010;&#24102;&#26377;&#20132;&#21449;&#19968;&#33268;&#24615;&#30340;&#22686;&#24378;&#25552;&#31034;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;SQL&#65288;Text2SQL&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24378;&#35843;&#21050;&#28608;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#30340;&#29992;&#25143;&#24847;&#22270;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#34920;&#31034;&#65292;&#31216;&#20026;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#24335;&#20449;&#24687;&#21644;&#20174;&#34920;&#26684;&#38543;&#26426;&#25277;&#26679;&#30340;&#21333;&#20803;&#26684;&#20540;&#65292;&#20197;&#25351;&#23548;LLM&#29983;&#25104;SQL&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#26816;&#32034;&#38382;&#39064;-SQL&#23545;&#20316;&#20026;&#23569;&#37327;&#28436;&#31034;&#65292;&#20419;&#20351;LLM&#29983;&#25104;&#21021;&#27493;SQL&#65288;PreSQL&#65289;&#12290;&#20043;&#21518;&#65292;&#35299;&#26512;PreSQL&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#36827;&#34892;&#27169;&#24335;&#38142;&#25509;&#65292;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#26377;&#29992;&#20449;&#24687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#38142;&#25509;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
&lt;/p&gt;</description></item><item><title>&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;&#21152;&#26435;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21152;&#26435;&#26641;&#33258;&#21160;&#26426;&#65292;&#25299;&#23637;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2403.09728</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#25311;&#24207;&#21015;&#21644;&#26641;&#19978;&#30340;&#21152;&#26435;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Simulating Weighted Automata over Sequences and Trees with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09728
&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;&#21152;&#26435;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21152;&#26435;&#26641;&#33258;&#21160;&#26426;&#65292;&#25299;&#23637;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#27169;&#22411;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#25512;&#29702;&#30340;&#26041;&#24335;&#20197;&#21450;&#35745;&#31639;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20154;&#20204;&#23545;&#27492;&#30693;&#20043;&#29978;&#23569;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#26159;&#25353;&#39034;&#24207;&#22788;&#29702;&#25968;&#25454;&#65292;&#21364;&#32988;&#36807;&#35832;&#22914;RNN&#30340;&#39034;&#24207;&#31070;&#32463;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#32039;&#20945;&#22320;&#27169;&#25311;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;DFAs&#65289;&#30340;&#24207;&#21015;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#21464;&#21387;&#22120;&#33021;&#21542;&#27169;&#25311;&#26356;&#22797;&#26434;&#30340;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#25512;&#29702;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;&#21152;&#26435;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;WFAs&#65289;&#65292;&#36825;&#26159;&#19968;&#31867;&#21253;&#21547;DFAs&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#21152;&#26435;&#26641;&#33258;&#21160;&#26426;&#65288;WTA&#65289;&#65292;&#19968;&#31181;&#21152;&#26435;&#33258;&#21160;&#26426;&#25512;&#24191;&#21040;&#26641;&#24418;&#36755;&#20837;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#36825;&#20123;&#35828;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#25152;&#38656;&#21464;&#21387;&#22120;&#27169;&#22411;&#22823;&#23567;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09728v1 Announce Type: cross  Abstract: Transformers are ubiquitous models in the natural language processing (NLP) community and have shown impressive empirical successes in the past few years. However, little is understood about how they reason and the limits of their computational capabilities. These models do not process data sequentially, and yet outperform sequential neural models such as RNNs. Recent work has shown that these models can compactly simulate the sequential reasoning abilities of deterministic finite automata (DFAs). This leads to the following question: can transformers simulate the reasoning of more complex finite state machines? In this work, we show that transformers can simulate weighted finite automata (WFAs), a class of models which subsumes DFAs, as well as weighted tree automata (WTA), a generalization of weighted automata to tree structured inputs. We prove these claims formally and provide upper bounds on the sizes of the transformer models nee
&lt;/p&gt;</description></item><item><title>RAG-based constructions are more efficient than models produced with FN for the development of AI-driven knowledge-based systems.</title><link>https://arxiv.org/abs/2403.09727</link><description>&lt;p&gt;
&#25506;&#31350;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#24494;&#35843;&#22312;&#21457;&#23637;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30693;&#35782;&#31995;&#32479;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09727
&lt;/p&gt;
&lt;p&gt;
RAG-based constructions are more efficient than models produced with FN for the development of AI-driven knowledge-based systems.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09727v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#39046;&#22495; &#25688;&#35201;: &#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(G-LLM)&#30340;&#21457;&#23637;&#20026;&#31867;&#20284;ChatGPT&#12289;Bing&#25110;Gemini&#30340;&#26032;&#22411;&#30693;&#35782;&#31995;&#32479;&#30340;&#24320;&#21457;&#25171;&#24320;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#24494;&#35843;(FN)&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#26159;&#21487;&#29992;&#20110;&#23454;&#29616;&#22522;&#20110;G-LLM&#30340;&#30693;&#35782;&#31995;&#32479;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#25216;&#26415;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;ROUGE&#12289;BLEU&#12289;METEOR&#20998;&#25968;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#27604;&#36739;&#24182;&#26816;&#39564;&#20102;GPT-J-6B&#12289;OPT-6.7B&#12289;LlaMA&#12289;LlaMA-2&#35821;&#35328;&#27169;&#22411;&#30340;RAG&#21644;FN&#30340;&#34920;&#29616;&#12290;&#22522;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;RAG&#30340;&#26500;&#24314;&#27604;&#20351;&#29992;FN&#20135;&#29983;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#25351;&#20986;&#23558;RAG&#21644;FN&#36830;&#25509;&#36215;&#26469;&#24182;&#19981;&#26159;&#36731;&#32780;&#26131;&#20030;&#30340;&#65292;&#22240;&#20026;&#23558;FN&#27169;&#22411;&#19982;RAG&#36830;&#25509;&#21487;&#33021;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;RAG&#30340;&#26550;&#26500;&#65292;&#24179;&#22343;&#22312;RO&#26041;&#38754;&#20248;&#20110;FN&#27169;&#22411;16%
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09727v1 Announce Type: cross  Abstract: The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance. Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16% in terms of the RO
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#23567;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25918;&#23556;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24494;&#35843;&#20855;&#26377;27&#20159;&#21442;&#25968;&#30340;Phi-2&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#30340;RadPhi-2-Base&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09725</link><description>&lt;p&gt;
RAD-PHI2&#65306;&#20026;&#25918;&#23556;&#23398;&#35843;&#25972;PHI-2&#30340;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
RAD-PHI2: Instruction Tuning PHI-2 for Radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#23567;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25918;&#23556;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24494;&#35843;&#20855;&#26377;27&#20159;&#21442;&#25968;&#30340;Phi-2&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#30340;RadPhi-2-Base&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#22312;&#19968;&#33324;&#39046;&#22495;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#25918;&#23556;&#23398;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;SLMs&#22312;&#19968;&#33324;&#25918;&#23556;&#23398;&#30693;&#35782;&#29305;&#21035;&#26159;&#19982;&#20102;&#35299;&#30151;&#29366;&#12289;&#25918;&#23556;&#23398;&#21457;&#29616;&#30340;&#22806;&#35266;&#12289;&#37492;&#21035;&#35786;&#26029;&#12289;&#35780;&#20272;&#39044;&#21518;&#20197;&#21450;&#38024;&#23545;&#19981;&#21516;&#22120;&#23448;&#31995;&#32479;&#30142;&#30149;&#30340;&#27835;&#30103;&#26041;&#38754;&#30340;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;SLMs&#24212;&#29992;&#20110;&#22788;&#29702;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#20219;&#21153;&#22312;&#22522;&#20110;AI&#30340;&#25918;&#23556;&#23398;&#24037;&#20316;&#27969;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;Radiopaedia&#36825;&#19968;&#21327;&#20316;&#22312;&#32447;&#25918;&#23556;&#23398;&#36164;&#28304;&#20013;&#30340;&#39640;&#36136;&#37327;&#25945;&#32946;&#20869;&#23481;&#23545;&#20855;&#26377;27&#20159;&#21442;&#25968;&#30340;Phi-2&#36827;&#34892;&#24494;&#35843;&#12290;&#25152;&#24471;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;RadPhi-2-Base&#65292;&#34920;&#29616;&#20986;&#20102;&#35299;&#20915;&#19968;&#33324;&#25918;&#23556;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09725v1 Announce Type: cross  Abstract: Small Language Models (SLMs) have shown remarkable performance in general domain language understanding, reasoning and coding tasks, but their capabilities in the medical domain, particularly concerning radiology text, is less explored. In this study, we investigate the application of SLMs for general radiology knowledge specifically question answering related to understanding of symptoms, radiological appearances of findings, differential diagnosis, assessing prognosis, and suggesting treatments w.r.t diseases pertaining to different organ systems. Additionally, we explore the utility of SLMs in handling text-related tasks with respect to radiology reports within AI-driven radiology workflows. We fine-tune Phi-2, a SLM with 2.7 billion parameters using high-quality educational content from Radiopaedia, a collaborative online radiology resource. The resulting language model, RadPhi-2-Base, exhibits the ability to address general radiol
&lt;/p&gt;</description></item><item><title>&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#65292;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2403.09722</link><description>&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Prediction of readmission of patients by extracting biomedical concepts from clinical texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09722
&lt;/p&gt;
&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#65292;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#20026;&#36827;&#34892;&#26088;&#22312;&#25913;&#21892;&#20026;&#24739;&#32773;&#25552;&#20379;&#30340;&#21307;&#30103;&#26381;&#21153;&#24182;&#38477;&#20302;&#21307;&#30103;&#31995;&#32479;&#25104;&#26412;&#30340;&#30740;&#31350;&#21019;&#36896;&#20102;&#28508;&#22312;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#21307;&#23398;&#39046;&#22495;&#22791;&#21463;&#20851;&#27880;&#30340;&#19968;&#20010;&#35805;&#39064;&#26159;&#35782;&#21035;&#20986;&#21018;&#20174;&#21307;&#38498;&#20986;&#38498;&#21518;&#21487;&#33021;&#24456;&#24555;&#20877;&#27425;&#20837;&#38498;&#30340;&#24739;&#32773;&#12290;&#36825;&#31181;&#35782;&#21035;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#21644;&#23545;&#24739;&#32773;&#30005;&#23376;&#25991;&#20214;&#20013;&#30340;&#20986;&#38498;&#25253;&#21578;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#26469;&#39044;&#27979;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#20004;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65306;&#35789;&#34955;&#27169;&#22411;&#21644;&#27010;&#24565;&#34955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09722v1 Announce Type: cross  Abstract: Today, the existence of a vast amount of electronic health data has created potential capacities for conducting studies aiming to improve the medical services provided to patients and reduce the costs of the healthcare system. One of the topics that has been receiving attention in the field of medicine in recent years is the identification of patients who are likely to be re-hospitalized shortly after being discharged from the hospital. This identification can help doctors choose appropriate treatment methods, thereby reducing the rate of patient re-hospitalization and resulting in effective treatment cost reduction. In this study, the prediction of patient re-hospitalization using text mining approaches and the processing of discharge report texts in the patient's electronic file has been discussed. To this end, the performance of various machine learning models has been evaluated using two approaches: bag of word and bag of concept, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#25552;&#21450;&#22270;&#22686;&#24378;&#27169;&#22411;&#65288;GAM&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#35821;&#20041;&#25552;&#21450;&#22270;&#21644;&#24341;&#20837;&#38598;&#25104;&#22270;&#21464;&#25442;&#22120;&#27169;&#22359;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#20013;&#30340;&#29420;&#31435;&#24314;&#27169;&#23454;&#20307;&#25552;&#21450;&#21644;&#25991;&#26723;&#25552;&#31034;&#38548;&#31163;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.09721</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#25552;&#21450;&#22270;&#22686;&#24378;&#27169;&#22411;&#30340;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
A Semantic Mention Graph Augmented Model for Document-Level Event Argument Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#25552;&#21450;&#22270;&#22686;&#24378;&#27169;&#22411;&#65288;GAM&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#35821;&#20041;&#25552;&#21450;&#22270;&#21644;&#24341;&#20837;&#38598;&#25104;&#22270;&#21464;&#25442;&#22120;&#27169;&#22359;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#20013;&#30340;&#29420;&#31435;&#24314;&#27169;&#23454;&#20307;&#25552;&#21450;&#21644;&#25991;&#26723;&#25552;&#31034;&#38548;&#31163;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#65288;DEAE&#65289;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#35782;&#21035;&#35770;&#20803;&#21450;&#20854;&#29305;&#23450;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#25552;&#21450;&#22270;&#22686;&#24378;&#27169;&#22411;&#65288;GAM&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;DEAE&#20013;&#29420;&#31435;&#24314;&#27169;&#23454;&#20307;&#25552;&#21450;&#21644;&#25991;&#26723;&#25552;&#31034;&#38548;&#31163;&#30340;&#38382;&#39064;&#12290;GAM&#26500;&#24314;&#20102;&#19968;&#20010;&#25429;&#33719;&#25991;&#26723;&#21644;&#25552;&#31034;&#20869;&#37096;&#21450;&#38388;&#37096;&#20851;&#31995;&#30340;&#35821;&#20041;&#25552;&#21450;&#22270;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#38598;&#25104;&#30340;&#22270;&#21464;&#25442;&#22120;&#27169;&#22359;&#26469;&#26377;&#25928;&#22788;&#29702;&#25552;&#21450;&#21450;&#20854;&#19977;&#31181;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09721v1 Announce Type: cross  Abstract: Document-level Event Argument Extraction (DEAE) aims to identify arguments and their specific roles from an unstructured document. The advanced approaches on DEAE utilize prompt-based methods to guide pre-trained language models (PLMs) in extracting arguments from input documents. They mainly concentrate on establishing relations between triggers and entity mentions within documents, leaving two unresolved problems: a) independent modeling of entity mentions; b) document-prompt isolation. To this end, we propose a semantic mention Graph Augmented Model (GAM) to address these two problems in this paper. Firstly, GAM constructs a semantic mention graph that captures relations within and between documents and prompts, encompassing co-existence, co-reference and co-type relations. Furthermore, we introduce an ensembled graph transformer module to address mentions and their three semantic relations effectively. Later, the graph-augmented en
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;Human Value Detection 2023&#20219;&#21153;&#20013;&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#25506;&#32034;&#65292;&#39564;&#35777;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#29702;&#35299;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;</title><link>https://arxiv.org/abs/2403.09720</link><description>&lt;p&gt;
&#24494;&#35843;&#19982;&#25552;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#20154;&#31867;&#20215;&#20540;&#35266;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning vs Prompting, Can Language Models Understand Human Values?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09720
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;Human Value Detection 2023&#20219;&#21153;&#20013;&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#25506;&#32034;&#65292;&#39564;&#35777;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#29702;&#35299;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22788;&#29702;&#21477;&#23376;&#20013;&#30340;&#28508;&#22312;&#25903;&#25345;&#30340;&#20215;&#20540;&#35266;&#23545;&#20110;&#29702;&#35299;&#35828;&#35805;&#32773;&#30340;&#20542;&#21521;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20013;&#21364;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#20154;&#31867;&#20215;&#20540;&#26816;&#27979;2023&#20013;&#24494;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#22312;&#36825;&#19968;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#39564;&#35777;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#26681;&#25454;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#33719;&#24471;&#30340;&#30693;&#35782;&#26377;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#19982;RLHF&#30340;&#33021;&#21147;&#24863;&#20852;&#36259;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09720v1 Announce Type: cross  Abstract: Accurately handling the underlying support values in sentences is crucial for understanding the speaker's tendencies, yet it poses a challenging task in natural language understanding (NLU). In this article, we explore the potential of fine-tuning and prompt tuning in this downstream task, using the Human Value Detection 2023. Additionally, we attempt to validate whether models can effectively solve the problem based on the knowledge acquired during the pre-training stage. Simultaneously, our interest lies in the capabilities of large language models (LLMs) aligned with RLHF in this task, and some preliminary attempts are presented.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#29992;&#20110;&#24076;&#20271;&#26469;&#35821;&#35328;&#30340;&#32467;&#35770;&#25552;&#21462;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25688;&#35201;&#21644;&#32467;&#35770;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#27169;&#22411;&#21644;&#20195;&#30721;</title><link>https://arxiv.org/abs/2403.09719</link><description>&lt;p&gt;
Mevaker: &#38024;&#23545;&#24076;&#20271;&#26469;&#35821;&#35328;&#30340;&#32467;&#35770;&#25552;&#21462;&#21644;&#20998;&#37197;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09719
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#29992;&#20110;&#24076;&#20271;&#26469;&#35821;&#35328;&#30340;&#32467;&#35770;&#25552;&#21462;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25688;&#35201;&#21644;&#32467;&#35770;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#27169;&#22411;&#21644;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#20197;&#33394;&#21015;&#22269;&#23478;&#23457;&#35745;&#38271;&#21644;&#24033;&#26597;&#21592;&#30340;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#24076;&#20271;&#26469;&#35821;&#35328;&#30340;&#25688;&#35201;MevakerSumm&#21644;&#32467;&#35770;&#25552;&#21462;MevakerConc&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20004;&#20010;&#36741;&#21161;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#32467;&#35770;&#25552;&#21462;(HeConE, HeConEspc)&#21644;&#32467;&#35770;&#20998;&#37197;(HeCross)&#30340;&#27169;&#22411;&#65292;&#26412;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26816;&#26597;&#28857;&#37117;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09719v1 Announce Type: cross  Abstract: In this paper, we introduce summarization MevakerSumm and conclusion extraction MevakerConc datasets for the Hebrew language based on the State Comptroller and Ombudsman of Israel reports, along with two auxiliary datasets. We accompany these datasets with models for conclusion extraction (HeConE, HeConEspc) and conclusion allocation (HeCross). All of the code, datasets, and model checkpoints used in this work are publicly available.
&lt;/p&gt;</description></item><item><title>TextCNN&#30340;&#20840;&#38754;&#23454;&#29616;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#31995;&#32479;&#25512;&#33616;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20026;NLP&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#24102;&#26469;&#20102;&#26032;&#30340;&#26631;&#20934;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.09718</link><description>&lt;p&gt;
&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#31995;&#32479;&#25512;&#33616;&#20043;&#38388;&#21327;&#20316;&#30340;TextCNN&#30340;&#20840;&#38754;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Implementation of TextCNN for Enhanced Collaboration between Natural Language Processing and System Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09718
&lt;/p&gt;
&lt;p&gt;
TextCNN&#30340;&#20840;&#38754;&#23454;&#29616;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#31995;&#32479;&#25512;&#33616;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20026;NLP&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#24102;&#26469;&#20102;&#26032;&#30340;&#26631;&#20934;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#20998;&#25903;&#65292;&#30740;&#31350;&#22914;&#20309;&#20351;&#35745;&#31639;&#26426;&#29702;&#35299;&#12289;&#22788;&#29702;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#12290;&#25991;&#26412;&#20998;&#31867;&#26159;NLP&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#25991;&#26412;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#25104;&#20026;NLP&#39046;&#22495;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#19982;&#25968;&#23383;&#21644;&#22270;&#29255;&#19981;&#21516;&#65292;&#25991;&#26412;&#22788;&#29702;&#24378;&#35843;&#31934;&#32454;&#22788;&#29702;&#33021;&#21147;&#12290;&#20256;&#32479;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23545;&#36755;&#20837;&#27169;&#22411;&#30340;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#24182;&#36890;&#36807;&#25163;&#21160;&#26041;&#24335;&#33719;&#24471;&#33391;&#22909;&#30340;&#26679;&#26412;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09718v1 Announce Type: cross  Abstract: Natural Language Processing (NLP) is an important branch of artificial intelligence that studies how to enable computers to understand, process, and generate human language. Text classification is a fundamental task in NLP, which aims to classify text into different predefined categories. Text classification is the most basic and classic task in natural language processing, and most of the tasks in natural language processing can be regarded as classification tasks. In recent years, deep learning has achieved great success in many research fields, and today, it has also become a standard technology in the field of NLP, which is widely integrated into text classification tasks. Unlike numbers and images, text processing emphasizes fine-grained processing ability. Traditional text classification methods generally require preprocessing the input model's text data. Additionally, they also need to obtain good sample features through manual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#24515;&#29702;&#29366;&#24577;&#36319;&#36394;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#26126;&#30830;&#24341;&#23548;&#25233;&#37057;&#30151;&#35786;&#26029;&#23548;&#21521;&#30340;&#32842;&#22825;&#65292;&#20174;&#32780;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#24739;&#32773;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#12289;&#24773;&#32490;&#25110;&#30151;&#29366;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.09717</link><description>&lt;p&gt;
&#36890;&#36807;&#24515;&#29702;&#29366;&#24577;&#36319;&#36394;&#25552;&#21319;&#38754;&#21521;&#25233;&#37057;&#30151;&#35786;&#26029;&#30340;&#32842;&#22825;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#24515;&#29702;&#29366;&#24577;&#36319;&#36394;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#26126;&#30830;&#24341;&#23548;&#25233;&#37057;&#30151;&#35786;&#26029;&#23548;&#21521;&#30340;&#32842;&#22825;&#65292;&#20174;&#32780;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#24739;&#32773;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#12289;&#24773;&#32490;&#25110;&#30151;&#29366;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#35786;&#26029;&#23548;&#21521;&#30340;&#32842;&#22825;&#26088;&#22312;&#24341;&#23548;&#24739;&#32773;&#36827;&#34892;&#33258;&#25105;&#34920;&#36798;&#65292;&#25910;&#38598;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#20851;&#38190;&#30151;&#29366;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#38598;&#20013;&#20110;&#32467;&#21512;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#21644;&#38386;&#32842;&#65292;&#27169;&#25311;&#22522;&#20110;&#35775;&#35848;&#30340;&#25233;&#37057;&#30151;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#24739;&#32773;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#12289;&#24773;&#32490;&#25110;&#30151;&#29366;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#36824;&#27809;&#26377;&#26126;&#30830;&#30340;&#26694;&#26550;&#29992;&#20110;&#24341;&#23548;&#23545;&#35805;&#65292;&#36825;&#23548;&#33268;&#19968;&#20123;&#26080;&#29992;&#30340;&#20132;&#27969;&#24433;&#21709;&#20102;&#20307;&#39564;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#24515;&#29702;&#29366;&#24577;&#36319;&#36394;&#65288;POST&#65289;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#65292;&#20197;&#26126;&#30830;&#24341;&#23548;&#25233;&#37057;&#30151;&#35786;&#26029;&#23548;&#21521;&#30340;&#32842;&#22825;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#29366;&#24577;&#28304;&#33258;&#20110;&#24515;&#29702;&#29702;&#35770;&#27169;&#22411;&#65292;&#21253;&#25324;&#22235;&#20010;&#32452;&#20214;&#65292;&#21363;&#38454;&#27573;&#12289;&#20449;&#24687;&#12289;&#24635;&#32467;&#21644;&#19979;&#19968;&#27493;&#12290;&#25105;&#20204;&#24494;&#35843;&#20102;&#19968;&#20010;LLM&#27169;&#22411;&#20197;&#29983;&#25104;&#21160;&#24577;&#24515;&#29702;&#29366;&#24577;&#65292;&#36827;&#32780;&#29992;&#20110;&#36741;&#21161;r
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09717v1 Announce Type: cross  Abstract: Depression-diagnosis-oriented chat aims to guide patients in self-expression to collect key symptoms for depression detection. Recent work focuses on combining task-oriented dialogue and chitchat to simulate the interview-based depression diagnosis. Whereas, these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues. Moreover, no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience. In this paper, we propose to integrate Psychological State Tracking (POST) within the large language model (LLM) to explicitly guide depression-diagnosis-oriented chat. Specifically, the state is adapted from a psychological theoretical model, which consists of four components, namely Stage, Information, Summary and Next. We fine-tune an LLM model to generate the dynamic psychological state, which is further used to assist r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#20197;&#38750;&#30417;&#30563;&#26041;&#24335;&#29983;&#25104;&#21477;&#27861;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#20540;&#34920;&#31034;&#30701;&#35821;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09714</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#35825;&#23548;&#35821;&#35328;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Linguistic Structure Induction from Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09714
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#20197;&#38750;&#30417;&#30563;&#26041;&#24335;&#29983;&#25104;&#21477;&#27861;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#20540;&#34920;&#31034;&#30701;&#35821;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22823;&#33041;&#20013;&#38544;&#24335;&#22320;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#26469;&#32452;&#32455;&#21333;&#35789;&#22312;&#21477;&#23376;&#20013;&#30340;&#32452;&#25104;&#65292;&#32780;&#36825;&#20123;&#32467;&#26500;&#24418;&#25104;&#20102;&#21333;&#35789;&#30340;&#32447;&#24615;&#24207;&#21015;&#12290;&#35821;&#35328;&#23398;&#23478;&#20204;&#24418;&#24335;&#21270;&#20102;&#19981;&#21516;&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#65307;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#20004;&#31181;&#21477;&#27861;&#26694;&#26550;&#26159;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#32467;&#26500;&#12290;&#30701;&#35821;&#32467;&#26500;&#23558;&#21477;&#23376;&#34920;&#31034;&#20026;&#30701;&#35821;&#30340;&#23884;&#22871;&#32452;&#65292;&#32780;&#20381;&#23384;&#32467;&#26500;&#36890;&#36807;&#20026;&#21333;&#35789;&#20043;&#38388;&#20998;&#37197;&#20851;&#31995;&#26469;&#34920;&#31034;&#21477;&#23376;&#12290;&#26368;&#36817;&#65292;&#23545;&#26234;&#33021;&#26426;&#22120;&#30340;&#36861;&#27714;&#20135;&#29983;&#20102;&#33021;&#22815;&#20197;&#20154;&#31867;&#27700;&#24179;&#23436;&#25104;&#35768;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#12290;&#35768;&#22810;&#30740;&#31350;&#29616;&#22312;&#36136;&#30097;LMs&#26159;&#21542;&#38544;&#24335;&#22320;&#34920;&#31034;&#21477;&#27861;&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#38598;&#20013;&#20110;&#22312;&#38750;&#30417;&#30563;&#35774;&#32622;&#20013;&#20174;LMs&#20013;&#29983;&#25104;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#32467;&#26500;&#12290;&#25105;&#22238;&#39038;&#20102;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#19968;&#39033;&#21033;&#29992;&#20108;&#20803;&#30701;&#35821;&#32467;&#26500;&#26641;&#30340;&#25968;&#20540;&#34920;&#31034;&#65288;&#35821;&#27861;&#36317;&#31163;&#65289;&#30340;&#24037;&#20316;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09714v1 Announce Type: cross  Abstract: Linear sequences of words are implicitly represented in our brains by hierarchical structures that organize the composition of words in sentences. Linguists formalize different frameworks to model this hierarchy; two of the most common syntactic frameworks are Constituency and Dependency. Constituency represents sentences as nested groups of phrases, while dependency represents a sentence by assigning relations between its words. Recently, the pursuit of intelligent machines has produced Language Models (LMs) capable of solving many language tasks with a human-level performance. Many studies now question whether LMs implicitly represent syntactic hierarchies. This thesis focuses on producing constituency and dependency structures from LMs in an unsupervised setting. I review the critical methods in this field and highlight a line of work that utilizes a numerical representation for binary constituency trees (Syntactic Distance). I pres
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;HyEnA&#65292;&#29992;&#20110;&#20174;&#24847;&#35265;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#36895;&#24230;&#21644;&#20154;&#31867;&#29702;&#35299;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09713</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#30340;&#28151;&#21512;&#26234;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Intelligence Method for Argument Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09713
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;HyEnA&#65292;&#29992;&#20110;&#20174;&#24847;&#35265;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#36895;&#24230;&#21644;&#20154;&#31867;&#29702;&#35299;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35843;&#26597;&#24037;&#20855;&#33021;&#22815;&#25910;&#38598;&#20844;&#27665;&#21453;&#39304;&#24847;&#35265;&#35821;&#26009;&#24211;&#12290;&#20174;&#24222;&#22823;&#19988;&#22024;&#26434;&#30340;&#24847;&#35265;&#38598;&#20013;&#25552;&#21462;&#20851;&#38190;&#35770;&#28857;&#26377;&#21161;&#20110;&#24555;&#36895;&#20934;&#30830;&#22320;&#29702;&#35299;&#24847;&#35265;&#12290;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#35770;&#28857;&#65292;&#20294;(1)&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#36739;&#39640;&#30340;&#27880;&#37322;&#25104;&#26412;; (2)&#23545;&#24050;&#30693;&#35266;&#28857;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#23545;&#26032;&#39062;&#35266;&#28857;&#25928;&#26524;&#27424;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyEnA&#65292;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20027;&#35266;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#30340;&#36895;&#24230;&#21644;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#20102;HyEnA&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#26041;&#38754;&#65292;&#19982;&#19968;&#32452;&#21508;&#31181;&#24847;&#35265;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;HyEnA&#22312;&#39640;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#35777;&#23454;&#20102;&#20154;&#31867;&#27934;&#23519;&#30340;&#24517;&#35201;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;HyEnA&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#21147;&#24037;&#20316;&#37327;&#65292;&#19988;&#19981;&#20250;&#29306;&#29298;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09713v1 Announce Type: new  Abstract: Large-scale survey tools enable the collection of citizen feedback in opinion corpora. Extracting the key arguments from a large and noisy set of opinions helps in understanding the opinions quickly and accurately. Fully automated methods can extract arguments but (1) require large labeled datasets that induce large annotation costs and (2) work well for known viewpoints, but not for novel points of view. We propose HyEnA, a hybrid (human + AI) method for extracting arguments from opinionated texts, combining the speed of automated processing with the understanding and reasoning capabilities of humans. We evaluate HyEnA on three citizen feedback corpora. We find that, on the one hand, HyEnA achieves higher coverage and precision than a state-of-the-art automated method when compared to a common set of diverse opinions, justifying the need for human insight. On the other hand, HyEnA requires less human effort and does not compromise quali
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#27880;&#20837;&#35838;&#31243;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;KICP&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#21644;&#21033;&#29992;&#20197;&#35299;&#20915;KBQA&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.09712</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38382;&#31572;&#30340;&#30693;&#35782;&#27880;&#20837;&#35838;&#31243;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Knowledge-Injected Curriculum Pretraining Framework for Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09712
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#27880;&#20837;&#35838;&#31243;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;KICP&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#21644;&#21033;&#29992;&#20197;&#35299;&#20915;KBQA&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#27880;&#20837;&#38382;&#31572;&#65288;KBQA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20063;&#26159;&#19968;&#31181;&#35775;&#38382;&#32593;&#32476;&#25968;&#25454;&#21644;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#27880;&#20837;&#35838;&#31243;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;KICP&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KBQA&#20219;&#21153;&#30340;&#20840;&#38754;KG&#23398;&#20064;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09712v1 Announce Type: cross  Abstract: Knowledge-based question answering (KBQA) is a key task in NLP research, and also an approach to access the web data and knowledge, which requires exploiting knowledge graphs (KGs) for reasoning. In the literature, one promising solution for KBQA is to incorporate the pretrained language model (LM) with KGs by generating KG-centered pretraining corpus, which has shown its superiority. However, these methods often depend on specific techniques and resources to work, which may not always be available and restrict its application. Moreover, existing methods focus more on improving language understanding with KGs, while neglect the more important human-like complex reasoning. To this end, in this paper, we propose a general Knowledge-Injected Curriculum Pretraining framework (KICP) to achieve comprehensive KG learning and exploitation for KBQA tasks, which is composed of knowledge injection (KI), knowledge adaptation (KA) and curriculum re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#22797;&#26434;SQL&#26597;&#35810;&#30340;schema-aware&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;schema&#38142;&#25509;&#37492;&#21035;&#22120;&#21644;&#23450;&#20041;6&#31181;&#20851;&#31995;&#31867;&#22411;&#26469;&#35299;&#20915;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09706</link><description>&lt;p&gt;
&#22797;&#26434;&#25991;&#26412;&#21040;SQL&#30340;Schema-Aware&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Schema-Aware Multi-Task Learning for Complex Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09706
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#22797;&#26434;SQL&#26597;&#35810;&#30340;schema-aware&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;schema&#38142;&#25509;&#37492;&#21035;&#22120;&#21644;&#23450;&#20041;6&#31181;&#20851;&#31995;&#31867;&#22411;&#26469;&#35299;&#20915;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#22120;&#22312;&#21512;&#25104;&#28041;&#21450;&#22810;&#20010;&#34920;&#25110;&#21015;&#30340;&#22797;&#26434;SQL&#26597;&#35810;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#22240;&#20026;&#35782;&#21035;&#27491;&#30830;&#30340;schema&#39033;&#21644;&#22312;&#38382;&#39064;&#19982;schema&#39033;&#20043;&#38388;&#36827;&#34892;&#20934;&#30830;&#23545;&#40784;&#30340;&#25361;&#25112;&#22266;&#26377;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;SQL&#26597;&#35810;&#30340;schema-aware&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65288;&#31216;&#20026;MTSQL&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;schema&#38142;&#25509;&#37492;&#21035;&#22120;&#27169;&#22359;&#26469;&#21306;&#20998;&#26377;&#25928;&#30340;&#38382;&#39064;-schema&#38142;&#25509;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#38142;&#25509;&#20851;&#31995;&#26126;&#30830;&#25351;&#23548;&#32534;&#30721;&#22120;&#20197;&#22686;&#24378;&#23545;&#40784;&#36136;&#37327;&#12290;&#22312;&#35299;&#30721;&#22120;&#26041;&#38754;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;6&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#26469;&#25551;&#36848;&#34920;&#21644;&#21015;&#20043;&#38388;&#30340;&#36830;&#25509;&#65288;&#20363;&#22914;&#65292;WHERE_TC&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#20197;&#25805;&#20316;&#31526;&#20026;&#20013;&#24515;&#30340;&#19977;&#20803;&#25552;&#21462;&#22120;&#26469;&#35782;&#21035;&#37027;&#20123;&#19982;&#39044;&#23450;&#20041;&#20851;&#31995;&#30456;&#20851;&#30340;schema&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#30340;&#19977;&#20803;&#32452;&#24314;&#31435;&#20102;&#19968;&#32452;&#35821;&#27861;&#32422;&#26463;&#35268;&#21017;&#38598;&#65292;&#20197;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09706v1 Announce Type: cross  Abstract: Conventional text-to-SQL parsers are not good at synthesizing complex SQL queries that involve multiple tables or columns, due to the challenges inherent in identifying the correct schema items and performing accurate alignment between question and schema items. To address the above issue, we present a schema-aware multi-task learning framework (named MTSQL) for complicated SQL queries. Specifically, we design a schema linking discriminator module to distinguish the valid question-schema linkings, which explicitly instructs the encoder by distinctive linking relations to enhance the alignment quality. On the decoder side, we define 6-type relationships to describe the connections between tables and columns (e.g., WHERE_TC), and introduce an operator-centric triple extractor to recognize those associated schema items with the predefined relationship. Also, we establish a rule set of grammar constraints via the predicted triples to filte
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#24494;&#22937;&#23545;&#35805;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;GPT4 Turbo&#21487;&#20197;&#19982;&#24050;&#39564;&#35777;&#30340;&#27835;&#30103;&#24072;&#34920;&#29616;&#20986;&#26356;&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09705</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#26032;&#39062;&#32454;&#33268;&#23545;&#35805;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09705
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#24494;&#22937;&#23545;&#35805;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;GPT4 Turbo&#21487;&#20197;&#19982;&#24050;&#39564;&#35777;&#30340;&#27835;&#30103;&#24072;&#34920;&#29616;&#20986;&#26356;&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23545;&#35805;&#33021;&#21147;&#21487;&#20197;&#24110;&#21161;&#20854;&#26356;&#35880;&#24910;&#21644;&#36866;&#24403;&#22320;&#37096;&#32626;&#65292;&#23545;&#20110;&#20687;&#24515;&#29702;&#20581;&#24247;&#36825;&#26679;&#30340;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#20013;&#26576;&#20154;&#30340;&#29983;&#21629;&#21487;&#33021;&#21462;&#20915;&#20110;&#23545;&#32039;&#24613;&#38382;&#39064;&#22238;&#22797;&#30340;&#30830;&#20999;&#25514;&#36766;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;LLMs&#24494;&#22937;&#23545;&#35805;&#33021;&#21147;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#22312;&#20854;&#20013;&#65292;&#25105;&#20204;&#20174;&#24515;&#29702;&#27835;&#30103;&#23545;&#35805;&#20998;&#26512;&#25991;&#29486;&#20013;&#21457;&#23637;&#20102;&#19968;&#31995;&#21015;&#23450;&#37327;&#25351;&#26631;&#12290;&#34429;&#28982;&#25105;&#20204;&#30830;&#20445;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#25351;&#26631;&#21487;&#20379;&#30740;&#31350;&#20154;&#21592;&#36716;&#31227;&#21040;&#30456;&#20851;&#37051;&#22495;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#24212;&#29992;&#21040;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#39564;&#35777;&#30340;&#24515;&#29702;&#20581;&#24247;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#21069;&#27839;LLMs&#27169;&#22411;&#65292;&#21253;&#25324;&#19968;&#20123;GPT&#21644;Llama&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;GPT4 Turbo&#22312;&#34920;&#29616;&#19978;&#19982;&#24050;&#39564;&#35777;&#30340;&#27835;&#30103;&#24072;&#30456;&#27604;&#19982;&#20854;&#20182;&#27169;&#22411;&#26356;&#20026;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09705v1 Announce Type: cross  Abstract: Understanding the conversation abilities of Large Language Models (LLMs) can help lead to its more cautious and appropriate deployment. This is especially important for safety-critical domains like mental health, where someone's life may depend on the exact wording of a response to an urgent question. In this paper, we propose a novel framework for evaluating the nuanced conversation abilities of LLMs. Within it, we develop a series of quantitative metrics developed from literature on using psychotherapy conversation analysis literature. While we ensure that our framework and metrics are transferable by researchers to relevant adjacent domains, we apply them to the mental health field. We use our framework to evaluate several popular frontier LLMs, including some GPT and Llama models, through a verified mental health dataset. Our results show that GPT4 Turbo can perform significantly more similarly to verified therapists than other sel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Alignment Studio &#26550;&#26500;&#65292;&#20351;&#24212;&#29992;&#24320;&#21457;&#32773;&#33021;&#22815;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33267;&#20182;&#20204;&#29305;&#23450;&#30340;&#20215;&#20540;&#35266;&#12289;&#31038;&#20250;&#35268;&#33539;&#12289;&#27861;&#24459;&#21644;&#20854;&#20182;&#27861;&#35268;&#65292;&#24182;&#21327;&#35843;&#28508;&#22312;&#20914;&#31361;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.09704</link><description>&lt;p&gt;
&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33267;&#29305;&#23450;&#24773;&#22659;&#35268;&#33539;&#30340; Alignment Studio
&lt;/p&gt;
&lt;p&gt;
Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Alignment Studio &#26550;&#26500;&#65292;&#20351;&#24212;&#29992;&#24320;&#21457;&#32773;&#33021;&#22815;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33267;&#20182;&#20204;&#29305;&#23450;&#30340;&#20215;&#20540;&#35266;&#12289;&#31038;&#20250;&#35268;&#33539;&#12289;&#27861;&#24459;&#21644;&#20854;&#20182;&#27861;&#35268;&#65292;&#24182;&#21327;&#35843;&#28508;&#22312;&#20914;&#31361;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#36890;&#24120;&#30001;&#27169;&#22411;&#25552;&#20379;&#32773;&#36827;&#34892;&#65292;&#20197;&#28155;&#21152;&#25110;&#25511;&#21046;&#36328;&#29992;&#20363;&#21644;&#24773;&#22659;&#20013;&#36890;&#29992;&#25110;&#26222;&#36941;&#29702;&#35299;&#30340;&#34892;&#20026;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21644;&#26550;&#26500;&#65292;&#36171;&#20104;&#24212;&#29992;&#24320;&#21457;&#32773;&#35843;&#25972;&#27169;&#22411;&#33267;&#20854;&#29305;&#23450;&#20215;&#20540;&#35266;&#12289;&#31038;&#20250;&#35268;&#33539;&#12289;&#27861;&#24459;&#21644;&#20854;&#20182;&#27861;&#35268;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#24773;&#22659;&#20013;&#21327;&#35843;&#28508;&#22312;&#20914;&#31361;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#36825;&#31181;&#23545;&#40784;&#24037;&#20316;&#23460;&#26550;&#26500;&#30340;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#26500;&#26550;&#32773;&#12289;&#25351;&#23548;&#32773;&#21644;&#23457;&#26680;&#32773;&#20849;&#21516;&#20316;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20225;&#19994;&#20869;&#37096;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#40784;&#21040;&#19994;&#21153;&#34892;&#20026;&#20934;&#21017;&#30340;&#23454;&#20363;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09704v1 Announce Type: cross  Abstract: The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts. In contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws and other regulations, and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors that work in concert to control the behavior of a language model. We illustrate this approach with a running example of aligning a company's internal-facing enterprise chatbot to its business conduct guidelines.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#65288;CoAT&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#35757;&#32451;&#22330;&#26223;&#65292;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#21033;&#29992;&#31867;&#27604;&#25512;&#29702;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;CoAT&#65292;&#39044;&#35757;&#32451;&#30340;transformers&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28436;&#31034;&#20013;&#30340;&#26032;&#28508;&#22312;&#27010;&#24565;&#65292;&#20351;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#20989;&#25968;&#21464;&#25442;&#26356;&#21152; robust&#12290;</title><link>https://arxiv.org/abs/2403.09703</link><description>&lt;p&gt;
&#27010;&#24565;&#24863;&#30693;&#25968;&#25454;&#26500;&#24314;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Concept-aware Data Construction Improves In-context Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#65288;CoAT&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#35757;&#32451;&#22330;&#26223;&#65292;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#21033;&#29992;&#31867;&#27604;&#25512;&#29702;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;CoAT&#65292;&#39044;&#35757;&#32451;&#30340;transformers&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28436;&#31034;&#20013;&#30340;&#26032;&#28508;&#22312;&#27010;&#24565;&#65292;&#20351;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#20989;&#25968;&#21464;&#25442;&#26356;&#21152; robust&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#34920;&#29616;&#20026;LMs&#33021;&#22815;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#26377;&#20851;&#31574;&#21010;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#30340;&#24037;&#20316;&#20551;&#23450;ICL&#26159;&#30001;&#20110;&#24040;&#22823;&#30340;&#36807;&#21442;&#25968;&#21270;&#25110;&#22810;&#20219;&#21153;&#35757;&#32451;&#35268;&#27169;&#23548;&#33268;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#24037;&#20316;&#23558;ICL&#33021;&#21147;&#24402;&#22240;&#20110;&#27010;&#24565;&#30456;&#20851;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22312;&#23567;&#35268;&#27169;&#12289;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#20102;&#21151;&#33021;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09703v1 Announce Type: cross  Abstract: Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges from a vast over-parametrization or the scale of multi-task training. However, recent theoretical work attributes the ICL ability to concept-dependent training data and creates functional in-context learners even in small-scale, synthetic settings.   In this work, we practically explore this newly identified axis of ICL quality. We propose Concept-aware Training (CoAT), a framework for constructing training scenarios that make it beneficial for the LM to learn to utilize the analogical reasoning concepts from demonstrations. We find that by using CoAT, pre-trained transformers can learn to better utilise new latent concepts from demonstrations and that such ability makes ICL more robust to the functio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Shapley Values&#37327;&#21270;&#33402;&#26415;&#23478;&#23545;&#29983;&#25104;&#27169;&#22411;&#25152;&#20570;&#36129;&#29486;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#24320;&#21457;&#32773;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#21512;&#20316;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2403.09700</link><description>&lt;p&gt;
Shapley Values &#39537;&#21160;&#30340;&#29992;&#20110;GenAI&#29983;&#25104;&#20869;&#23481;&#30340;&#20844;&#24179;&#22870;&#21169;&#20998;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Shapley Values-Powered Framework for Fair Reward Split in Content Produced by GenAI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09700
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Shapley Values&#37327;&#21270;&#33402;&#26415;&#23478;&#23545;&#29983;&#25104;&#27169;&#22411;&#25152;&#20570;&#36129;&#29486;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#24320;&#21457;&#32773;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#21512;&#20316;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26126;&#26174;&#30340;&#26159;&#65292;&#24403;&#21069;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#34987;&#20154;&#31867;&#19987;&#19994;&#20154;&#22763;&#36229;&#36234;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#36825;&#31181;&#24046;&#36317;&#23558;&#20250;&#32553;&#23567;&#65292;&#23548;&#33268;&#37027;&#20123;&#25237;&#20837;&#20102;&#22810;&#24180;&#26102;&#38388;&#26469;&#25484;&#25569;&#19968;&#39033;&#25216;&#33021;&#30340;&#20010;&#20307;&#22240;&#20854;&#39640;&#26114;&#25104;&#26412;&#32780;&#21464;&#24471;&#36807;&#26102;&#65292;&#36825;&#31181;&#25104;&#26412;&#19982;&#20182;&#20204;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#32039;&#23494;&#30456;&#36830; -- &#19968;&#39033;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#25110;&#20960;&#31186;&#38047;&#20869;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#36991;&#20813;&#26410;&#26469;&#30340;&#31038;&#20250;&#21160;&#33633;&#65292;&#25105;&#20204;&#24517;&#39035;&#21363;&#21051;&#24605;&#32771;&#22914;&#20309;&#20844;&#24179;&#35780;&#20272;&#36825;&#20123;&#20010;&#20307;&#22312;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#36129;&#29486;&#65292;&#24182;&#22914;&#20309;&#34917;&#20607;&#20182;&#20204;&#30001;&#27492;&#32780;&#23548;&#33268;&#30340;&#25910;&#20837;&#20943;&#23569;&#25110;&#23436;&#20840;&#20007;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#26500;&#24314;&#27169;&#22411;&#24320;&#21457;&#32773;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;Shapley Values&#26469;&#37327;&#21270;Stable Diffusion-v1.5&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#33402;&#26415;&#23478;&#30340;&#36129;&#29486;&#65292;&#24182;&#20844;&#24179;&#22320;&#23545;&#24453;&#20182;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09700v1 Announce Type: cross  Abstract: It is evident that, currently, generative models are surpassed in quality by human professionals. However, with the advancements in Artificial Intelligence, this gap will narrow, leading to scenarios where individuals who have dedicated years of their lives to mastering a skill become obsolete due to their high costs, which are inherently linked to the time they require to complete a task -- a task that AI could accomplish in minutes or seconds. To avoid future social upheavals, we must, even now, contemplate how to fairly assess the contributions of such individuals in training generative models and how to compensate them for the reduction or complete loss of their incomes. In this work, we propose a method to structure collaboration between model developers and data providers. To achieve this, we employ Shapley Values to quantify the contribution of artist(s) in an image generated by the Stable Diffusion-v1.5 model and to equitably a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#26041;&#27861;&#65292;&#22312;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#25552;&#21319;&#65292;&#20197;&#21450;&#35757;&#32451;&#26102;&#38388;&#21644;&#25512;&#29702;&#26102;&#38388;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2403.09680</link><description>&lt;p&gt;
&#39044;&#25490;&#24207;Tsetlin&#26426;&#22120;&#65288;&#22522;&#22240;K-Medoid&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#26041;&#27861;&#65292;&#22312;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#25552;&#21319;&#65292;&#20197;&#21450;&#35757;&#32451;&#26102;&#38388;&#21644;&#25512;&#29702;&#26102;&#38388;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#24555;&#36895;&#36951;&#20256;&#31639;&#27861;&#20174;&#25968;&#25454;&#38598;&#20013;&#30830;&#23450;N&#20010;&#25968;&#25454;&#28857;&#65292;&#20197;&#35299;&#20915;&#26368;&#22823;&#31163;&#25955;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34987;&#29992;&#20316;&#36816;&#34892;K-Medoid&#32858;&#31867;&#31639;&#27861;&#30340;&#21021;&#22987;&#25918;&#32622;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#24555;&#36895;&#36951;&#20256;&#31639;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#27721;&#26126;&#36317;&#31163;&#26469;&#23545;&#40784;N&#20010;&#29420;&#31435;&#30340;Tsetlin Machines&#12290;&#23545;&#20110;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#32467;&#26524;&#26174;&#31034;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;&#39640;&#36798;10&#65285;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#32422;383&#20493;&#65292;&#25512;&#29702;&#26102;&#38388;&#20943;&#23569;&#20102;&#32422;86&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09680v1 Announce Type: cross  Abstract: This paper proposes a machine learning pre-sort stage to traditional supervised learning using Tsetlin Machines. Initially, N data-points are identified from the dataset using an expedited genetic algorithm to solve the maximum dispersion problem. These are then used as the initial placement to run the K-Medoid clustering algorithm. Finally, an expedited genetic algorithm is used to align N independent Tsetlin Machines by maximising hamming distance. For MNIST level classification problems, results demonstrate up to 10% improvement in accuracy, approx. 383X reduction in training time and approx. 86X reduction in inference time.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27450;&#39575;&#34892;&#20026;&#24182;&#20998;&#31867;&#35752;&#35770;&#20854;&#24341;&#21457;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.09676</link><description>&lt;p&gt;
&#25581;&#24320;AI&#30340;&#38452;&#24433;&#65306;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27450;&#39575;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27450;&#39575;&#34892;&#20026;&#24182;&#20998;&#31867;&#35752;&#35770;&#20854;&#24341;&#21457;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#20154;&#24037;&#26234;&#33021;&#27450;&#39575;&#30340;&#22797;&#26434;&#39046;&#22495;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#23548;&#33322;&#65292;&#38598;&#20013;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27450;&#39575;&#34892;&#20026;&#12290;&#20316;&#32773;&#30340;&#30446;&#26631;&#26159;&#38416;&#26126;&#36825;&#19968;&#38382;&#39064;&#65292;&#23457;&#35270;&#22260;&#32469;&#23427;&#30340;&#35752;&#35770;&#65292;&#38543;&#21518;&#28145;&#20837;&#20854;&#20998;&#31867;&#21644;&#21518;&#26524;&#12290;&#25991;&#31456;&#20174;&#35780;&#20272;2033&#24180;AI&#23433;&#20840;&#23792;&#20250;&#65288;ASS&#65289;&#65292;&#20197;&#21450;LLMs&#30340;&#20171;&#32461;&#24320;&#22987;&#65292;&#24378;&#35843;&#20102;&#28508;&#22312;&#23548;&#33268;&#23427;&#20204;&#27450;&#39575;&#34892;&#20026;&#30340;&#22810;&#32500;&#20559;&#35265;&#12290;&#25991;&#29486;&#32508;&#36848;&#28085;&#30422;&#20102;&#22235;&#31181;&#20998;&#31867;&#30340;&#27450;&#39575;&#65306;&#25112;&#30053;&#27450;&#39575;&#12289;&#27169;&#20223;&#12289;&#35844;&#23194;&#21644;&#19981;&#24544;&#25512;&#29702;&#65292;&#20197;&#21450;&#23427;&#20204;&#25152;&#24102;&#26469;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#39118;&#38505;&#12290;&#26368;&#21518;&#65292;&#20316;&#32773;&#23545;&#19982;&#24212;&#23545;&#27450;&#39575;AI&#30340;&#25345;&#20037;&#25361;&#25112;&#30456;&#20851;&#30340;&#21508;&#20010;&#26041;&#38754;&#37319;&#21462;&#20102;&#35780;&#20272;&#31435;&#22330;&#12290;&#36825;&#21253;&#25324;&#32771;&#34385;&#22269;&#38469;&#21327;&#20316;&#27835;&#29702;&#12289;&#20010;&#20154;&#19982;AI&#37325;&#26032;&#26500;&#24314;&#30340;&#20114;&#21160;&#65292;&#25552;&#20986;&#23454;&#38469;&#35843;&#25972;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09676v1 Announce Type: cross  Abstract: This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reasoning, along with the social implications and risks they entail. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI. This encompasses considerations of international collaborative governance, the reconfigured engagement of individuals with AI, proposal of practical adjustments, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#65292;&#24182;&#21019;&#24314;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09673</link><description>&lt;p&gt;
FoldToken&#65306;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#21450;&#26356;&#22810;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
FoldToken: Learning Protein Language via Vector Quantization and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09673
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#65292;&#24182;&#21019;&#24314;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#21516;&#26102;&#25551;&#36848;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#22806;&#35821;&#65311;&#30001;&#20110;&#36830;&#32493;3D&#28857;&#34920;&#31034;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#19982;&#31163;&#25955;&#24207;&#21015;&#30340;&#23545;&#27604;&#24314;&#27169;&#26041;&#24335;&#65292;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;\textbf{FoldTokenizer}&#65292;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#34920;&#31034;&#20026;&#31163;&#25955;&#31526;&#21495;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#28041;&#21450;&#23558;&#27531;&#22522;&#31867;&#22411;&#21644;&#32467;&#26500;&#25237;&#23556;&#21040;&#19968;&#20010;&#31163;&#25955;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#20449;&#24687;&#20445;&#23384;&#30340;&#37325;&#26500;&#25439;&#22833;&#36827;&#34892;&#25351;&#23548;&#12290;&#25105;&#20204;&#23558;&#23398;&#20064;&#21040;&#30340;&#31163;&#25955;&#31526;&#21495;&#31216;&#20026;\textbf{FoldToken}&#65292;&#32780;FoldTokens&#30340;&#24207;&#21015;&#21017;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#65292;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#36716;&#21270;&#20026;&#19968;&#31181;&#32479;&#19968;&#30340;&#24418;&#24577;&#12290;&#25105;&#20204;&#23558;&#21019;&#24314;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#24212;&#29992;&#20110;&#26222;&#36890;&#20027;&#24178;&#20462;&#34917;&#21644;&#25239;&#20307;&#35774;&#35745;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#39318;&#20010;GPT&#39118;&#26684;&#27169;&#22411;(\textbf{FoldGPT})&#29992;&#20110;&#20855;&#26377;&#33391;&#22909;&#32467;&#26524;&#30340;&#24207;&#21015;-&#32467;&#26500;&#20849;&#29983;&#20135;&#12290;&#25105;&#20204;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#26174;&#33879;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09673v1 Announce Type: cross  Abstract: Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancem
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31995;&#32479;&#32423;&#29366;&#24577;&#35780;&#20272;&#27169;&#22411;&#21644;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#30340;&#22810;&#36793;&#21327;&#20316;&#35745;&#31639;&#23454;&#26102;&#35843;&#24230;&#22120; CoRaiS&#65292;&#20197;&#20248;&#21270;&#20998;&#21457;&#20998;&#24067;&#21040;&#36798;&#30340;&#35831;&#27714;&#65292;&#25552;&#39640;&#22810;&#36793;&#31995;&#32479;&#30340;&#21327;&#20316;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09671</link><description>&lt;p&gt;
CoRaiS: &#36731;&#37327;&#32423;&#22810;&#36793;&#21327;&#20316;&#35745;&#31639;&#23454;&#26102;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
CoRaiS: Lightweight Real-Time Scheduler for Multi-Edge Cooperative Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31995;&#32479;&#32423;&#29366;&#24577;&#35780;&#20272;&#27169;&#22411;&#21644;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#30340;&#22810;&#36793;&#21327;&#20316;&#35745;&#31639;&#23454;&#26102;&#35843;&#24230;&#22120; CoRaiS&#65292;&#20197;&#20248;&#21270;&#20998;&#21457;&#20998;&#24067;&#21040;&#36798;&#30340;&#35831;&#27714;&#65292;&#25552;&#39640;&#22810;&#36793;&#31995;&#32479;&#30340;&#21327;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36793;&#21327;&#20316;&#35745;&#31639;&#23558;&#22810;&#20010;&#36793;&#32536;&#30340;&#21463;&#38480;&#36164;&#28304;&#21512;&#24182;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#36164;&#28304;&#27744;&#65292;&#20855;&#26377;&#25552;&#20379;&#24040;&#22823;&#35745;&#31639;&#33021;&#21147;&#12289;&#25913;&#36827;&#21709;&#24212;&#26102;&#38388;&#21644;&#26356;&#22810;&#26679;&#21270;&#26381;&#21153;&#31561;&#28508;&#21147;&#24102;&#26469;&#24040;&#22823;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#24322;&#26500;&#36164;&#28304;&#32452;&#25104;&#21644;&#32570;&#20047;&#35843;&#24230;&#31574;&#30053;&#20351;&#24471;&#22810;&#36793;&#35745;&#31639;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#21327;&#20316;&#21464;&#24471;&#29305;&#21035;&#22797;&#26434;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#32423;&#29366;&#24577;&#35780;&#20272;&#27169;&#22411;&#65292;&#29992;&#20110;&#20445;&#25252;&#22797;&#26434;&#30340;&#30828;&#20214;&#37197;&#32622;&#24182;&#37325;&#26032;&#23450;&#20041;&#24322;&#26500;&#36793;&#32536;&#30340;&#19981;&#21516;&#26381;&#21153;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#20197;&#20415;&#26368;&#20248;&#22320;&#35843;&#24230;&#20998;&#24067;&#24335;&#21040;&#36798;&#30340;&#35831;&#27714;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#36731;&#37327;&#32423;&#23454;&#26102;&#35843;&#24230;&#22120; CoRaiS&#12290;CoRaiS&#23884;&#20837;&#20102;&#22810;&#36793;&#31995;&#32479;&#30340;&#23454;&#26102;&#29366;&#24577;&#21644;&#35831;&#27714;&#20449;&#24687;&#65292;&#24182;&#23558;&#36825;&#20123;&#23884;&#20837;&#19982;&#31574;&#30053;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09671v1 Announce Type: cross  Abstract: Multi-edge cooperative computing that combines constrained resources of multiple edges into a powerful resource pool has the potential to deliver great benefits, such as a tremendous computing power, improved response time, more diversified services. However, the mass heterogeneous resources composition and lack of scheduling strategies make the modeling and cooperating of multi-edge computing system particularly complicated. This paper first proposes a system-level state evaluation model to shield the complex hardware configurations and redefine the different service capabilities at heterogeneous edges. Secondly, an integer linear programming model is designed to cater for optimally dispatching the distributed arriving requests. Finally, a learning-based lightweight real-time scheduler, CoRaiS, is proposed. CoRaiS embeds the real-time states of multi-edge system and requests information, and combines the embeddings with a policy netwo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;STREAM&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#24403;&#21069;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#23545;&#20110;&#26102;&#31354;&#29305;&#24615;&#30340;&#19981;&#36275;</title><link>https://arxiv.org/abs/2403.09669</link><description>&lt;p&gt;
STREAM&#65306;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#31354;&#35780;&#20272;&#21644;&#20998;&#26512;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09669
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;STREAM&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#24403;&#21069;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#23545;&#20110;&#26102;&#31354;&#29305;&#24615;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36924;&#30495;&#22810;&#26679;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24471;&#30410;&#20110;&#21508;&#31181;&#35780;&#20272;&#24230;&#37327;&#30340;&#20840;&#38754;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#30701;&#35270;&#39057;&#29255;&#27573;&#26102;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#32570;&#20047;&#25552;&#20379;&#25913;&#36827;&#35265;&#35299;&#30340;&#24037;&#20855;&#12290;&#30446;&#21069;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#26159;&#36890;&#36807;&#23558;&#23884;&#20837;&#35270;&#39057;&#23884;&#20837;&#32593;&#32476;&#26469;&#31616;&#21333;&#35843;&#25972;&#22270;&#20687;&#24230;&#37327;&#26041;&#27861;&#32780;&#24471;&#21040;&#30340;&#65292;&#36825;&#21487;&#33021;&#20302;&#20272;&#20102;&#35270;&#39057;&#30340;&#29420;&#29305;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;Frechet Video Distance (FVD) &#22312;&#31354;&#38388;&#26041;&#38754;&#30340;&#37325;&#35270;&#31243;&#24230;&#35201;&#22823;&#20110;&#35270;&#39057;&#30340;&#26102;&#38388;&#33258;&#28982;&#24615;&#65292;&#19988;&#21463;&#21040;&#25152;&#20351;&#29992;&#30340;&#23884;&#20837;&#32593;&#32476;&#36755;&#20837;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#20165;&#38480;&#20110;16&#24103;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;&#23427;&#34920;&#29616;&#20986;&#30456;&#24403;&#22823;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#19982;&#20154;&#31867;&#35780;&#20272;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STREAM&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65292;&#29420;&#29305;&#22320;&#35774;&#35745;&#20197;&#35299;&#20915;&#24403;&#21069;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09669v1 Announce Type: cross  Abstract: Image generative models have made significant progress in generating realistic and diverse images, supported by comprehensive guidance from various evaluation metrics. However, current video generative models struggle to generate even short video clips, with limited tools that provide insights for improvements. Current video evaluation metrics are simple adaptations of image metrics by switching the embeddings with video embedding networks, which may underestimate the unique characteristics of video. Our analysis reveals that the widely used Frechet Video Distance (FVD) has a stronger emphasis on the spatial aspect than the temporal naturalness of video and is inherently constrained by the input size of the embedding networks used, limiting it to 16 frames. Additionally, it demonstrates considerable instability and diverges from human evaluations. To address the limitations, we propose STREAM, a new video evaluation metric uniquely des
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#23450;&#24615;&#21487;&#35299;&#37322;&#22270;&#65288;QXG&#65289;&#65292;&#36890;&#36807;&#23558;&#26102;&#31354;&#22270;&#21644;&#23450;&#24615;&#32422;&#26463;&#24212;&#29992;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#33021;&#22815;&#23454;&#29616;&#22312;&#33258;&#21160;&#39550;&#39542;&#36807;&#31243;&#20013;&#23545;&#22330;&#26223;&#36827;&#34892;&#29702;&#35299;&#21644;&#35299;&#37322;&#65292;&#20026;&#23454;&#26102;&#20915;&#31574;&#25552;&#20379;&#21487;&#38752;&#30340;&#22330;&#26223;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09668</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#24615;&#22330;&#26223;&#29702;&#35299;&#21644;&#35299;&#37322;&#23454;&#29616;&#21487;&#20449;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Automated Driving through Qualitative Scene Understanding and Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09668
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23450;&#24615;&#21487;&#35299;&#37322;&#22270;&#65288;QXG&#65289;&#65292;&#36890;&#36807;&#23558;&#26102;&#31354;&#22270;&#21644;&#23450;&#24615;&#32422;&#26463;&#24212;&#29992;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#33021;&#22815;&#23454;&#29616;&#22312;&#33258;&#21160;&#39550;&#39542;&#36807;&#31243;&#20013;&#23545;&#22330;&#26223;&#36827;&#34892;&#29702;&#35299;&#21644;&#35299;&#37322;&#65292;&#20026;&#23454;&#26102;&#20915;&#31574;&#25552;&#20379;&#21487;&#38752;&#30340;&#22330;&#26223;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23450;&#24615;&#21487;&#35299;&#37322;&#22270;&#65288;QXG&#65289;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#31526;&#21495;&#21644;&#23450;&#24615;&#34920;&#31034;&#65292;&#29992;&#20110;&#22478;&#24066;&#31227;&#21160;&#20013;&#30340;&#22330;&#26223;&#29702;&#35299;&#12290;QXG&#21033;&#29992;&#26102;&#31354;&#22270;&#21644;&#23450;&#24615;&#32422;&#26463;&#20174;&#21407;&#22987;&#20256;&#24863;&#22120;&#36755;&#20837;&#65288;&#22914;LiDAR&#21644;&#25668;&#20687;&#22836;&#25968;&#25454;&#65289;&#20013;&#25552;&#21462;&#22330;&#26223;&#35821;&#20041;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#22330;&#26223;&#27169;&#22411;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;QXG&#21487;&#20197;&#22312;&#23454;&#26102;&#20013;&#36827;&#34892;&#22686;&#37327;&#26500;&#24314;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#36710;&#20869;&#35299;&#37322;&#21644;&#21508;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#30340;&#23454;&#26102;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;QXG&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#65292;&#23427;&#36890;&#36807;&#23558;&#22270;&#19982;&#36710;&#36742;&#21160;&#20316;&#32852;&#31995;&#36215;&#26469;&#65292;&#38416;&#26126;&#20102;&#20915;&#31574;&#21407;&#29702;&#12290;&#36825;&#20123;&#35299;&#37322;&#26381;&#21153;&#20110;&#22810;&#31181;&#30446;&#30340;&#65292;&#20174;&#21521;&#20056;&#23458;&#25552;&#20379;&#20449;&#24687;&#21040;&#35686;&#31034;&#24369;&#21183;&#36947;&#36335;&#20351;&#29992;&#32773;&#65288;VRUs&#65289;&#65292;&#20877;&#21040;&#23454;&#29616;&#20107;&#21518;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09668v1 Announce Type: cross  Abstract: We present the Qualitative Explainable Graph (QXG): a unified symbolic and qualitative representation for scene understanding in urban mobility. QXG enables the interpretation of an automated vehicle's environment using sensor data and machine learning models. It leverages spatio-temporal graphs and qualitative constraints to extract scene semantics from raw sensor inputs, such as LiDAR and camera data, offering an intelligible scene model. Crucially, QXG can be incrementally constructed in real-time, making it a versatile tool for in-vehicle explanations and real-time decision-making across various sensor types. Our research showcases the transformative potential of QXG, particularly in the context of automated driving, where it elucidates decision rationales by linking the graph with vehicle actions. These explanations serve diverse purposes, from informing passengers and alerting vulnerable road users (VRUs) to enabling post-analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#20013;&#19968;&#20010;&#32463;&#20856;&#24037;&#20316;CycleGAN&#30340;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#65292;&#24182;&#20551;&#35774;&#36825;&#20123;&#22833;&#36133;&#19982;GAN&#30340;&#31283;&#23450;&#24615;&#26377;&#20851;&#12290;</title><link>https://arxiv.org/abs/2403.09646</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#30417;&#30563;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#21644;GAN&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Unsupervised Image-to-image translation and GAN stability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#20013;&#19968;&#20010;&#32463;&#20856;&#24037;&#20316;CycleGAN&#30340;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#65292;&#24182;&#20551;&#35774;&#36825;&#20123;&#22833;&#36133;&#19982;GAN&#30340;&#31283;&#23450;&#24615;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#30340;&#38382;&#39064;&#26082;&#26377;&#36259;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#23545;&#20854;&#20182;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#65288;&#22914;&#30528;&#33394;&#12289;&#20462;&#34917;&#12289;&#20998;&#21106;&#31561;&#65289;&#20855;&#26377;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#65288;&#38750;&#37197;&#23545;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#39640;&#24230;&#22797;&#26434;&#30340;&#25216;&#26415;&#20174;&#19968;&#20010;&#39046;&#22495;&#20013;&#25552;&#21462;&#27169;&#24335;&#24182;&#25104;&#21151;&#24212;&#29992;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#65292;&#36817;&#24180;&#26469;&#65292;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#26159;&#20854;&#20013;&#19968;&#20010;&#39318;&#27425;&#25104;&#21151;&#24212;&#29992;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21462;&#24471;&#20102;&#23454;&#38469;&#24433;&#21709;&#32780;&#38750;&#20165;&#20165;&#26159;&#29702;&#35770;&#23454;&#21147;&#23637;&#31034;&#30340;&#24778;&#20154;&#32467;&#26524;&#65292;&#36825;&#31181;&#32467;&#26524;&#20027;&#23548;&#20102;GAN&#30340;&#19990;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20010;&#32463;&#20856;&#24037;&#20316;CycleGAN [1] &#30340;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#65292;&#24182;&#20551;&#35774;&#36825;&#20123;&#22833;&#36133;&#19982;GAN&#30340;&#31283;&#23450;&#24615;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09646v1 Announce Type: cross  Abstract: The problem of image-to-image translation is one that is intruiging and challenging at the same time, for the impact potential it can have on a wide variety of other computer vision applications like colorization, inpainting, segmentation and others. Given the high-level of sophistication needed to extract patterns from one domain and successfully applying them to another, especially, in a completely unsupervised (unpaired) manner, this problem has gained much attention as of the last few years. It is one of the first problems where successful applications to deep generative models, and especially Generative Adversarial Networks achieved astounding results that are actually of realworld impact, rather than just a show of theoretical prowess; the such that has been dominating the GAN world. In this work, we study some of the failure cases of a seminal work in the field, CycleGAN [1] and hypothesize that they are GAN-stability related, a
&lt;/p&gt;</description></item><item><title>&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;</title><link>https://arxiv.org/abs/2403.09539</link><description>&lt;p&gt;
API&#20445;&#25252;&#30340;LLMs&#30340;&#26631;&#24535;&#27844;&#38706;&#19987;&#26377;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Logits of API-Protected LLMs Leak Proprietary Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09539
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21830;&#19994;&#21270;&#23548;&#33268;&#20102;&#39640;&#32423;API-only&#25509;&#20837;&#19987;&#26377;&#27169;&#22411;&#30340;&#24120;&#35265;&#23454;&#36341;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#27169;&#22411;&#26550;&#26500;&#26377;&#20445;&#23432;&#30340;&#20551;&#35774;&#65292;&#20063;&#21487;&#20197;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;API&#26597;&#35810;&#20013;&#23398;&#20064;&#20851;&#20110;API&#20445;&#25252;&#30340;LLM&#30340;&#22823;&#37327;&#38750;&#20844;&#24320;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;OpenAI&#30340;gpt-3.5-turbo&#20165;&#33457;&#36153;&#19981;&#21040;1000&#32654;&#20803;&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#38598;&#20013;&#22312;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#19978;&#65306;&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;&#20102;softmax&#29942;&#39048;&#30340;&#24433;&#21709;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#36755;&#20986;&#21040;&#23436;&#25972;&#36755;&#20986;&#31354;&#38388;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#27169;&#22411;&#22270;&#20687;&#25110;&#27169;&#22411;&#31614;&#21517;&#65292;&#20174;&#32780;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#35299;&#38145;&#20102;&#20960;&#31181;&#21151;&#33021;&#65306;&#26377;&#25928;&#21457;&#29616;LLM&#30340;&#38544;&#34255;&#22823;&#23567;&#65292;&#33719;&#21462;&#23436;&#25972;&#35789;&#27719;&#36755;&#20986;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#19981;&#21516;&#27169;&#22411;&#26356;&#26032;&#65292;&#35782;&#21035;&#32473;&#23450;&#21333;&#20010;&#23436;&#25972;LLM&#36755;&#20986;&#30340;&#28304;LLM&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08838</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08838
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33337;&#33334;&#36712;&#36857;&#32858;&#31867;&#26088;&#22312;&#23547;&#25214;&#30456;&#20284;&#30340;&#36712;&#36857;&#27169;&#24335;&#65292;&#22312;&#28023;&#19978;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#35268;&#21017;&#21644;&#38408;&#20540;&#26469;&#35782;&#21035;&#31163;&#25955;&#30340;&#33337;&#33334;&#34892;&#20026;&#65292;&#20294;&#23384;&#22312;&#26080;&#27861;&#34920;&#31034;&#28436;&#21464;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#65288;PC-HiV&#65289;&#30340;&#26041;&#27861;&#12290;PC-HiV&#39318;&#20808;&#20351;&#29992;&#20998;&#23618;&#34920;&#31034;&#23558;&#27599;&#26465;&#36712;&#36857;&#36716;&#25442;&#20026;&#34892;&#20026;&#24207;&#21015;&#65292;&#28982;&#21518;&#22522;&#20110;&#36825;&#20123;&#34920;&#31034;&#22312;&#27599;&#20010;&#26102;&#38388;&#25139;&#39044;&#27979;&#28436;&#21270;&#12290;&#36890;&#36807;&#24212;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;PC-HiV&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#12290;&#22312;&#30495;&#23454;AIS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;PC-HiV&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25429;&#25417;&#33337;&#33334;&#34892;&#20026;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08838v1 Announce Type: cross  Abstract: Vessel trajectory clustering, which aims to find similar trajectory patterns, has been widely leveraged in overwater applications. Most traditional methods use predefined rules and thresholds to identify discrete vessel behaviors. They aim for high-quality clustering and conduct clustering on entire sequences, whether the original trajectory or its sub-trajectories, failing to represent their evolution. To resolve this problem, we propose a Predictive Clustering of Hierarchical Vessel Behavior (PC-HiV). PC-HiV first uses hierarchical representations to transform every trajectory into a behavioral sequence. Then, it predicts evolution at each timestamp of the sequence based on the representations. By applying predictive clustering and latent encoding, PC-HiV improves clustering and predictions simultaneously. Experiments on real AIS datasets demonstrate PC-HiV's superiority over existing methods, showcasing its effectiveness in capturin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08281</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#25484;&#25569;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;
&lt;/p&gt;
&lt;p&gt;
Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08281
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#12289;&#32534;&#31243;&#20195;&#30721;&#21644;&#25968;&#23398;&#31526;&#21495;&#30340;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#24040;&#22823;&#65292;&#23545;&#20110;&#37027;&#20123;&#21162;&#21147;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20986;&#20102;&#22797;&#26434;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#34701;&#21512;&#24050;&#32463;&#39640;&#24230;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#26694;&#26550;UltraFuser&#21253;&#25324;&#19977;&#20010;&#24050;&#32463;&#22312;&#35821;&#35328;&#12289;&#32534;&#30721;&#21644;&#25968;&#23398;&#19978;&#24471;&#21040;&#20805;&#20998;&#35757;&#32451;&#30340;&#19987;&#23478;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#26469;&#28151;&#21512;&#19987;&#23478;&#30340;&#36755;&#20986;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20276;&#38543;&#24179;&#34913;&#37319;&#26679;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#20197;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#34701;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#32418;&#38431;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#39564;&#39640;&#20809;&#35889;&#22270;&#20687;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#26500;&#24314;&#20986;&#19968;&#20010;&#21482;&#20351;&#29992;1%&#30340;&#36755;&#20837;&#29305;&#24449;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#36739;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08017</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#26512;&#32418;&#38431;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#32418;&#38431;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#39564;&#39640;&#20809;&#35889;&#22270;&#20687;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#26500;&#24314;&#20986;&#19968;&#20010;&#21482;&#20351;&#29992;1%&#30340;&#36755;&#20837;&#29305;&#24449;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#36739;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#39046;&#22495;&#35201;&#27714;&#21487;&#38752;&#12289;&#31283;&#20581;&#19988;&#32463;&#36807;&#36136;&#37327;&#20445;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#24471;&#32418;&#38431;&#25104;&#20026;&#35782;&#21035;&#21644;&#26292;&#38706;&#28508;&#22312;&#32570;&#38519;&#21644;&#20559;&#35265;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;HYPERVIEW&#25361;&#25112;&#36187;&#20013;&#36816;&#34892;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#22303;&#22756;&#21442;&#25968;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#65292;&#23545;&#36194;&#24471;HYPERVIEW&#25361;&#25112;&#36187;&#24182;&#20316;&#20026;INTUITION-1&#39640;&#20809;&#35889;&#20219;&#21153;&#19978;&#37096;&#32626;&#27169;&#22411;&#28789;&#24863;&#30340;&#34920;&#29616;&#26368;&#20339;&#27169;&#22411;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25351;&#20986;&#21644;&#39564;&#35777;&#20851;&#38190;&#32570;&#38519;&#65292;&#26500;&#24314;&#20986;&#19968;&#20010;&#21482;&#20351;&#29992;1%&#30340;&#36755;&#20837;&#29305;&#24449;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#36739;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08017v1 Announce Type: cross  Abstract: Remote sensing (RS) applications in the space domain demand machine learning (ML) models that are reliable, robust, and quality-assured, making red teaming a vital approach for identifying and exposing potential flaws and biases. Since both fields advance independently, there is a notable gap in integrating red teaming strategies into RS. This paper introduces a methodology for examining ML models operating on hyperspectral images within the HYPERVIEW challenge, focusing on soil parameters' estimation. We use post-hoc explanation methods from the Explainable AI (XAI) domain to critically assess the best performing model that won the HYPERVIEW challenge and served as an inspiration for the model deployed on board the INTUITION-1 hyperspectral mission. Our approach effectively red teams the model by pinpointing and validating key shortcomings, constructing a model that achieves comparable performance using just 1% of the input features a
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#35780;&#20272;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#12289;&#26377;&#29992;&#21644;&#21487;&#38752;AI&#27169;&#22411;&#65292;&#24357;&#21512;AI&#27169;&#22411;&#24320;&#21457;&#19982;&#23454;&#38469;&#21463;&#30410;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;</title><link>https://arxiv.org/abs/2403.07911</link><description>&lt;p&gt;
&#31449;&#22312;FURM&#22522;&#30784;&#19978;-&#35780;&#20272;&#21307;&#30103;&#31995;&#32479;&#20013;&#20844;&#24179;&#12289;&#26377;&#29992;&#21644;&#21487;&#38752;AI&#27169;&#22411;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Standing on FURM ground -- A framework for evaluating Fair, Useful, and Reliable AI Models in healthcare systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07911
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#35780;&#20272;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#12289;&#26377;&#29992;&#21644;&#21487;&#38752;AI&#27169;&#22411;&#65292;&#24357;&#21512;AI&#27169;&#22411;&#24320;&#21457;&#19982;&#23454;&#38469;&#21463;&#30410;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25351;&#23548;&#24739;&#32773;&#25252;&#29702;&#25110;&#25805;&#20316;&#27969;&#31243;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;AI&#27169;&#22411;&#30340;&#36755;&#20986;&#12289;&#22522;&#20110;&#35813;&#36755;&#20986;&#30340;&#20915;&#31574;&#21327;&#35758;&#20197;&#21450;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#37319;&#21462;&#24517;&#35201;&#21518;&#32493;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;&#22312;&#37096;&#32626;&#21069;&#20272;&#35745;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#37096;&#32626;&#21518;&#23454;&#26102;&#30740;&#31350;&#23427;&#65292;&#23545;&#20110;&#24357;&#21512;AI&#27169;&#22411;&#24320;&#21457;&#19982;&#21487;&#23454;&#29616;&#21033;&#30410;&#20043;&#38388;&#30340;&#40511;&#27807;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#26031;&#22374;&#31119;&#21307;&#30103;&#20445;&#20581;&#30340;&#25968;&#25454;&#31185;&#23398;&#22242;&#38431;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#36890;&#36807;&#36827;&#34892;&#20262;&#29702;&#23457;&#26597;&#20197;&#35782;&#21035;&#28508;&#22312;&#30340;&#20215;&#20540;&#19981;&#21305;&#37197;&#12289;&#20223;&#30495;&#20272;&#31639;&#26377;&#29992;&#24615;&#12289;&#36130;&#21153;&#39044;&#27979;&#35780;&#20272;&#21487;&#25345;&#32493;&#24615;&#65292;&#20197;&#21450;&#20998;&#26512;&#30830;&#23450;IT&#21487;&#34892;&#24615;&#12289;&#35774;&#35745;&#37096;&#32626;&#31574;&#30053;&#65292;&#24182;&#24314;&#35758;&#21069;&#30651;&#24615;&#30417;&#27979;&#21644;&#35780;&#20272;&#35745;&#21010;&#26469;&#35782;&#21035;&#20844;&#24179;&#12289;&#26377;&#29992;&#21644;&#21487;&#38752;&#30340;AI&#27169;&#22411;&#65288;FURM&#65289;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;FURM&#35780;&#20272;&#36827;&#34892;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07911v1 Announce Type: cross  Abstract: The impact of using artificial intelligence (AI) to guide patient care or operational processes is an interplay of the AI model's output, the decision-making protocol based on that output, and the capacity of the stakeholders involved to take the necessary subsequent action. Estimating the effects of this interplay before deployment, and studying it in real time afterwards, are essential to bridge the chasm between AI model development and achievable benefit. To accomplish this, the Data Science team at Stanford Health Care has developed a mechanism to identify fair, useful and reliable AI models (FURM) by conducting an ethical review to identify potential value mismatches, simulations to estimate usefulness, financial projections to assess sustainability, as well as analyses to determine IT feasibility, design a deployment strategy, and recommend a prospective monitoring and evaluation plan. We report on FURM assessments done to evalu
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07769</link><description>&lt;p&gt;
&#23558;&#31454;&#20105;&#36716;&#21270;&#20026;&#21512;&#20316;&#65306;&#22810;Agent&#31995;&#32479;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#20195;&#32452;&#32455;&#20013;&#30340;&#38761;&#21629;&#24615;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07769
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#65288;SMA&#65289;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35745;&#31639;&#23454;&#20307;&#30340;&#21160;&#24577;&#24433;&#21709;&#65292;&#20854;&#29305;&#28857;&#26159;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20316;&#20026;&#19968;&#31181;&#38761;&#26032;&#20154;&#31867;&#29992;&#25143;&#20132;&#20114;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#21033;&#29992;&#19987;&#38376;&#30340;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#20174;&#25805;&#20316;&#32452;&#32455;&#27969;&#31243;&#21040;&#22522;&#20110;&#24212;&#29992;&#30693;&#35782;&#21644;&#20154;&#30340;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#12290; &#20808;&#21069;&#30340;&#35843;&#26597;&#26174;&#31034;&#65292;&#22312;&#22788;&#29702;&#26032;&#25361;&#25112;&#21644;&#23454;&#29992;&#20219;&#21153;&#65288;&#22914;&#24341;&#21457;&#36923;&#36753;&#25512;&#29702;&#21644;&#38382;&#39064;&#35299;&#20915;&#65289;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#24037;&#20195;&#29702;&#30340;&#33258;&#20027;&#26041;&#27861;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290; &#36824;&#32771;&#34385;&#21040;&#65292;&#20256;&#32479;&#25216;&#26415;&#65292;&#22914;&#28608;&#21457;&#24605;&#24819;&#38142;&#65292;&#38656;&#35201;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#23548;&#12290; &#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24320;&#21457;&#30340;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07769v1 Announce Type: new  Abstract: This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27493;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#34880;&#31958;&#25511;&#21046;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36716;&#25442;BG&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#32771;&#34385;&#33647;&#29289;&#20316;&#29992;&#30340;&#24310;&#36831;&#21644;&#25345;&#20037;&#24615;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07566</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27493;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#34880;&#31958;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27493;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#34880;&#31958;&#25511;&#21046;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36716;&#25442;BG&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#32771;&#34385;&#33647;&#29289;&#20316;&#29992;&#30340;&#24310;&#36831;&#21644;&#25345;&#20037;&#24615;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34880;&#31958;&#65288;BG&#65289;&#25511;&#21046;&#28041;&#21450;&#36890;&#36807;&#20307;&#22806;&#33008;&#23707;&#32032;&#27880;&#23556;&#23558;&#20010;&#20307;&#30340;&#34880;&#31958;&#32500;&#25345;&#22312;&#20581;&#24247;&#33539;&#22260;&#20869;&#65292;&#36825;&#23545;&#20110;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24739;&#32773;&#33258;&#25105;&#31649;&#29702;&#26041;&#24335;&#32321;&#29712;&#19988;&#21361;&#38505;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#21644;&#33258;&#21160;&#21270;&#30340;BG&#25511;&#21046;&#26041;&#27861;&#65292;&#20854;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26174;&#31034;&#20986;&#20316;&#20026;&#26032;&#20852;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33647;&#29289;&#27987;&#24230;&#30340;&#25351;&#25968;&#34928;&#20943;&#27169;&#22411;&#23558;BG&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#36716;&#25442;&#20026;MDP&#65292;&#32771;&#34385;&#21040;&#33647;&#29289;&#20316;&#29992;&#30340;&#24310;&#36831;&#21644;&#25345;&#20037;&#24615;&#65292;&#20174;PAE-POMDP&#65288;&#25345;&#32493;&#34892;&#21160;&#25928;&#26524;-&#37096;&#20998;&#21487;&#35265;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65289;&#21040;MDP&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#27493;DRL&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20854;&#20013;&#36824;&#20351;&#29992;&#20102;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#65288;PER&#65289;&#37319;&#26679;&#26041;&#27861;&#12290;&#19982;&#21333;&#27493;&#33258;&#20030;&#26356;&#26032;&#30456;&#27604;&#65292;&#22810;&#27493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07566v1 Announce Type: new  Abstract: Blood Glucose (BG) control involves keeping an individual's BG within a healthy range through extracorporeal insulin injections is an important task for people with type 1 diabetes. However,traditional patient self-management is cumbersome and risky. Recent research has been devoted to exploring individualized and automated BG control approaches, among which Deep Reinforcement Learning (DRL) shows potential as an emerging approach. In this paper, we use an exponential decay model of drug concentration to convert the formalization of the BG control problem, which takes into account the delay and prolongedness of drug effects, from a PAE-POMDP (Prolonged Action Effect-Partially Observable Markov Decision Process) to a MDP, and we propose a novel multi-step DRL-based algorithm to solve the problem. The Prioritized Experience Replay (PER) sampling method is also used in it. Compared to single-step bootstrapped updates, multi-step learning is
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#36866;&#21512;&#22823;&#23398;&#29983;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#19982;&#29616;&#26377;&#23478;&#20855;&#23610;&#23544;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#24182;&#26356;&#21152;&#20860;&#23481;</title><link>https://arxiv.org/abs/2403.05589</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#23398;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65306;&#19968;&#20010;&#20851;&#20110;&#20154;&#20307;&#27979;&#37327;&#12289;&#23478;&#20855;&#35774;&#35745;&#21644;ANOVA&#27979;&#35797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Computer Lab Ergonomics in Universities: A Study on Anthropometric Measurements, Furniture Design, and ANOVA Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#36866;&#21512;&#22823;&#23398;&#29983;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#19982;&#29616;&#26377;&#23478;&#20855;&#23610;&#23544;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#24182;&#26356;&#21152;&#20860;&#23481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20307;&#24037;&#31243;&#23398;&#35774;&#35745;&#30340;&#23478;&#20855;&#33021;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#36523;&#24515;&#20581;&#24247;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#25104;&#20026;&#23398;&#29983;&#23398;&#26415;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#65292;&#23427;&#20204;&#22312;&#26410;&#26469;&#23558;&#36827;&#19968;&#27493;&#26222;&#21450;&#12290;&#25105;&#20204;&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#65292;&#36866;&#21512;&#22823;&#23398;&#29983;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#30340;&#20154;&#20307;&#24037;&#31243;&#23398;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;380&#21517;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;11&#39033;&#20154;&#20307;&#27979;&#37327;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;11&#39033;&#23478;&#20855;&#23610;&#23544;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#23478;&#20855;&#65306;&#38750;&#21487;&#35843;&#26885;&#23376;&#19982;&#38750;&#21487;&#35843;&#26700;&#23376;&#65292;&#20197;&#21450;&#21487;&#35843;&#26885;&#23376;&#19982;&#38750;&#21487;&#35843;&#26700;&#23376;&#12290;&#19981;&#21305;&#37197;&#35745;&#31639;&#26174;&#31034;&#23478;&#20855;&#23610;&#23544;&#19982;&#20154;&#20307;&#27979;&#37327;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26174;&#33879;&#27700;&#24179;&#20026;5%&#30340;&#21333;&#22240;&#32032;&#26041;&#24046;&#20998;&#26512;&#27979;&#35797;&#36824;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21644;&#29616;&#26377;&#30340;&#23478;&#20855;&#23610;&#23544;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#23610;&#23544;&#26356;&#21152;&#20860;&#23481;&#65292;&#20943;&#23569;&#20102;&#19981;&#21305;&#37197;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05589v1 Announce Type: cross  Abstract: Many studies have shown how ergonomically designed furniture improves productivity and well-being. As computers have become a part of students' academic lives, they will grow further in the future. We propose anthropometric-based furniture dimensions suitable for university students to improve computer laboratory ergonomics. We collected data from 380 participants and analyzed 11 anthropometric measurements, correlating them to 11 furniture dimensions. Two types of furniture were studied: a non-adjustable chair with a non-adjustable table and an adjustable chair with a non-adjustable table. The mismatch calculation showed a significant difference between furniture dimensions and anthropometric measurements. The one-way ANOVA test with a significance level of 5% also showed a significant difference between proposed and existing furniture dimensions. The proposed dimensions were found to be more compatible and reduced mismatch percentage
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#35782;&#21035;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#33041;&#32593;&#32476;&#20013;&#30830;&#23450;&#28385;&#36275;&#20851;&#38190;&#22240;&#26524;&#20805;&#20998;&#24615;&#38656;&#27714;&#30340;&#20851;&#38190;&#22806;&#28304;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.05407</link><description>&lt;p&gt;
&#31639;&#27861;&#35782;&#21035;&#22823;&#33041;&#32593;&#32476;&#22240;&#26524;&#20805;&#20998;&#24615;&#20013;&#30340;&#20851;&#38190;&#22806;&#28304;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Identification of Essential Exogenous Nodes for Causal Sufficiency in Brain Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#35782;&#21035;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#33041;&#32593;&#32476;&#20013;&#30830;&#23450;&#28385;&#36275;&#20851;&#38190;&#22240;&#26524;&#20805;&#20998;&#24615;&#38656;&#27714;&#30340;&#20851;&#38190;&#22806;&#28304;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30740;&#31350;&#20219;&#20309;&#22240;&#26524;&#26426;&#21046;&#65292;&#22914;&#22823;&#33041;&#30340;&#22240;&#26524;&#32593;&#32476;&#26102;&#65292;&#22240;&#26524;&#20805;&#20998;&#24615;&#30340;&#20551;&#35774;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26126;&#26174;&#22320;&#65292;&#24573;&#35270;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#38169;&#35823;&#65292;&#36825;&#19968;&#20107;&#23454;&#22312;&#22823;&#33041;&#32593;&#32476;&#30340;&#22240;&#26524;&#20998;&#26512;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#35782;&#21035;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#28385;&#36275;&#22240;&#26524;&#20805;&#20998;&#24615;&#30340;&#20851;&#38190;&#22806;&#28304;&#33410;&#28857;&#65292;&#20197;&#22312;&#27492;&#31867;&#30740;&#31350;&#20013;&#36981;&#24490;&#23427;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#25429;&#25417;Peter-Clark (PC)&#31639;&#27861;&#30340;&#26412;&#36136;&#65292;&#25105;&#20204;&#23545;&#32593;&#32476;&#20869;&#30340;&#21306;&#22495;&#23545;&#20197;&#21450;&#23545;&#26469;&#33258;&#20854;&#20182;&#32593;&#32476;&#33410;&#28857;&#26465;&#20214;&#30340;&#30456;&#21516;&#23545;&#36827;&#34892;&#29420;&#31435;&#24615;&#26816;&#39564;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21033;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#21306;&#20998;&#20505;&#36873;&#28151;&#26434;&#22240;&#32032;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#38750;&#22240;&#23376;&#21270;&#21487;&#35782;&#21035;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05407v1 Announce Type: new  Abstract: In the investigation of any causal mechanisms, such as the brain's causal networks, the assumption of causal sufficiency plays a critical role. Notably, neglecting this assumption can result in significant errors, a fact that is often disregarded in the causal analysis of brain networks. In this study, we propose an algorithmic identification approach for determining essential exogenous nodes that satisfy the critical need for causal sufficiency to adhere to it in such inquiries. Our approach consists of three main steps: First, by capturing the essence of the Peter-Clark (PC) algorithm, we conduct independence tests for pairs of regions within a network, as well as for the same pairs conditioned on nodes from other networks. Next, we distinguish candidate confounders by analyzing the differences between the conditional and unconditional results, using the Kolmogorov-Smirnov test. Subsequently, we utilize Non-Factorized identifiable Vari
&lt;/p&gt;</description></item><item><title>DyRoNet&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#24182;&#32467;&#21512;&#20998;&#25903;&#32593;&#32476;&#20248;&#21270;&#27969;&#23186;&#20307;&#24863;&#30693;&#24615;&#33021;&#65292;&#20026;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#35774;&#23450;&#20102;&#26032;&#30340;&#24615;&#33021;&#26631;&#26438;</title><link>https://arxiv.org/abs/2403.05050</link><description>&lt;p&gt;
DyRoNet&#65306;&#19968;&#31181;&#20302;&#31209;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;&#21160;&#24577;&#36335;&#30001;&#32593;&#32476;&#65292;&#29992;&#20110;&#27969;&#23186;&#20307;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for Streaming Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05050
&lt;/p&gt;
&lt;p&gt;
DyRoNet&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#24182;&#32467;&#21512;&#20998;&#25903;&#32593;&#32476;&#20248;&#21270;&#27969;&#23186;&#20307;&#24863;&#30693;&#24615;&#33021;&#65292;&#20026;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#35774;&#23450;&#20102;&#26032;&#30340;&#24615;&#33021;&#26631;&#26438;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#24863;&#30693;&#26469;&#24212;&#23545;&#22797;&#26434;&#29615;&#22659;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#36335;&#30001;&#32593;&#32476;&#65288;DyRoNet&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#20197;&#22686;&#24378;&#27969;&#23186;&#20307;&#24863;&#30693;&#12290;&#36890;&#36807;&#38598;&#25104;&#19987;&#38376;&#39044;&#35757;&#32451;&#30340;&#20998;&#25903;&#32593;&#32476;&#65292;&#38024;&#23545;&#21508;&#31181;&#29615;&#22659;&#26465;&#20214;&#36827;&#34892;&#24494;&#35843;&#65292;DyRoNet&#22312;&#24310;&#36831;&#21644;&#31934;&#24230;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#20854;&#26680;&#24515;&#29305;&#24449;&#26159;&#36895;&#24230;&#36335;&#30001;&#27169;&#22359;&#65292;&#26234;&#33021;&#22320;&#23558;&#36755;&#20837;&#25968;&#25454;&#24341;&#23548;&#21040;&#26368;&#36866;&#21512;&#30340;&#20998;&#25903;&#32593;&#32476;&#65292;&#20248;&#21270;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;DyRoNet&#26377;&#25928;&#22320;&#36866;&#24212;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#65292;&#20026;&#21508;&#31181;&#22330;&#26223;&#24615;&#33021;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#26438;&#12290;DyRoNet&#19981;&#20165;&#20026;&#27969;&#23186;&#20307;&#24863;&#30693;&#24314;&#31435;&#20102;&#26032;&#30340;&#26631;&#26438;&#65292;&#36824;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#24037;&#31243;&#27934;&#35265;&#12290;&#26377;&#20851;&#26356;&#22810;&#39033;&#30446;&#20449;&#24687;&#65292;&#35831;&#35775;&#38382; https://tastevision.github.io/DyRoNet/
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05050v1 Announce Type: cross  Abstract: Autonomous driving systems demand real-time, accurate perception to navigate complex environments. Addressing this, we introduce the Dynamic Router Network (DyRoNet), a framework that innovates with low-rank dynamic routing for enhanced streaming perception. By integrating specialized pre-trained branch networks, fine-tuned for various environmental conditions, DyRoNet achieves a balance between latency and precision. Its core feature, the speed router module, intelligently directs input data to the best-suited branch network, optimizing performance. The extensive evaluations reveal that DyRoNet adapts effectively to multiple branch selection strategies, setting a new benchmark in performance across a range of scenarios. DyRoNet not only establishes a new benchmark for streaming perception but also provides valuable engineering insights for future work. More project information is available at https://tastevision.github.io/DyRoNet/
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#23545;&#20110;&#29289;&#20307;&#19982;&#32972;&#26223;&#20043;&#38388;&#22810;&#26679;&#21270;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#24341;&#20837;&#19981;&#21516;&#23545;&#35937;&#26041;&#38754;&#21464;&#21270;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.04701</link><description>&lt;p&gt;
ObjectCompose: &#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#22312;&#29289;&#20307;&#19982;&#32972;&#26223;&#32452;&#21512;&#21464;&#21270;&#19978;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04701
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#23545;&#20110;&#29289;&#20307;&#19982;&#32972;&#26223;&#20043;&#38388;&#22810;&#26679;&#21270;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#24341;&#20837;&#19981;&#21516;&#23545;&#35937;&#26041;&#38754;&#21464;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35757;&#32451;&#24182;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#20102;&#35299;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#31243;&#24230;&#23545;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#38024;&#23545;&#19981;&#21516;&#30340;&#29289;&#20307;&#19982;&#32972;&#26223;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#38887;&#24615;&#12290;&#22823;&#22810;&#25968;&#40065;&#26834;&#24615;&#35780;&#20272;&#26041;&#27861;&#24341;&#20837;&#20102;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#35825;&#23548;&#29289;&#20307;&#29305;&#24449;&#65288;&#35270;&#28857;&#12289;&#23610;&#24230;&#12289;&#39068;&#33394;&#65289;&#30340;&#21464;&#21270;&#65292;&#25110;&#32773;&#21033;&#29992;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#65288;&#23545;&#25239;&#24615;&#21464;&#21270;&#12289;&#24120;&#35265;&#30772;&#22351;&#65289;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#27169;&#25311;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#32972;&#26223;&#30340;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#22312;&#25552;&#20379;&#23545;&#35201;&#36827;&#34892;&#30340;&#26356;&#25913;&#30340;&#25511;&#21046;&#26041;&#38754;&#19981;&#36275;&#65292;&#35201;&#20040;&#25197;&#26354;&#20102;&#29289;&#20307;&#30340;&#35821;&#20041;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24341;&#20837;&#21508;&#31181;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04701v1 Announce Type: cross  Abstract: Given the large-scale multi-modal training of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment. In this work, we evaluate the resilience of current vision-based models against diverse object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Recent works have explored leveraging large language models and diffusion models to generate changes in the background. However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task. Our method, on the other hand, can induce diverse objec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#20013;&#24515;&#20219;&#21153;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38646;&#21021;&#22987;&#21270;&#30340;Shifted Window Attention&#12289;&#30456;&#20284;&#24615;&#31579;&#36873;&#26631;&#35760;&#31561;&#26041;&#24335;&#23545;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#21516;&#26102;&#25299;&#23637;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04473</link><description>&lt;p&gt;
TextMonkey&#65306;&#19968;&#31181;&#26080;&#38656;OCR&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04473
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#20013;&#24515;&#20219;&#21153;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38646;&#21021;&#22987;&#21270;&#30340;Shifted Window Attention&#12289;&#30456;&#20284;&#24615;&#31579;&#36873;&#26631;&#35760;&#31561;&#26041;&#24335;&#23545;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#21516;&#26102;&#25299;&#23637;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TextMonkey&#65292;&#19968;&#20010;&#19987;&#20026;&#25991;&#26412;&#20013;&#24515;&#20219;&#21153;&#23450;&#21046;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#65292;&#21253;&#25324;&#25991;&#26723;&#38382;&#31572;&#65288;DocVQA&#65289;&#21644;&#22330;&#26223;&#25991;&#26412;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#25913;&#36827;&#65306;&#36890;&#36807;&#37319;&#29992;&#38646;&#21021;&#22987;&#21270;&#30340;Shifted Window Attention&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#39640;&#36755;&#20837;&#20998;&#36776;&#29575;&#30340;&#36328;&#31383;&#21475;&#36830;&#25509;&#24182;&#31283;&#23450;&#20102;&#26089;&#26399;&#35757;&#32451;&#65307;&#25105;&#20204;&#20551;&#35774;&#22270;&#20687;&#21487;&#33021;&#21253;&#21547;&#20887;&#20313;&#26631;&#35760;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#20284;&#24615;&#26469;&#31579;&#36873;&#20986;&#37325;&#35201;&#26631;&#35760;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20248;&#21270;&#26631;&#35760;&#38271;&#24230;&#65292;&#36824;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25193;&#23637;&#25105;&#20204;&#27169;&#22411;&#30340;&#33021;&#21147;&#20197;&#28085;&#30422;&#25991;&#26412;&#23450;&#20301;&#21644;&#23450;&#20301;&#65292;&#24182;&#23558;&#20301;&#32622;&#20449;&#24687;&#32435;&#20837;&#21709;&#24212;&#20013;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#24182;&#20943;&#23569;&#20102;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;TextMonkey &#21487;&#20197;&#24494;&#35843;&#20197;&#33719;&#24471;&#29702;&#35299;&#28857;&#20987;&#25130;&#22270;&#21629;&#20196;&#30340;&#33021;&#21147;&#12290;&#24635;&#20307;&#26469;&#30475;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04473v1 Announce Type: cross  Abstract: We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks, including document question answering (DocVQA) and scene text analysis. Our approach introduces enhancement across several dimensions: by adopting Shifted Window Attention with zero-initialization, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability and minimize hallucinations. Additionally, TextMonkey can be finetuned to gain the ability to comprehend commands for clicking screenshots. Overall, our method notably boosts performance across
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30693;&#35782;&#34701;&#21512;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;MKF-ADS&#65292;&#37319;&#29992;STcAM&#21644;PatchST&#27169;&#22359;&#65292;&#26088;&#22312;&#25552;&#39640;IDS&#22312;CAN&#24635;&#32447;&#28431;&#27934;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#38477;&#20302;&#35823;&#25253;&#35686;&#12290;</title><link>https://arxiv.org/abs/2403.04293</link><description>&lt;p&gt;
MKF-ADS&#65306;&#29992;&#20110;&#27773;&#36710;&#30340;&#22810;&#30693;&#35782;&#34701;&#21512;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MKF-ADS: A Multi-Knowledge Fused Anomaly Detection System for Automotive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04293
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30693;&#35782;&#34701;&#21512;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;MKF-ADS&#65292;&#37319;&#29992;STcAM&#21644;PatchST&#27169;&#22359;&#65292;&#26088;&#22312;&#25552;&#39640;IDS&#22312;CAN&#24635;&#32447;&#28431;&#27934;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#38477;&#20302;&#35823;&#25253;&#35686;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITSs&#65289;&#23545;&#36710;&#36742;&#30005;&#23376;&#25511;&#21046;&#21333;&#20803;&#65288;ECUs&#65289;&#19982;&#22806;&#37096;&#19990;&#30028;&#24191;&#27867;&#36830;&#25509;&#30340;&#38656;&#27714;&#65292;&#23433;&#20840;&#24615;&#21644;&#23433;&#20840;&#24615;&#24050;&#25104;&#20026;&#20005;&#23803;&#38382;&#39064;&#12290; &#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDSs&#65289;&#22312;&#35299;&#20915;&#25511;&#21046;&#21306;&#22495;&#32593;&#32476;&#65288;CAN&#65289;&#24635;&#32447;&#28431;&#27934;&#26041;&#38754;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#23433;&#20840;&#32452;&#20214;&#12290; &#20294;&#26159;&#65292;&#22522;&#20110;&#30417;&#30563;&#30340;IDS&#26080;&#27861;&#35782;&#21035;&#22797;&#26434;&#25915;&#20987;&#65292;&#22522;&#20110;&#24322;&#24120;&#30340;IDS&#30001;&#20110;&#33021;&#21147;&#29942;&#39048;&#32780;&#20135;&#29983;&#26356;&#39640;&#30340;&#35823;&#25253;&#35686;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30693;&#35782;&#34701;&#21512;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#31216;&#20026;MKF-ADS&#12290; &#20855;&#20307;&#22320;&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#21253;&#25324;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65288;STcAM&#65289;&#27169;&#22359;&#21644;&#34917;&#19969;&#31232;&#30095;&#21464;&#25442;&#22120;&#27169;&#22359;&#65288;PatchST&#65289;&#12290; STcAM&#36890;&#36807;&#31934;&#32454;&#20462;&#21098;&#20351;&#29992;&#19968;&#32500;&#21367;&#31215;&#65288;Conv1D&#65289;&#26469;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#65292;&#38543;&#21518;&#21033;&#29992;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;Bi-LSTM&#65289;&#26469;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04293v1 Announce Type: new  Abstract: With the requirements of Intelligent Transport Systems (ITSs) for extensive connectivity of Electronic Control Units (ECUs) to the outside world, safety and security have become stringent problems. Intrusion detection systems (IDSs) are a crucial safety component in remediating Controller Area Network (CAN) bus vulnerabilities. However, supervised-based IDSs fail to identify complexity attacks and anomaly-based IDSs have higher false alarms owing to capability bottleneck. In this paper, we propose a novel multi-knowledge fused anomaly detection model, called MKF-IDS. Specifically, the method designs an integration framework, including spatial-temporal correlation with an attention mechanism (STcAM) module and patch sparse-transformer module (PatchST). The STcAM with fine-pruning uses one-dimensional convolution (Conv1D) to extract spatial features and subsequently utilizes the Bidirectional Long Short Term Memory (Bi-LSTM) to extract the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#25511;&#21046;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#65292;&#32771;&#34385;&#20102;&#36947;&#36335;&#19978;&#20854;&#20182;&#36710;&#36742;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#28608;&#21169;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.03359</link><description>&lt;p&gt;
RACE-SM:&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#24335;&#21277;&#36947;&#21512;&#27969;&#33258;&#20027;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#25511;&#21046;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#65292;&#32771;&#34385;&#20102;&#36947;&#36335;&#19978;&#20854;&#20182;&#36710;&#36742;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#28608;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#24182;&#34892;&#24335;&#21277;&#36947;&#21512;&#27969;&#22312;&#20154;&#25511;&#36710;&#36742;&#20132;&#36890;&#20013;&#20173;&#28982;&#26159;&#33258;&#20027;&#36710;&#36742;&#25511;&#21046;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#38750;&#23398;&#20064;&#22411;&#36710;&#36742;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#20381;&#36182;&#35268;&#21017;&#21644;&#20248;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#23637;&#29616;&#20102;&#24076;&#26395;&#65292;&#24182;&#21463;&#21040;&#20102;&#37325;&#35201;&#23398;&#26415;&#20851;&#27880;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#20854;&#20182;&#39640;&#36895;&#20844;&#36335;&#36710;&#36742;&#20851;&#27880;&#19981;&#36275;&#65292;&#19988;&#32463;&#24120;&#20381;&#36182;&#19981;&#20934;&#30830;&#30340;&#36947;&#36335;&#20132;&#36890;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#24182;&#34892;&#24335;&#24773;&#20917;&#24456;&#23569;&#34987;&#32771;&#34385;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21152;&#36895;&#21644;&#21464;&#36947;&#20915;&#31574;&#21046;&#23450;&#65292;&#35813;&#27169;&#22411;&#26126;&#30830;&#32771;&#34385;&#20102;&#23545;&#20110;&#36710;&#36742;&#26412;&#36523;&#21450;&#20854;&#21608;&#22260;&#36710;&#36742;&#65288;&#21487;&#33021;&#21512;&#20316;&#25110;&#19981;&#21512;&#20316;&#65289;&#30340;&#25928;&#29992;&#65292;&#20197;&#20135;&#29983;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#20989;&#25968;&#21033;&#29992;&#31038;&#20132;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03359v1 Announce Type: new  Abstract: Autonomous parallel-style on-ramp merging in human controlled traffic continues to be an existing issue for autonomous vehicle control. Existing non-learning based solutions for vehicle control rely on rules and optimization primarily. These methods have been seen to present significant challenges. Recent advancements in Deep Reinforcement Learning have shown promise and have received significant academic interest however the available learning based approaches show inadequate attention to other highway vehicles and often rely on inaccurate road traffic assumptions. In addition, the parallel-style case is rarely considered. A novel learning based model for acceleration and lane change decision making that explicitly considers the utility to both the ego vehicle and its surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable is proposed. The novel reward function makes use of Social 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#24191;&#27867;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#30456;&#20851;&#21457;&#29616;&#65292;&#33021;&#22815;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#32467;&#26524;&#65292;&#39044;&#31034;&#30528;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20849;&#21516;&#36827;&#34892;&#21457;&#29616;&#30340;&#26410;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.03230</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#32467;&#26524;&#26041;&#38754;&#36229;&#36234;&#20154;&#31867;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Large language models surpass human experts in predicting neuroscience results
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03230
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#24191;&#27867;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#30456;&#20851;&#21457;&#29616;&#65292;&#33021;&#22815;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#32467;&#26524;&#65292;&#39044;&#31034;&#30528;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20849;&#21516;&#36827;&#34892;&#21457;&#29616;&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21457;&#29616;&#24120;&#24120;&#21462;&#20915;&#20110;&#32508;&#21512;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#65292;&#36825;&#19968;&#20219;&#21153;&#21487;&#33021;&#36229;&#20986;&#20154;&#31867;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#24191;&#27867;&#30340;&#31185;&#23398;&#25991;&#29486;&#19978;&#35757;&#32451;&#30340;LLMs&#21487;&#33021;&#33021;&#22815;&#25972;&#21512;&#22024;&#26434;&#20294;&#30456;&#20851;&#30340;&#21457;&#29616;&#65292;&#20197;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#26469;&#39044;&#27979;&#26032;&#39062;&#32467;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;BrainBench&#65292;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#39044;&#27979;&#31070;&#32463;&#31185;&#23398;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#39044;&#27979;&#23454;&#39564;&#32467;&#26524;&#26041;&#38754;&#36229;&#36234;&#20102;&#19987;&#23478;&#12290;&#22312;&#31070;&#32463;&#31185;&#23398;&#25991;&#29486;&#19978;&#35843;&#25972;&#30340;&#19968;&#20010;LLM&#65292;BrainGPT&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#19982;&#20154;&#31867;&#19987;&#23478;&#19968;&#26679;&#65292;&#24403;LLMs&#23545;&#20182;&#20204;&#30340;&#39044;&#27979;&#26377;&#20449;&#24515;&#26102;&#65292;&#20182;&#20204;&#26356;&#26377;&#21487;&#33021;&#26159;&#27491;&#30830;&#30340;&#65292;&#36825;&#39044;&#31034;&#30528;&#26410;&#26469;&#20154;&#31867;&#21644;LLMs&#23558;&#21512;&#20316;&#36827;&#34892;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#38750;&#29305;&#23450;&#20110;&#31070;&#32463;&#31185;&#23398;&#65292;&#24182;&#19988;&#21487;&#36716;&#31227;&#21040;&#20854;&#20182;&#30693;&#35782;&#23494;&#38598;&#22411;&#20107;&#19994;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03230v1 Announce Type: cross  Abstract: Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00895</link><description>&lt;p&gt;
&#31934;&#30830;&#25512;&#33616;&#30340;&#31471;&#21040;&#31471;&#22270;-&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Graph-Sequential Representation Learning for Accurate Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25512;&#33616;&#31995;&#32479;&#30340;&#35768;&#22810;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#24320;&#21457;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#19978;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#34892;&#20026;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#20010;&#24615;&#21270;&#25490;&#21517;&#21644;&#19979;&#19968;&#20010;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#30410;&#30340;&#25104;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20174;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#30340;&#20449;&#21495;&#25130;&#28982;&#19981;&#21516;&#12290;&#21069;&#32773;&#30452;&#25509;&#36890;&#36807;&#19982;&#26368;&#36817;&#29289;&#21697;&#30340;&#26377;&#24207;&#20132;&#20114;&#26469;&#34920;&#31034;&#29992;&#25143;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#25429;&#25417;&#20132;&#20114;&#22270;&#20013;&#30340;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#20114;&#35757;&#32451;&#24207;&#21015;&#21644;&#22270;&#32452;&#20214;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.16363</link><description>&lt;p&gt;
LLM&#25512;&#26029;&#25581;&#31034;&#65306;&#35843;&#26597;&#19982;Roofline&#27169;&#22411;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLM Inference Unveiled: Survey and Roofline Model Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#20379;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#29420;&#29305;&#32467;&#21512;&#12290;&#34429;&#28982;&#35813;&#39046;&#22495;&#24050;&#32463;&#25193;&#23637;&#24182;&#20805;&#28385;&#27963;&#21147;&#65292;&#20294;&#33267;&#20170;&#36824;&#27809;&#26377;&#19968;&#20010;&#31616;&#26126;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;LLM&#25512;&#26029;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#28165;&#26224;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#19981;&#20165;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#36824;&#22522;&#20110;Roofline&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#12290;&#36825;&#19968;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#35782;&#21035;LLM&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20174;&#32780;&#20026;&#37096;&#32626;LLM&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27719;&#24635;&#20102;&#39640;&#25928;LLM&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20851;&#38190;&#39046;&#22495;&#65292;&#27604;&#22914;&#26435;&#37325;&#20248;&#21270;&#65288;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#23454;&#29616;&#20102;&#23545;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;PDE&#30340;&#35299;&#36827;&#34892;&#22788;&#29702;&#19982;&#25968;&#25454;&#21516;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15656</link><description>&lt;p&gt;
&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#65306;&#39044;&#27979;&#21644;&#25968;&#25454;&#21516;&#21270;&#30340;&#32479;&#19968;&#36882;&#24402;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning Semilinear Neural Operators : A Unified Recursive Framework For Prediction And Data Assimilation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15656
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21322;&#32447;&#24615;&#31070;&#32463;&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#23454;&#29616;&#20102;&#23545;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;PDE&#30340;&#35299;&#36827;&#34892;&#22788;&#29702;&#19982;&#25968;&#25454;&#21516;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#31639;&#23376;&#65288;NOs&#65289;&#29702;&#35770;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#35745;&#31639;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25551;&#36848;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#35299;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#24403;&#21069;&#22522;&#20110;NO&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22788;&#29702;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#26102;&#31354;PDE&#26102;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#21069;&#30340;NO&#29702;&#35770;&#27809;&#26377;&#25552;&#20986;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#20197;&#20415;&#26681;&#25454;&#31232;&#30095;&#37319;&#26679;&#30340;&#22024;&#26434;&#27979;&#37327;&#26377;&#25928;&#22320;&#32416;&#27491;PDE&#35299;&#30340;&#28436;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#29366;&#24577;&#31354;&#38388;&#26041;&#27861;&#26469;&#35745;&#31639;&#26080;&#38480;&#32500;&#21322;&#32447;&#24615;PDE&#30340;&#35299;&#31639;&#23376;&#12290;&#21033;&#29992;&#21322;&#32447;&#24615;PDE&#30340;&#32467;&#26500;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#38750;&#32447;&#24615;&#35266;&#27979;&#32773;&#29702;&#35770;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#36882;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#27979;&#21644;&#26657;&#27491;&#25805;&#20316;&#65292;&#20801;&#35768;&#21516;&#26102;&#36827;&#34892;&#39044;&#27979;&#21644;&#25968;&#25454;&#21516;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15656v1 Announce Type: cross  Abstract: Recent advances in the theory of Neural Operators (NOs) have enabled fast and accurate computation of the solutions to complex systems described by partial differential equations (PDEs). Despite their great success, current NO-based solutions face important challenges when dealing with spatio-temporal PDEs over long time scales. Specifically, the current theory of NOs does not present a systematic framework to perform data assimilation and efficiently correct the evolution of PDE solutions over time based on sparsely sampled noisy measurements. In this paper, we propose a learning-based state-space approach to compute the solution operators to infinite-dimensional semilinear PDEs. Exploiting the structure of semilinear PDEs and the theory of nonlinear observers in function spaces, we develop a flexible recursive method that allows for both prediction and data assimilation by combining prediction and correction operations. The proposed 
&lt;/p&gt;</description></item><item><title>BAIs&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#26367;&#31070;&#32463;-&#35748;&#30693;&#22788;&#29702;&#31649;&#32447;&#30340;&#37096;&#20998;&#65292;&#35753;&#35748;&#30693;&#21151;&#33021;&#21463;&#25439;&#30340;&#20010;&#20307;&#33021;&#22815;&#36890;&#36807;&#39640;&#23618;&#24847;&#22270;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#20363;&#22914;&#36890;&#36807;&#20027;&#35266;&#25552;&#20379;&#24847;&#22270;&#23436;&#25104;&#27169;&#25311;&#30005;&#35805;&#23545;&#35805;&#12290;</title><link>https://arxiv.org/abs/2402.15011</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#35805;&#24335;&#33041;-&#20154;&#24037;&#26234;&#33021;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
A Conversational Brain-Artificial Intelligence Interface
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15011
&lt;/p&gt;
&lt;p&gt;
BAIs&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#26367;&#31070;&#32463;-&#35748;&#30693;&#22788;&#29702;&#31649;&#32447;&#30340;&#37096;&#20998;&#65292;&#35753;&#35748;&#30693;&#21151;&#33021;&#21463;&#25439;&#30340;&#20010;&#20307;&#33021;&#22815;&#36890;&#36807;&#39640;&#23618;&#24847;&#22270;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#20363;&#22914;&#36890;&#36807;&#20027;&#35266;&#25552;&#20379;&#24847;&#22270;&#23436;&#25104;&#27169;&#25311;&#30005;&#35805;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#33041;-&#20154;&#24037;&#26234;&#33021;&#30028;&#38754;&#65288;BAIs&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#65288;BCIs&#65289;&#31867;&#21035;&#24341;&#20837;&#12290;&#19981;&#21516;&#20110;&#20381;&#36182;&#23436;&#22909;&#35748;&#30693;&#21151;&#33021;&#30340;&#20256;&#32479;BCIs&#65292;BAIs&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#21147;&#37327;&#26469;&#26367;&#20195;&#31070;&#32463;-&#35748;&#30693;&#22788;&#29702;&#31649;&#32447;&#30340;&#37096;&#20998;&#12290;BAIs&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#25552;&#20379;&#39640;&#23618;&#24847;&#22270;&#26469;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#32780;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;AI&#20195;&#29702;&#30830;&#23450;&#20302;&#23618;&#27425;&#32454;&#33410;&#12290;&#35813;&#26041;&#27861;&#23558;BCIs&#30340;&#30446;&#26631;&#21463;&#20247;&#25193;&#22823;&#21040;&#35748;&#30693;&#21151;&#33021;&#21463;&#25439;&#30340;&#20010;&#20307;&#65292;&#36825;&#26159;&#24120;&#24120;&#34987;&#25490;&#38500;&#22312;&#20256;&#32479;BCIs&#22909;&#22788;&#20043;&#22806;&#30340;&#20154;&#32676;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BAIs&#30340;&#19968;&#33324;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#23545;&#35805;&#24335;BAI&#23637;&#31034;&#20102;&#36825;&#31181;&#26032;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#30005;&#35805;&#23545;&#35805;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#23545;&#35805;&#24335;BAI&#33021;&#22815;&#23454;&#29616;&#22797;&#26434;&#36890;&#20449;&#65292;&#32780;&#26080;&#38656;&#29983;&#25104;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#35777;&#26126;&#20102;&#65292;&#23545;&#20110;&#31532;&#19968;&#27425;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15011v1 Announce Type: cross  Abstract: We introduce Brain-Artificial Intelligence Interfaces (BAIs) as a new class of Brain-Computer Interfaces (BCIs). Unlike conventional BCIs, which rely on intact cognitive capabilities, BAIs leverage the power of artificial intelligence to replace parts of the neuro-cognitive processing pipeline. BAIs allow users to accomplish complex tasks by providing high-level intentions, while a pre-trained AI agent determines low-level details. This approach enlarges the target audience of BCIs to individuals with cognitive impairments, a population often excluded from the benefits of conventional BCIs. We present the general concept of BAIs and illustrate the potential of this new approach with a Conversational BAI based on EEG. In particular, we show in an experiment with simulated phone conversations that the Conversational BAI enables complex communication without the need to generate language. Our work thus demonstrates, for the first time, th
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36890;&#36807;LLMs&#36827;&#34892;&#38646;-shot&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.10948</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36827;&#34892;&#38646;-shot&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Explainable Mental Health Analysis on Social Media by incorporating Mental Scales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36890;&#36807;LLMs&#36827;&#34892;&#38646;-shot&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#27861;&#22312;&#23481;&#37327;&#26041;&#38754;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#35268;&#27169;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#26377;&#28508;&#21147;&#25670;&#33073;&#32321;&#37325;&#30340;&#27880;&#37322;&#24182;&#25552;&#20379;&#35299;&#37322;&#12290;&#21463;&#21040;&#20351;&#29992;&#37327;&#34920;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#30340;&#24515;&#29702;&#35780;&#20272;&#23454;&#36341;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;LLMs&#32467;&#21512;&#20102;&#20004;&#20010;&#31243;&#24207;&#12290;&#39318;&#20808;&#65292;&#24739;&#32773;&#23436;&#25104;&#24515;&#29702;&#20581;&#24247;&#38382;&#21367;&#65292;&#20854;&#27425;&#65292;&#24515;&#29702;&#23398;&#23478;&#35299;&#37322;&#26469;&#33258;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#25910;&#38598;&#20449;&#24687;&#24182;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20854;&#20182;&#38646;-shot&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10948v1 Announce Type: cross  Abstract: Traditional discriminative approaches in mental health analysis are known for their strong capacity but lack interpretability and demand large-scale annotated data. On the other hand, generative approaches, such as those based on large language models (LLMs),have the potential to get rid of heavy annotations and provide explanations. However, their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is a black-box process. Inspired by the psychological assessment practice of using scales to evaluate mental states, our method incorporates two procedures via LLMs. First, the patient completes mental health questionnaires, and second, the psychologist interprets the collected information from the mental health questions and makes informed decisions. Experimental results show that our method outperforms other zero-shot methods. Our 
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10877</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#26234;&#33021;&#20307;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust agents learn causal world models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10877
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#30452;&#26377;&#20154;&#20551;&#35774;&#22240;&#26524;&#25512;&#29702;&#22312;&#24378;&#20581;&#19988;&#20855;&#26377;&#36890;&#29992;&#26234;&#33021;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#65292;&#28982;&#32780;&#19981;&#28165;&#26970;&#26234;&#33021;&#20307;&#26159;&#21542;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#25110;&#32773;&#20854;&#20182;&#24402;&#32435;&#20559;&#24046;&#26159;&#21542;&#36275;&#22815;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#34920;&#26126;&#20219;&#20309;&#33021;&#22815;&#22312;&#22823;&#37327;&#20998;&#24067;&#36716;&#21464;&#19979;&#28385;&#36275;&#21518;&#24724;&#30028;&#38480;&#30340;&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#36817;&#20284;&#22240;&#26524;&#27169;&#22411;&#65292;&#23545;&#20110;&#20248;&#21270;&#26234;&#33021;&#20307;&#26469;&#35828;&#65292;&#35813;&#36817;&#20284;&#27169;&#22411;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.03388</link><description>&lt;p&gt;
&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#34892;&#20026;&#29992;&#25143;&#20998;&#21106;&#20013;&#30340;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Delivery Optimized Discovery in Behavioral User Segmentation under Budget Constrain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03388
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#22312;&#32447;&#34892;&#20026;&#36275;&#36857;&#21487;&#20197;&#20351;&#20844;&#21496;&#21457;&#29616;&#22522;&#20110;&#34892;&#20026;&#30340;&#29992;&#25143;&#32454;&#20998;&#65292;&#24182;&#21521;&#29992;&#25143;&#21457;&#36865;&#29305;&#23450;&#32454;&#20998;&#30340;&#20449;&#24687;&#12290;&#22312;&#21457;&#29616;&#32454;&#20998;&#20043;&#21518;&#65292;&#36890;&#36807;&#20687;Facebook&#21644;Google&#36825;&#26679;&#30340;&#39318;&#36873;&#23186;&#20307;&#28192;&#36947;&#21521;&#29992;&#25143;&#21457;&#36865;&#20449;&#24687;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21482;&#26377;&#37096;&#20998;&#34892;&#20026;&#32454;&#20998;&#20013;&#30340;&#29992;&#25143;&#22312;&#23186;&#20307;&#19978;&#25214;&#21040;&#21305;&#37197;&#65292;&#24182;&#19988;&#21482;&#26377;&#20854;&#20013;&#19968;&#23567;&#37096;&#20998;&#30475;&#21040;&#28040;&#24687;&#65288;&#26333;&#20809;&#65289;&#12290;&#21363;&#20351;&#39640;&#36136;&#37327;&#30340;&#21457;&#29616;&#20063;&#20250;&#22312;&#20256;&#36882;&#22833;&#36133;&#26102;&#21464;&#24471;&#26080;&#29992;&#12290;&#35768;&#22810;&#22797;&#26434;&#30340;&#31639;&#27861;&#29992;&#20110;&#21457;&#29616;&#34892;&#20026;&#32454;&#20998;&#65292;&#28982;&#32780;&#36825;&#20123;&#31639;&#27861;&#24573;&#30053;&#20102;&#20256;&#36882;&#32452;&#20214;&#12290;&#38382;&#39064;&#21464;&#24471;&#22797;&#26434;&#26159;&#22240;&#20026;&#65288;i&#65289;&#21457;&#29616;&#26159;&#22312;&#20844;&#21496;&#25968;&#25454;&#65288;&#20363;&#22914;&#29992;&#25143;&#28857;&#20987;&#65289;&#30340;&#34892;&#20026;&#25968;&#25454;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#65292;&#32780;&#20256;&#36882;&#21017;&#26159;&#22522;&#20110;&#23186;&#20307;&#23450;&#20041;&#30340;&#38745;&#24577;&#25968;&#25454;&#31354;&#38388;&#65288;&#20363;&#22914;&#22320;&#29702;&#20301;&#32622;&#65292;&#24180;&#40836;&#65289;&#36827;&#34892;&#30340;&#65307;&#65288;ii&#65289;&#20844;&#21496;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#36816;&#20316;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users' behavioral footprints online enable firms to discover behavior-based user segments (or, segments) and deliver segment specific messages to users. Following the discovery of segments, delivery of messages to users through preferred media channels like Facebook and Google can be challenging, as only a portion of users in a behavior segment find match in a medium, and only a fraction of those matched actually see the message (exposure). Even high quality discovery becomes futile when delivery fails. Many sophisticated algorithms exist for discovering behavioral segments; however, these ignore the delivery component. The problem is compounded because (i) the discovery is performed on the behavior data space in firms' data (e.g., user clicks), while the delivery is predicated on the static data space (e.g., geo, age) as defined by media; and (ii) firms work under budget constraint. We introduce a stochastic optimization based algorithm for delivery optimized discovery of behavioral u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20840;&#21487;&#24494;&#30456;&#20851;&#39537;&#21160;&#32593;&#32476;&#29992;&#20110;X&#23556;&#32447;&#19982;CT&#22270;&#20687;&#34701;&#21512;&#30340;&#37197;&#20934;&#65292;&#36890;&#36807;&#21452;&#20998;&#25903;CNN-Transformer&#32534;&#30721;&#22120;&#23454;&#29616;&#20302;&#39057;&#20840;&#23616;&#29305;&#24449;&#21644;&#39640;&#39057;&#23616;&#37096;&#29305;&#24449;&#30340;&#25552;&#21462;&#21644;&#20998;&#31163;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#30456;&#20851;&#39537;&#21160;&#25439;&#22833;&#36827;&#34892;&#29305;&#24449;&#20998;&#35299;&#65292;&#24182;&#24212;&#29992;&#20984;&#24418;&#30456;&#20284;&#20989;&#25968;&#23398;&#20064;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#20840;&#21487;&#24494;&#23398;&#20064;&#37197;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02498</link><description>&lt;p&gt;
X&#23556;&#32447;&#19982;CT&#22270;&#20687;&#34701;&#21512;&#30340;&#20840;&#21487;&#24494;&#30456;&#20851;&#39537;&#21160;2D/3D&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Fully Differentiable Correlation-driven 2D/3D Registration for X-ray to CT Image Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20840;&#21487;&#24494;&#30456;&#20851;&#39537;&#21160;&#32593;&#32476;&#29992;&#20110;X&#23556;&#32447;&#19982;CT&#22270;&#20687;&#34701;&#21512;&#30340;&#37197;&#20934;&#65292;&#36890;&#36807;&#21452;&#20998;&#25903;CNN-Transformer&#32534;&#30721;&#22120;&#23454;&#29616;&#20302;&#39057;&#20840;&#23616;&#29305;&#24449;&#21644;&#39640;&#39057;&#23616;&#37096;&#29305;&#24449;&#30340;&#25552;&#21462;&#21644;&#20998;&#31163;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#30456;&#20851;&#39537;&#21160;&#25439;&#22833;&#36827;&#34892;&#29305;&#24449;&#20998;&#35299;&#65292;&#24182;&#24212;&#29992;&#20984;&#24418;&#30456;&#20284;&#20989;&#25968;&#23398;&#20064;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#20840;&#21487;&#24494;&#23398;&#20064;&#37197;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#21018;&#24615;2D/3D&#37197;&#20934;&#26159;&#23548;&#21521;&#22806;&#31185;&#25163;&#26415;&#24178;&#39044;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#20123;&#22522;&#20110;&#23398;&#20064;&#30340;&#20840;&#21487;&#24494;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#30410;&#30340;&#25104;&#26524;&#65292;&#20294;&#29305;&#24449;&#25552;&#21462;&#21644;&#26799;&#24230;&#20256;&#36882;&#36807;&#31243;&#20173;&#32570;&#20047;&#21487;&#25511;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#21487;&#24494;&#30456;&#20851;&#39537;&#21160;&#32593;&#32476;&#65292;&#21033;&#29992;&#21452;&#20998;&#25903;CNN-Transformer&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#20302;&#39057;&#20840;&#23616;&#29305;&#24449;&#21644;&#39640;&#39057;&#23616;&#37096;&#29305;&#24449;&#30340;&#25552;&#21462;&#21644;&#20998;&#31163;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23884;&#20449;&#24687;&#30340;&#20302;&#39057;&#29305;&#24449;&#21644;&#39640;&#39057;&#29305;&#24449;&#20998;&#35299;&#30340;&#30456;&#20851;&#39537;&#21160;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#19968;&#31181;&#23398;&#20064;&#36924;&#36817;&#20984;&#24418;&#30456;&#20284;&#20989;&#25968;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#33258;&#24314;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#20840;&#21487;&#24494;&#23398;&#20064;&#37197;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based rigid 2D/3D registration is a critical technique for fluoroscopic guided surgical interventions. In recent years, some learning-based fully differentiable methods have produced beneficial outcomes while the process of feature extraction and gradient flow transmission still lack controllability and interpretability. To alleviate these problems, in this work, we propose a novel fully differentiable correlation-driven network using a dual-branch CNN-transformer encoder which enables the network to extract and separate low-frequency global features from high-frequency local features. A correlation-driven loss is further proposed for low-frequency feature and high-frequency feature decomposition based on embedded information. Besides, a training strategy that learns to approximate a convex-shape similarity function is applied in our work. We test our approach on a in-house datasetand show that it outperforms both existing fully differentiable learning-based registration approach
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#25200;&#21160;&#30772;&#22351;&#21644;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#24494;&#35843;&#20928;&#21270;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.16352</link><description>&lt;p&gt;
&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#65306;&#25552;&#21319;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16352
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#25200;&#21160;&#30772;&#22351;&#21644;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#24494;&#35843;&#20928;&#21270;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#35748;&#20026;&#26131;&#21463;&#35774;&#35745;&#31934;&#33391;&#30340;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#12290;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#30340;&#26368;&#25104;&#21151;&#38450;&#24481;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#29305;&#23450;&#25915;&#20987;&#19979;&#30340;&#26368;&#20339;&#40065;&#26834;&#24615;&#65292;&#20294;&#26080;&#27861;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#30693;&#25915;&#20987;&#12290;&#22522;&#20110;&#23545;&#25239;&#20928;&#21270;&#65288;AP&#65289;&#30340;&#21478;&#19968;&#26377;&#25928;&#38450;&#24481;&#25216;&#26415;&#21487;&#20197;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#26080;&#27861;&#23454;&#29616;&#26368;&#20339;&#40065;&#26834;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#26631;&#20934;&#20934;&#30830;&#24615;&#38477;&#32423;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27969;&#31243;&#65292;&#31216;&#20026;&#23545;&#20928;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AToP&#65289;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#65288;RT&#65289;&#30772;&#22351;&#25200;&#21160;&#65292;&#20197;&#36991;&#20813;&#23545;&#24050;&#30693;&#25915;&#20987;&#30340;&#36807;&#24230;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26410;&#30693;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#27867;&#21270;&#65307;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#23545;&#20928;&#21270;&#22120;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65288;FT&#65289;&#65292;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16352v2 Announce Type: replace-cross  Abstract: The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. Meanwhile, both methods share one common limitation on the degraded standard accuracy. To mitigate these issues, we propose a novel pipeline called Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss. RT is essential to avoid overlearning to known attacks resulting in the robustness generalization to unseen attacks and FT is essential for the improvement of robustness. To evaluate our method in an e
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.07518</link><description>&lt;p&gt;
&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35843;&#26597;&#65306;&#20998;&#31867;&#20307;&#31995;&#12289;&#31995;&#32479;&#32508;&#36848;&#21644;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07518
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#25216;&#26415;&#20998;&#26512;&#25991;&#26412;&#65292;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#12289;&#21830;&#19994;&#21644;&#25945;&#32946;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;NLP&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#25945;&#23398;&#21644;&#23398;&#20064;&#26041;&#38754;&#30340;&#24110;&#21161;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#35299;&#20915;&#19982;&#25945;&#32946;&#39046;&#22495;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;NLP&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;&#20171;&#32461;&#30456;&#20851;&#32972;&#26223;&#24320;&#22987;&#65292;&#28982;&#21518;&#25552;&#20986;&#25945;&#32946;&#39046;&#22495;NLP&#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#26681;&#25454;&#19978;&#36848;&#20998;&#31867;&#31995;&#32479;&#35828;&#26126;&#20219;&#21153;&#23450;&#20041;&#12289;&#25361;&#25112;&#21644;&#30456;&#24212;&#30340;&#25216;&#26415;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#29616;&#26377;&#28436;&#31034;&#65292;&#24182;&#24635;&#32467;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) aims to analyze the text via techniques in the computer science field. It serves the applications in healthcare, commerce, and education domains. Particularly, NLP has been applied to the education domain to help teaching and learning. In this survey, we review recent advances in NLP with a focus on solving problems related to the education domain. In detail, we begin with introducing the relevant background. Then, we present the taxonomy of NLP in the education domain. Next, we illustrate the task definition, challenges, and corresponding techniques based on the above taxonomy. After that, we showcase some off-the-shelf demonstrations in this domain and conclude with future directions.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27431;&#30431;&#27861;&#24459;&#27861;&#35268;&#20013;&#30340;&#24212;&#29992;&#24341;&#21457;&#20102;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#23545;&#29616;&#26377;&#21644;&#25311;&#35758;&#30340;&#27861;&#24459;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2401.07348</link><description>&lt;p&gt;
&#27431;&#30431;&#27861;&#24459;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07348
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27431;&#30431;&#27861;&#24459;&#27861;&#35268;&#20013;&#30340;&#24212;&#29992;&#24341;&#21457;&#20102;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#23545;&#29616;&#26377;&#21644;&#25311;&#35758;&#30340;&#27861;&#24459;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21450;&#20854;&#21518;&#32487;&#27169;&#22411;&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#27425;&#33539;&#24335;&#36716;&#21464;&#12290;&#20808;&#36827;&#30340;LLMs&#34920;&#29616;&#20986;&#22810;&#27169;&#24577;&#24615;&#65292;&#33021;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#26684;&#24335;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#26032;&#20852;&#33258;&#27835;&#24615;&#24341;&#20837;&#20102;&#39044;&#27979;&#24615;&#21644;&#27861;&#24459;&#36981;&#20174;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;LLMs&#22312;&#27431;&#30431;&#32972;&#26223;&#19979;&#30340;&#27861;&#24459;&#21644;&#30417;&#31649;&#24433;&#21709;&#65292;&#20998;&#26512;&#20102;&#36131;&#20219;&#12289;&#38544;&#31169;&#12289;&#30693;&#35782;&#20135;&#26435;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#26041;&#38754;&#12290;&#23427;&#25209;&#21028;&#24615;&#22320;&#23457;&#35270;&#20102;&#29616;&#26377;&#21644;&#25311;&#35758;&#30340;&#27431;&#30431;&#31435;&#27861;&#65288;&#21253;&#25324;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#12299;&#33609;&#26696;&#65289;&#22312;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26222;&#36941;&#21644;LLMs&#29305;&#21035;&#25361;&#25112;&#26041;&#38754;&#30340;&#20805;&#20998;&#24615;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#31435;&#27861;&#26694;&#26550;&#20013;&#30340;&#28508;&#22312;&#24046;&#36317;&#21644;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#24314;&#35758;&#20197;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07348v2 Announce Type: replace-cross  Abstract: The advent of Generative AI, particularly through Large Language Models (LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI landscape. Advanced LLMs exhibit multimodality, handling diverse data formats, thereby broadening their application scope. However, the complexity and emergent autonomy of these models introduce challenges in predictability and legal compliance. This paper delves into the legal and regulatory implications of Generative AI and LLMs in the European Union context, analyzing aspects of liability, privacy, intellectual property, and cybersecurity. It critically examines the adequacy of the existing and proposed EU legislation, including the Artificial Intelligence Act (AIA) draft, in addressing the unique challenges posed by Generative AI in general and LLMs in particular. The paper identifies potential gaps and shortcomings in the legislative framework and proposes recommendations to ensur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#12289;&#27880;&#24847;&#26426;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#24863;&#20852;&#36259;&#21306;&#22495;&#26816;&#27979;&#31639;&#27861;&#21644;&#20840;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#26356;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;</title><link>https://arxiv.org/abs/2401.00587</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#12289;&#27880;&#24847;&#26426;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Brain Tumor Segmentation Based on Deep Learning, Attention Mechanisms, and Energy-Based Uncertainty Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#12289;&#27880;&#24847;&#26426;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#24863;&#20852;&#36259;&#21306;&#22495;&#26816;&#27979;&#31639;&#27861;&#21644;&#20840;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#26356;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#26159;&#26368;&#33268;&#21629;&#30340;&#30284;&#30151;&#20043;&#19968;&#65292;&#20854;&#27515;&#20129;&#29575;&#36229;&#36807;80%&#12290;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#35786;&#26029;&#23545;&#25552;&#39640;&#29983;&#23384;&#20960;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#20998;&#26512;&#20013;&#65292;&#23545;&#33041;&#32959;&#30244;&#30340;&#25163;&#21160;&#27880;&#37322;&#21644;&#20998;&#21106;&#21487;&#33021;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36890;&#24120;&#20250;&#20998;&#26512;&#22810;&#31181;MRI&#27169;&#24577;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#26377;&#20851;&#32959;&#30244;&#21306;&#22495;&#30340;&#29420;&#29305;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;MRI&#27169;&#24577;&#26377;&#21161;&#20110;&#20998;&#21106;&#33014;&#36136;&#30244;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#22686;&#21152;&#36807;&#25311;&#21512;&#21644;&#35745;&#31639;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24863;&#20852;&#36259;&#21306;&#22495;&#26816;&#27979;&#31639;&#27861;&#65292;&#23427;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#26399;&#38388;&#23454;&#26045;&#65292;&#20197;&#23450;&#20301;&#26174;&#33879;&#29305;&#24449;&#24182;&#21435;&#38500;&#22810;&#20313;&#30340;MRI&#25968;&#25454;&#12290;&#36825;&#20943;&#23567;&#20102;&#36755;&#20837;&#22823;&#23567;&#65292;&#20801;&#35768;&#26356;&#28608;&#36827;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#22788;&#29702;MRI&#27169;&#24577;&#20043;&#21518;&#65292;&#19968;&#20010;&#20855;&#26377;&#36719;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20840;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#23545;&#19981;&#21516;&#30340;&#33041;MRIs&#36827;&#34892;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00587v2 Announce Type: replace-cross  Abstract: Brain tumors are one of the deadliest forms of cancer with a mortality rate of over 80%. A quick and accurate diagnosis is crucial to increase the chance of survival. However, in medical analysis, the manual annotation and segmentation of a brain tumor can be a complicated task. Multiple MRI modalities are typically analyzed as they provide unique information regarding the tumor regions. Although these MRI modalities are helpful for segmenting gliomas, they tend to increase overfitting and computation. This paper proposes a region of interest detection algorithm that is implemented during data preprocessing to locate salient features and remove extraneous MRI data. This decreases the input size, allowing for more aggressive data augmentations and deeper neural networks. Following the preprocessing of the MRI modalities, a fully convolutional autoencoder with soft attention segments the different brain MRIs. When these deep lear
&lt;/p&gt;</description></item><item><title>VideoPoet&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22810;&#31181;&#26465;&#20214;&#20449;&#21495;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#21450;&#21305;&#37197;&#38899;&#39057;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.14125</link><description>&lt;p&gt;
VideoPoet&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VideoPoet: A Large Language Model for Zero-Shot Video Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14125
&lt;/p&gt;
&lt;p&gt;
VideoPoet&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22810;&#31181;&#26465;&#20214;&#20449;&#21495;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#21450;&#21305;&#37197;&#38899;&#39057;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VideoPoet&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#21508;&#31181;&#19981;&#21516;&#30340;&#26465;&#20214;&#20449;&#21495;&#20013;&#21512;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#21450;&#21305;&#37197;&#38899;&#39057;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;VideoPoet&#37319;&#29992;&#35299;&#30721;&#22120;-&#20165;Transformer&#26550;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#12290;&#35757;&#32451;&#21327;&#35758;&#36981;&#24490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#39044;&#35757;&#32451;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;VideoPoet&#22312;&#33258;&#22238;&#24402;Transformer&#26694;&#26550;&#20013;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#29983;&#25104;&#30446;&#26631;&#30340;&#28151;&#21512;&#12290;&#39044;&#35757;&#32451;&#30340;LLM&#20316;&#20026;&#19968;&#20010;&#22522;&#30784;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#33021;&#21147;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;VideoPoet&#29983;&#25104;&#39640;&#20445;&#30495;&#36816;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14125v2 Announce Type: replace-cross  Abstract: We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#22788;&#29702;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.12467</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#25509;&#35302;&#32593;&#26684;&#21464;&#25442;&#22120;&#23398;&#20064;&#28789;&#27963;&#36523;&#20307;&#30896;&#25758;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#22788;&#29702;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#22522;&#20110;&#32593;&#26684;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#24050;&#34987;&#25552;&#20986;&#29992;&#26469;&#24314;&#27169;&#22797;&#26434;&#30340;&#39640;&#32500;&#29289;&#29702;&#31995;&#32479;&#12290;&#19982;&#20256;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#23601;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#27714;&#35299;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#30340;&#26159;&#23427;&#20204;&#26159;&#21542;&#26377;&#25928;&#22320;&#24212;&#23545;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#30340;&#25361;&#25112;&#65292;&#21363;&#30636;&#26102;&#30896;&#25758;&#21457;&#29983;&#22312;&#26497;&#30701;&#26102;&#38388;&#20869;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#36523;&#20307;&#31354;&#38388;&#20301;&#32622;&#20043;&#38388;&#65288;&#30001;&#30896;&#25758;&#24341;&#36215;&#30340;&#65289;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;--&#22312;&#26356;&#39640;&#32423;&#21035;&#32593;&#26684;&#20013;&#30340;&#20004;&#20010;&#25509;&#36817;&#20301;&#32622;&#23545;&#24212;&#20110;&#36523;&#20307;&#20013;&#30340;&#20004;&#20010;&#36828;&#36317;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12467v2 Announce Type: replace-cross  Abstract: Recently, many mesh-based graph neural network (GNN) models have been proposed for modeling complex high-dimensional physical systems. Remarkable achievements have been made in significantly reducing the solving time compared to traditional numerical solvers. These methods are typically designed to i) reduce the computational cost in solving physical dynamics and/or ii) propose techniques to enhance the solution accuracy in fluid and rigid body dynamics. However, it remains under-explored whether they are effective in addressing the challenges of flexible body dynamics, where instantaneous collisions occur within a very short timeframe. In this paper, we present Hierarchical Contact Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn long-range dependencies (occurred by collisions) among spatially distant positions of a body -- two close positions in a higher-level mesh corresponds to two distant posi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#23398;&#20064;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#26032;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#22914;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2312.05742</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
The Generalization Gap in Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#23398;&#20064;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#26032;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#22914;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#31163;&#32447;&#23398;&#20064;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#26159;&#22312;&#30456;&#21516;&#30340;&#29615;&#22659;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12289;&#31163;&#32447;RL&#12289;&#24207;&#21015;&#24314;&#27169;&#21644;&#34892;&#20026;&#20811;&#38534;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#26032;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#19981;&#22914;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#31163;&#32447;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;Procgen&#65288;2D&#35270;&#39057;&#28216;&#25103;&#65289;&#21644;WebShop&#65288;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#65289;&#25910;&#38598;&#20102;&#22810;&#31181;&#22823;&#23567;&#21644;&#25216;&#33021;&#27700;&#24179;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#28216;&#25103;&#20851;&#21345;&#25110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#36712;&#36857;&#65292;&#27979;&#35797;&#26102;&#65292;&#20195;&#29702;&#35201;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#30340;&#20851;&#21345;&#25110;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#37117;&#24456;&#38590;&#19982;&#22312;&#32447;RL&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05742v2 Announce Type: replace-cross  Abstract: Despite recent progress in offline learning, these methods are still trained and tested on the same environment. In this paper, we compare the generalization abilities of widely used online and offline learning methods such as online reinforcement learning (RL), offline RL, sequence modeling, and behavioral cloning. Our experiments show that offline learning algorithms perform worse on new environments than online learning ones. We also introduce the first benchmark for evaluating generalization in offline learning, collecting datasets of varying sizes and skill-levels from Procgen (2D video games) and WebShop (e-commerce websites). The datasets contain trajectories for a limited number of game levels or natural language instructions and at test time, the agent has to generalize to new levels or instructions. Our experiments reveal that existing offline learning algorithms struggle to match the performance of online RL on both 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#35270;&#20026;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#32479;&#19968;&#26816;&#32034;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RetriNet&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#31934;&#30830;&#26816;&#32034;&#20027;&#39064;&#23646;&#24615;&#24182;&#36807;&#28388;&#26080;&#20851;&#20449;&#24687;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#22312;&#20154;&#33080;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.02521</link><description>&lt;p&gt;
&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#26816;&#32034;&#26465;&#20214;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Retrieving Conditions from Reference Images for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#35270;&#20026;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#32479;&#19968;&#26816;&#32034;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RetriNet&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#31934;&#30830;&#26816;&#32034;&#20027;&#39064;&#23646;&#24615;&#24182;&#36807;&#28388;&#26080;&#20851;&#20449;&#24687;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#22312;&#20154;&#33080;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#24320;&#21457;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25216;&#26415;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21508;&#31181;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#24341;&#36215;&#20102;&#21508;&#31181;&#24212;&#29992;&#30340;&#30456;&#24403;&#22823;&#20852;&#36259;&#12290;&#19968;&#20010;&#26222;&#36941;&#30340;&#22330;&#26223;&#26159;&#22522;&#20110;&#21442;&#32771;&#22270;&#20687;&#20013;&#30340;&#19968;&#20010;&#20027;&#39064;&#29983;&#25104;&#26032;&#30340;&#22270;&#20687;&#12290;&#36825;&#20010;&#20027;&#39064;&#21487;&#20197;&#26159;&#39118;&#26684;&#21270;&#22836;&#20687;&#30340;&#38754;&#37096;&#36523;&#20221;&#65292;&#34394;&#25311;&#35797;&#31359;&#30340;&#36523;&#20307;&#21644;&#26381;&#35013;&#31561;&#12290;&#28385;&#36275;&#36825;&#19968;&#35201;&#27714;&#27491;&#22312;&#28436;&#21464;&#25104;&#19968;&#38376;&#31216;&#20026;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#35270;&#20026;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#32479;&#19968;&#26816;&#32034;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RetriNet&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#31934;&#30830;&#22320;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#26816;&#32034;&#20027;&#39064;&#23646;&#24615;&#24182;&#36807;&#28388;&#25481;&#26080;&#20851;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;RetriNet&#22312;&#20154;&#33080;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02521v2 Announce Type: replace-cross  Abstract: Newly developed diffusion-based techniques have showcased phenomenal abilities in producing a wide range of high-quality images, sparking considerable interest in various applications. A prevalent scenario is to generate new images based on a subject from reference images. This subject could be face identity for styled avatars, body and clothing for virtual try-on and so on. Satisfying this requirement is evolving into a field called Subject-Driven Generation. In this paper, we consider Subject-Driven Generation as a unified retrieval problem with diffusion models. We introduce a novel diffusion model architecture, named RetriNet, designed to address and solve these problems by retrieving subject attributes from reference images precisely, and filter out irrelevant information. RetriNet demonstrates impressive performance when compared to existing state-of-the-art approaches in face generation. We further propose a research and
&lt;/p&gt;</description></item><item><title>MUFFIN&#26159;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20219;&#21153;&#65292;&#36890;&#36807;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#20016;&#23500;&#22810;&#26679;&#12290;</title><link>https://arxiv.org/abs/2312.02436</link><description>&lt;p&gt;
MUFFIN: &#29992;&#20110;&#25913;&#21892;&#25351;&#31034;&#36981;&#24490;&#30340;&#22810;&#26041;&#38754;&#25351;&#21335;&#30340;&#31574;&#21010;
&lt;/p&gt;
&lt;p&gt;
MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02436
&lt;/p&gt;
&lt;p&gt;
MUFFIN&#26159;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20219;&#21153;&#65292;&#36890;&#36807;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#20016;&#23500;&#22810;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#20013;&#65292;&#21152;&#24378;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36890;&#24120;&#28041;&#21450;&#31574;&#21010;&#24191;&#27867;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;MUFFIN&#65292;&#20855;&#20307;&#22320;&#36890;&#36807;&#29992;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20197;&#20016;&#23500;&#36825;&#20123;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02436v2 Announce Type: replace-cross  Abstract: In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair (without requiring a separate input anymore). However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39640;&#25928;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#65292;&#33021;&#22815;&#22312;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#20013;&#23454;&#29616;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#21160;&#20316;&#29983;&#25104;</title><link>https://arxiv.org/abs/2312.02256</link><description>&lt;p&gt;
&#39640;&#25928;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#29992;&#20110;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#21160;&#20316;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02256
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39640;&#25928;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#65292;&#33021;&#22815;&#22312;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#20013;&#23454;&#29616;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#21160;&#20316;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#65292;&#29992;&#20110;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#21160;&#20316;&#29983;&#25104;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#22312;&#36861;&#27714;&#24555;&#36895;&#29983;&#25104;&#30340;&#21516;&#26102;&#29306;&#29298;&#20102;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EMDM&#65292;&#23427;&#36890;&#36807;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#22810;&#27425;&#37319;&#26679;&#27493;&#39588;&#20013;&#25429;&#25417;&#22797;&#26434;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#21644;&#29983;&#25104;&#36807;&#31243;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02256v2 Announce Type: replace-cross  Abstract: We introduce Efficient Motion Diffusion Model (EMDM) for fast and high-quality human motion generation. Current state-of-the-art generative diffusion models have produced impressive results but struggle to achieve fast generation without sacrificing quality. On the one hand, previous works, like motion latent diffusion, conduct diffusion within a latent space for efficiency, but learning such a latent space can be a non-trivial effort. On the other hand, accelerating generation by naively increasing the sampling step size, e.g., DDIM, often leads to quality degradation as it fails to approximate the complex denoising distribution. To address these issues, we propose EMDM, which captures the complex distribution during multiple sampling steps in the diffusion model, allowing for much fewer sampling steps and significant acceleration in generation. This is achieved by a conditional denoising diffusion GAN to capture multimodal da
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#20801;&#35768;&#22312;&#22797;&#21512;&#39640;&#26031;&#20998;&#24067;&#31867;&#20013;&#36827;&#34892;&#38382;&#39064;&#29305;&#23450;&#32479;&#35745;&#20808;&#39564;&#36873;&#25321;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#26032;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#36845;&#20195;&#31639;&#27861;&#24191;&#20041;&#22797;&#21512;&#39640;&#26031;&#26368;&#23567;&#20108;&#20056;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#23637;&#24320;&#24471;&#21040;&#30340;&#26032;&#39062;&#28145;&#24230;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;DR-CG-Net&#12290;</title><link>https://arxiv.org/abs/2311.17248</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#28145;&#24230;&#27491;&#21017;&#21270;&#22797;&#21512;&#39640;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Regularized Compound Gaussian Network for Solving Linear Inverse Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17248
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#20801;&#35768;&#22312;&#22797;&#21512;&#39640;&#26031;&#20998;&#24067;&#31867;&#20013;&#36827;&#34892;&#38382;&#39064;&#29305;&#23450;&#32479;&#35745;&#20808;&#39564;&#36873;&#25321;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#26032;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#36845;&#20195;&#31639;&#27861;&#24191;&#20041;&#22797;&#21512;&#39640;&#26031;&#26368;&#23567;&#20108;&#20056;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#23637;&#24320;&#24471;&#21040;&#30340;&#26032;&#39062;&#28145;&#24230;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;DR-CG-Net&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20808;&#39564;&#20449;&#24687;&#32435;&#20837;&#36870;&#38382;&#39064;&#65292;&#20363;&#22914;&#36890;&#36807;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#65292;&#26159;&#20419;&#36827;&#31283;&#20581;&#36870;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#25991;&#20026;&#20801;&#35768;&#22312;&#22797;&#21512;&#39640;&#26031;&#65288;CG&#65289;&#20998;&#24067;&#31867;&#20013;&#36827;&#34892;&#38382;&#39064;&#29305;&#23450;&#32479;&#35745;&#20808;&#39564;&#36873;&#25321;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#35774;&#35745;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#31216;&#20026;&#24191;&#20041;&#22797;&#21512;&#39640;&#26031;&#26368;&#23567;&#20108;&#20056;&#65288;G-CG-LS&#65289;&#65292;&#23427;&#26368;&#23567;&#21270;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#30446;&#26631;&#20989;&#25968;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#24378;&#21046;&#25191;&#34892;&#19968;&#20010;CG&#20808;&#39564;&#12290;&#28982;&#21518;&#23558;G-CG-LS&#23637;&#24320;&#65292;&#25552;&#20379;&#25105;&#20204;&#30340;&#31532;&#20108;&#31181;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#27491;&#21017;&#21270;&#65288;DR&#65289;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;DR-CG-Net&#65292;&#21487;&#20197;&#23398;&#20064;&#20808;&#39564;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17248v2 Announce Type: replace-cross  Abstract: Incorporating prior information into inverse problems, e.g. via maximum-a-posteriori estimation, is an important technique for facilitating robust inverse problem solutions. In this paper, we devise two novel approaches for linear inverse problems that permit problem-specific statistical prior selections within the compound Gaussian (CG) class of distributions. The CG class subsumes many commonly used priors in signal and image reconstruction methods including those of sparsity-based approaches. The first method developed is an iterative algorithm, called generalized compound Gaussian least squares (G-CG-LS), that minimizes a regularized least squares objective function where the regularization enforces a CG prior. G-CG-LS is then unrolled, or unfolded, to furnish our second method, which is a novel deep regularized (DR) neural network, called DR-CG-Net, that learns the prior information. A detailed computational theory on conv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Face-diffuser&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#35299;&#20915;&#20027;&#20307;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#35757;&#32451;&#19981;&#24179;&#34913;&#21644;&#36136;&#37327;&#22949;&#21327;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.10329</link><description>&lt;p&gt;
&#39640;&#20445;&#30495;&#24230;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20027;&#20307;&#21040;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
High-fidelity Person-centric Subject-to-Image Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10329
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Face-diffuser&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#35299;&#20915;&#20027;&#20307;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#35757;&#32451;&#19981;&#24179;&#34913;&#21644;&#36136;&#37327;&#22949;&#21327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20197;&#20027;&#20307;&#39537;&#21160;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22270;&#20687;&#29983;&#25104;&#20013;&#36935;&#21040;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#21407;&#22240;&#22312;&#20110;&#23427;&#20204;&#36890;&#36807;&#24494;&#35843;&#36890;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#26469;&#23398;&#20064;&#35821;&#20041;&#22330;&#26223;&#21644;&#20154;&#29289;&#29983;&#25104;&#65292;&#36825;&#28041;&#21450;&#21040;&#19968;&#31181;&#26080;&#27861;&#35843;&#21644;&#30340;&#35757;&#32451;&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Face-diffuser&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#26088;&#22312;&#28040;&#38500;&#19978;&#36848;&#35757;&#32451;&#19981;&#24179;&#34913;&#21644;&#36136;&#37327;&#22949;&#21327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10329v3 Announce Type: replace-cross  Abstract: Current subject-driven image generation methods encounter significant challenges in person-centric image generation. The reason is that they learn the semantic scene and person generation by fine-tuning a common pre-trained diffusion, which involves an irreconcilable training imbalance. Precisely, to generate realistic persons, they need to sufficiently tune the pre-trained model, which inevitably causes the model to forget the rich semantic scene prior and makes scene generation over-fit to the training data. Moreover, even with sufficient fine-tuning, these methods can still not generate high-fidelity persons since joint learning of the scene and person generation also lead to quality compromise. In this paper, we propose Face-diffuser, an effective collaborative generation pipeline to eliminate the above training imbalance and quality compromise. Specifically, we first develop two specialized pre-trained diffusion models, i.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#24341;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;TKGF&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25429;&#25417;&#20851;&#31995;&#25551;&#36848;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#20851;&#31995;&#22312;&#24314;&#27169;&#26102;&#33021;&#20855;&#26377;&#30456;&#20284;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.10112</link><description>&lt;p&gt;
zrLLM&#65306;&#22312;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#24341;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;TKGF&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25429;&#25417;&#20851;&#31995;&#25551;&#36848;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#20851;&#31995;&#22312;&#24314;&#27169;&#26102;&#33021;&#20855;&#26377;&#30456;&#20284;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21270;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#30693;&#35782;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;(TKGs)&#19978;&#24050;&#25104;&#20026;&#19968;&#20010;&#28861;&#28909;&#35805;&#39064;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#39044;&#27979;TKGs&#19978;&#30340;&#38142;&#25509;&#12290;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#65292;&#20854;&#20013;&#23398;&#20064;&#38544;&#34255;&#34920;&#31034;&#20197;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#22270;&#19978;&#19979;&#25991;&#26469;&#34920;&#31034;&#30693;&#35782;&#22270;(KG)&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;TKG&#39044;&#27979;(TKGF)&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#24314;&#27169;&#27809;&#26377;&#20808;&#21069;&#22270;&#19978;&#19979;&#25991;&#30340;&#26410;&#35265;&#36807;&#30340;&#38646;&#26679;&#26412;&#20851;&#31995;&#19978;&#38754;&#20020;&#24378;&#28872;&#25361;&#25112;&#12290;&#26412;&#25991;&#23581;&#35797;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#22914;&#19979;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;KG&#20851;&#31995;&#30340;&#25991;&#26412;&#25551;&#36848;&#36755;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20197;&#29983;&#25104;&#20851;&#31995;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#24341;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;TKGF&#26041;&#27861;&#20013;&#12290;LLM&#22686;&#24378;&#30340;&#34920;&#31034;&#21487;&#20197;&#25429;&#25417;&#20851;&#31995;&#25551;&#36848;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#20851;&#31995;&#65292;&#26080;&#35770;&#26159;&#24050;&#35265;&#36824;&#26159;&#26410;&#35265;&#30340;&#65292;&#37117;&#33021;&#22815;&#33719;&#24471;&#31867;&#20284;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10112v2 Announce Type: replace  Abstract: Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become a heated topic. Various methods have been proposed to forecast links on TKGs. Most of them are embedding-based, where hidden representations are learned to represent knowledge graph (KG) entities and relations based on the observed graph contexts. Although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the unseen zero-shot relations that have no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic mean
&lt;/p&gt;</description></item><item><title>&#23558;&#20844;&#24179;&#24615;&#35270;&#20026;&#20998;&#24067;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#24067;&#23545;&#40784;&#25439;&#22833;&#21644;&#35843;&#25972;DFT&#20004;&#39033;&#25216;&#26415;&#36129;&#29486;&#65292;&#26174;&#33879;&#20943;&#23569;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20132;&#21449;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2311.07604</link><description>&lt;p&gt;
&#20026;&#20844;&#24179;&#24615;&#35843;&#25972;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Finetuning Text-to-Image Diffusion Models for Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07604
&lt;/p&gt;
&lt;p&gt;
&#23558;&#20844;&#24179;&#24615;&#35270;&#20026;&#20998;&#24067;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#24067;&#23545;&#40784;&#25439;&#22833;&#21644;&#35843;&#25972;DFT&#20004;&#39033;&#25216;&#26415;&#36129;&#29486;&#65292;&#26174;&#33879;&#20943;&#23569;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20132;&#21449;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#37319;&#29992;&#20984;&#26174;&#20102;&#35299;&#20915;&#20854;&#20559;&#35265;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22914;&#26524;&#19981;&#36827;&#34892;&#24178;&#39044;&#65292;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20256;&#25773;&#20986;&#25197;&#26354;&#30340;&#19990;&#30028;&#35266;&#65292;&#24182;&#38480;&#21046;&#23569;&#25968;&#32676;&#20307;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20844;&#24179;&#24615;&#35270;&#20026;&#19968;&#20010;&#20998;&#24067;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#65306;(1)&#19968;&#20010;&#20998;&#24067;&#23545;&#40784;&#25439;&#22833;&#65292;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#29305;&#23450;&#29305;&#24449;&#24341;&#21521;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#20998;&#24067;&#65292;&#20197;&#21450;(2)&#35843;&#25972;&#20102;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#36807;&#31243;&#30340;&#30452;&#25509;&#24494;&#35843;&#65288;&#35843;&#25972;DFT&#65289;&#65292;&#23427;&#21033;&#29992;&#35843;&#25972;&#21518;&#30340;&#26799;&#24230;&#30452;&#25509;&#20248;&#21270;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#19978;&#23450;&#20041;&#30340;&#25439;&#22833;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#32844;&#19994;&#25552;&#31034;&#30340;&#24615;&#21035;&#12289;&#31181;&#26063;&#21450;&#20854;&#20132;&#21449;&#20559;&#35265;&#12290;&#21363;&#20351;&#21482;&#23545;&#20116;&#20010;&#36719;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#24615;&#21035;&#20559;&#35265;&#20063;&#22823;&#22823;&#20943;&#23569;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#22810;&#20803;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07604v2 Announce Type: replace-cross  Abstract: The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#22312;&#24605;&#32771;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#35821;&#35328;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#30340;&#28789;&#24863;&#65292;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27807;&#36890;&#25216;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25311;&#20154;&#21270;&#21644;&#20027;&#21160;&#24615;&#65292;&#21560;&#24341;&#29992;&#25143;&#36827;&#34892;&#26356;&#38271;&#26102;&#38388;&#30340;&#23545;&#35805;</title><link>https://arxiv.org/abs/2311.07445</link><description>&lt;p&gt;
&#24910;&#35328;&#65306;&#36890;&#36807;&#20869;&#24515;&#29420;&#30333;&#22521;&#20859;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27807;&#36890;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07445
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#22312;&#24605;&#32771;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#35821;&#35328;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#30340;&#28789;&#24863;&#65292;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27807;&#36890;&#25216;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25311;&#20154;&#21270;&#21644;&#20027;&#21160;&#24615;&#65292;&#21560;&#24341;&#29992;&#25143;&#36827;&#34892;&#26356;&#38271;&#26102;&#38388;&#30340;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#27969;&#30021;&#12289;&#36830;&#36143;&#21644;&#22810;&#26679;&#21270;&#30340;&#22238;&#22797;&#12290;&#28982;&#32780;&#65292;LLMs&#20173;&#28982;&#32570;&#20047;&#19968;&#39033;&#20851;&#38190;&#33021;&#21147;&#65306;&#27807;&#36890;&#25216;&#24039;&#12290;&#36825;&#31181;&#23616;&#38480;&#20351;&#23427;&#20204;&#26356;&#20687;&#20449;&#24687;&#25628;&#32034;&#24037;&#20855;&#65292;&#32780;&#19981;&#26159;&#25311;&#20154;&#21270;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#38656;&#35201;&#32771;&#34385;&#27807;&#36890;&#25216;&#33021;&#65292;&#22914;&#20027;&#39064;&#36807;&#28193;&#12289;&#20027;&#21160;&#25552;&#38382;&#12289;&#27010;&#24565;&#24341;&#23548;&#12289;&#21516;&#29702;&#24515;&#21644;&#24635;&#32467;&#65292;&#20351;LLMs&#22312;&#23545;&#35805;&#20013;&#26356;&#20855;&#25311;&#20154;&#21270;&#21644;&#20027;&#21160;&#24615;&#65292;&#20174;&#32780;&#22686;&#21152;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#21560;&#24341;&#20182;&#20204;&#36827;&#34892;&#26356;&#38271;&#26102;&#38388;&#30340;&#20132;&#35848;&#12290;&#28982;&#32780;&#65292;&#22312;&#40657;&#21283;&#23376;LLMs&#20013;&#21551;&#29992;&#36825;&#20123;&#27807;&#36890;&#25216;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#19982;&#30495;&#20154;&#30456;&#21516;&#30340;&#35805;&#35821;&#24418;&#25104;&#27169;&#24335;&#65306;&#20808;&#24605;&#21518;&#35828;&#12290;&#21463;&#35821;&#35328;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#20869;&#22312;&#24605;&#32771;&#36171;&#20104;LLMs&#27807;&#36890;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07445v2 Announce Type: replace-cross  Abstract: The emergence of large language models (LLMs) further improves the capabilities of open-domain dialogue systems and can generate fluent, coherent, and diverse responses. However, LLMs still lack a crucial ability: communication skills. This limitation renders them more like information seeking tools rather than anthropomorphic chatbots. Communication skills, such as topic transition, proactively asking questions, concept guidance, empathy, and summarising often should be taken into consideration, to make LLMs more anthropomorphic and proactive during the conversation, thereby increasing the interest of users and attracting them to chat for longer. However, enabling these communication skills in black-box LLMs remains a key challenge because they do not have the same utterance formation mode as real people: think before speaking. Inspired by linguistics and cognitive science, we empower LLMs with communication skills through inn
&lt;/p&gt;</description></item><item><title>LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.19791</link><description>&lt;p&gt;
LILO&#65306;&#36890;&#36807;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#23398;&#20064;&#21487;&#35299;&#37322;&#24211;
&lt;/p&gt;
&lt;p&gt;
LILO: Learning Interpretable Libraries by Compressing and Documenting Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19791
&lt;/p&gt;
&lt;p&gt;
LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;&#37325;&#26500;&#30340;&#33402;&#26415;&#65306;&#23558;&#20195;&#30721;&#25972;&#21512;&#21040;&#21487;&#37325;&#29992;&#21644;&#21487;&#35835;&#30340;&#31243;&#24207;&#24211;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LILO&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#24211;&#12290;LILO&#23558;LLM&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;Stitch&#33258;&#21160;&#37325;&#26500;&#30340;&#36817;&#26399;&#31639;&#27861;&#36827;&#23637;&#30456;&#32467;&#21512;&#65306;Stitch&#26159;&#19968;&#20010;&#31526;&#21495;&#21387;&#32553;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#22823;&#22411;&#20195;&#30721;&#35821;&#26009;&#24211;&#20013;&#30340;&#26368;&#20339;lambda&#25277;&#35937;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#25277;&#35937;&#21487;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#25991;&#26723;&#65288;AutoDoc&#65289;&#36807;&#31243;&#65292;&#23427;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#25512;&#26029;&#20986;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#21644;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#38500;&#20102;&#25552;&#39640;&#20154;&#31867;&#21487;&#35835;&#24615;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;AutoDoc&#36890;&#36807;&#24110;&#21161;LILO&#30340;&#21512;&#25104;&#22120;&#35299;&#37322;&#21644;&#37096;&#32626;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LILO&#36827;&#34892;&#20102;&#19977;&#20010;&#24402;&#32435;&#24335;&#31243;&#24207;&#32508;&#21512;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
&lt;/p&gt;</description></item><item><title>CLIP&#22312;&#32463;&#36807;&#37325;&#29616;ImageNet&#35757;&#32451;-&#27979;&#35797;&#30456;&#20284;&#24615;&#30340;&#21098;&#26525;LAION&#20998;&#21106;&#37325;&#26032;&#35757;&#32451;&#21518;&#65292;&#34429;&#28982;&#22312;&#26576;&#20123;&#22522;&#20934;&#19978;&#34920;&#29616;&#26377;&#25152;&#19979;&#38477;&#65292;&#20294;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#24456;&#39640;</title><link>https://arxiv.org/abs/2310.09562</link><description>&lt;p&gt;
CLIP&#30340;&#27867;&#21270;&#24615;&#33021;&#20027;&#35201;&#28304;&#20110;&#35757;&#32451;-&#27979;&#35797;&#20043;&#38388;&#30340;&#39640;&#30456;&#20284;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does CLIP's Generalization Performance Mainly Stem from High Train-Test Similarity?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09562
&lt;/p&gt;
&lt;p&gt;
CLIP&#22312;&#32463;&#36807;&#37325;&#29616;ImageNet&#35757;&#32451;-&#27979;&#35797;&#30456;&#20284;&#24615;&#30340;&#21098;&#26525;LAION&#20998;&#21106;&#37325;&#26032;&#35757;&#32451;&#21518;&#65292;&#34429;&#28982;&#22312;&#26576;&#20123;&#22522;&#20934;&#19978;&#34920;&#29616;&#26377;&#25152;&#19979;&#38477;&#65292;&#20294;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#24456;&#39640;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; CLIP &#31561;&#22522;&#30784;&#27169;&#22411;&#34987;&#35757;&#32451;&#22312;&#25968;&#20159;&#26679;&#26412;&#19978;&#65292;&#33021;&#22815;&#36731;&#26494;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#21644;&#36755;&#20837;&#12290;CLIP &#20986;&#33394;&#22320;&#23637;&#31034;&#20102;&#22312;&#24191;&#27867;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#22522;&#20934;&#19978;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#32780;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;&#20854;&#24402;&#22240;&#20110;&#24403;&#20170;&#30340;&#22823;&#35268;&#27169;&#21644;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#22914; LAION&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110; CLIP &#26469;&#35828;&#65292;&#20687;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#36825;&#26679;&#30340;&#26415;&#35821;&#26159;&#21542;&#20855;&#26377;&#24847;&#20041;&#26159;&#20540;&#24471;&#24576;&#30097;&#30340;&#65292;&#22240;&#20026;&#20687; LAION &#36825;&#26679;&#30340;&#32593;&#39029;&#35268;&#27169;&#25968;&#25454;&#38598;&#21487;&#33021;&#21482;&#26159;&#21253;&#21547;&#35768;&#22810;&#19982;&#26368;&#21021;&#20026; ImageNet &#35774;&#35745;&#30340;&#24120;&#35265; OOD &#22522;&#20934;&#30456;&#20284;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#22797;&#21046; ImageNet &#30340;&#35757;&#32451;-&#27979;&#35797;&#30456;&#20284;&#24615;&#30456;&#23545;&#20110;&#24120;&#35265; OOD &#22522;&#20934;&#30340;&#21098;&#26525; LAION &#20998;&#21106;&#19978;&#37325;&#26032;&#35757;&#32451; CLIP&#12290;&#34429;&#28982;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#19968;&#20123;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;CLIP &#30340;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#24456;&#39640;&#12290;&#36825;&#34920;&#26126;&#39640;&#35757;&#32451;-&#27979;&#35797;&#30456;&#20284;&#24615;&#26159;&#19981;&#36275;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09562v2 Announce Type: replace-cross  Abstract: Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful terms like out-of-distribution generalization are for CLIP as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#36951;&#24536;&#35299;&#20915;&#26041;&#26696;&#30340;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#26126;&#30830;&#20102;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2308.07061</link><description>&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65306;&#35299;&#20915;&#26041;&#26696;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning: Solutions and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#36951;&#24536;&#35299;&#20915;&#26041;&#26696;&#30340;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#26126;&#30830;&#20102;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26080;&#24847;&#20013;&#35760;&#20303;&#25935;&#24863;&#12289;&#26410;&#32463;&#25480;&#26435;&#25110;&#24694;&#24847;&#25968;&#25454;&#65292;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#23433;&#20840;&#28431;&#27934;&#21644;&#24615;&#33021;&#38477;&#32423;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26426;&#22120;&#36951;&#24536;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#28040;&#38500;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#28857;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;&#26426;&#22120;&#36951;&#24536;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20998;&#20026;&#23436;&#20840;&#36951;&#24536;&#26041;&#27861;&#21644;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#24433;&#21709;&#30340;&#36817;&#20284;&#36951;&#24536;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#22238;&#39038;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35752;&#35770;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#25512;&#36827;&#26426;&#22120;&#36951;&#24536;&#24182;&#23558;&#20854;&#24314;&#31435;&#20026;&#20540;&#24471;&#20449;&#36182;&#21644;&#36866;&#24212;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#33021;&#21147;&#12290;&#26412;&#25991;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20221;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07061v2 Announce Type: replace-cross  Abstract: Machine learning models may inadvertently memorize sensitive, unauthorized, or malicious data, posing risks of privacy breaches, security vulnerabilities, and performance degradation. To address these issues, machine unlearning has emerged as a critical technique to selectively remove specific training data points' influence on trained models. This paper provides a comprehensive taxonomy and analysis of the solutions in machine unlearning. We categorize existing solutions into exact unlearning approaches that remove data influence thoroughly and approximate unlearning approaches that efficiently minimize data influence. By comprehensively reviewing solutions, we identify and discuss their strengths and limitations. Furthermore, we propose future directions to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning models. This paper provides researchers with a roadmap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;JSON&#25968;&#25454;&#20013;&#26631;&#35760;&#35821;&#20041;&#31867;&#22411;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2307.12807</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#29702;&#35299;JSON&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comprehending Semantic Types in JSON Data with Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.12807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;JSON&#25968;&#25454;&#20013;&#26631;&#35760;&#35821;&#20041;&#31867;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#31867;&#22411;&#26159;&#19968;&#31181;&#27604;&#21407;&#23376;&#31867;&#22411;&#65288;&#22914;&#23383;&#31526;&#20018;&#25110;&#25972;&#25968;&#65289;&#26356;&#24378;&#22823;&#21644;&#35814;&#32454;&#30340;&#25551;&#36848;&#25968;&#25454;&#30340;&#26041;&#24335;&#12290;&#23427;&#20204;&#22312;&#21015;&#20043;&#38388;&#21644;&#19982;&#29616;&#23454;&#19990;&#30028;&#27010;&#24565;&#20043;&#38388;&#24314;&#31435;&#36830;&#25509;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#32454;&#33268;&#21644;&#31934;&#32454;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#33258;&#21160;&#21270;&#25968;&#25454;&#28165;&#27927;&#12289;&#27169;&#24335;&#21305;&#37197;&#21644;&#25968;&#25454;&#21457;&#29616;&#31561;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;&#24050;&#26377;&#30340;&#22312;&#22823;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#20851;&#31995;&#25968;&#25454;&#30340;&#21333;&#21015;&#35821;&#20041;&#31867;&#22411;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#35821;&#20041;&#31867;&#22411;&#39044;&#27979;&#38382;&#39064;&#25193;&#23637;&#21040;JSON&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;JSON&#36335;&#24452;&#20026;&#31867;&#22411;&#36827;&#34892;&#26631;&#35760;&#12290;&#19982;&#20851;&#31995;&#25968;&#25454;&#20013;&#30340;&#21015;&#31867;&#20284;&#65292;JSON&#36335;&#24452;&#26159;&#19968;&#31181;&#26597;&#35810;&#35821;&#35328;&#65292;&#36890;&#36807;&#25351;&#23450;&#20803;&#32032;&#30340;&#20301;&#32622;&#21644;&#20869;&#23481;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;JSON&#25968;&#25454;&#32467;&#26500;&#30340;&#23548;&#33322;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#29702;&#35299;JSON&#25991;&#26723;&#38598;&#21512;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.12807v1 Announce Type: cross  Abstract: Semantic types are a more powerful and detailed way of describing data than atomic types such as strings or integers. They establish connections between columns and concepts from the real world, providing more nuanced and fine-grained information that can be useful for tasks such as automated data cleaning, schema matching, and data discovery. Existing deep learning models trained on large text corpora have been successful at performing single-column semantic type prediction for relational data. However, in this work, we propose an extension of the semantic type prediction problem to JSON data, labeling the types based on JSON Paths. Similar to columns in relational data, JSON Path is a query language that enables the navigation of complex JSON data structures by specifying the location and content of the elements. We use a graph neural network to comprehend the structural information within collections of JSON documents. Our model out
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2302.05614</link><description>&lt;p&gt;
&#20855;&#26377;&#21407;&#22411;&#30340;&#36328;&#39046;&#22495;&#38543;&#26426;&#39044;&#35757;&#32451;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Random Pre-training with Prototypes for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27492;&#24037;&#20316;&#24050;&#25552;&#20132;&#32473;IEEE&#36827;&#34892;&#21487;&#33021;&#30340;&#20986;&#29256;&#12290; CRPTpro&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;RL&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#12290; CRPTpro&#37319;&#29992;&#20102;&#36328;&#39046;&#22495;&#38543;&#26426;&#31574;&#30053;&#65292;&#21487;&#20197;&#36731;&#26494;&#24555;&#36895;&#22320;&#20174;&#22810;&#20010;&#39046;&#22495;&#20013;&#25277;&#26679;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#25439;&#22833;&#36827;&#34892;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#12290;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36328;&#39046;&#22495;&#32534;&#30721;&#22120;&#21487;&#20197;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290; &#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#22914;APT&#21644;Proto-RL&#30456;&#27604;&#65292;CRP
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05614v2 Announce Type: replace-cross  Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Task-agnostic cross-domain pre-training shows great potential in image-based Reinforcement Learning (RL) but poses a big challenge. In this paper, we propose CRPTpro, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL. CRPTpro employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency. Moreover, prototypical representation learning with a novel intrinsic loss is proposed to pre-train an effective and generic encoder across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream visual-control RL tasks defined in different domains efficiently. Compared with prior arts like APT and Proto-RL, CRP
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25856;&#23721;&#24555;&#25346;&#19978;&#23433;&#35013;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#37319;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#25856;&#23721;&#27963;&#21160;&#20013;&#26816;&#27979;&#25856;&#23721;&#32773;&#19979;&#38477;&#24773;&#20917;&#30340;&#25216;&#26415;&#65292;&#20445;&#25252;&#25856;&#23721;&#32773;&#38544;&#31169;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20415;&#21033;&#24615;&#12290;</title><link>https://arxiv.org/abs/2301.10164</link><description>&lt;p&gt;
&#22522;&#20110;&#20256;&#24863;&#22120;&#22686;&#24378;&#24555;&#25346;&#30340;&#26397;&#21521;&#30340;&#25856;&#23721;&#20013;&#19979;&#38477;&#34892;&#20026;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Lowering Detection in Sport Climbing Based on Orientation of the Sensor Enhanced Quickdraw
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.10164
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25856;&#23721;&#24555;&#25346;&#19978;&#23433;&#35013;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#37319;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#25856;&#23721;&#27963;&#21160;&#20013;&#26816;&#27979;&#25856;&#23721;&#32773;&#19979;&#38477;&#24773;&#20917;&#30340;&#25216;&#26415;&#65292;&#20445;&#25252;&#25856;&#23721;&#32773;&#38544;&#31169;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20415;&#21033;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36319;&#36394;&#25856;&#23721;&#32773;&#30340;&#27963;&#21160;&#20197;&#25913;&#21892;&#26381;&#21153;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#20182;&#20204;&#30340;&#22522;&#30784;&#35774;&#26045;&#26159;&#25856;&#23721;&#20581;&#36523;&#25151;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#24517;&#39035;&#20174;&#24320;&#22987;&#20998;&#26512;&#27599;&#20010;&#25856;&#23721;&#27963;&#21160;&#30452;&#21040;&#25856;&#30331;&#32773;&#38477;&#19979;&#26469;&#12290;&#22240;&#27492;&#65292;&#21457;&#29616;&#25856;&#23721;&#32773;&#19979;&#38477;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#36825;&#26631;&#24535;&#30528;&#25856;&#30331;&#32467;&#26463;&#12290;&#24517;&#39035;&#22312;&#20445;&#25252;&#25856;&#23721;&#32773;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#38544;&#31169;&#21644;&#20415;&#21033;&#24615;&#30340;&#21516;&#26102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#30828;&#20214;&#21407;&#22411;&#65292;&#20351;&#29992;&#38468;&#22312;&#22681;&#19978;&#30340;&#25856;&#23721;&#35774;&#22791;&#19978;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#25910;&#38598;&#25968;&#25454;&#65292;&#31216;&#20026;&#24555;&#25346;&#65292;&#23427;&#36830;&#25509;&#25856;&#23721;&#32499;&#21644;&#34746;&#26643;&#38170;&#28857;&#12290;&#30456;&#24212;&#30340;&#20256;&#24863;&#22120;&#34987;&#37197;&#32622;&#20026;&#33410;&#33021;&#65292;&#22240;&#27492;&#22312;&#25856;&#23721;&#20581;&#36523;&#25151;&#22823;&#37327;&#20351;&#29992;&#26102;&#22312;&#36153;&#29992;&#21644;&#26356;&#25442;&#25152;&#38656;&#26102;&#38388;&#26041;&#38754;&#21464;&#24471;&#23454;&#29992;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#30828;&#20214;&#35268;&#26684;&#65292;&#24182;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;&#27979;&#24471;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.10164v2 Announce Type: replace-cross  Abstract: Tracking climbers' activity to improve services and make the best use of their infrastructure is a concern for climbing gyms. Each climbing session must be analyzed from beginning till lowering of the climber. Therefore, spotting the climbers descending is crucial since it indicates when the ascent has come to an end. This problem must be addressed while preserving privacy and convenience of the climbers and the costs of the gyms. To this aim, a hardware prototype is developed to collect data using accelerometer sensors attached to a piece of climbing equipment mounted on the wall, called quickdraw, that connects the climbing rope to the bolt anchors. The corresponding sensors are configured to be energy-efficient, hence become practical in terms of expenses and time consumption for replacement when using in large quantity in a climbing gym. This paper describes hardware specifications, studies data measured by the sensors in u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#22270;&#20687;&#35270;&#22270;&#21512;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#20960;&#20309;&#21464;&#25442;&#65292;&#21033;&#29992;&#39640;&#25928;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#8220;&#36343;&#36343;&#26495;&#8221;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2209.07105</link><description>&lt;p&gt;
&#23558;&#38544;&#24335;&#21644;&#26174;&#24335;&#20960;&#20309;&#21464;&#25442;&#26725;&#25509;&#29992;&#20110;&#21333;&#22270;&#20687;&#35270;&#22270;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Bridging Implicit and Explicit Geometric Transformation for Single-Image View Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.07105
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#22270;&#20687;&#35270;&#22270;&#21512;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#20960;&#20309;&#21464;&#25442;&#65292;&#21033;&#29992;&#39640;&#25928;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#8220;&#36343;&#36343;&#26495;&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#21019;&#24314;&#26032;&#35270;&#22270;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#22240;&#20026;&#19981;&#21487;&#35265;&#21306;&#22495;&#24517;&#39035;&#20174;&#21487;&#35265;&#22330;&#26223;&#20869;&#23481;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#22270;&#65292;&#20294;&#21482;&#20351;&#29992;&#19968;&#20010;&#26174;&#24335;&#25110;&#38544;&#24335;&#30340;3D&#20960;&#20309;&#20307;&#36827;&#34892;&#21512;&#25104;&#23384;&#22312;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#36343;&#36343;&#26495;&#8221;&#38382;&#39064;&#65306;1&#65289;&#20445;&#30041;&#37325;&#26032;&#25237;&#24433;&#30340;&#20869;&#23481;&#65292;2&#65289;&#23436;&#25104;&#36924;&#30495;&#30340;&#35270;&#37326;&#20043;&#22806;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#22270;&#20687;&#35270;&#22270;&#21512;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#32531;&#35299;&#8220;&#36343;&#36343;&#26495;&#8221;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#39640;&#25928;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#21463;&#21040;&#26174;&#24335;&#26041;&#27861;&#24456;&#22909;&#22320;&#20445;&#30041;&#37325;&#26032;&#25237;&#24433;&#20687;&#32032;&#21644;&#38544;&#24335;&#26041;&#27861;&#23436;&#25104;&#36924;&#30495;&#35270;&#37326;&#22806;&#21306;&#22495;&#30340;&#29305;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26469;&#34917;&#20805;&#20004;&#20010;&#28210;&#26579;&#22120;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#20419;&#36827;&#20102;&#26174;&#24335;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.07105v3 Announce Type: replace-cross  Abstract: Creating novel views from a single image has achieved tremendous strides with advanced autoregressive models, as unseen regions have to be inferred from the visible scene contents. Although recent methods generate high-quality novel views, synthesizing with only one explicit or implicit 3D geometry has a trade-off between two objectives that we call the "seesaw" problem: 1) preserving reprojected contents and 2) completing realistic out-of-view regions. Also, autoregressive models require a considerable computational cost. In this paper, we propose a single-image view synthesis framework for mitigating the seesaw problem while utilizing an efficient non-autoregressive model. Motivated by the characteristics that explicit methods well preserve reprojected pixels and implicit methods complete realistic out-of-view regions, we introduce a loss function to complement two renderers. Our loss function promotes that explicit features 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diana&#30340;&#21160;&#24577;&#26550;&#26500;&#32456;&#36523;QA&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;QA&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2208.14602</link><description>&lt;p&gt;
&#20855;&#26377;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#25345;&#32493;&#38382;&#31572;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continuous QA Learning with Structured Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.14602
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diana&#30340;&#21160;&#24577;&#26550;&#26500;&#32456;&#36523;QA&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;QA&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#32456;&#36523;&#23398;&#20064;&#65288;LL&#65289;&#33021;&#21147;&#30340;QA&#27169;&#22411;&#23545;&#20110;&#23454;&#38469;&#30340;QA&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#22522;&#20110;&#26550;&#26500;&#30340;LL&#26041;&#27861;&#34987;&#25253;&#21578;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#20808;&#21069;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;QA&#20219;&#21153;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#22312;&#27979;&#35797;&#38454;&#27573;&#38656;&#35201;&#35775;&#38382;&#20219;&#21153;&#26631;&#35782;&#65292;&#35201;&#20040;&#19981;&#26126;&#30830;&#22320;&#23545;&#26469;&#33258;&#26410;&#35265;&#20219;&#21153;&#30340;&#26679;&#26412;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diana&#65306;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#32456;&#36523;QA&#27169;&#22411;&#65292;&#35797;&#22270;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#12290;&#22312;Diana&#20013;&#20351;&#29992;&#20102;&#22235;&#31181;&#23618;&#27425;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;QA&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#32423;&#25552;&#31034;&#29992;&#20110;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#20197;&#20445;&#25345;&#39640;LL&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#23454;&#20363;&#32423;&#25552;&#31034;&#26469;&#23398;&#20064;&#36328;&#19981;&#21516;&#36755;&#20837;&#26679;&#26412;&#20849;&#20139;&#30340;&#30693;&#35782;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.14602v3 Announce Type: replace-cross  Abstract: QA models with lifelong learning (LL) abilities are important for practical QA applications, and architecture-based LL methods are reported to be an effective implementation for these models. However, it is non-trivial to extend previous approaches to QA tasks since they either require access to task identities in the testing phase or do not explicitly model samples from unseen tasks. In this paper, we propose Diana: a dynamic architecture-based lifelong QA model that tries to learn a sequence of QA tasks with a prompt enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture QA knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across different input samples to improve the model's generalization performance. Moreover, we dedi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#30561;&#30496;&#20449;&#24687;&#65292;&#20026;&#30740;&#31350;&#30561;&#30496;&#19982;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21457;&#30149;&#20851;&#32852;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;</title><link>https://arxiv.org/abs/2204.09601</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#30340;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#30561;&#30496;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Extraction of Sleep Information from Clinical Notes of Patients with Alzheimer's Disease Using Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.09601
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#30561;&#30496;&#20449;&#24687;&#65292;&#20026;&#30740;&#31350;&#30561;&#30496;&#19982;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21457;&#30149;&#20851;&#32852;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Alzheimer's Disease&#65288;AD&#65289;&#26159;&#32654;&#22269;&#26368;&#24120;&#35265;&#30340;&#30196;&#21574;&#24418;&#24335;&#65292;&#30561;&#30496;&#26159;&#24433;&#21709;&#32769;&#24180;&#35748;&#30693;&#21151;&#33021;&#26368;&#20851;&#38190;&#30340;&#29983;&#27963;&#26041;&#24335;&#22240;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#30561;&#30496;&#19982;AD&#21457;&#30149;&#20043;&#38388;&#20851;&#32852;&#30340;&#30740;&#31350;&#21294;&#20047;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25163;&#21160;&#27880;&#37322;UPMC&#25910;&#38598;&#30340;7,266&#21517;AD&#24739;&#32773;&#30340;192,000&#20221;&#20020;&#24202;&#35760;&#24405;&#20013;&#30340;&#38543;&#26426;&#25277;&#26679;&#25991;&#26723;&#65292;&#21019;&#24314;&#20102;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;NLP&#31639;&#27861;&#65292;&#20197;&#33258;&#21160;&#21270;&#25552;&#21462;&#30561;&#30496;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.09601v2 Announce Type: replace-cross  Abstract: Alzheimer's Disease (AD) is the most common form of dementia in the United States. Sleep is one of the lifestyle-related factors that has been shown critical for optimal cognitive function in old age. However, there is a lack of research studying the association between sleep and AD incidence. A major bottleneck for conducting such research is that the traditional way to acquire sleep information is time-consuming, inefficient, non-scalable, and limited to patients' subjective experience. A gold standard dataset is created from manual annotation of 570 randomly sampled clinical note documents from the adSLEEP, a corpus of 192,000 de-identified clinical notes of 7,266 AD patients retrieved from the University of Pittsburgh Medical Center (UPMC). We developed a rule-based Natural Language Processing (NLP) algorithm, machine learning models, and Large Language Model(LLM)-based NLP algorithms to automate the extraction of sleep-rel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#22810;&#30446;&#26631;&#21270;&#65288;MMO&#65289;&#27169;&#22411;&#65292;&#23558;&#36741;&#21161;&#24615;&#33021;&#30446;&#26631;&#29992;&#20110;&#20351;&#34920;&#29616;&#30456;&#20284;&#20294;&#37197;&#32622;&#19981;&#21516;&#30340;&#37197;&#32622;&#38590;&#20197;&#27604;&#36739;&#65292;&#20174;&#32780;&#38450;&#27490;&#25628;&#32034;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;</title><link>https://arxiv.org/abs/2112.07303</link><description>&lt;p&gt;
MMO: &#20803;&#22810;&#30446;&#26631;&#21270;&#29992;&#20110;&#36719;&#20214;&#37197;&#32622;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
MMO: Meta Multi-Objectivization for Software Configuration Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.07303
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#22810;&#30446;&#26631;&#21270;&#65288;MMO&#65289;&#27169;&#22411;&#65292;&#23558;&#36741;&#21161;&#24615;&#33021;&#30446;&#26631;&#29992;&#20110;&#20351;&#34920;&#29616;&#30456;&#20284;&#20294;&#37197;&#32622;&#19981;&#21516;&#30340;&#37197;&#32622;&#38590;&#20197;&#27604;&#36739;&#65292;&#20174;&#32780;&#38450;&#27490;&#25628;&#32034;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#37197;&#32622;&#35843;&#25972;&#23545;&#20110;&#20248;&#21270;&#32473;&#23450;&#30340;&#24615;&#33021;&#30446;&#26631;&#65288;&#20363;&#22914;&#26368;&#23567;&#21270;&#24310;&#36831;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36719;&#20214;&#22266;&#26377;&#30340;&#22797;&#26434;&#37197;&#32622;&#26223;&#35266;&#21644;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#21462;&#24471;&#20102;&#30456;&#23545;&#36739;&#20026;&#28201;&#21644;&#30340;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22312;&#38450;&#27490;&#25628;&#32034;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#26041;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#19981;&#26159;&#19987;&#27880;&#20110;&#25913;&#36827;&#20248;&#21270;&#22120;&#65292;&#32780;&#26159;&#22312;&#20248;&#21270;&#27169;&#22411;&#30340;&#23618;&#38754;&#19978;&#36827;&#34892;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36741;&#21161;&#24615;&#33021;&#30446;&#26631;&#65288;&#20363;&#22914;&#21534;&#21520;&#37327;&#65289;&#30340;&#20803;&#22810;&#30446;&#26631;&#21270;&#65288;MMO&#65289;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#25105;&#20204;&#24182;&#19981;&#20248;&#21270;&#36741;&#21161;&#24615;&#33021;&#30446;&#26631;&#65292;&#32780;&#26159;&#21033;&#29992;&#23427;&#20351;&#34920;&#29616;&#30456;&#20284;&#20294;&#37197;&#32622;&#19981;&#21516;&#30340;&#37197;&#32622;&#38590;&#20197;&#27604;&#36739;&#65288;&#21363;&#24444;&#27492;&#20043;&#38388;&#26159;&#24085;&#32047;&#25176;&#38750;&#25903;&#37197;&#30340;&#65289;&#65292;&#20174;&#32780;&#38450;&#27490;&#25628;&#32034;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.07303v3 Announce Type: replace-cross  Abstract: Software configuration tuning is essential for optimizing a given performance objective (e.g., minimizing latency). Yet, due to the software's intrinsically complex configuration landscape and expensive measurement, there has been a rather mild success, particularly in preventing the search from being trapped in local optima. To address this issue, in this paper we take a different perspective. Instead of focusing on improving the optimizer, we work on the level of optimization model and propose a meta multi-objectivization (MMO) model that considers an auxiliary performance objective (e.g., throughput in addition to latency). What makes this model distinct is that we do not optimize the auxiliary performance objective, but rather use it to make similarly-performing while different configurations less comparable (i.e. Pareto nondominated to each other), thus preventing the search from being trapped in local optima. Importantly,
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#39062;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#25277;&#35937;&#29366;&#24577;&#34920;&#31034;&#30340;&#20805;&#20998;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#36870;&#27169;&#22411;&#20272;&#35745;&#21644;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#30340;&#23454;&#29992;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#35757;&#32451;&#65292;&#19981;&#20381;&#36182;&#22870;&#21169;&#20449;&#21495;&#20294;&#21487;&#20197;&#21033;&#29992;&#22870;&#21169;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2106.04379</link><description>&lt;p&gt;
&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#25277;&#35937;&#20197;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Markov State Abstractions for Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.04379
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#39062;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#25277;&#35937;&#29366;&#24577;&#34920;&#31034;&#30340;&#20805;&#20998;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#36870;&#27169;&#22411;&#20272;&#35745;&#21644;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#30340;&#23454;&#29992;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#35757;&#32451;&#65292;&#19981;&#20381;&#36182;&#22870;&#21169;&#20449;&#21495;&#20294;&#21487;&#20197;&#21033;&#29992;&#22870;&#21169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20551;&#35774;&#26159;&#65292;&#30456;&#20851;&#30340;&#20915;&#31574;&#36807;&#31243;&#23454;&#38469;&#19978;&#26159;&#39532;&#23572;&#21487;&#22827;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;MDPs&#20855;&#26377;&#20016;&#23500;&#30340;&#35266;&#27979;&#26102;&#65292;&#20195;&#29702;&#36890;&#24120;&#36890;&#36807;&#25277;&#35937;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#31181;&#34920;&#31034;&#26410;&#24517;&#33021;&#20445;&#25345;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#36275;&#20197;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#25277;&#35937;&#29366;&#24577;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#36870;&#27169;&#22411;&#20272;&#35745;&#21644;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20284;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#30340;&#25277;&#35937;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#35757;&#32451;&#30446;&#26631;&#36866;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#35757;&#32451;&#65306;&#23427;&#19981;&#38656;&#35201;&#22870;&#21169;&#20449;&#21495;&#65292;&#20294;&#24403;&#21487;&#29992;&#26102;&#65292;&#20195;&#29702;&#21487;&#20197;&#21033;&#29992;&#22870;&#21169;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35270;&#35273;&#26684;&#23376;&#19990;&#30028;&#22495;&#21644;&#19968;&#32452;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.04379v4 Announce Type: replace-cross  Abstract: A fundamental assumption of reinforcement learning in Markov decision processes (MDPs) is that the relevant decision process is, in fact, Markov. However, when MDPs have rich observations, agents typically learn by way of an abstract state representation, and such representations are not guaranteed to preserve the Markov property. We introduce a novel set of conditions and prove that they are sufficient for learning a Markov abstract state representation. We then describe a practical training procedure that combines inverse model estimation and temporal contrastive learning to learn an abstraction that approximately satisfies these conditions. Our novel training objective is compatible with both online and offline training: it does not require a reward signal, but agents can capitalize on reward information when available. We empirically evaluate our approach on a visual gridworld domain and a set of continuous control benchmar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12689</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#12289;&#20551;&#35774;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#24182;&#19981;&#24120;&#35265;&#12290;&#33258;&#21160;&#27169;&#22411;&#35780;&#20272;&#65288;AutoEval&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#24037;&#20316;&#27969;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#19968;&#20010;&#25509;&#36817;&#39044;&#27979;&#24615;&#33021;&#30340;&#27979;&#35797;&#31649;&#32447;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#31614;&#30340;&#23384;&#22312;&#12290;&#23613;&#31649;AutoEval&#26694;&#26550;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26041;&#24335;&#8212;&#8212;&#20803;&#20998;&#24067;&#33021;&#37327;&#65288;MDE&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;AutoEval&#26694;&#26550;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;MDE&#30340;&#26680;&#24515;&#26159;&#24314;&#31435;&#19968;&#20010;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#65288;&#33021;&#37327;&#65289;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#33021;&#37327;&#30340;&#23398;&#20064;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;MDE&#19982;&#20998;&#31867;&#25439;&#22833;&#30456;&#36830;&#25509;&#65292;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#25454;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.01989</link><description>&lt;p&gt;
&#37325;&#35775;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#20174;&#20301;&#32622;&#20559;&#35265;&#30340;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias. (arXiv:2401.01989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#26469;&#34920;&#24449;&#21644;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#20808;&#21069;&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#30340;&#26356;&#20026;&#38480;&#21046;&#24615;&#30340;&#24341;&#23548;&#20559;&#35265;&#29616;&#35937;&#30340;&#19968;&#33324;&#34920;&#36848;&#12290;&#20301;&#32622;&#20559;&#35265;&#25429;&#25417;&#21040;&#27169;&#22411;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#26576;&#20123;&#37096;&#20998;&#19978;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#20449;&#24687;&#65292;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLM&#27169;&#22411;&#22914;GPT 3.5-Turbo&#65292;Llama-2&#21644;Dolly-v2&#20013;&#30340;&#20301;&#32622;&#20559;&#35265;&#65292;&#20197;&#21450;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#22914;Pegasus&#21644;BART&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize and study zero-shot abstractive summarization in Large Language Models (LLMs) by measuring position bias, which we propose as a general formulation of the more restrictive lead bias phenomenon studied previously in the literature. Position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others, leading to undesirable behavior. Through numerous experiments on four diverse real-world datasets, we study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization models such as Pegasus and BART. Our findings lead to novel insights and discussion on performance and position bias of models for zero-shot summarization tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#25968;&#25454;&#24402;&#22240;&#26041;&#38754;&#65292;&#19968;&#20123;&#22312;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#33021;&#22815;&#22312;&#23454;&#38469;&#20013;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#20844;&#24179;&#34917;&#20607;&#25110;&#35748;&#21487;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2311.00500</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#24402;&#22240;&#30340;&#26377;&#36259;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing Properties of Data Attribution on Diffusion Models. (arXiv:2311.00500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#25968;&#25454;&#24402;&#22240;&#26041;&#38754;&#65292;&#19968;&#20123;&#22312;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#33021;&#22815;&#22312;&#23454;&#38469;&#20013;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#20844;&#24179;&#34917;&#20607;&#25110;&#35748;&#21487;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#26088;&#22312;&#23558;&#27169;&#22411;&#36755;&#20986;&#36861;&#28335;&#21040;&#35757;&#32451;&#25968;&#25454;&#12290;&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25968;&#25454;&#24402;&#22240;&#24050;&#25104;&#20026;&#19968;&#20010;&#29702;&#24819;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#20026;&#39640;&#36136;&#37327;&#25110;&#29256;&#26435;&#20445;&#25252;&#30340;&#35757;&#32451;&#26679;&#26412;&#27491;&#30830;&#20998;&#37197;&#20215;&#20540;&#65292;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#24471;&#21040;&#20844;&#24179;&#30340;&#34917;&#20607;&#25110;&#35748;&#21487;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22312;&#29702;&#35770;&#19978;&#26377;&#21160;&#26426;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25968;&#25454;&#24402;&#22240;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#26524;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;CIFAR-10&#21644;CelebA&#19978;&#35757;&#32451;&#30340;DDPM&#20197;&#21450;&#22312;ArtBench&#19978;&#36827;&#34892;&#32454;&#35843;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;LoRA&#30340;&#24402;&#22240;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#22312;&#23454;&#38469;&#20013;&#22823;&#24133;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#22522;&#32447;&#65292;&#26080;&#35770;&#26159;&#22312;&#32447;&#24615;&#25968;&#25454;&#24314;&#27169;&#24471;&#20998;&#36824;&#26159;&#21453;&#20107;&#23454;&#35780;&#20272;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21576;&#29616;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data attribution seeks to trace model outputs back to training data. With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated or credited. Several theoretically motivated methods have been proposed to implement data attribution, in an effort to improve the trade-off between computational scalability and effectiveness. In this work, we conduct extensive experiments and ablation studies on attributing diffusion models, specifically focusing on DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive observations that theoretically unjustified design choices for attribution empirically outperform previous baselines by a large margin, in terms of both linear datamodeling score and counterfactual evaluation. Our work presents a signific
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#20559;&#35265;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18913</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#21435;&#38500;&#20559;&#35265;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Debiasing Algorithm through Model Adaptation. (arXiv:2310.18913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#20559;&#35265;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#25104;&#20026;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23481;&#37327;&#30340;&#22686;&#38271;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#25152;&#20135;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#38382;&#39064;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#21457;&#29616;&#20013;&#19978;&#23618;&#21069;&#39304;&#23618;&#26368;&#23481;&#26131;&#20256;&#36882;&#20559;&#35265;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#25237;&#24433;&#23558;&#36825;&#20123;&#23618;&#20056;&#20197;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DAMA&#36890;&#36807;&#21508;&#31181;&#24230;&#37327;&#25351;&#26631;&#26126;&#26174;&#20943;&#23569;&#20102;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#20445;&#25345;&#20102;LLaMA&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#20559;&#35265;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.
&lt;/p&gt;</description></item><item><title>DyST&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#22330;&#26223;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#20174;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#20102;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#30456;&#26426;&#21644;&#22330;&#26223;&#20869;&#23481;&#30340;&#29420;&#31435;&#25511;&#21046;&#35270;&#22270;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.06020</link><description>&lt;p&gt;
DyST&#65306;&#38754;&#21521;&#23454;&#38469;&#35270;&#39057;&#30340;&#21160;&#24577;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DyST: Towards Dynamic Neural Scene Representations on Real-World Videos. (arXiv:2310.06020v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06020
&lt;/p&gt;
&lt;p&gt;
DyST&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#22330;&#26223;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#20174;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#20102;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#30456;&#26426;&#21644;&#22330;&#26223;&#20869;&#23481;&#30340;&#29420;&#31435;&#25511;&#21046;&#35270;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#19990;&#30028;&#30340;&#35270;&#35273;&#29702;&#35299;&#36229;&#36234;&#20102;&#21333;&#20010;&#22270;&#20687;&#30340;&#35821;&#20041;&#21644;&#24179;&#38754;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#21333;&#30446;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#23454;&#38469;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;Dynamic Scene Transformer&#65288;DyST&#65289;&#27169;&#22411;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#30740;&#31350;&#25104;&#26524;&#65292;&#23398;&#20064;&#20102;&#21333;&#30446;&#23454;&#38469;&#35270;&#39057;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#21253;&#25324;&#22330;&#26223;&#20869;&#23481;&#12289;&#27599;&#20010;&#35270;&#35282;&#30340;&#22330;&#26223;&#21160;&#24577;&#21644;&#30456;&#26426;&#23039;&#24577;&#12290;&#36890;&#36807;&#22312;&#21333;&#30446;&#35270;&#39057;&#21644;&#25105;&#20204;&#30340;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;DySO&#19978;&#36827;&#34892;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36825;&#31181;&#20998;&#31163;&#12290;DyST&#23398;&#20064;&#21040;&#20102;&#21160;&#24577;&#22330;&#26223;&#30340;&#20855;&#20307;&#28508;&#22312;&#34920;&#31034;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#22330;&#26223;&#30340;&#30456;&#26426;&#21644;&#20869;&#23481;&#36827;&#34892;&#29420;&#31435;&#25511;&#21046;&#30340;&#35270;&#22270;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.
&lt;/p&gt;</description></item><item><title>Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04345</link><description>&lt;p&gt;
Neur2RO: &#31070;&#32463;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neur2RO: Neural Two-Stage Robust Optimization. (arXiv:2310.04345v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04345
&lt;/p&gt;
&lt;p&gt;
Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#24314;&#27169;&#21644;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;&#26412;&#24037;&#20316;&#35299;&#20915;&#20102;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#65288;&#20063;&#31216;&#20026;&#21487;&#35843;&#25972;&#40065;&#26834;&#20248;&#21270;&#65289;&#38382;&#39064;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#23454;&#29616;&#20043;&#21069;&#21644;&#20043;&#21518;&#36827;&#34892;&#31532;&#19968;&#38454;&#27573;&#21644;&#31532;&#20108;&#38454;&#27573;&#30340;&#20915;&#31574;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#35745;&#31639;&#19978;&#26469;&#35828;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#20915;&#31574;&#26159;&#31163;&#25955;&#30340;&#26102;&#20505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Neur2RO&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#65288;CCG&#65289;&#30340;&#23454;&#20363;&#31639;&#27861;&#65292;CCG&#26159;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#30340;&#32463;&#20856;&#36845;&#20195;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#36825;&#31181;&#26550;&#26500;&#26131;&#20110;&#20248;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;CCG&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#24555;&#36895;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#65292;&#36825;&#22312;&#20004;&#20010;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#65288;&#32972;&#21253;&#38382;&#39064;&#21644;&#36164;&#26412;&#39044;&#31639;&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called adjustable robust optimization), wherein first-stage and second-stage decisions are made before and after uncertainty is realized, respectively. This results in a nested min-max-min optimization problem which is extremely challenging computationally, especially when the decisions are discrete. We propose Neur2RO, an efficient machine learning-driven instantiation of column-and-constraint generation (CCG), a classical iterative algorithm for 2RO. Specifically, we learn to estimate the value function of the second-stage problem via a novel neural network architecture that is easy to optimize over by design. Embedding our neural network into CCG yields high-quality solutions quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital budgeting. For knapsack, Ne
&lt;/p&gt;</description></item><item><title>SNIP&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02227</link><description>&lt;p&gt;
SNIP: &#29992;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#36830;&#25509;&#25968;&#23398;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training. (arXiv:2310.02227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02227
&lt;/p&gt;
&lt;p&gt;
SNIP&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#26080;&#27861;&#32570;&#23569;&#31526;&#21495;&#25968;&#23398;&#26041;&#31243;&#26469;&#24314;&#27169;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#30340;&#26102;&#20195;&#65292;&#31185;&#23398;&#25506;&#31350;&#24448;&#24448;&#28041;&#21450;&#21040;&#25910;&#38598;&#35266;&#23519;&#25968;&#25454;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#27934;&#23519;&#21147;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#29305;&#21270;&#20110;&#25968;&#20540;&#39046;&#22495;&#25110;&#31526;&#21495;&#39046;&#22495;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#20026;&#29305;&#23450;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#30417;&#30563;&#24335;&#35757;&#32451;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#31526;&#21495;&#26041;&#31243;&#21644;&#20854;&#25968;&#20540;&#23545;&#24212;&#29289;&#20043;&#38388;&#21487;&#33021;&#20135;&#29983;&#30340;&#37325;&#22823;&#22909;&#22788;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SNIP&#65292;&#19968;&#31181;&#31526;&#21495;-&#25968;&#20540;&#38598;&#25104;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#23884;&#20837;&#20013;&#30340;&#30456;&#20114;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#36827;&#34892;&#28508;&#31354;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SNIP&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era where symbolic mathematical equations are indispensable for modeling complex natural phenomena, scientific inquiry often involves collecting observations and translating them into mathematical expressions. Recently, deep learning has emerged as a powerful tool for extracting insights from data. However, existing models typically specialize in either numeric or symbolic domains, and are usually trained in a supervised manner tailored to specific tasks. This approach neglects the substantial benefits that could arise from a task-agnostic unified understanding between symbolic equations and their numeric counterparts. To bridge the gap, we introduce SNIP, a Symbolic-Numeric Integrated Pre-training, which employs joint contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the pre-trained embeddings. By performing latent space analysis, we observe that SNIP provides cross-domain insights into the representations, revealing that symbolic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.00535</link><description>&lt;p&gt;
JoMA: &#36890;&#36807;MLP&#21644;&#27880;&#24847;&#21147;&#30340;&#32852;&#21512;&#21160;&#21147;&#23398;&#26469;&#35299;&#23494;&#22810;&#23618;Transformer
&lt;/p&gt;
&lt;p&gt;
JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention. (arXiv:2310.00535v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;Transformer&#20013;&#21435;&#38500;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#24471;&#21040;&#20165;&#21253;&#21547;MLP&#23618;&#30340;&#20462;&#25913;&#21518;&#21160;&#24577;&#12290;JoMA&#28040;&#38500;&#20102;&#20808;&#21069;&#20998;&#26512;&#20013;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#32570;&#20047;&#27531;&#24046;&#36830;&#25509;&#65289;&#65292;&#24182;&#39044;&#27979;&#27880;&#24847;&#21147;&#22312;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#24773;&#20917;&#19979;&#39318;&#20808;&#21464;&#24471;&#31232;&#30095;&#65288;&#20026;&#20102;&#23398;&#20064;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#28982;&#21518;&#21464;&#24471;&#23494;&#38598;&#65288;&#20026;&#20102;&#23398;&#20064;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#32780;&#22312;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;&#23427;&#19982;&#29616;&#26377;&#30740;&#31350;&#19968;&#33268;&#65292;&#26174;&#31034;&#20986;&#27880;&#24847;&#21147;&#38543;&#26102;&#38388;&#21464;&#24471;&#31232;&#30095;&#12290;&#25105;&#20204;&#21033;&#29992;JoMA&#23450;&#24615;&#22320;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#22914;&#20309;&#23558;&#26631;&#35760;&#32452;&#21512;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#24403;&#36755;&#20837;&#26631;&#35760;&#26159;&#30001;&#28508;&#22312;&#30340;&#23618;&#27425;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#26102;&#12290;&#22312;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;Wikitext2/Wikitext103&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;OPT&#65292;Pythia&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DeepRepViz&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#35782;&#21035;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#24230;&#37327;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24102;&#26469;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2309.15551</link><description>&lt;p&gt;
&#20351;&#29992;DeepRepViz&#26469;&#35782;&#21035;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Identifying confounders in deep-learning-based model predictions using DeepRepViz. (arXiv:2309.15551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DeepRepViz&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#35782;&#21035;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#24230;&#37327;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24102;&#26469;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#25581;&#31034;&#22823;&#33041;&#12289;&#22823;&#33041;&#30149;&#29702;&#21644;&#24515;&#29702;&#29305;&#24449;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#21442;&#19982;&#32773;&#24180;&#40836;&#12289;&#24615;&#21035;&#25110;&#24433;&#20687;&#20266;&#24433;&#31561;&#22806;&#37096;&#30340;&#8220;&#28151;&#28102;&#22240;&#32032;&#8221;&#21464;&#37327;&#21487;&#33021;&#20250;&#20559;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#23398;&#20064;&#30456;&#20851;&#30340;&#33041;-&#34920;&#22411;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;DeepRepViz&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#31995;&#32479;&#22320;&#26816;&#27979;DL&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;(1)&#24230;&#37327;&#21487;&#33021;&#28151;&#28102;&#22240;&#32032;&#30340;&#24433;&#21709;&#31243;&#24230;&#30340;&#25351;&#26631;&#21644;(2)&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#23450;&#24615;&#26816;&#26597;DL&#27169;&#22411;&#23398;&#20064;&#20869;&#23481;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#30340;&#30410;&#22788;&#12290;&#20363;&#22914;&#65292;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#24615;&#21035;&#26159;DL&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#26174;&#33879;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) models are increasingly used to analyze neuroimaging data and uncover insights about the brain, brain pathologies, and psychological traits. However, extraneous `confounders' variables such as the age of the participants, sex, or imaging artifacts can bias model predictions, preventing the models from learning relevant brain-phenotype relationships. In this study, we provide a solution called the `DeepRepViz' framework that enables researchers to systematically detect confounders in their DL model predictions. The framework consists of (1) a metric that quantifies the effect of potential confounders and (2) a visualization tool that allows researchers to qualitatively inspect what the DL model is learning. By performing experiments on simulated and neuroimaging datasets, we demonstrate the benefits of using DeepRepViz in combination with DL models. For example, experiments on the neuroimaging datasets reveal that sex is a significant confounder in a DL model predicti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2309.14780</link><description>&lt;p&gt;
&#36716;&#31227;&#27668;&#20505;&#21464;&#21270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Transferring climate change knowledge. (arXiv:2309.14780v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14780
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27668;&#20505;&#39044;&#27979;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#20110;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#22312;&#23545;&#23567;&#23610;&#24230;&#29289;&#29702;&#36807;&#31243;&#65288;&#20363;&#22914;&#20113;&#65289;&#30340;&#34920;&#31034;&#20013;&#26412;&#36136;&#19978;&#36827;&#34892;&#20102;&#36817;&#20284;&#65292;&#36825;&#26159;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#23545;&#22686;&#21152;&#30340;&#28201;&#23460;&#27668;&#20307;&#27987;&#24230;&#30340;&#21709;&#24212;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26681;&#28304;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#21382;&#21490;&#35266;&#27979;&#32422;&#26463;&#26410;&#26469;&#39044;&#27979;&#65292;&#24182;&#20943;&#23569;&#27668;&#20505;&#39044;&#27979;&#21644;&#27668;&#20505;&#21453;&#39304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#27668;&#20505;&#31995;&#32479;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#21644;&#25972;&#21512;&#20174;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate climate projections are required for climate adaptation and mitigation. Earth system model simulations, used to project climate change, inherently make approximations in their representation of small-scale physical processes, such as clouds, that are at the root of the uncertainties in global mean temperature's response to increased greenhouse gas concentrations. Several approaches have been developed to use historical observations to constrain future projections and reduce uncertainties in climate projections and climate feedbacks. Yet those methods cannot capture the non-linear complexity inherent in the climate system. Using a Transfer Learning approach, we show that Machine Learning, in particular Deep Neural Networks, can be used to optimally leverage and merge the knowledge gained from Earth system model simulations and historical observations to more accurately project global surface temperature fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Wave-RNN (wRNN)&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26053;&#34892;&#27874;&#26426;&#21046;&#22914;&#20309;&#26377;&#25928;&#22320;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#24182;&#22312;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#20013;&#27604;&#27874;&#21160;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.08045</link><description>&lt;p&gt;
&#26053;&#34892;&#27874;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#24182;&#22686;&#24378;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Traveling Waves Encode the Recent Past and Enhance Sequence Learning. (arXiv:2309.08045v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Wave-RNN (wRNN)&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26053;&#34892;&#27874;&#26426;&#21046;&#22914;&#20309;&#26377;&#25928;&#22320;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#24182;&#22312;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#20013;&#27604;&#27874;&#21160;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27963;&#21160;&#30340;&#26053;&#34892;&#27874;&#29616;&#35937;&#22312;&#22823;&#33041;&#30340;&#19981;&#21516;&#21306;&#22495;&#21644;&#23610;&#24230;&#19978;&#37117;&#26377;&#25152;&#35266;&#23519;&#21040;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#35282;&#33394;&#19978;&#30340;&#20855;&#20307;&#20316;&#29992;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#20551;&#35774;&#35748;&#20026;&#65292;&#30382;&#36136;&#23618;&#21487;&#20197;&#20687;&#27874;&#21160;&#22330;&#19968;&#26679;&#65292;&#36890;&#36807;&#27839;&#30528;&#30382;&#36136;&#34920;&#38754;&#20256;&#25773;&#30340;&#27874;&#21160;&#26469;&#23384;&#20648;&#39034;&#24207;&#21050;&#28608;&#30340;&#30701;&#26399;&#35760;&#24518;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#31616;&#21333;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#33021;&#22815;&#23637;&#29616;&#20986;&#36825;&#31181;&#27874;&#21160;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#24819;&#27861;&#30340;&#35745;&#31639;&#24847;&#20041;&#19968;&#30452;&#26159;&#20551;&#35774;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Wave-RNN (wRNN)&#65292;&#24182;&#23637;&#31034;&#20102;&#36830;&#36890;&#24615;&#32422;&#26463;&#21644;&#21021;&#22987;&#21270;&#22312;&#27874;&#21160;&#21160;&#21147;&#23398;&#20986;&#29616;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#20102;&#36825;&#26679;&#30340;&#26550;&#26500;&#30340;&#30830;&#36890;&#36807;&#19968;&#31995;&#21015;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;wRNN&#27604;&#27874;&#21160;&#27169;&#22411;&#23398;&#20064;&#26356;&#24555;&#12289;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traveling waves of neural activity have been observed throughout the brain at a diversity of regions and scales; however, their precise computational role is still debated. One physically grounded hypothesis suggests that the cortical sheet may act like a wave-field capable of storing a short-term memory of sequential stimuli through induced waves traveling across the cortical surface. To date, however, the computational implications of this idea have remained hypothetical due to the lack of a simple recurrent neural network architecture capable of exhibiting such waves. In this work, we introduce a model to fill this gap, which we denote the Wave-RNN (wRNN), and demonstrate how both connectivity constraints and initialization play a crucial role in the emergence of wave-like dynamics. We then empirically show how such an architecture indeed efficiently encodes the recent past through a suite of synthetic memory tasks where wRNNs learn faster and perform significantly better than wave-
&lt;/p&gt;</description></item><item><title>&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#21019;&#26032;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.06799</link><description>&lt;p&gt;
&#24403;&#22320;&#29699;&#31185;&#23398;&#36935;&#35265;&#22522;&#30784;&#27169;&#22411;&#65306;&#36208;&#21521;&#36890;&#29992;&#22320;&#29699;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
When Geoscience Meets Foundation Models: Towards General Geoscience Artificial Intelligence System. (arXiv:2309.06799v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06799
&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#21019;&#26032;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20195;&#34920;&#20102;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#23427;&#20204;&#20174;&#30334;&#19975;&#20159;&#23383;&#33410;&#30340;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25581;&#31034;&#20986;&#27934;&#23519;&#21147;&#12290;&#28789;&#27963;&#30340;&#20219;&#21153;&#35268;&#33539;&#12289;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#20197;&#21450;&#22810;&#27169;&#24577;&#30340;&#30693;&#35782;&#34920;&#31034;&#20351;&#24471;&#32508;&#21512;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22320;&#29699;&#31185;&#23398;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#20801;&#35768;&#35299;&#20915;&#19982;&#22320;&#29699;&#31995;&#32479;&#30456;&#20114;&#20316;&#29992;&#30456;&#20851;&#30340;&#22810;&#31181;&#39044;&#27979;&#12289;&#27169;&#25311;&#21644;&#20915;&#31574;&#25361;&#25112;&#12290;&#39046;&#22495;&#19987;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#20043;&#38388;&#30340;&#21512;&#20316;&#25512;&#21160;&#20102;&#36825;&#20123;&#23453;&#36149;&#24037;&#20855;&#22312;&#29702;&#35299;&#25105;&#20204;&#22320;&#29699;&#30340;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#26041;&#38754;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#22686;&#24378;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#23558;&#26377;&#21161;&#20110;&#25512;&#21160;&#22320;&#29699;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geoscience foundation models represent a revolutionary approach in the field of Earth sciences by integrating massive cross-disciplinary data to simulate and understand the Earth systems dynamics. As a data-centric artificial intelligence (AI) paradigm, they uncover insights from petabytes of structured and unstructured data. Flexible task specification, diverse inputs and outputs and multi-modal knowledge representation enable comprehensive analysis infeasible with individual data sources. Critically, the scalability and generalizability of geoscience models allow for tackling diverse prediction, simulation, and decision challenges related to Earth systems interactions. Collaboration between domain experts and computer scientists leads to innovations in these invaluable tools for understanding the past, present, and future of our planet. However, challenges remain in validation and verification, scale, interpretability, knowledge representation, and social bias. Going forward, enhanci
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04849</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#31934;&#28860;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#24773;&#24863;&#34920;&#31034;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations. (arXiv:2309.04849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#19968;&#20018;&#35821;&#38899;&#20449;&#21495;&#26469;&#36827;&#34892;&#21333;&#27169;&#24577;SER&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#24182;&#36991;&#20813;&#36816;&#34892;&#26102;&#30340;&#36716;&#24405;&#21644;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#38169;&#35823;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19968;&#23545;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#20013;&#30340;&#23884;&#20837;&#21644;&#36923;&#36753;&#23618;&#38754;&#33976;&#39311;&#20449;&#24687;&#12290;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#24182;&#36798;&#21040;&#20102;77.49&#65285;&#30340;&#26080;&#26435;&#37325;&#20934;&#30830;&#29575;&#21644;78.91&#65285;&#30340;&#21152;&#26435;&#20934;&#30830;&#29575;&#30340;&#26368;&#26032;&#25104;&#32489;&#12290;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose EmoDistill, a novel speech emotion recognition (SER) framework that leverages cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech. During inference, our method only uses a stream of speech signals to perform unimodal SER thus reducing computation overhead and avoiding run-time transcription and prosodic feature extraction errors. During training, our method distills information at both embedding and logit levels from a pair of pre-trained Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on the IEMOCAP benchmark demonstrate that our method outperforms other unimodal and multimodal techniques by a considerable margin, and achieves state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted accuracy. Detailed ablation studies demonstrate the impact of each component of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.02427</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Cognitive Architectures for Language Agents. (arXiv:2309.02427v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#22686;&#21152;&#20102;&#22806;&#37096;&#36164;&#28304;&#65288;&#20363;&#22914;&#20114;&#32852;&#32593;&#65289;&#25110;&#20869;&#37096;&#25511;&#21046;&#27969;&#65288;&#20363;&#22914;&#25552;&#31034;&#38142;&#65289;&#65292;&#29992;&#20110;&#38656;&#35201;&#22522;&#20110;&#35821;&#22659;&#25110;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31867;&#26032;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#23613;&#31649;&#36825;&#20123;&#20195;&#29702;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#26694;&#26550;&#26469;&#32452;&#32455;&#29616;&#26377;&#20195;&#29702;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#20016;&#23500;&#21382;&#21490;&#65292;&#25552;&#20986;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;&#65288;CoALA&#65289;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#29992;&#20110;&#19982;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#30340;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#20197;&#21450;&#36873;&#25321;&#34892;&#21160;&#30340;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;CoALA&#23545;&#26368;&#36817;&#30340;&#22823;&#37327;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#21644;&#32452;&#32455;&#65292;&#24182;&#23637;&#26395;&#20102;&#26356;&#24378;&#22823;&#20195;&#29702;&#30340;&#21487;&#34892;&#26041;&#21521;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CoALA&#23558;&#24403;&#20170;&#30340;&#35821;&#35328;&#20195;&#29702;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#26368;&#22823;&#20844;&#32422;&#25968;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26126;&#30830;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.15594</link><description>&lt;p&gt;
&#21464;&#24418;&#37329;&#21018;&#26159;&#21542;&#33021;&#23398;&#20250;&#26368;&#22823;&#20844;&#32422;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can transformers learn the greatest common divisor?. (arXiv:2308.15594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#26368;&#22823;&#20844;&#32422;&#25968;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26126;&#30830;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#20004;&#20010;&#27491;&#25972;&#25968;&#30340;&#26368;&#22823;&#20844;&#32422;&#25968;&#65288;GCD&#65289;&#30340;&#33021;&#21147;&#12290;&#24403;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#20180;&#32454;&#36873;&#25321;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;98%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#27491;&#30830;&#39044;&#27979;&#21069;100&#20010;GCD&#20013;&#30340;91&#20010;&#12290;&#27169;&#22411;&#30340;&#39044;&#27979;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#24182;&#19988;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#23398;&#20250;&#23558;&#20855;&#26377;&#30456;&#21516;GCD&#30340;&#36755;&#20837;&#23545;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20854;&#38500;&#25968;&#36827;&#34892;&#20998;&#31867;&#12290;&#22522;&#26412;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#22522;&#25968;&#32534;&#30721;&#30340;&#22343;&#21248;&#25805;&#20316;&#25968;&#20165;&#35745;&#31639;&#23569;&#25968;GCD&#65288;&#26368;&#22810;100&#20010;&#20013;&#30340;38&#20010;&#65289;&#65306;&#22522;&#25968;&#30340;&#38500;&#25968;&#20056;&#31215;&#12290;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#22823;&#30340;&#22522;&#25968;&#20801;&#35768;&#19968;&#20123;&#27169;&#22411;&#8220;&#20102;&#35299;&#8221;&#23567;&#30340;&#32032;&#25968;GCD&#12290;&#20351;&#29992;&#23545;&#25968;&#22343;&#21248;&#25805;&#20316;&#25968;&#36827;&#34892;&#35757;&#32451;&#23558;&#24615;&#33021;&#25552;&#21319;&#21040;&#27491;&#30830;&#30340;73&#20010;GCD&#65292;&#24182;&#36890;&#36807;&#20174;&#20498;&#25968;&#24179;&#26041;&#21040;&#23545;&#25968;&#22343;&#21248;&#30340;GCD&#35757;&#32451;&#20998;&#24067;&#30340;&#24179;&#34913;&#65292;&#20351;&#24615;&#33021;&#36798;&#21040;91&#20010;GCD&#12290;&#20174;GCD&#30340;&#22343;&#21248;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#27169;&#22411;&#30772;&#22351;&#20102;&#30830;&#23450;&#24615;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
I investigate the capability of small transformers to compute the greatest common divisor (GCD) of two positive integers. When the training distribution and the representation base are carefully chosen, models achieve 98% accuracy and correctly predict 91 of the 100 first GCD. Model predictions are deterministic and fully interpretable. During training, the models learn to cluster input pairs with the same GCD, and classify them by their divisors. Basic models, trained from uniform operands encoded on small bases, only compute a handful of GCD (up to 38 out of 100): the products of divisors of the base. Longer training and larger bases allow some models to "grok" small prime GCD. Training from log-uniform operands boosts performance to 73 correct GCD, and balancing the training distribution of GCD, from inverse square to log-uniform, to 91 GCD. Training models from a uniform distribution of GCD breaks the deterministic model behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2305.08553</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#30701;&#26399;&#21040;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distilling Knowledge for Short-to-Long Term Trajectory Prediction. (arXiv:2305.08553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#22256;&#38590;&#22312;&#20110;&#38543;&#30528;&#26102;&#38388;&#33539;&#22260;&#30340;&#22686;&#38271;&#65292;&#36712;&#36857;&#30340;&#28436;&#21464;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Di-Long&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#23398;&#29983;&#32593;&#32476;&#20801;&#35768;&#30340;&#35266;&#27979;&#24207;&#21015;&#21644;&#34917;&#20805;&#30446;&#26631;&#24207;&#21015;&#30340;&#24635;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#35753;&#23398;&#29983;&#21644;&#25945;&#24072;&#23545;&#21516;&#19968;&#20010;&#23436;&#25972;&#36712;&#36857;&#23450;&#20041;&#20004;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65306;&#23398;&#29983;&#35266;&#23519;&#19968;&#20010;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#19968;&#20010;&#38271;&#36712;&#36857;&#65292;&#32780;&#25945;&#24072;&#35266;&#23519;&#19968;&#20010;&#26356;&#38271;&#30340;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#19979;&#30340;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term trajectory forecasting is an important and challenging problem in the fields of computer vision, machine learning, and robotics. One fundamental difficulty stands in the evolution of the trajectory that becomes more and more uncertain and unpredictable as the time horizon grows, subsequently increasing the complexity of the problem. To overcome this issue, in this paper, we propose Di-Long, a new method that employs the distillation of a short-term trajectory model forecaster that guides a student network for long-term trajectory prediction during the training process. Given a total sequence length that comprehends the allowed observation for the student network and the complementary target sequence, we let the student and the teacher solve two different related tasks defined over the same full trajectory: the student observes a short sequence and predicts a long trajectory, whereas the teacher observes a longer sequence and predicts the remaining short target trajectory. The
&lt;/p&gt;</description></item><item><title>Musketeer&#26159;&#19968;&#31181;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#65288;TEP&#65289;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#24322;&#26500;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#22343;&#21248;</title><link>http://arxiv.org/abs/2305.07019</link><description>&lt;p&gt;
Musketeer&#65288;&#19968;&#20154;&#20043;&#21147;&#65292;&#19975;&#20154;&#20043;&#21147;&#65289;&#65306;&#20855;&#26377;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts. (arXiv:2305.07019v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07019
&lt;/p&gt;
&lt;p&gt;
Musketeer&#26159;&#19968;&#31181;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#65288;TEP&#65289;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#24322;&#26500;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#22343;&#21248;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65288;&#19975;&#20154;&#20043;&#21147;&#65289;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#23436;&#20840;&#20849;&#20139;&#65288;&#19968;&#20154;&#20043;&#21147;&#65289;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;Musketeer&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a sequence-to-sequence vision-language model whose parameters are jointly trained on all tasks (all for one) and fully shared among multiple tasks (one for all), resulting in a single model which we named Musketeer. The integration of knowledge across heterogeneous tasks is enabled by a novel feature called Task Explanation Prompt (TEP). TEP reduces interference among tasks, allowing the model to focus on their shared structure. With a single model, Musketeer achieves results comparable to or better than strong baselines trained on single tasks, almost uniformly across multiple tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;</title><link>http://arxiv.org/abs/2304.11657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#24378;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#24341;&#23548;&#24605;&#32500;&#38142; (CoT) &#20316;&#20026;&#31034;&#33539;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21487;&#20197;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs &#29983;&#25104;&#30340;&#28436;&#31034;&#25512;&#29702;&#38142;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#19981;&#24688;&#24403;&#30340;&#31034;&#20363; (&#36807;&#20110;&#31616;&#21333;&#25110;&#22797;&#26434;) &#21487;&#20197;&#24433;&#21709;&#22312;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#19979;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Iter-CoT (&#36845;&#20195;&#24341;&#23548;&#24605;&#32500;&#38142;&#25552;&#31034;) &#30340;&#36845;&#20195;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#23454;&#20363;&#24182;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#22686;&#24378;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLMs &#33258;&#20027;&#26356;&#27491;&#38169;&#35823;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#31934;&#30830;&#12289;&#20840;&#38754;&#30340;&#25512;&#29702;&#38142;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs &#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;PKG&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#25968;&#25454;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#35201;&#35299;&#38145;PKG&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;PKG&#30340;&#32508;&#21512;&#35270;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.09572</link><description>&lt;p&gt;
&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#29983;&#24577;&#31995;&#32479;&#65306;&#35843;&#26597;&#19982;&#30740;&#31350;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
An Ecosystem for Personal Knowledge Graphs: A Survey and Research Roadmap. (arXiv:2304.09572v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;PKG&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#25968;&#25454;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#35201;&#35299;&#38145;PKG&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;PKG&#30340;&#32508;&#21512;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#36890;&#24120;&#23450;&#20041;&#20026;&#26377;&#20851;&#20010;&#20154;&#30456;&#20851;&#23454;&#20307;&#12289;&#20854;&#23646;&#24615;&#21644;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#36164;&#28304;&#12290;PKG&#26159;&#23433;&#20840;&#12289;&#31934;&#23494;&#30340;&#20010;&#20154;&#25968;&#25454;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20851;&#38190;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;PKG&#33021;&#22815;&#24191;&#27867;&#24212;&#29992;&#20043;&#21069;&#38656;&#35201;&#35299;&#20915;&#19968;&#20123;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#20851;&#20110;PKG&#30340;&#23450;&#20041;&#65292;&#22240;&#20026;&#26415;&#35821;&#26377;&#22810;&#31181;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;PKG&#23450;&#20041;&#65292;&#24378;&#35843;&#20102;&#65288;1&#65289;&#21333;&#20010;&#20010;&#20307;&#25317;&#26377;&#25968;&#25454;&#21644;&#65288;2&#65289;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#20316;&#20026;&#20027;&#35201;&#30446;&#30340;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;PKG&#35270;&#22270;&#65292;&#38656;&#35201;&#35299;&#38145;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;PKG&#26159;&#26356;&#22823;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#19982;&#25968;&#25454;&#26381;&#21153;&#21644;&#25968;&#25454;&#28304;&#30340;&#25509;&#21475;&#12290;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#23545;&#24403;&#21069;PKG&#30740;&#31350;&#30340;&#20840;&#38754;&#35843;&#26597;&#21644;&#30740;&#31350;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an ecosystem for personal knowledge graphs (PKG), commonly defined as resources of structured information about entities related to an individual, their attributes, and the relations between them. PKGs are a key enabler of secure and sophisticated personal data management and personalized services. However, there are challenges that need to be addressed before PKGs can achieve widespread adoption. One of the fundamental challenges is the very definition of what constitutes a PKG, as there are multiple interpretations of the term. We propose our own definition of a PKG, emphasizing the aspects of (1) data ownership by a single individual and (2) the delivery of personalized services as the primary purpose. We further argue that a holistic view of PKGs is needed to unlock their full potential, and propose a unified framework for PKGs, where the PKG is a part of a larger ecosystem with clear interfaces towards data services and data sources. A comprehensive survey and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21330;&#20013;&#24739;&#32773;&#27835;&#30103;&#36807;&#31243;&#30340;&#38203;&#28860;&#20449;&#24687;&#65292;&#24182;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.13466</link><description>&lt;p&gt;
&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#24247;&#22797;&#38203;&#28860;&#20449;&#24687;&#65306;&#22522;&#20110;&#35268;&#21017;&#21644;&#26426;&#22120;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques. (arXiv:2303.13466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21330;&#20013;&#24739;&#32773;&#27835;&#30103;&#36807;&#31243;&#30340;&#38203;&#28860;&#20449;&#24687;&#65292;&#24182;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24247;&#22797;&#38203;&#28860;&#22312;&#21330;&#20013;&#21518;&#24739;&#32773;&#30340;&#24247;&#22797;&#36807;&#31243;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#36890;&#36807;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21487;&#20197;&#20351;&#24247;&#22797;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#22312;&#39044;&#27979;&#24314;&#27169;&#20026;&#24739;&#32773;&#20998;&#37197;&#27835;&#30103;&#35745;&#21010;&#20043;&#21069;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#26159;&#20174;&#38750;&#32467;&#26500;&#21270;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#24247;&#22797;&#38203;&#28860;&#20449;&#24687;&#25152;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#26469;&#27880;&#37322;&#21330;&#20013;&#24739;&#32773;&#30340;&#27835;&#30103;&#36807;&#31243;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#22312;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#37096;&#32626;&#21040;&#26080;&#26631;&#31614;&#25991;&#26723;&#20043;&#21069;&#65292;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#30340;&#30740;&#31350;&#65292;&#20294;&#23450;&#21046;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical rehabilitation plays a crucial role in the recovery process of post-stroke patients. By personalizing therapies for patients leveraging predictive modeling and electronic health records (EHRs), healthcare providers can make the rehabilitation process more efficient. Before predictive modeling can provide decision support for the assignment of treatment plans, automated methods are necessary to extract physical rehabilitation exercise information from unstructured EHRs. We introduce a rule-based natural language processing algorithm to annotate therapeutic procedures for stroke patients and compare it to several small machine learning models. We find that our algorithm outperforms these models in extracting half of the concepts where sufficient data is available, and individual exercise descriptions can be assigned binary labels with an f-score of no less than 0.75 per concept. More research needs to be done before these algorithms can be deployed on unlabeled documents, but cu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26080;softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;(SOFT)&#65292;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#36924;&#36817;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.03341</link><description>&lt;p&gt;
&#26080;Softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Softmax-free Linear Transformers. (arXiv:2207.03341v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26080;softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;(SOFT)&#65292;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#36924;&#36817;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;(ViTs)&#22312;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#20013;&#36215;&#21040;&#20102;&#25512;&#21160;&#20316;&#29992;&#12290;ViTs&#30340;&#26680;&#24515;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#20986;&#22312;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#36924;&#36817;&#33258;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#30340;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#35270;&#35273;&#35782;&#21035;&#26041;&#38754;&#35201;&#20040;&#22312;&#29702;&#35770;&#19978;&#26377;&#32570;&#38519;&#65292;&#35201;&#20040;&#22312;&#23454;&#36341;&#20013;&#26080;&#25928;&#12290;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#26469;&#28304;&#20110;&#22312;&#36924;&#36817;&#36807;&#31243;&#20013;&#32487;&#25215;&#20102;&#22522;&#20110;softmax&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#20351;&#29992;softmax&#20989;&#25968;&#23545;&#20196;&#29260;&#29305;&#24449;&#21521;&#37327;&#20043;&#38388;&#30340;&#32553;&#25918;&#28857;&#31215;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;&#30001;&#20110;&#23384;&#22312;&#36825;&#20010;softmax&#25805;&#20316;&#65292;&#25361;&#25112;&#20102;&#20219;&#20309;&#21518;&#32493;&#30340;&#32447;&#24615;&#21270;&#24037;&#20316;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26080;softmax&#30340;&#21464;&#25442;&#22120;(SOFT)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#26367;&#20195;&#28857;&#31215;&#30456;&#20284;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#20840;&#33258;&#27880;&#24847;&#30697;&#38453;&#30340;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have pushed the state-of-the-art for visual perception tasks. The self-attention mechanism underpinning the strength of ViTs has a quadratic complexity in both computation and memory usage. This motivates the development of approximating the self-attention at linear complexity. However, an in-depth analysis in this work reveals that existing methods are either theoretically flawed or empirically ineffective for visual recognition. We identify that their limitations are rooted in the inheritance of softmax-based self-attention during approximations, that is, normalizing the scaled dot-product between token feature vectors using the softmax function. As preserving the softmax operation challenges any subsequent linearization efforts. By this insight, a family of Softmax-Free Transformers (SOFT) are proposed. Specifically, a Gaussian kernel function is adopted to replace the dot-product similarity, enabling a full self-attention matrix to be approximated under l
&lt;/p&gt;</description></item></channel></rss>