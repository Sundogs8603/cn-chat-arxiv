<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#31867;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27492;&#26041;&#27861;&#33021;&#22815;&#35757;&#32451;&#20986;&#24179;&#28369;&#19988;&#20934;&#30830;&#30340;&#38750;&#25235;&#21462;&#24179;&#38754;&#25805;&#20316;&#25511;&#21046;&#22120;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#30340;&#28151;&#21512;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#20351;&#29992;&#30340;&#21333;&#27169;&#24577;&#25506;&#32034;&#31574;&#30053;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#24179;&#28369;&#36712;&#36857;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02459</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#20998;&#31867;&#25506;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#38750;&#25235;&#21462;&#24179;&#38754;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration. (arXiv:2308.02459v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#31867;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27492;&#26041;&#27861;&#33021;&#22815;&#35757;&#32451;&#20986;&#24179;&#28369;&#19988;&#20934;&#30830;&#30340;&#38750;&#25235;&#21462;&#24179;&#38754;&#25805;&#20316;&#25511;&#21046;&#22120;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#30340;&#28151;&#21512;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#20351;&#29992;&#30340;&#21333;&#27169;&#24577;&#25506;&#32034;&#31574;&#30053;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#24179;&#28369;&#36712;&#36857;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#23454;&#29616;&#28789;&#24039;&#30340;&#38750;&#25235;&#21462;&#24179;&#38754;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#38382;&#39064;&#30340;&#27424;&#39537;&#21160;&#21644;&#28151;&#21512;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#20877;&#21152;&#19978;&#25705;&#25830;&#21147;&#20132;&#20114;&#25152;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#25511;&#21046;&#34892;&#20026;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#24320;&#21457;&#27492;&#31867;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;RL&#25991;&#29486;&#35299;&#20915;&#38750;&#25235;&#21462;&#25512;&#21160;&#20219;&#21153;&#26102;&#30340;&#20934;&#30830;&#24615;&#20302;&#12289;&#36712;&#36857;&#19981;&#24179;&#28369;&#65292;&#24182;&#19988;&#20165;&#23454;&#29616;&#31616;&#21333;&#30340;&#36816;&#21160;&#65292;&#21363;&#19981;&#26059;&#36716;&#34987;&#25805;&#20316;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#25512;&#27979;&#20043;&#21069;&#20351;&#29992;&#30340;&#21333;&#27169;&#24577;&#25506;&#32034;&#31574;&#30053;&#26080;&#27861;&#25429;&#25417;&#20219;&#21153;&#30340;&#28151;&#21512;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#21644;&#29289;&#20307;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#19981;&#21516;&#30340;&#25509;&#35302;&#20132;&#20114;&#27169;&#24335;&#65292;&#22914;&#31896;&#30528;&#12289;&#28369;&#21160;&#21644;&#20998;&#31163;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#31867;&#20998;&#24067;&#36827;&#34892;&#22810;&#27169;&#24577;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#20986;&#24179;&#28369;&#19988;&#20934;&#30830;&#30340;&#38750;&#25235;&#21462;&#24179;&#38754;&#25805;&#20316;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing robot controllers capable of achieving dexterous nonprehensile manipulation, such as pushing an object on a table, is challenging. The underactuated and hybrid-dynamics nature of the problem, further complicated by the uncertainty resulting from the frictional interactions, requires sophisticated control behaviors. Reinforcement Learning (RL) is a powerful framework for developing such robot controllers. However, previous RL literature addressing the nonprehensile pushing task achieves low accuracy, non-smooth trajectories, and only simple motions, i.e. without rotation of the manipulated object. We conjecture that previously used unimodal exploration strategies fail to capture the inherent hybrid-dynamics of the task, arising from the different possible contact interaction modes between the robot and the object, such as sticking, sliding, and separation. In this work, we propose a multimodal exploration approach through categorical distributions, which enables us to train p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#20851;&#20110;&#26102;&#24577;&#30693;&#35782;&#22270;&#34917;&#20840;&#30340;&#32508;&#36848;&#65292;&#20027;&#35201;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#21069;&#26223;&#23637;&#26395;&#12290;&#30001;&#20110;&#26032;&#30693;&#35782;&#30340;&#19981;&#26029;&#28044;&#29616;&#12289;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#31639;&#27861;&#19981;&#36275;&#20197;&#21450;&#28304;&#25968;&#25454;&#38598;&#20013;&#20449;&#24687;&#30340;&#32570;&#22833;&#65292;&#26102;&#24577;&#30693;&#35782;&#22270;&#24448;&#24448;&#19981;&#23436;&#25972;&#12290;&#22240;&#27492;&#65292;&#26102;&#24577;&#30693;&#35782;&#22270;&#34917;&#20840;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#26088;&#22312;&#22522;&#20110;&#21487;&#29992;&#20449;&#24687;&#39044;&#27979;&#32570;&#22833;&#39033;&#12290;</title><link>http://arxiv.org/abs/2308.02457</link><description>&lt;p&gt;
&#20851;&#20110;&#26102;&#24577;&#30693;&#35782;&#22270;&#34917;&#20840;&#30340;&#32508;&#36848;&#65306;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#21069;&#26223;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
A Survey on Temporal Knowledge Graph Completion: Taxonomy, Progress, and Prospects. (arXiv:2308.02457v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02457
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#20851;&#20110;&#26102;&#24577;&#30693;&#35782;&#22270;&#34917;&#20840;&#30340;&#32508;&#36848;&#65292;&#20027;&#35201;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#21069;&#26223;&#23637;&#26395;&#12290;&#30001;&#20110;&#26032;&#30693;&#35782;&#30340;&#19981;&#26029;&#28044;&#29616;&#12289;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#31639;&#27861;&#19981;&#36275;&#20197;&#21450;&#28304;&#25968;&#25454;&#38598;&#20013;&#20449;&#24687;&#30340;&#32570;&#22833;&#65292;&#26102;&#24577;&#30693;&#35782;&#22270;&#24448;&#24448;&#19981;&#23436;&#25972;&#12290;&#22240;&#27492;&#65292;&#26102;&#24577;&#30693;&#35782;&#22270;&#34917;&#20840;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#26088;&#22312;&#22522;&#20110;&#21487;&#29992;&#20449;&#24687;&#39044;&#27979;&#32570;&#22833;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#29305;&#24449;&#22312;&#22823;&#37327;&#30693;&#35782;&#20013;&#26126;&#26174;&#21487;&#35265;&#65292;&#36825;&#31361;&#26174;&#20102;&#26102;&#24577;&#30693;&#35782;&#22270;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26102;&#24577;&#30693;&#35782;&#22270;&#24448;&#24448;&#22240;&#20026;&#19977;&#20010;&#20027;&#35201;&#21407;&#22240;&#32780;&#19981;&#23436;&#25972;&#65306;&#26032;&#30693;&#35782;&#30340;&#19981;&#26029;&#20986;&#29616;&#65292;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#31639;&#27861;&#30340;&#19981;&#36275;&#65292;&#20197;&#21450;&#28304;&#25968;&#25454;&#38598;&#20013;&#20449;&#24687;&#30340;&#32570;&#22833;&#12290;&#22240;&#27492;&#65292;&#26102;&#24577;&#30693;&#35782;&#22270;&#34917;&#20840;&#20219;&#21153;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#22522;&#20110;&#21487;&#29992;&#20449;&#24687;&#39044;&#27979;&#32570;&#22833;&#39033;&#12290;&#26412;&#25991;&#23545;&#26102;&#24577;&#30693;&#35782;&#22270;&#34917;&#20840;&#26041;&#27861;&#21450;&#20854;&#32454;&#33410;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20027;&#35201;&#30001;&#19977;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#21363;1&#65289;&#32972;&#26223;&#65292;&#28085;&#30422;&#20102;&#26102;&#24577;&#30693;&#35782;&#22270;&#34917;&#20840;&#26041;&#27861;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#35757;&#32451;&#25152;&#38656;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#21327;&#35758;&#65307;2&#65289;&#25554;&#20540;&#65292;&#29992;&#20110;&#20272;&#35745;&#21644;&#39044;&#27979;&#32570;&#22833;&#30340;&#20803;&#32032;&#25110;&#20803;&#32032;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Temporal characteristics are prominently evident in a substantial volume of knowledge, which underscores the pivotal role of Temporal Knowledge Graphs (TKGs) in both academia and industry. However, TKGs often suffer from incompleteness for three main reasons: the continuous emergence of new knowledge, the weakness of the algorithm for extracting structured information from unstructured data, and the lack of information in the source dataset. Thus, the task of Temporal Knowledge Graph Completion (TKGC) has attracted increasing attention, aiming to predict missing items based on the available information. In this paper, we provide a comprehensive review of TKGC methods and their details. Specifically, this paper mainly consists of three components, namely, 1)Background, which covers the preliminaries of TKGC methods, loss functions required for training, as well as the dataset and evaluation protocol; 2)Interpolation, that estimates and predicts the missing elements or set of elements th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Federated Machine Learning&#65288;FedML&#65289;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29616;&#29366;&#65292;&#24182;&#21457;&#29616;&#20102;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.02454</link><description>&lt;p&gt;
&#30740;&#31350;Federated Machine Learning&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
SoK: Assessing the State of Applied Federated Machine Learning. (arXiv:2308.02454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Federated Machine Learning&#65288;FedML&#65289;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29616;&#29366;&#65292;&#24182;&#21457;&#29616;&#20102;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#65292;&#23427;&#22312;&#38656;&#35201;&#20445;&#25252;&#38544;&#31169;&#30340;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;Federated Machine Learning&#65288;FedML&#65289;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#25968;&#25454;&#30340;&#27169;&#22411;&#26041;&#27861;&#65292;&#26356;&#21152;&#37325;&#35270;&#25968;&#25454;&#38544;&#31169;&#12290;&#36890;&#36807;&#20351;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#28304;&#32780;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#65292;FedML&#25552;&#20379;&#20102;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#20445;&#25252;&#38544;&#31169;&#30340;&#29615;&#22659;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;FedML&#22312;&#23454;&#36341;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#24212;&#29992;FedML&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#30830;&#23450;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20840;&#38754;&#31995;&#32479;&#30340;&#25991;&#29486;&#22238;&#39038;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;74&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#20998;&#26512;&#20102;FedML&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20851;&#27880;FedML&#23454;&#29616;&#30340;&#29305;&#24449;&#21644;&#26032;&#20852;&#36235;&#21183;&#65292;&#20197;&#21450;&#39537;&#21160;&#21147;&#21644;&#24212;&#29992;&#26041;&#38754;&#30340;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has shown significant potential in various applications; however, its adoption in privacy-critical domains has been limited due to concerns about data privacy. A promising solution to this issue is Federated Machine Learning (FedML), a model-to-data approach that prioritizes data privacy. By enabling ML algorithms to be applied directly to distributed data sources without sharing raw data, FedML offers enhanced privacy protections, making it suitable for privacy-critical environments. Despite its theoretical benefits, FedML has not seen widespread practical implementation. This study aims to explore the current state of applied FedML and identify the challenges hindering its practical adoption. Through a comprehensive systematic literature review, we assess 74 relevant papers to analyze the real-world applicability of FedML. Our analysis focuses on the characteristics and emerging trends of FedML implementations, as well as the motivational drivers and application
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#20891;&#20107;&#21040;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#37319;&#32435;&#21644;&#25193;&#23637;&#20262;&#29702;&#21407;&#21017;&#20197;&#24212;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#34892;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02448</link><description>&lt;p&gt;
&#20174;&#20891;&#20107;&#21040;&#21307;&#30103;&#20445;&#20581;&#65306;&#37319;&#32435;&#21644;&#25193;&#23637;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence. (arXiv:2308.02448v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#20891;&#20107;&#21040;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#37319;&#32435;&#21644;&#25193;&#23637;&#20262;&#29702;&#21407;&#21017;&#20197;&#24212;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#34892;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2020&#24180;&#65292;&#32654;&#22269;&#22269;&#38450;&#37096;&#27491;&#24335;&#20844;&#24067;&#20102;&#19968;&#22871;&#25351;&#23548;&#26410;&#26469;&#25112;&#22330;&#19978;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20351;&#29992;&#30340;&#20262;&#29702;&#21407;&#21017;&#12290;&#23613;&#31649;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#20294;&#20891;&#20107;&#21644;&#21307;&#30103;&#26381;&#21153;&#20043;&#38388;&#23384;&#22312;&#26680;&#24515;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25112;&#22330;&#19978;&#30340;&#25112;&#22763;&#32463;&#24120;&#38754;&#20020;&#38656;&#35201;&#24555;&#36895;&#20915;&#31574;&#30340;&#25913;&#21464;&#29983;&#27963;&#30340;&#24773;&#20917;&#12290;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#21307;&#30103;&#29615;&#22659;&#20013;&#20063;&#38754;&#20020;&#31867;&#20284;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#22312;&#24613;&#35786;&#31185;&#25110;&#27835;&#30103;&#21361;&#21450;&#29983;&#21629;&#30340;&#29366;&#20917;&#19979;&#36827;&#34892;&#25163;&#26415;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#26088;&#22312;&#39640;&#25928;&#29983;&#25104;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#38543;&#30528;&#35745;&#31639;&#33021;&#21147;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#22823;&#37327;&#30340;&#20581;&#24247;&#25968;&#25454;&#65288;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12289;&#24515;&#30005;&#22270;&#21644;&#21307;&#23398;&#22270;&#20687;&#65289;&#30340;&#22686;&#21152;&#65292;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24517;&#23558;&#34987;&#36825;&#39033;&#25216;&#26415;&#38761;&#21629;&#21270;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#30740;&#31350;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#20262;&#29702;&#38382;&#39064;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has captivated the research community, leading to debates about 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;AI&#25991;&#29486;&#32508;&#36848;&#22871;&#20214;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21270;&#25991;&#29486;&#32508;&#36848;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25628;&#32034;&#12289;&#19979;&#36733;&#12289;&#25972;&#29702;PDF&#25991;&#20214;&#12289;&#25552;&#21462;&#25991;&#31456;&#20869;&#23481;&#65292;&#24182;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#32508;&#36848;&#25688;&#35201;&#12290;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#21644;&#20248;&#21270;&#23398;&#26415;&#21644;&#24037;&#19994;&#30740;&#31350;&#20013;&#25991;&#29486;&#32508;&#36848;&#36807;&#31243;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.02443</link><description>&lt;p&gt;
AI&#25991;&#29486;&#32508;&#36848;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
AI Literature Review Suite. (arXiv:2308.02443v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02443
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;AI&#25991;&#29486;&#32508;&#36848;&#22871;&#20214;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21270;&#25991;&#29486;&#32508;&#36848;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25628;&#32034;&#12289;&#19979;&#36733;&#12289;&#25972;&#29702;PDF&#25991;&#20214;&#12289;&#25552;&#21462;&#25991;&#31456;&#20869;&#23481;&#65292;&#24182;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#32508;&#36848;&#25688;&#35201;&#12290;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#21644;&#20248;&#21270;&#23398;&#26415;&#21644;&#24037;&#19994;&#30740;&#31350;&#20013;&#25991;&#29486;&#32508;&#36848;&#36807;&#31243;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#25991;&#29486;&#32508;&#36848;&#24448;&#24448;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#24037;&#20316;&#37327;&#22823;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#25991;&#29486;&#32508;&#36848;&#22871;&#20214;&#65292;&#38598;&#25104;&#20102;&#22810;&#31181;&#21151;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#24320;&#25918;&#35775;&#38382;&#31185;&#23398;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;PDF&#25991;&#20214;&#30340;&#25628;&#32034;&#12289;&#19979;&#36733;&#21644;&#25972;&#29702;&#65292;&#20197;&#21450;&#20174;&#25991;&#31456;&#20013;&#25552;&#21462;&#20869;&#23481;&#12290;&#35821;&#20041;&#25628;&#32034;&#26597;&#35810;&#29992;&#20110;&#25968;&#25454;&#26816;&#32034;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#21644;&#25688;&#35201;&#25552;&#20379;&#31616;&#26126;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#65292;&#22686;&#24378;&#20102;&#19982;PDF&#30340;&#20132;&#20114;&#12290;&#35813;&#22871;&#20214;&#36824;&#20855;&#26377;&#29992;&#20110;&#25991;&#29486;&#36164;&#26009;&#32452;&#32455;&#12289;&#20132;&#20114;&#21644;&#26597;&#35810;&#20197;&#21450;&#25991;&#29486;&#32508;&#36848;&#25688;&#35201;&#30340;&#38598;&#25104;&#31243;&#24207;&#12290;&#36825;&#20010;&#24037;&#20855;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#33258;&#21160;&#21270;&#21644;&#20248;&#21270;&#23398;&#26415;&#21644;&#24037;&#19994;&#30740;&#31350;&#20013;&#30340;&#25991;&#29486;&#32508;&#36848;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of conducting literature reviews is often time-consuming and labor-intensive. To streamline this process, I present an AI Literature Review Suite that integrates several functionalities to provide a comprehensive literature review. This tool leverages the power of open access science, large language models (LLMs) and natural language processing to enable the searching, downloading, and organizing of PDF files, as well as extracting content from articles. Semantic search queries are used for data retrieval, while text embeddings and summarization using LLMs present succinct literature reviews. Interaction with PDFs is enhanced through a user-friendly graphical user interface (GUI). The suite also features integrated programs for bibliographic organization, interaction and query, and literature review summaries. This tool presents a robust solution to automate and optimize the process of literature review in academic and industrial research.
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#35774;&#35745;&#21644;&#24320;&#23637;&#39640;&#31561;&#25945;&#32946;&#35838;&#31243;&#38656;&#35201;&#32771;&#34385;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#36981;&#24490;&#25945;&#32946;&#23398;&#25945;&#32946;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25968;&#25454;&#20998;&#26512;&#30740;&#31350;&#20102;&#19971;&#22330;&#32771;&#35797;&#30340;&#32467;&#26524;&#65292;&#21457;&#29616;&#23398;&#29983;&#30340;&#23398;&#20064;&#25104;&#32489;&#19982;ChatGPT&#30340;&#20351;&#29992;&#27809;&#26377;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02441</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#35774;&#35745;&#21644;&#24320;&#23637;&#39640;&#31561;&#25945;&#32946;&#35838;&#31243;&#65306;&#22522;&#20110;&#32771;&#35797;&#25968;&#25454;&#20998;&#26512;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
How to Design and Deliver Courses for Higher Education in the AI Era: Insights from Exam Data Analysis. (arXiv:2308.02441v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02441
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#35774;&#35745;&#21644;&#24320;&#23637;&#39640;&#31561;&#25945;&#32946;&#35838;&#31243;&#38656;&#35201;&#32771;&#34385;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#36981;&#24490;&#25945;&#32946;&#23398;&#25945;&#32946;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25968;&#25454;&#20998;&#26512;&#30740;&#31350;&#20102;&#19971;&#22330;&#32771;&#35797;&#30340;&#32467;&#26524;&#65292;&#21457;&#29616;&#23398;&#29983;&#30340;&#23398;&#20064;&#25104;&#32489;&#19982;ChatGPT&#30340;&#20351;&#29992;&#27809;&#26377;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#31687;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20027;&#24352;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#35838;&#31243;&#21644;&#32771;&#35797;&#30340;&#35774;&#35745;&#24517;&#39035;&#22522;&#20110;&#20004;&#20010;&#22240;&#32032;&#65306;&#65288;1&#65289;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#65288;2&#65289;&#25945;&#32946;&#23398;&#25945;&#32946;&#30446;&#26631;&#12290;&#22522;&#20110;&#23545;Delors&#25945;&#32946;&#25253;&#21578;&#30340;&#27934;&#23519;[1]&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#25945;&#32946;&#30340;&#20316;&#29992;&#65292;&#24182;&#22238;&#39038;&#20102;&#25945;&#32946;&#26426;&#26500;&#22312;&#20219;&#20309;&#25216;&#26415;&#19979;&#37117;&#24517;&#39035;&#21162;&#21147;&#23454;&#29616;&#30340;&#20027;&#35201;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#26681;&#25454;&#36825;&#20123;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#35774;&#35745;&#35838;&#31243;&#21644;&#32771;&#35797;&#65292;&#24182;&#25552;&#20379;&#20102;IT&#12289;&#33521;&#35821;&#21644;&#33402;&#26415;&#39046;&#22495;&#30340;&#19981;&#21516;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#20174;2023&#24180;1&#26376;&#21040;2023&#24180;5&#26376;&#37319;&#29992;&#20102;&#21463;&#33487;&#26684;&#25289;&#24213;&#25945;&#23398;&#27861;&#21551;&#21457;&#30340;&#25945;&#23398;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;2022&#24180;12&#26376;&#33267;2023&#24180;3&#26376;&#20030;&#34892;&#30340;&#19971;&#22330;ChatGPT&#25480;&#26435;&#32771;&#35797;&#30340;&#25968;&#25454;&#20998;&#26512;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32771;&#35797;&#25968;&#25454;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#29983;&#30340;&#23398;&#20064;&#25104;&#32489;&#19982;&#32771;&#35797;&#20013;&#30340;ChatGPT &#19981;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this position paper, we advocate for the idea that courses and exams in the AI era have to be designed based on two factors: (1) the strengths and limitations of AI, and (2) the pedagogical educational objectives. Based on insights from the Delors report on education [1], we first address the role of education and recall the main objectives that educational institutes must strive to achieve independently of any technology. We then explore the strengths and limitations of AI, based on current advances in AI. We explain how courses and exams can be designed based on these strengths and limitations of AI, providing different examples in the IT, English, and Art domains. We show how we adopted a pedagogical approach that is inspired from the Socratic teaching method from January 2023 to May 2023. Then, we present the data analysis results of seven ChatGPT-authorized exams conducted between December 2022 and March 2023. Our exam data results show that there is no correlation between stud
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24037;&#20855;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#25945;&#23398;&#65292;&#33021;&#22815;&#33258;&#21160;&#22238;&#31572;&#24320;&#25918;&#24615;&#38382;&#39064;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21453;&#39304;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#23398;&#29983;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02439</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#25945;&#23398;&#30340;&#25945;&#32946;&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#24320;&#25918;&#24615;&#38382;&#39064;&#20316;&#20986;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
A large language model-assisted education tool to provide feedback on open-ended responses. (arXiv:2308.02439v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02439
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24037;&#20855;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#25945;&#23398;&#65292;&#33021;&#22815;&#33258;&#21160;&#22238;&#31572;&#24320;&#25918;&#24615;&#38382;&#39064;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21453;&#39304;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#23398;&#29983;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24615;&#38382;&#39064;&#26159;&#25945;&#24072;&#35780;&#20272;&#23398;&#29983;&#29702;&#35299;&#21644;&#40723;&#21169;&#23545;&#35838;&#31243;&#26448;&#26009;&#36827;&#34892;&#25209;&#21028;&#24615;&#25506;&#32034;&#30340;&#24120;&#29992;&#24037;&#20855;&#12290;&#20026;&#36825;&#20123;&#22238;&#31572;&#25552;&#20379;&#21453;&#39304;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#23548;&#33268;&#25945;&#24072;&#19981;&#22570;&#37325;&#36127;&#65292;&#21453;&#39304;&#36136;&#37327;&#19979;&#38477;&#12290;&#35768;&#22810;&#25945;&#24072;&#36716;&#32780;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#38382;&#39064;&#26684;&#24335;&#65292;&#22914;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#36825;&#31181;&#26684;&#24335;&#25552;&#20379;&#21363;&#26102;&#21453;&#39304;&#65292;&#20294;&#26080;&#27861;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#28145;&#20837;&#30340;&#35780;&#35770;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#26681;&#25454;&#25945;&#24072;&#23450;&#20041;&#30340;&#26631;&#20934;&#65292;&#33258;&#21160;&#22238;&#31572;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#25552;&#20379;&#24555;&#36895;&#30340;&#20010;&#24615;&#21270;&#21453;&#39304;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#24555;&#36895;&#27979;&#35797;&#20182;&#20204;&#30340;&#30693;&#35782;&#24182;&#25214;&#20986;&#38656;&#35201;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24320;&#28304;&#30340;&#21442;&#32771;&#23454;&#29616;&#65292;&#26082;&#21487;&#20197;&#20316;&#20026;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#19982;&#25945;&#23398;&#32534;&#30721;&#25110;&#25968;&#23398;&#31508;&#35760;&#26412;&#19968;&#36215;&#20351;&#29992;&#30340;Jupyter Notebook&#23567;&#37096;&#20214;&#12290;&#22312;&#25945;&#24072;&#30340;&#25351;&#23548;&#19979;&#65292;LLMs&#26377;&#26395;&#25552;&#39640;&#23398;&#29983;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-ended questions are a favored tool among instructors for assessing student understanding and encouraging critical exploration of course material. Providing feedback for such responses is a time-consuming task that can lead to overwhelmed instructors and decreased feedback quality. Many instructors resort to simpler question formats, like multiple-choice questions, which provide immediate feedback but at the expense of personalized and insightful comments. Here, we present a tool that uses large language models (LLMs), guided by instructor-defined criteria, to automate responses to open-ended questions. Our tool delivers rapid personalized feedback, enabling students to quickly test their knowledge and identify areas for improvement. We provide open-source reference implementations both as a web application and as a Jupyter Notebook widget that can be used with instructional coding or math notebooks. With instructor guidance, LLMs hold promise to enhance student learning outcomes a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#21644;&#23457;&#35745;&#20449;&#25176;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#20027;&#20307;&#22312;&#19982;&#22797;&#26434;&#25216;&#26415;&#31995;&#32479;&#20132;&#20114;&#26102;&#21516;&#24847;&#19981;&#23436;&#20840;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02435</link><description>&lt;p&gt;
&#35774;&#35745;&#20449;&#25176;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Designing Fiduciary Artificial Intelligence. (arXiv:2308.02435v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#21644;&#23457;&#35745;&#20449;&#25176;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#20027;&#20307;&#22312;&#19982;&#22797;&#26434;&#25216;&#26415;&#31995;&#32479;&#20132;&#20114;&#26102;&#21516;&#24847;&#19981;&#23436;&#20840;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#25176;&#20154;&#26159;&#19968;&#20301;&#21463;&#20449;&#20219;&#30340;&#20195;&#29702;&#20154;&#65292;&#20855;&#26377;&#23545;&#38599;&#20323;&#20182;&#20204;&#30340;&#22996;&#25176;&#20154;&#20197;&#24544;&#35802;&#21644;&#20851;&#24515;&#30340;&#27861;&#24459;&#20041;&#21153;&#12290;&#24403;&#20449;&#25176;&#32452;&#32455;&#36890;&#36807;&#25968;&#23383;&#30028;&#38754;&#19982;&#29992;&#25143;&#20132;&#20114;&#65292;&#25110;&#20197;&#20154;&#24037;&#26234;&#33021;&#26041;&#24335;&#33258;&#21160;&#21270;&#20854;&#25805;&#20316;&#26102;&#65292;&#20182;&#20204;&#38656;&#35201;&#35774;&#35745;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20197;&#31526;&#21512;&#20182;&#20204;&#30340;&#32844;&#36131;&#12290;&#26412;&#25991;&#32508;&#21512;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#21644;&#23457;&#35745;&#20449;&#25176;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#12290;&#20449;&#25176;&#20154;&#24037;&#26234;&#33021;&#30340;&#35774;&#35745;&#32773;&#24212;&#20102;&#35299;&#31995;&#32479;&#30340;&#32972;&#26223;&#65292;&#30830;&#23450;&#20854;&#22996;&#25176;&#20154;&#65292;&#24182;&#35780;&#20272;&#36825;&#20123;&#22996;&#25176;&#20154;&#30340;&#26368;&#20339;&#21033;&#30410;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#32773;&#24517;&#39035;&#20197;&#19982;&#21033;&#30410;&#30456;&#20851;&#30340;&#26041;&#24335;&#24544;&#35802;&#19988;&#35880;&#24910;&#12290;&#25105;&#20204;&#23558;&#27492;&#36807;&#31243;&#20013;&#30340;&#27493;&#39588;&#19982;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#32500;&#24230;&#65288;&#22914;&#38544;&#31169;&#21644;&#19968;&#33268;&#24615;&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#20449;&#25176;&#20154;&#24037;&#26234;&#33021;&#26159;&#35299;&#20915;&#25968;&#25454;&#20027;&#20307;&#22312;&#19982;&#22797;&#26434;&#25216;&#26415;&#31995;&#32479;&#20132;&#20114;&#26102;&#21516;&#24847;&#19981;&#23436;&#20840;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fiduciary is a trusted agent that has the legal duty to act with loyalty and care towards a principal that employs them. When fiduciary organizations interact with users through a digital interface, or otherwise automate their operations with artificial intelligence, they will need to design these AI systems to be compliant with their duties. This article synthesizes recent work in computer science and law to develop a procedure for designing and auditing Fiduciary AI. The designer of a Fiduciary AI should understand the context of the system, identify its principals, and assess the best interests of those principals. Then the designer must be loyal with respect to those interests, and careful in an contextually appropriate way. We connect the steps in this procedure to dimensions of Trustworthy AI, such as privacy and alignment. Fiduciary AI is a promising means to address the incompleteness of data subject's consent when interacting with complex technical systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#25152;&#24212;&#29992;&#31185;&#23398;&#22823;&#23398;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#26412;&#31185;&#35838;&#31243;&#20013;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#23427;&#20204;&#20316;&#20026;&#25945;&#23398;&#36741;&#21161;&#24037;&#20855;&#65292;&#22312;&#19981;&#21516;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#35838;&#31243;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#22312;&#36825;&#26679;&#19968;&#20010;&#23398;&#20301;&#35838;&#31243;&#30340;&#32972;&#26223;&#19979;&#30340;&#38480;&#21046;&#21644;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2308.02432</link><description>&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#20301;&#35838;&#31243;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Performance of Large Language Models in a Computer Science Degree Program. (arXiv:2308.02432v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#25152;&#24212;&#29992;&#31185;&#23398;&#22823;&#23398;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#26412;&#31185;&#35838;&#31243;&#20013;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#23427;&#20204;&#20316;&#20026;&#25945;&#23398;&#36741;&#21161;&#24037;&#20855;&#65292;&#22312;&#19981;&#21516;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#35838;&#31243;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#22312;&#36825;&#26679;&#19968;&#20010;&#23398;&#20301;&#35838;&#31243;&#30340;&#32972;&#26223;&#19979;&#30340;&#38480;&#21046;&#21644;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT-3.5&#21644;GPT-4.0&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24403;&#21069;&#30340;&#35752;&#35770;&#20013;&#26080;&#22788;&#19981;&#22312;&#24182;&#19988;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#23427;&#20204;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#33021;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#25105;&#20204;&#19982;&#65288;&#22522;&#20110;&#25991;&#26412;&#30340;&#65289;&#20449;&#24687;&#20114;&#21160;&#21644;&#21033;&#29992;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#27599;&#22825;&#37117;&#26377;&#26032;&#30340;&#21487;&#33021;&#24615;&#26469;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#19968;&#25152;&#24212;&#29992;&#31185;&#23398;&#22823;&#23398;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#26412;&#31185;&#35838;&#31243;&#20013;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#30740;&#31350;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#23427;&#20204;&#20316;&#20026;&#25945;&#23398;&#36741;&#21161;&#24037;&#20855;&#22312;&#35838;&#31243;&#20013;&#20351;&#29992;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#35838;&#22530;&#26448;&#26009;&#12289;&#32451;&#20064;&#20219;&#21153;&#21644;&#36807;&#21435;&#30340;&#32771;&#35797;&#26469;&#21551;&#21457;&#27169;&#22411;&#65292;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#23427;&#20204;&#22312;&#19981;&#21516;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#22312;&#36825;&#26679;&#19968;&#20010;&#23398;&#20301;&#35838;&#31243;&#30340;&#32972;&#26223;&#19979;&#30340;&#38480;&#21046;&#21644;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and dominate the current discourse. Their transformative capabilities have led to a paradigm shift in how we interact with and utilize (text-based) information. Each day, new possibilities to leverage the capabilities of these models emerge. This paper presents findings on the performance of different large language models in a university of applied sciences' undergraduate computer science degree program. Our primary objective is to assess the effectiveness of these models within the curriculum by employing them as educational aids. By prompting the models with lecture material, exercise tasks, and past exams, we aim to evaluate their proficiency across different computer science domains. We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10 tested 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#23398;&#20064;&#27169;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;PyTorch&#23454;&#29616;&#20102;&#25193;&#23637;&#21040;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#21367;&#31215;&#38750;&#36127;&#30456;&#20284;&#24615;&#21305;&#37197;&#65292;&#24341;&#20837;&#20102;&#23616;&#37096;&#30417;&#30563;&#30340;SM&#30446;&#26631;&#65292;&#20197;&#21450;&#21033;&#29992;PyTorch&#23454;&#29616;&#39044;&#35757;&#32451;&#32593;&#32476;&#32467;&#26500;&#30340;&#29305;&#24449;&#35780;&#20272;&#23545;&#27604;&#12290;&#36825;&#23558;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#30340;&#31639;&#27861;&#19982;&#35745;&#31639;&#25104;&#26412;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#32447;&#12289;&#26412;&#22320;&#21270;&#19988;&#20855;&#26377;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02427</link><description>&lt;p&gt;
&#35299;&#38145;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#28508;&#21147;&#65306;&#21487;&#25193;&#23637;&#24615;&#12289;&#30417;&#30563;&#21644;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Similarity Matching: Scalability, Supervision and Pre-training. (arXiv:2308.02427v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#23398;&#20064;&#27169;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;PyTorch&#23454;&#29616;&#20102;&#25193;&#23637;&#21040;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#21367;&#31215;&#38750;&#36127;&#30456;&#20284;&#24615;&#21305;&#37197;&#65292;&#24341;&#20837;&#20102;&#23616;&#37096;&#30417;&#30563;&#30340;SM&#30446;&#26631;&#65292;&#20197;&#21450;&#21033;&#29992;PyTorch&#23454;&#29616;&#39044;&#35757;&#32451;&#32593;&#32476;&#32467;&#26500;&#30340;&#29305;&#24449;&#35780;&#20272;&#23545;&#27604;&#12290;&#36825;&#23558;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#30340;&#31639;&#27861;&#19982;&#35745;&#31639;&#25104;&#26412;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#32447;&#12289;&#26412;&#22320;&#21270;&#19988;&#20855;&#26377;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#25928;&#65292;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#22312;&#29983;&#29289;&#21512;&#29702;&#24615;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#22312;&#32447;&#23398;&#20064;&#36866;&#24212;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#23545;&#20110;&#22522;&#20110;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#30340;&#21487;&#26367;&#20195;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#20027;&#35201;&#26159;&#38750;&#30417;&#30563;&#30340;&#30456;&#20284;&#24615;&#21305;&#37197;(SM)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19982;&#29983;&#29289;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#26426;&#21046;&#30456;&#19968;&#33268;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#32447;&#12289;&#26412;&#22320;&#21270;&#21644;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#12290;i) &#20026;&#20102;&#23558;&#30456;&#20284;&#24615;&#21305;&#37197;(SM)&#25193;&#23637;&#21040;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;PyTorch&#30340;&#21367;&#31215;&#38750;&#36127;SM&#30340;&#23454;&#29616;&#12290;ii) &#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#20856;&#22411;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#23616;&#37096;&#30417;&#30563;SM&#30446;&#26631;&#65292;&#20415;&#20110;&#22534;&#21472;SM&#23618;&#12290;iii) &#25105;&#20204;&#21033;&#29992;PyTorch&#23454;&#29616;&#39044;&#35757;&#32451;&#32593;&#32476;&#32467;&#26500;&#65292;&#22914;LeNet&#65292;&#24182;&#23558;&#20854;&#29305;&#24449;&#35780;&#20272;&#19982;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#23558;&#20855;&#26377;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#31639;&#27861;&#19982;&#35745;&#31639;&#25104;&#26412;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
While effective, the backpropagation (BP) algorithm exhibits limitations in terms of biological plausibility, computational cost, and suitability for online learning. As a result, there has been a growing interest in developing alternative biologically plausible learning approaches that rely on local learning rules. This study focuses on the primarily unsupervised similarity matching (SM) framework, which aligns with observed mechanisms in biological systems and offers online, localized, and biologically plausible algorithms. i) To scale SM to large datasets, we propose an implementation of Convolutional Nonnegative SM using PyTorch. ii) We introduce a localized supervised SM objective reminiscent of canonical correlation analysis, facilitating stacking SM layers. iii) We leverage the PyTorch implementation for pre-training architectures such as LeNet and compare the evaluation of features against BP-trained models. This work combines biologically plausible algorithms with computationa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;NFT&#31199;&#36161;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#25353;&#36190;&#20184;&#36153;&#30340;&#23450;&#20215;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21306;&#22359;&#38142;&#36153;&#29992;&#21487;&#33021;&#19981;&#20844;&#24179;&#65292;&#24182;&#19988;&#23545;&#20110;&#23567;&#20247;&#33402;&#26415;&#23478;&#21487;&#33021;&#20250;&#20135;&#29983;&#38459;&#30861;&#25991;&#21270;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.02424</link><description>&lt;p&gt;
&#23454;&#29616;&#26234;&#33021;&#21512;&#32422;&#65306;NFT&#31199;&#36161;&#19982;&#25353;&#36190;&#20184;&#36153;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Implementing Smart Contracts: The case of NFT-rental with pay-per-like. (arXiv:2308.02424v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;NFT&#31199;&#36161;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#25353;&#36190;&#20184;&#36153;&#30340;&#23450;&#20215;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21306;&#22359;&#38142;&#36153;&#29992;&#21487;&#33021;&#19981;&#20844;&#24179;&#65292;&#24182;&#19988;&#23545;&#20110;&#23567;&#20247;&#33402;&#26415;&#23478;&#21487;&#33021;&#20250;&#20135;&#29983;&#38459;&#30861;&#25991;&#21270;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65288;NFTs&#65289;&#27491;&#22312;&#20852;&#36215;&#12290;&#23427;&#20204;&#21487;&#20197;&#20195;&#34920;&#22312;&#20844;&#21496;&#32593;&#39029;&#25110;&#22312;&#32447;&#21830;&#24215;&#19978;&#23637;&#31034;&#30340;&#33402;&#26415;&#21697;&#8212;&#8212;&#19982;&#23454;&#20307;&#33402;&#26415;&#21697;&#31867;&#20284;&#36827;&#34892;&#24066;&#22330;&#23459;&#20256;&#12290;NFT&#30340;&#20986;&#31199;&#26159;&#25152;&#26377;&#32773;&#30340;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340; pass passive &#25910;&#20837;&#24418;&#24335;&#65292;&#20294;&#20063;&#20276;&#38543;&#30528;&#39118;&#38505;&#65288;&#20363;&#22914;&#65292;&#29289;&#21697;&#27809;&#26377;&#24402;&#36824;&#65289;&#21644;&#20195;&#31649;&#26426;&#26500;&#30340;&#36153;&#29992;&#12290;&#21516;&#26679;&#65292;&#31199;&#20511;&#20154;&#38590;&#20197;&#39044;&#27979;&#33402;&#26415;&#21697;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#65292;NFT&#30340;&#35266;&#20247;&#22914;&#20309;&#24863;&#30693;&#23427;&#20204;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;NFT&#31199;&#36161;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#25353;&#36190;&#20184;&#36153;&#30340;&#23450;&#20215;&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#20197;&#22826;&#22346;&#38142;&#30340;&#26234;&#33021;&#21512;&#32422;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21306;&#22359;&#38142;&#35299;&#20915;&#26041;&#26696;&#20139;&#26377;&#35768;&#22810;&#20063;&#36866;&#29992;&#20110;&#20854;&#20182;&#24212;&#29992;&#31243;&#24207;&#30340;&#20248;&#21183;&#65292;&#20294;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65288;&#22823;&#22411;&#65289;&#21306;&#22359;&#38142;&#36153;&#29992;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#21306;&#22359;&#38142;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#23567;&#20247;&#33402;&#26415;&#23478;&#20284;&#20046;&#26159;&#19981;&#20844;&#24179;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#38459;&#30861;&#25991;&#21270;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#20986;&#29616;&#20102;&#20449;&#20219;&#25104;&#26412;&#26435;&#34913;&#20197;&#22788;&#29702;&#22806;&#37096; parties &#24341;&#36215;&#30340;&#27450;&#35784;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-fungible tokens(NFTs) are on the rise. They can represent artworks exhibited for marketing purposes on webpages of companies or online stores -analogously to physical artworks. Lending of NFTs is an attractive form of passive income for owners but comes with risks (e.g., items are not returned) and costs for escrow agents. Similarly, renters have difficulties in anticipating the impact of artworks, e.g., how spectators of NFTs perceive them. To address these challenges, we introduce an NFT rental solution based on a pay-per-like pricing model using blockchain technology, i.e., smart contracts based on the Ethereum chain. We find that blockchain solutions enjoy many advantages also reported for other applications, but interestingly, we also observe dark sides of (large) blockchain fees. Blockchain solutions appear unfair to niche artists and potentially hamper cultural diversity. Furthermore, a trust-cost tradeoff arises to handle fraud caused by manipulation from parties outside 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31359;&#25140;&#35774;&#22791;&#30340;RSSI&#21644;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#36827;&#34892;&#23460;&#20869;&#23450;&#20301;&#65292;&#20197;&#25913;&#21892;&#24403;&#21069;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23460;&#20869;&#23450;&#20301;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#33647;&#29289;&#20351;&#29992;&#24773;&#20917;&#26469;&#35780;&#20272;&#36816;&#21160;&#27874;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.02419</link><description>&lt;p&gt;
&#22312;&#24085;&#37329;&#26862;&#30149;&#20013;&#29992;&#20110;&#26816;&#27979;&#33647;&#29289;&#20351;&#29992;&#30340;&#22810;&#27169;&#24577;&#23460;&#20869;&#23450;&#20301;&#65306;&#22312;&#33258;&#30001;&#29983;&#27963;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;&#24615;&#35797;&#39564;
&lt;/p&gt;
&lt;p&gt;
Multimodal Indoor Localisation in Parkinson's Disease for Detecting Medication Use: Observational Pilot Study in a Free-Living Setting. (arXiv:2308.02419v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02419
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31359;&#25140;&#35774;&#22791;&#30340;RSSI&#21644;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#36827;&#34892;&#23460;&#20869;&#23450;&#20301;&#65292;&#20197;&#25913;&#21892;&#24403;&#21069;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23460;&#20869;&#23450;&#20301;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#33647;&#29289;&#20351;&#29992;&#24773;&#20917;&#26469;&#35780;&#20272;&#36816;&#21160;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#26159;&#19968;&#31181;&#32531;&#24930;&#36827;&#23637;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#23548;&#33268;&#21253;&#25324;&#27493;&#24577;&#38556;&#30861;&#22312;&#20869;&#30340;&#36816;&#21160;&#30151;&#29366;&#12290;&#36816;&#21160;&#27874;&#21160;&#26159;&#25351;&#22312;&#24038;&#22810;&#24052;&#30103;&#27861;&#65288;&#8220;&#24320;&#8221;&#65289;&#21644;PD&#30151;&#29366;&#20877;&#24230;&#20986;&#29616;&#65288;&#8220;&#20851;&#8221;&#65289;&#20043;&#38388;&#30340;&#21464;&#21270;&#65292;&#22240;&#33647;&#29289;&#25928;&#26524;&#20943;&#36864;&#32780;&#24341;&#36215;&#12290;&#36825;&#20123;&#27874;&#21160;&#32463;&#24120;&#24433;&#21709;&#27493;&#24577;&#36895;&#24230;&#65292;&#24182;&#38543;&#30528;PD&#36827;&#23637;&#32780;&#22686;&#21152;&#20854;&#33268;&#27531;&#24433;&#21709;&#12290;&#20026;&#20102;&#25552;&#39640;&#24403;&#21069;&#23460;&#20869;&#23450;&#20301;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#31034;&#22120;&#65288;RSSI&#65289;&#21644;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25552;&#20379;&#20114;&#34917;&#30340;&#36816;&#21160;&#35270;&#35282;&#12290;&#19968;&#20010;&#27425;&#30446;&#26631;&#26088;&#22312;&#35780;&#20272;&#23460;&#20869;&#23450;&#20301;&#65292;&#21253;&#25324;&#20854;&#23478;&#24237;&#27493;&#24577;&#36895;&#24230;&#29305;&#24449;&#65288;&#21363;&#22312;&#25151;&#38388;&#20043;&#38388;&#34892;&#36208;&#25152;&#38656;&#30340;&#26102;&#38388;&#65289;&#65292;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#36890;&#36807;&#26816;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#26159;&#21542;&#27491;&#22312;&#20351;&#29992;&#24038;&#22810;&#24052;&#33647;&#29289;&#25110;&#24739;&#32773;&#26159;&#21542;&#20572;&#29992;&#33647;&#29289;&#26469;&#35780;&#20272;&#36816;&#21160;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is a slowly progressive, debilitating neurodegenerative disease which causes motor symptoms including gait dysfunction. Motor fluctuations are alterations between periods with a positive response to levodopa therapy ("on") and periods marked by re-emergency of PD symptoms ("off") as the response to medication wears off. These fluctuations often affect gait speed and they increase in their disabling impact as PD progresses. To improve the effectiveness of current indoor localisation methods, a transformer-based approach utilising dual modalities which provide complementary views of movement, Received Signal Strength Indicator (RSSI) and accelerometer data from wearable devices, is proposed. A sub-objective aims to evaluate whether indoor localisation, including its in-home gait speed features (i.e. the time taken to walk between rooms), could be used to evaluate motor fluctuations by detecting whether the person with PD is taking levodopa medications or withhold
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#29983;&#29289;&#20256;&#24863;&#22120;&#65292;&#36890;&#36807;&#20998;&#26512;&#20809;&#30005;&#23481;&#31215;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#26469;&#35780;&#20272;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2308.02415</link><description>&lt;p&gt;
&#36890;&#36807;1D&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#35780;&#20272;&#39550;&#39542;&#21592;&#30340;&#30130;&#21171;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Car-Driver Drowsiness Assessment through 1D Temporal Convolutional Networks. (arXiv:2308.02415v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02415
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#29983;&#29289;&#20256;&#24863;&#22120;&#65292;&#36890;&#36807;&#20998;&#26512;&#20809;&#30005;&#23481;&#31215;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#26469;&#35780;&#20272;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65288;ADAS&#65289;&#30340;&#31185;&#23398;&#36827;&#23637;&#22312;&#25552;&#39640;&#39550;&#39542;&#30340;&#25972;&#20307;&#23433;&#20840;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;ADAS&#25216;&#26415;&#20351;&#24471;&#33021;&#22815;&#20027;&#21160;&#25511;&#21046;&#36710;&#36742;&#20197;&#39044;&#38450;&#28508;&#22312;&#30340;&#21361;&#38505;&#24773;&#20917;&#12290;&#30740;&#31350;&#20154;&#21592;&#20851;&#27880;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#27700;&#24179;&#30340;&#20998;&#26512;&#65292;&#22240;&#20026;&#26368;&#36817;&#30340;&#25253;&#21578;&#35777;&#23454;&#20102;&#30001;&#30130;&#21171;&#25110;&#32570;&#20047;&#27880;&#24847;&#21147;&#23548;&#33268;&#30340;&#20107;&#25925;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21508;&#31181;&#30740;&#31350;&#24314;&#35758;&#30417;&#27979;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#29366;&#24577;&#65292;&#22240;&#20026;&#33258;&#20027;&#31070;&#32463;&#31995;&#32479;&#65288;ANS&#65289;&#19982;&#27880;&#24847;&#21147;&#27700;&#24179;&#20043;&#38388;&#23384;&#22312;&#30528;&#24456;&#22909;&#30340;&#32852;&#31995;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#29983;&#29289;&#20256;&#24863;&#22120;&#65292;&#21253;&#25324;&#36817;&#32418;&#22806;LED&#21457;&#23556;&#22120;&#21644;&#20809;&#30005;&#20256;&#24863;&#22120;&#65292;&#29305;&#21035;&#26159;&#30789;&#20809;&#20493;&#22686;&#22120;&#35013;&#32622;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;&#30456;&#20851;&#30340;&#20809;&#30005;&#23481;&#31215;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#26469;&#35780;&#20272;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the scientific progress of Advanced Driver Assistance System solutions (ADAS) has played a key role in enhancing the overall safety of driving. ADAS technology enables active control of vehicles to prevent potentially risky situations. An important aspect that researchers have focused on is the analysis of the driver attention level, as recent reports confirmed a rising number of accidents caused by drowsiness or lack of attentiveness. To address this issue, various studies have suggested monitoring the driver physiological state, as there exists a well-established connection between the Autonomic Nervous System (ANS) and the level of attention. For our study, we designed an innovative bio-sensor comprising near-infrared LED emitters and photo-detectors, specifically a Silicon PhotoMultiplier device. This allowed us to assess the driver physiological status by analyzing the associated PhotoPlethysmography (PPG) signal.Furthermore, we developed an embedded time-domain hyper-fi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22522;&#20110;WiFi CSI&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#36827;&#34892;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;CSI&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#19987;&#23478;&#30693;&#35782;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#23545;&#32570;&#20047;&#26631;&#35760;CSI&#25968;&#25454;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25972;&#29702;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.02412</link><description>&lt;p&gt;
&#22522;&#20110;WiFi CSI&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65306;&#31995;&#32479;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study. (arXiv:2308.02412v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22522;&#20110;WiFi CSI&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#36827;&#34892;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;CSI&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#19987;&#23478;&#30693;&#35782;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#23545;&#32570;&#20047;&#26631;&#35760;CSI&#25968;&#25454;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25972;&#29702;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;WiFi CSI&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;(HAR)&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#22522;&#20110;CSI&#30340;HAR&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#19981;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;CSI&#30340;HAR&#30340;&#32972;&#26223;&#19979;&#65292;&#26631;&#35760;CSI&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#20173;&#28982;&#26159;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#26368;&#31361;&#20986;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#30001;&#20110;CSI&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#21644;&#19981;&#21487;&#29702;&#35299;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#37325;&#35270;&#26631;&#35760;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#21033;&#29992;SSL&#31639;&#27861;&#26469;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;SSL&#31639;&#27861;&#30340;&#28508;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25972;&#29702;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#20197;&#21069;&#30740;&#31350;&#36807;&#30340;&#31639;&#27861;&#21644;&#37027;&#20123;&#36824;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, with the advancement of the Internet of Things (IoT), WiFi CSI-based HAR has gained increasing attention from academic and industry communities. By integrating the deep learning technology with CSI-based HAR, researchers achieve state-of-the-art performance without the need of expert knowledge. However, the scarcity of labeled CSI data remains the most prominent challenge when applying deep learning models in the context of CSI-based HAR due to the privacy and incomprehensibility of CSI-based HAR data. On the other hand, SSL has emerged as a promising approach for learning meaningful representations from data without heavy reliance on labeled examples. Therefore, considerable efforts have been made to address the challenge of insufficient data in deep learning by leveraging SSL algorithms. In this paper, we undertake a comprehensive inventory and analysis of the potential held by different categories of SSL algorithms, including those that have been previously studied and tho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33041;&#30005;&#20449;&#21495;&#23545;&#24515;&#29702;&#36127;&#33655;&#36827;&#34892;&#20998;&#31867;&#21644;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#22312;&#26102;&#38388;&#22495;&#20013;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#22810;&#32500;&#27531;&#24046;&#22359;&#12290;</title><link>http://arxiv.org/abs/2308.02409</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#22810;&#31354;&#38388;&#28145;&#24230;&#27169;&#22411;&#30340;&#33041;&#30005;&#20449;&#21495;&#26469;&#20272;&#35745;&#24515;&#29702;&#36127;&#33655;
&lt;/p&gt;
&lt;p&gt;
Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models. (arXiv:2308.02409v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33041;&#30005;&#20449;&#21495;&#23545;&#24515;&#29702;&#36127;&#33655;&#36827;&#34892;&#20998;&#31867;&#21644;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#22312;&#26102;&#38388;&#22495;&#20013;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#22810;&#32500;&#27531;&#24046;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33041;&#22312;&#24037;&#20316;&#21644;&#20241;&#24687;&#26102;&#37117;&#22788;&#20110;&#25345;&#32493;&#27963;&#21160;&#30340;&#29366;&#24577;&#12290;&#24515;&#29702;&#27963;&#21160;&#26159;&#26085;&#24120;&#36807;&#31243;&#20013;&#30340;&#19968;&#37096;&#20998;&#65292;&#24403;&#22823;&#33041;&#36807;&#24230;&#21171;&#32047;&#26102;&#65292;&#20250;&#23545;&#20154;&#20307;&#20581;&#24247;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20110;&#26089;&#26399;&#26816;&#27979;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#37325;&#35270;&#36880;&#28176;&#22686;&#21152;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#24110;&#21161;&#39044;&#38450;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#29983;&#27963;&#36136;&#37327;&#12290;&#22810;&#31181;&#20449;&#21495;&#34987;&#29992;&#20110;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#65292;&#20294;&#30001;&#20110;&#22823;&#37327;&#25552;&#20379;&#20851;&#20110;&#22823;&#33041;&#20449;&#24687;&#30340;&#29305;&#28857;&#65292;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#34987;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#24515;&#29702;&#36127;&#33655;&#20998;&#20026;&#19977;&#31181;&#29366;&#24577;&#24182;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#24515;&#29702;&#20272;&#35745;&#32467;&#26524;&#12290;&#22312;&#26102;&#38388;&#22495;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#32500;&#27531;&#24046;&#22359;&#30340;&#26032;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#27531;&#24046;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#25506;&#32034;&#20102;&#22312;&#33041;&#30005;&#22270;&#35299;&#30721;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#28145;&#24230;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#65292;&#20063;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#32431;&#30417;&#30563;&#26041;&#27861;&#65292;&#25913;&#36827;&#24133;&#24230;&#39640;&#36798;28%&#12290;</title><link>http://arxiv.org/abs/2308.02408</link><description>&lt;p&gt;
&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#35780;&#20272;&#35748;&#30693;&#20219;&#21153;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Evaluating the structure of cognitive tasks with transfer learning. (arXiv:2308.02408v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#25506;&#32034;&#20102;&#22312;&#33041;&#30005;&#22270;&#35299;&#30721;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#28145;&#24230;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#65292;&#20063;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#32431;&#30417;&#30563;&#26041;&#27861;&#65292;&#25913;&#36827;&#24133;&#24230;&#39640;&#36798;28%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#65292;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35299;&#30721;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#36801;&#31227;&#23398;&#20064;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#20294;&#23427;&#20551;&#35774;&#21487;&#20256;&#36882;&#30340;&#25968;&#25454;&#39046;&#22495;&#21644;&#20219;&#21153;&#26159;&#24050;&#30693;&#30340;&#65292;&#32780;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#24182;&#38750;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;EEG&#35299;&#30721;&#20219;&#21153;&#20043;&#38388;&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35299;&#30721;&#27169;&#22411;&#23545;&#26368;&#36817;&#21457;&#24067;&#30340;ERP CORE&#21644;M$^3$CV&#20004;&#20010;EEG&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;140&#20010;&#21463;&#35797;&#32773;&#21644;11&#20010;&#19981;&#21516;&#30340;&#35748;&#30693;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#35299;&#30721;&#21518;&#32493;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26469;&#34913;&#37327;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#35299;&#30721;&#24615;&#33021;&#25913;&#36827;&#65292;&#30456;&#36739;&#20110;&#32431;&#30417;&#30563;&#26041;&#27861;&#65292;&#25913;&#36827;&#24133;&#24230;&#39640;&#36798;28%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#35777;&#25454;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) decoding is a challenging task due to the limited availability of labelled data. While transfer learning is a promising technique to address this challenge, it assumes that transferable data domains and task are known, which is not the case in this setting. This study investigates the transferability of deep learning representations between different EEG decoding tasks. We conduct extensive experiments using state-of-the-art decoding models on two recently released EEG datasets, ERP CORE and M$^3$CV, containing over 140 subjects and 11 distinct cognitive tasks. We measure the transferability of learned representations by pre-training deep neural networks on one task and assessing their ability to decode subsequent tasks. Our experiments demonstrate that, even with linear probing transfer, significant improvements in decoding performance can be obtained, with gains of up to 28% compare with the pure supervised approach. Additionally, we discover evidence tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#29983;&#23384;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;FedSurF++&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#22788;&#29702;&#20998;&#24067;&#24335;&#30340;&#12289;&#20855;&#26377;&#38544;&#31169;&#20445;&#23494;&#35201;&#27714;&#30340;&#29983;&#23384;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#24314;&#27169;&#21644;&#22788;&#29702;&#29983;&#23384;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#20445;&#23494;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02382</link><description>&lt;p&gt;
&#12298;&#20351;&#29992;&#32852;&#21512;&#29983;&#23384;&#26862;&#26519;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#25193;&#23637;&#29983;&#23384;&#20998;&#26512;: &#24515;&#21147;&#34928;&#31469;&#21644;&#20083;&#33146;&#30284;&#22522;&#22240;&#32452;&#30340;&#27604;&#36739;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics. (arXiv:2308.02382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#29983;&#23384;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;FedSurF++&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#22788;&#29702;&#20998;&#24067;&#24335;&#30340;&#12289;&#20855;&#26377;&#38544;&#31169;&#20445;&#23494;&#35201;&#27714;&#30340;&#29983;&#23384;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#24314;&#27169;&#21644;&#22788;&#29702;&#29983;&#23384;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#20445;&#23494;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#21307;&#23398;&#20013;&#30340;&#19968;&#31181;&#22522;&#26412;&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#20154;&#32676;&#20013;&#21457;&#29983;&#24863;&#20852;&#36259;&#20107;&#20214;&#30340;&#26102;&#38388;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#29983;&#23384;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#12289;&#34987;&#23457;&#26597;&#12289;&#20998;&#24067;&#24335;&#21644;&#20445;&#23494;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#23494;&#33267;&#20851;&#37325;&#35201;&#30340;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#12290;&#25968;&#25454;&#31232;&#32570;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#23384;&#27169;&#22411;&#22312;&#20381;&#36182;&#22823;&#22411;&#25968;&#25454;&#27744;&#30340;&#20998;&#24067;&#24335;&#24212;&#29992;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#32852;&#21512;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#26395;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#19981;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#30340;&#25216;&#26415;&#65292;&#22240;&#27492;&#29305;&#21035;&#36866;&#21512;&#35299;&#20915;&#29983;&#23384;&#25968;&#25454;&#21644;&#22823;&#35268;&#27169;&#29983;&#23384;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#32852;&#21512;&#23398;&#20064;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#21457;&#23637;&#65292;&#20294;&#22312;&#29983;&#23384;&#20998;&#26512;&#39046;&#22495;&#20173;&#26377;&#35768;&#22810;&#26410;&#25506;&#32034;&#30340;&#26041;&#21521;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#29983;&#23384;&#26862;&#26519;&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#31216;&#20026;FedSurF++&#12290;&#36825;&#31181;&#32852;&#21512;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#23545;&#22823;&#35268;&#27169;&#29983;&#23384;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#24182;&#22788;&#29702;&#20998;&#24067;&#24335;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is a fundamental tool in medicine, modeling the time until an event of interest occurs in a population. However, in real-world applications, survival data are often incomplete, censored, distributed, and confidential, especially in healthcare settings where privacy is critical. The scarcity of data can severely limit the scalability of survival models to distributed applications that rely on large data pools. Federated learning is a promising technique that enables machine learning models to be trained on multiple datasets without compromising user privacy, making it particularly well-suited for addressing the challenges of survival data and large-scale survival applications. Despite significant developments in federated learning for classification and regression, many directions remain unexplored in the context of survival analysis. In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method const
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#25506;&#27979;&#36710;&#25968;&#25454;&#39044;&#27979;&#20132;&#36890;&#20449;&#21495;&#28783;&#23450;&#26102;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;XGBoost&#27169;&#22411;&#20272;&#35745;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30830;&#23450;&#32418;&#28783;&#26102;&#38388;&#65292;&#24182;&#26681;&#25454;&#21608;&#26399;&#38271;&#24230;&#21644;&#32418;&#28783;&#26102;&#38388;&#35745;&#31639;&#32511;&#28783;&#26102;&#38388;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#30340;&#20272;&#35745;&#35823;&#24046;&#23567;&#20110;0.56&#31186;&#65292;&#32418;&#28783;&#26102;&#38388;&#30340;&#39044;&#27979;&#35823;&#24046;&#24179;&#22343;&#22312;7.2&#31186;&#20197;&#20869;&#12290;</title><link>http://arxiv.org/abs/2308.02370</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#25506;&#27979;&#36710;&#25968;&#25454;&#39044;&#27979;&#20132;&#36890;&#20449;&#21495;&#28783;&#23450;&#26102;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data. (arXiv:2308.02370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#25506;&#27979;&#36710;&#25968;&#25454;&#39044;&#27979;&#20132;&#36890;&#20449;&#21495;&#28783;&#23450;&#26102;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;XGBoost&#27169;&#22411;&#20272;&#35745;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30830;&#23450;&#32418;&#28783;&#26102;&#38388;&#65292;&#24182;&#26681;&#25454;&#21608;&#26399;&#38271;&#24230;&#21644;&#32418;&#28783;&#26102;&#38388;&#35745;&#31639;&#32511;&#28783;&#26102;&#38388;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#30340;&#20272;&#35745;&#35823;&#24046;&#23567;&#20110;0.56&#31186;&#65292;&#32418;&#28783;&#26102;&#38388;&#30340;&#39044;&#27979;&#35823;&#24046;&#24179;&#22343;&#22312;7.2&#31186;&#20197;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#28783;&#22312;&#20132;&#36890;&#36816;&#36755;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#33021;&#22815;&#31649;&#29702;&#20132;&#36890;&#27969;&#37327;&#24182;&#30830;&#20445;&#20132;&#21449;&#36335;&#21475;&#30340;&#23433;&#20840;&#12290;&#27492;&#22806;&#65292;&#20102;&#35299;&#20132;&#36890;&#20449;&#21495;&#28783;&#30340;&#30456;&#20301;&#21644;&#23450;&#26102;&#25968;&#25454;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#36710;&#36742;&#34892;&#39542;&#36335;&#32447;&#65292;&#25552;&#39640;&#26102;&#38388;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#36827;&#34892;&#29983;&#24577;&#39550;&#39542;&#20197;&#21450;&#20934;&#30830;&#27169;&#25311;&#26377;&#20449;&#21495;&#30340;&#36947;&#36335;&#32593;&#32476;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#36710;&#36742;&#25506;&#27979;&#25968;&#25454;&#20272;&#35745;&#20132;&#36890;&#20449;&#21495;&#28783;&#23450;&#26102;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#36710;&#36742;&#25506;&#27979;&#25968;&#25454;&#30830;&#23450;&#20132;&#36890;&#20449;&#21495;&#28783;&#23450;&#26102;&#21442;&#25968;&#30340;&#24037;&#20316;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26497;&#38480;&#26799;&#24230;&#22686;&#24378;&#65288;XGBoost&#65289;&#27169;&#22411;&#26469;&#20272;&#35745;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20174;&#25506;&#27979;&#25968;&#25454;&#20013;&#30830;&#23450;&#30456;&#24212;&#30340;&#32418;&#28783;&#26102;&#38388;&#12290;&#28982;&#21518;&#26681;&#25454;&#21608;&#26399;&#38271;&#24230;&#21644;&#32418;&#28783;&#26102;&#38388;&#35745;&#31639;&#32511;&#28783;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20449;&#21495;&#21608;&#26399;&#38271;&#24230;&#30340;&#35823;&#24046;&#23567;&#20110;0.56&#31186;&#65292;&#32418;&#28783;&#26102;&#38388;&#30340;&#39044;&#27979;&#35823;&#24046;&#24179;&#22343;&#22312;7.2&#31186;&#20197;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic signals play an important role in transportation by enabling traffic flow management, and ensuring safety at intersections. In addition, knowing the traffic signal phase and timing data can allow optimal vehicle routing for time and energy efficiency, eco-driving, and the accurate simulation of signalized road networks. In this paper, we present a machine learning (ML) method for estimating traffic signal timing information from vehicle probe data. To the authors best knowledge, very few works have presented ML techniques for determining traffic signal timing parameters from vehicle probe data. In this work, we develop an Extreme Gradient Boosting (XGBoost) model to estimate signal cycle lengths and a neural network model to determine the corresponding red times per phase from probe data. The green times are then be derived from the cycle length and red times. Our results show an error of less than 0.56 sec for cycle length, and red times predictions within 7.2 sec error on ave
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#38450;&#24481;&#24213;&#32441;&#34917;&#19969;&#65288;UDUP&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#20462;&#25913;&#25991;&#26412;&#22270;&#20687;&#30340;&#24213;&#32441;&#32780;&#38750;&#23383;&#31526;&#65292;&#26377;&#25928;&#22320;&#38450;&#24481;&#26410;&#32463;&#25480;&#26435;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#30423;&#29256;&#65292;&#26080;&#35770;&#25130;&#23631;&#33539;&#22260;&#25110;&#22270;&#20687;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#65292;&#37117;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.02369</link><description>&lt;p&gt;
&#36890;&#29992;&#38450;&#24481;&#24213;&#32441;&#34917;&#19969;&#65306;&#20351;&#24744;&#30340;&#25991;&#26412;&#23545;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#21464;&#24471;&#38544;&#24418;
&lt;/p&gt;
&lt;p&gt;
Universal Defensive Underpainting Patch: Making Your Text Invisible to Optical Character Recognition. (arXiv:2308.02369v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#38450;&#24481;&#24213;&#32441;&#34917;&#19969;&#65288;UDUP&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#20462;&#25913;&#25991;&#26412;&#22270;&#20687;&#30340;&#24213;&#32441;&#32780;&#38750;&#23383;&#31526;&#65292;&#26377;&#25928;&#22320;&#38450;&#24481;&#26410;&#32463;&#25480;&#26435;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#30423;&#29256;&#65292;&#26080;&#35770;&#25130;&#23631;&#33539;&#22260;&#25110;&#22270;&#20687;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#65292;&#37117;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#21487;&#20197;&#20174;&#25195;&#25551;&#25110;&#25968;&#23383;&#21270;&#30340;&#25991;&#26412;&#22270;&#20687;&#20013;&#33258;&#21160;&#25552;&#21462;&#25991;&#26412;&#65292;&#20294;&#20063;&#20351;&#24471;&#20174;&#36825;&#20123;&#22270;&#20687;&#20013;&#30423;&#21462;&#26377;&#20215;&#20540;&#25110;&#25935;&#24863;&#30340;&#25991;&#26412;&#21464;&#24471;&#23481;&#26131;&#12290;&#20197;&#24448;&#29992;&#20110;&#38450;&#27490;OCR&#30423;&#29256;&#30340;&#26041;&#27861;&#36890;&#36807;&#25197;&#26354;&#25991;&#26412;&#22270;&#20687;&#20013;&#30340;&#23383;&#31526;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26159;&#19981;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#30423;&#29256;&#32773;&#21487;&#20197;&#25429;&#33719;&#25991;&#26412;&#22270;&#20687;&#30340;&#20219;&#24847;&#37096;&#20998;&#65292;&#20351;&#38450;&#24481;&#25514;&#26045;&#22833;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#31216;&#20026;&#36890;&#29992;&#38450;&#24481;&#24213;&#32441;&#34917;&#19969;&#65288;UDUP&#65289;&#65292;&#23427;&#20462;&#25913;&#25991;&#26412;&#22270;&#20687;&#30340;&#24213;&#32441;&#32780;&#19981;&#26159;&#23383;&#31526;&#12290;UDUP&#26159;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#21019;&#24314;&#30340;&#65292;&#29992;&#20110;&#21046;&#20316;&#19968;&#20010;&#23567;&#22411;&#30340;&#12289;&#22266;&#23450;&#22823;&#23567;&#30340;&#38450;&#24481;&#34917;&#19969;&#65292;&#21487;&#20197;&#20026;&#20219;&#24847;&#22823;&#23567;&#30340;&#25991;&#26412;&#22270;&#20687;&#29983;&#25104;&#38750;&#37325;&#21472;&#30340;&#24213;&#32441;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20219;&#20309;&#25130;&#23631;&#33539;&#22260;&#25110;&#22797;&#26434;&#22270;&#20687;&#32972;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;UDUP&#26377;&#25928;&#22320;&#38450;&#24481;&#26410;&#32463;&#25480;&#26435;&#30340;OCR&#12290;&#23427;&#23545;&#20869;&#23481;&#12289;&#22823;&#23567;&#12289;&#39068;&#33394;&#21644;&#35821;&#35328;&#37117;&#26159;&#26080;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical Character Recognition (OCR) enables automatic text extraction from scanned or digitized text images, but it also makes it easy to pirate valuable or sensitive text from these images. Previous methods to prevent OCR piracy by distorting characters in text images are impractical in real-world scenarios, as pirates can capture arbitrary portions of the text images, rendering the defenses ineffective. In this work, we propose a novel and effective defense mechanism termed the Universal Defensive Underpainting Patch (UDUP) that modifies the underpainting of text images instead of the characters. UDUP is created through an iterative optimization process to craft a small, fixed-size defensive patch that can generate non-overlapping underpainting for text images of any size. Experimental results show that UDUP effectively defends against unauthorized OCR under the setting of any screenshot range or complex image background. It is agnostic to the content, size, colors, and languages of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#24046;&#20998;&#38544;&#31169;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;VFL&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#33539;&#25968;&#21098;&#35009;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#29305;&#24449;&#23884;&#20837;&#30340;&#23610;&#24230;&#21644;&#20998;&#24067;&#26469;&#20248;&#21270;&#20219;&#21153;&#25928;&#29992;&#65292;&#32780;&#19981;&#25439;&#23475;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2308.02362</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#24046;&#20998;&#38544;&#31169;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#19982;&#33258;&#36866;&#24212;&#29305;&#24449;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Flexible Differentially Private Vertical Federated Learning with Adaptive Feature Embeddings. (arXiv:2308.02362v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#24046;&#20998;&#38544;&#31169;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;VFL&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#33539;&#25968;&#21098;&#35009;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#29305;&#24449;&#23884;&#20837;&#30340;&#23610;&#24230;&#21644;&#20998;&#24067;&#26469;&#20248;&#21270;&#20219;&#21153;&#25928;&#29992;&#65292;&#32780;&#19981;&#25439;&#23475;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#38544;&#31169;&#20445;&#25252;&#19981;&#23436;&#21892;&#30340;&#25285;&#24551;&#65292;&#22240;&#20026;&#20849;&#20139;&#30340;&#29305;&#24449;&#23884;&#20837;&#21487;&#33021;&#22312;&#38544;&#31169;&#25915;&#20987;&#19979;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;VFL&#22312;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#19979;&#25968;&#25454;&#38544;&#31169;&#21644;&#20219;&#21153;&#25928;&#29992;&#30446;&#26631;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20004;&#20010;&#30446;&#26631;&#20998;&#35299;&#24182;&#36880;&#27493;&#35299;&#20915;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#20849;&#20139;&#29305;&#24449;&#23884;&#20837;&#24212;&#29992;&#33539;&#25968;&#21098;&#35009;&#65292;&#24471;&#21040;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#23545;&#29305;&#24449;&#23884;&#20837;&#30340;&#23610;&#24230;&#21644;&#20998;&#24067;&#36827;&#34892;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#20197;&#19968;&#31181;&#27880;&#37325;&#20934;&#30830;&#24615;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20248;&#21270;&#20219;&#21153;&#25928;&#29992;&#65292;&#32780;&#19981;&#25439;&#23475;&#24050;&#24314;&#31435;&#30340;DP&#26426;&#21046;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#20855;&#20307;&#21270;&#20026;&#25552;&#20986;&#30340;VFL-AFE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#25239;&#20102;&#20808;&#39564;&#25915;&#20987;&#65292;&#24182;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of vertical federated learning (VFL) has stimulated concerns about the imperfection in privacy protection, as shared feature embeddings may reveal sensitive information under privacy attacks. This paper studies the delicate equilibrium between data privacy and task utility goals of VFL under differential privacy (DP). To address the generality issue of prior arts, this paper advocates a flexible and generic approach that decouples the two goals and addresses them successively. Specifically, we initially derive a rigorous privacy guarantee by applying norm clipping on shared feature embeddings, which is applicable across various datasets and models. Subsequently, we demonstrate that task utility can be optimized via adaptive adjustments on the scale and distribution of feature embeddings in an accuracy-appreciative way, without compromising established DP mechanisms. We concretize our observation into the proposed VFL-AFE framework, which exhibits effectiveness against pri
&lt;/p&gt;</description></item><item><title>Text2KGBench&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#26412;&#20307;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#19971;&#20010;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20107;&#23454;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.02357</link><description>&lt;p&gt;
Text2KGBench&#65306;&#19968;&#31181;&#20174;&#25991;&#26412;&#29983;&#25104;&#26412;&#20307;&#39537;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text. (arXiv:2308.02357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02357
&lt;/p&gt;
&lt;p&gt;
Text2KGBench&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#26412;&#20307;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#19971;&#20010;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20107;&#23454;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#20855;&#26377;&#26032;&#20852;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#25913;&#21892;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;LLM&#21644;&#30693;&#35782;&#22270;&#35889;(KG)&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#65292;LLM&#21487;&#20197;&#29992;&#20110;KG&#30340;&#26500;&#24314;&#25110;&#34917;&#20840;&#65292;&#32780;&#29616;&#26377;&#30340;KG&#21487;&#20197;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20351;LLM&#30340;&#36755;&#20986;&#26356;&#26131;&#35299;&#37322;&#25110;&#36827;&#34892;&#31867;&#33041;&#31526;&#21495;&#21270;&#30340;&#20107;&#23454;&#26816;&#26597;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Text2KGBench&#65292;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#26412;&#20307;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#26412;&#20307;&#21644;&#19968;&#32452;&#21477;&#23376;&#65292;&#20219;&#21153;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20107;&#23454;&#65292;&#21516;&#26102;&#31526;&#21512;&#32473;&#23450;&#30340;&#26412;&#20307;(&#27010;&#24565;&#12289;&#20851;&#31995;&#12289;&#22495;/&#20540;&#33539;&#22260;&#32422;&#26463;)&#24182;&#24544;&#23454;&#20110;&#36755;&#20837;&#21477;&#23376;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;(i)&#20855;&#26377;10&#20010;&#26412;&#20307;&#21644;13,474&#20010;&#21477;&#23376;&#30340;Wikidata-TekGen&#21644;(ii)&#20855;&#26377;19&#20010;&#26412;&#20307;&#21644;4,860&#20010;&#21477;&#23376;&#30340;DBpedia-WebNLG&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19971;&#20010;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20107;&#23454;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in large language models (LLM) and foundation models with emergent capabilities have been shown to improve the performance of many NLP tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs can be used for KG construction or completion while existing KGs can be used for different tasks such as making LLM outputs explainable or fact-checking in Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to evaluate the capabilities of language models to generate KGs from natural language text guided by an ontology. Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19 ontologies and 4,860 sentences. We define seven evaluation metrics to measure fact 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#22270;&#24418;&#22240;&#26524;&#35299;&#37322;&#22120; (DyGRACE)&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#24182;&#21033;&#29992;&#24050;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#21021;&#22987;&#30693;&#35782;&#65292;&#22312;&#21160;&#24577;&#25968;&#25454;&#29615;&#22659;&#20013;&#25628;&#32034;&#26377;&#25928;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#29420;&#31435;&#20110;&#24213;&#23618;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.02353</link><description>&lt;p&gt;
&#36866;&#24212;&#21464;&#21270;&#65306;&#22312;&#21160;&#24577;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#24378;&#38887;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes. (arXiv:2308.02353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02353
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#22270;&#24418;&#22240;&#26524;&#35299;&#37322;&#22120; (DyGRACE)&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#24182;&#21033;&#29992;&#24050;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#21021;&#22987;&#30693;&#35782;&#65292;&#22312;&#21160;&#24577;&#25968;&#25454;&#29615;&#22659;&#20013;&#25628;&#32034;&#26377;&#25928;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#29420;&#31435;&#20110;&#24213;&#23618;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#22270;&#24418;&#22240;&#26524;&#35299;&#37322;&#22120; (GCE) &#26041;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#22270;&#24418;&#22240;&#26524;&#35299;&#37322;&#22120; (DyGRACE)&#12290;&#23427;&#21033;&#29992;&#24050;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#21021;&#22987;&#30693;&#35782;&#65292;&#22312;&#25628;&#32034;&#26377;&#25928;&#30340;&#22240;&#26524;&#35299;&#37322;&#26102;&#36991;&#20813;&#20351;&#29992;&#28508;&#22312;&#36807;&#26102;&#30340;&#20915;&#31574;&#20989;&#25968;&#30340;&#20449;&#24687;&#12290;DyGRACE&#21033;&#29992;&#20004;&#20010;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120; (GAE) &#26469;&#23398;&#20064;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#27599;&#20010;&#31867;&#30340;&#34920;&#31034;&#12290;GAE&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#23567;&#21270;&#21407;&#22987;&#22270;&#24418;&#19982;&#20854;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#30340;&#37325;&#24314;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324; (i) &#36890;&#36807;&#26368;&#22823;&#21270;&#20107;&#23454;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#24314;&#35823;&#24046;&#26469;&#20248;&#21270;&#21442;&#25968;&#21270;&#23494;&#24230;&#20989;&#25968; (&#23454;&#29616;&#20026;&#36923;&#36753;&#22238;&#24402;&#20989;&#25968;) &#65292;&#20197;&#35782;&#21035;&#22240;&#26524;&#35299;&#37322;&#65292;(ii) &#26368;&#23567;&#21270;&#22240;&#26524;&#33258;&#32534;&#30721;&#22120;&#30340;&#35823;&#24046;&#65292;(iii) &#26368;&#22823;&#21270;&#20107;&#23454;&#22270;&#24418;&#19982;&#22240;&#26524;&#22270;&#24418;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36825;&#31181;&#21322;&#30417;&#30563;&#26041;&#27861;&#29420;&#31435;&#20110;&#22522;&#30784;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel semi-supervised Graph Counterfactual Explainer (GCE) methodology, Dynamic GRAph Counterfactual Explainer (DyGRACE). It leverages initial knowledge about the data distribution to search for valid counterfactuals while avoiding using information from potentially outdated decision functions in subsequent time steps. Employing two graph autoencoders (GAEs), DyGRACE learns the representation of each class in a binary classification scenario. The GAEs minimise the reconstruction error between the original graph and its learned representation during training. The method involves (i) optimising a parametric density function (implemented as a logistic regression function) to identify counterfactuals by maximising the factual autoencoder's reconstruction error, (ii) minimising the counterfactual autoencoder's error, and (iii) maximising the similarity between the factual and counterfactual graphs. This semi-supervised approach is independent of an underlying black-box oracle
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#20559;&#24046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02335</link><description>&lt;p&gt;
RAHNet: &#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02335
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#20559;&#24046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26159;&#35768;&#22810;&#23454;&#38469;&#22810;&#23186;&#20307;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#22270;&#21487;&#20197;&#34920;&#31034;&#21508;&#31181;&#22810;&#23186;&#20307;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#31038;&#20132;&#32593;&#32476;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#65292;&#20854;&#20013;&#31867;&#20998;&#24067;&#26159;&#24179;&#34913;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#65292;&#23548;&#33268;&#22312;&#20351;&#29992;GNN&#26102;&#23545;&#22836;&#37096;&#31867;&#21035;&#23384;&#22312;&#20559;&#24046;&#65292;&#19988;&#23545;&#23614;&#37096;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#26032;&#24179;&#34913;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26410;&#33021;&#26126;&#30830;&#24341;&#20837;&#26032;&#30693;&#35782;&#65292;&#24182;&#29306;&#29298;&#20102;&#22836;&#37096;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#65292;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#22120;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22270;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#30456;&#20851;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant grap
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21487;&#25511;&#30340;&#21512;&#20316;&#21019;&#20316;&#20195;&#29702;&#65292;&#25913;&#21892;&#20102;&#28216;&#25103;&#31995;&#32479;&#35774;&#35745;&#20013;&#21512;&#20316;&#21019;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#28216;&#25103;&#65292;&#24182;&#35299;&#20915;&#20102;&#20154;&#31867;&#35774;&#35745;&#24072;&#30340;&#21019;&#36896;&#21147;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02317</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25511;&#30340;&#21512;&#20316;&#21019;&#20316;&#20195;&#29702;&#29992;&#20110;&#28216;&#25103;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Controllable Co-Creative Agent for Game System Design. (arXiv:2308.02317v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02317
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21487;&#25511;&#30340;&#21512;&#20316;&#21019;&#20316;&#20195;&#29702;&#65292;&#25913;&#21892;&#20102;&#28216;&#25103;&#31995;&#32479;&#35774;&#35745;&#20013;&#21512;&#20316;&#21019;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#28216;&#25103;&#65292;&#24182;&#35299;&#20915;&#20102;&#20154;&#31867;&#35774;&#35745;&#24072;&#30340;&#21019;&#36896;&#21147;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28216;&#25103;&#20013;&#65292;&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#28151;&#21512;&#20513;&#35758;&#21512;&#20316;&#65292;&#23545;&#20154;&#31867;&#35774;&#35745;&#24072;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#28216;&#25103;&#29983;&#25104;&#30340;&#21512;&#20316;&#21019;&#20316;&#31995;&#32479;&#36890;&#24120;&#23616;&#38480;&#20110;&#29305;&#23450;&#30340;&#31867;&#22411;&#12289;&#35268;&#21017;&#25110;&#28216;&#25103;&#65292;&#38480;&#21046;&#20102;&#35774;&#35745;&#24072;&#30340;&#21019;&#36896;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#23545;&#28216;&#25103;&#36827;&#34892;&#36275;&#22815;&#25277;&#35937;&#30340;&#24314;&#27169;&#65292;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#28216;&#25103;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#28216;&#25103;&#31995;&#32479;&#21644;&#26426;&#21046;&#30340;&#35774;&#35745;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#21487;&#25511;&#30340;&#21512;&#20316;&#21019;&#20316;&#20195;&#29702;&#26469;&#20849;&#21516;&#35774;&#35745;&#36825;&#20123;&#28216;&#25103;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#31867;&#20284;&#29366;&#24577;&#26426;&#30340;&#32452;&#20214;&#21644;&#36164;&#28304;&#27969;&#30340;&#28216;&#25103;&#27169;&#22411;&#65292;&#19968;&#32452;&#21487;&#25511;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#27169;&#25311;&#28216;&#29609;&#30340;&#35774;&#35745;&#35780;&#20272;&#22120;&#65292;&#20197;&#21450;&#36827;&#21270;&#35774;&#35745;&#24179;&#34913;&#22120;&#21644;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#31995;&#32479;&#33021;&#22815;&#34920;&#36798;&#24191;&#27867;&#30340;&#28216;&#25103;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#26410;&#26469;&#30340;&#21512;&#20316;&#21019;&#20316;&#24212;&#29992;&#25552;&#20379;&#20154;&#31867;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many advancements have been made in procedural content generation for games, and with mixed-initiative co-creativity, have the potential for great benefits to human designers. However, co-creative systems for game generation are typically limited to specific genres, rules, or games, limiting the creativity of the designer. We seek to model games abstractly enough to apply to any genre, focusing on designing game systems and mechanics, and create a controllable, co-creative agent that can collaborate on these designs. We present a model of games using state-machine-like components and resource flows, a set of controllable metrics, a design evaluator simulating playthroughs with these metrics, and an evolutionary design balancer and generator. We find this system to be both able to express a wide range of games and able to be human-controllable for future co-creative applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#21644;&#21487;&#29992;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22238;&#31572;&#20013;&#26377;52%&#38169;&#35823;&#65292;77%&#20887;&#38271;&#65292;&#20294;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;&#20173;&#28982;&#22312;39.34%&#30340;&#24773;&#20917;&#19979;&#34987;&#20351;&#29992;&#32773;&#20559;&#22909;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.02312</link><description>&lt;p&gt;
&#35841;&#22238;&#31572;&#30340;&#26356;&#22909;&#65311;&#23545;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Questions. (arXiv:2308.02312v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#21644;&#21487;&#29992;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22238;&#31572;&#20013;&#26377;52%&#38169;&#35823;&#65292;77%&#20887;&#38271;&#65292;&#20294;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;&#20173;&#28982;&#22312;39.34%&#30340;&#24773;&#20917;&#19979;&#34987;&#20351;&#29992;&#32773;&#20559;&#22909;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Q&amp;A&#24179;&#21488;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#19968;&#30452;&#26159;&#31243;&#24207;&#21592;&#32593;&#19978;&#27714;&#21161;&#34892;&#20026;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;&#32593;&#19978;&#27714;&#21161;&#34892;&#20026;&#30340;&#33539;&#24335;&#27491;&#22312;&#21457;&#29983;&#21464;&#21270;&#12290;&#23613;&#31649;ChatGPT&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23578;&#26410;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#26469;&#35780;&#20272;ChatGPT&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#25110;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;ChatGPT&#22238;&#31572;517&#20010;Stack Overflow&#65288;SO&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#23545;ChatGPT&#22238;&#31572;&#30340;&#27491;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#32508;&#21512;&#24615;&#21644;&#31616;&#27905;&#24615;&#36827;&#34892;&#20102;&#26816;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#20998;&#26512;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;ChatGPT&#22238;&#31572;&#22312;&#35821;&#35328;&#21644;&#20154;&#31867;&#26041;&#38754;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;52&#65285;&#30340;ChatGPT&#22238;&#31572;&#26159;&#38169;&#35823;&#30340;&#65292;77&#65285;&#30340;&#22238;&#31572;&#20887;&#38271;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;ChatGPT&#22238;&#31572;&#20173;&#28982;&#22312;39.34&#65285;&#30340;&#24773;&#20917;&#19979;&#21463;&#21040;&#38738;&#30544;&#12290;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A platforms have been an integral part of the web-help-seeking behavior of programmers over the past decade. However, with the recent introduction of ChatGPT, the paradigm of web-help-seeking behavior is experiencing a shift. Despite the popularity of ChatGPT, no comprehensive study has been conducted to evaluate the characteristics or usability of ChatGPT's answers to software engineering questions. To bridge the gap, we conducted the first in-depth analysis of ChatGPT's answers to 517 Stack Overflow (SO) questions and examined the correctness, consistency, comprehensiveness, and conciseness of ChatGPT's answers. Furthermore, we conducted a large-scale linguistic analysis, and a user study to understand the characteristics of ChatGPT answers from linguistic and human aspects. Our analysis shows that 52\% of ChatGPT answers are incorrect and 77\% are verbose. Nonetheless, ChatGPT answers are still preferred 39.34\% of the time due to their comprehensiveness and well-articulated langu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DHS-ConvQA&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20250;&#35805;&#24335;&#38382;&#31572;&#20013;&#21160;&#24577;&#36873;&#25321;&#30456;&#20851;&#30340;&#21382;&#21490;&#36716;&#25240;&#28857;&#65292;&#20197;&#25351;&#23548;&#31572;&#26696;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02294</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#20250;&#35805;&#38382;&#31572;&#20013;&#30456;&#20851;&#30340;&#21382;&#21490;&#23545;&#35805;&#36716;&#25240;&#28857;
&lt;/p&gt;
&lt;p&gt;
Learning to Select the Relevant History Turns in Conversational Question Answering. (arXiv:2308.02294v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02294
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DHS-ConvQA&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20250;&#35805;&#24335;&#38382;&#31572;&#20013;&#21160;&#24577;&#36873;&#25321;&#30456;&#20851;&#30340;&#21382;&#21490;&#36716;&#25240;&#28857;&#65292;&#20197;&#25351;&#23548;&#31572;&#26696;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#32593;&#32476;&#30340;&#25968;&#23383;&#21161;&#25163;&#30340;&#26085;&#30410;&#38656;&#27714;&#65292;&#24341;&#36215;&#20102;&#20449;&#24687;&#26816;&#32034;(IR)&#31038;&#21306;&#23545;&#20250;&#35805;&#24335;&#38382;&#31572;(ConvQA)&#39046;&#22495;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;ConvQA&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#26377;&#25928;&#36873;&#25321;&#20250;&#35805;&#21382;&#21490;&#36716;&#25240;&#28857;&#20197;&#22238;&#31572;&#24403;&#21069;&#38382;&#39064;&#12290;&#30456;&#20851;&#21382;&#21490;&#36873;&#25321;&#19982;&#27491;&#30830;&#31572;&#26696;&#39044;&#27979;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26159;&#19968;&#20010;&#26377;&#36259;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#36873;&#25321;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#21487;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#31995;&#32479;&#22312;&#25991;&#31456;&#20013;&#23547;&#25214;&#31572;&#26696;&#30340;&#30830;&#20999;&#20301;&#32622;&#12290;&#32780;&#19981;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21017;&#32473;&#31995;&#32479;&#24102;&#26469;&#22122;&#38899;&#65292;&#20174;&#32780;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;DHS-ConvQA&#65288;&#20250;&#35805;&#38382;&#31572;&#20013;&#30340;&#21160;&#24577;&#21382;&#21490;&#36873;&#25321;&#65289;&#65292;&#39318;&#20808;&#20026;&#25152;&#26377;&#21382;&#21490;&#36716;&#25240;&#28857;&#29983;&#25104;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#23454;&#20307;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#38382;&#39064;&#30340;&#30456;&#20284;&#24230;&#36827;&#34892;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing demand for the web-based digital assistants has given a rapid rise in the interest of the Information Retrieval (IR) community towards the field of conversational question answering (ConvQA). However, one of the critical aspects of ConvQA is the effective selection of conversational history turns to answer the question at hand. The dependency between relevant history selection and correct answer prediction is an intriguing but under-explored area. The selected relevant context can better guide the system so as to where exactly in the passage to look for an answer. Irrelevant context, on the other hand, brings noise to the system, thereby resulting in a decline in the model's performance. In this paper, we propose a framework, DHS-ConvQA (Dynamic History Selection in Conversational Question Answering), that first generates the context and question entities for all the history turns, which are then pruned on the basis of similarity they share in common with the question at
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02293</link><description>&lt;p&gt;
&#29992;&#27491;&#21017;&#21270;&#39640;&#38454;&#24635;&#21464;&#24046;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation. (arXiv:2308.02293v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02293
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#39640;&#24230;&#34920;&#36798;&#30340;&#21442;&#25968;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#22797;&#26434;&#27010;&#24565;&#65292;&#20294;&#35757;&#32451;&#36825;&#31181;&#39640;&#24230;&#38750;&#32447;&#24615;&#27169;&#22411;&#24050;&#30693;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;k&#38454;&#24635;&#21464;&#24046;&#65288;k-TV&#65289;&#27491;&#21017;&#21270;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#35201;&#35757;&#32451;&#30340;&#21442;&#25968;&#27169;&#22411;&#30340;k&#38454;&#23548;&#25968;&#30340;&#24179;&#26041;&#31215;&#20998;&#65292;&#36890;&#36807;&#24809;&#32602;k-TV&#26469;&#20135;&#29983;&#19968;&#20010;&#26356;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#23613;&#31649;&#23558;k-TV&#39033;&#24212;&#29992;&#20110;&#19968;&#33324;&#30340;&#21442;&#25968;&#27169;&#22411;&#30001;&#20110;&#31215;&#20998;&#32780;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#24102;&#26377;k-TV&#27491;&#21017;&#21270;&#30340;&#19968;&#33324;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26174;&#24335;&#30340;&#25968;&#20540;&#31215;&#20998;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#32467;&#26500;&#20219;&#24847;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#21363;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $k$th order total variation ($k$-TV) regularization, which is defined as the squared integral of the $k$th order derivative of the parametric models to be trained; penalizing the $k$-TV is expected to yield a smoother function, which is expected to avoid overfitting. While the $k$-TV terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $k$-TV regularization without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradien
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#25216;&#26415;&#65288;DuRM&#65289;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#26799;&#24230;&#30340;&#26041;&#24046;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02287</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#23454;&#29616;&#20196;&#20154;&#27822;&#20007;&#30340;&#27169;&#22411;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Model Generalization by Dummy Risk Minimization. (arXiv:2308.02287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02287
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#25216;&#26415;&#65288;DuRM&#65289;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#26799;&#24230;&#30340;&#26041;&#24046;&#26469;&#20419;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#34394;&#25311;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;DuRM&#65289;&#65292;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#21644;&#36890;&#29992;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;ERM&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;DuRM&#38750;&#24120;&#31616;&#21333;&#23454;&#29616;&#65306;&#21482;&#38656;&#25193;&#22823;&#36755;&#20986;logits&#30340;&#32500;&#24230;&#65292;&#28982;&#21518;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#39564;&#35777;DuRM&#30340;&#26377;&#25928;&#24615;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DuRM&#23548;&#33268;&#26356;&#22823;&#30340;&#26799;&#24230;&#26041;&#24046;&#65292;&#36890;&#36807;&#35266;&#23519;&#26356;&#22909;&#30340;&#24179;&#22374;&#23616;&#37096;&#26368;&#23567;&#20540;&#20419;&#36827;&#27169;&#22411;&#27867;&#21270;&#12290;&#20174;&#32463;&#39564;&#19978;&#35762;&#65292;&#25105;&#20204;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#27169;&#24577;&#21644;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;DuRM&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#20256;&#32479;&#20998;&#31867;&#65292;&#35821;&#20041;&#20998;&#21106;&#65292;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#23545;&#25239;&#35757;&#32451;&#21644;&#38271;&#23614;&#35782;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DuRM&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical risk minimization (ERM) is a fundamental machine learning paradigm. However, its generalization ability is limited in various tasks. In this paper, we devise Dummy Risk Minimization (DuRM), a frustratingly easy and general technique to improve the generalization of ERM. DuRM is extremely simple to implement: just enlarging the dimension of the output logits and then optimizing using standard gradient descent. Moreover, we validate the efficacy of DuRM on both theoretical and empirical analysis. Theoretically, we show that DuRM derives greater variance of the gradient, which facilitates model generalization by observing better flat local minima. Empirically, we conduct evaluations of DuRM across different datasets, modalities, and network architectures on diverse tasks, including conventional classification, semantic segmentation, out-of-distribution generalization, adverserial training, and long-tailed recognition. Results demonstrate that DuRM could consistently improve the 
&lt;/p&gt;</description></item><item><title>DIVERSIFY&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#31163;&#32676;&#26816;&#27979;&#21644;&#25512;&#24191;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#23376;&#22495;&#26469;&#23545;&#25239;&#26102;&#38388;&#24207;&#21015;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#20943;&#23567;&#28508;&#22312;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.02282</link><description>&lt;p&gt;
DIVERSIFY: &#19968;&#33324;&#21270;&#26102;&#38388;&#24207;&#21015;&#31163;&#32676;&#26816;&#27979;&#21644;&#25512;&#24191;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization. (arXiv:2308.02282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02282
&lt;/p&gt;
&lt;p&gt;
DIVERSIFY&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#31163;&#32676;&#26816;&#27979;&#21644;&#25512;&#24191;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#23376;&#22495;&#26469;&#23545;&#25239;&#26102;&#38388;&#24207;&#21015;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#20943;&#23567;&#28508;&#22312;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#27169;&#24577;&#20043;&#19968;&#12290;&#30001;&#20110;&#20854;&#38750;&#24179;&#31283;&#24615;&#36136;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#31163;&#32676;&#26816;&#27979;&#21644;&#25512;&#24191;&#24448;&#24448;&#21463;&#21040;&#22256;&#25200;&#65292;&#21363;&#20998;&#24067;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21160;&#24577;&#20998;&#24067;&#32473;&#29616;&#26377;&#31639;&#27861;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#39046;&#22495;&#20449;&#24687;&#24050;&#30693;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#23376;&#22495;&#26469;&#23545;&#25239;&#38750;&#24179;&#31283;&#24615;&#24341;&#21457;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#24191;&#20041;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DIVERSIFY&#65292;&#19968;&#20010;&#38024;&#23545;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#20998;&#24067;&#30340;&#31163;&#32676;&#26816;&#27979;&#21644;&#25512;&#24191;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;DIVERSIFY&#37319;&#29992;&#36845;&#20195;&#36807;&#31243;&#65306;&#39318;&#20808;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#33719;&#24471;&#8220;&#26368;&#22351;&#24773;&#20917;&#8221;&#28508;&#22312;&#20998;&#24067;&#24773;&#26223;&#65292;&#28982;&#21518;&#20943;&#23567;&#36825;&#20123;&#28508;&#22312;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#29616;&#26377;&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#23454;&#29616;&#20102;DIVERSIFY&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series remains one of the most challenging modalities in machine learning research. The out-of-distribution (OOD) detection and generalization on time series tend to suffer due to its non-stationary property, i.e., the distribution changes over time. The dynamic distributions inside time series pose great challenges to existing algorithms to identify invariant distributions since they mainly focus on the scenario where the domain information is given as prior knowledge. In this paper, we attempt to exploit subdomains within a whole dataset to counteract issues induced by non-stationary for generalized representation learning. We propose DIVERSIFY, a general framework, for OOD detection and generalization on dynamic distributions of time series. DIVERSIFY takes an iterative process: it first obtains the "worst-case" latent distribution scenario via adversarial training, then reduces the gap between these latent distributions. We implement DIVERSIFY via combining existing OOD detect
&lt;/p&gt;</description></item><item><title>DTF-Net&#26159;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#31070;&#32463;&#22330;&#30340;&#23039;&#24577;&#20272;&#35745;&#21644;&#24418;&#29366;&#37325;&#24314;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#21487;&#21464;&#24418;&#27169;&#26495;&#22330;&#26469;&#25429;&#25417;&#29289;&#20307;&#31867;&#21035;&#30340;&#24418;&#29366;&#29305;&#24449;&#21644;&#20960;&#20309;&#21464;&#24418;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.02239</link><description>&lt;p&gt;
DTF-Net: &#22522;&#20110;&#21487;&#21464;&#24418;&#27169;&#26495;&#22330;&#30340;&#29289;&#20307;&#31867;&#21035;&#32423;&#23039;&#24577;&#20272;&#35745;&#21644;&#24418;&#29366;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
DTF-Net: Category-Level Pose Estimation and Shape Reconstruction via Deformable Template Field. (arXiv:2308.02239v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02239
&lt;/p&gt;
&lt;p&gt;
DTF-Net&#26159;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#31070;&#32463;&#22330;&#30340;&#23039;&#24577;&#20272;&#35745;&#21644;&#24418;&#29366;&#37325;&#24314;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#21487;&#21464;&#24418;&#27169;&#26495;&#22330;&#26469;&#25429;&#25417;&#29289;&#20307;&#31867;&#21035;&#30340;&#24418;&#29366;&#29305;&#24449;&#21644;&#20960;&#20309;&#21464;&#24418;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;RGB-&#28145;&#24230;&#22270;&#20687;&#23545;&#20013;&#20272;&#35745;&#29289;&#20307;&#30340;&#20845;&#32500;&#23039;&#24577;&#21644;&#37325;&#24314;&#19977;&#32500;&#24418;&#29366;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#23398;&#20064;&#19982;&#29305;&#23450;&#27169;&#26495;&#23545;&#24212;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#32780;&#24573;&#35270;&#20102;&#21516;&#19968;&#31867;&#21035;&#20013;&#29289;&#20307;&#30340;&#24418;&#29366;&#21464;&#21270;&#21644;&#23039;&#24577;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#23454;&#20363;&#26102;&#24615;&#33021;&#19981;&#20339;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#24402;&#19968;&#21270;&#30340;&#20960;&#20309;&#32467;&#26500;&#20808;&#39564;&#26469;&#23454;&#29616;&#31867;&#21035;&#32423;&#20272;&#35745;&#21644;&#37325;&#24314;&#65292;&#20294;&#22522;&#20110;&#38745;&#24577;&#20808;&#39564;&#30340;&#37325;&#24314;&#22312;&#31867;&#20869;&#21464;&#21270;&#22823;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DTF-Net&#65292;&#19968;&#31181;&#22522;&#20110;&#29289;&#20307;&#31867;&#21035;&#30340;&#38544;&#24335;&#31070;&#32463;&#22330;&#30340;&#23039;&#24577;&#20272;&#35745;&#21644;&#24418;&#29366;&#37325;&#24314;&#26032;&#26694;&#26550;&#12290;&#22312;DTF-Net&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#21464;&#24418;&#27169;&#26495;&#22330;&#65292;&#29992;&#20110;&#34920;&#31034;&#36890;&#29992;&#30340;&#31867;&#21035;&#24418;&#29366;&#28508;&#22312;&#29305;&#24449;&#21644;&#31867;&#20869;&#20960;&#20309;&#21464;&#24418;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating 6D poses and reconstructing 3D shapes of objects in open-world scenes from RGB-depth image pairs is challenging. Many existing methods rely on learning geometric features that correspond to specific templates while disregarding shape variations and pose differences among objects in the same category. As a result, these methods underperform when handling unseen object instances in complex environments. In contrast, other approaches aim to achieve category-level estimation and reconstruction by leveraging normalized geometric structure priors, but the static prior-based reconstruction struggles with substantial intra-class variations. To solve these problems, we propose the DTF-Net, a novel framework for pose estimation and shape reconstruction based on implicit neural fields of object categories. In DTF-Net, we design a deformable template field to represent the general category-wise shape latent features and intra-category geometric deformation features. The field establishe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25351;&#20986;&#22825;&#30495;&#30340;&#32593;&#32476;&#25235;&#21462;&#31243;&#24207;&#21487;&#33021;&#23548;&#33268;&#25910;&#38598;&#25968;&#25454;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#65292;&#24182;&#25551;&#36848;&#20102;&#26469;&#28304;&#20110;&#32593;&#32476;&#20869;&#23481;&#26131;&#21464;&#24615;&#12289;&#20010;&#24615;&#21270;&#21644;&#26410;&#32034;&#24341;&#30340;&#25277;&#26679;&#20559;&#24046;&#12290;&#36890;&#36807;&#20363;&#23376;&#35828;&#26126;&#20102;&#25277;&#26679;&#20559;&#24046;&#30340;&#26222;&#36941;&#24615;&#21644;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#20811;&#26381;&#25277;&#26679;&#20559;&#24046;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.02231</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#30456;&#20449;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Should we trust web-scraped data?. (arXiv:2308.02231v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25351;&#20986;&#22825;&#30495;&#30340;&#32593;&#32476;&#25235;&#21462;&#31243;&#24207;&#21487;&#33021;&#23548;&#33268;&#25910;&#38598;&#25968;&#25454;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#65292;&#24182;&#25551;&#36848;&#20102;&#26469;&#28304;&#20110;&#32593;&#32476;&#20869;&#23481;&#26131;&#21464;&#24615;&#12289;&#20010;&#24615;&#21270;&#21644;&#26410;&#32034;&#24341;&#30340;&#25277;&#26679;&#20559;&#24046;&#12290;&#36890;&#36807;&#20363;&#23376;&#35828;&#26126;&#20102;&#25277;&#26679;&#20559;&#24046;&#30340;&#26222;&#36941;&#24615;&#21644;&#31243;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#20811;&#26381;&#25277;&#26679;&#20559;&#24046;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#35777;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23548;&#33268;&#20102;&#23545;&#19968;&#31181;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#30340;&#24191;&#27867;&#20351;&#29992;&#65306;&#32593;&#32476;&#25235;&#21462;&#12290;&#32593;&#32476;&#25235;&#21462;&#25351;&#30340;&#26159;&#20351;&#29992;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#31243;&#24207;&#35775;&#38382;&#32593;&#31449;&#24182;&#19979;&#36733;&#20854;&#20869;&#23481;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#35770;&#28857;&#26159;&#65292;&#22825;&#30495;&#30340;&#32593;&#32476;&#25235;&#21462;&#31243;&#24207;&#21487;&#33021;&#20250;&#23548;&#33268;&#25910;&#38598;&#25968;&#25454;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#20013;&#30340;&#19977;&#31181;&#25277;&#26679;&#20559;&#24046;&#26469;&#28304;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25277;&#26679;&#20559;&#24046;&#28304;&#20110;&#32593;&#32476;&#20869;&#23481;&#30340;&#26131;&#21464;&#24615;&#65288;&#21363;&#21487;&#33021;&#21457;&#29983;&#21464;&#21270;&#65289;&#12289;&#20010;&#24615;&#21270;&#65288;&#21363;&#26681;&#25454;&#35831;&#27714;&#29305;&#24449;&#21576;&#29616;&#65289;&#21644;&#26410;&#32034;&#24341;&#65288;&#21363;&#20154;&#21475;&#30331;&#35760;&#31807;&#30340;&#20016;&#23500;&#24615;&#65289;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20363;&#23376;&#65292;&#25105;&#35828;&#26126;&#20102;&#25277;&#26679;&#20559;&#24046;&#30340;&#26222;&#36941;&#24615;&#21644;&#31243;&#24230;&#12290;&#20026;&#20102;&#25903;&#25345;&#30740;&#31350;&#20154;&#21592;&#21644;&#23457;&#31295;&#20154;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#23545;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#30340;&#25277;&#26679;&#20559;&#24046;&#36827;&#34892;&#39044;&#26399;&#12289;&#26816;&#27979;&#21644;&#20811;&#26381;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing adoption of econometric and machine-learning approaches by empirical researchers has led to a widespread use of one data collection method: web scraping. Web scraping refers to the use of automated computer programs to access websites and download their content. The key argument of this paper is that na\"ive web scraping procedures can lead to sampling bias in the collected data. This article describes three sources of sampling bias in web-scraped data. More specifically, sampling bias emerges from web content being volatile (i.e., being subject to change), personalized (i.e., presented in response to request characteristics), and unindexed (i.e., abundance of a population register). In a series of examples, I illustrate the prevalence and magnitude of sampling bias. To support researchers and reviewers, this paper provides recommendations on anticipating, detecting, and overcoming sampling bias in web-scraped data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25216;&#26415;&#22522;&#30784;&#21644;&#28508;&#22312;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#37319;&#29992;&#31574;&#30053;&#26694;&#26550;&#65292;&#24182;&#25351;&#20986;&#32852;&#37030;&#23398;&#20064;&#20026;&#21830;&#19994;&#21644;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#23398;&#30028;&#25552;&#20379;&#20102;&#36328;&#23398;&#31185;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2308.02219</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65306;&#26426;&#26500;&#26426;&#36935;&#12289;&#25361;&#25112;&#21644;&#37319;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Federated Learning: Organizational Opportunities, Challenges, and Adoption Strategies. (arXiv:2308.02219v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25216;&#26415;&#22522;&#30784;&#21644;&#28508;&#22312;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#37319;&#29992;&#31574;&#30053;&#26694;&#26550;&#65292;&#24182;&#25351;&#20986;&#32852;&#37030;&#23398;&#20064;&#20026;&#21830;&#19994;&#21644;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#23398;&#30028;&#25552;&#20379;&#20102;&#36328;&#23398;&#31185;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#65292;&#25968;&#25454;&#20849;&#20139;&#30340;&#38480;&#21046;&#24615;&#35268;&#21017;&#23548;&#33268;&#32852;&#37030;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20801;&#35768;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#21512;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#19982;&#20182;&#20154;&#20849;&#20139;&#20854;&#21508;&#33258;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#39318;&#20808;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25216;&#26415;&#22522;&#30784;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#23558;&#32452;&#32455;&#25353;&#29031;&#20854;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#21644;&#29615;&#22659;&#36827;&#34892;&#20102;&#26144;&#23556;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#19981;&#21516;&#34892;&#19994;&#30340;&#20856;&#22411;&#32452;&#32455;&#65292;&#21253;&#25324;&#34892;&#19994;&#32852;&#30431;&#12289;&#24314;&#31435;&#38134;&#34892;&#12289;&#20844;&#20849;&#26426;&#26500;&#21644;&#25968;&#25454;&#23494;&#38598;&#22411;&#20013;&#23567;&#20225;&#19994;&#21487;&#33021;&#32771;&#34385;&#19981;&#21516;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#32852;&#37030;&#23398;&#20064;&#20026;&#21830;&#19994;&#21644;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#23398;&#30028;&#25552;&#20379;&#20102;&#20805;&#28385;&#36328;&#23398;&#31185;&#30740;&#31350;&#26426;&#20250;&#30340;&#26426;&#26500;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restrictive rules for data sharing in many industries have led to the development of \ac{FL}. \ac{FL} is a \ac{ML} technique that allows distributed clients to train models collaboratively without the need to share their respective training data with others. In this article, we first explore the technical basics of FL and its potential applications. Second, we present a conceptual framework for the adoption of \ac{FL}, mapping organizations along the lines of their \ac{AI} capabilities and environment. We then discuss why exemplary organizations in different industries, including industry consortia, established banks, public authorities, and data-intensive SMEs might consider different approaches to \ac{FL}. To conclude, we argue that \ac{FL} presents an institutional shift with ample interdisciplinary research opportunities for the business and information systems engineering community.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;GEMRec-18K&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#20998;&#21035;&#26159;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#21644;&#29983;&#25104;&#39033;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.02205</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#29983;&#25104;&#24335;&#25512;&#33616;&#30340;&#20010;&#24615;&#21270;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Prompt-Model Retrieval for Generative Recommendation. (arXiv:2308.02205v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;GEMRec-18K&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#20998;&#21035;&#26159;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#21644;&#29983;&#25104;&#39033;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#26816;&#32034;&#19982;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#20505;&#36873;&#24211;&#36890;&#24120;&#30001;&#19968;&#32452;&#24050;&#20934;&#22791;&#22909;&#30340;&#39033;&#32452;&#25104;&#65292;&#22914;&#35270;&#39057;&#12289;&#20135;&#21697;&#25110;&#25991;&#31456;&#12290;&#38543;&#30528;&#29983;&#25104;&#24335;AI&#22914;GPT&#21644;Diffusion&#27169;&#22411;&#30340;&#36817;&#26399;&#36827;&#23637;&#65292;&#36824;&#26377;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#25512;&#33616;&#20219;&#21153;&#24453;&#25506;&#32034;&#65292;&#21363;&#36890;&#36807;&#20010;&#24615;&#21270;&#25552;&#31034;&#30001;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#39033;&#12290;&#20197;&#22270;&#20687;&#29983;&#25104;&#20026;&#20363;&#65292;&#20973;&#20511;&#29992;&#25143;&#30340;&#21333;&#20010;&#25552;&#31034;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#22312;&#20960;&#20998;&#38047;&#20869;&#21487;&#20197;&#29983;&#25104;&#25968;&#30334;&#20010;&#26032;&#22270;&#20687;&#12290;&#22312;&#8220;&#26080;&#38480;&#8221;&#39033;&#30340;&#23384;&#22312;&#19979;&#65292;&#25105;&#20204;&#22914;&#20309;&#23454;&#29616;&#20010;&#24615;&#21270;&#65311;&#22312;&#36825;&#39033;&#21021;&#27493;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#21363;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#21644;&#29983;&#25104;&#39033;&#25490;&#24207;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#26032;&#30340;&#20219;&#21153;&#24418;&#24335;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;GEMRec-18K&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;200&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;18K&#22270;&#20687;&#65292;&#24182;&#19982;&#19968;&#22871;&#22810;&#26679;&#21270;&#30340;90&#20010;&#25991;&#26412;&#25552;&#31034;&#30456;&#37197;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems are built to retrieve relevant items to satisfy users' information needs. The candidate corpus usually consists of a finite set of items that are ready to be served, such as videos, products, or articles. With recent advances in Generative AI such as GPT and Diffusion models, a new form of recommendation task is yet to be explored where items are to be created by generative models with personalized prompts. Taking image generation as an example, with a single prompt from the user and access to a generative model, it is possible to generate hundreds of new images in a few minutes. How shall we attain personalization in the presence of "infinite" items? In this preliminary study, we propose a two-stage framework, namely Prompt-Model Retrieval and Generated Item Ranking, to approach this new task formulation. We release GEMRec-18K, a prompt-model interaction dataset with 18K images generated by 200 publicly-available generative models paired with a diverse set of 90 te
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#22238;&#39038;&#20102;17&#20010;&#19987;&#27880;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#30340;&#36129;&#29486;&#65292;&#24182;&#23545;&#26368;&#30456;&#20851;&#30340;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;3000&#22810;&#20010;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#20415;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#25361;&#25112;&#65292;&#25152;&#26377;&#27979;&#35797;&#36807;&#30340;&#35821;&#26009;&#24211;&#21644;&#26368;&#20339;&#27169;&#22411;&#37117;&#34987;&#20197;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#20844;&#24320;&#12290;</title><link>http://arxiv.org/abs/2308.02199</link><description>&lt;p&gt;
&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Spanish Clinical Language Models. (arXiv:2308.02199v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02199
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#22238;&#39038;&#20102;17&#20010;&#19987;&#27880;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#30340;&#36129;&#29486;&#65292;&#24182;&#23545;&#26368;&#30456;&#20851;&#30340;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;3000&#22810;&#20010;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#20415;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#25361;&#25112;&#65292;&#25152;&#26377;&#27979;&#35797;&#36807;&#30340;&#35821;&#26009;&#24211;&#21644;&#26368;&#20339;&#27169;&#22411;&#37117;&#34987;&#20197;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32858;&#28966;&#20110;&#20351;&#29992;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#39046;&#22495;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;17&#20010;&#20027;&#35201;&#19987;&#27880;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#30340;&#36129;&#29486;&#65292;&#28982;&#21518;&#21015;&#20986;&#20102;&#26368;&#30456;&#20851;&#30340;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21487;&#29992;&#35821;&#26009;&#24211;&#30340;&#31934;&#36873;&#23376;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#27604;&#36739;&#65292;&#20197;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65307;&#24635;&#20849;&#36229;&#36807;3000&#20010;&#27169;&#22411;&#34987;&#38024;&#23545;&#36825;&#39033;&#30740;&#31350;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25152;&#26377;&#27979;&#35797;&#30340;&#35821;&#26009;&#24211;&#21644;&#26368;&#20339;&#27169;&#22411;&#37117;&#20197;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#20844;&#24320;&#65292;&#20197;&#20415;&#29420;&#31435;&#22242;&#38431;&#21487;&#20197;&#37325;&#29616;&#32467;&#26524;&#25110;&#22312;&#26410;&#26469;&#21019;&#24314;&#26032;&#30340;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#26102;&#36827;&#34892;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey focuses in encoder Language Models for solving tasks in the clinical domain in the Spanish language. We review the contributions of 17 corpora focused mainly in clinical tasks, then list the most relevant Spanish Language Models and Spanish Clinical Language models. We perform a thorough comparison of these models by benchmarking them over a curated subset of the available corpora, in order to find the best-performing ones; in total more than 3000 models were fine-tuned for this study. All the tested corpora and the best models are made publically available in an accessible way, so that the results can be reproduced by independent teams or challenged in the future when new Spanish Clinical Language models are created.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#20851;&#31995;&#20998;&#31867;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#35821;&#20041;&#33539;&#22260;&#20998;&#26512;&#27169;&#22411;&#30340;&#20915;&#31574;&#27169;&#24335;&#12290;&#35821;&#20041;&#33539;&#22260;&#26159;&#20851;&#20110;&#20998;&#31867;&#20915;&#31574;&#30340;&#25991;&#26412;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35821;&#20041;&#33539;&#22260;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#27169;&#22411;&#24448;&#24448;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#20102;&#24555;&#25463;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.02193</link><description>&lt;p&gt;
&#29992;&#35821;&#20041;&#33539;&#22260;&#35299;&#37322;&#20851;&#31995;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Explaining Relation Classification Models with Semantic Extents. (arXiv:2308.02193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#20851;&#31995;&#20998;&#31867;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#35821;&#20041;&#33539;&#22260;&#20998;&#26512;&#27169;&#22411;&#30340;&#20915;&#31574;&#27169;&#24335;&#12290;&#35821;&#20041;&#33539;&#22260;&#26159;&#20851;&#20110;&#20998;&#31867;&#20915;&#31574;&#30340;&#25991;&#26412;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35821;&#20041;&#33539;&#22260;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#27169;&#22411;&#24448;&#24448;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#20102;&#24555;&#25463;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#21644;GPT&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#25913;&#36827;&#20102;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#65292;&#21253;&#25324;&#20851;&#31995;&#20998;&#31867;&#12290;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#22312;&#31185;&#23398;&#22522;&#20934;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#26159;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#22797;&#26434;&#22240;&#32032;&#12290;&#21487;&#29702;&#35299;&#30340;&#31995;&#32479;&#23545;&#20110;&#38450;&#27490;&#26377;&#20559;&#35265;&#12289;&#36829;&#21453;&#30452;&#35273;&#25110;&#26377;&#23475;&#30340;&#20915;&#31574;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#26512;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#20915;&#31574;&#27169;&#24335;&#30340;&#27010;&#24565;&#65292;&#21363;&#35821;&#20041;&#33539;&#22260;&#12290;&#35821;&#20041;&#33539;&#22260;&#26159;&#20851;&#20110;&#20998;&#31867;&#20915;&#31574;&#30340;&#25991;&#26412;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#20801;&#35768;&#31867;&#20284;&#30340;&#36807;&#31243;&#26469;&#30830;&#23450;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35821;&#20041;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27880;&#37322;&#24037;&#20855;&#21644;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;&#65292;&#20197;&#20415;&#26041;&#20415;&#12289;&#21487;&#37325;&#22797;&#22320;&#30830;&#23450;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35821;&#20041;&#33539;&#22260;&#12290;&#27604;&#36739;&#20004;&#32773;&#21457;&#29616;&#65292;&#27169;&#22411;&#24448;&#24448;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#20102;&#24555;&#25463;&#27169;&#24335;&#12290;&#36825;&#20123;&#27169;&#24335;&#24456;&#38590;&#34987;&#20154;&#31867;&#35299;&#37322;&#25110;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the development of large pretrained language models, such as BERT and GPT, significantly improved information extraction systems on various tasks, including relation classification. State-of-the-art systems are highly accurate on scientific benchmarks. A lack of explainability is currently a complicating factor in many real-world applications. Comprehensible systems are necessary to prevent biased, counterintuitive, or harmful decisions.  We introduce semantic extents, a concept to analyze decision patterns for the relation classification task. Semantic extents are the most influential parts of texts concerning classification decisions. Our definition allows similar procedures to determine semantic extents for humans and models. We provide an annotation tool and a software framework to determine semantic extents for humans and models conveniently and reproducibly. Comparing both reveals that models tend to learn shortcut patterns from data. These patterns are hard to d
&lt;/p&gt;</description></item><item><title>AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.02182</link><description>&lt;p&gt;
AutoML4ETC: &#33258;&#21160;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23454;&#29616;&#29616;&#23454;&#19990;&#30028;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification. (arXiv:2308.02182v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02182
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#29615;&#22659;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;DL&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#38543;&#26102;&#38388;&#19981;&#21487;&#36991;&#20813;&#22320;&#19979;&#38477;&#12290;&#20165;&#20165;&#23545;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#21482;&#33021;&#37096;&#20998;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25163;&#21160;&#35843;&#25972;&#27169;&#22411;&#26550;&#26500;&#20197;&#28385;&#36275;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26399;&#26395;&#32791;&#26102;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;AutoML4ETC&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#20197;&#36827;&#34892;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#24378;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#19987;&#38376;&#38024;&#23545;&#20351;&#29992;&#25968;&#25454;&#21253;&#22836;&#23383;&#33410;&#36827;&#34892;&#36817;&#23454;&#26102;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#25628;&#32034;&#31354;&#38388;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AutoML4ETC&#29983;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#21453;&#39304;&#26469;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#20195;&#29702;&#33021;&#22815;&#20174;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.02151</link><description>&lt;p&gt;
Retroformer&#65306;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization. (arXiv:2308.02151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#21453;&#39304;&#26469;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#20195;&#29702;&#33021;&#22815;&#20174;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#26376;&#65292;&#20986;&#29616;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26032;&#36235;&#21183;&#65292;&#21363;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#25104;&#33021;&#22815;&#33258;&#20027;&#23436;&#25104;&#30446;&#26631;&#23548;&#21521;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#35821;&#35328;&#20195;&#29702;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22238;&#31572;&#20154;&#31867;&#29992;&#25143;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#27809;&#26377;&#20351;&#29992;&#29615;&#22659;&#29305;&#23450;&#30340;&#22870;&#21169;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#19968;&#20123;&#20195;&#29702;&#36890;&#36807;&#21475;&#22836;&#21453;&#39304;&#23454;&#29616;&#20102;&#36845;&#20195;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#20197;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#22870;&#21169;&#23398;&#20064;&#30456;&#20860;&#23481;&#30340;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22238;&#39038;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#33258;&#21160;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#29615;&#22659;&#21453;&#39304;&#20013;&#20248;&#21270;&#20195;&#29702;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20195;&#29702;&#26550;&#26500;&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#30340;&#22870;&#21169;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#24341;&#23548;&#30340;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#29305;&#24449;&#21644;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#26469;&#25913;&#36827;&#33258;&#21160;&#39550;&#39542;&#20195;&#29702;&#30340;&#33322;&#28857;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02126</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#24341;&#23548;&#30340;&#22522;&#20110;Transformer&#30340;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#25913;&#36827;&#33322;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semantics-guided Transformer-based Sensor Fusion for Improved Waypoint Prediction. (arXiv:2308.02126v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#24341;&#23548;&#30340;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#29305;&#24449;&#21644;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#26469;&#25913;&#36827;&#33258;&#21160;&#39550;&#39542;&#20195;&#29702;&#30340;&#33322;&#28857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26234;&#33021;&#33258;&#21160;&#39550;&#39542;&#20195;&#29702;&#26469;&#35828;&#65292;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#20173;&#28982;&#26159;&#39550;&#39542;&#22330;&#26223;&#29702;&#35299;&#30340;&#20851;&#38190;&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#35270;&#35273;&#20840;&#23616;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#23616;&#37096;&#33322;&#28857;&#39044;&#27979;&#20219;&#21153;&#65292;&#21333;&#27169;&#24577;&#32593;&#32476;&#20173;&#28982;&#21463;&#38480;&#20110;&#23545;&#36755;&#20837;&#20256;&#24863;&#22120;&#30340;&#28789;&#25935;&#24230;&#30340;&#24378;&#20381;&#36182;&#24615;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#24037;&#20316;&#25512;&#24191;&#20102;&#22312;&#29305;&#24449;&#32423;&#21035;&#19978;&#34701;&#21512;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#20351;&#29992;&#12290;&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#33021;&#20419;&#36827;&#30456;&#20114;&#30340;&#19978;&#19979;&#25991;&#20132;&#20114;&#65292;&#20294;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#23454;&#26102;&#36827;&#34892;&#20840;&#23616;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#24182;&#36827;&#34892;&#26368;&#23567;&#35745;&#31639;&#65292;&#22240;&#27492;&#22312;&#32473;&#23450;&#26377;&#38480;&#25968;&#37327;&#30340;&#21487;&#23454;&#38469;&#20351;&#29992;&#30340;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#35757;&#32451;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#26356;&#21152;&#31361;&#20986;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#34701;&#21512;&#19982;&#30446;&#26631;&#20219;&#21153;&#65288;&#22914;&#20132;&#36890;&#28783;&#35782;&#21035;&#21644;&#35821;&#20041;&#20998;&#21106;&#65289;&#39640;&#24230;&#30456;&#20851;&#30340;&#31934;&#24515;&#36873;&#21462;&#30340;&#36741;&#21161;&#20219;&#21153;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#22836;&#20026;&#33322;&#28857;&#39044;&#27979;&#36827;&#34892;&#20102;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sensor fusion approaches for intelligent self-driving agents remain key to driving scene understanding given visual global contexts acquired from input sensors. Specifically, for the local waypoint prediction task, single-modality networks are still limited by strong dependency on the sensitivity of the input sensor, and thus recent works promote the use of multiple sensors in fusion in feature level. While it is well known that multiple data modalities promote mutual contextual exchange, deployment to practical driving scenarios requires global 3D scene understanding in real-time with minimal computations, thus placing greater significance on training strategies given a limited number of practically usable sensors. In this light, we exploit carefully selected auxiliary tasks that are highly correlated with the target task of interest (e.g., traffic light recognition and semantic segmentation) by fusing auxiliary task features and also using auxiliary heads for waypoint prediction base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2308.02121</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;DNA&#30340;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Model Provenance via Model DNA. (arXiv:2308.02121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#29983;&#21629;&#21608;&#26399;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#65288;&#20363;&#22914;&#65292;&#20102;&#35299;&#27169;&#22411;&#30340;&#26469;&#28304;&#65292;&#35757;&#32451;&#26041;&#24335;&#20197;&#21450;&#20351;&#29992;&#26041;&#24335;&#65289;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#36825;&#19968;&#39046;&#22495;&#20869;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#65288;MP&#65289;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#30446;&#26631;&#27169;&#22411;&#19982;&#20854;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#19968;&#20010;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#30693;&#35782;&#20135;&#26435;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22810;&#20851;&#27880;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#21363;&#27169;&#22411;DNA&#65292;&#23427;&#20195;&#34920;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#32534;&#30721;&#20026;&#27169;&#22411;&#30340;&#32039;&#20945;&#19988;&#20840;&#38754;&#30340;&#34920;&#31034;&#65288;&#21363;DNA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this 
&lt;/p&gt;</description></item><item><title>VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2308.02117</link><description>&lt;p&gt;
VQGraph: &#22270;&#24418;&#21521;&#37327;&#37327;&#21270;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs
&lt;/p&gt;
&lt;p&gt;
VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. (arXiv:2308.02117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02117
&lt;/p&gt;
&lt;p&gt;
VQGraph&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#23427;&#37319;&#29992;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#12290;&#36890;&#36807; VQGraph&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#20449;&#24687;&#20256;&#36882;&#65292;&#32858;&#21512;&#23616;&#37096;&#37051;&#23621;&#20197;&#26356;&#26032;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#20449;&#24687;&#20256;&#36882;&#23548;&#33268;&#22312;&#23454;&#38469;&#30340;&#24310;&#36831;&#32422;&#26463;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#27169;&#20223;GNN&#30340;&#36755;&#20986;&#26469;&#23398;&#20064;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#34920;&#31034;&#31354;&#38388;&#21487;&#33021;&#19981;&#36275;&#20197;&#34920;&#31034;&#24213;&#23618;&#22270;&#30340;&#22810;&#26679;&#21270;&#23616;&#37096;&#32467;&#26500;&#65292;&#36825;&#38480;&#21046;&#20102;&#20174;GNN&#21040;MLP&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;VQGraph&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#24418;&#34920;&#31034;&#31354;&#38388;&#65292;&#29992;&#20110;&#36830;&#25509;GNN&#21644;MLPs&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#21464;&#20307;&#30340;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#22270;&#26631;&#35760;&#22120;&#65292;&#23427;&#23558;&#22810;&#26679;&#21270;&#30340;&#23616;&#37096;&#32467;&#26500;&#33410;&#28857;&#26126;&#30830;&#34920;&#31034;&#20026;&#22823;&#37327;&#31163;&#25955;&#20196;&#29260;&#65292;&#24182;&#26500;&#25104;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#20195;&#30721;&#20070;&#12290;&#37197;&#22791;&#20102;&#23398;&#20064;&#30340;&#20195;&#30721;&#20070;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propos
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AdvFAS&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20004;&#20010;&#32806;&#21512;&#24471;&#20998;&#20934;&#30830;&#21306;&#20998;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#20154;&#33080;&#22270;&#20687;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#23637;&#31034;&#20102;&#39640;&#25928;&#30340;&#38450;&#24481;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.02116</link><description>&lt;p&gt;
AdvFAS&#65306;&#19968;&#31181;&#38024;&#23545;&#23545;&#25239;&#26679;&#26412;&#30340;&#24378;&#40065;&#26834;&#20154;&#33080;&#38450;&#30095;&#26494;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AdvFAS: A robust face anti-spoofing framework against adversarial examples. (arXiv:2308.02116v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02116
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AdvFAS&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20004;&#20010;&#32806;&#21512;&#24471;&#20998;&#20934;&#30830;&#21306;&#20998;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#20154;&#33080;&#22270;&#20687;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#23637;&#31034;&#20102;&#39640;&#25928;&#30340;&#38450;&#24481;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#23545;&#20184;&#23637;&#31034;&#25915;&#20987;&#30340;&#21487;&#38752;&#24615;&#38656;&#35201;&#37096;&#32626;&#20154;&#33080;&#21453;&#30095;&#26494;&#25216;&#26415;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20063;&#24456;&#38590;&#38450;&#24481;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#23545;&#25239;&#24615;&#38450;&#24481;&#31574;&#30053;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21463;&#38480;&#20110;&#26222;&#36866;&#24615;&#12289;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#23545;&#25239;&#24615;&#26816;&#27979;&#19982;&#20154;&#33080;&#21453;&#30095;&#26494;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#40065;&#26834;&#30340;&#20154;&#33080;&#21453;&#30095;&#26494;&#26694;&#26550;&#65292;&#21363;AdvFAS&#65292;&#23427;&#21033;&#29992;&#20004;&#20010;&#32806;&#21512;&#24471;&#20998;&#20934;&#30830;&#21306;&#20998;&#27491;&#30830;&#26816;&#27979;&#21644;&#38169;&#35823;&#26816;&#27979;&#30340;&#20154;&#33080;&#22270;&#20687;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;&#25915;&#20987;&#12289;&#25968;&#25454;&#38598;&#21644;&#39592;&#24178;&#32593;&#32476;&#31561;&#22810;&#31181;&#29615;&#22659;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring the reliability of face recognition systems against presentation attacks necessitates the deployment of face anti-spoofing techniques. Despite considerable advancements in this domain, the ability of even the most state-of-the-art methods to defend against adversarial examples remains elusive. While several adversarial defense strategies have been proposed, they typically suffer from constrained practicability due to inevitable trade-offs between universality, effectiveness, and efficiency. To overcome these challenges, we thoroughly delve into the coupled relationship between adversarial detection and face anti-spoofing. Based on this, we propose a robust face anti-spoofing framework, namely AdvFAS, that leverages two coupled scores to accurately distinguish between correctly detected and wrongly detected face images. Extensive experiments demonstrate the effectiveness of our framework in a variety of settings, including different attacks, datasets, and backbones, meanwhile e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;N-gram&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35268;&#33539;&#21270;&#30446;&#26631;&#35789;&#32452;&#26469;&#25913;&#21892;&#19978;&#19979;&#25991;&#20559;&#24046;&#65292;&#25552;&#39640;&#20851;&#38190;&#35789;&#35782;&#21035;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.02092</link><description>&lt;p&gt;
N-gram&#22686;&#24378;&#65306;&#36890;&#36807;&#35268;&#33539;&#21270;N-gram&#30446;&#26631;&#25913;&#21892;&#19978;&#19979;&#25991;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets. (arXiv:2308.02092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;N-gram&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35268;&#33539;&#21270;&#30446;&#26631;&#35789;&#32452;&#26469;&#25913;&#21892;&#19978;&#19979;&#25991;&#20559;&#24046;&#65292;&#25552;&#39640;&#20851;&#38190;&#35789;&#35782;&#21035;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21830;&#21153;&#35848;&#35805;&#30340;&#35821;&#38899;&#21040;&#25991;&#26412;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#36716;&#24405;&#19987;&#26377;&#21517;&#35789;&#21644;&#25216;&#26415;&#26415;&#35821;&#23588;&#20026;&#37325;&#35201;&#12290;&#36825;&#20123;&#35789;&#23545;&#20110;&#29702;&#35299;&#23545;&#35805;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#24456;&#23569;&#20986;&#29616;&#65292;&#22240;&#27492;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#35757;&#32451;&#25968;&#25454;&#20013;&#24456;&#21487;&#33021;&#32570;&#20047;&#65292;&#36825;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#36896;&#25104;&#20102;&#24456;&#22823;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#20851;&#38190;&#35789;&#22686;&#24378;&#26426;&#21046;&#65292;&#23427;&#25104;&#21151;&#22320;&#20351;&#29992;&#35268;&#33539;&#21270;&#30340;&#21333;&#35789;&#21644;n-gram&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#26631;&#35760;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#20851;&#38190;&#35789;&#30446;&#26631;&#30340;&#20002;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35843;&#25972;&#22686;&#24378;&#26435;&#37325;&#36923;&#36753;&#20197;&#36991;&#20813;&#36807;&#24230;&#22686;&#24378;&#22810;&#20010;&#26631;&#35760;&#30340;&#20851;&#38190;&#35789;&#12290;&#22312;&#25105;&#20204;&#30340;&#19987;&#26377;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#35789;&#35782;&#21035;&#29575;&#30456;&#23545;&#25552;&#39640;&#20102;26&#65285;&#65292;&#22312;LibriSpeech&#19978;&#25552;&#39640;&#20102;2&#65285;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#28041;&#21450;&#38750;&#23383;&#27597;&#23383;&#31526;&#25110;&#20855;&#26377;&#38750;&#26631;&#20934;&#21457;&#38899;&#30340;&#30446;&#26631;&#29305;&#21035;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate transcription of proper names and technical terms is particularly important in speech-to-text applications for business conversations. These words, which are essential to understanding the conversation, are often rare and therefore likely to be under-represented in text and audio training data, creating a significant challenge in this domain. We present a two-step keyword boosting mechanism that successfully works on normalized unigrams and n-grams rather than just single tokens, which eliminates missing hits issues with boosting raw targets. In addition, we show how adjusting the boosting weight logic avoids over-boosting multi-token keywords. This improves our keyword recognition rate by 26% relative on our proprietary in-domain dataset and 2% on LibriSpeech. This method is particularly useful on targets that involve non-alphabetic characters or have non-standard pronunciations.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39046;&#22495;&#28418;&#31227;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;DNN&#21644;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26816;&#27979;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;... (&#25688;&#35201;&#20869;&#23481;&#30465;&#30053;)</title><link>http://arxiv.org/abs/2308.02084</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#31471;&#30340;&#39640;&#25928;&#27169;&#22411;&#36866;&#24212;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Model Adaptation for Continual Learning at the Edge. (arXiv:2308.02084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02084
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39046;&#22495;&#28418;&#31227;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;DNN&#21644;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26816;&#27979;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;... (&#25688;&#35201;&#20869;&#23481;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#22266;&#23450;&#21644;&#21305;&#37197;&#30340;&#65292;&#20294;&#36825;&#36890;&#24120;&#26159;&#38169;&#35823;&#30340;&#20551;&#35774;&#12290;&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#22312;&#30495;&#23454;&#35774;&#22791;&#19978;&#26102;&#65292;&#25968;&#25454;&#20998;&#24067;&#24120;&#24120;&#20250;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#21407;&#22240;&#26159;&#29615;&#22659;&#22240;&#32032;&#12289;&#20256;&#24863;&#22120;&#29305;&#24615;&#21644;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Encoder-Adaptor-Reconfigurator&#65288;EAR&#65289;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39046;&#22495;&#28418;&#31227;&#19979;&#30340;&#39640;&#25928;&#25345;&#32493;&#23398;&#20064;&#12290;EAR&#26694;&#26550;&#21033;&#29992;&#22266;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#26469;&#22788;&#29702;&#26032;&#25968;&#25454;&#12290;EAR&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#23558;DNN&#19982;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#30456;&#32467;&#21512;&#65292;&#26816;&#27979;&#20986;&#26032;&#25968;&#25454;&#26159;&#21542;&#23646;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;
&lt;/p&gt;
&lt;p&gt;
Most machine learning (ML) systems assume stationary and matching data distributions during training and deployment. This is often a false assumption. When ML models are deployed on real devices, data distributions often shift over time due to changes in environmental factors, sensor characteristics, and task-of-interest. While it is possible to have a human-in-the-loop to monitor for distribution shifts and engineer new architectures in response to these shifts, such a setup is not cost-effective. Instead, non-stationary automated ML (AutoML) models are needed. This paper presents the Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning under domain shifts. The EAR framework uses a fixed deep neural network (DNN) feature encoder and trains shallow networks on top of the encoder to handle novel data. The EAR framework is capable of 1) detecting when new data is out-of-distribution (OOD) by combining DNNs with hyperdimensional computing (HDC), 2) identifying l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ETR-NLP&#27169;&#22411;&#26469;&#20943;&#36731;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24178;&#25200;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#21644;&#26174;&#24335;&#20219;&#21153;&#36335;&#30001;&#30340;&#21327;&#21516;&#32452;&#21512;&#65292;&#22312;&#20849;&#20139;&#20998;&#25903;&#21644;&#20219;&#21153;&#29305;&#23450;&#20998;&#25903;&#20013;&#26174;&#24335;&#22320;&#20998;&#31163;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#20219;&#21153;&#20043;&#38388;&#30340;&#26368;&#23567;&#21270;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2308.02066</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#30340;&#26174;&#24335;&#20219;&#21153;&#36335;&#30001;&#26469;&#20943;&#36731;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives. (arXiv:2308.02066v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ETR-NLP&#27169;&#22411;&#26469;&#20943;&#36731;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24178;&#25200;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#21644;&#26174;&#24335;&#20219;&#21153;&#36335;&#30001;&#30340;&#21327;&#21516;&#32452;&#21512;&#65292;&#22312;&#20849;&#20139;&#20998;&#25903;&#21644;&#20219;&#21153;&#29305;&#23450;&#20998;&#25903;&#20013;&#26174;&#24335;&#22320;&#20998;&#31163;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#20219;&#21153;&#20043;&#38388;&#30340;&#26368;&#23567;&#21270;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#26469;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#26469;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MTL&#27169;&#22411;&#24050;&#32463;&#34987;&#21457;&#29616;&#23384;&#22312;&#36127;&#38754;&#24178;&#25200;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#36731;&#20219;&#21153;&#24178;&#25200;&#65292;&#24050;&#26377;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#25439;&#22833;/&#26799;&#24230;&#24179;&#34913;&#25110;&#38544;&#24335;&#21442;&#25968;&#21010;&#20998;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ETR-NLP&#26469;&#36890;&#36807;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#65288;NLP&#65289;&#21644;&#26174;&#24335;&#20219;&#21153;&#36335;&#30001;&#65288;ETR&#65289;&#30340;&#21327;&#21516;&#32452;&#21512;&#26469;&#20943;&#36731;&#20219;&#21153;&#24178;&#25200;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#26469;&#25552;&#21462;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#37325;&#26032;&#32452;&#21512;&#25104;&#19968;&#20010;&#20849;&#20139;&#20110;&#25152;&#26377;&#20219;&#21153;&#30340;&#20998;&#25903;&#21644;&#19987;&#38376;&#20026;&#27599;&#20010;&#20219;&#21153;&#20445;&#30041;&#30340;&#26174;&#24335;&#20219;&#21153;&#29305;&#23450;&#20998;&#25903;&#12290;&#38750;&#21487;&#23398;&#20064;&#21407;&#35821;&#21644;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#26174;&#24335;&#35299;&#32806;&#20026;&#26368;&#23567;&#21270;&#20219;&#21153;&#24178;&#25200;&#25552;&#20379;&#20102;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ETR-NLP&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networ
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22312;&#36229;&#29699;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;&#30340;&#20154;&#33080;&#22270;&#20687;&#30340;&#29983;&#29289;&#29305;&#24449;&#23481;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.02065</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#20154;&#33080;&#27169;&#22411;&#30340;&#29983;&#29289;&#29305;&#24449;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
On the Biometric Capacity of Generative Face Models. (arXiv:2308.02065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02065
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22312;&#36229;&#29699;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;&#30340;&#20154;&#33080;&#22270;&#20687;&#30340;&#29983;&#29289;&#29305;&#24449;&#23481;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#29983;&#25104;&#36924;&#30495;&#38754;&#23380;&#30340;&#36827;&#23637;&#38750;&#24120;&#22823;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#36827;&#23637;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#65306;&#8220;&#32473;&#23450;&#19968;&#20010;&#29983;&#25104;&#30340;&#20154;&#33080;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#22810;&#23569;&#20010;&#29420;&#29305;&#30340;&#36523;&#20221;&#65311;&#8221;&#25442;&#21477;&#35805;&#35828;&#65292;&#29983;&#25104;&#20154;&#33080;&#27169;&#22411;&#30340;&#29983;&#29289;&#29305;&#24449;&#23481;&#37327;&#26159;&#22810;&#23569;&#65311;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#31185;&#23398;&#20381;&#25454;&#23558;&#26377;&#21161;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#29983;&#25104;&#20154;&#33080;&#27169;&#22411;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#30340;&#21487;&#25193;&#23637;&#24615;&#30340;&#19978;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#26469;&#20272;&#35745;&#22312;&#19968;&#20010;&#36229;&#29699;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;&#30340;&#20154;&#33080;&#22270;&#20687;&#30340;&#29983;&#29289;&#29305;&#24449;&#23481;&#37327;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#19978;&#24212;&#29992;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26080;&#26465;&#20214;&#29983;&#25104;&#22120;&#22914;StyleGAN&#12289;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#8220;&#29983;&#25104;&#30340;&#29031;&#29255;&#8221;&#65292;&#20197;&#21450;DCF&#38754;&#65292;&#19968;&#20010;&#31867;&#26465;&#20214;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#19982;&#24615;&#21035;&#21644;&#24180;&#40836;&#31561;&#20154;&#21475;&#23646;&#24615;&#30456;&#20851;&#30340;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;&#23481;&#37327;&#20272;&#35745;&#34920;&#26126;&#65292;&#22312;&#34394;&#20551;&#25509;&#21463;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;ArcFace&#34920;&#31034;&#65306;
&lt;/p&gt;
&lt;p&gt;
There has been tremendous progress in generating realistic faces with high fidelity over the past few years. Despite this progress, a crucial question remains unanswered: "Given a generative face model, how many unique identities can it generate?" In other words, what is the biometric capacity of the generative face model? A scientific basis for answering this question will benefit evaluating and comparing different generative face models and establish an upper bound on their scalability. This paper proposes a statistical approach to estimate the biometric capacity of generated face images in a hyperspherical feature space. We employ our approach on multiple generative models, including unconditional generators like StyleGAN, Latent Diffusion Model, and "Generated Photos," as well as DCFace, a class-conditional generator. We also estimate capacity w.r.t. demographic attributes such as gender and age. Our capacity estimates indicate that (a) under ArcFace representation at a false accep
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#39640;&#31232;&#30095;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20351;&#29992;&#20256;&#32479;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#25928;&#26524;&#19981;&#20339;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02060</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#31232;&#30095;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accurate Neural Network Pruning Requires Rethinking Sparse Optimization. (arXiv:2308.02060v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02060
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#39640;&#31232;&#30095;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20351;&#29992;&#20256;&#32479;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#25928;&#26524;&#19981;&#20339;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#65292;&#33719;&#24471;&#26082;&#39640;&#31934;&#30830;&#21448;&#39640;&#31232;&#30095;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29256;&#26412;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#31038;&#21306;&#24050;&#32463;&#23545;&#20960;&#31181;&#39640;&#24615;&#33021;&#30340;&#21098;&#26525;&#25216;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#31232;&#30095;&#24615;&#21644;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#30340;&#26631;&#20934;&#38543;&#26426;&#20248;&#21270;&#25216;&#26415;&#30340;&#20132;&#20114;&#20102;&#35299;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20351;&#29992;&#26631;&#20934;&#30340;&#23494;&#38598;&#35757;&#32451;&#35745;&#21010;&#21644;&#36229;&#21442;&#25968;&#26469;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31232;&#30095;&#22522;&#20934;&#26469;&#30740;&#31350;&#39640;&#31232;&#30095;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20351;&#29992;&#26631;&#20934;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#26159;&#27425;&#20248;&#30340;&#65292;&#23548;&#33268;&#27424;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26082;&#21487;&#20197;&#29992;&#20110;&#35270;&#35273;&#27169;&#22411;&#65288;&#22914;ResNet50/ImageNet&#65289;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT/GLUE&#65289;&#30340;&#31232;&#30095;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.02058</link><description>&lt;p&gt;
&#25972;&#21512;&#40065;&#33725;&#34892;&#20026;&#21040;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Recklessness to Collaborative Filtering based Recommender Systems. (arXiv:2308.02058v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#21487;&#38752;&#24615;&#27979;&#37327;&#30340;&#25512;&#33616;&#31995;&#32479;&#24448;&#24448;&#22312;&#39044;&#27979;&#20013;&#26356;&#21152;&#20445;&#23432;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20445;&#25345;&#21487;&#38752;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26032;&#39062;&#24615;&#30340;&#26174;&#33879;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#30697;&#38453;&#20998;&#35299;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#39033;&#26032;&#30340;&#39033;&#65292;&#31216;&#20026;&#40065;&#33725;&#34892;&#20026;&#65292;&#23427;&#21487;&#20197;&#25511;&#21046;&#22312;&#20570;&#20986;&#20851;&#20110;&#39044;&#27979;&#21487;&#38752;&#24615;&#30340;&#20915;&#31574;&#26102;&#25152;&#24076;&#26395;&#30340;&#39118;&#38505;&#27700;&#24179;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#40065;&#33725;&#34892;&#20026;&#19981;&#20165;&#20801;&#35768;&#36827;&#34892;&#39118;&#38505;&#35843;&#25511;&#65292;&#36824;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#30340;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems that include some reliability measure of their predictions tend to be more conservative in forecasting, due to their constraint to preserve reliability. This leads to a significant drop in the coverage and novelty that these systems can provide. In this paper, we propose the inclusion of a new term in the learning process of matrix factorization-based recommender systems, called recklessness, which enables the control of the risk level desired when making decisions about the reliability of a prediction. Experimental results demonstrate that recklessness not only allows for risk regulation but also improves the quantity and quality of predictions provided by the recommender system.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#22696;&#35199;&#21733;&#24037;&#20154;&#19968;&#30452;&#24314;&#35758;&#20302;&#34218;&#24037;&#20316;&#65292;&#24182;&#21521;&#22899;&#24615;&#26356;&#20542;&#21521;&#20110;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;LLMs&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02053</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#24179;&#31561;&#26426;&#20250;: &#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#25581;&#31034;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations. (arXiv:2308.02053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02053
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#22696;&#35199;&#21733;&#24037;&#20154;&#19968;&#30452;&#24314;&#35758;&#20302;&#34218;&#24037;&#20316;&#65292;&#24182;&#21521;&#22899;&#24615;&#26356;&#20542;&#21521;&#20110;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;LLMs&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#20102;&#35299;&#36825;&#20123;&#20559;&#35265;&#23545;&#20110;&#29702;&#35299;&#22312;&#20351;&#29992;LLMs&#36827;&#34892;&#20915;&#31574;&#26102;&#28508;&#22312;&#30340;&#21518;&#32493;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21382;&#21490;&#19978;&#22788;&#20110;&#21155;&#21183;&#30340;&#32676;&#20307;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#30340;&#35282;&#24230;&#20998;&#26512;&#21644;&#27604;&#36739;LLMs&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;ChatGPT&#21644;LLaMA&#36825;&#20004;&#20010;&#21069;&#27839;LLMs&#20869;&#30340;&#20132;&#21449;&#20559;&#35265;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20027;&#35201;&#38598;&#20013;&#22312;&#25581;&#31034;&#24615;&#21035;&#35748;&#21516;&#21644;&#22269;&#31821;&#20559;&#35265;&#19978;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#20154;&#21475;&#32479;&#35745;&#36523;&#20221;&#30340;&#20132;&#21449;&#20559;&#35265;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#26126;&#26174;&#30340;&#20559;&#35265;&#65292;&#20363;&#22914;&#20004;&#20010;&#27169;&#22411;&#19968;&#30452;&#24314;&#35758;&#22696;&#35199;&#21733;&#24037;&#20154;&#20174;&#20107;&#20302;&#34218;&#24037;&#20316;&#65292;&#25110;&#32773;&#26356;&#20542;&#21521;&#20110;&#21521;&#22899;&#24615;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#27979;&#37327;&#21644;&#29702;&#35299;LLMs&#20013;&#30340;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen widespread deployment in various real-world applications. Understanding these biases is crucial to comprehend the potential downstream consequences when using LLMs to make decisions, particularly for historically disadvantaged groups. In this work, we propose a simple method for analyzing and comparing demographic bias in LLMs, through the lens of job recommendations. We demonstrate the effectiveness of our method by measuring intersectional biases within ChatGPT and LLaMA, two cutting-edge LLMs. Our experiments primarily focus on uncovering gender identity and nationality bias; however, our method can be extended to examine biases associated with any intersection of demographic identities. We identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for Mexican workers or preferring to recommend secretarial roles to women. Our study highlights the importance of measu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25209;&#21028;&#24615;&#35780;&#20272;&#20102;&#27431;&#27954;&#25552;&#35758;&#30340;AI&#27861;&#26696;&#30340;&#39118;&#38505;&#31649;&#29702;&#21644;&#39118;&#38505;&#21487;&#25509;&#21463;&#24615;&#26041;&#27861;&#12290;&#19982;&#27861;&#26696;&#30340;&#29421;&#20041;&#35299;&#37322;&#19981;&#21516;&#65292;&#35758;&#20250;&#30340;&#33609;&#26696;&#20462;&#25913;&#24341;&#20837;&#20102;&#26356;&#20855;&#21487;&#34892;&#24615;&#30340;&#8220;&#21512;&#29702;&#24615;&#8221;&#21407;&#21017;&#65292;&#24182;&#26356;&#36879;&#26126;&#22320;&#34920;&#26126;&#39118;&#38505;&#21487;&#25509;&#21463;&#24615;&#21028;&#26029;&#30340;&#20215;&#20540;&#21462;&#21521;&#21644;&#24773;&#22659;&#24615;&#36136;&#65292;&#26356;&#22909;&#22320;&#24179;&#34913;&#20102;&#30456;&#31216;&#24615;&#21644;&#21487;&#20449;&#36182;&#24615;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.02047</link><description>&lt;p&gt;
&#27431;&#27954;&#25552;&#35758;&#30340;AI&#27861;&#26696;&#20013;&#21487;&#25509;&#21463;&#30340;&#39118;&#38505;&#65306;&#23545;&#20915;&#23450;&#39118;&#38505;&#31649;&#29702;&#30340;&#21512;&#29702;&#24615;&#21644;&#20854;&#20182;&#21407;&#21017;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough. (arXiv:2308.02047v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#21028;&#24615;&#35780;&#20272;&#20102;&#27431;&#27954;&#25552;&#35758;&#30340;AI&#27861;&#26696;&#30340;&#39118;&#38505;&#31649;&#29702;&#21644;&#39118;&#38505;&#21487;&#25509;&#21463;&#24615;&#26041;&#27861;&#12290;&#19982;&#27861;&#26696;&#30340;&#29421;&#20041;&#35299;&#37322;&#19981;&#21516;&#65292;&#35758;&#20250;&#30340;&#33609;&#26696;&#20462;&#25913;&#24341;&#20837;&#20102;&#26356;&#20855;&#21487;&#34892;&#24615;&#30340;&#8220;&#21512;&#29702;&#24615;&#8221;&#21407;&#21017;&#65292;&#24182;&#26356;&#36879;&#26126;&#22320;&#34920;&#26126;&#39118;&#38505;&#21487;&#25509;&#21463;&#24615;&#21028;&#26029;&#30340;&#20215;&#20540;&#21462;&#21521;&#21644;&#24773;&#22659;&#24615;&#36136;&#65292;&#26356;&#22909;&#22320;&#24179;&#34913;&#20102;&#30456;&#31216;&#24615;&#21644;&#21487;&#20449;&#36182;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27431;&#27954;&#22996;&#21592;&#20250;&#25552;&#35758;&#30340;AI&#27861;&#26696;&#20013;&#30340;&#39118;&#38505;&#31649;&#29702;&#21644;&#39118;&#38505;&#21487;&#25509;&#21463;&#24615;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#35813;&#27861;&#26696;&#38024;&#23545;&#23545;&#22522;&#26412;&#26435;&#21033;&#21644;&#23433;&#20840;&#26500;&#25104;&#39118;&#38505;&#30340;&#39640;&#39118;&#38505;AI&#31995;&#32479;&#12290;&#35813;&#27861;&#26696;&#26088;&#22312;&#25512;&#24191;&#8220;&#21487;&#20449;&#36182;&#8221;&#30340;AI&#65292;&#24182;&#25215;&#25285;&#30456;&#31216;&#30340;&#30417;&#31649;&#36127;&#25285;&#12290;&#20854;&#20851;&#20110;&#39118;&#38505;&#21487;&#25509;&#21463;&#24615;&#30340;&#35268;&#23450;&#35201;&#27714;&#23613;&#21487;&#33021;&#20943;&#23569;&#25110;&#28040;&#38500;&#39640;&#39118;&#38505;&#31995;&#32479;&#30340;&#27531;&#20313;&#39118;&#38505;&#65292;&#32771;&#34385;&#8220;&#25216;&#26415;&#27700;&#24179;&#8221;&#12290;&#29305;&#21035;&#26159;&#22914;&#26524;&#29421;&#20041;&#35299;&#37322;&#65292;&#36825;&#20010;&#26631;&#20934;&#26159;&#34892;&#19981;&#36890;&#30340;&#65292;&#26082;&#19981;&#20419;&#36827;&#30456;&#31216;&#30340;&#30417;&#31649;&#36127;&#25285;&#65292;&#20063;&#19981;&#20419;&#36827;&#21487;&#20449;&#36182;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35758;&#20250;&#23545;&#39118;&#38505;&#31649;&#29702;&#35268;&#23450;&#26368;&#36817;&#30340;&#33609;&#26696;&#20462;&#25913;&#24341;&#20837;&#20102;&#8220;&#21512;&#29702;&#24615;&#8221;&#12289;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#65292;&#24182;&#23545;&#39118;&#38505;&#21487;&#25509;&#21463;&#24615;&#21028;&#26029;&#30340;&#20215;&#20540;&#21462;&#21521;&#21644;&#24773;&#22659;&#24615;&#36136;&#26356;&#21152;&#36879;&#26126;&#12290;&#26412;&#25991;&#35748;&#20026;&#35758;&#20250;&#30340;&#26041;&#27861;&#26356;&#20855;&#21487;&#34892;&#24615;&#65292;&#26356;&#33021;&#24179;&#34913;&#30456;&#31216;&#24615;&#21644;&#21487;&#20449;&#36182;&#24615;&#30340;&#30446;&#26631;&#12290;&#35299;&#37322;&#20102;&#35758;&#20250;&#30340;&#26041;&#27861;&#22914;&#20309;&#29615;&#32469;&#39118;&#38505;&#31649;&#29702;&#30340;&#19981;&#21516;&#26041;&#38754;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper critically evaluates the European Commission's proposed AI Act's approach to risk management and risk acceptability for high-risk AI systems that pose risks to fundamental rights and safety. The Act aims to promote "trustworthy" AI with a proportionate regulatory burden. Its provisions on risk acceptability require residual risks from high-risk systems to be reduced or eliminated "as far as possible", having regard to the "state of the art". This criterion, especially if interpreted narrowly, is unworkable and promotes neither proportionate regulatory burden, nor trustworthiness. By contrast the Parliament's most recent draft amendments to the risk management provisions introduce "reasonableness", cost-benefit analysis, and are more transparent about the value-laden and contextual nature of risk acceptability judgements. This paper argues that the Parliament's approach is more workable, and better balances the goals of proportionality and trustworthiness. It explains what re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26723;&#26696;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#33258;&#21160;&#36716;&#24405;&#21644;&#26657;&#27491;&#25163;&#31295;&#65292;&#20197;&#21450;&#26631;&#20934;&#21270;&#25991;&#26412;&#12290;&#36890;&#36807;&#27979;&#35797;ChatGPT&#31995;&#32479;&#65292;&#22312;&#23545;&#20449;&#20989;&#36827;&#34892;&#25991;&#26412;&#26631;&#20934;&#21270;&#26102;&#21462;&#24471;&#20102;&#19968;&#23450;&#25928;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25968;&#23383;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26723;&#26696;&#21644;&#21382;&#21490;&#30740;&#31350;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02044</link><description>&lt;p&gt;
&#26723;&#26696;&#21644;&#21382;&#21490;&#23398;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65306;HTS&#21644;ChatGPT
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence in archival and historical scholarship workflow: HTS and ChatGPT. (arXiv:2308.02044v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26723;&#26696;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#33258;&#21160;&#36716;&#24405;&#21644;&#26657;&#27491;&#25163;&#31295;&#65292;&#20197;&#21450;&#26631;&#20934;&#21270;&#25991;&#26412;&#12290;&#36890;&#36807;&#27979;&#35797;ChatGPT&#31995;&#32479;&#65292;&#22312;&#23545;&#20449;&#20989;&#36827;&#34892;&#25991;&#26412;&#26631;&#20934;&#21270;&#26102;&#21462;&#24471;&#20102;&#19968;&#23450;&#25928;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25968;&#23383;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26723;&#26696;&#21644;&#21382;&#21490;&#30740;&#31350;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#23545;&#26723;&#26696;&#36951;&#20135;&#25968;&#23383;&#21270;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#23545;&#25163;&#31295;&#30340;&#33258;&#21160;&#36716;&#24405;&#12289;&#26657;&#27491;&#21644;&#26631;&#20934;&#21270;&#30340;&#24433;&#21709;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#23383;&#21270;&#25512;&#21160;&#23398;&#32773;&#37325;&#26032;&#23450;&#20041;&#26723;&#26696;&#21644;&#21382;&#21490;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#25968;&#23383;&#21270;&#21644;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#25552;&#20379;&#27169;&#25311;&#28304;&#25991;&#20214;&#30340;&#20415;&#25463;&#24615;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#20004;&#20010;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20998;&#21035;&#26159;Transkribus&#21644;ChatGPT&#65292;&#23427;&#20204;&#20351;&#24471;&#23545;&#25968;&#23383;&#21270;&#28304;&#25991;&#20214;&#30340;&#39640;&#25928;&#20998;&#26512;&#21644;&#36716;&#24405;&#25104;&#20026;&#21487;&#33021;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#23545;ChatGPT&#30340;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#29992;&#20110;&#23545;&#20445;&#23384;&#22312;Biscari&#26723;&#26696;&#65288;&#21345;&#22612;&#23612;&#20122;&#65289;&#30340;&#20449;&#20989;&#37096;&#20998;&#20013;&#30340;366&#23553;&#20449;&#20214;&#36827;&#34892;&#25991;&#26412;&#26631;&#20934;&#21270;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#23548;&#33268;&#19968;&#20123;&#19981;&#20934;&#30830;&#24615;&#65292;&#20294;&#32416;&#27491;&#21518;&#30340;&#25991;&#26412;&#20173;&#28982;&#36798;&#21040;&#20102;&#26399;&#26395;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25991;&#31456;&#24471;&#20986;&#32467;&#35770;&#65292;&#25968;&#23383;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26723;&#26696;&#21644;&#21382;&#21490;&#30740;&#31350;&#65292;&#20801;&#35768;&#23545;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#21644;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article examines the impact of Artificial Intelligence on the archival heritage digitization processes, specifically regarding the manuscripts' automatic transcription, their correction, and normalization. It highlights how digitality has compelled scholars to redefine Archive and History field and has facilitated the accessibility of analogue sources through digitization and integration into big data. The study focuses on two AI systems, namely Transkribus and ChatGPT, which enable efficient analysis and transcription of digitized sources. The article presents a test of ChatGPT, which was utilized to normalize the text of 366 letters stored in the Correspondence section of the Biscari Archive (Catania). Although the AI exhibited some limitations that resulted in inaccuracies, the corrected texts met expectations. Overall, the article concludes that digitization and AI can significantly enhance archival and historical research by allowing the analysis of vast amounts of data and t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36828;&#31243;&#25910;&#38598;&#24739;&#32773;&#25968;&#25454;&#65292;&#21033;&#29992;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#36828;&#31243;&#30417;&#27979;&#25216;&#26415;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#30142;&#30149;&#27934;&#23519;&#21147;&#65292;&#20174;&#32780;&#34917;&#20805;&#20256;&#32479;&#21307;&#30103;&#29615;&#22659;&#20013;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02043</link><description>&lt;p&gt;
&#36890;&#36807;&#36828;&#31243;&#25910;&#38598;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#25163;&#26426;&#25968;&#25454;&#24320;&#21457;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#23545;&#30142;&#30149;&#30340;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Disease Insight through Digital Biomarkers Developed by Remotely Collected Wearables and Smartphone Data. (arXiv:2308.02043v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02043
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36828;&#31243;&#25910;&#38598;&#24739;&#32773;&#25968;&#25454;&#65292;&#21033;&#29992;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#36828;&#31243;&#30417;&#27979;&#25216;&#26415;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#30142;&#30149;&#27934;&#23519;&#21147;&#65292;&#20174;&#32780;&#34917;&#20805;&#20256;&#32479;&#21307;&#30103;&#29615;&#22659;&#20013;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#36828;&#31243;&#24739;&#32773;&#30417;&#27979;&#21487;&#20197;&#21450;&#26102;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20102;&#35299;&#24739;&#32773;&#24212;&#23545;&#30149;&#24773;&#65288;&#30142;&#30149;&#36827;&#23637;&#12289;&#27835;&#30103;&#21453;&#24212;&#31561;&#65289;&#30340;&#24773;&#20917;&#65292;&#20197;&#34917;&#20805;&#20256;&#32479;&#21307;&#30103;&#29615;&#22659;&#20013;&#30340;&#27835;&#30103;&#12290;&#23884;&#20837;&#21644;&#36830;&#25509;&#20256;&#24863;&#22120;&#30340;&#26234;&#33021;&#25163;&#26426;&#36890;&#36807;&#21508;&#31181;&#24212;&#29992;&#21644;&#31227;&#21160;&#20581;&#24247;&#65288;mHealth&#65289;&#24179;&#21488;&#20855;&#26377;&#24040;&#22823;&#30340;&#25913;&#36827;&#21307;&#30103;&#20445;&#20581;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#36828;&#31243;&#25910;&#38598;&#24739;&#32773;&#30340;&#38271;&#26399;&#32437;&#21521;&#25968;&#25454;&#26469;&#24320;&#21457;&#21487;&#38752;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;RADAR-base&#65292;&#20197;&#25903;&#25345;&#36828;&#31243;&#30417;&#27979;&#30740;&#31350;&#20013;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#25910;&#38598;&#12290;RADAR-base&#26159;&#19968;&#20010;&#29616;&#20195;&#21270;&#30340;&#36828;&#31243;&#25968;&#25454;&#25910;&#38598;&#24179;&#21488;&#65292;&#22522;&#20110;Confluent&#30340;Apache Kafka&#26500;&#24314;&#65292;&#25903;&#25345;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#25299;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#24615;&#21644;&#25968;&#25454;&#36136;&#37327;&#12290;&#23427;&#25552;&#20379;&#20102;&#30740;&#31350;&#35774;&#35745;&#21644;&#35774;&#32622;&#12289;&#20027;&#21160;&#65288;&#22914;&#24739;&#32773;&#25253;&#21578;&#30340;&#27979;&#37327;&#65289;&#21644;&#34987;&#21160;&#65288;&#22914;&#25163;&#26426;&#20256;&#24863;&#22120;&#12289;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#65289;&#36828;&#31243;&#25968;&#25454;&#25910;&#38598;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital Biomarkers and remote patient monitoring can provide valuable and timely insights into how a patient is coping with their condition (disease progression, treatment response, etc.), complementing treatment in traditional healthcare settings.Smartphones with embedded and connected sensors have immense potential for improving healthcare through various apps and mHealth (mobile health) platforms. This capability could enable the development of reliable digital biomarkers from long-term longitudinal data collected remotely from patients. We built an open-source platform, RADAR-base, to support large-scale data collection in remote monitoring studies. RADAR-base is a modern remote data collection platform built around Confluent's Apache Kafka, to support scalability, extensibility, security, privacy and quality of data. It provides support for study design and set-up, active (eg PROMs) and passive (eg. phone sensors, wearable devices and IoT) remote data collection capabilities with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22238;&#39038;&#36807;&#21435;&#20004;&#24180;&#21457;&#34920;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25991;&#26412;&#25366;&#25496;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#25429;&#25417;&#20102;&#20844;&#20849;&#37096;&#38376;&#20013;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#21644;&#24212;&#29992;&#30340;&#25968;&#25454;&#38544;&#31169;&#12289;&#20262;&#29702;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#20844;&#24179;&#24615;&#31561;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#65292;&#20844;&#24179;&#24615;&#26159;&#26368;&#39057;&#32321;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#32780;&#25968;&#25454;&#38544;&#31169;&#26159;&#26368;&#19981;&#31361;&#20986;&#30340;&#35805;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02042</link><description>&lt;p&gt;
&#22312;&#20844;&#20849;&#37096;&#38376;&#20013;&#25903;&#25345;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25991;&#26412;&#25366;&#25496;&#30340;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#21644;&#24212;&#29992;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#30340;&#31616;&#35201;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A short review of the main concerns in A.I. development and application within the public sector supported by NLP and TM. (arXiv:2308.02042v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22238;&#39038;&#36807;&#21435;&#20004;&#24180;&#21457;&#34920;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25991;&#26412;&#25366;&#25496;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#25429;&#25417;&#20102;&#20844;&#20849;&#37096;&#38376;&#20013;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#21644;&#24212;&#29992;&#30340;&#25968;&#25454;&#38544;&#31169;&#12289;&#20262;&#29702;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#20844;&#24179;&#24615;&#31561;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#65292;&#20844;&#24179;&#24615;&#26159;&#26368;&#39057;&#32321;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#32780;&#25968;&#25454;&#38544;&#31169;&#26159;&#26368;&#19981;&#31361;&#20986;&#30340;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24182;&#19981;&#26159;&#19968;&#20010;&#26032;&#30340;&#35838;&#39064;&#65292;&#21830;&#19994;&#12289;&#24037;&#19994;&#21644;&#20844;&#20849;&#37096;&#38376;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#21644;&#32972;&#26223;&#20351;&#29992;&#23427;&#65292;&#32771;&#34385;&#22810;&#20010;&#20851;&#27880;&#28857;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;ACM Digital Library&#21644;IEEE Xplore&#20250;&#35758;&#35770;&#25991;&#38598;&#20013;&#36807;&#21435;&#20004;&#24180;&#21457;&#34920;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#22238;&#39038;&#65292;&#24182;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25991;&#26412;&#25366;&#25496;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#26088;&#22312;&#25429;&#25417;&#20844;&#20849;&#37096;&#38376;&#20013;&#25968;&#25454;&#38544;&#31169;&#12289;&#20262;&#29702;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#20844;&#24179;&#30340;&#27934;&#23519;&#12290;&#35813;&#26041;&#27861;&#33410;&#30465;&#20102;&#20998;&#26512;&#26102;&#38388;&#65292;&#24182;&#33021;&#26816;&#32034;&#21040;&#21253;&#21547;&#30456;&#20851;&#20449;&#24687;&#30340;&#35770;&#25991;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20844;&#24179;&#24615;&#26159;&#26368;&#39057;&#32321;&#24341;&#36215;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#26368;&#19981;&#31361;&#20986;&#30340;&#35805;&#39064;&#26159;&#25968;&#25454;&#38544;&#31169;&#65288;&#23613;&#31649;&#23884;&#20837;&#22312;&#22823;&#22810;&#25968;&#25991;&#31456;&#20013;&#65289;&#65292;&#32780;&#26368;&#31361;&#20986;&#30340;&#26159;&#21487;&#20449;&#24230;&#12290;&#26368;&#21518;&#65292;&#20063;&#25104;&#21151;&#22320;&#33719;&#24471;&#20102;&#26377;&#20851;&#20844;&#20849;&#37096;&#38376;&#20013;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30456;&#20851;&#38382;&#39064;&#30340;&#26377;&#30410;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence is not a new subject, and business, industry and public sectors have used it in different ways and contexts and considering multiple concerns. This work reviewed research papers published in ACM Digital Library and IEEE Xplore conference proceedings in the last two years supported by fundamental concepts of Natural Language Processing (NLP) and Text Mining (TM). The objective was to capture insights regarding data privacy, ethics, interpretability, explainability, trustworthiness, and fairness in the public sector. The methodology has saved analysis time and could retrieve papers containing relevant information. The results showed that fairness was the most frequent concern. The least prominent topic was data privacy (although embedded in most articles), while the most prominent was trustworthiness. Finally, gathering helpful insights about those concerns regarding A.I. applications in the public sector was also possible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36816;&#29992;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#35265;&#35299;&#65292;&#28548;&#28165;&#20102;&#27431;&#30431;AI&#27861;&#26696;&#20013;&#30340;&#26415;&#35821;&#24182;&#25552;&#39640;&#20102;&#20445;&#25252;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02041</link><description>&lt;p&gt;
&#35843;&#33410;AI&#25805;&#32437;&#65306;&#36816;&#29992;&#34892;&#20026;&#32463;&#27982;&#23398;&#21644;&#24515;&#29702;&#23398;&#30340;&#35265;&#35299;&#22686;&#24378;&#27431;&#30431;AI&#27861;&#26696;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Regulating AI manipulation: Applying Insights from behavioral economics and psychology to enhance the practicality of the EU AI Act. (arXiv:2308.02041v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36816;&#29992;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#35265;&#35299;&#65292;&#28548;&#28165;&#20102;&#27431;&#30431;AI&#27861;&#26696;&#20013;&#30340;&#26415;&#35821;&#24182;&#25552;&#39640;&#20102;&#20445;&#25252;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;AI&#27861;&#26696;&#31532;5&#26465;&#26088;&#22312;&#35843;&#33410;AI&#25805;&#32437;&#65292;&#20197;&#38450;&#27490;&#28508;&#22312;&#30340;&#26377;&#23475;&#21518;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26415;&#35821;&#27169;&#31946;&#21644;&#25805;&#32437;&#25216;&#26415;&#34920;&#36848;&#19981;&#28165;&#26224;&#65292;&#36825;&#39033;&#31435;&#27861;&#30340;&#23454;&#38469;&#25191;&#34892;&#23384;&#22312;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#31532;5&#26465;&#20063;&#21463;&#21040;&#20445;&#25252;&#25928;&#26524;&#19981;&#36275;&#30340;&#25209;&#35780;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#25972;&#21512;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#35265;&#35299;&#65292;&#28548;&#28165;&#26415;&#35821;&#24182;&#25552;&#39640;&#20445;&#25252;&#25928;&#26524;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#36816;&#29992;&#35748;&#30693;&#24515;&#29702;&#23398;&#30740;&#31350;&#38416;&#26126;&#28508;&#24847;&#35782;&#25216;&#24039;&#21450;&#20854;&#30456;&#20851;&#34920;&#36848;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23558;&#34892;&#20026;&#32463;&#27982;&#23398;&#20013;&#21487;&#20197;&#24341;&#21457;&#34892;&#20026;&#21464;&#21270;&#30340;&#19968;&#32452;&#24605;&#32500;&#24555;&#25463;&#26041;&#24335;&#65292;&#21363;&#21551;&#21457;&#24335;&#65292;&#25193;&#23637;&#21040;&#25805;&#32437;&#25216;&#26415;&#39046;&#22495;&#12290;&#26415;&#35821;&#30340;&#38416;&#26126;&#21644;&#25193;&#23637;&#19981;&#20165;&#25552;&#20379;&#20102;&#23545;&#27861;&#24459;&#35268;&#23450;&#26356;&#20934;&#30830;&#30340;&#29702;&#35299;&#65292;&#36824;&#22686;&#24378;&#20102;&#20854;&#20445;&#25252;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The EU AI Act Article 5 is designed to regulate AI manipulation to prevent potential harmful consequences. However, the practical implementation of this legislation is challenging due to the ambiguous terminologies and the unclear presentations of manipulative techniques. Moreover, the Article 5 also suffers criticize of inadequate protective efficacy. This paper attempts to clarify terminologies and to enhance the protective efficacy by integrating insights from psychology and behavioral economics. Firstly, this paper employs cognitive psychology research to elucidate the term subliminal techniques and its associated representation. Additionally, this paper extends the study of heuristics: a set of thinking shortcuts which can be aroused for behavior changing from behavior economics to the realm of manipulative techniques. The elucidation and expansion of terminologies not only provide a more accurate understanding of the legal provision but also enhance its protective efficacy. Secon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#36716;&#25442;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21327;&#20316;&#23398;&#20064;&#20013;&#35780;&#20272;&#21644;&#39044;&#27979;&#23398;&#29983;&#30340;&#32489;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.02038</link><description>&lt;p&gt;
CLGT: &#29992;&#20110;&#21327;&#20316;&#23398;&#20064;&#20013;&#23398;&#29983;&#32489;&#25928;&#39044;&#27979;&#30340;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
CLGT: A Graph Transformer for Student Performance Prediction in Collaborative Learning. (arXiv:2308.02038v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#36716;&#25442;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21327;&#20316;&#23398;&#20064;&#20013;&#35780;&#20272;&#21644;&#39044;&#27979;&#23398;&#29983;&#30340;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21327;&#20316;&#23398;&#20064;&#33539;&#24335;&#20013;&#23398;&#29983;&#30340;&#32489;&#25928;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#21327;&#20316;&#23398;&#20064;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35752;&#35770;&#35770;&#22363;&#21644;&#31038;&#20132;&#23398;&#20064;&#32593;&#32476;&#19978;&#12290;&#21482;&#26377;&#24456;&#23569;&#30340;&#30740;&#31350;&#28041;&#21450;&#23398;&#29983;&#22312;&#22242;&#38431;&#39033;&#30446;&#20013;&#22914;&#20309;&#30456;&#20114;&#20132;&#20114;&#20197;&#21450;&#36825;&#31181;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#20182;&#20204;&#30340;&#23398;&#26415;&#34920;&#29616;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#36873;&#25321;&#19968;&#20010;&#36719;&#20214;&#24037;&#31243;&#35838;&#31243;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#12290;&#21442;&#19982;&#36719;&#20214;&#24037;&#31243;&#35838;&#31243;&#30340;&#23398;&#29983;&#38656;&#35201;&#32452;&#38431;&#23436;&#25104;&#19968;&#20010;&#36719;&#20214;&#39033;&#30446;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#22242;&#38431;&#30340;&#23398;&#29983;&#27963;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#22270;&#12290;&#22522;&#20110;&#36825;&#20010;&#23398;&#29983;&#20132;&#20114;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;&#21327;&#20316;&#23398;&#20064;&#22270;&#36716;&#25442;&#22120;&#26694;&#26550;&#65288;CLGT&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#39044;&#27979;&#23398;&#29983;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;CLGT&#21253;&#21547;&#19968;&#20010;&#35299;&#37322;&#27169;&#22359;&#65292;&#29992;&#20110;&#35299;&#37322;&#23398;&#29983;&#30340;&#32489;&#25928;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and predicting the performance of students in collaborative learning paradigms is an important task. Most of the research presented in literature regarding collaborative learning focuses on the discussion forums and social learning networks. There are only a few works that investigate how students interact with each other in team projects and how such interactions affect their academic performance. In order to bridge this gap, we choose a software engineering course as the study subject. The students who participate in a software engineering course are required to team up and complete a software project together. In this work, we construct an interaction graph based on the activities of students grouped in various teams. Based on this student interaction graph, we present an extended graph transformer framework for collaborative learning (CLGT) for evaluating and predicting the performance of students. Moreover, the proposed CLGT contains an interpretation module that explains
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#39044;&#27979;&#20102;&#32654;&#22269;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#37327;&#30340;&#22686;&#38271;&#65292;&#24182;&#35780;&#20272;&#20102;&#24433;&#21709;&#30005;&#21160;&#33258;&#34892;&#36710;&#20351;&#29992;&#30340;&#22240;&#32032;&#12290;&#26681;&#25454;&#39044;&#27979;&#32467;&#26524;&#65292;&#39044;&#35745;2025&#24180;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#21806;&#37327;&#20026;130&#19975;&#36742;&#65292;2028&#24180;&#20026;211.3&#19975;&#36742;&#12290;</title><link>http://arxiv.org/abs/2308.02034</link><description>&lt;p&gt;
&#30005;&#21160;&#33258;&#34892;&#36710;&#20351;&#29992;&#30340;&#22686;&#38271;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Growth of E-Bike Use: A Machine Learning Approach. (arXiv:2308.02034v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#39044;&#27979;&#20102;&#32654;&#22269;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#37327;&#30340;&#22686;&#38271;&#65292;&#24182;&#35780;&#20272;&#20102;&#24433;&#21709;&#30005;&#21160;&#33258;&#34892;&#36710;&#20351;&#29992;&#30340;&#22240;&#32032;&#12290;&#26681;&#25454;&#39044;&#27979;&#32467;&#26524;&#65292;&#39044;&#35745;2025&#24180;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#21806;&#37327;&#20026;130&#19975;&#36742;&#65292;2028&#24180;&#20026;211.3&#19975;&#36742;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30005;&#21160;&#33258;&#34892;&#36710;&#65288;&#30005;&#21333;&#36710;&#65289;&#21450;&#20854;&#23545;&#32654;&#22269;&#25919;&#31574;&#21046;&#23450;&#32773;&#30340;&#24433;&#21709;&#12290;&#30005;&#21160;&#33258;&#34892;&#36710;&#20316;&#20026;&#19968;&#31181;&#24555;&#36895;&#21644;&#29615;&#20445;&#30340;&#20132;&#36890;&#36873;&#25321;&#65292;&#24050;&#32463;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#27969;&#34892;&#12290;&#22312;&#25105;&#20204;&#36861;&#27714;&#21487;&#25345;&#32493;&#33021;&#28304;&#35745;&#21010;&#30340;&#36807;&#31243;&#20013;&#65292;&#20102;&#35299;&#30005;&#21160;&#33258;&#34892;&#36710;&#30340;&#22686;&#38271;&#21644;&#24433;&#21709;&#23545;&#25919;&#31574;&#21046;&#23450;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#25968;&#23398;&#27169;&#22411;&#20026;&#30005;&#21160;&#33258;&#34892;&#36710;&#30340;&#20215;&#20540;&#21644;&#20854;&#22312;&#26410;&#26469;&#30340;&#35282;&#33394;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#25105;&#20204;&#20351;&#29992;ARIMA&#27169;&#22411;&#65292;&#19968;&#31181;&#30417;&#30563;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#39044;&#27979;&#20102;&#32654;&#22269;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#37327;&#30340;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#20174;2006&#24180;1&#26376;&#21040;2022&#24180;12&#26376;&#30340;&#21382;&#21490;&#38144;&#21806;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#39044;&#27979;2025&#24180;&#38144;&#21806;&#37327;&#20026;130&#19975;&#36742;&#65292;2028&#24180;&#38144;&#21806;&#37327;&#20026;211.3&#19975;&#36742;&#12290;&#20026;&#20102;&#35780;&#20272;&#24433;&#21709;&#30005;&#21160;&#33258;&#34892;&#36710;&#20351;&#29992;&#30340;&#22240;&#32032;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#27169;&#22411;&#12290;&#24433;&#21709;&#30005;&#21160;&#33258;&#34892;&#36710;&#38144;&#37327;&#22686;&#38271;&#26368;&#26174;&#33879;&#30340;&#22240;&#32032;&#26159;&#21487;&#25903;&#37197;&#20010;&#20154;&#25910;&#20837;&#21644;&#21463;&#27426;&#36814;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#30005;&#21160;&#33258;&#34892;&#36710;&#23545;&#29615;&#22659;&#21644;&#20581;&#24247;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our work on electric bicycles (e-bikes) and their implications for policymakers in the United States. E-bikes have gained significant popularity as a fast and eco-friendly transportation option. As we strive for a sustainable energy plan, understanding the growth and impact of e-bikes is crucial for policymakers. Our mathematical modeling offers insights into the value of e-bikes and their role in the future. Using an ARIMA model, a supervised machine-learning algorithm, we predicted the growth of e-bike sales in the U.S. Our model, trained on historical sales data from January 2006 to December 2022, projected sales of 1.3 million units in 2025 and 2.113 million units in 2028. To assess the factors contributing to e-bike usage, we employed a Random Forest regression model. The most significant factors influencing e-bike sales growth were disposable personal income and popularity. Furthermore, we examined the environmental and health impacts of e-bikes. Through Monte Carlo si
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#24212;&#23545;&#29983;&#25104;&#24335;AI&#20013;&#30340;&#24040;&#22836;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#26576;&#20123;AI&#36719;&#20214;&#20316;&#20026;&#26680;&#24515;&#24179;&#21488;&#26381;&#21153;&#65292;&#23558;&#26576;&#20123;&#24320;&#21457;&#32773;&#20998;&#31867;&#20026;&#23432;&#38376;&#20154;&#65292;&#24182;&#23545;&#30456;&#20851;&#20041;&#21153;&#36827;&#34892;&#35780;&#20272;&#30340;&#24314;&#35758;&#12290;&#36825;&#26377;&#21161;&#20110;&#27431;&#30431;&#22312;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#20013;&#32500;&#25252;&#22810;&#26679;&#24615;&#21644;&#24320;&#25918;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02033</link><description>&lt;p&gt;
AI&#19982;&#27431;&#30431;&#25968;&#23383;&#24066;&#22330;&#27861;&#26696;&#65306;&#24212;&#23545;&#29983;&#25104;&#24335;AI&#20013;&#30340;&#24040;&#22836;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
AI and the EU Digital Markets Act: Addressing the Risks of Bigness in Generative AI. (arXiv:2308.02033v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#24212;&#23545;&#29983;&#25104;&#24335;AI&#20013;&#30340;&#24040;&#22836;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#26576;&#20123;AI&#36719;&#20214;&#20316;&#20026;&#26680;&#24515;&#24179;&#21488;&#26381;&#21153;&#65292;&#23558;&#26576;&#20123;&#24320;&#21457;&#32773;&#20998;&#31867;&#20026;&#23432;&#38376;&#20154;&#65292;&#24182;&#23545;&#30456;&#20851;&#20041;&#21153;&#36827;&#34892;&#35780;&#20272;&#30340;&#24314;&#35758;&#12290;&#36825;&#26377;&#21161;&#20110;&#27431;&#30431;&#22312;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#20013;&#32500;&#25252;&#22810;&#26679;&#24615;&#21644;&#24320;&#25918;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23545;&#25968;&#23383;&#24066;&#22330;&#24040;&#22836;&#39118;&#38505;&#30340;&#20851;&#27880;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#27431;&#30431;&#30340;&#25968;&#23383;&#24066;&#22330;&#27861;&#26696;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26694;&#26550;&#21487;&#33021;&#26410;&#33021;&#20805;&#20998;&#35206;&#30422;&#21487;&#33021;&#25104;&#20026;AI&#26381;&#21153;&#20837;&#21475;&#30340;&#29983;&#25104;&#24335;AI&#31995;&#32479;&#12290;&#26412;&#25991;&#20027;&#24352;&#23558;&#26576;&#20123;AI&#36719;&#20214;&#25972;&#21512;&#20026;&#26680;&#24515;&#24179;&#21488;&#26381;&#21153;&#65292;&#24182;&#23558;&#26576;&#20123;&#24320;&#21457;&#32773;&#20998;&#31867;&#20026;&#25968;&#23383;&#24066;&#22330;&#27861;&#26696;&#30340;&#23432;&#38376;&#20154;&#12290;&#25105;&#20204;&#36824;&#25552;&#35758;&#23545;&#23432;&#38376;&#20154;&#20041;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#28085;&#30422;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#12290;&#38543;&#30528;&#27431;&#30431;&#32771;&#34385;&#29983;&#25104;&#24335;AI&#29305;&#23450;&#35268;&#21017;&#21644;&#21487;&#33021;&#30340;&#25968;&#23383;&#24066;&#22330;&#27861;&#26696;&#20462;&#27491;&#26696;&#65292;&#26412;&#25991;&#20026;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#30340;&#22810;&#26679;&#21270;&#21644;&#24320;&#25918;&#24615;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI technology advances rapidly, concerns over the risks of bigness in digital markets are also growing. The EU's Digital Markets Act (DMA) aims to address these risks. Still, the current framework may not adequately cover generative AI systems that could become gateways for AI-based services. This paper argues for integrating certain AI software as core platform services and classifying certain developers as gatekeepers under the DMA. We also propose an assessment of gatekeeper obligations to ensure they cover generative AI services. As the EU considers generative AI-specific rules and possible DMA amendments, this paper provides insights towards diversity and openness in generative AI services.
&lt;/p&gt;</description></item><item><title>JusticeBot&#26159;&#19968;&#31181;&#20026;&#38750;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#25552;&#20379;&#27861;&#24459;&#20915;&#31574;&#25903;&#25345;&#30340;&#22686;&#24378;&#26234;&#33021;&#24037;&#20855;&#65292;&#36890;&#36807;&#28151;&#21512;&#26696;&#20363;&#21644;&#35268;&#21017;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#20182;&#20204;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#27861;&#24459;&#26435;&#21033;&#65292;&#24182;&#25552;&#20379;&#30456;&#20851;&#30340;&#27861;&#24459;&#20449;&#24687;&#21644;&#26696;&#20363;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2308.02032</link><description>&lt;p&gt;
JusticeBot&#65306;&#19968;&#31181;&#20026;&#38750;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#26500;&#24314;&#22686;&#24378;&#26234;&#33021;&#24037;&#20855;&#20197;&#22686;&#21152;&#20844;&#20247;&#33719;&#21462;&#21496;&#27861;&#36164;&#28304;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
JusticeBot: A Methodology for Building Augmented Intelligence Tools for Laypeople to Increase Access to Justice. (arXiv:2308.02032v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02032
&lt;/p&gt;
&lt;p&gt;
JusticeBot&#26159;&#19968;&#31181;&#20026;&#38750;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#25552;&#20379;&#27861;&#24459;&#20915;&#31574;&#25903;&#25345;&#30340;&#22686;&#24378;&#26234;&#33021;&#24037;&#20855;&#65292;&#36890;&#36807;&#28151;&#21512;&#26696;&#20363;&#21644;&#35268;&#21017;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#20182;&#20204;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#27861;&#24459;&#26435;&#21033;&#65292;&#24182;&#25552;&#20379;&#30456;&#20851;&#30340;&#27861;&#24459;&#20449;&#24687;&#21644;&#26696;&#20363;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#24120;&#24120;&#22312;&#35299;&#20915;&#27861;&#24459;&#38382;&#39064;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JusticeBot&#26041;&#27861;&#35770;&#12290;&#35813;&#26041;&#27861;&#35770;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#27861;&#24459;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20197;&#28151;&#21512;&#26696;&#20363;&#21644;&#35268;&#21017;&#25512;&#29702;&#30340;&#26041;&#24335;&#24110;&#21161;&#38750;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#25506;&#35752;&#20182;&#20204;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#27861;&#24459;&#26435;&#21033;&#12290;&#31995;&#32479;&#20250;&#35810;&#38382;&#29992;&#25143;&#19982;&#20182;&#20204;&#24773;&#20917;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#20182;&#20204;&#25552;&#20379;&#27861;&#24459;&#20449;&#24687;&#12289;&#21442;&#32771;&#20197;&#21069;&#31867;&#20284;&#26696;&#20363;&#21644;&#21487;&#33021;&#30340;&#19979;&#19968;&#27493;&#34892;&#21160;&#12290;&#36825;&#20123;&#20449;&#24687;&#21487;&#33021;&#26377;&#21161;&#20110;&#29992;&#25143;&#35299;&#20915;&#38382;&#39064;&#65292;&#20363;&#22914;&#36890;&#36807;&#21644;&#35299;&#26696;&#20214;&#25110;&#22312;&#27861;&#24237;&#19978;&#32500;&#25252;&#33258;&#24049;&#30340;&#26435;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26500;&#24314;&#27492;&#31867;&#24037;&#20855;&#30340;&#26041;&#27861;&#35770;&#65292;&#21253;&#25324;&#20174;&#27861;&#35268;&#21644;&#26696;&#20363;&#27861;&#20013;&#21457;&#29616;&#36890;&#24120;&#36866;&#29992;&#30340;&#27861;&#24459;&#35268;&#21017;&#65292;&#24182;&#23545;&#20197;&#21069;&#30340;&#26696;&#20363;&#36827;&#34892;&#32534;&#30721;&#20197;&#25903;&#25345;&#29992;&#25143;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20351;&#29992;&#35813;&#26041;&#27861;&#35770;&#26500;&#24314;&#24037;&#20855;&#30340;&#30028;&#38754;&#65292;&#24182;&#23637;&#31034;&#20102;&#39318;&#20010;&#37096;&#32626;&#30340;JusticeBot&#29256;&#26412;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#22303;&#22320;&#31199;&#36161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Laypeople (i.e. individuals without legal training) may often have trouble resolving their legal problems. In this work, we present the JusticeBot methodology. This methodology can be used to build legal decision support tools, that support laypeople in exploring their legal rights in certain situations, using a hybrid case-based and rule-based reasoning approach. The system ask the user questions regarding their situation and provides them with legal information, references to previous similar cases and possible next steps. This information could potentially help the user resolve their issue, e.g. by settling their case or enforcing their rights in court. We present the methodology for building such tools, which consists of discovering typically applied legal rules from legislation and case law, and encoding previous cases to support the user. We also present an interface to build tools using this methodology and a case study of the first deployed JusticeBot version, focused on landlo
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22686;&#24378;&#30340;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#30693;&#35782;&#22270;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.02031</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy. (arXiv:2308.02031v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02031
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#30693;&#35782;&#22270;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26032;&#20852;&#19988;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#23427;&#23558;&#65288;&#28145;&#24230;&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#23376;&#31526;&#21495;&#20248;&#21183;&#19982;&#30693;&#35782;&#22270;&#20013;&#30340;&#26174;&#24335;&#31526;&#21495;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#24403;&#21069;&#19968;&#20195;&#31995;&#32479;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#23427;&#20204;&#26080;&#27861;&#20026;&#20854;&#32467;&#26524;&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#23384;&#22312;&#8220;&#26410;&#30693;&#26410;&#30693;&#8221;&#65288;&#20363;&#22914;&#32593;&#32476;&#23433;&#20840;&#12289;&#38544;&#31169;&#65289;&#24773;&#20917;&#19979;&#30830;&#20445;&#23433;&#20840;&#34892;&#20026;&#12290;&#31070;&#32463;&#32593;&#32476;&#65288;&#25797;&#38271;&#25506;&#32034;&#22797;&#26434;&#25968;&#25454;&#31354;&#38388;&#65289;&#21644;&#31526;&#21495;&#30693;&#35782;&#22270;&#30340;&#25972;&#21512;&#65288;&#20195;&#34920;&#39046;&#22495;&#30693;&#35782;&#65289;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#20197;&#19987;&#23478;&#21487;&#29702;&#35299;&#30340;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#12289;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#22312;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#24182;&#19988;&#36825;&#20004;&#20010;&#39046;&#22495;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#38656;&#27714;&#26368;&#22823;&#65292;&#21487;&#20197;&#20174;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Artificial Intelligence (AI) is an emerging and quickly advancing field that combines the subsymbolic strengths of (deep) neural networks and explicit, symbolic knowledge contained in knowledge graphs to enhance explainability and safety in AI systems. This approach addresses a key criticism of current generation systems, namely their inability to generate human-understandable explanations for their outcomes and ensure safe behaviors, especially in scenarios with \textit{unknown unknowns} (e.g. cybersecurity, privacy). The integration of neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows AI systems to reason, learn, and generalize in a manner understandable to experts. This article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for AI to be explainable while being highly accurate in complex environments, can benefit from Neuro-Symb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;&#20449;&#24687;&#27969;&#20998;&#31867;&#19979;&#20010;&#20307;&#23545;AI&#30340;&#30475;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#21442;&#19982;&#32773;&#20851;&#27880;AI&#23545;&#23601;&#19994;&#12289;&#38544;&#31169;&#21644;&#20449;&#24687;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#20063;&#35748;&#35782;&#21040;AI&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#21644;&#22686;&#21152;&#20415;&#21033;&#12290;&#23545;&#20110;&#25919;&#24220;&#22312;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#20013;&#30340;&#35282;&#33394;&#65292;&#24847;&#35265;&#21508;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.02030</link><description>&lt;p&gt;
&#23545;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#21644;&#20154;&#24037;&#26234;&#33021;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#30340;&#30475;&#27861;
&lt;/p&gt;
&lt;p&gt;
Perceptions of the Fourth Industrial Revolution and Artificial Intelligence Impact on Society. (arXiv:2308.02030v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;&#20449;&#24687;&#27969;&#20998;&#31867;&#19979;&#20010;&#20307;&#23545;AI&#30340;&#30475;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#21442;&#19982;&#32773;&#20851;&#27880;AI&#23545;&#23601;&#19994;&#12289;&#38544;&#31169;&#21644;&#20449;&#24687;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#20063;&#35748;&#35782;&#21040;AI&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#21644;&#22686;&#21152;&#20415;&#21033;&#12290;&#23545;&#20110;&#25919;&#24220;&#22312;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#20013;&#30340;&#35282;&#33394;&#65292;&#24847;&#35265;&#21508;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#65292;&#23588;&#20854;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#23545;&#31038;&#20250;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;&#24433;&#21709;&#21644;&#20262;&#29702;&#32771;&#34385;&#30340;&#25285;&#24551;&#12290;&#25991;&#26412;&#29983;&#25104;AI&#24037;&#20855;&#65288;&#22914;ChatGPT&#65289;&#30340;&#20986;&#29616;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#20154;&#20204;&#23545;&#20262;&#29702;&#12289;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#29256;&#26435;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#19981;&#21516;&#20449;&#24687;&#27969;&#20998;&#31867;&#30340;&#20010;&#20307;&#23545;AI&#30340;&#30475;&#27861;&#12290;&#32467;&#26524;&#25581;&#31034;&#20102;&#21442;&#19982;&#32773;&#25552;&#20379;&#30340;AI&#21644;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#30340;&#23450;&#20041;&#20013;&#30340;&#20851;&#38190;&#20027;&#39064;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#22797;&#21046;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#33258;&#21160;&#21270;&#21644;&#25968;&#23383;&#25216;&#26415;&#30340;&#25972;&#21512;&#12290;&#21442;&#19982;&#32773;&#23545;AI&#26367;&#20195;&#24037;&#20316;&#23703;&#20301;&#12289;&#20405;&#29359;&#38544;&#31169;&#20197;&#21450;AI&#25552;&#20379;&#30340;&#19981;&#20934;&#30830;&#20449;&#24687;&#34920;&#31034;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#20063;&#35748;&#35782;&#21040;AI&#30340;&#22909;&#22788;&#65292;&#22914;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#21644;&#22686;&#21152;&#20415;&#21033;&#12290;&#20851;&#20110;&#25919;&#24220;&#22312;&#22609;&#36896;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#26041;&#38754;&#30340;&#21442;&#19982;&#65292;&#24847;&#35265;&#19981;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fourth Industrial Revolution, particularly Artificial Intelligence (AI), has had a profound impact on society, raising concerns about its implications and ethical considerations. The emergence of text generative AI tools like ChatGPT has further intensified concerns regarding ethics, security, privacy, and copyright. This study aims to examine the perceptions of individuals in different information flow categorizations toward AI. The results reveal key themes in participant-supplied definitions of AI and the fourth industrial revolution, emphasizing the replication of human intelligence, machine learning, automation, and the integration of digital technologies. Participants expressed concerns about job replacement, privacy invasion, and inaccurate information provided by AI. However, they also recognized the benefits of AI, such as solving complex problems and increasing convenience. Views on government involvement in shaping the fourth industrial revolution varied, with some advoc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;STT-MRAM&#20316;&#20026;&#35757;&#32451;&#21152;&#36895;&#22120;&#20013;&#30340;Scratchpad&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#23427;&#20855;&#26377;&#39640;&#23494;&#24230;&#12289;&#20302;&#27844;&#28431;&#21151;&#29575;&#21644;&#21512;&#29702;&#30340;&#35775;&#38382;&#26102;&#38388;&#31561;&#20248;&#28857;&#65292;&#20294;&#20889;&#25805;&#20316;&#38656;&#35201;&#36739;&#39640;&#30340;&#33021;&#37327;&#21644;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2308.02024</link><description>&lt;p&gt;
STT-MRAM&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#20013;&#30340;Scratchpad&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of STT-MRAM as a Scratchpad for Training in ML Accelerators. (arXiv:2308.02024v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;STT-MRAM&#20316;&#20026;&#35757;&#32451;&#21152;&#36895;&#22120;&#20013;&#30340;Scratchpad&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#23427;&#20855;&#26377;&#39640;&#23494;&#24230;&#12289;&#20302;&#27844;&#28431;&#21151;&#29575;&#21644;&#21512;&#29702;&#30340;&#35775;&#38382;&#26102;&#38388;&#31561;&#20248;&#28857;&#65292;&#20294;&#20889;&#25805;&#20316;&#38656;&#35201;&#36739;&#39640;&#30340;&#33021;&#37327;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#26159;&#30001;&#20110;&#35757;&#32451;&#26356;&#22823;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#33021;&#21147;&#25512;&#21160;&#30340;&#65292;&#36825;&#23548;&#33268;&#20102;&#36828;&#36828;&#36229;&#36807;&#25705;&#23572;&#23450;&#24459;&#25552;&#20379;&#30340;&#30828;&#20214;&#24615;&#33021;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#35757;&#32451;DNNs&#26159;&#19968;&#20010;&#26497;&#20854;&#20869;&#23384;&#23494;&#38598;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#23384;&#20648;&#25972;&#20010;minibatch&#30340;&#27169;&#22411;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;&#26799;&#24230;&#12290;&#25552;&#20379;&#39640;&#23494;&#24230;&#21644;&#20302;&#27844;&#28431;&#30340;&#29255;&#19978;&#23384;&#20648;&#22120;&#30340;&#38656;&#27714;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#26032;&#20852;&#30340;&#38750;&#26131;&#22833;&#24615;&#23384;&#20648;&#22120;&#29992;&#20110;&#35757;&#32451;&#21152;&#36895;&#22120;&#12290;&#33258;&#26059;&#36716;&#30697;&#20256;&#36755;&#30913;&#38459;&#23384;&#20648;&#22120;(STT-MRAM)&#22312;&#35757;&#32451;&#21152;&#36895;&#22120;&#20013;&#20855;&#26377;&#20960;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#27604;SRAM&#39640;3-4&#20493;&#30340;&#23494;&#24230;&#65292;&#22823;&#22823;&#38477;&#20302;&#30340;&#27844;&#28431;&#21151;&#29575;&#65292;&#39640;&#32784;&#20037;&#24615;&#21644;&#21512;&#29702;&#30340;&#35775;&#38382;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;MRAM&#30340;&#20889;&#25805;&#20316;&#30001;&#20110;&#38656;&#35201;&#30830;&#20445;&#21487;&#38752;&#20999;&#25442;&#65292;&#20250;&#20135;&#29983;&#39640;&#20889;&#33021;&#37327;&#21644;&#24310;&#36831;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20174;&#22120;&#20214;&#21040;&#31995;&#32479;&#30340;&#35780;&#20272;&#21644;&#21327;&#21516;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in artificial intelligence and machine learning over the past decade has been driven by the ability to train larger deep neural networks (DNNs), leading to a compute demand that far exceeds the growth in hardware performance afforded by Moore's law. Training DNNs is an extremely memory-intensive process, requiring not just the model weights but also activations and gradients for an entire minibatch to be stored. The need to provide high-density and low-leakage on-chip memory motivates the exploration of emerging non-volatile memory for training accelerators. Spin-Transfer-Torque MRAM (STT-MRAM) offers several desirable properties for training accelerators, including 3-4x higher density than SRAM, significantly reduced leakage power, high endurance and reasonable access time. On the one hand, MRAM write operations require high write energy and latency due to the need to ensure reliable switching.  In this study, we perform a comprehensive device-to-system evaluation and co-opti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#24605;&#32500;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23398;&#20064;&#36807;&#28193;&#34920;&#31034;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21338;&#24328;&#21644;&#24378;&#21270;&#23398;&#20064;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#20449;&#24687;&#30340;&#21387;&#32553;&#21644;&#31526;&#21495;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.02000</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#34920;&#31034;&#21040;&#31526;&#21495;&#30693;&#35782;&#30340;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
On the Transition from Neural Representation to Symbolic Knowledge. (arXiv:2308.02000v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#24605;&#32500;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23398;&#20064;&#36807;&#28193;&#34920;&#31034;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21338;&#24328;&#21644;&#24378;&#21270;&#23398;&#20064;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#20449;&#24687;&#30340;&#21387;&#32553;&#21644;&#31526;&#21495;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#21512;&#31070;&#32463;&#34920;&#31034;&#19982;&#31526;&#21495;&#34920;&#31034;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#21487;&#33021;&#20351;&#31526;&#21495;&#24605;&#32500;&#20174;&#26412;&#36136;&#19978;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#12290;&#21463;&#20154;&#31867;&#22914;&#20309;&#36880;&#28176;&#20174;&#36890;&#36807;&#30693;&#35273;&#21644;&#29615;&#22659;&#20132;&#20114;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#31526;&#21495;&#26500;&#24314;&#22797;&#26434;&#30340;&#31526;&#21495;&#34920;&#31034;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#65288;TDL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;EM&#31639;&#27861;&#23398;&#20064;&#25968;&#25454;&#30340;&#36807;&#28193;&#34920;&#31034;&#65292;&#23558;&#36755;&#20837;&#30340;&#39640;&#32500;&#35270;&#35273;&#37096;&#20998;&#20449;&#24687;&#21387;&#32553;&#21040;&#19968;&#32452;&#24352;&#37327;&#20316;&#20026;&#31070;&#32463;&#21464;&#37327;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36755;&#20837;&#20998;&#35299;&#35270;&#20026;&#21512;&#20316;&#21338;&#24328;&#26469;&#23454;&#29616;&#26694;&#26550;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#35859;&#35789;&#65292;&#24182;&#36890;&#36807;RL&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#36827;&#19968;&#27493;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#20197;&#34701;&#20837;&#20027;&#35266;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. Motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. We propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. We implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. We additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22312;&#32447;&#24066;&#22330;&#20013;&#25340;&#20889;&#38169;&#35823;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#22312;&#24494;&#36719;AppSource&#24066;&#22330;&#30340;&#23454;&#26102;&#25512;&#26029;API&#20013;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#25511;&#21046;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#24403;&#21069;&#22823;&#35821;&#35328;&#27169;&#22411;&#25152;&#20381;&#36182;&#30340;&#24040;&#22823;&#19988;&#38590;&#20197;&#25511;&#21046;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.01976</link><description>&lt;p&gt;
&#25340;&#20889;&#26816;&#26597;&#22120;&#22312;&#22312;&#32447;&#24066;&#22330;&#20013;&#30340;&#39046;&#22495;&#29305;&#24322;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65306;&#20197;&#22312;&#32447;&#24066;&#22330;&#25628;&#32034;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces. (arXiv:2308.01976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22312;&#32447;&#24066;&#22330;&#20013;&#25340;&#20889;&#38169;&#35823;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#22312;&#24494;&#36719;AppSource&#24066;&#22330;&#30340;&#23454;&#26102;&#25512;&#26029;API&#20013;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#25511;&#21046;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#24403;&#21069;&#22823;&#35821;&#35328;&#27169;&#22411;&#25152;&#20381;&#36182;&#30340;&#24040;&#22823;&#19988;&#38590;&#20197;&#25511;&#21046;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#32447;&#24066;&#22330;&#30340;&#39046;&#22495;&#29305;&#23450;&#24615;&#21644;&#29992;&#25143;&#30701;&#26597;&#35810;&#30340;&#29305;&#28857;&#65292;&#38169;&#23383;&#26159;&#22312;&#32447;&#24066;&#22330;&#35775;&#38382;&#32773;&#30340;&#20027;&#35201;&#22256;&#25200;&#12290;&#20256;&#32479;&#30340;&#25340;&#20889;&#26816;&#26597;&#35299;&#20915;&#26041;&#26696;&#22312;&#32416;&#27491;&#25340;&#20889;&#38169;&#35823;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25340;&#20889;&#38169;&#35823;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20102;&#19978;&#19979;&#25991;&#38480;&#21046;&#30340;&#39046;&#22495;&#29305;&#23450;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#34987;&#37096;&#32626;&#22312;&#24494;&#36719;AppSource&#24066;&#22330;&#30340;&#23454;&#26102;&#25512;&#26029;API&#20013;&#65292;&#20197;&#22312;&#38169;&#35823;&#25340;&#20889;&#30340;&#29992;&#25143;&#26597;&#35810;&#21644;&#21487;&#29992;&#20135;&#21697;&#21517;&#31216;&#20043;&#38388;&#25214;&#21040;&#26368;&#25509;&#36817;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#21463;&#21040;&#24403;&#21069;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25511;&#21046;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#24040;&#22823;&#19988;&#38590;&#20197;&#25511;&#21046;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#22270;&#34920;&#25968;&#25454;&#30340;&#26032;&#22411;&#33258;&#19979;&#32780;&#19978;&#26041;&#27861;&#12290;&#36890;&#36807;&#26816;&#27979;&#36830;&#32493;&#21644;&#31163;&#25955;&#30340;&#20851;&#38190;&#28857;&#26469;&#23454;&#29616;&#23545;&#32472;&#22270;&#21306;&#22495;&#20869;&#32452;&#20214;&#30340;&#37325;&#26500;&#65292;&#36827;&#32780;&#33719;&#21462;&#25968;&#25454;&#31995;&#21015;&#21517;&#31216;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01971</link><description>&lt;p&gt;
SpaDen: &#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#22270;&#34920;&#29702;&#35299;&#30340;&#31232;&#30095;&#21644;&#31264;&#23494;&#20851;&#38190;&#28857;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SpaDen : Sparse and Dense Keypoint Estimation for Real-World Chart Understanding. (arXiv:2308.01971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#22270;&#34920;&#25968;&#25454;&#30340;&#26032;&#22411;&#33258;&#19979;&#32780;&#19978;&#26041;&#27861;&#12290;&#36890;&#36807;&#26816;&#27979;&#36830;&#32493;&#21644;&#31163;&#25955;&#30340;&#20851;&#38190;&#28857;&#26469;&#23454;&#29616;&#23545;&#32472;&#22270;&#21306;&#22495;&#20869;&#32452;&#20214;&#30340;&#37325;&#26500;&#65292;&#36827;&#32780;&#33719;&#21462;&#25968;&#25454;&#31995;&#21015;&#21517;&#31216;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#19979;&#32780;&#19978;&#26041;&#27861;&#26469;&#25552;&#21462;&#22270;&#34920;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#22270;&#34920;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#23398;&#20064;&#26816;&#27979;&#20851;&#38190;&#28857;&#65288;KP&#65289;&#65292;&#29992;&#20110;&#37325;&#26500;&#32472;&#22270;&#21306;&#22495;&#20869;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#26816;&#27979;&#36830;&#32493;&#21644;&#31163;&#25955;KP&#30340;&#34701;&#21512;&#65292;&#20316;&#20026;&#39044;&#27979;&#30340;&#28909;&#22270;&#12290;&#25105;&#20204;&#24212;&#29992;&#31232;&#30095;&#21644;&#31264;&#23494;&#36880;&#20687;&#32032;&#30446;&#26631;&#30340;&#32452;&#21512;&#65292;&#32467;&#21512;&#21333;&#27169;&#24577;&#33258;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#23618;&#26469;&#23398;&#20064;KP&#23884;&#20837;&#12290;&#36827;&#19968;&#27493;&#21033;&#29992;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#21487;&#20197;&#23558;&#22270;&#34920;&#32472;&#22270;&#21306;&#22495;&#20998;&#21106;&#25104;&#21508;&#31181;&#23545;&#35937;&#12290;&#36890;&#36807;&#23558;&#22270;&#34920;&#32452;&#20214;&#19982;&#22270;&#20363;&#36827;&#34892;&#21305;&#37197;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#25968;&#25454;&#31995;&#21015;&#21517;&#31216;&#12290;&#23545;KP&#23884;&#20837;&#24212;&#29992;&#21518;&#22788;&#29702;&#38408;&#20540;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#23545;&#35937;&#30340;&#37325;&#26500;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21253;&#25324;&#35780;&#20272;&#19981;&#21516;&#27169;&#22359;&#29992;&#20110;KP&#20272;&#35745;&#20197;&#21450;&#28145;&#23618;&#32858;&#21512;&#21644;&#35282;&#28857;&#27744;&#21270;&#26041;&#27861;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel bottom-up approach for the extraction of chart data. Our model utilizes images of charts as inputs and learns to detect keypoints (KP), which are used to reconstruct the components within the plot area. Our novelty lies in detecting a fusion of continuous and discrete KP as predicted heatmaps. A combination of sparse and dense per-pixel objectives coupled with a uni-modal self-attention-based feature-fusion layer is applied to learn KP embeddings. Further leveraging deep metric learning for unsupervised clustering, allows us to segment the chart plot area into various objects. By further matching the chart components to the legend, we are able to obtain the data series names. A post-processing threshold is applied to the KP embeddings to refine the object reconstructions and improve accuracy. Our extensive experiments include an evaluation of different modules for KP estimation and the combination of deep layer aggregation and corner pooling approaches. The results
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#30340;&#21028;&#21035;&#24615;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#24322;&#24120;&#22270;&#20449;&#24687;&#21644;&#37319;&#29992;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#20449;&#24687;&#24046;&#26469;&#35782;&#21035;&#24322;&#24120;&#22270;&#65292;&#24182;&#24341;&#20837;&#25945;&#24072;&#27169;&#22411;&#21644;&#20004;&#20010;&#31454;&#20105;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01947</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#30340;&#21028;&#21035;&#24615;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Discriminative Graph-level Anomaly Detection via Dual-students-teacher Model. (arXiv:2308.01947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#30340;&#21028;&#21035;&#24615;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#24322;&#24120;&#22270;&#20449;&#24687;&#21644;&#37319;&#29992;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#20449;&#24687;&#24046;&#26469;&#35782;&#21035;&#24322;&#24120;&#22270;&#65292;&#24182;&#24341;&#20837;&#25945;&#24072;&#27169;&#22411;&#21644;&#20004;&#20010;&#31454;&#20105;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20110;&#24403;&#21069;&#30340;&#33410;&#28857;&#32423;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#65292;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30446;&#26631;&#26159;&#23547;&#25214;&#22312;&#22270;&#38598;&#20013;&#19982;&#20854;&#20182;&#22270;&#26174;&#33879;&#19981;&#21516;&#30340;&#24322;&#24120;&#22270;&#12290;&#30001;&#20110;&#23545;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#20851;&#20110;&#22270;&#32423;&#24322;&#24120;&#30340;&#35814;&#32454;&#25551;&#36848;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#24037;&#20316;&#38598;&#20013;&#20110;&#25429;&#25417;&#24322;&#24120;&#22270;&#20449;&#24687;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#34920;&#31034;&#65292;&#20294;&#24573;&#35270;&#20102;&#23545;&#35780;&#20272;&#24322;&#24120;&#22270;&#30340;&#26377;&#25928;&#24322;&#24120;&#24471;&#20998;&#20989;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#22312;&#22270;&#38598;&#20013;&#21253;&#25324;&#33410;&#28857;&#21644;&#22270;&#23646;&#24615;&#24322;&#24120;&#30340;&#24322;&#24120;&#22270;&#20449;&#24687;&#65292;&#24182;&#20998;&#21035;&#37319;&#29992;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#20449;&#24687;&#24046;&#26469;&#35782;&#21035;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;&#23398;&#29983;-&#25945;&#24072;&#27169;&#22411;&#30340;&#21028;&#21035;&#24615;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#36890;&#36807;&#19968;&#31181;&#21551;&#21457;&#24335;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;&#22270;&#34920;&#31034;&#26356;&#20026;&#20998;&#25955;&#12290;&#28982;&#21518;&#65292;&#20004;&#20010;&#31454;&#20105;&#30340;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#20105;&#22842;&#20219;&#21153;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different from the current node-level anomaly detection task, the goal of graph-level anomaly detection is to find abnormal graphs that significantly differ from others in a graph set. Due to the scarcity of research on the work of graph-level anomaly detection, the detailed description of graph-level anomaly is insufficient. Furthermore, existing works focus on capturing anomalous graph information to learn better graph representations, but they ignore the importance of an effective anomaly score function for evaluating abnormal graphs. Thus, in this work, we first define anomalous graph information including node and graph property anomalies in a graph set and adopt node-level and graph-level information differences to identify them, respectively. Then, we introduce a discriminative graph-level anomaly detection framework with dual-students-teacher model, where the teacher model with a heuristic loss are trained to make graph representations more divergent. Then, two competing studen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#21452;&#32990;&#32974;&#33041;&#20316;&#20026;&#19968;&#20010;&#26725;&#26753;&#65292;&#23558;&#29983;&#29289;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#32467;&#21512;&#36215;&#26469;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#22823;&#33041;&#30340;&#22797;&#26434;&#24615;&#21644;&#22914;&#20309;&#20135;&#29983;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01941</link><description>&lt;p&gt;
&#25968;&#23383;&#21452;&#32990;&#32974;&#33041;&#65306;&#29983;&#29289;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Digital twin brain: a bridge between biological intelligence and artificial intelligence. (arXiv:2308.01941v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#21452;&#32990;&#32974;&#33041;&#20316;&#20026;&#19968;&#20010;&#26725;&#26753;&#65292;&#23558;&#29983;&#29289;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#32467;&#21512;&#36215;&#26469;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#22823;&#33041;&#30340;&#22797;&#26434;&#24615;&#21644;&#22914;&#20309;&#20135;&#29983;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#20026;&#29702;&#35299;&#22823;&#33041;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#36890;&#36807;&#35745;&#31639;&#31995;&#32479;&#26469;&#27169;&#25311;&#22823;&#33041;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#30340;&#21069;&#27839;&#36827;&#23637;&#25581;&#31034;&#20102;&#22823;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#24378;&#35843;&#20102;&#32593;&#32476;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#22312;&#26159;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#26356;&#22909;&#22320;&#25581;&#31034;&#26234;&#33021;&#26159;&#22914;&#20309;&#20174;&#22823;&#33041;&#30340;&#22810;&#23610;&#24230;&#23384;&#20648;&#24211;&#20013;&#20135;&#29983;&#30340;&#26102;&#20505;&#20102;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#25968;&#23383;&#21452;&#32990;&#32974;&#33041;(DTB)&#20316;&#20026;&#19968;&#20010;&#23558;&#29983;&#29289;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#32852;&#31995;&#36215;&#26469;&#30340;&#36716;&#21464;&#24615;&#24179;&#21488;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#26680;&#24515;&#20803;&#32032;&#65306;&#23545;&#21452;&#32990;&#32974;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#33041;&#32467;&#26500;&#12289;&#29992;&#20110;&#29983;&#25104;&#22823;&#33041;&#21151;&#33021;&#30340;&#24213;&#23618;&#27169;&#22411;&#65292;&#20197;&#21450;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#20851;&#38190;&#26159;&#65292;&#22823;&#33041;&#22270;&#35889;&#25552;&#20379;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20445;&#25345;&#20102;&#22823;&#33041;&#30340;&#32593;&#32476;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, advances in neuroscience and artificial intelligence have paved the way for unprecedented opportunities for understanding the complexity of the brain and its emulation by computational systems. Cutting-edge advancements in neuroscience research have revealed the intricate relationship between brain structure and function, while the success of artificial neural networks highlights the importance of network architecture. Now is the time to bring them together to better unravel how intelligence emerges from the brain's multiscale repositories. In this review, we propose the Digital Twin Brain (DTB) as a transformative platform that bridges the gap between biological and artificial intelligence. It consists of three core elements: the brain structure that is fundamental to the twinning process, bottom-layer models to generate brain functions, and its wide spectrum of applications. Crucially, brain atlases provide a vital constraint, preserving the brain's network organizat
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.01937</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01937
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#19978;&#35757;&#32451;&#19981;&#21516;&#25193;&#25955;&#27169;&#22411;&#65288;&#25110;&#25552;&#31034;&#65289;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#23396;&#31435;&#29366;&#24577;&#19979;&#12289;&#22312;&#19981;&#21516;&#26102;&#38388;&#12289;&#22312;&#19981;&#21516;&#20998;&#24067;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#21518;&#32493;&#32452;&#21512;&#20197;&#36798;&#21040;&#19982;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#30340;&#29702;&#24819;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#21482;&#21253;&#21547;&#20854;&#22312;&#35757;&#32451;&#26399;&#38388;&#25509;&#35302;&#21040;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#12290;&#29305;&#21035;&#26159;&#65292;CDM&#26159;&#31532;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#36951;&#24536;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20801;&#35768;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;CDM&#36824;&#21487;&#20197;&#30830;&#23450;&#29983;&#25104;&#29305;&#23450;&#26679;&#26412;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#25512;&#29702;&#26102;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#25552;&#20379;&#36229;&#36234;&#25991;&#23383;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#24378;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.01936</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#25105;&#20204;&#38656;&#35201;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26469;&#24314;&#27169;&#23454;&#29992;&#30340;&#31867;&#27604;?
&lt;/p&gt;
&lt;p&gt;
Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?. (arXiv:2308.01936v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#25512;&#29702;&#26102;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#25552;&#20379;&#36229;&#36234;&#25991;&#23383;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#24378;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#20010;&#29305;&#28857;&#26159;&#33021;&#22815;&#21033;&#29992;&#29087;&#24713;&#30340;&#39046;&#22495;&#23545;&#19981;&#37027;&#20040;&#29087;&#24713;&#30340;&#39046;&#22495;&#36827;&#34892;&#25512;&#29702;&#65292;&#21363;&#31867;&#27604;&#25512;&#29702;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22235;&#20010;&#19981;&#21516;&#22797;&#26434;&#32423;&#21035;&#30340;&#31867;&#27604;&#65306;&#35789;&#27719;&#31867;&#27604;&#12289;&#21477;&#27861;&#31867;&#27604;&#12289;&#35821;&#20041;&#31867;&#27604;&#21644;&#23454;&#29992;&#31867;&#27604;&#12290;&#38543;&#30528;&#31867;&#27604;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#38656;&#35201;&#36229;&#20986;&#25991;&#26412;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#36825;&#22312;&#25903;&#25345;LLMs&#30340;&#35789;&#27719;&#20849;&#29616;&#32479;&#35745;&#20013;&#19981;&#22826;&#21487;&#33021;&#25214;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#37319;&#29992;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24517;&#35201;&#24615;&#65292;&#36825;&#20123;&#25216;&#26415;&#32467;&#21512;&#20102;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65292;&#26681;&#25454;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#20379;&#20449;&#24687;&#20197;&#31361;&#20986;&#21644;&#22686;&#24378;&#30456;&#20851;&#20869;&#23481;&#65292;&#25552;&#20379;&#25277;&#35937;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#22312;&#20445;&#25345;LLMs&#30340;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hallmark of intelligence is the ability to use a familiar domain to make inferences about a less familiar domain, known as analogical reasoning. In this article, we delve into the performance of Large Language Models (LLMs) in dealing with progressively complex analogies expressed in unstructured text. We discuss analogies at four distinct levels of complexity: lexical analogies, syntactic analogies, semantic analogies, and pragmatic analogies. As the analogies become more complex, they require increasingly extensive, diverse knowledge beyond the textual content, unlikely to be found in the lexical co-occurrence statistics that power LLMs. To address this, we discuss the necessity of employing Neuro-symbolic AI techniques that combine statistical and symbolic AI, informing the representation of unstructured text to highlight and augment relevant content, provide abstraction and guide the mapping process. Our knowledge-informed approach maintains the efficiency of LLMs while preservin
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20809;&#30005;&#33033;&#25615;&#20449;&#21495;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;PPG&#20449;&#21495;&#21644;LR&#12289;XGBoost&#31639;&#27861;&#30340;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#20813;&#21019;&#20260;&#19988;&#36830;&#32493;&#30417;&#27979;&#30340;&#31958;&#23615;&#30149;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.01930</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#30005;&#33033;&#25615;&#20449;&#21495;&#29305;&#24449;&#30340;&#31958;&#23615;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Based Diabetes Detection Using Photoplethysmography Signal Features. (arXiv:2308.01930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01930
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20809;&#30005;&#33033;&#25615;&#20449;&#21495;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;PPG&#20449;&#21495;&#21644;LR&#12289;XGBoost&#31639;&#27861;&#30340;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#20813;&#21019;&#20260;&#19988;&#36830;&#32493;&#30417;&#27979;&#30340;&#31958;&#23615;&#30149;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#24930;&#24615;&#30142;&#30149;&#65292;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24433;&#21709;&#30528;&#25968;&#30334;&#19975;&#20154;&#30340;&#20581;&#24247;&#12290;&#38656;&#35201;&#26080;&#21019;&#26041;&#27861;&#26469;&#39044;&#38450;&#21644;&#25511;&#21046;&#31958;&#23615;&#30149;&#65292;&#20294;&#22823;&#22810;&#25968;&#29992;&#20110;&#27979;&#37327;&#34880;&#31958;&#27700;&#24179;&#30340;&#35774;&#22791;&#26159;&#26377;&#21019;&#30340;&#65292;&#19981;&#36866;&#21512;&#36830;&#32493;&#30417;&#27979;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#20405;&#20837;&#24615;&#20809;&#23398;&#20809;&#30005;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#31958;&#23615;&#30149;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;PPG&#20449;&#21495;&#21644;&#20803;&#25968;&#25454;&#23545;&#38750;&#31958;&#23615;&#30149;&#21644;&#31958;&#23615;&#30149;&#24739;&#32773;&#36827;&#34892;&#20998;&#31867;&#65292;&#29992;&#20110;&#35757;&#32451;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;PPG&#20449;&#21495;&#12290;&#20026;&#20102;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#20998;&#25104;&#20116;&#20010;&#37096;&#20998;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#12290;&#36890;&#36807;&#30830;&#20445;&#35757;&#32451;&#38598;&#20013;&#30340;&#24739;&#32773;&#19981;&#22312;&#27979;&#35797;&#38598;&#20013;&#65292;&#21487;&#20197;&#22312;&#26410;&#35265;&#36807;&#30340;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#19978;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;F1-Score&#21644;AUC&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;$58.8\pm20.0\%$&#21644;$7
&lt;/p&gt;
&lt;p&gt;
Diabetes is a prevalent chronic condition that compromises the health of millions of people worldwide. Minimally invasive methods are needed to prevent and control diabetes but most devices for measuring glucose levels are invasive and not amenable for continuous monitoring. Here, we present an alternative method to overcome these shortcomings based on non-invasive optical photoplethysmography (PPG) for detecting diabetes. We classify non-Diabetic and Diabetic patients using the PPG signal and metadata for training Logistic Regression (LR) and eXtreme Gradient Boosting (XGBoost) algorithms. We used PPG signals from a publicly available dataset. To prevent overfitting, we divided the data into five folds for cross-validation. By ensuring that patients in the training set are not in the testing set, the model's performance can be evaluated on unseen subjects' data, providing a more accurate assessment of its generalization. Our model achieved an F1-Score and AUC of $58.8\pm20.0\%$ and $7
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#19993;&#27850;&#37210;&#21644;&#29790;&#33452;&#22826;&#23612;&#30340;&#40635;&#37257;&#28145;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#21644;&#38376;&#25511;&#27531;&#24046;&#32593;&#32476;&#26469;&#25552;&#39640;&#29305;&#24449;&#34701;&#21512;&#30340;&#25928;&#29575;&#65292;&#24182;&#24212;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#21457;&#29616;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;PK-PD&#27169;&#22411;&#21644;&#20808;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#40635;&#37257;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.01929</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#38774;&#25511;&#36755;&#27880;&#19993;&#27850;&#37210;&#21644;&#29790;&#33452;&#22826;&#23612;&#40635;&#37257;&#28145;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Transformer-based Prediction Method for Depth of Anesthesia During Target-controlled Infusion of Propofol and Remifentanil. (arXiv:2308.01929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01929
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#19993;&#27850;&#37210;&#21644;&#29790;&#33452;&#22826;&#23612;&#30340;&#40635;&#37257;&#28145;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#21644;&#38376;&#25511;&#27531;&#24046;&#32593;&#32476;&#26469;&#25552;&#39640;&#29305;&#24449;&#34701;&#21512;&#30340;&#25928;&#29575;&#65292;&#24182;&#24212;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#21457;&#29616;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;PK-PD&#27169;&#22411;&#21644;&#20808;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#40635;&#37257;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#40635;&#37257;&#25928;&#26524;&#23545;&#20110;&#38774;&#25511;&#36755;&#27880;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#33647;&#29289;&#21160;&#21147;&#23398;-&#33647;&#25928;&#23398;&#27169;&#22411;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#65292;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21482;&#33021;&#25429;&#25417;&#21040;&#19968;&#33324;&#36235;&#21183;&#65292;&#21487;&#33021;&#26080;&#27861;&#39044;&#27979;BIS&#30340;&#31361;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19993;&#27850;&#37210;&#21644;&#29790;&#33452;&#22826;&#23612;&#30340;&#33647;&#29289;&#36755;&#27880;&#26469;&#39044;&#27979;&#40635;&#37257;&#28145;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#27531;&#24046;&#32593;&#32476;&#65288;GRN&#65289;&#26469;&#25552;&#39640;&#29305;&#24449;&#34701;&#21512;&#30340;&#25928;&#29575;&#65292;&#24182;&#24212;&#29992;&#20102;&#27880;&#24847;&#26426;&#21046;&#26469;&#21457;&#29616;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26631;&#31614;&#20998;&#24067;&#24179;&#28369;&#21644;&#37325;&#26032;&#21152;&#26435;&#25439;&#22833;&#26469;&#22788;&#29702;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;PK-PD&#27169;&#22411;&#21644;&#20808;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#40635;&#37257;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting anesthetic effects is essential for target-controlled infusion systems. The traditional (PK-PD) models for Bispectral index (BIS) prediction require manual selection of model parameters, which can be challenging in clinical settings. Recently proposed deep learning methods can only capture general trends and may not predict abrupt changes in BIS. To address these issues, we propose a transformer-based method for predicting the depth of anesthesia (DOA) using drug infusions of propofol and remifentanil. Our method employs long short-term memory (LSTM) and gate residual network (GRN) networks to improve the efficiency of feature fusion and applies an attention mechanism to discover the interactions between the drugs. We also use label distribution smoothing and reweighting losses to address data imbalance. Experimental results show that our proposed method outperforms traditional PK-PD models and previous deep learning methods, effectively predicting anesthetic dept
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01923</link><description>&lt;p&gt;
&#22810;&#37325;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fairness Improvement with Multiple Protected Attributes. (arXiv:2308.01923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36719;&#20214;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#65292;&#20294;&#32771;&#34385;&#21040;&#35768;&#22810;&#29992;&#25143;&#20855;&#26377;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#23545;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#32771;&#34385;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;ML&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25913;&#21892;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#22823;&#22823;&#38477;&#20302;&#20102;&#26410;&#32771;&#34385;&#30340;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;88.3&#65285;&#30340;&#24773;&#20917;&#19979;&#35266;&#23519;&#21040;&#36825;&#31181;&#38477;&#20302;&#65288;&#24179;&#22343;&#20026;57.5&#65285;&#65289;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#32771;&#34385;&#21333;&#20010;&#21644;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#20934;&#30830;&#29575;&#25439;&#22833;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.01921</link><description>&lt;p&gt;
&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#29983;&#29289;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#33647;&#29289;&#20998;&#23376;&#24555;&#36895;&#31579;&#36873;&#26159;&#33647;&#29289;&#21457;&#29616;&#31649;&#32447;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#39640;&#36890;&#37327;&#21644;&#39640;&#20934;&#30830;&#24615;&#20998;&#23376;&#23545;&#25509;&#20195;&#29702;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#32422;30&#19975;&#31181;&#33647;&#29289;&#20505;&#36873;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22270;&#31070;&#32463;&#25351;&#32441;&#23545;&#25509;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23545;&#22823;&#22810;&#25968;&#23545;&#25509;&#38774;&#28857;&#30340;&#22343;&#26041;&#35823;&#24046;&#20302;&#20110;0.21 kcal/mol&#65292;&#30456;&#27604;&#20256;&#32479;&#22278;&#24418;&#25351;&#32441;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#20026;&#20102;&#20351;&#31070;&#32463;&#25351;&#32441;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#38774;&#28857;&#19978;&#35757;&#32451;&#30340;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast screening of drug molecules based on the ligand binding affinity is an important step in the drug discovery pipeline. Graph neural fingerprint is a promising method for developing molecular docking surrogates with high throughput and great fidelity. In this study, we built a COVID-19 drug docking dataset of about 300,000 drug candidates on 23 coronavirus protein targets. With this dataset, we trained graph neural fingerprint docking models for high-throughput virtual COVID-19 drug screening. The graph neural fingerprint models yield high prediction accuracy on docking scores with the mean squared error lower than $0.21$ kcal/mol for most of the docking targets, showing significant improvement over conventional circular fingerprint methods. To make the neural fingerprints transferable for unknown targets, we also propose a transferable graph neural fingerprint method trained on multiple targets. With comparable accuracy to target-specific graph neural fingerprint models, the transf
&lt;/p&gt;</description></item><item><title>ME-MHACL&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#26410;&#26631;&#35760;&#30340;&#30005;&#29983;&#29702;&#20449;&#21495;&#20013;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01919</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#30005;&#29983;&#29702;&#22810;&#22836;&#27880;&#24847;&#21147;&#23545;&#27604;&#23398;&#20064;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition based on multi-modal electrophysiology multi-head attention Contrastive Learning. (arXiv:2308.01919v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01919
&lt;/p&gt;
&lt;p&gt;
ME-MHACL&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#26410;&#26631;&#35760;&#30340;&#30005;&#29983;&#29702;&#20449;&#21495;&#20013;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#24110;&#21161;&#26426;&#22120;&#29702;&#35299;&#21644;&#36866;&#24212;&#20154;&#31867;&#30340;&#24773;&#24863;&#29366;&#24577;&#12290;&#22810;&#27169;&#24577;&#30005;&#29983;&#29702;&#20449;&#21495;&#65292;&#22914;&#33041;&#30005;&#22270;(EEG)&#65292;&#30382;&#32932;&#30005;(ED)&#65292;&#21628;&#21560;(Resp)&#21644;&#28201;&#24230;(Temp)&#65292;&#26159;&#21453;&#26144;&#20154;&#31867;&#24773;&#24863;&#21464;&#21270;&#30340;&#26377;&#25928;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#30005;&#29983;&#29702;&#20449;&#21495;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#38754;&#20020;&#25968;&#25454;&#31232;&#32570;&#12289;&#26631;&#27880;&#19981;&#19968;&#33268;&#12289;&#38590;&#20197;&#22312;&#20010;&#20307;&#38388;&#27867;&#21270;&#31561;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ME-MHACL&#65292;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26410;&#26631;&#35760;&#30340;&#30005;&#29983;&#29702;&#20449;&#21495;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;Meiosis&#26041;&#27861;&#23545;&#26410;&#26631;&#35760;&#30340;&#30005;&#29983;&#29702;&#20449;&#21495;&#36827;&#34892;&#26679;&#26412;&#32452;&#21512;&#21644;&#22686;&#24378;&#65292;&#24182;&#35774;&#35745;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#29305;&#24449;&#34920;&#31034;&#22120;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#22810;&#27169;&#24577;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition is an important research direction in artificial intelligence, helping machines understand and adapt to human emotional states. Multimodal electrophysiological(ME) signals, such as EEG, GSR, respiration(Resp), and temperature(Temp), are effective biomarkers for reflecting changes in human emotions. However, using electrophysiological signals for emotion recognition faces challenges such as data scarcity, inconsistent labeling, and difficulty in cross-individual generalization. To address these issues, we propose ME-MHACL, a self-supervised contrastive learning-based multimodal emotion recognition method that can learn meaningful feature representations from unlabeled electrophysiological signals and use multi-head attention mechanisms for feature fusion to improve recognition performance. Our method includes two stages: first, we use the Meiosis method to group sample and augment unlabeled electrophysiological signals and design a self-supervised contrastive learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21322;&#30417;&#30563;&#20803;&#23398;&#20064;&#22312;&#26102;&#31354;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23558;&#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#29366;&#24577;-of-the-art&#34920;&#31034;&#23398;&#20064;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#35270;&#39057;&#37325;&#24314;&#21644;&#21160;&#20316;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01916</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#20803;&#23398;&#20064;&#22312;&#26102;&#31354;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Semi Supervised Meta Learning for Spatiotemporal Learning. (arXiv:2308.01916v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21322;&#30417;&#30563;&#20803;&#23398;&#20064;&#22312;&#26102;&#31354;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23558;&#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#29366;&#24577;-of-the-art&#34920;&#31034;&#23398;&#20064;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#35270;&#39057;&#37325;&#24314;&#21644;&#21160;&#20316;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#26469;&#23558;&#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#20197;&#36827;&#34892;&#26102;&#31354;&#23398;&#20064;&#12290;&#24191;&#20041;&#19978;&#35828;&#65292;&#25105;&#20204;&#26088;&#22312;&#29702;&#35299;&#23558;&#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;&#26550;&#26500;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#27979;&#35797;&#26102;&#31354;&#23398;&#20064;&#65306;&#20165;&#20803;&#23398;&#20064;&#26550;&#26500;&#12289;&#20165;&#34920;&#31034;&#23398;&#20064;&#26550;&#26500;&#20197;&#21450;&#23558;&#34920;&#31034;&#23398;&#20064;&#19982;&#20803;&#23398;&#20064;&#26550;&#26500;&#30456;&#32467;&#21512;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22686;&#24378;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#65288;MANN&#65289;&#26550;&#26500;&#23558;&#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23581;&#35797;&#22312;&#23567;&#35268;&#27169;&#26102;&#31354;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36827;&#34892;&#35270;&#39057;&#37325;&#24314;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23581;&#35797;&#35757;&#32451;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#22120;&#65292;&#24182;&#24212;&#29992;&#20998;&#31867;&#22836;&#36827;&#34892;&#21160;&#20316;&#20998;&#31867;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#22312;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#22522;&#30784;&#19978;&#65292;&#29992;MANN&#39592;&#24178;&#36827;&#34892;&#24494;&#35843;&#65292;&#29992;&#20110;&#21160;&#20316;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We approached the goal of applying meta-learning to self-supervised masked autoencoders for spatiotemporal learning in three steps. Broadly, we seek to understand the impact of applying meta-learning to existing state-of-the-art representation learning architectures. Thus, we test spatiotemporal learning through: a meta-learning architecture only, a representation learning architecture only, and an architecture applying representation learning alongside a meta learning architecture. We utilize the Memory Augmented Neural Network (MANN) architecture to apply meta-learning to our framework. Specifically, we first experiment with applying a pre-trained MAE and fine-tuning on our small-scale spatiotemporal dataset for video reconstruction tasks. Next, we experiment with training an MAE encoder and applying a classification head for action classification tasks. Finally, we experiment with applying a pre-trained MAE and fine-tune with MANN backbone for action classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2307.16104</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#20840;&#29699;&#21487;&#38752;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27946;&#27700;&#26159;&#26368;&#24120;&#35265;&#21644;&#24433;&#21709;&#26368;&#22823;&#30340;&#33258;&#28982;&#28798;&#23475;&#20043;&#19968;&#65292;&#23545;&#21457;&#23637;&#20013;&#22269;&#23478;&#23588;&#20854;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22269;&#23478;&#24448;&#24448;&#32570;&#20047;&#23494;&#38598;&#30340;&#27700;&#27969;&#30417;&#27979;&#32593;&#32476;&#12290;&#20934;&#30830;&#21450;&#26102;&#30340;&#39044;&#35686;&#23545;&#20110;&#20943;&#36731;&#27946;&#27700;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20934;&#30830;&#30340;&#27700;&#25991;&#27169;&#25311;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26681;&#25454;&#27599;&#20010;&#24212;&#29992;&#30340;&#27969;&#22495;&#20013;&#30340;&#38271;&#26102;&#38388;&#25968;&#25454;&#35760;&#24405;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;7&#22825;&#20869;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#22823;&#27954;&#12289;&#21069;&#23548;&#26102;&#38388;&#21644;&#37325;&#29616;&#26399;&#20013;&#22343;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20840;&#29699;&#27700;&#25991;&#27169;&#22411;&#65288;Copernicus&#24212;&#24613;&#31649;&#29702;&#26381;&#21153;&#20840;&#29699;&#27946;&#27700;&#24847;&#35782;&#31995;&#32479;&#65289;&#12290;AI&#22312;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#20013;&#30340;&#39044;&#27979;&#23588;&#20854;&#26377;&#25928;&#65292;&#36825;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20840;&#29699;&#21482;&#26377;&#30334;&#20998;&#20043;&#20960;&#30340;&#27969;&#22495;&#20855;&#26377;&#27969;&#37327;&#35266;&#27979;&#31449;&#65292;&#32780;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#25968;&#37327;&#21344;&#27604;&#24456;&#39640;&#65292;&#23545;&#20154;&#31867;&#29305;&#21035;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35848;&#21028;&#21327;&#35758;&#26469;&#35299;&#20915;&#30899;&#27844;&#28431;&#38382;&#39064;&#65292;&#36890;&#36807;&#19982;&#20195;&#34920;&#24615;&#27987;&#24230;&#36335;&#24452;&#21644;&#20849;&#20139;&#31038;&#20250;&#32463;&#27982;&#36335;&#24452;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#27492;&#22806;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#21327;&#35758;&#30340;&#21512;&#35268;&#24615;&#12289;&#21487;&#34892;&#24615;&#21644;&#20262;&#29702;&#20851;&#20999;&#12290;</title><link>http://arxiv.org/abs/2307.13892</link><description>&lt;p&gt;
AI4GCC - &#22242;&#38431;: &#28023;&#24179;&#38754;&#20197;&#19979;: &#35780;&#20998;&#21644;&#23454;&#38469;&#19990;&#30028;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI4GCC - Team: Below Sea Level: Score and Real World Relevance. (arXiv:2307.13892v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13892
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35848;&#21028;&#21327;&#35758;&#26469;&#35299;&#20915;&#30899;&#27844;&#28431;&#38382;&#39064;&#65292;&#36890;&#36807;&#19982;&#20195;&#34920;&#24615;&#27987;&#24230;&#36335;&#24452;&#21644;&#20849;&#20139;&#31038;&#20250;&#32463;&#27982;&#36335;&#24452;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#27492;&#22806;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#21327;&#35758;&#30340;&#21512;&#35268;&#24615;&#12289;&#21487;&#34892;&#24615;&#21644;&#20262;&#29702;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#25105;&#20204;&#21442;&#21152;AI for Global Climate Cooperation (AI4GCC)&#31454;&#36187;&#30340;&#31532;&#19977;&#39033;&#36319;&#36394;&#20219;&#21153;&#30340;&#25552;&#20132;&#65292;&#25105;&#20204;&#38024;&#23545;RICE-N&#27668;&#20505;&#32463;&#27982;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#31181;&#35848;&#21028;&#21327;&#35758;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#36890;&#36807;&#21463;&#21040;&#30899;&#36793;&#22659;&#35843;&#25972;&#26426;&#21046;(CBAM)&#21644;&#27668;&#20505;&#20465;&#20048;&#37096;(CC)&#21551;&#21457;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30899;&#27844;&#28431;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27169;&#25311;&#32467;&#26524;&#19982;&#20195;&#34920;&#24615;&#27987;&#24230;&#36335;&#24452;(RCP)&#21644;&#20849;&#20139;&#31038;&#20250;&#32463;&#27982;&#36335;&#24452;(SSP)&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#21327;&#35758;&#23548;&#33268;&#20102;&#19982;RCP 3.4/4.5&#21644;SSP 2&#30456;&#24403;&#30340;&#28201;&#24230;&#19978;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#21327;&#35758;&#30340;&#19990;&#30028;&#36152;&#26131;&#32452;&#32455;&#21512;&#35268;&#24615;&#12289;&#34892;&#25919;&#21644;&#25919;&#27835;&#21487;&#34892;&#24615;&#20197;&#21450;&#20262;&#29702;&#20851;&#20999;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#25105;&#20204;&#30340;&#25552;&#35758;&#21487;&#33021;&#20250;&#23545;&#26368;&#19981;&#21457;&#36798;&#22269;&#23478;&#36896;&#25104;&#20260;&#23475;&#65292;&#22240;&#27492;&#25105;&#20204;&#24314;&#35758;&#37319;&#21462;&#29305;&#23450;&#30340;&#32416;&#27491;&#25514;&#26045;&#65292;&#36991;&#20813;&#21152;&#21095;&#29616;&#26377;&#30340;&#19981;&#24179;&#31561;&#65292;&#20363;&#22914;&#25216;&#26415;&#20849;&#20139;&#21644;&#36130;&#23500;&#20877;&#20998;&#37197;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#25913;&#36827;...
&lt;/p&gt;
&lt;p&gt;
As our submission for track three of the AI for Global Climate Cooperation (AI4GCC) competition, we propose a negotiation protocol for use in the RICE-N climate-economic simulation. Our proposal seeks to address the challenges of carbon leakage through methods inspired by the Carbon Border Adjustment Mechanism (CBAM) and Climate Clubs (CC). We demonstrate the effectiveness of our approach by comparing simulated outcomes to representative concentration pathways (RCP) and shared socioeconomic pathways (SSP). Our protocol results in a temperature rise comparable to RCP 3.4/4.5 and SSP 2. Furthermore, we provide an analysis of our protocol's World Trade Organization compliance, administrative and political feasibility, and ethical concerns. We recognize that our proposal risks hurting the least developing countries, and we suggest specific corrective measures to avoid exacerbating existing inequalities, such as technology sharing and wealth redistribution. Future research should improve th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.00925</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#31181;&#19982;&#35745;&#31639;&#26426;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#38598;&#21512;&#31574;&#30053;&#26469;&#30830;&#20445;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#19968;&#32452;&#20505;&#36873;&#24230;&#37327;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26368;&#22823;&#21270;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#30340;&#38598;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#38598;&#21512;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26082;&#23637;&#31034;&#20102;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#27604;&#36739;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#20063;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#23545;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#23646;&#24615;&#24402;&#32435;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-3.5&#26377;&#19968;&#20123;&#22256;&#38590;&#65292;&#20294;GPT-4&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30456;&#20284;&#65292;&#38500;&#20102;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20316;&#26410;&#26469;&#30740;&#31350;&#22522;&#20934;&#30340;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06548</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24402;&#32435;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Inductive reasoning in humans and large language models. (arXiv:2306.06548v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#23545;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#23646;&#24615;&#24402;&#32435;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-3.5&#26377;&#19968;&#20123;&#22256;&#38590;&#65292;&#20294;GPT-4&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30456;&#20284;&#65292;&#38500;&#20102;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20316;&#26410;&#26469;&#30740;&#31350;&#22522;&#20934;&#30340;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#26159;&#21542;&#33021;&#20316;&#20026;&#26222;&#36890;&#26234;&#33021;&#30340;&#27169;&#22411;&#25110;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#30340;&#31243;&#24230;&#30340;&#30097;&#38382;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;GPT-3.5&#21644;GPT-4&#24212;&#29992;&#20110;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#19968;&#20010;&#32463;&#20856;&#38382;&#39064;&#65292;&#21363;&#23646;&#24615;&#24402;&#32435;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;&#20154;&#31867;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#30340;&#23646;&#24615;&#24402;&#32435;&#20219;&#21153;&#19978;&#30340;&#21028;&#26029;&#12290;&#23613;&#31649;GPT-3.5&#22312;&#25429;&#25417;&#20154;&#31867;&#34892;&#20026;&#30340;&#35768;&#22810;&#26041;&#38754;&#19978;&#26377;&#22256;&#38590;&#65292;&#20294;GPT-4&#26356;&#21152;&#25104;&#21151;&#65306;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#65292;&#23427;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#22312;&#36136;&#19978;&#30456;&#21305;&#37197;&#65292;&#21807;&#19968;&#26174;&#33879;&#30340;&#20363;&#22806;&#26159;&#20854;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#23646;&#24615;&#24402;&#32435;&#21487;&#20197;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#36827;&#34892;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive recent performance of large language models has led many to wonder to what extent they can serve as models of general intelligence or are similar to human cognition. We address this issue by applying GPT-3.5 and GPT-4 to a classic problem in human inductive reasoning known as property induction. Over two experiments, we elicit human judgments on a range of property induction tasks spanning multiple domains. Although GPT-3.5 struggles to capture many aspects of human behaviour, GPT-4 is much more successful: for the most part, its performance qualitatively matches that of humans, and the only notable exception is its failure to capture the phenomenon of premise non-monotonicity. Our work demonstrates that property induction allows for interesting comparisons between human and machine intelligence and provides two large datasets that can serve as benchmarks for future work in this vein.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#20195;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#23398;&#29983;&#32593;&#32476;&#21487;&#20197;&#20174;&#26679;&#26412;&#30340;&#22810;&#31181;&#35266;&#28857;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#20248;&#20110;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00393</link><description>&lt;p&gt;
Teacher Agent&#65306;&#19968;&#31181;&#22522;&#20110;&#37325;&#22797;&#35757;&#32451;&#30340;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#38750;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning. (arXiv:2306.00393v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#20195;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#23398;&#29983;&#32593;&#32476;&#21487;&#20197;&#20174;&#26679;&#26412;&#30340;&#22810;&#31181;&#35266;&#28857;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#20248;&#20110;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#35270;&#39057;&#30340;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#19981;&#26029;&#26377;&#26032;&#30340;&#35270;&#39057;&#31867;&#21035;&#34987;&#29983;&#25104;&#65292;&#36843;&#20999;&#38656;&#35201;&#31283;&#20581;&#30340;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#26469;&#29702;&#35299;&#36825;&#20123;&#35270;&#39057;&#12290;&#20854;&#20013;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#21516;&#26102;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#20064;&#36807;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#23558;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30340;&#37325;&#35201;&#20449;&#24687;&#20256;&#36755;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#26469;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26368;&#22909;&#26377;&#19968;&#20010;&#24378;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#26412;&#36523;&#30340;&#26377;&#38480;&#34920;&#29616;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21457;&#29983;&#21487;&#33021;&#23548;&#33268;&#25945;&#24072;&#32593;&#32476;&#23545;&#26576;&#20123;&#35760;&#24518;&#26679;&#26412;&#20570;&#20986;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23398;&#29983;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#20195;&#29702;&#65292;&#33021;&#22815;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#23398;&#29983;&#32593;&#32476;&#21487;&#20197;&#20174;&#26679;&#26412;&#30340;&#22810;&#31181;&#35266;&#28857;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#26631;&#20934;&#22522;&#20934;&#19978;&#20248;&#20110;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise in popularity of video-based social media, new categories of videos are constantly being generated, creating an urgent need for robust incremental learning techniques for video understanding. One of the biggest challenges in this task is catastrophic forgetting, where the network tends to forget previously learned data while learning new categories. To overcome this issue, knowledge distillation is a widely used technique for rehearsal-based video incremental learning that involves transferring important information on similarities among different categories to enhance the student model. Therefore, it is preferable to have a strong teacher model to guide the students. However, the limited performance of the network itself and the occurrence of catastrophic forgetting can result in the teacher network making inaccurate predictions for some memory exemplars, ultimately limiting the student network's performance. Based on these observations, we propose a teacher agent capabl
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#20004;&#31181;&#27169;&#22411;&#33539;&#24335;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.19860</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Models for Recommendation. (arXiv:2305.19860v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#20004;&#31181;&#27169;&#22411;&#33539;&#24335;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#28023;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24050;&#22312;&#23398;&#20064;&#36890;&#29992;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#65292;&#24182;&#26377;&#21487;&#33021;&#36890;&#36807;&#19968;&#20123;&#26377;&#25928;&#30340;&#36716;&#31227;&#25216;&#26415;&#65288;&#22914;&#24494;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#65289;&#31561;&#25163;&#27573;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25512;&#33616;&#36136;&#37327;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#23427;&#20204;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#29305;&#24449;&#34920;&#31034;&#21644;&#22823;&#37327;&#30340;&#22806;&#37096;&#30693;&#35782;&#35206;&#30422;&#65292;&#24314;&#31435;&#39033;&#30446;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#26412;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#33539;&#24335;&#65292;&#20998;&#21035;&#26159;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;&#33539;&#24335;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discrimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19148</link><description>&lt;p&gt;
&#32531;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26631;&#31614;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Biases for In-context Learning. (arXiv:2305.19148v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21508;&#31181;&#35774;&#35745;&#35774;&#32622;&#65292;&#22914;&#36873;&#25321;&#21644;&#39034;&#24207;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#21487;&#33021;&#20351;&#27169;&#22411;&#23545;&#26576;&#31181;&#29305;&#23450;&#39044;&#27979;&#20559;&#35265;&#65292;&#32780;&#36825;&#31181;&#39044;&#27979;&#24182;&#19981;&#21453;&#26144;&#23545;&#20219;&#21153;&#30340;&#29702;&#35299;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#35752;&#35770;&#20102;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#65292;&#20294;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#21644;&#20943;&#32531;&#20854;&#24433;&#21709;&#30340;&#31995;&#32479;&#35843;&#26597;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#23450;&#20041;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65306;&#39321;&#33609;&#26631;&#31614;&#20559;&#24046;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20559;&#24046;&#21644;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#65288;&#25105;&#20204;&#39318;&#27425;&#27010;&#24565;&#21270;&#21644;&#26816;&#27979;&#21040;&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20808;&#21069;&#30340;&#26631;&#31614;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#25152;&#26377;&#19977;&#31181;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#20351;LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21482;&#33021;&#23454;&#29616;&#38543;&#26426;&#32423;&#21035;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#31649;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36873;&#25321;&#22914;&#20309;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias a model toward a particular prediction without being reflective of an understanding of the task. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time).  Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#21345;&#36890;&#39118;&#26684;&#22270;&#29255;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#25200;&#21160;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#65292;&#36825;&#19968;&#21151;&#33021;&#24471;&#20197;&#23454;&#29616;&#12290;&#22238;&#28378;&#25200;&#21160;&#33021;&#22815;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#25928;&#36716;&#25442;&#25104;&#21345;&#36890;&#22270;&#20687;&#65292;&#32780;&#22270;&#20687;&#25200;&#21160;&#21017;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#30340;&#21345;&#36890;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.06710</link><description>&lt;p&gt;
&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#21345;&#36890;&#39118;&#26684;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator. (arXiv:2305.06710v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#21345;&#36890;&#39118;&#26684;&#22270;&#29255;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#25200;&#21160;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#65292;&#36825;&#19968;&#21151;&#33021;&#24471;&#20197;&#23454;&#29616;&#12290;&#22238;&#28378;&#25200;&#21160;&#33021;&#22815;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#25928;&#36716;&#25442;&#25104;&#21345;&#36890;&#22270;&#20687;&#65292;&#32780;&#22270;&#20687;&#25200;&#21160;&#21017;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#30340;&#21345;&#36890;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26159;&#25193;&#25955;&#27169;&#22411;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#25216;&#26415;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#22312;&#25991;&#26412;&#25351;&#23548;&#26041;&#21521;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#22806;&#25512;&#65292;&#24182;&#36828;&#31163;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#21345;&#36890;&#39118;&#26684;&#29983;&#25104;&#22120;&#65292;&#21363;&#36890;&#36807;&#31616;&#21333;&#22320;&#25200;&#21160;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#23601;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#36716;&#25442;&#25104;&#21345;&#36890;&#22270;&#20687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25200;&#21160;&#26041;&#27861;&#65306;&#22238;&#28378;&#25200;&#21160;&#65288;Back-D&#65289;&#21644;&#22270;&#20687;&#25200;&#21160;&#65288;Image-D&#65289;&#65292;&#29992;&#20110;&#26500;&#36896;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#29992;&#20110;&#39044;&#27979;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#21644;&#25991;&#26412;&#25351;&#23548;&#30340;&#22024;&#26434;&#22270;&#20687;&#20043;&#38388;&#30340;&#38169;&#20301;&#12290;Back-D&#36890;&#36807;&#36890;&#36807;&#23558;$x_t$&#26367;&#25442;&#20026;$x_{t+\Delta t}$&#26469;&#25913;&#21464;&#26080;&#26631;&#27880;&#22024;&#26434;&#22270;&#20687;&#30340;&#22122;&#22768;&#27700;&#24179;&#20174;&#32780;&#23454;&#29616;&#21345;&#36890;&#21270;&#12290;Image-D&#21017;&#36890;&#36807;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#30340;&#21345;&#36890;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifier-free guidance is an effective sampling technique in diffusion models that has been widely adopted. The main idea is to extrapolate the model in the direction of text guidance and away from null-text guidance. In this paper, we demonstrate that null-text guidance in diffusion models is secretly a cartoon-style creator, i.e., the generated images can be efficiently transformed into cartoons by simply perturbing the null-text guidance. Specifically, we proposed two disturbance methods, i.e., Rollback disturbance (Back-D) and Image disturbance (Image-D), to construct misalignment between the noisy images used for predicting null-text guidance and text guidance (subsequently referred to as \textbf{null-text noisy image} and \textbf{text noisy image} respectively) in the sampling process. Back-D achieves cartoonization by altering the noise level of null-text noisy image via replacing $x_t$ with $x_{t+\Delta t}$. Image-D, alternatively, produces high-fidelity, diverse cartoons by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;3D&#28857;&#20113;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#32593;&#32476;MvNet&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#29616;&#26377;&#30340;2D&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#32531;&#35299;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#23545;&#22823;&#35268;&#27169;&#27880;&#37322;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10224</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#35270;&#35273;&#25552;&#31034;&#34701;&#21512;&#32593;&#32476;&#65306;2D&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#21542;&#22686;&#24378;3D&#28857;&#20113;&#25968;&#25454;&#31232;&#32570;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Multi-view Vision-Prompt Fusion Network: Can 2D Pre-trained Model Boost 3D Point Cloud Data-scarce Learning?. (arXiv:2304.10224v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;3D&#28857;&#20113;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#32593;&#32476;MvNet&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#29616;&#26377;&#30340;2D&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#32531;&#35299;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#23545;&#22823;&#35268;&#27169;&#27880;&#37322;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#30340;3D&#28145;&#24230;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#23478;&#24237;&#26426;&#22120;&#20154;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#35270;&#22270;&#35270;&#35273;&#25552;&#31034;&#34701;&#21512;&#32593;&#32476;&#65288;MvNet&#65289;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;3D&#28857;&#20113;&#20998;&#31867;&#65292;&#28789;&#24863;&#28304;&#33258;&#20110;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25552;&#31034;&#24615;&#23398;&#20064;&#12290;MvNet &#25506;&#35752;&#20102;&#21033;&#29992;&#29616;&#26377;2D&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#23569;&#26679;&#26412;&#20998;&#31867;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#21487;&#20197;&#32531;&#35299;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#23545;&#22823;&#35268;&#27169;&#27880;&#37322;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MvNet&#39318;&#20808;&#23558;3D&#28857;&#20113;&#32534;&#30721;&#25104;&#22810;&#35270;&#22270;&#22270;&#20687;&#29305;&#24449;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#22270;&#25552;&#31034;&#34701;&#21512;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#20449;&#24687;&#65292;&#20197;&#24357;&#21512;3D&#28857;&#20113;&#25968;&#25454;&#21644;2D&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#21518;&#21487;&#20197;&#27966;&#29983;&#19968;&#32452;2D&#22270;&#20687;&#25552;&#31034;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#36866;&#24403;&#30340;&#20808;&#39564;&#30693;&#35782;&#20197;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point cloud based 3D deep model has wide applications in many applications such as autonomous driving, house robot, and so on. Inspired by the recent prompt learning in natural language processing, this work proposes a novel Multi-view Vision-Prompt Fusion Network (MvNet) for few-shot 3D point cloud classification. MvNet investigates the possibility of leveraging the off-the-shelf 2D pre-trained models to achieve the few-shot classification, which can alleviate the over-dependence issue of the existing baseline models towards the large-scale annotated 3D point cloud data. Specifically, MvNet first encodes a 3D point cloud into multi-view image features for a number of different views. Then, a novel multi-view prompt fusion module is developed to effectively fuse information from different views to bridge the gap between 3D point cloud data and 2D pre-trained models. A set of 2D image prompts can then be derived to better describe the suitable prior knowledge for a large-scale pre-train
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.</title><link>http://arxiv.org/abs/2303.13035</link><description>&lt;p&gt;
SPeC&#65306;&#36719;&#25552;&#31034;&#26657;&#20934;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13035
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#23384;&#20648;&#30528;&#21253;&#25324;&#30149;&#21382;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#26816;&#27979;&#32467;&#26524;&#22312;&#20869;&#30340;&#22823;&#37327;&#24739;&#32773;&#20449;&#24687;&#12290;&#36825;&#20123;&#35760;&#24405;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20570;&#20986;&#26126;&#26234;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#38750;&#24120;&#20851;&#38190;&#12290;&#25688;&#35201;&#20020;&#24202;&#31508;&#35760;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#21457;&#29616;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#65292;&#20197;&#21450;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#36825;&#19968;&#36807;&#31243;&#36890;&#36807;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#26368;&#30456;&#20851;&#21644;&#26368;&#26032;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#38169;&#35823;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#25252;&#29702;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#25552;&#31034;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#20063;&#20250;&#23548;&#33268;&#36755;&#20986;&#26041;&#24046;&#22686;&#21152;&#65292;&#21363;&#20351;&#25552;&#31034;&#24847;&#20041;&#30456;&#20284;&#65292;&#36755;&#20986;&#20063;&#20250;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#36719;&#25552;&#31034;&#26657;&#20934;&#65288;SPeC&#65289;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#37319;&#29992;&#36719;&#25552;&#31034;&#23884;&#20837;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SPeC&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;LLM&#30340;&#24615;&#33021;&#21464;&#24322;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;DOLOS&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#26368;&#22823;&#30340;&#28216;&#25103;&#33410;&#30446;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1,675&#20010;&#35270;&#39057;&#29255;&#27573;&#21644;&#20016;&#23500;&#30340;&#27450;&#39575;&#23545;&#35805;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;PECL&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.12745</link><description>&lt;p&gt;
&#38899;&#35270;&#39057;&#27450;&#39575;&#26816;&#27979;&#65306;DOLOS&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#39640;&#25928;&#36328;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Deception Detection: DOLOS Dataset and Parameter-Efficient Crossmodal Learning. (arXiv:2303.12745v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;DOLOS&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#26368;&#22823;&#30340;&#28216;&#25103;&#33410;&#30446;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1,675&#20010;&#35270;&#39057;&#29255;&#27573;&#21644;&#20016;&#23500;&#30340;&#27450;&#39575;&#23545;&#35805;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;PECL&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#27450;&#35784;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22312;&#21830;&#19994;&#30340;&#21487;&#20449;&#24230;&#35780;&#20272;&#12289;&#22810;&#23186;&#20307;&#38450;&#27450;&#35784;&#21644;&#23450;&#21046;&#23433;&#20840;&#31561;&#35768;&#22810;&#39046;&#22495;&#37117;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#27450;&#35784;&#25968;&#25454;&#38598;&#20197;&#21450;&#23398;&#20064;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#22256;&#38590;&#65292;&#27450;&#35784;&#26816;&#27979;&#30740;&#31350;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DOLOS&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#21253;&#21547;&#20016;&#23500;&#30340;&#27450;&#39575;&#23545;&#35805;&#30340;&#26368;&#22823;&#28216;&#25103;&#33410;&#30446;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;DOLOS&#21253;&#25324;1,675&#20010;&#35270;&#39057;&#29255;&#27573;&#65292;&#28041;&#21450;213&#20010;&#34987;&#35797;&#32773;&#65292;&#24182;&#19988;&#24050;&#32463;&#29992;&#38899;&#35270;&#39057;&#29305;&#24449;&#27880;&#37322;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35757;&#32451;-&#27979;&#35797;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#24615;&#21035;&#21327;&#35758;&#26469;&#35843;&#26597;&#19981;&#21516;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20808;&#21069;&#25552;&#20986;&#30340;&#27450;&#39575;&#26816;&#27979;&#26041;&#27861;&#19978;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#36890;&#36807;&#24494;&#35843;&#26356;&#23569;&#30340;&#21442;&#25968;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#36328;&#27169;&#24577;&#23398;&#20064;&#65288;PECL&#65289;&#65292;&#20854;&#20013;&#32479;&#19968;&#26102;&#38388;&#36866;&#37197;&#22120;&#65288;UT-Adapter&#65289;&#25506;&#32034;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Deception detection in conversations is a challenging yet important task, having pivotal applications in many fields such as credibility assessment in business, multimedia anti-frauds, and custom security. Despite this, deception detection research is hindered by the lack of high-quality deception datasets, as well as the difficulties of learning multimodal features effectively. To address this issue, we introduce DOLOS, the largest gameshow deception detection dataset with rich deceptive conversations. DOLOS includes 1,675 video clips featuring 213 subjects, and it has been labeled with audio-visual feature annotations. We provide train-test, duration, and gender protocols to investigate the impact of different factors. We benchmark our dataset on previously proposed deception detection approaches. To further improve the performance by fine-tuning fewer parameters, we propose Parameter-Efficient Crossmodal Learning (PECL), where a Uniform Temporal Adapter (UT-Adapter) explores tempora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25353;&#38656;&#20056;&#36710;&#26381;&#21153;&#25805;&#20316;&#30340;&#22810;&#21151;&#33021;&#27169;&#25311;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#65292;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#25805;&#20316;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#35780;&#20272;&#65292;&#20248;&#21270;&#31639;&#27861;&#35843;&#25972;&#20197;&#21450;&#23454;&#26102;&#21160;&#24577;&#25805;&#20316;&#25511;&#21046;&#31561;&#26041;&#38754;&#22343;&#33021;&#21457;&#25381;&#20316;&#29992;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#30340;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2303.12336</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25353;&#38656;&#20056;&#36710;&#26381;&#21153;&#36816;&#33829;&#30340;&#22810;&#21151;&#33021;&#27169;&#25311;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
A multi-functional simulation platform for on-demand ride service operations. (arXiv:2303.12336v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25353;&#38656;&#20056;&#36710;&#26381;&#21153;&#25805;&#20316;&#30340;&#22810;&#21151;&#33021;&#27169;&#25311;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#65292;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#25805;&#20316;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#35780;&#20272;&#65292;&#20248;&#21270;&#31639;&#27861;&#35843;&#25972;&#20197;&#21450;&#23454;&#26102;&#21160;&#24577;&#25805;&#20316;&#25511;&#21046;&#31561;&#26041;&#38754;&#22343;&#33021;&#21457;&#25381;&#20316;&#29992;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#25353;&#38656;&#20056;&#36710;&#26381;&#21153;&#25110;&#20056;&#36710;&#20849;&#20139;&#26381;&#21153;&#39134;&#36895;&#21457;&#23637;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#25968;&#23398;&#27169;&#22411;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24110;&#21161;&#20056;&#36710;&#20849;&#20139;&#24179;&#21488;&#35774;&#35745;&#26356;&#39640;&#25928;&#30340;&#36816;&#33829;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#26412;&#21644;&#21487;&#38752;&#24615;&#38382;&#39064;&#65288;&#23545;&#20110;&#30495;&#23454;&#25805;&#20316;&#23454;&#29616;&#19981;&#25104;&#29087;&#30340;&#31639;&#27861;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#27874;&#21160;&#65289;&#65292;&#22312;&#23454;&#38469;&#19990;&#30028;&#20056;&#36710;&#20849;&#20139;&#24179;&#21488;&#20869;&#39564;&#35777;&#36825;&#20123;&#27169;&#22411;&#24182;&#35757;&#32451;/&#27979;&#35797;&#36825;&#20123;&#20248;&#21270;&#31639;&#27861;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20316;&#20026;&#19968;&#20010;&#26377;&#29992;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#20056;&#36710;&#20849;&#20139;&#31995;&#32479;&#30340;&#27169;&#25311;&#24179;&#21488;&#23558;&#38750;&#24120;&#37325;&#35201;&#65292;&#20197;&#36890;&#36807;&#35797;&#39564;&#21644;&#35823;&#24046;&#36827;&#34892;&#31639;&#27861;&#35757;&#32451;/&#27979;&#35797;&#25110;&#27169;&#22411;&#39564;&#35777;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#20026;&#20182;&#20204;&#33258;&#24049;&#30340;&#20219;&#21153;&#24314;&#31435;&#20102;&#21508;&#31181;&#27169;&#25311;&#22120;&#65292;&#20294;&#32570;&#23569;&#19968;&#20010;&#20844;&#27491;&#21644;&#20844;&#24320;&#30340;&#24179;&#21488;&#26469;&#27604;&#36739;&#19981;&#21516;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#30340;&#27169;&#22411;&#25110;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#20174;&#28789;&#27963;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21040;&#30495;&#23454;&#24230;&#37117;&#26377;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25353;&#38656;&#20056;&#36710;&#26381;&#21153;&#25805;&#20316;&#30340;&#22810;&#21151;&#33021;&#27169;&#25311;&#24179;&#21488;&#65292;&#20854;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#29992;&#25143;&#22330;&#26223;&#21644;&#31995;&#32479;&#37197;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#24179;&#21488;&#30340;&#26377;&#29992;&#24615;&#65292;&#21253;&#25324;&#25805;&#20316;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#35780;&#20272;&#65292;&#20248;&#21270;&#31639;&#27861;&#35843;&#25972;&#20197;&#21450;&#23454;&#26102;&#21160;&#24577;&#25805;&#20316;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#19981;&#20165;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#27604;&#36739;&#21508;&#31181;&#31639;&#27861;&#21644;&#27169;&#22411;&#30340;&#20844;&#20849;&#24179;&#21488;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#30001;&#23454;&#36341;&#32773;&#30452;&#25509;&#20351;&#29992;&#65292;&#20197;&#25552;&#39640;&#20182;&#20204;&#30340;&#25805;&#20316;&#25928;&#29575;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-demand ride services or ride-sourcing services have been experiencing fast development in the past decade. Various mathematical models and optimization algorithms have been developed to help ride-sourcing platforms design operational strategies with higher efficiency. However, due to cost and reliability issues (implementing an immature algorithm for real operations may result in system turbulence), it is commonly infeasible to validate these models and train/test these optimization algorithms within real-world ride sourcing platforms. Acting as a useful test bed, a simulation platform for ride-sourcing systems will be very important to conduct algorithm training/testing or model validation through trails and errors. While previous studies have established a variety of simulators for their own tasks, it lacks a fair and public platform for comparing the models or algorithms proposed by different researchers. In addition, the existing simulators still face many challenges, ranging fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#23454;&#39564;&#35777;&#26126;&#25237;&#24433;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12289;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#21644;&#36719;&#26368;&#22823;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23481;&#37327;&#24046;&#24322;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21450;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11098</link><description>&lt;p&gt;
&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A closer look at the training dynamics of knowledge distillation. (arXiv:2303.11098v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#23454;&#39564;&#35777;&#26126;&#25237;&#24433;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12289;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#21644;&#36719;&#26368;&#22823;&#20989;&#25968;&#30340;&#36873;&#25321;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#23481;&#37327;&#24046;&#24322;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21450;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#30340;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#23558;&#30693;&#35782;&#33976;&#39311;&#20316;&#20026;&#20989;&#25968;&#21305;&#37197;&#21644;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#39564;&#35777;&#19977;&#20010;&#37325;&#35201;&#35774;&#35745;&#20915;&#31574;&#65292;&#21363;&#26631;&#20934;&#21270;&#12289;&#36719;&#26368;&#22823;&#20989;&#25968;&#21644;&#25237;&#24433;&#23618;&#20316;&#20026;&#20851;&#38190;&#35201;&#32032;&#65292;&#25105;&#20204;&#26377;&#29702;&#35770;&#22320;&#26174;&#31034;&#20986;&#25237;&#24433;&#22120;&#38544;&#21547;&#22320;&#32534;&#30721;&#20102;&#20851;&#20110;&#36807;&#21435;&#26679;&#26412;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#23398;&#29983;&#25552;&#20379;&#20102;&#20851;&#32852;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#19982;&#25237;&#24433;&#22120;&#30340;&#35757;&#32451;&#21160;&#24577;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#21487;&#33021;&#23545;&#23398;&#29983;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#36719;&#26368;&#22823;&#20989;&#25968;&#21487;&#20197;&#29992;&#26469;&#35299;&#20915;&#20219;&#20309;&#26174;&#33879;&#23481;&#37327;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30456;&#23218;&#32654;&#25110;&#20248;&#20110;&#20854;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#20445;&#22312;&#36229;&#36234;&#23454;&#26102;&#25110;&#38750;&#24120;&#32531;&#24930;&#30340;&#25805;&#20316;&#20013;&#30340;&#33258;&#20027;&#31995;&#32479;&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#31435;&#8220;&#39044;&#20808;&#25511;&#21046;&#25351;&#20196;&#8221;&#26694;&#26550;&#30340;&#8220;&#33258;&#20027;&#21629;&#20196;&#8221;&#65292;&#26469;&#23454;&#29616;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#30340;&#38382;&#36131;&#21644;&#36131;&#20219;&#25152;&#38656;&#30340;&#28145;&#24605;&#29087;&#34385;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.06813</link><description>&lt;p&gt;
&#24847;&#20041;&#28145;&#36828;&#30340;&#20154;&#31867;&#25351;&#20196;&#65306;&#20316;&#20026;&#23454;&#29616;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#36947;&#24503;&#21644;&#27861;&#24459;&#36131;&#20219;&#30340;&#26041;&#27861;&#30340;&#39640;&#32423;&#25511;&#21046;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
Meaningful human command: Advance control directives as a method to enable moral and legal responsibility for autonomous weapons systems. (arXiv:2303.06813v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#20445;&#22312;&#36229;&#36234;&#23454;&#26102;&#25110;&#38750;&#24120;&#32531;&#24930;&#30340;&#25805;&#20316;&#20013;&#30340;&#33258;&#20027;&#31995;&#32479;&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#31435;&#8220;&#39044;&#20808;&#25511;&#21046;&#25351;&#20196;&#8221;&#26694;&#26550;&#30340;&#8220;&#33258;&#20027;&#21629;&#20196;&#8221;&#65292;&#26469;&#23454;&#29616;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#30340;&#38382;&#36131;&#21644;&#36131;&#20219;&#25152;&#38656;&#30340;&#28145;&#24605;&#29087;&#34385;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
21&#19990;&#32426;&#25112;&#20105;&#30340;&#36895;&#24230;&#27491;&#22312;&#21152;&#24555;&#65292;&#24120;&#35268;&#37096;&#38431;&#19982;&#22823;&#35268;&#27169;&#20351;&#29992;&#33258;&#20027;&#31995;&#32479;&#21644;&#20154;&#26426;&#38598;&#25104;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#20154;&#31867;&#22914;&#20309;&#30830;&#20445;&#22312;&#27491;&#24120;&#26102;&#38388;&#21442;&#25968;&#20043;&#22806;&#36816;&#34892;&#30340;&#31995;&#32479;&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#36131;&#20219;&#12290;&#26412;&#31456;&#32771;&#34385;&#20102;&#20154;&#31867;&#26159;&#21542;&#21487;&#20197;&#31449;&#22312;&#23454;&#26102;&#20043;&#22806;&#65292;&#24182;&#36890;&#36807;&#20808;&#24314;&#31435;&#21512;&#21516;&#25480;&#26435;&#33258;&#20027;&#31995;&#32479;&#30340;&#34892;&#21160;&#65292;&#22312;&#26410;&#26469;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26159;&#22312;&#36229;&#36234;&#23454;&#26102;&#25110;&#38750;&#24120;&#32531;&#24930;&#30340;&#25805;&#20316;&#20013;&#65292;&#20154;&#31867;&#30340;&#24847;&#35782;&#21644;&#38598;&#20013;&#21147;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#30693;&#24773;&#12290;&#22312;&#8220;&#39044;&#20808;&#21307;&#30103;&#27861;&#24459;&#20808;&#20363;&#8221;&#20013;&#25214;&#21040;&#30340;&#32463;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#8220;&#39044;&#20808;&#25511;&#21046;&#25351;&#20196;&#8221;&#65288;ACD&#65289;&#21487;&#20197;&#23454;&#29616;&#27494;&#22120;&#31995;&#32479;&#30340;&#38382;&#36131;&#21644;&#36131;&#20219;&#25152;&#38656;&#30340;&#32791;&#26102;&#12289;&#28145;&#24605;&#29087;&#34385;&#30340;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#20027;&#21629;&#20196;&#8221;&#30340;&#26500;&#24819;&#65292;&#24182;&#36890;&#36807;ACD&#30340;&#26500;&#24314;&#21644;&#27861;&#24459;&#20262;&#29702;&#26694;&#26550;&#36827;&#34892;&#25903;&#25745;&#21644;&#21512;&#27861;&#21270;&#12290;&#36825;&#23558;&#20351;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#30340;&#25509;&#21463;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#24314;&#31435;&#24847;&#20041;&#28145;&#36828;&#30340;&#20154;&#31867;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
21st Century war is increasing in speed, with conventional forces combined with massed use of autonomous systems and human-machine integration. However, a significant challenge is how humans can ensure moral and legal responsibility for systems operating outside of normal temporal parameters. This chapter considers whether humans can stand outside of real time and authorise actions for autonomous systems by the prior establishment of a contract, for actions to occur in a future context particularly in faster than real time or in very slow operations where human consciousness and concentration could not remain well informed. The medical legal precdent found in 'advance care directives' suggests how the time-consuming, deliberative process required for accountability and responsibility of weapons systems may be achievable outside real time captured in an 'advance control driective' (ACD). The chapter proposes 'autonomy command' scaffolded and legitimised through the construction of ACD a
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#22609;&#24418;&#24418;&#24335;&#65292;&#21033;&#29992;&#22810;&#23618;&#27425;&#30340;&#25277;&#35937;&#26469;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#23545;&#25277;&#35937;&#27169;&#22411;&#30340;&#35774;&#35745;&#35201;&#27714;&#36739;&#23569;&#65292;&#20855;&#26377;&#23481;&#24525;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.00516</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#22609;&#24418;&#22312;&#22522;&#20110;&#24773;&#33410;&#30340;RL&#20013;&#21033;&#29992;&#22810;&#37325;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Exploiting Multiple Abstractions in Episodic RL via Reward Shaping. (arXiv:2303.00516v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00516
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#22609;&#24418;&#24418;&#24335;&#65292;&#21033;&#29992;&#22810;&#23618;&#27425;&#30340;&#25277;&#35937;&#26469;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#23545;&#25277;&#35937;&#27169;&#22411;&#30340;&#35774;&#35745;&#35201;&#27714;&#36739;&#23569;&#65292;&#20855;&#26377;&#23481;&#24525;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35768;&#22810;&#23454;&#38469;&#39046;&#22495;&#24212;&#29992;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#32447;&#24615;&#23618;&#27425;&#30340;&#25277;&#35937;&#23618;&#27425;&#12290;&#27599;&#20010;&#23618;&#27425;&#37117;&#26159;&#19968;&#20010;&#34920;&#31034;&#27604;&#23618;&#27425;&#32467;&#26500;&#19979;&#26041;&#21363;&#21051;&#27169;&#22411;&#26356;&#31895;&#31961;&#30340;&#27169;&#22411;&#30340;MDP&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#22609;&#24418;&#24418;&#24335;&#65292;&#20854;&#20013;&#22312;&#25277;&#35937;&#23618;&#38754;&#33719;&#24471;&#30340;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#21521;&#26356;&#20855;&#20307;&#30340;MDP&#25552;&#20379;&#22870;&#21169;&#65292;&#20197;&#20351;&#25277;&#35937;&#35299;&#20915;&#26041;&#26696;&#25351;&#23548;&#26356;&#22797;&#26434;&#39046;&#22495;&#20013;&#30340;&#23398;&#20064;&#12290;&#19982;&#23618;&#27425;RL&#20013;&#30340;&#20854;&#20182;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#25277;&#35937;&#27169;&#22411;&#30340;&#35774;&#35745;&#26041;&#38754;&#26377;&#24456;&#23569;&#30340;&#35201;&#27714;&#65292;&#24182;&#19988;&#20063;&#23545;&#24314;&#27169;&#38169;&#35823;&#20855;&#26377;&#23481;&#24525;&#24615;&#65292;&#20174;&#32780;&#20351;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21464;&#24471;&#23454;&#29992;&#12290;&#25105;&#20204;&#27491;&#24335;&#20998;&#26512;&#20102;&#25277;&#35937;&#27169;&#22411;&#19982;&#24341;&#21457;&#25506;&#32034;&#21551;&#21457;&#24335;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
One major limitation to the applicability of Reinforcement Learning (RL) to many practical domains is the large number of samples required to learn an optimal policy. To address this problem and improve learning efficiency, we consider a linear hierarchy of abstraction layers of the Markov Decision Process (MDP) underlying the target domain. Each layer is an MDP representing a coarser model of the one immediately below in the hierarchy. In this work, we propose a novel form of Reward Shaping where the solution obtained at the abstract level is used to offer rewards to the more concrete MDP, in such a way that the abstract solution guides the learning in the more complex domain. In contrast with other works in Hierarchical RL, our technique has few requirements in the design of the abstract models and it is also tolerant to modeling errors, thus making the proposed approach practical. We formally analyze the relationship between the abstract models and the exploration heuristic induced 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36816;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#22320;&#35782;&#21035;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2302.11239</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#30340;&#21487;&#35299;&#37322;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Contextual Anomaly Detection using Quantile Regression Forests. (arXiv:2302.11239v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11239
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36816;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#22320;&#35782;&#21035;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#24179;&#31561;&#23545;&#24453;&#25152;&#26377;&#29305;&#24449;&#26469;&#35782;&#21035;&#20559;&#31163;&#22823;&#22810;&#25968;&#20854;&#20182;&#23545;&#35937;&#30340;&#23545;&#35937;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#23558;&#29305;&#24449;&#21010;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#24449;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#26088;&#22312;&#26816;&#27979;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20381;&#36182;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#30001;&#27492;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20869;&#22312;&#30340;&#21487;&#35299;&#37322;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#12290;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#19978;&#19979;&#25991;&#24322;&#24120;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-art&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional anomaly detection methods aim to identify objects that deviate from most other objects by treating all features equally. In contrast, contextual anomaly detection methods aim to detect objects that deviate from other objects within a context of similar objects by dividing the features into contextual features and behavioral features. In this paper, we develop connections between dependency-based traditional anomaly detection methods and contextual anomaly detection methods. Based on resulting insights, we propose a novel approach to inherently interpretable contextual anomaly detection that uses Quantile Regression Forests to model dependencies between features. Extensive experiments on various synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art anomaly detection methods in identifying contextual anomalies in terms of accuracy and interpretability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#19978;&#19979;&#25991;&#35843;&#33410;&#23454;&#29616;&#36890;&#29992;&#24418;&#24577;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#24418;&#24577;&#30456;&#20851;&#30340;&#25511;&#21046;&#21442;&#25968;&#20197;&#21450;&#21033;&#29992;&#22266;&#23450;&#30340;&#27880;&#24847;&#26426;&#21046;&#35843;&#33410;&#26426;&#22120;&#20154;&#20013;&#19981;&#21516;&#32930;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11070</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#35843;&#25511;&#23454;&#29616;&#36890;&#29992;&#24418;&#24577;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Universal Morphology Control via Contextual Modulation. (arXiv:2302.11070v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#19978;&#19979;&#25991;&#35843;&#33410;&#23454;&#29616;&#36890;&#29992;&#24418;&#24577;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#24418;&#24577;&#30456;&#20851;&#30340;&#25511;&#21046;&#21442;&#25968;&#20197;&#21450;&#21033;&#29992;&#22266;&#23450;&#30340;&#27880;&#24847;&#26426;&#21046;&#35843;&#33410;&#26426;&#22120;&#20154;&#20013;&#19981;&#21516;&#32930;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#25511;&#21046;&#20013;&#65292;&#23398;&#20064;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#26426;&#22120;&#20154;&#24418;&#24577;&#30340;&#36890;&#29992;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20250;&#24102;&#26469;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#22240;&#20026;&#26368;&#20248;&#31574;&#30053;&#21487;&#33021;&#22312;&#19981;&#21516;&#26426;&#22120;&#20154;&#20043;&#38388;&#26377;&#24456;&#22823;&#24046;&#24322;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#24418;&#24577;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25110;transformers&#26469;&#22788;&#29702;&#19981;&#21516;&#24418;&#24577;&#20043;&#38388;&#30340;&#24322;&#26500;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#20294;&#23545;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#31574;&#30053;&#19982;&#24418;&#24577;&#19978;&#19979;&#25991;&#30340;&#20381;&#36182;&#24615;&#20851;&#27880;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#35843;&#33410;&#26356;&#22909;&#22320;&#24314;&#27169;&#36825;&#31181;&#20381;&#36182;&#20851;&#31995;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#23376;&#27169;&#22359;&#65306;&#65288;1&#65289;&#25105;&#20204;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#24418;&#24577;&#30456;&#20851;&#30340;&#25511;&#21046;&#21442;&#25968;&#65292;&#32780;&#19981;&#26159;&#23545;&#26426;&#22120;&#20154;&#20043;&#38388;&#24378;&#21046;&#36827;&#34892;&#30828;&#21442;&#25968;&#20849;&#20139;&#65307;&#65288;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#20165;&#20381;&#36182;&#20110;&#24418;&#24577;&#65292;&#26469;&#35843;&#33410;&#26426;&#22120;&#20154;&#20013;&#19981;&#21516;&#32930;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a universal policy across different robot morphologies can significantly improve learning efficiency and generalization in continuous control. However, it poses a challenging multi-task reinforcement learning problem, as the optimal policy may be quite different across robots and critically depend on the morphology. Existing methods utilize graph neural networks or transformers to handle heterogeneous state and action spaces across different morphologies, but pay little attention to the dependency of a robot's control policy on its morphology context. In this paper, we propose a hierarchical architecture to better model this dependency via contextual modulation, which includes two key submodules: (1) Instead of enforcing hard parameter sharing across robots, we use hypernetworks to generate morphology-dependent control parameters; (2) We propose a fixed attention mechanism that solely depends on the morphology to modulate the interactions between different limbs in a robot. Ex
&lt;/p&gt;</description></item><item><title>DiSProD&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#27010;&#29575;&#36716;&#31227;&#30340;&#22312;&#32447;&#35268;&#21010;&#22120;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#36817;&#20284;&#20256;&#25773;&#29983;&#25104;&#21487;&#24494;&#20998;&#30340;&#31526;&#21495;&#22270;&#34920;&#31034;&#31574;&#30053;&#20215;&#20540;&#65292;&#22312;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#21644;&#38543;&#26426;&#29615;&#22659;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#35268;&#21010;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.01491</link><description>&lt;p&gt;
DiSProD&#65306;&#29992;&#20110;&#35268;&#21010;&#30340;&#21487;&#24494;&#20998;&#31526;&#21495;&#20998;&#24067;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
DiSProD: Differentiable Symbolic Propagation of Distributions for Planning. (arXiv:2302.01491v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01491
&lt;/p&gt;
&lt;p&gt;
DiSProD&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#27010;&#29575;&#36716;&#31227;&#30340;&#22312;&#32447;&#35268;&#21010;&#22120;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#36817;&#20284;&#20256;&#25773;&#29983;&#25104;&#21487;&#24494;&#20998;&#30340;&#31526;&#21495;&#22270;&#34920;&#31034;&#31574;&#30053;&#20215;&#20540;&#65292;&#22312;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#21644;&#38543;&#26426;&#29615;&#22659;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#32447;&#35268;&#21010;&#22120;DiSProD&#65292;&#29992;&#20110;&#22788;&#29702;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#27010;&#29575;&#36716;&#31227;&#30340;&#29615;&#22659;&#12290;DiSProD&#24314;&#31435;&#19968;&#20010;&#31526;&#21495;&#22270;&#65292;&#25429;&#25417;&#26410;&#26469;&#36712;&#36857;&#30340;&#20998;&#24067;&#65292;&#22522;&#20110;&#32473;&#23450;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#29420;&#31435;&#20551;&#35774;&#21644;&#27010;&#29575;&#20998;&#24067;&#30340;&#36817;&#20284;&#20256;&#25773;&#12290;&#35813;&#31526;&#21495;&#22270;&#25552;&#20379;&#20102;&#31574;&#30053;&#20215;&#20540;&#30340;&#21487;&#24494;&#20998;&#34920;&#31034;&#65292;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#38271;&#26102;&#38388;&#25628;&#32034;&#30340;&#39640;&#25928;&#26799;&#24230;&#20248;&#21270;&#12290;&#36817;&#20284;&#20998;&#24067;&#30340;&#20256;&#25773;&#21487;&#20197;&#30475;&#20316;&#26159;&#35768;&#22810;&#36712;&#36857;&#30340;&#32858;&#21512;&#65292;&#38750;&#24120;&#36866;&#21512;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#21644;&#38543;&#26426;&#29615;&#22659;&#12290;&#36890;&#36807;&#22312;&#31163;&#25955;&#26102;&#38388;&#35268;&#21010;&#21644;&#23454;&#26102;&#25511;&#21046;&#26426;&#22120;&#20154;&#31995;&#32479;&#26041;&#38754;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#24615;&#35780;&#20272;&#65292;&#35813;&#35770;&#25991;&#23558;DiSProD&#19982;&#26368;&#20808;&#36827;&#30340;&#35268;&#21010;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#38543;&#26426;&#29615;&#22659;&#12289;&#23545;&#25628;&#32034;&#28145;&#24230;&#30340;&#25935;&#24863;&#24615;&#12289;&#22870;&#21169;&#30340;&#31232;&#30095;&#24615;&#21644;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#26041;&#38754;&#27604;&#29616;&#26377;&#35268;&#21010;&#22120;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper introduces DiSProD, an online planner developed for environments with probabilistic transitions in continuous state and action spaces. DiSProD builds a symbolic graph that captures the distribution of future trajectories, conditioned on a given policy, using independence assumptions and approximate propagation of distributions. The symbolic graph provides a differentiable representation of the policy's value, enabling efficient gradient-based optimization for long-horizon search. The propagation of approximate distributions can be seen as an aggregation of many trajectories, making it well-suited for dealing with sparse rewards and stochastic environments. An extensive experimental evaluation compares DiSProD to state-of-the-art planners in discrete-time planning and real-time control of robotic systems. The proposed method improves over existing planners in handling stochastic environments, sensitivity to search depth, sparsity of rewards, and large action spaces. Additional
&lt;/p&gt;</description></item><item><title>GPT-3&#22312;&#35768;&#22810;&#31867;&#27604;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09196</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31867;&#27604;&#25512;&#29702;&#30340;&#32039;&#24613;&#24615;
&lt;/p&gt;
&lt;p&gt;
Emergent Analogical Reasoning in Large Language Models. (arXiv:2212.09196v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09196
&lt;/p&gt;
&lt;p&gt;
GPT-3&#22312;&#35768;&#22810;&#31867;&#27604;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#37325;&#26032;&#28857;&#29123;&#20102;&#20154;&#20204;&#23545;&#20110;&#36825;&#26679;&#19968;&#31181;&#38382;&#39064;&#30340;&#36777;&#35770;&#65306;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#21542;&#33021;&#20351;&#36825;&#20123;&#36890;&#29992;&#27169;&#22411;&#20869;&#28085;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#12290;&#29305;&#21035;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#8212;&#8212;&#19981;&#32463;&#36807;&#20219;&#20309;&#30452;&#25509;&#35757;&#32451;&#65292;&#23601;&#33021;&#22815;&#25512;&#29702;&#20986;&#26032;&#38382;&#39064;&#65292;&#29305;&#21035;&#20196;&#20154;&#20851;&#27880;&#12290;&#22312;&#20154;&#31867;&#35748;&#30693;&#20013;&#65292;&#36825;&#31181;&#33021;&#21147;&#19982;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#31867;&#27604;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#30452;&#25509;&#30340;&#20154;&#26426;&#27604;&#36739;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#30697;&#38453;&#25512;&#29702;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#19982; Raven's Progressive Matrices&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3&#21576;&#29616;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#25277;&#35937;&#27169;&#24335;&#24402;&#32435;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#19982;&#25110;&#29978;&#33267;&#36229;&#36234;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#33719;&#24471;&#20102;&#22312;&#24191;&#27867;&#30340;&#31867;&#27604;&#38382;&#39064;&#19978;&#25214;&#21040;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here, we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of GPT-3) on a range of analogical tasks, including a novel text-based matrix reasoning task closely modeled on Raven's Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#65292;&#23545;&#20110;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2212.00679</link><description>&lt;p&gt;
&#26410;&#30693;&#21160;&#24577;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#25511;&#21046;&#22120;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Formal Controller Synthesis for Markov Jump Linear Systems with Uncertain Dynamics. (arXiv:2212.00679v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#65292;&#23545;&#20110;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#65292;&#23545;&#20110;&#25511;&#21046;&#22120;&#30340;&#33258;&#21160;&#21270;&#21512;&#25104;&#21487;&#20197;&#30830;&#20445;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#28151;&#21512;&#29305;&#24615;&#21644;&#38543;&#26426;&#25110;&#26410;&#30693;&#30340;&#34892;&#20026;&#20351;&#24471;&#21512;&#25104;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#36825;&#26159;&#19968;&#31867;&#31163;&#25955;&#26102;&#38047;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#12290;&#19968;&#20010;MJLS&#30001;&#19968;&#32452;&#26377;&#38480;&#30340;&#38543;&#26426;&#32447;&#24615;&#21160;&#24577;&#21644;&#36825;&#20123;&#21160;&#24577;&#20043;&#38388;&#30340;&#31163;&#25955;&#36339;&#21464;&#32452;&#25104;&#65292;&#36825;&#20123;&#36339;&#21464;&#30001;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#31649;&#29702;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20010;MDP&#30340;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#26377;&#38480;&#29366;&#24577;&#25277;&#35937;&#65292;&#25429;&#25417;&#20102;MJLS&#30340;&#31163;&#25955;&#65288;&#27169;&#24335;&#36339;&#36291;&#65289;&#21644;&#36830;&#32493;&#65288;&#38543;&#26426;&#32447;&#24615;&#65289;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25277;&#35937;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21306;&#38388;MDP&#65288;iMDP&#65289;&#65292;&#28982;&#21518;&#35745;&#31639;&#20102;&#29366;&#24577;&#36716;&#31227;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated synthesis of provably correct controllers for cyber-physical systems is crucial for deployment in safety-critical scenarios. However, hybrid features and stochastic or unknown behaviours make this problem challenging. We propose a method for synthesising controllers for Markov jump linear systems (MJLSs), a class of discrete-time models for cyber-physical systems, so that they certifiably satisfy probabilistic computation tree logic (PCTL) formulae. An MJLS consists of a finite set of stochastic linear dynamics and discrete jumps between these dynamics that are governed by a Markov decision process (MDP). We consider the cases where the transition probabilities of this MDP are either known up to an interval or completely unknown. Our approach is based on a finite-state abstraction that captures both the discrete (mode-jumping) and continuous (stochastic linear) behaviour of the MJLS. We formalise this abstraction as an interval MDP (iMDP) for which we compute intervals of tra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#39063;&#31890;&#29289;&#23545;&#20202;&#22120;&#30340;&#21361;&#23475;&#38382;&#39064;&#65292;&#36890;&#36807;Bi-GRU&#31639;&#27861;&#26500;&#24314;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#65292;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#26377;&#28508;&#21147;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#12290;</title><link>http://arxiv.org/abs/2210.08549</link><description>&lt;p&gt;
&#22269;&#38469;&#31354;&#38388;&#31449;&#33258;&#21160;&#32039;&#24613;&#26080;&#23576;&#35299;&#20915;&#26041;&#26696;: &#24102;&#26377;Bi-GRU&#30340;(AED-ISS)
&lt;/p&gt;
&lt;p&gt;
Automatic Emergency Dust-Free solution on-board International Space Station with Bi-GRU (AED-ISS). (arXiv:2210.08549v2 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#39063;&#31890;&#29289;&#23545;&#20202;&#22120;&#30340;&#21361;&#23475;&#38382;&#39064;&#65292;&#36890;&#36807;Bi-GRU&#31639;&#27861;&#26500;&#24314;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#65292;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#26377;&#28508;&#21147;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;PM2.5&#25110;PM0.3&#38382;&#39064;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#21152;&#65292;&#39063;&#31890;&#29289;&#19981;&#20165;&#23545;&#29615;&#22659;&#21644;&#20154;&#31867;&#26500;&#25104;&#28508;&#22312;&#23041;&#32961;&#65292;&#32780;&#19988;&#23545;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#30340;&#20202;&#22120;&#20063;&#20250;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#22242;&#38431;&#26088;&#22312;&#23558;&#21508;&#31181;&#39063;&#31890;&#29289;&#27987;&#24230;&#19982;&#30913;&#22330;&#12289;&#28287;&#24230;&#12289;&#21152;&#36895;&#24230;&#12289;&#28201;&#24230;&#12289;&#21387;&#21147;&#21644;CO2&#27987;&#24230;&#20851;&#32852;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;(EWS)&#65292;&#33021;&#22815;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#65292;&#20197;&#20445;&#25252;&#20182;&#20204;&#22312;&#26576;&#20123;&#23454;&#39564;&#20013;&#30340;&#20202;&#22120;&#65292;&#25110;&#32773;&#25552;&#39640;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#65307;&#27492;&#22806;&#65292;&#25152;&#26500;&#24314;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#30340;&#21407;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23454;&#29616;Bi-GRU(&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;)&#31639;&#27861;&#65292;&#25910;&#38598;&#36807;&#21435;90&#20998;&#38047;&#30340;&#25968;&#25454;&#65292;&#24182;&#39044;&#27979;&#36229;&#36807;2.5&#24494;&#31859;&#30340;&#39063;&#31890;&#29289;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a rising attention for the issue of PM2.5 or PM0.3, particulate matters have become not only a potential threat to both the environment and human, but also a harming existence to instruments onboard International Space Station (ISS). Our team is aiming to relate various concentration of particulate matters to magnetic fields, humidity, acceleration, temperature, pressure and CO2 concentration. Our goal is to establish an early warning system (EWS), which is able to forecast the levels of particulate matters and provides ample reaction time for astronauts to protect their instruments in some experiments or increase the accuracy of the measurements; In addition, the constructed model can be further developed into a prototype of a remote-sensing smoke alarm for applications related to fires. In this article, we will implement the Bi-GRU (Bidirectional Gated Recurrent Unit) algorithms that collect data for past 90 minutes and predict the levels of particulates which over 2.5 micromete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSIVD-Net&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#26292;&#21147;&#35782;&#21035;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26174;&#33879;-&#36229;&#32423;&#22270;&#20687;&#34920;&#31034;&#20943;&#23569;&#20102;3D&#35270;&#39057;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#12289;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8220;Salient-Classifier&#8221;&#65292;&#23558;&#26680;&#26041;&#27861;&#21644;&#27531;&#24046;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2207.12850</link><description>&lt;p&gt;
SSIVD-Net&#65306;&#19968;&#31181;&#26032;&#30340;&#27494;&#22120;&#21270;&#26292;&#21147;&#26174;&#33879;&#36229;&#32423;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
SSIVD-Net: A Novel Salient Super Image Classification &amp; Detection Technique for Weaponized Violence. (arXiv:2207.12850v6 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSIVD-Net&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#26292;&#21147;&#35782;&#21035;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26174;&#33879;-&#36229;&#32423;&#22270;&#20687;&#34920;&#31034;&#20943;&#23569;&#20102;3D&#35270;&#39057;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;&#25512;&#26029;&#12289;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8220;Salient-Classifier&#8221;&#65292;&#23558;&#26680;&#26041;&#27861;&#21644;&#27531;&#24046;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38381;&#36335;&#30005;&#35270;&#65288;CCTV&#65289;&#30417;&#25511;&#24405;&#20687;&#20013;&#26816;&#27979;&#26292;&#21147;&#21644;&#27494;&#22120;&#21270;&#26292;&#21147;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#26234;&#24935;&#22478;&#24066;CCTV&#26292;&#21147;&#26816;&#27979;&#65288;SCVD&#65289;&#8221;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#30417;&#25511;&#35270;&#39057;&#20013;&#27494;&#22120;&#20998;&#24067;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#26512;3D&#30417;&#25511;&#35270;&#39057;&#36827;&#34892;&#26292;&#21147;&#35782;&#21035;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;SSIVD-Net&#65288;&#29992;&#20110;&#26292;&#21147;&#26816;&#27979;&#30340;&#26174;&#33879;-&#36229;&#32423;-&#22270;&#20687;&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#26174;&#33879;-&#36229;&#32423;&#22270;&#20687;&#34920;&#31034;&#20943;&#23569;3D&#35270;&#39057;&#25968;&#25454;&#22797;&#26434;&#24615;&#12289;&#38477;&#32500;&#21644;&#20449;&#24687;&#25439;&#22833;&#65292;&#21516;&#26102;&#25552;&#39640;&#25512;&#26029;&#12289;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#32771;&#34385;&#21040;&#26410;&#26469;&#26234;&#24935;&#22478;&#24066;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#35201;&#27714;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8220;Salient-Classifier&#8221;&#65292;&#23558;&#26680;&#26041;&#27861;&#21644;&#27531;&#24046;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;SSIVD-Net&#22312;SCVD&#12289;Hockey Fight&#12289;Moviescope&#20197;&#21450;Large-Scale Fight Detection&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#29616;&#26377;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of violence and weaponized violence in closed-circuit television (CCTV) footage requires a comprehensive approach. In this work, we introduce the \emph{Smart-City CCTV Violence Detection (SCVD)} dataset, specifically designed to facilitate the learning of weapon distribution in surveillance videos. To tackle the complexities of analyzing 3D surveillance video for violence recognition tasks, we propose a novel technique called, \emph{SSIVD-Net} (\textbf{S}alient-\textbf{S}uper-\textbf{I}mage for \textbf{V}iolence \textbf{D}etection). Our method reduces 3D video data complexity, dimensionality, and information loss while improving inference, performance, and explainability through the use of Salient-Super-Image representations. Considering the scalability and sustainability requirements of futuristic smart cities, the authors introduce the \emph{Salient-Classifier}, a novel architecture combining a kernelized approach with a residual learning strategy. We evaluate variations of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22312;&#32447;Mondrian Forest&#20998;&#31867;&#31639;&#27861;&#36866;&#24212;&#21040;&#22312;&#25968;&#25454;&#27969;&#19978;&#20855;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#24182;&#35774;&#35745;&#20102;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#21644;&#20462;&#21098;&#26426;&#21046;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#37197;&#32622;&#20013;&#65292;Extend Node&#31574;&#30053;&#26159;&#26368;&#20339;&#30340;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2205.07871</link><description>&lt;p&gt;
&#22312;&#20869;&#23384;&#38480;&#21046;&#19979;&#29992;&#20110;&#25968;&#25454;&#27969;&#20998;&#31867;&#30340;Mondrian Forest&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Mondrian Forest for Data Stream Classification Under Memory Constraints. (arXiv:2205.07871v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22312;&#32447;Mondrian Forest&#20998;&#31867;&#31639;&#27861;&#36866;&#24212;&#21040;&#22312;&#25968;&#25454;&#27969;&#19978;&#20855;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#24182;&#35774;&#35745;&#20102;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#21644;&#20462;&#21098;&#26426;&#21046;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#37197;&#32622;&#20013;&#65292;Extend Node&#31574;&#30053;&#26159;&#26368;&#20339;&#30340;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#26377;&#36275;&#22815;&#30340;&#20869;&#23384;&#26469;&#23384;&#20648;&#25968;&#25454;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#29289;&#32852;&#32593;&#20013;&#65292;&#24403;&#25968;&#25454;&#20197;&#26080;&#38480;&#25968;&#25454;&#27969;&#30340;&#24418;&#24335;&#20986;&#29616;&#26102;&#65292;&#25110;&#32773;&#24403;&#23398;&#20064;&#31639;&#27861;&#37096;&#32626;&#22312;&#20855;&#26377;&#36739;&#23569;&#20869;&#23384;&#30340;&#35774;&#22791;&#19978;&#26102;&#65292;&#36825;&#20010;&#20551;&#35774;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22312;&#32447;Mondrian Forest&#20998;&#31867;&#31639;&#27861;&#36866;&#24212;&#21040;&#22312;&#25968;&#25454;&#27969;&#19978;&#20855;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20116;&#31181;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#65292;&#20197;&#22312;&#36798;&#21040;&#20869;&#23384;&#38480;&#21046;&#26102;&#26356;&#26032;Mondrian&#26641;&#30340;&#26032;&#25968;&#25454;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20462;&#21098;&#26426;&#21046;&#65292;&#22312;&#20869;&#23384;&#38480;&#21046;&#19979;&#20351;Mondrian&#26641;&#23545;&#27010;&#24565;&#28418;&#31227;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#26368;&#21518;&#32473;&#20986;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#20204;&#30340;&#24314;&#35758;&#65306;&#22312;&#25152;&#26377;&#37197;&#32622;&#20013;&#65292;Extend Node&#31574;&#30053;&#20284;&#20046;&#26159;&#26368;&#20339;&#30340;&#20869;&#23384;&#19981;&#36275;&#31574;&#30053;&#65292;&#32780;&#20462;&#21098;&#26426;&#21046;&#21017;&#22240;&#20855;&#20307;&#24773;&#20917;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning algorithms generally assume the availability of enough memory to store their data model during the training and test phases. However, in the Internet of Things, this assumption is unrealistic when data comes in the form of infinite data streams, or when learning algorithms are deployed on devices with reduced amounts of memory. In this paper, we adapt the online Mondrian forest classification algorithm to work with memory constraints on data streams. In particular, we design five out-of-memory strategies to update Mondrian trees with new data points when the memory limit is reached. Moreover, we design trimming mechanisms to make Mondrian trees more robust to concept drifts under memory constraints. We evaluate our algorithms on a variety of real and simulated datasets, and we conclude with recommendations on their use in different situations: the Extend Node strategy appears as the best out-of-memory strategy in all configurations, whereas different trimming mechan
&lt;/p&gt;</description></item></channel></rss>