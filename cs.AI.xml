<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TeCH&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#26041;&#27861;&#26469;&#37325;&#24314;&#36924;&#30495;&#30340;&#26381;&#39280;&#20154;&#29289;&#12290;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#24674;&#22797;&#8220;&#26410;&#26366;&#30475;&#35265;&#30340;&#21306;&#22495;&#8221;&#24182;&#28155;&#21152;&#39640;&#32423;&#32454;&#33410;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;DMTet&#30340;&#28151;&#21512;3D&#34920;&#31034;&#20197;&#36798;&#21040;&#26356;&#20302;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.08545</link><description>&lt;p&gt;
TeCH: &#25991;&#26412;&#24341;&#23548;&#30340;&#36924;&#30495;&#26381;&#39280;&#20154;&#29289;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
TeCH: Text-guided Reconstruction of Lifelike Clothed Humans. (arXiv:2308.08545v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08545
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TeCH&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#26041;&#27861;&#26469;&#37325;&#24314;&#36924;&#30495;&#30340;&#26381;&#39280;&#20154;&#29289;&#12290;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#24674;&#22797;&#8220;&#26410;&#26366;&#30475;&#35265;&#30340;&#21306;&#22495;&#8221;&#24182;&#28155;&#21152;&#39640;&#32423;&#32454;&#33410;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;DMTet&#30340;&#28151;&#21512;3D&#34920;&#31034;&#20197;&#36798;&#21040;&#26356;&#20302;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#22312;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#37325;&#24314;&#30528;&#35013;&#20154;&#29289;&#26041;&#38754;&#21462;&#24471;&#20102;&#30740;&#31350;&#36827;&#23637;&#65292;&#20294;&#20934;&#30830;&#24674;&#22797;&#8220;&#26410;&#26366;&#30475;&#35265;&#30340;&#21306;&#22495;&#8221;&#24182;&#28155;&#21152;&#39640;&#32423;&#32454;&#33410;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#32570;&#20047;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#20250;&#29983;&#25104;&#36807;&#20110;&#24179;&#28369;&#30340;&#32972;&#38754;&#34920;&#38754;&#21644;&#27169;&#31946;&#30340;&#32441;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TeCH&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;1&#65289;&#36890;&#36807;&#26381;&#35013;&#35299;&#26512;&#27169;&#22411;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#25551;&#36848;&#24615;&#25991;&#26412;&#25552;&#31034;&#65288;&#20363;&#22914;&#65292;&#26381;&#39280;&#12289;&#39068;&#33394;&#12289;&#21457;&#22411;&#65289;&#65307;2&#65289;&#32463;&#36807;&#20010;&#24615;&#21270;&#24494;&#35843;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;T2I&#65289;&#26469;&#23398;&#20064;&#8220;&#26080;&#27861;&#25551;&#36848;&#8221;&#30340;&#22806;&#35266;&#65292;&#20174;&#32780;&#23545;3D&#20154;&#31867;&#24418;&#35937;&#36827;&#34892;&#37325;&#24314;&#12290;&#20026;&#20102;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#34920;&#31034;&#39640;&#20998;&#36776;&#29575;&#30340;3D&#26381;&#39280;&#20154;&#29289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;DMTet&#30340;&#28151;&#21512;3D&#34920;&#31034;&#65292;&#23427;&#30001;&#26126;&#30830;&#30340;&#36523;&#20307;&#24418;&#29366;&#32593;&#26684;&#21644;&#19968;&#20010;&#8230;&#8230;&#65288;&#25688;&#35201;&#36739;&#38271;&#65292;&#30465;&#30053;&#37096;&#20998;&#20869;&#23481;&#65289;
&lt;/p&gt;
&lt;p&gt;
Despite recent research advancements in reconstructing clothed humans from a single image, accurately restoring the "unseen regions" with high-level details remains an unsolved challenge that lacks attention. Existing methods often generate overly smooth back-side surfaces with a blurry texture. But how to effectively capture all visual attributes of an individual from a single image, which are sufficient to reconstruct unseen areas (e.g., the back view)? Motivated by the power of foundation models, TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the "indescribable" appearance. To represent high-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D representation based on DMTet, which consists of an explicit body shape grid and an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2308.08536</link><description>&lt;p&gt;
Transformers&#33021;&#21542;&#23398;&#20064;&#29992;&#20110;&#26410;&#30693;&#31995;&#32479;&#30340;&#26368;&#20248;&#28388;&#27874;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Transformers Learn Optimal Filtering for Unknown Systems?. (arXiv:2308.08536v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#23427;&#20351;&#29992;&#36807;&#21435;&#30340;&#25152;&#26377;&#36755;&#20986;&#26469;&#29983;&#25104;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20808;&#39564;&#20998;&#24067;&#30340;&#21508;&#31181;&#31995;&#32479;&#26469;&#35757;&#32451;transformer&#65292;&#28982;&#21518;&#22312;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#30456;&#21516;&#20998;&#24067;&#30340;&#31995;&#32479;&#19978;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33719;&#24471;&#30340;transformer&#23601;&#20687;&#19968;&#20010;&#39044;&#27979;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#24182;&#24555;&#36895;&#36866;&#24212;&#21644;&#39044;&#27979;&#19981;&#21516;&#30340;&#31995;&#32479;&#65292;&#22240;&#27492;&#25105;&#20204;&#31216;&#20043;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;MOP&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated remarkable success in natural language processing; however, their potential remains mostly unexplored for problems arising in dynamical systems. In this work, we investigate the optimal output estimation problem using transformers, which generate output predictions using all the past ones. We train the transformer using various systems drawn from a prior distribution and then evaluate its performance on previously unseen systems from the same distribution. As a result, the obtained transformer acts like a prediction algorithm that learns in-context and quickly adapts to and predicts well for different systems - thus we call it meta-output-predictor (MOP). MOP matches the performance of the optimal output estimator, based on Kalman filter, for most linear dynamical systems even though it does not have access to a model. We observe via extensive numerical experiments that MOP also performs well in challenging scenarios with non-i.i.d. noise, time-varying dy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#28857;&#23545;&#27880;&#24847;&#21147;&#24863;&#30693;&#26426;&#21046;&#30340;&#21452;&#21521;&#23545;&#24212;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#28857;&#21644;&#22330;&#26223;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#28857;&#23545;&#21305;&#37197;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#35266;&#23519;&#36136;&#37327;&#21644;&#36974;&#25377;&#30340;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08518</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#21521;&#39044;&#27979;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#28857;&#23545;&#27880;&#24847;&#21147;&#30340;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction. (arXiv:2308.08518v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#28857;&#23545;&#27880;&#24847;&#21147;&#24863;&#30693;&#26426;&#21046;&#30340;&#21452;&#21521;&#23545;&#24212;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#28857;&#21644;&#22330;&#26223;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#28857;&#23545;&#21305;&#37197;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#35266;&#23519;&#36136;&#37327;&#21644;&#36974;&#25377;&#30340;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20960;&#20309;&#27880;&#20876;&#20272;&#35745;&#26041;&#27861;&#20165;&#38544;&#24335;&#22320;&#21033;&#29992;CAD&#27169;&#22411;&#65292;&#36825;&#23548;&#33268;&#23427;&#20204;&#23545;&#35266;&#23519;&#36136;&#37327;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#36974;&#25377;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#28857;&#23545;&#27880;&#24847;&#21147;&#24863;&#30693;&#26426;&#21046;&#30340;&#21452;&#21521;&#23545;&#24212;&#39044;&#27979;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#19981;&#20165;&#35201;&#27714;&#27169;&#22411;&#28857;&#39044;&#27979;&#23545;&#24212;&#20851;&#31995;&#65292;&#36824;&#26126;&#30830;&#22320;&#23545;&#35266;&#23519;&#21644;&#27169;&#22411;&#20808;&#39564;&#20043;&#38388;&#30340;&#20960;&#20309;&#30456;&#20284;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#27599;&#20010;&#27169;&#22411;&#28857;&#21644;&#22330;&#26223;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20026;&#23398;&#20064;&#28857;&#23545;&#21305;&#37197;&#25552;&#20379;&#20102;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35299;&#20915;&#29305;&#24449;&#20998;&#24067;&#20998;&#27495;&#24102;&#26469;&#30340;&#30456;&#20851;&#24615;&#22122;&#22768;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20266;&#23402;&#29983;&#32593;&#32476;&#26469;&#25913;&#21892;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;&#32447;MOD&#12289;YCB-Video&#21644;Occ-LineMOD&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional geometric registration based estimation methods only exploit the CAD model implicitly, which leads to their dependence on observation quality and deficiency to occlusion.To address the problem,the paper proposes a bidirectional correspondence prediction network with a point-wise attention-aware mechanism. This network not only requires the model points to predict the correspondence but also explicitly models the geometric similarities between observations and the model prior.} Our key insight is that the correlations between each model point and scene point provide essential information for learning point-pair matches. To further tackle the correlation noises brought by feature distribution divergence, we design a simple but effective pseudo-siamese network to improve feature homogeneity.Experimental results on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show that the proposed method achieves better performance than other state-of-the-art methods under the sa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#39044;&#27979;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#36755;&#20837;&#29305;&#24449;&#26102;&#30340;&#38480;&#21046;&#65292;&#24182;&#36991;&#20813;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08502</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#29992;&#20110;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction. (arXiv:2308.08502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08502
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#39044;&#27979;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#36755;&#20837;&#29305;&#24449;&#26102;&#30340;&#38480;&#21046;&#65292;&#24182;&#36991;&#20813;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21508;&#22320;&#30340;&#20844;&#21496;&#37117;&#28212;&#26395;&#23450;&#20301;&#28508;&#22312;&#30340;&#39640;&#20215;&#20540;&#23458;&#25143;&#65292;&#20197;&#25193;&#22823;&#25910;&#20837;&#65292;&#32780;&#36825;&#21482;&#33021;&#36890;&#36807;&#26356;&#22909;&#22320;&#20102;&#35299;&#23458;&#25143;&#26469;&#23454;&#29616;&#12290;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#65288;CLV&#65289;&#26159;&#23458;&#25143;&#22312;&#19968;&#27573;&#35268;&#23450;&#30340;&#26102;&#38388;&#20869;&#19982;&#20225;&#19994;&#36827;&#34892;&#30340;&#20132;&#26131;/&#36141;&#20080;&#30340;&#24635;&#36135;&#24065;&#20215;&#20540;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#23458;&#25143;&#20114;&#21160;&#12290;CLV&#22312;&#22810;&#20010;&#19981;&#21516;&#30340;&#21830;&#19994;&#39046;&#22495;&#20013;&#37117;&#26377;&#24212;&#29992;&#65292;&#22914;&#38134;&#34892;&#65292;&#20445;&#38505;&#65292;&#22312;&#32447;&#23089;&#20048;&#65292;&#28216;&#25103;&#21644;&#30005;&#23376;&#21830;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#24067;&#21644;&#22522;&#26412;&#65288;&#26368;&#36817;&#24615;&#65292;&#39057;&#29575;&#21644;&#37329;&#39069;&#65289;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#36755;&#20837;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#26356;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#39046;&#22495;&#20013;&#22686;&#21152;&#20102;&#19981;&#24517;&#35201;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26082;&#33021;&#22815;&#26377;&#25928;&#21448;&#20840;&#38754;&#31616;&#21333;&#26131;&#25026;&#30340;&#31995;&#32479;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Companies across the globe are keen on targeting potential high-value customers in an attempt to expand revenue and this could be achieved only by understanding the customers more. Customer Lifetime Value (CLV) is the total monetary value of transactions/purchases made by a customer with the business over an intended period of time and is used as means to estimate future customer interactions. CLV finds application in a number of distinct business domains such as Banking, Insurance, Online-entertainment, Gaming, and E-Commerce. The existing distribution-based and basic (recency, frequency &amp; monetary) based models face a limitation in terms of handling a wide variety of input features. Moreover, the more advanced Deep learning approaches could be superfluous and add an undesirable element of complexity in certain application areas. We, therefore, propose a system which is able to qualify both as effective, and comprehensive yet simple and interpretable. With that in mind, we develop a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InTune&#65292;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#27969;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#22312;Netflix&#35745;&#31639;&#38598;&#32676;&#20013;&#30340;DLRM&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#27969;&#31243;&#20248;&#21270;&#22120;&#23384;&#22312;&#24615;&#33021;&#19981;&#20339;&#12289;&#39057;&#32321;&#23849;&#28291;&#25110;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#30340;&#38598;&#32676;&#37325;&#32452;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08500</link><description>&lt;p&gt;
InTune:&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#27969;&#20248;&#21270;&#29992;&#20110;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models. (arXiv:2308.08500v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InTune&#65292;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#27969;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#22312;Netflix&#35745;&#31639;&#38598;&#32676;&#20013;&#30340;DLRM&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#27969;&#31243;&#20248;&#21270;&#22120;&#23384;&#22312;&#24615;&#33021;&#19981;&#20339;&#12289;&#39057;&#32321;&#23849;&#28291;&#25110;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#30340;&#38598;&#32676;&#37325;&#32452;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#27169;&#22411;(DLRM)&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#19968;&#20123;&#20844;&#21496;&#27491;&#22312;&#24314;&#35774;&#22823;&#22411;&#35745;&#31639;&#38598;&#32676;&#19987;&#38376;&#29992;&#20110;DLRM&#35757;&#32451;&#65292;&#36827;&#32780;&#25512;&#21160;&#20102;&#23545;&#25104;&#26412;&#21644;&#26102;&#38388;&#30340;&#33410;&#32422;&#20248;&#21270;&#30340;&#26032;&#20852;&#20852;&#36259;&#12290;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#25152;&#38754;&#20020;&#30340;&#31995;&#32479;&#25361;&#25112;&#26159;&#29420;&#29305;&#30340;&#65307;&#23613;&#31649;&#20856;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20219;&#21153;&#30001;&#27169;&#22411;&#25191;&#34892;&#20027;&#23548;&#65292;&#20294;DLRM&#35757;&#32451;&#24615;&#33021;&#20013;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#24448;&#24448;&#26159;&#32447;&#19978;&#25968;&#25454;&#25668;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#25968;&#25454;&#25668;&#20837;&#38382;&#39064;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;DLRM&#35757;&#32451;&#27969;&#31243;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;Netflix&#35745;&#31639;&#38598;&#32676;&#20013;&#30495;&#23454;&#30340;DLRM&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35266;&#23519;&#20102;&#32447;&#19978;&#25668;&#20837;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#24182;&#35782;&#21035;&#20986;&#29616;&#26377;&#27969;&#31243;&#20248;&#21270;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#24037;&#20855;&#35201;&#20040;&#20135;&#29983;&#27425;&#20248;&#24615;&#33021;&#65292;&#35201;&#20040;&#32463;&#24120;&#23849;&#28291;&#65292;&#35201;&#20040;&#38656;&#35201;&#19981;&#29616;&#23454;&#30340;&#38598;&#32676;&#37325;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- and time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning training jobs are dominated by model execution, the most important factor in DLRM training performance is often online data ingestion.  In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into DLRM training pipeline bottlenecks and challenges. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and to identify shortfalls in existing pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#31038;&#20132;&#29289;&#32852;&#32593;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26381;&#21153;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#28508;&#22312;&#29305;&#24449;&#20132;&#20114;&#21644;&#24314;&#27169;&#35774;&#22791;-&#26381;&#21153;&#23545;&#30340;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35780;&#32423;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#30340;&#26381;&#21153;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2308.08499</link><description>&lt;p&gt;
&#38754;&#21521;&#31038;&#20132;&#29289;&#32852;&#32593;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26381;&#21153;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Service Recommendation System for the Social Internet of Things. (arXiv:2308.08499v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#31038;&#20132;&#29289;&#32852;&#32593;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26381;&#21153;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#28508;&#22312;&#29305;&#24449;&#20132;&#20114;&#21644;&#24314;&#27169;&#35774;&#22791;-&#26381;&#21153;&#23545;&#30340;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35780;&#32423;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#30340;&#26381;&#21153;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#29289;&#32852;&#32593;&#65288;SIoT&#65289;&#20351;&#26234;&#33021;&#35774;&#22791;&#30456;&#20114;&#36830;&#25509;&#24182;&#20849;&#20139;&#25968;&#25454;&#21644;&#26381;&#21153;&#65292;&#20026;&#20010;&#24615;&#21270;&#26381;&#21153;&#25512;&#33616;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#32463;&#24120;&#24573;&#35270;&#20102;&#21487;&#20197;&#25552;&#39640;SIoT&#29615;&#22659;&#19979;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29616;&#26377;&#25216;&#26415;&#24448;&#24448;&#21482;&#32771;&#34385;&#20102;&#35774;&#22791;&#20043;&#38388;&#30340;&#31038;&#20132;&#20851;&#31995;&#30340;&#25552;&#21462;&#65292;&#32780;&#24573;&#35270;&#20102;&#26381;&#21153;&#35780;&#35770;&#30340;&#19978;&#19979;&#25991;&#21576;&#29616;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#27599;&#20010;&#35774;&#22791;-&#26381;&#21153;&#23545;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#26469;&#22635;&#34917;&#36825;&#20123;&#32570;&#21475;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#29305;&#24449;&#32452;&#21512;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#32858;&#21512;SIoT&#20013;&#30340;&#35774;&#22791;-&#35774;&#22791;&#20851;&#31995;&#26469;&#25429;&#25417;&#28508;&#22312;&#29305;&#24449;&#20132;&#20114;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#23376;&#20998;&#35299;&#26426;&#26469;&#24314;&#27169;&#27599;&#20010;SIoT&#35774;&#22791;-&#26381;&#21153;&#23545;&#29305;&#23450;&#30340;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#35780;&#32423;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35780;&#32423;&#39044;&#27979;&#30340;SIoT&#26381;&#21153;&#25512;&#33616;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Social Internet of Things (SIoT) enables interconnected smart devices to share data and services, opening up opportunities for personalized service recommendations. However, existing research often overlooks crucial aspects that can enhance the accuracy and relevance of recommendations in the SIoT context. Specifically, existing techniques tend to consider the extraction of social relationships between devices and neglect the contextual presentation of service reviews. This study aims to address these gaps by exploring the contextual representation of each device-service pair. Firstly, we propose a latent features combination technique that can capture latent feature interactions, by aggregating the device-device relationships within the SIoT. Then, we leverage Factorization Machines to model higher-order feature interactions specific to each SIoT device-service pair to accomplish accurate rating prediction. Finally, we propose a service recommendation framework for SIoT based on r
&lt;/p&gt;</description></item><item><title>HyperBandit&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#27969;&#23186;&#20307;&#25512;&#33616;&#31995;&#32479;&#20013;&#26102;&#38388;&#21464;&#21270;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#23427;&#36890;&#36807;&#24314;&#31435;&#26102;&#38388;&#29305;&#24449;&#21644;&#29992;&#25143;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#21160;&#24577;&#35843;&#25972;&#25512;&#33616;&#27169;&#22411;&#20197;&#36866;&#24212;&#21160;&#24577;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.08497</link><description>&lt;p&gt;
HyperBandit: &#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26102;&#38388;&#21464;&#21270;&#29992;&#25143;&#20559;&#22909;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27969;&#23186;&#20307;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
HyperBandit: Contextual Bandit with Hypernewtork for Time-Varying User Preferences in Streaming Recommendation. (arXiv:2308.08497v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08497
&lt;/p&gt;
&lt;p&gt;
HyperBandit&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#27969;&#23186;&#20307;&#25512;&#33616;&#31995;&#32479;&#20013;&#26102;&#38388;&#21464;&#21270;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#23427;&#36890;&#36807;&#24314;&#31435;&#26102;&#38388;&#29305;&#24449;&#21644;&#29992;&#25143;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#21160;&#24577;&#35843;&#25972;&#25512;&#33616;&#27169;&#22411;&#20197;&#36866;&#24212;&#21160;&#24577;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#27969;&#23186;&#20307;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#20559;&#22909;&#32463;&#24120;&#22312;&#26102;&#38388;&#19978;&#21160;&#24577;&#21464;&#21270;&#65288;&#20363;&#22914;&#65292;&#22312;&#24037;&#20316;&#26085;&#21644;&#21608;&#26411;&#29992;&#25143;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#65289;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27969;&#23186;&#20307;&#25512;&#33616;&#27169;&#22411;&#21482;&#23558;&#26102;&#38388;&#35270;&#20026;&#26102;&#38388;&#25139;&#65292;&#27809;&#26377;&#26126;&#30830;&#22320;&#24314;&#27169;&#26102;&#38388;&#21464;&#37327;&#19982;&#26102;&#38388;&#21464;&#21270;&#30340;&#29992;&#25143;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#23548;&#33268;&#25512;&#33616;&#27169;&#22411;&#26080;&#27861;&#24555;&#36895;&#36866;&#24212;&#21160;&#24577;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;HyperBandit&#65292;&#20854;&#23558;&#26102;&#38388;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#21160;&#24577;&#35843;&#25972;&#25512;&#33616;&#27169;&#22411;&#20197;&#36866;&#24212;&#26102;&#38388;&#21464;&#21270;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;HyperBandit&#32500;&#25252;&#20102;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#29992;&#20110;&#20272;&#35745;&#26102;&#38388;&#21464;&#21270;&#22870;&#21169;&#30340;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32771;&#34385;&#20102;&#26102;&#38388;&#29305;&#24449;&#21644;&#29992;&#25143;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20351;&#29992;&#20272;&#35745;&#30340;&#26102;&#38388;&#21464;&#21270;&#22870;&#21169;&#65292;&#25105;&#20204;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#36827;&#34892;&#22312;&#32447;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world streaming recommender systems, user preferences often dynamically change over time (e.g., a user may have different preferences during weekdays and weekends). Existing bandit-based streaming recommendation models only consider time as a timestamp, without explicitly modeling the relationship between time variables and time-varying user preferences. This leads to recommendation models that cannot quickly adapt to dynamic scenarios. To address this issue, we propose a contextual bandit approach using hypernetwork, called HyperBandit, which takes time features as input and dynamically adjusts the recommendation model for time-varying user preferences. Specifically, HyperBandit maintains a neural network capable of generating the parameters for estimating time-varying rewards, taking into account the correlation between time features and user preferences. Using the estimated time-varying rewards, a bandit policy is employed to make online recommendations by learning the laten
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#30740;&#31350;&#20102;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#24847;&#22270;&#24314;&#27169;&#30340;&#30456;&#20851;&#27169;&#22411;&#21644;&#29305;&#24449;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#27169;&#22411;&#36873;&#25321;&#12289;&#36136;&#37327;&#38382;&#39064;&#21644;&#35780;&#20272;&#25351;&#26631;&#31561;&#26041;&#38754;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2308.08496</link><description>&lt;p&gt;
&#29702;&#35299;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#24847;&#22270;&#24314;&#27169;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Understanding User Intent Modeling for Conversational Recommender Systems: A Systematic Literature Review. (arXiv:2308.08496v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#30740;&#31350;&#20102;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#24847;&#22270;&#24314;&#27169;&#30340;&#30456;&#20851;&#27169;&#22411;&#21644;&#29305;&#24449;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#27169;&#22411;&#36873;&#25321;&#12289;&#36136;&#37327;&#38382;&#39064;&#21644;&#35780;&#20272;&#25351;&#26631;&#31561;&#26041;&#38754;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#29992;&#25143;&#24847;&#22270;&#24314;&#27169;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#36807;&#31243;&#65292;&#26088;&#22312;&#35782;&#21035;&#29992;&#25143;&#35831;&#27714;&#32972;&#21518;&#30340;&#28508;&#22312;&#30446;&#30340;&#65292;&#20174;&#32780;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#21709;&#24212;&#12290;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#22823;&#37327;&#30340;&#26041;&#27861;&#65288;&#36229;&#36807;13,000&#31687;&#35770;&#25991;&#65289;&#65292;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30456;&#20851;&#27010;&#24565;&#21644;&#24120;&#29992;&#27169;&#22411;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#25910;&#38598;&#20102;&#35774;&#35745;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#36890;&#24120;&#37319;&#29992;&#30340;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#20174;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20915;&#31574;&#27169;&#22411;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#31995;&#32479;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#20915;&#31574;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;59&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#24182;&#30830;&#23450;&#20102;74&#20010;&#24120;&#29992;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#28508;&#22312;&#30340;&#27169;&#22411;&#32452;&#21512;&#12289;&#27169;&#22411;&#36873;&#25321;&#36235;&#21183;&#12289;&#36136;&#37327;&#38382;&#39064;&#12289;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#32463;&#24120;&#20351;&#29992;&#30340;&#25968;&#25454;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: User intent modeling is a crucial process in Natural Language Processing that aims to identify the underlying purpose behind a user's request, enabling personalized responses. With a vast array of approaches introduced in the literature (over 13,000 papers in the last decade), understanding the related concepts and commonly used models in AI-based systems is essential. Method: We conducted a systematic literature review to gather data on models typically employed in designing conversational recommender systems. From the collected data, we developed a decision model to assist researchers in selecting the most suitable models for their systems. Additionally, we performed two case studies to evaluate the effectiveness of our proposed decision model. Results: Our study analyzed 59 distinct models and identified 74 commonly used features. We provided insights into potential model combinations, trends in model selection, quality concerns, evaluation measures, and frequently used dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#30340;&#20004;&#31181;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20934;&#30830;&#23545;&#40784;&#38899;&#39057;&#21644;&#35270;&#39057;&#27969;&#65292;&#24182;&#19988;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08488</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder. (arXiv:2308.08488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#30340;&#20004;&#31181;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20934;&#30830;&#23545;&#40784;&#38899;&#39057;&#21644;&#35270;&#39057;&#27969;&#65292;&#24182;&#19988;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#35266;&#23519;&#21040;&#65292;&#22312;&#20302;&#36136;&#37327;&#35270;&#39057;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#19979;&#65292;&#20174;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21040;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#30053;&#26377;&#25913;&#36827;&#12290;&#25454;&#35748;&#20026;&#65292;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#19987;&#38376;&#30340;&#36755;&#20837;&#34920;&#31034;&#23548;&#33268;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#65288;AVSR&#65289;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#35757;&#32451;&#26694;&#26550;&#19979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26222;&#36890;&#35805;&#20013;&#22068;&#21767;&#24418;&#29366;&#21644;&#38899;&#33410;&#32423;&#38899;&#32032;&#23383;&#21333;&#20803;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#24314;&#31435;&#20934;&#30830;&#30340;&#24103;&#32423;&#38899;&#33410;&#36793;&#30028;&#12290;&#36825;&#20351;&#24471;&#22312;&#35270;&#35273;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#36807;&#31243;&#20013;&#33021;&#22815;&#23545;&#40784;&#35270;&#39057;&#21644;&#38899;&#39057;&#27969;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;&#24341;&#23548;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#65288;CMFE&#65289;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#20027;&#35201;&#35757;&#32451;&#21442;&#25968;&#26469;&#23454;&#29616;&#22810;&#20010;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#23618;&#30340;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;&#22312;&#23454;&#39564;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24555;&#25463;&#21435;&#20559;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#20219;&#21153;&#23545;&#20559;&#35265;&#23646;&#24615;&#30340;&#23398;&#20064;&#20174;&#20559;&#35265;&#29305;&#24449;&#36716;&#31227;&#21040;&#24555;&#25463;&#29305;&#24449;&#19978;&#65292;&#24182;&#37319;&#29992;&#22240;&#26524;&#24178;&#39044;&#26469;&#28040;&#38500;&#20559;&#35265;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20381;&#36182;&#25935;&#24863;&#31038;&#20250;&#23646;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08482</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24555;&#25463;&#29305;&#24449;&#23454;&#29616;&#20844;&#24179;&#35270;&#35273;&#35782;&#21035;&#30340;&#33391;&#24615;&#25463;&#24452;&#28040;&#38500;&#20559;&#35265;&#65306;&#19968;&#31687;&#23454;&#20363;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features. (arXiv:2308.08482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24555;&#25463;&#21435;&#20559;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#20219;&#21153;&#23545;&#20559;&#35265;&#23646;&#24615;&#30340;&#23398;&#20064;&#20174;&#20559;&#35265;&#29305;&#24449;&#36716;&#31227;&#21040;&#24555;&#25463;&#29305;&#24449;&#19978;&#65292;&#24182;&#37319;&#29992;&#22240;&#26524;&#24178;&#39044;&#26469;&#28040;&#38500;&#20559;&#35265;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20381;&#36182;&#25935;&#24863;&#31038;&#20250;&#23646;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#23398;&#20250;&#20381;&#36182;&#20110;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#25935;&#24863;&#31038;&#20250;&#23646;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#36825;&#22312;&#31038;&#20250;&#24212;&#29992;&#20013;&#65292;&#22914;&#25307;&#32856;&#12289;&#38134;&#34892;&#21644;&#21009;&#20107;&#21496;&#27861;&#20013;&#65292;&#24102;&#26469;&#37325;&#22823;&#20844;&#24179;&#39118;&#38505;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#20013;&#19982;&#31038;&#20250;&#23646;&#24615;&#30456;&#20851;&#30340;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#20219;&#21153;&#21644;&#36825;&#20123;&#31038;&#20250;&#23646;&#24615;&#20043;&#38388;&#30340;&#39640;&#30456;&#20851;&#24615;&#20351;&#24471;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23398;&#20064;&#19982;&#21435;&#20559;&#19981;&#20860;&#23481;&#12290;&#37492;&#20110;&#27169;&#22411;&#20559;&#35265;&#26159;&#30001;&#20110;&#23398;&#20064;&#20559;&#35265;&#29305;&#24449;&#65288;&#20363;&#22914;&#24615;&#21035;&#65289;&#26469;&#20248;&#21270;&#30446;&#26631;&#20219;&#21153;&#32780;&#24341;&#36215;&#30340;&#65292;&#25105;&#20204;&#25506;&#35752;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#24555;&#25463;&#29305;&#24449;&#26469;&#26367;&#20195;&#20559;&#35265;&#29305;&#24449;&#22312;&#21435;&#20559;&#30340;&#30446;&#26631;&#20219;&#21153;&#20248;&#21270;&#20013;&#30340;&#20316;&#29992;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24555;&#25463;&#21435;&#20559;&#8221;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#30446;&#26631;&#20219;&#21153;&#23545;&#20559;&#35265;&#23646;&#24615;&#30340;&#23398;&#20064;&#20174;&#20559;&#35265;&#29305;&#24449;&#36716;&#31227;&#21040;&#24555;&#25463;&#29305;&#24449;&#19978;&#65292;&#28982;&#21518;&#37319;&#29992;&#22240;&#26524;&#24178;&#39044;&#26469;&#28040;&#38500;&#20559;&#35265;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Existing work tackles this issue by minimizing the employed information about social attributes in models for debiasing. However, the high correlation between target task and these social attributes makes learning on the target task incompatible with debiasing. Given that model bias arises due to the learning of bias features (\emph{i.e}., gender) that help target task optimization, we explore the following research question: \emph{Can we leverage shortcut features to replace the role of bias feature in target task optimization for debiasing?} To this end, we propose \emph{Shortcut Debiasing}, to first transfer the target task's learning of bias attributes from bias features to shortcut features, and then employ causal intervention to eliminate sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MOSR&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#37038;&#20214;&#37325;&#26032;&#25490;&#24207;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#27169;&#22411;&#26469;&#24179;&#34913;&#20146;&#23494;&#24230;&#65292;&#21450;&#26102;&#24615;&#21644;&#31616;&#27905;&#24615;&#31561;&#26631;&#20934;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;&#22312;&#24681;&#38534;&#37038;&#20214;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOSR&#22312;&#38750;&#31283;&#24577;&#20559;&#22909;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#39640;&#26041;&#24046;&#30340;&#37038;&#20214;&#29305;&#24449;&#30340;&#23567;&#22411;&#25277;&#26679;&#25968;&#25454;&#38598;&#19978;&#20063;&#33021;&#20445;&#25345;&#31283;&#23450;&#30340;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.08460</link><description>&lt;p&gt;
&#21160;&#24577;&#37038;&#20214;&#37325;&#26032;&#25490;&#24207;&#38382;&#39064;&#30340;&#22266;&#23450;&#31639;&#27861;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Stationary Algorithmic Balancing For Dynamic Email Re-Ranking Problem. (arXiv:2308.08460v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MOSR&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#37038;&#20214;&#37325;&#26032;&#25490;&#24207;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#27169;&#22411;&#26469;&#24179;&#34913;&#20146;&#23494;&#24230;&#65292;&#21450;&#26102;&#24615;&#21644;&#31616;&#27905;&#24615;&#31561;&#26631;&#20934;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;&#22312;&#24681;&#38534;&#37038;&#20214;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOSR&#22312;&#38750;&#31283;&#24577;&#20559;&#22909;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#39640;&#26041;&#24046;&#30340;&#37038;&#20214;&#29305;&#24449;&#30340;&#23567;&#22411;&#25277;&#26679;&#25968;&#25454;&#38598;&#19978;&#20063;&#33021;&#20445;&#25345;&#31283;&#23450;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#37038;&#20214;&#24179;&#21488;&#38656;&#35201;&#29983;&#25104;&#28385;&#36275;&#29992;&#25143;&#20559;&#22909;&#24182;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20010;&#24615;&#21270;&#37038;&#20214;&#25490;&#24207;&#12290;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#22522;&#20110;&#19977;&#20010;&#26631;&#20934;&#30340;&#25512;&#33616;&#38382;&#39064;&#26469;&#22788;&#29702;&#65306;&#20146;&#23494;&#24230;&#65288;&#21457;&#20214;&#20154;&#21644;&#20027;&#39064;&#19982;&#29992;&#25143;&#30340;&#30456;&#20851;&#24230;&#65289;&#65292;&#21450;&#26102;&#24615;&#65288;&#37038;&#20214;&#30340;&#26368;&#36817;&#31243;&#24230;&#65289;&#21644;&#31616;&#27905;&#24615;&#65288;&#37038;&#20214;&#30340;&#31616;&#30701;&#31243;&#24230;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MOSR&#65288;&#22810;&#30446;&#26631;&#31283;&#24577;&#25512;&#33616;&#22120;&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#27169;&#22411;&#26469;&#21160;&#24577;&#24179;&#34913;&#36825;&#20123;&#26631;&#20934;&#24182;&#36866;&#24212;&#20559;&#22909;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#24681;&#38534;&#37038;&#20214;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;MOSR&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#23454;&#38469;&#37038;&#20214;&#30340;&#38598;&#21512;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;MOSR&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#31283;&#24577;&#20559;&#22909;&#19979;&#65292;&#29992;&#25143;&#38543;&#26102;&#38388;&#19981;&#21516;&#31243;&#24230;&#22320;&#35780;&#20272;&#19981;&#21516;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#23567;&#22411;&#25277;&#26679;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;MOSR&#30340;&#40065;&#26834;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#30340;&#37038;&#20214;&#29305;&#24449;&#21464;&#21270;&#24456;&#22823;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#19981;&#21516;&#26679;&#26412;&#20013;&#20445;&#25345;&#31283;&#23450;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Email platforms need to generate personalized rankings of emails that satisfy user preferences, which may vary over time. We approach this as a recommendation problem based on three criteria: closeness (how relevant the sender and topic are to the user), timeliness (how recent the email is), and conciseness (how brief the email is). We propose MOSR (Multi-Objective Stationary Recommender), a novel online algorithm that uses an adaptive control model to dynamically balance these criteria and adapt to preference changes. We evaluate MOSR on the Enron Email Dataset, a large collection of real emails, and compare it with other baselines. The results show that MOSR achieves better performance, especially under non-stationary preferences, where users value different criteria more or less over time. We also test MOSR's robustness on a smaller down-sampled dataset that exhibits high variance in email characteristics, and show that it maintains stable rankings across different samples. Our work
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#25552;&#31034;&#35843;&#20248;&#30340;&#39034;&#24207;&#25512;&#33616;(KP4SR)&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#24211;&#21644;&#26500;&#24314;&#30693;&#35782;&#25552;&#31034;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#35821;&#20041;&#24046;&#36317;&#21644;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08459</link><description>&lt;p&gt;
&#30693;&#35782;&#25552;&#31034;&#35843;&#20248;&#30340;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Knowledge Prompt-tuning for Sequential Recommendation. (arXiv:2308.08459v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#25552;&#31034;&#35843;&#20248;&#30340;&#39034;&#24207;&#25512;&#33616;(KP4SR)&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#24211;&#21644;&#26500;&#24314;&#30693;&#35782;&#25552;&#31034;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#35821;&#20041;&#24046;&#36317;&#21644;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#22312;&#39034;&#24207;&#25512;&#33616;(SR)&#20013;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#29992;&#20110;&#25552;&#21462;&#36890;&#29992;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#19988;&#24456;&#38590;&#25429;&#25417;&#29992;&#25143;&#30340;&#32454;&#31890;&#24230;&#20559;&#22909;&#12290;&#21516;&#26102;&#65292;&#35768;&#22810;&#20256;&#32479;&#30340;SR&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#36741;&#21161;&#20449;&#24687;&#26469;&#25913;&#21892;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#21364;&#36973;&#21463;&#20449;&#24687;&#25439;&#22833;&#30340;&#22256;&#25200;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#35748;&#20026;&#19968;&#20010;&#22909;&#30340;&#25512;&#33616;&#31995;&#32479;&#24212;&#35813;&#21516;&#26102;&#21033;&#29992;&#36890;&#29992;&#30693;&#35782;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22806;&#37096;&#30693;&#35782;&#24211;&#65292;&#24182;&#25552;&#20986;&#20102;&#30693;&#35782;&#25552;&#31034;&#35843;&#20248;&#30340;&#39034;&#24207;&#25512;&#33616;(KP4SR)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#32452;&#20851;&#31995;&#27169;&#26495;&#65292;&#24182;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;(KG)&#36716;&#21270;&#20026;&#30693;&#35782;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#35821;&#20041;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#25552;&#31034;&#30772;&#22351;&#20102;&#21407;&#22987;&#25968;&#25454;&#32467;&#26500;&#24182;&#24341;&#20837;&#20102;&#22823;&#37327;&#30340;&#22122;&#38899;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#26641;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#26641;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#22122;&#38899;&#24182;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have demonstrated strong performance in sequential recommendation (SR), which are utilized to extract general knowledge. However, existing methods still lack domain knowledge and struggle to capture users' fine-grained preferences. Meanwhile, many traditional SR methods improve this issue by integrating side information while suffering from information loss. To summarize, we believe that a good recommendation system should utilize both general and domain knowledge simultaneously. Therefore, we introduce an external knowledge base and propose Knowledge Prompt-tuning for Sequential Recommendation (\textbf{KP4SR}). Specifically, we construct a set of relationship templates and transform a structured knowledge graph (KG) into knowledge prompts to solve the problem of the semantic gap. However, knowledge prompts disrupt the original data structure and introduce a significant amount of noise. We further construct a knowledge tree and propose a knowledge tre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21152;&#26435;&#26377;&#21521;&#22270;&#30340;&#26368;&#20005;&#26684;&#21487;&#25509;&#21463;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65292;&#21033;&#29992;&#36793;&#26435;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#35745;&#31639;&#25104;&#26412;&#20132;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#35299;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.08453</link><description>&lt;p&gt;
&#26368;&#20005;&#26684;&#21487;&#25509;&#21463;&#30340;&#26368;&#30701;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Tightest Admissible Shortest Path. (arXiv:2308.08453v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21152;&#26435;&#26377;&#21521;&#22270;&#30340;&#26368;&#20005;&#26684;&#21487;&#25509;&#21463;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65292;&#21033;&#29992;&#36793;&#26435;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#35745;&#31639;&#25104;&#26412;&#20132;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#35299;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#22522;&#30784;&#24615;&#30340;&#12290;&#20960;&#20046;&#25152;&#26377;&#38382;&#39064;&#30340;&#21464;&#31181;&#21644;&#30456;&#20851;&#31639;&#27861;&#37117;&#24573;&#30053;&#20102;&#36793;&#26435;&#35745;&#31639;&#26102;&#38388;&#21450;&#20854;&#19982;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#30340;&#24120;&#35265;&#20851;&#31995;&#12290;&#36825;&#24847;&#21619;&#30528;&#32771;&#34385;&#36825;&#20123;&#22240;&#32032;&#21487;&#33021;&#20250;&#22312;&#30456;&#20851;&#24212;&#29992;&#20013;&#24102;&#26469;&#24615;&#33021;&#25552;&#21319;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21152;&#26435;&#26377;&#21521;&#22270;&#30340;&#25512;&#24191;&#26694;&#26550;&#65292;&#21487;&#20197;&#22810;&#27425;&#35745;&#31639;&#65288;&#20272;&#35745;&#65289;&#36793;&#26435;&#65292;&#38543;&#30528;&#31934;&#24230;&#30340;&#22686;&#21152;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#22686;&#21152;&#12290;&#25105;&#20204;&#22312;&#27492;&#26694;&#26550;&#19978;&#24341;&#20837;&#20102;&#23547;&#25214;&#26368;&#20005;&#26684;&#21487;&#25509;&#21463;&#30340;&#26368;&#30701;&#36335;&#24452;&#65288;TASP&#65289;&#30340;&#38382;&#39064;&#65307;&#36825;&#26159;&#23558;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#25512;&#24191;&#21040;&#26377;&#30028;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#25104;&#26412;&#26469;&#20132;&#25442;&#36793;&#26435;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;TASP&#65292;&#24182;&#20445;&#35777;&#20102;&#35299;&#30340;&#36136;&#37327;&#12290;&#23454;&#35777;&#35780;&#20272;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The shortest path problem in graphs is fundamental to AI. Nearly all variants of the problem and relevant algorithms that solve them ignore edge-weight computation time and its common relation to weight uncertainty. This implies that taking these factors into consideration can potentially lead to a performance boost in relevant applications. Recently, a generalized framework for weighted directed graphs was suggested, where edge-weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense. We build on this framework to introduce the problem of finding the tightest admissible shortest path (TASP); a path with the tightest suboptimality bound on the optimal cost. This is a generalization of the shortest path problem to bounded uncertainty, where edge-weight uncertainty can be traded for computational cost. We present a complete algorithm for solving TASP, with guarantees on solution quality. Empirical evaluation supports the effectiveness of this approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20013;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#35752;&#35770;&#20102;AIGC&#30340;&#22522;&#30784;&#25216;&#26415;&#12289;&#24066;&#22330;&#29366;&#20917;&#21644;&#21457;&#23637;&#36712;&#36857;&#65292;&#24182;&#37325;&#28857;&#24378;&#35843;&#20102;AIGC&#30340;&#29983;&#24577;&#24314;&#35774;&#12290;&#39044;&#27979;&#20102;&#34892;&#19994;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.08451</link><description>&lt;p&gt;
&#20013;&#22269;AIGC&#30340;&#29616;&#29366;&#21450;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
AIGC In China: Current Developments And Future Outlook. (arXiv:2308.08451v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20013;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#35752;&#35770;&#20102;AIGC&#30340;&#22522;&#30784;&#25216;&#26415;&#12289;&#24066;&#22330;&#29366;&#20917;&#21644;&#21457;&#23637;&#36712;&#36857;&#65292;&#24182;&#37325;&#28857;&#24378;&#35843;&#20102;AIGC&#30340;&#29983;&#24577;&#24314;&#35774;&#12290;&#39044;&#27979;&#20102;&#34892;&#19994;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#24050;&#32463;&#22312;&#26085;&#24120;&#29983;&#27963;&#12289;&#24037;&#19994;&#21046;&#36896;&#21644;&#23398;&#26415;&#30028;&#31561;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#37492;&#20110;AIGC&#21457;&#23637;&#30340;&#20840;&#29699;&#36235;&#21183;&#21644;&#31454;&#20105;&#21147;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#20013;&#22269;&#22312;&#35813;&#39046;&#22495;&#30340;&#29616;&#29366;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#27010;&#36848;&#20102;AIGC&#30340;&#22522;&#30784;&#25216;&#26415;&#21644;&#24403;&#21069;&#24212;&#29992;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#20851;&#38190;&#35789;&#25628;&#32034;&#35782;&#21035;&#30456;&#20851;&#23398;&#26415;&#35770;&#25991;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#20013;&#22269;&#30340;AIGC&#24066;&#22330;&#29366;&#20917;&#12289;&#25919;&#31574;&#29615;&#22659;&#21644;&#21457;&#23637;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20840;&#38754;&#23457;&#35270;&#20102;AIGC&#20135;&#21697;&#21450;&#20854;&#30456;&#24212;&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;AIGC&#30340;&#29983;&#24577;&#24314;&#35774;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;AIGC&#34892;&#19994;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#65292;&#24182;&#26681;&#25454;AIGC&#30340;&#31454;&#20105;&#24615;&#27934;&#23519;&#23637;&#26395;&#20102;&#34892;&#19994;&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing attention given to AI Generated Content (AIGC) has brought a profound impact on various aspects of daily life, industrial manufacturing, and the academic sector. Recognizing the global trends and competitiveness in AIGC development, this study aims to analyze China's current status in the field. The investigation begins with an overview of the foundational technologies and current applications of AIGC. Subsequently, the study delves into the market status, policy landscape, and development trajectory of AIGC in China, utilizing keyword searches to identify relevant scholarly papers. Furthermore, the paper provides a comprehensive examination of AIGC products and their corresponding ecosystem, emphasizing the ecological construction of AIGC. Finally, this paper discusses the challenges and risks faced by the AIGC industry while presenting a forward-looking perspective on the industry's future based on competitive insights in AIGC.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#24212;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#65292;&#36890;&#36807;&#27604;&#36739;qGAN&#21644;QCBM&#31561;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08448</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#29616;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;qGAN&#65289;&#21644;QCBM
&lt;/p&gt;
&lt;p&gt;
Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance. (arXiv:2308.08448v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#24212;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#65292;&#36890;&#36807;&#27604;&#36739;qGAN&#21644;QCBM&#31561;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#26159;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#39046;&#22495;&#65292;&#30001;&#20004;&#20010;&#26368;&#20855;&#21019;&#26032;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#32452;&#25104;&#65306;&#37327;&#23376;&#35745;&#31639;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;ML&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#34987;&#35748;&#20026;&#26159;&#23558;&#21463;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#20852;&#36215;&#24433;&#21709;&#30340;&#31532;&#19968;&#20010;&#39046;&#22495;&#12290;&#36825;&#39033;&#24037;&#20316;&#35752;&#35770;&#20102;&#22312;&#37329;&#34701;&#20013;&#24212;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#30340;&#19968;&#20123;&#26032;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#24050;&#22312;&#37329;&#34701;&#30028;&#24341;&#36215;&#20851;&#27880;&#30340;QML&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#30495;&#23454;&#37329;&#34701;&#25968;&#25454;&#38598;&#23545;qGAN&#65288;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65289;&#21644;QCBM&#65288;&#37327;&#23376;&#30005;&#36335;Born&#26426;&#65289;&#31561;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;qGAN&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#26410;&#26469;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#36890;&#36807;QML&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning (QML) is a cross-disciplinary subject made up of two of the most exciting research areas: quantum computing and classical machine learning (ML), with ML and artificial intelligence (AI) being projected as the first fields that will be impacted by the rise of quantum machines. Quantum computers are being used today in drug discovery, material &amp; molecular modelling and finance. In this work, we discuss some upcoming active new research areas in application of quantum machine learning (QML) in finance. We discuss certain QML models that has become areas of active interest in the financial world for various applications. We use real world financial dataset and compare models such as qGAN (quantum generative adversarial networks) and QCBM (quantum circuit Born machine) among others, using simulated environments. For the qGAN, we define quantum circuits for discriminators and generators and show promises of future quantum advantage via QML in finance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#26041;&#24335;&#12290;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#30830;&#20445;&#20154;&#20204;&#23545;AI&#31995;&#32479;&#30340;&#20449;&#20219;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#38500;&#20102;&#35299;&#37322;&#24615;&#20043;&#22806;&#65292;&#36824;&#28041;&#21450;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#31561;&#26041;&#38754;&#12290;&#35813;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#36817;&#26399;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.08407</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;:&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#26041;&#24335;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities. (arXiv:2308.08407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08407
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#26041;&#24335;&#12290;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#30830;&#20445;&#20154;&#20204;&#23545;AI&#31995;&#32479;&#30340;&#20449;&#20219;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#38500;&#20102;&#35299;&#37322;&#24615;&#20043;&#22806;&#65292;&#36824;&#28041;&#21450;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#31561;&#26041;&#38754;&#12290;&#35813;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#36817;&#26399;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#25104;&#26524;&#65292;&#22312;&#35786;&#26029;&#21644;&#30142;&#30149;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23545;&#20854;&#19981;&#36879;&#26126;&#24615;&#12289;&#28508;&#22312;&#20559;&#35265;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#30340;&#25285;&#24551;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#20026;&#20102;&#30830;&#20445;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20449;&#20219;&#21644;&#21487;&#38752;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#21487;&#35299;&#37322;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#21487;&#35299;&#37322;&#24615;&#36890;&#24120;&#25351;&#30340;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21521;&#20154;&#31867;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#20854;&#20915;&#31574;&#36923;&#36753;&#25110;&#20915;&#31574;&#26412;&#36523;&#30340;&#31283;&#20581;&#35299;&#37322;&#33021;&#21147;&#12290;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#65292;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#31561;&#20854;&#20182;&#26041;&#38754;&#30340;&#21487;&#35299;&#37322;&#24615;&#20063;&#20195;&#34920;&#20102;&#36229;&#36234;&#21487;&#35299;&#37322;&#24615;&#26412;&#36523;&#30340;&#37325;&#35201;&#27010;&#24565;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#19968;&#36215;&#25110;&#21487;&#20114;&#25442;&#22320;&#20351;&#29992;&#12290;&#26412;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#21457;&#23637;&#36827;&#23637;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highligh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#25104;&#21253;&#21547;&#36807;&#31243;&#25968;&#25454;&#21644;&#23545;&#24212;&#31243;&#24207;&#21270;&#30693;&#35782;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#23884;&#20837;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08371</link><description>&lt;p&gt;
PDPK: &#29992;&#20110;&#21046;&#36896;&#19994;&#30340;&#21512;&#25104;&#36807;&#31243;&#25968;&#25454;&#21644;&#23545;&#24212;&#31243;&#24207;&#21270;&#30693;&#35782;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing. (arXiv:2308.08371v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08371
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#25104;&#21253;&#21547;&#36807;&#31243;&#25968;&#25454;&#21644;&#23545;&#24212;&#31243;&#24207;&#21270;&#30693;&#35782;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#23884;&#20837;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21270;&#30693;&#35782;&#25551;&#36848;&#20102;&#22914;&#20309;&#23436;&#25104;&#20219;&#21153;&#21644;&#35299;&#20915;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#30693;&#35782;&#36890;&#24120;&#30001;&#39046;&#22495;&#19987;&#23478;&#25345;&#26377;&#65292;&#20363;&#22914;&#21046;&#36896;&#19994;&#20013;&#35843;&#25972;&#21442;&#25968;&#20197;&#36798;&#21040;&#36136;&#37327;&#30446;&#26631;&#30340;&#25805;&#20316;&#21592;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#21253;&#21547;&#36807;&#31243;&#25968;&#25454;&#21644;&#23545;&#24212;&#31243;&#24207;&#21270;&#30693;&#35782;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30446;&#21069;&#36824;&#19981;&#23384;&#22312;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#20225;&#19994;&#23545;&#30693;&#35782;&#36827;&#23637;&#30340;&#25439;&#22833;&#23384;&#22312;&#25285;&#24515;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#30340;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#30340;&#35774;&#35745;&#36873;&#25321;&#21463;&#21040;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#30340;&#20004;&#20010;&#23454;&#38469;&#30340;&#31243;&#24207;&#21270;&#30693;&#35782;&#25968;&#25454;&#38598;&#30340;&#21551;&#21457;&#12290;&#38500;&#20102;&#21253;&#21547;&#31526;&#21512;&#36164;&#28304;&#25551;&#36848;&#26694;&#26550;&#65288;RDF&#65289;&#26631;&#20934;&#30340;&#30693;&#35782;&#22270;&#24418;&#20013;&#30340;&#31243;&#24207;&#21270;&#30693;&#35782;&#30340;&#34920;&#31034;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#27169;&#25311;&#21442;&#25968;&#21270;&#36807;&#31243;&#24182;&#25552;&#20379;&#19968;&#33268;&#30340;&#36807;&#31243;&#25968;&#25454;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22312;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#24418;&#19978;&#30340;&#24050;&#24314;&#31435;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#21738;&#20123;&#24320;&#31665;&#21363;&#29992;&#30340;&#26041;&#27861;&#20855;&#26377;&#37325;&#26032;&#35299;&#37322;&#30693;&#35782;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural knowledge describes how to accomplish tasks and mitigate problems. Such knowledge is commonly held by domain experts, e.g. operators in manufacturing who adjust parameters to achieve quality targets. To the best of our knowledge, no real-world datasets containing process data and corresponding procedural knowledge are publicly available, possibly due to corporate apprehensions regarding the loss of knowledge advances. Therefore, we provide a framework to generate synthetic datasets that can be adapted to different domains. The design choices are inspired by two real-world datasets of procedural knowledge we have access to. Apart from containing representations of procedural knowledge in Resource Description Framework (RDF)-compliant knowledge graphs, the framework simulates parametrisation processes and provides consistent process data. We compare established embedding methods on the resulting knowledge graphs, detailing which out-of-the-box methods have the potential to rep
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20957;&#32858;Transformer&#65288;AGER&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#20013;&#20197;&#21333;&#38454;&#27573;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#28789;&#27963;&#21033;&#29992;&#39069;&#22806;&#30340;&#23454;&#20363;&#32423;&#25552;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#32858;&#31867;&#34917;&#19969;&#26631;&#35760;&#24182;&#23558;&#20854;&#19982;&#25991;&#26412;&#23545;&#40784;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23454;&#20363;&#32423;&#25552;&#31034;&#30340;&#25552;&#21462;&#25928;&#26524;&#65292;&#24182;&#22312;HICO-Det&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;36.75 mAP&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08370</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#30340;&#20957;&#32858;Transformer
&lt;/p&gt;
&lt;p&gt;
Agglomerative Transformer for Human-Object Interaction Detection. (arXiv:2308.08370v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08370
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20957;&#32858;Transformer&#65288;AGER&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#20013;&#20197;&#21333;&#38454;&#27573;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#28789;&#27963;&#21033;&#29992;&#39069;&#22806;&#30340;&#23454;&#20363;&#32423;&#25552;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#32858;&#31867;&#34917;&#19969;&#26631;&#35760;&#24182;&#23558;&#20854;&#19982;&#25991;&#26412;&#23545;&#40784;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23454;&#20363;&#32423;&#25552;&#31034;&#30340;&#25552;&#21462;&#25928;&#26524;&#65292;&#24182;&#22312;HICO-Det&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;36.75 mAP&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20957;&#32858;Transformer&#65288;AGER&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#22522;&#20110;Transformer&#30340;&#20154;-&#29289;&#20132;&#20114;&#65288;HOI&#65289;&#26816;&#27979;&#22120;&#33021;&#22815;&#39318;&#27425;&#20197;&#21333;&#38454;&#27573;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#28789;&#27963;&#21033;&#29992;&#39069;&#22806;&#30340;&#23454;&#20363;&#32423;&#25552;&#31034;&#12290;AGER&#36890;&#36807;&#21160;&#24577;&#32858;&#31867;&#34917;&#19969;&#26631;&#35760;&#26469;&#33719;&#24471;&#23454;&#20363;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#25351;&#23548;&#23558;&#32858;&#31867;&#20013;&#24515;&#19982;&#23454;&#20363;&#23545;&#40784;&#65292;&#20174;&#32780;&#33719;&#24471;&#20004;&#20010;&#20248;&#21183;&#65306;1&#65289;&#23436;&#25972;&#24615;&#65306;&#27599;&#20010;&#23454;&#20363;&#26631;&#35760;&#37117;&#40723;&#21169;&#21253;&#21547;&#23454;&#20363;&#30340;&#25152;&#26377;&#26377;&#21306;&#21035;&#29305;&#24449;&#21306;&#22495;&#65292;&#36825;&#23545;&#20110;&#25552;&#21462;&#19981;&#21516;&#30340;&#23454;&#20363;&#32423;&#25552;&#31034;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#38543;&#21518;&#23548;&#33268;HOI&#26816;&#27979;&#22312;HICO-Det&#19978;&#36798;&#21040;36.75&#20010;mAP&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;2&#65289;&#25928;&#29575;&#65306;&#21160;&#24577;&#32858;&#31867;&#26426;&#21046;&#20351;&#24471;AGER&#33021;&#22815;&#19982;Transformer&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#23398;&#20064;&#19968;&#36215;&#29983;&#25104;&#23454;&#20363;&#26631;&#35760;&#65292;&#28040;&#38500;&#20102;&#20808;&#21069;&#26041;&#27861;&#20013;&#38656;&#35201;&#39069;&#22806;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#25110;&#23454;&#20363;&#35299;&#30721;&#22120;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#20801;&#35768;&#25552;&#21462;&#21487;&#21462;&#30340;&#39069;&#22806;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an agglomerative Transformer (AGER) that enables Transformer-based human-object interaction (HOI) detectors to flexibly exploit extra instance-level cues in a single-stage and end-to-end manner for the first time. AGER acquires instance tokens by dynamically clustering patch tokens and aligning cluster centers to instances with textual guidance, thus enjoying two benefits: 1) Integrality: each instance token is encouraged to contain all discriminative feature regions of an instance, which demonstrates a significant improvement in the extraction of different instance-level cues and subsequently leads to a new state-of-the-art performance of HOI detection with 36.75 mAP on HICO-Det. 2) Efficiency: the dynamical clustering mechanism allows AGER to generate instance tokens jointly with the feature learning of the Transformer encoder, eliminating the need of an additional object detector or instance decoder in prior methods, thus allowing the extraction of desirable extra cues fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#20803;&#23398;&#20064;&#25216;&#26415;&#22312;&#22788;&#29702;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#24050;&#25104;&#20026;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#24182;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#36825;&#20123;&#31995;&#32479;&#25317;&#26377;&#24222;&#22823;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#25968;&#37327;&#65292;&#19988;&#26377;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.08354</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#21542;&#26159;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#27491;&#30830;&#26041;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?. (arXiv:2308.08354v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#20803;&#23398;&#20064;&#25216;&#26415;&#22312;&#22788;&#29702;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#24050;&#25104;&#20026;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#24182;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#36825;&#20123;&#31995;&#32479;&#25317;&#26377;&#24222;&#22823;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#25968;&#37327;&#65292;&#19988;&#26377;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#29616;&#20195;&#22312;&#32447;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#22522;&#30784;&#26500;&#24314;&#27169;&#22359;&#65292;&#24182;&#23545;&#29992;&#25143;&#20307;&#39564;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#29616;&#20195;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#20919;&#21551;&#21160;&#35774;&#32622;&#19979;&#30340;&#25512;&#33616;&#38382;&#39064;&#65292;&#20363;&#22914;&#24403;&#29992;&#25143;&#22312;&#31995;&#32479;&#20013;&#30340;&#20114;&#21160;&#26377;&#38480;&#26102;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#36828;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#65292;&#26368;&#36817;&#24050;&#25104;&#20026;&#23398;&#26415;&#30740;&#31350;&#25991;&#29486;&#20013;&#22788;&#29702;&#25512;&#33616;&#31995;&#32479;&#20013;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#25317;&#26377;&#25968;&#21313;&#20159;&#29992;&#25143;&#21644;&#29289;&#21697;&#20197;&#21450;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#30340;&#29616;&#23454;&#25512;&#33616;&#31995;&#32479;&#26469;&#35828;&#24182;&#19981;&#23454;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#29992;&#22522;&#20934;&#19978;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#39640;&#24615;&#33021;&#26159;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have become fundamental building blocks of modern online products and services, and have a substantial impact on user experience. In the past few years, deep learning methods have attracted a lot of research, and are now heavily used in modern real-world recommender systems. Nevertheless, dealing with recommendations in the cold-start setting, e.g., when a user has done limited interactions in the system, is a problem that remains far from solved. Meta-learning techniques, and in particular optimization-based meta-learning, have recently become the most popular approaches in the academic research literature for tackling the cold-start problem in deep learning models for recommender systems. However, current meta-learning approaches are not practical for real-world recommender systems, which have billions of users and items, and strict latency requirements. In this paper we show that it is possible to obtaining similar, or higher, performance on commonly used benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;OOD-GMixup&#8221;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#22270;&#30340;&#24102;&#22806;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#22270;&#21512;&#29702;&#24615;&#21644;&#29983;&#25104;&#34394;&#25311;&#26679;&#26412;&#30340;&#26041;&#24335;&#26469;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08344</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#22270;&#30340;&#24102;&#22806;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph Out-of-Distribution Generalization with Controllable Data Augmentation. (arXiv:2308.08344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;OOD-GMixup&#8221;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#22270;&#30340;&#24102;&#22806;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#22270;&#21512;&#29702;&#24615;&#21644;&#29983;&#25104;&#34394;&#25311;&#26679;&#26412;&#30340;&#26041;&#24335;&#26469;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#23646;&#24615;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#36873;&#25321;&#20559;&#24046;&#65288;&#20363;&#22914;&#65292;&#22312;&#23567;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#22823;&#22270;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25110;&#22312;&#31264;&#23494;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#31232;&#30095;&#22270;&#19978;&#36827;&#34892;&#27979;&#35797;&#65289;&#65292;&#20998;&#24067;&#20559;&#24046;&#24456;&#26222;&#36941;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#32463;&#24120;&#35266;&#23519;&#21040;&#23613;&#31649;&#26377;&#21333;&#36793;&#20559;&#21521;&#30340;&#25968;&#25454;&#20998;&#21306;&#65292;&#20294;&#21364;&#23384;&#22312;&#30528;&#21516;&#26102;&#20855;&#26377;&#35268;&#27169;&#21644;&#23494;&#24230;&#30340;&#28151;&#21512;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#12290;&#28151;&#21512;&#20998;&#24067;&#20559;&#31227;&#20013;&#30340;&#20266;&#30456;&#20851;&#24615;&#38477;&#20302;&#20102;&#20808;&#21069;GNN&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;OOD-GMixup&#8221;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#20197;&#32852;&#21512;&#25805;&#20316;&#35757;&#32451;&#20998;&#24067;&#30340;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#21462;&#22270;&#21512;&#29702;&#24615;&#26469;&#28040;&#38500;&#30001;&#20110;&#19981;&#30456;&#20851;&#20449;&#24687;&#32780;&#24341;&#36215;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#22270;&#21512;&#29702;&#24615;&#36827;&#34892;&#25200;&#21160;&#29983;&#25104;&#34394;&#25311;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe \emph{hybrid structure distribution shift} of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose \texttt{OOD-GMixup} to jointly manipulate the training distribution with \emph{controllable data augmentation} in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Secondly, we generate virtual samples with perturbation on graph rationale r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.08334</link><description>&lt;p&gt;
&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning Logic Programs by Discovering Higher-Order Abstractions. (arXiv:2308.08334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#26032;&#39062;&#30340;&#25277;&#35937;&#23545;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#65288;&#20363;&#22914;map&#12289;filter&#21644;fold&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65292;&#21363;&#20174;&#31034;&#20363;&#21644;&#32972;&#26223;&#30693;&#35782;&#20013;&#24402;&#32435;&#36923;&#36753;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#38454;&#37325;&#26500;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#25277;&#35937;&#26469;&#21387;&#32553;&#36923;&#36753;&#31243;&#24207;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#22312;STEVIE&#20013;&#65292;&#23427;&#23558;&#39640;&#38454;&#37325;&#26500;&#38382;&#39064;&#24314;&#27169;&#20026;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#31243;&#24207;&#21512;&#25104;&#21644;&#35270;&#35273;&#25512;&#29702;&#65292;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#27809;&#26377;&#37325;&#26500;&#30456;&#27604;&#65292;STEVIE&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;27%&#24182;&#23558;&#23398;&#20064;&#26102;&#38388;&#20943;&#23569;47%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;STEVIE&#21487;&#20197;&#21457;&#29616;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering novel abstractions is important for human-level AI. We introduce an approach to discover higher-order abstractions, such as map, filter, and fold. We focus on inductive logic programming, which induces logic programs from examples and background knowledge. We introduce the higher-order refactoring problem, where the goal is to compress a logic program by introducing higher-order abstractions. We implement our approach in STEVIE, which formulates the higher-order refactoring problem as a constraint optimisation problem. Our experimental results on multiple domains, including program synthesis and visual reasoning, show that, compared to no refactoring, STEVIE can improve predictive accuracies by 27% and reduce learning times by 47%. We also show that STEVIE can discover abstractions that transfer to different domains
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25968;&#23398;&#20248;&#21270;&#20013;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#21487;&#35299;&#37322;&#24615;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#36807;&#21435;&#31867;&#20284;&#24773;&#20917;&#19979;&#30340;&#35299;&#36827;&#34892;&#27604;&#36739;&#26469;&#25214;&#21040;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#21487;&#35299;&#37322;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;NP-hard&#30340;&#65292;&#20294;&#22312;&#19968;&#20123;&#22810;&#39033;&#24335;&#21487;&#35299;&#30340;&#24773;&#20917;&#19979;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2308.08309</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#21487;&#35299;&#37322;&#24615;&#22312;&#25968;&#23398;&#20248;&#21270;&#20013;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Data-Driven Explainability in Mathematical Optimization. (arXiv:2308.08309v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08309
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25968;&#23398;&#20248;&#21270;&#20013;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#21487;&#35299;&#37322;&#24615;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#36807;&#21435;&#31867;&#20284;&#24773;&#20917;&#19979;&#30340;&#35299;&#36827;&#34892;&#27604;&#36739;&#26469;&#25214;&#21040;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#21487;&#35299;&#37322;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;NP-hard&#30340;&#65292;&#20294;&#22312;&#19968;&#20123;&#22810;&#39033;&#24335;&#21487;&#35299;&#30340;&#24773;&#20917;&#19979;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#23398;&#35268;&#21010;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#29616;&#22312;&#26377;&#33021;&#21147;&#26377;&#25928;&#22320;&#35299;&#20915;&#20960;&#21313;&#24180;&#21069;&#34987;&#35748;&#20026;&#26080;&#27861;&#35299;&#20915;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20248;&#21270;&#36719;&#20214;&#34987;&#35270;&#20026;&#40657;&#30418;&#23376;&#65292;&#19968;&#20123;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#35299;&#21487;&#33021;&#19981;&#34987;&#25509;&#21463;&#12290;&#34429;&#28982;&#31185;&#23398;&#23478;&#20204;&#23545;&#27492;&#24456;&#20102;&#35299;&#65292;&#20294;&#23545;&#20110;&#23454;&#36341;&#32773;&#26469;&#35828;&#21364;&#24456;&#38590;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#23558;&#35299;&#37322;&#24615;&#20316;&#20026;&#21478;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#24341;&#20837;&#35299;&#20915;&#26041;&#26696;&#65292;&#26082;&#21253;&#25324;&#30446;&#26631;&#20540;&#65292;&#20063;&#21253;&#25324;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#36825;&#20004;&#20010;&#26631;&#20934;&#20043;&#38388;&#25214;&#21040;&#26435;&#34913;&#35299;&#12290;&#36890;&#36807;&#23558;&#35299;&#20915;&#26041;&#26696;&#19982;&#36807;&#21435;&#31867;&#20284;&#24773;&#20917;&#19979;&#23454;&#26045;&#30340;&#65288;&#19981;&#19968;&#23450;&#26159;&#26368;&#20248;&#30340;&#65289;&#35299;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#36798;&#21040;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22240;&#27492;&#65292;&#26356;&#21916;&#27426;&#23637;&#29616;&#30456;&#20284;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#31616;&#21333;&#24773;&#20917;&#19979;&#21487;&#35299;&#37322;&#27169;&#22411;&#20063;&#26159;NP-hard&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#30456;&#20851;&#30340;&#22810;&#39033;&#24335;&#21487;&#35299;&#24773;&#20917;&#65292;&#22914;&#21487;&#35299;&#37322;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#12290;&#22312;ar&#30340;&#25968;&#20540;&#23454;&#39564;&#19978;&#20063;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in mathematical programming have made it possible to efficiently tackle large-scale real-world problems that were deemed intractable just a few decades ago. However, provably optimal solutions may not be accepted due to the perception of optimization software as a black box. Although well understood by scientists, this lacks easy accessibility for practitioners. Hence, we advocate for introducing the explainability of a solution as another evaluation criterion, next to its objective value, which enables us to find trade-off solutions between these two criteria. Explainability is attained by comparing against (not necessarily optimal) solutions that were implemented in similar situations in the past. Thus, solutions are preferred that exhibit similar features. Although we prove that already in simple cases the explainable model is NP-hard, we characterize relevant polynomially solvable cases such as the explainable shortest-path problem. Our numerical experiments on both ar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#27169;&#31946;&#29615;&#22659;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#35748;&#30693;&#22320;&#22270;&#23398;&#20064;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#22312;&#22788;&#29702;&#20855;&#26377;&#27169;&#31946;&#20449;&#24687;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#30340;&#35268;&#21010;&#20013;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.08307</link><description>&lt;p&gt;
&#22312;&#27169;&#31946;&#29615;&#22659;&#20013;&#34701;&#21512;&#35748;&#30693;&#22320;&#22270;&#23398;&#20064;&#21644;&#20027;&#21160;&#25512;&#29702;&#20197;&#36827;&#34892;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Integrating cognitive map learning and active inference for planning in ambiguous environments. (arXiv:2308.08307v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#27169;&#31946;&#29615;&#22659;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#35748;&#30693;&#22320;&#22270;&#23398;&#20064;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#22312;&#22788;&#29702;&#20855;&#26377;&#27169;&#31946;&#20449;&#24687;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#30340;&#35268;&#21010;&#20013;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#38656;&#35201;&#21516;&#26102;&#33719;&#24471;&#35748;&#30693;&#22320;&#22270;&#26469;&#23398;&#20064;&#19990;&#30028;&#30340;&#32467;&#26500;&#21644;&#33021;&#22815;&#24212;&#23545;&#23548;&#33322;&#27169;&#31946;&#29615;&#22659;&#25361;&#25112;&#30340;&#35268;&#21010;&#26426;&#21046;&#12290;&#23613;&#31649;&#22312;&#27599;&#20010;&#29420;&#31435;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#23427;&#20204;&#38598;&#25104;&#22312;&#19968;&#36215;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#19968;&#31181;&#32479;&#35745;&#27169;&#22411;&#30340;&#35748;&#30693;&#22320;&#22270;&#24418;&#25104;&#19982;&#25903;&#25345;&#19981;&#30830;&#23450;&#24615;&#35268;&#21010;&#30340;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#36827;&#34892;&#38598;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20811;&#38534;&#32467;&#26500;&#35748;&#30693;&#22270;(CSCG)&#27169;&#22411;&#30340;&#35748;&#30693;&#22320;&#22270;&#24418;&#25104;&#65292;&#24182;&#27604;&#36739;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20811;&#38534;&#22270;&#20195;&#29702;&#21644;&#19968;&#20010;&#20197;&#20027;&#21160;&#25512;&#29702;&#20026;&#39537;&#21160;&#30340;&#20811;&#38534;&#22270;&#20195;&#29702;&#22312;&#19977;&#20010;&#31354;&#38388;&#23548;&#33322;&#22330;&#26223;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#20004;&#31181;&#20195;&#29702;&#22312;&#31616;&#21333;&#22330;&#26223;&#20013;&#37117;&#26377;&#25928;&#65292;&#20294;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#22312;&#38754;&#23545;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#35268;&#21010;&#19978;&#26356;&#26377;&#25928;&#65292;&#20854;&#20013;&#24863;&#30693;&#35266;&#23519;&#25552;&#20379;&#26377;&#20851;&#20301;&#32622;&#30340;&#27169;&#31946;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Living organisms need to acquire both cognitive maps for learning the structure of the world and planning mechanisms able to deal with the challenges of navigating ambiguous environments. Although significant progress has been made in each of these areas independently, the best way to integrate them is an open research question. In this paper, we propose the integration of a statistical model of cognitive map formation within an active inference agent that supports planning under uncertainty. Specifically, we examine the clone-structured cognitive graph (CSCG) model of cognitive map formation and compare a naive clone graph agent with an active inference-driven clone graph agent, in three spatial navigation scenarios. Our findings demonstrate that while both agents are effective in simple scenarios, the active inference agent is more effective when planning in challenging scenarios, in which sensory observations provide ambiguous information about location.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RoBOS&#30340;&#40065;&#26834;&#36125;&#21494;&#26031;&#28385;&#36275;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#19978;&#19979;&#25991;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#26102;&#30340;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#19968;&#23450;&#30340;&#20998;&#24067;&#20559;&#31227;&#37327;&#19979;&#20445;&#35777;&#20111;&#24471;&#19981;&#20005;&#37325;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#20998;&#24067;&#20559;&#31227;&#37327;&#26080;&#20851;&#30340;&#36739;&#24369;&#36951;&#25022;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.08291</link><description>&lt;p&gt;
&#40065;&#26834;&#36125;&#21494;&#26031;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Bayesian Satisficing. (arXiv:2308.08291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RoBOS&#30340;&#40065;&#26834;&#36125;&#21494;&#26031;&#28385;&#36275;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#19978;&#19979;&#25991;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#26102;&#30340;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#19968;&#23450;&#30340;&#20998;&#24067;&#20559;&#31227;&#37327;&#19979;&#20445;&#35777;&#20111;&#24471;&#19981;&#20005;&#37325;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#20998;&#24067;&#20559;&#31227;&#37327;&#26080;&#20851;&#30340;&#36739;&#24369;&#36951;&#25022;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#23545;&#20110;&#23454;&#29616;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#40065;&#26834;&#28385;&#36275;&#65288;RS&#65289;&#22312;&#23454;&#29616;&#36229;&#36807;&#26399;&#26395;&#38408;&#20540;&#30340;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#23547;&#27714;&#23545;&#20110;&#26410;&#25351;&#23450;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#40065;&#26834;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#19978;&#19979;&#25991;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#23384;&#22312;&#30495;&#23454;&#21644;&#21442;&#32771;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#26102;&#30340;&#40065;&#26834;&#28385;&#36275;&#65288;RS&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RoBOS&#30340;&#26032;&#22411;&#22122;&#22768;&#40657;&#31665;&#20248;&#21270;&#30340;&#40065;&#26834;&#36125;&#21494;&#26031;&#28385;&#36275;&#31639;&#27861;&#12290;&#22312;&#26576;&#20123;&#20851;&#20110;&#20998;&#24067;&#20559;&#31227;&#37327;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#35777;&#20111;&#24471;&#19981;&#20005;&#37325;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#36739;&#24369;&#30340;&#36951;&#25022;&#27010;&#24565;&#65292;&#31216;&#20026;&#40065;&#26834;&#28385;&#36275;&#36951;&#25022;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19982;&#20998;&#24067;&#20559;&#31227;&#37327;&#26080;&#20851;&#30340;&#23376;&#32447;&#24615;&#19978;&#30028;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22914;&#20998;&#24067;&#20132;&#25442;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally rob
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;OOD&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#31070;&#31192;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27867;&#21270;&#25968;&#23383;&#36816;&#31639;&#26102;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.08268</link><description>&lt;p&gt;
&#23427;&#20854;&#23454;&#19981;&#37027;&#20040;&#31967;&#31957;&#65306;&#29702;&#35299;&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#23545;OOD&#27867;&#21270;&#30340;&#31070;&#31192;&#24615;&#33021;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models. (arXiv:2308.08268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08268
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;OOD&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#31070;&#31192;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27867;&#21270;&#25968;&#23383;&#36816;&#31639;&#26102;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#27809;&#26377;&#23436;&#20840;&#34987;&#29702;&#35299;&#65292;&#24182;&#19988;&#24182;&#19981;&#24635;&#26159;&#20196;&#20154;&#28385;&#24847;&#12290;&#30740;&#31350;&#20154;&#21592;&#20174;&#22522;&#26412;&#30340;&#25968;&#23398;&#20219;&#21153;&#65288;&#22914;n&#20301;&#25968;&#30340;&#21152;&#27861;&#25110;&#20056;&#27861;&#65289;&#24320;&#22987;&#65292;&#20316;&#20026;&#37325;&#35201;&#35270;&#35282;&#26469;&#30740;&#31350;&#27169;&#22411;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35266;&#23519;&#21040;&#24403;&#27169;&#22411;&#22312;n&#20301;&#25968;&#36816;&#31639;&#65288;&#20363;&#22914;&#21152;&#27861;&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#38271;&#24230;&#20026;n&#20301;&#30340;&#36755;&#20837;&#19978;&#21487;&#20197;&#25104;&#21151;&#27867;&#21270;&#65288;&#21363;&#20869;&#20998;&#24067;&#27867;&#21270;&#65289;&#65292;&#20294;&#22312;&#38271;&#24230;&#26356;&#38271;&#12289;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#22806;&#20998;&#24067;&#27867;&#21270;&#65289;&#20250;&#22833;&#36133;&#24182;&#19988;&#34920;&#29616;&#31070;&#31192;&#12290;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#20462;&#25913;&#20301;&#32622;&#23884;&#20837;&#12289;&#24494;&#35843;&#21644;&#24341;&#20837;&#26356;&#24191;&#27867;&#25110;&#26356;&#26377;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30340;&#31283;&#20581;&#24615;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Transformer-based models have achieved remarkable proficiency on solving diverse problems. However, their generalization ability is not fully understood and not always satisfying. Researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. Curiously, it is observed that when training on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably and mysteriously on longer, unseen cases (out-of-distribution (OOD) generalization). Studies try to bridge this gap with workarounds such as modifying position embedding, fine-tuning, and priming with more extensive or instructive data. However, without addressing the essential mechanism, there is hardly any guarantee regarding the robustness of these solutions. We bring this unexplained performan
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#20840;&#31216;&#37327;&#21270;&#27010;&#24565;&#25193;&#23637;&#20102;&#25551;&#36848;&#36923;&#36753;$\mathcal{EL}$&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#27169;&#24335;&#35821;&#20041;&#21644;&#20108;&#38454;&#35821;&#20041;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#24615;&#36136;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26377;&#29992;&#29255;&#27573;&#20013;&#30340;&#32467;&#35770;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2308.08252</link><description>&lt;p&gt;
&#25551;&#36848;&#36923;&#36753;&#36827;&#20837;&#20108;&#38454;--&#29992;&#20840;&#31216;&#37327;&#21270;&#27010;&#24565;&#25193;&#23637;EL
&lt;/p&gt;
&lt;p&gt;
Description Logics Go Second-Order -- Extending EL with Universally Quantified Concepts. (arXiv:2308.08252v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08252
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#20840;&#31216;&#37327;&#21270;&#27010;&#24565;&#25193;&#23637;&#20102;&#25551;&#36848;&#36923;&#36753;$\mathcal{EL}$&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#27169;&#24335;&#35821;&#20041;&#21644;&#20108;&#38454;&#35821;&#20041;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#24615;&#36136;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26377;&#29992;&#29255;&#27573;&#20013;&#30340;&#32467;&#35770;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#19978;&#65292;&#25551;&#36848;&#36923;&#36753;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21487;&#20197;&#32763;&#35793;&#25104;&#21487;&#21028;&#23450;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#30340;&#29305;&#24449;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#24323;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#23547;&#25214;&#26377;&#29992;&#19988;&#21487;&#21028;&#23450;&#30340;&#19968;&#38454;&#36923;&#36753;&#20197;&#22806;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#31216;&#37327;&#21270;&#27010;&#24565;&#65292;&#23427;&#20204;&#20197;&#21487;&#20197;&#34987;&#20219;&#24847;&#27010;&#24565;&#26367;&#25442;&#30340;&#21464;&#37327;&#24418;&#24335;&#20986;&#29616;&#65292;&#24182;&#23450;&#20041;&#20102;&#36825;&#20010;&#25193;&#23637;&#30340;&#20004;&#31181;&#35821;&#20041;&#12290;&#27169;&#24335;&#35821;&#20041;&#21482;&#20801;&#35768;&#27010;&#24565;&#21464;&#37327;&#34987;&#29305;&#23450;&#35821;&#35328;&#30340;&#27010;&#24565;&#26367;&#25442;&#65292;&#20135;&#29983;&#31867;&#20284;&#27169;&#24577;&#36923;&#36753;&#30340;&#20844;&#29702;&#27169;&#24335;&#12290;&#20108;&#38454;&#35821;&#20041;&#20801;&#35768;&#27010;&#24565;&#21464;&#37327;&#34987;&#22495;&#30340;&#20219;&#24847;&#23376;&#38598;&#26367;&#25442;&#65292;&#31867;&#20284;&#20110;&#20108;&#38454;&#36923;&#36753;&#20013;&#30340;&#37327;&#21270;&#35859;&#35789;&#12290;&#20026;&#20102;&#30740;&#31350;&#25152;&#25552;&#20986;&#30340;&#35821;&#20041;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#25551;&#36848;&#36923;&#36753;$\mathcal{EL}$&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25193;&#23637;&#30340;&#26377;&#29992;&#29255;&#27573;&#20013;&#65292;&#19981;&#21516;&#35821;&#20041;&#25152;&#34164;&#21547;&#30340;&#32467;&#35770;&#26159;&#30456;&#21516;&#30340;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#32463;&#20856;&#36923;&#36753;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of Description Logics have been historically mostly focused on features that can be translated to decidable fragments of first-order logic. In this paper, we leave this restriction behind and look for useful and decidable extensions outside first-order logic. We introduce universally quantified concepts, which take the form of variables that can be replaced with arbitrary concepts, and define two semantics of this extension. A schema semantics allows replacements of concept variables only by concepts from a particular language, giving us axiom schemata similar to modal logics. A second-order semantics allows replacement of concept variables with arbitrary subsets of the domain, which is similar to quantified predicates in second-order logic.  To study the proposed semantics, we focus on the extension of the description logic $\mathcal{EL}$. We show that for a useful fragment of the extension, the conclusions entailed by the different semantics coincide, allowing us to use cla
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#26041;&#27861;&#26469;&#28608;&#27963;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#32467;&#26524;&#27809;&#26377;&#26126;&#26174;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08241</link><description>&lt;p&gt;
TEST: &#25991;&#26412;&#21407;&#22411;&#23545;&#40784;&#23884;&#20837;&#20197;&#28608;&#27963;LLM&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series. (arXiv:2308.08241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08241
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#26041;&#27861;&#26469;&#28608;&#27963;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#32467;&#26524;&#27809;&#26377;&#26126;&#26174;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#65288;TS&#65289;&#20219;&#21153;&#30340;&#31574;&#30053;&#65306;LLM-for-TS&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#19968;&#20010;&#38024;&#23545;TS&#25968;&#25454;&#30340;&#22522;&#30784;&#22823;&#27169;&#22411;&#65307;TS-for-LLM&#65292;&#20351;&#39044;&#35757;&#32451;&#30340;LLM&#33021;&#22815;&#22788;&#29702;TS&#25968;&#25454;&#12290;&#37492;&#20110;&#25968;&#25454;&#31215;&#32047;&#19981;&#36275;&#12289;&#36164;&#28304;&#26377;&#38480;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;TS-for-LLM&#26041;&#27861;&#65292;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;LLM&#30340;TS&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#28608;&#27963;LLM&#23545;TS&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;TEST&#12290;&#23427;&#39318;&#20808;&#23545;TS&#36827;&#34892;&#26631;&#35760;&#21270;&#22788;&#29702;&#65292;&#24314;&#31435;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23454;&#20363;&#12289;&#29305;&#24449;&#21644;&#25991;&#26412;&#21407;&#22411;&#23545;&#40784;&#23545;&#23427;&#20204;&#36827;&#34892;&#23884;&#20837;&#65292;&#28982;&#21518;&#21019;&#24314;&#25552;&#31034;&#20197;&#20351;LLM&#26356;&#23481;&#26131;&#25509;&#21463;&#23884;&#20837;&#65292;&#24182;&#26368;&#32456;&#23454;&#26045;TS&#20219;&#21153;&#12290;&#20351;&#29992;8&#20010;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#22823;&#23567;&#30340;LLM&#23545;TS&#20998;&#31867;&#21644;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23613;&#31649;&#20854;&#32467;&#26524;&#19981;&#33021;&#26174;&#33879;&#36229;&#36234;&#24403;&#21069;&#20026;TS&#20219;&#21153;&#23450;&#21046;&#30340;SOTA&#27169;&#22411;&#65292;&#20294;&#36890;&#36807;&#23558;LLM&#35270;&#20026;&#27169;&#24335;&#26426;&#22120;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;TS&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work summarizes two strategies for completing time-series (TS) tasks using today's language model (LLM): LLM-for-TS, design and train a fundamental large model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS data. Considering the insufficient data accumulation, limited resources, and semantic context requirements, this work focuses on TS-for-LLM methods, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed them by instance-wise, feature-wise, and text-prototype-aligned contrast, and then creates prompts to make LLM more open to embeddings, and finally implements TS tasks. Experiments are carried out on TS classification and forecasting tasks using 8 LLMs with different structures and sizes. Although its results cannot significantly outperform the current SOTA models customized for TS tasks, by treating LLM as the pattern machine, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#36890;&#36807;&#23545;NLP&#20013;&#22522;&#20110;Transformer&#30340;MTL&#26041;&#27861;&#20197;&#21450;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#38454;&#27573;&#30340;&#25361;&#25112;&#36827;&#34892;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;&#30456;&#20851;&#39046;&#22495;&#30340;&#27010;&#36848;&#21644;&#21160;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.08234</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey. (arXiv:2308.08234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#36890;&#36807;&#23545;NLP&#20013;&#22522;&#20110;Transformer&#30340;MTL&#26041;&#27861;&#20197;&#21450;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#38454;&#27573;&#30340;&#25361;&#25112;&#36827;&#34892;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;&#30456;&#20851;&#39046;&#22495;&#30340;&#27010;&#36848;&#21644;&#21160;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20174;&#35757;&#32451;&#21040;&#22312;&#29983;&#20135;&#20013;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#26377;&#25928;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#26356;&#26032;&#22810;&#20010;&#27169;&#22411;&#21487;&#33021;&#22797;&#26434;&#12289;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#20316;&#20026;&#25913;&#36827;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#26412;&#35843;&#30740;&#39318;&#20808;&#27010;&#36848;&#20102;NLP&#20013;&#22522;&#20110;Transformer&#30340;MTL&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#20013;&#20351;&#29992;MTL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#24037;&#31243;&#12289;&#27169;&#22411;&#24320;&#21457;&#12289;&#37096;&#32626;&#21644;&#30417;&#25511;&#38454;&#27573;&#30340;&#25361;&#25112;&#12290;&#26412;&#39033;&#35843;&#30740;&#38598;&#20013;&#20110;&#22522;&#20110;Transformer&#30340;MTL&#26550;&#26500;&#65292;&#24182;&#25454;&#25105;&#20204;&#25152;&#30693;&#26159;&#39318;&#21019;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#65292;&#24212;&#29992;&#20110;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#35770;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08206</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#22312;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Explainable Multi-View Deep Networks Methodology for Experimental Physics. (arXiv:2308.08206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#65292;&#24212;&#29992;&#20110;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#35770;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23454;&#39564;&#24120;&#28041;&#21450;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#65292;&#22914;X&#23556;&#32447;&#25195;&#25551;&#21644;&#26174;&#24494;&#22270;&#20687;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#36825;&#20123;&#23454;&#39564;&#30340;&#30417;&#30563;&#20998;&#26512;&#20013;&#12290;&#21512;&#24182;&#19981;&#21516;&#30340;&#22270;&#20687;&#34920;&#36798;&#32463;&#24120;&#38656;&#35201;&#27491;&#30830;&#20998;&#26512;&#21644;&#20570;&#20986;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#22810;&#35270;&#35282;&#25968;&#25454;&#24212;&#36816;&#32780;&#29983; - &#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#26679;&#26412;&#30001;&#26469;&#33258;&#19981;&#21516;&#35282;&#24230;&#12289;&#26469;&#28304;&#25110;&#27169;&#24577;&#30340;&#35270;&#22270;&#25551;&#36848;&#12290;&#22810;&#35270;&#35282;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22810;&#35270;&#35282;&#27169;&#22411;&#32570;&#20047;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#30001;&#20110;&#20854;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#19981;&#21516;&#22810;&#35270;&#35282;&#26550;&#26500;&#65292;&#27599;&#20010;&#26550;&#26500;&#37117;&#36866;&#21512;&#35299;&#20915;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#37322;&#22810;&#35270;&#35282;&#27169;&#22411;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical experiments often involve multiple imaging representations, such as X-ray scans and microscopic images. Deep learning models have been widely used for supervised analysis in these experiments. Combining different image representations is frequently required to analyze and make a decision properly. Consequently, multi-view data has emerged - datasets where each sample is described by views from different angles, sources, or modalities. These problems are addressed with the concept of multi-view learning. Understanding the decision-making process of deep learning models is essential for reliable and credible analysis. Hence, many explainability methods have been devised recently. Nonetheless, there is a lack of proper explainability in multi-view models, which are challenging to explain due to their architectures. In this paper, we suggest different multi-view architectures for the vision domain, each suited to another problem, and we also present a methodology for explaining th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OWL DL&#26412;&#20307;&#35770;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35268;&#21010;&#35268;&#33539;&#21644;&#26412;&#20307;&#20998;&#24320;&#65292;&#24182;&#20351;&#29992;&#25509;&#21475;&#23558;&#20854;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#35268;&#21010;&#19987;&#23478;&#19982;&#26412;&#20307;&#19987;&#23478;&#30340;&#32039;&#23494;&#21512;&#20316;&#12290;&#36825;&#31181;&#26041;&#27861;&#20248;&#21270;&#20102;&#30456;&#23545;&#36739;&#23567;&#39046;&#22495;&#19979;&#30340;&#26412;&#20307;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#25972;&#20010;OWL DL&#29255;&#27573;&#12290;</title><link>http://arxiv.org/abs/2308.08200</link><description>&lt;p&gt;
&#22522;&#20110;OWL DL&#26412;&#20307;&#35770;&#30340;&#35268;&#21010;&#26041;&#27861;&#65288;&#25193;&#23637;&#29256;&#26412;&#65289;
&lt;/p&gt;
&lt;p&gt;
Towards Ontology-Mediated Planning with OWL DL Ontologies (Extended Version). (arXiv:2308.08200v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08200
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OWL DL&#26412;&#20307;&#35770;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35268;&#21010;&#35268;&#33539;&#21644;&#26412;&#20307;&#20998;&#24320;&#65292;&#24182;&#20351;&#29992;&#25509;&#21475;&#23558;&#20854;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#35268;&#21010;&#19987;&#23478;&#19982;&#26412;&#20307;&#19987;&#23478;&#30340;&#32039;&#23494;&#21512;&#20316;&#12290;&#36825;&#31181;&#26041;&#27861;&#20248;&#21270;&#20102;&#30456;&#23545;&#36739;&#23567;&#39046;&#22495;&#19979;&#30340;&#26412;&#20307;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#25972;&#20010;OWL DL&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35268;&#21010;&#35821;&#35328;&#20551;&#35774;&#39046;&#22495;&#26159;&#23553;&#38381;&#30340;&#65292;&#24182;&#20855;&#26377;&#23553;&#38381;&#19990;&#30028;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#25193;&#23637;&#36825;&#20123;&#35821;&#35328;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#19982;DL&#25512;&#29702;&#32467;&#21512;&#65292;&#28982;&#21518;&#25353;&#29031;&#36890;&#24120;&#30340;&#24320;&#25918;&#19990;&#30028;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#12290;&#30446;&#21069;&#30340;DL&#26412;&#20307;&#35268;&#21010;&#26041;&#27861;&#23558;DL&#30452;&#25509;&#38598;&#25104;&#21040;&#35268;&#21010;&#35821;&#35328;&#20013;&#65292;&#24182;&#22522;&#20110;&#19968;&#38454;&#37325;&#20889;&#25110;&#37325;&#20889;&#20026;datalog&#26469;&#24320;&#21457;&#23454;&#38469;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#35268;&#21010;&#35268;&#33539;&#21644;&#26412;&#20307;&#20998;&#24320;&#65292;&#24182;&#36890;&#36807;&#25509;&#21475;&#23558;&#23427;&#20204;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#36825;&#20351;&#24471;&#35268;&#21010;&#19987;&#23478;&#33021;&#22815;&#20351;&#29992;&#29087;&#24713;&#30340;&#24418;&#24335;&#21270;&#35821;&#35328;&#24037;&#20316;&#65292;&#21516;&#26102;&#26412;&#20307;&#19987;&#23478;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21644;&#25193;&#23637;&#29616;&#26377;&#26412;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38024;&#23545;&#30456;&#23545;&#36739;&#23567;&#30340;&#39046;&#22495;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#24182;&#25903;&#25345;&#25972;&#20010;OWL DL&#29255;&#27573;&#12290;&#20855;&#20307;&#24605;&#36335;&#26159;&#23558;&#26412;&#20307;&#35268;&#21010;&#38382;&#39064;&#37325;&#20889;&#20026;&#32463;&#20856;&#30340;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
While classical planning languages make the closed-domain and closed-world assumption, there have been various approaches to extend those with DL reasoning, which is then interpreted under the usual open-world semantics. Current approaches for planning with DL ontologies integrate the DL directly into the planning language, and practical approaches have been developed based on first-order rewritings or rewritings into datalog. We present here a new approach in which the planning specification and ontology are kept separate, and are linked together using an interface. This allows planning experts to work in a familiar formalism, while existing ontologies can be easily integrated and extended by ontology experts. Our approach for planning with those ontology-mediated planning problems is optimized for cases with comparatively small domains, and supports the whole OWL DL fragment. The idea is to rewrite the ontology-mediated planning problem into a classical planning problem to be process
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#33258;&#21160;&#27010;&#29575;&#35268;&#21010;&#21644;&#21160;&#24577;&#22270;&#20998;&#26512;&#24314;&#27169;COVID-19&#22312;&#23460;&#20869;&#31354;&#38388;&#30340;&#20256;&#25773;&#65292;&#36890;&#36807;&#38750;&#33647;&#29289;&#24178;&#39044;&#25511;&#21046;&#30142;&#30149;&#30340;&#20256;&#25773;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#24178;&#39044;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08190</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#27010;&#29575;&#35268;&#21010;&#24314;&#27169;COVID-19&#22312;&#23460;&#20869;&#31354;&#38388;&#20013;&#30340;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Modelling the Spread of COVID-19 in Indoor Spaces using Automated Probabilistic Planning. (arXiv:2308.08190v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#33258;&#21160;&#27010;&#29575;&#35268;&#21010;&#21644;&#21160;&#24577;&#22270;&#20998;&#26512;&#24314;&#27169;COVID-19&#22312;&#23460;&#20869;&#31354;&#38388;&#30340;&#20256;&#25773;&#65292;&#36890;&#36807;&#38750;&#33647;&#29289;&#24178;&#39044;&#25511;&#21046;&#30142;&#30149;&#30340;&#20256;&#25773;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#24178;&#39044;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#35268;&#21010;&#21644;&#21160;&#24577;&#22270;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#27169;&#25311;COVID-19&#22312;&#23460;&#20869;&#31354;&#38388;&#20013;&#30340;&#20256;&#25773;&#12290;&#25105;&#20204;&#36890;&#36807;&#38750;&#33647;&#29289;&#24178;&#39044;&#25511;&#21046;&#30142;&#30149;&#30340;&#20256;&#25773;&#65292;&#22914;&#24378;&#21046;&#20329;&#25140;&#21475;&#32617;&#21644;&#25509;&#31181;&#30123;&#33495;&#65292;&#24182;&#27604;&#36739;&#25317;&#25380;&#21644;&#23481;&#37327;&#38480;&#21046;&#23545;COVID-19&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#27010;&#29575;&#35268;&#21010;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#24178;&#39044;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The coronavirus disease 2019 (COVID-19) pandemic has been ongoing for around 3 years, and has infected over 750 million people and caused over 6 million deaths worldwide at the time of writing. Throughout the pandemic, several strategies for controlling the spread of the disease have been debated by healthcare professionals, government authorities, and international bodies. To anticipate the potential impact of the disease, and to simulate the effectiveness of different mitigation strategies, a robust model of disease spread is needed. In this work, we explore a novel approach based on probabilistic planning and dynamic graph analysis to model the spread of COVID-19 in indoor spaces. We endow the planner with means to control the spread of the disease through non-pharmaceutical interventions (NPIs) such as mandating masks and vaccines, and we compare the impact of crowds and capacity limits on the spread of COVID-19 in these settings. We demonstrate that the use of probabilistic planni
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#31639;&#27861;&#34917;&#25937;&#20013;&#30340;&#20869;&#29983;&#21160;&#21147;&#23398;&#21644;&#23545;&#31574;&#24433;&#21709;&#20854;&#20182;&#20010;&#20307;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#25581;&#31034;&#20102;&#23545;&#31574;&#30340;&#38544;&#34255;&#25104;&#26412;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08187</link><description>&lt;p&gt;
&#31639;&#27861;&#34917;&#25937;&#20013;&#30340;&#20869;&#29983;&#23439;&#35266;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Endogenous Macrodynamics in Algorithmic Recourse. (arXiv:2308.08187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08187
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#31639;&#27861;&#34917;&#25937;&#20013;&#30340;&#20869;&#29983;&#21160;&#21147;&#23398;&#21644;&#23545;&#31574;&#24433;&#21709;&#20854;&#20182;&#20010;&#20307;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#25581;&#31034;&#20102;&#23545;&#31574;&#30340;&#38544;&#34255;&#25104;&#26412;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;&#23545;&#31574;&#35299;&#37322;&#21644;&#31639;&#27861;&#34917;&#25937;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#30340;&#21333;&#20010;&#20010;&#20307;&#19978;&#65306;&#22312;&#32473;&#23450;&#19968;&#20123;&#20272;&#35745;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#30340;&#21333;&#20010;&#23454;&#20363;&#30340;&#26377;&#25928;&#23545;&#31574;&#35299;&#37322;&#12290;&#36825;&#20123;&#23545;&#31574;&#35299;&#37322;&#30340;&#33021;&#21147;&#22788;&#29702;&#25968;&#25454;&#21644;&#27169;&#22411;&#28418;&#31227;&#31561;&#21160;&#24577;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#23569;&#30740;&#31350;&#30340;&#25361;&#25112;&#12290;&#19982;&#27492;&#30456;&#20851;&#30340;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#19968;&#20010;&#20010;&#20307;&#23545;&#31574;&#23454;&#26045;&#30340;&#23454;&#38469;&#24433;&#21709;&#20854;&#20182;&#20010;&#20307;&#30340;&#38382;&#39064;&#65292;&#20851;&#20110;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#30456;&#24403;&#23569;&#12290;&#36890;&#36807;&#36825;&#39033;&#24037;&#20316;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#29616;&#26377;&#30340;&#35768;&#22810;&#26041;&#27861;&#21487;&#20197;&#34987;&#32479;&#19968;&#25551;&#36848;&#20026;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#26694;&#26550;&#27809;&#26377;&#32771;&#34385;&#21040;&#19968;&#20010;&#38544;&#34255;&#30340;&#23545;&#31574;&#25104;&#26412;&#65292;&#22312;&#30740;&#31350;&#32676;&#20307;&#23618;&#38754;&#19978;&#30340;&#20869;&#29983;&#21160;&#21147;&#23398;&#26102;&#25165;&#20250;&#26174;&#29616;&#20986;&#26469;&#12290;&#36890;&#36807;&#28041;&#21450;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#23545;&#31574;&#35299;&#37322;&#26041;&#27861;&#30340;&#20223;&#30495;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Existing work on Counterfactual Explanations (CE) and Algorithmic Recourse (AR) has largely focused on single individuals in a static environment: given some estimated model, the goal is to find valid counterfactuals for an individual instance that fulfill various desiderata. The ability of such counterfactuals to handle dynamics like data and model drift remains a largely unexplored research challenge. There has also been surprisingly little work on the related question of how the actual implementation of recourse by one individual may affect other individuals. Through this work, we aim to close that gap. We first show that many of the existing methodologies can be collectively described by a generalized framework. We then argue that the existing framework does not account for a hidden external cost of recourse, that only reveals itself when studying the endogenous dynamics of recourse at the group level. Through simulation experiments involving various state-of the-art counterfactual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31616;&#21333;&#30340;&#32531;&#23384;&#25552;&#39640;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24494;&#35843;&#26816;&#32034;&#27169;&#22359;&#24182;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#23545;&#35805;&#31995;&#32479;&#27169;&#22411;&#65292;&#31995;&#32479;&#21487;&#20197;&#21160;&#24577;&#26356;&#26032;&#24182;&#22788;&#29702;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#23545;&#35805;&#22330;&#26223;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#22312;&#38750;&#31354;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#19978;&#25552;&#39640;&#20102;6.7%&#12290;</title><link>http://arxiv.org/abs/2308.08169</link><description>&lt;p&gt;
&#22686;&#24378;&#24615;&#33021;&#65306;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#22411;&#31995;&#32479;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#23545;&#35805;&#22330;&#26223;&#19979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System. (arXiv:2308.08169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31616;&#21333;&#30340;&#32531;&#23384;&#25552;&#39640;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24494;&#35843;&#26816;&#32034;&#27169;&#22359;&#24182;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#23545;&#35805;&#31995;&#32479;&#27169;&#22411;&#65292;&#31995;&#32479;&#21487;&#20197;&#21160;&#24577;&#26356;&#26032;&#24182;&#22788;&#29702;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#23545;&#35805;&#22330;&#26223;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#22312;&#38750;&#31354;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#19978;&#25552;&#39640;&#20102;6.7%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#36827;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#32531;&#23384;&#20351;&#24471;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#26356;&#21152;&#28789;&#27963;&#12290;&#36825;&#20010;&#32531;&#23384;&#33021;&#22815;&#21160;&#24577;&#26356;&#26032;&#31995;&#32479;&#24182;&#22788;&#29702;&#29616;&#26377;&#21644;&#26410;&#30693;&#30340;&#23545;&#35805;&#22330;&#26223;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#26816;&#32034;&#27169;&#22359;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20415;&#20174;&#32531;&#23384;&#20013;&#26377;&#25928;&#22320;&#26816;&#32034;&#21040;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#23545;&#35805;&#31995;&#32479;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#23545;&#35805;&#26102;&#21487;&#20197;&#24341;&#29992;&#21644;&#32852;&#31995;&#23545;&#35805;&#21382;&#21490;&#21644;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#12290;&#32531;&#23384;&#30340;&#26500;&#24314;&#38750;&#24120;&#31616;&#21333;&#65292;&#32780;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#20027;&#24178;&#27169;&#22411;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#20860;&#23481;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#38750;&#31354;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#25552;&#39640;&#20102;6.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end task-oriented dialogue (TOD) systems have achieved promising performance by leveraging sophisticated natural language understanding and natural language generation capabilities of pre-trained models. This work enables the TOD systems with more flexibility through a simple cache. The cache provides the flexibility to dynamically update the TOD systems and handle both existing and unseen dialogue scenarios. Towards this end, we first fine-tune a retrieval module to effectively retrieve the most relevant information entries from the cache. We then train end-to-end TOD models that can refer to and ground on both dialogue history and retrieved information during TOD generation. The cache is straightforward to construct, and the backbone models of TOD systems are compatible with existing pre-trained generative models. Extensive experiments demonstrate the superior performance of our framework, with a notable improvement in non-empty joint goal accuracy by 6.7% compared to strong b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21407;&#22411;&#37096;&#20214;&#35299;&#37322;&#30340;&#31354;&#38388;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#37322;&#24615;&#22522;&#20934;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#34917;&#20607;&#19981;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#34920;&#26126;&#20102;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#34917;&#20607;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08162</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#21407;&#22411;&#37096;&#20214;&#35299;&#37322;&#30340;&#31354;&#38388;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#37322;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations. (arXiv:2308.08162v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21407;&#22411;&#37096;&#20214;&#35299;&#37322;&#30340;&#31354;&#38388;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#37322;&#24615;&#22522;&#20934;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#34917;&#20607;&#19981;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#34920;&#26126;&#20102;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#34917;&#20607;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24544;&#23454;&#30340;&#33258;&#35299;&#37322;&#24615;&#65292;&#21407;&#22411;&#37096;&#20214;&#32593;&#32476;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#22270;&#35745;&#31639;&#22312;&#20498;&#25968;&#31532;&#20108;&#23618;&#32593;&#32476;&#20013;&#12290;&#22240;&#27492;&#65292;&#21407;&#22411;&#28608;&#27963;&#21306;&#22495;&#30340;&#24863;&#21463;&#37326;&#36890;&#24120;&#20381;&#36182;&#20110;&#22270;&#20687;&#21306;&#22495;&#22806;&#30340;&#37096;&#20998;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#31216;&#20026;&#31354;&#38388;&#35299;&#37322;&#19981;&#19968;&#33268;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35299;&#37322;&#24615;&#22522;&#20934;&#65292;&#25552;&#20379;&#19968;&#22871;&#19987;&#29992;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#19968;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20607;&#19981;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#20197;&#21450;&#25552;&#20986;&#30340;&#34917;&#20607;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototypical parts-based networks are becoming increasingly popular due to their faithful self-explanations. However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations. We name this undesired behavior a spatial explanation misalignment and introduce an interpretability benchmark with a set of dedicated metrics for quantifying this phenomenon. In addition, we propose a method for misalignment compensation and apply it to existing state-of-the-art models. We show the expressiveness of our benchmark and the effectiveness of the proposed compensation methodology through extensive empirical studies.
&lt;/p&gt;</description></item><item><title>AutoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#19979;&#19968;&#20195;LLM&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#20248;&#38597;&#22320;&#22788;&#29702;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.08155</link><description>&lt;p&gt;
AutoGen:&#36890;&#36807;&#22810;&#20195;&#29702;&#23545;&#35805;&#26694;&#26550;&#23454;&#29616;&#19979;&#19968;&#20195;LLM&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. (arXiv:2308.08155v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08155
&lt;/p&gt;
&lt;p&gt;
AutoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#19979;&#19968;&#20195;LLM&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#20248;&#38597;&#22320;&#22788;&#29702;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;AutoGen&#65292;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#26469;&#24320;&#21457;LLM&#24212;&#29992;&#31243;&#24207;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;AutoGen&#20195;&#29702;&#21487;&#20197;&#23450;&#21046;&#12289;&#21487;&#23545;&#35805;&#65292;&#24182;&#19988;&#21487;&#20197;&#26080;&#32541;&#22320;&#20801;&#35768;&#20154;&#31867;&#21442;&#19982;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#21508;&#31181;&#27169;&#24335;&#19979;&#36816;&#34892;&#65292;&#21033;&#29992;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#24037;&#20855;&#30340;&#32452;&#21512;&#12290;AutoGen&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#22810;&#20010;&#20248;&#21183;&#65306;a&#65289;&#23427;&#33021;&#22815;&#20248;&#38597;&#22320;&#22788;&#29702;&#36825;&#20123;LLM&#30340;&#24378;&#22823;&#20294;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65307;b&#65289;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#36890;&#36807;&#20195;&#29702;&#20043;&#38388;&#30340;&#23545;&#35805;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#33258;&#21160;&#21270;&#65307;c&#65289;&#23427;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#22797;&#26434;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29616;&#65292;&#20316;&#20026;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#24320;&#21457;&#20154;&#21592;&#22914;&#20309;&#36731;&#26494;&#20351;&#29992;AutoGen&#26377;&#25928;&#22320;&#35299;&#20915;&#20219;&#21153;&#25110;&#26500;&#24314;&#24212;&#29992;&#31243;&#24207;&#65292;&#28085;&#30422;&#32534;&#31243;&#12289;&#25968;&#23398;&#12289;&#36816;&#31609;&#23398;&#12289;&#23089;&#20048;&#12289;&#22312;&#32447;&#20915;&#31574;&#12289;&#38382;&#31572;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents AutoGen, a new framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. AutoGen's design offers multiple advantages: a) it gracefully navigates the strong but imperfect generation and reasoning abilities of these LLMs; b) it leverages human understanding and intelligence, while providing valuable automation through conversations between agents; c) it simplifies and unifies the implementation of complex LLM workflows as automated agent chats. We provide many diverse examples of how developers can easily use AutoGen to effectively solve tasks or build applications, ranging from coding, mathematics, operations research, entertainment, online decision-making, question answering, etc.
&lt;/p&gt;</description></item><item><title>SYENet&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#22810;&#20010;&#20302;&#32423;&#35270;&#35273;&#20219;&#21153;&#30340;&#23454;&#26102;&#22788;&#29702;&#12290;&#35813;&#32593;&#32476;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#37327;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#30340;&#26500;&#24314;&#27169;&#22359;&#21644;&#20108;&#27425;&#36830;&#25509;&#21333;&#20803;&#23454;&#29616;&#20102;&#19981;&#23545;&#31216;&#20998;&#25903;&#32467;&#26524;&#30340;&#26377;&#25928;&#36830;&#25509;&#12290;&#36890;&#36807;&#24341;&#20837;&#24322;&#24120;&#20540;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;PSNR&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08137</link><description>&lt;p&gt;
SYENet&#65306;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#20855;&#26377;&#23454;&#26102;&#24615;&#33021;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#20302;&#32423;&#35270;&#35273;&#20219;&#21153;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SYENet: A Simple Yet Effective Network for Multiple Low-Level Vision Tasks with Real-time Performance on Mobile Device. (arXiv:2308.08137v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08137
&lt;/p&gt;
&lt;p&gt;
SYENet&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#22810;&#20010;&#20302;&#32423;&#35270;&#35273;&#20219;&#21153;&#30340;&#23454;&#26102;&#22788;&#29702;&#12290;&#35813;&#32593;&#32476;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#37327;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#30340;&#26500;&#24314;&#27169;&#22359;&#21644;&#20108;&#27425;&#36830;&#25509;&#21333;&#20803;&#23454;&#29616;&#20102;&#19981;&#23545;&#31216;&#20998;&#25903;&#32467;&#26524;&#30340;&#26377;&#25928;&#36830;&#25509;&#12290;&#36890;&#36807;&#24341;&#20837;&#24322;&#24120;&#20540;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;PSNR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#19978;&#35299;&#20915;&#21508;&#31181;&#20302;&#32423;&#35270;&#35273;&#20219;&#21153;&#36880;&#28176;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#20219;&#21153;&#29305;&#23450;&#30340;&#31639;&#27861;&#20351;&#23427;&#20204;&#38590;&#20197;&#38598;&#25104;&#21040;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#22823;&#37327;&#30340;&#21442;&#25968;&#20351;&#24471;&#23454;&#26102;&#25512;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;SYENet&#65292;&#21482;&#26377;&#22823;&#32422;6K&#20010;&#21442;&#25968;&#65292;&#20197;&#23454;&#26102;&#26041;&#24335;&#22788;&#29702;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#22810;&#20010;&#20302;&#32423;&#35270;&#35273;&#20219;&#21153;&#12290;SYENet&#30001;&#20004;&#20010;&#19981;&#23545;&#31216;&#20998;&#25903;&#21644;&#31616;&#21333;&#30340;&#26500;&#24314;&#27169;&#22359;&#32452;&#25104;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#36830;&#25509;&#19981;&#23545;&#31216;&#20998;&#25903;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#27425;&#36830;&#25509;&#21333;&#20803;&#65288;QCU&#65289;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#20540;&#24863;&#30693;&#25439;&#22833;&#26469;&#22788;&#29702;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#19982;&#23454;&#26102;&#24212;&#29992;&#20013;&#30340;&#20854;&#20182;&#32593;&#32476;&#30456;&#27604;&#20855;&#26377;&#26368;&#20339;PSNR&#26469;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of AI hardware accelerators, applying deep learning-based algorithms to solve various low-level vision tasks on mobile devices has gradually become possible. However, two main problems still need to be solved: task-specific algorithms make it difficult to integrate them into a single neural network architecture, and large amounts of parameters make it difficult to achieve real-time inference. To tackle these problems, we propose a novel network, SYENet, with only $~$6K parameters, to handle multiple low-level vision tasks on mobile devices in a real-time manner. The SYENet consists of two asymmetrical branches with simple building blocks. To effectively connect the results by asymmetrical branches, a Quadratic Connection Unit(QCU) is proposed. Furthermore, to improve performance, a new Outlier-Aware Loss is proposed to process the image. The proposed method proves its superior performance with the best PSNR as compared with other networks in real-time applica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#26816;&#32034;&#30340;&#25490;&#21517;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2308.08131</link><description>&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#25490;&#21517;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ranking-aware Uncertainty for Text-guided Image Retrieval. (arXiv:2308.08131v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#26816;&#32034;&#30340;&#25490;&#21517;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#26816;&#32034;&#26159;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#24847;&#22270;&#32780;&#23558;&#26465;&#20214;&#25991;&#26412;&#32435;&#20837;&#20854;&#20013;&#12290;&#20256;&#32479;&#19978;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#30528;&#37325;&#20110;&#36890;&#36807;&#25552;&#20379;&#30340;&#19977;&#20803;&#32452;&lt;&#28304;&#22270;&#20687;&#65292;&#28304;&#25991;&#26412;&#65292;&#30446;&#26631;&#22270;&#20687;&gt;&#26469;&#26368;&#23567;&#21270;&#23884;&#20837;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19977;&#20803;&#32452;&#20248;&#21270;&#21487;&#33021;&#20250;&#38480;&#21046;&#23398;&#20064;&#21040;&#26356;&#35814;&#32454;&#25490;&#21517;&#20449;&#24687;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#20363;&#22914;&#65292;&#36825;&#20123;&#19977;&#20803;&#32452;&#26159;&#19968;&#23545;&#19968;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#19988;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#21040;&#22312;&#21453;&#39304;&#35821;&#35328;&#21644;&#22270;&#20687;&#20013;&#20986;&#29616;&#30340;&#22810;&#23545;&#22810;&#23545;&#24212;&#20851;&#31995;&#36896;&#25104;&#30340;&#35821;&#20041;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#25429;&#25417;&#26356;&#22810;&#30340;&#25490;&#21517;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25490;&#21517;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#25552;&#20379;&#30340;&#19977;&#20803;&#32452;&#26469;&#24314;&#27169;&#22810;&#23545;&#22810;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#23398;&#20064;&#26469;&#23398;&#20064;&#29305;&#24449;&#30340;&#38543;&#26426;&#25490;&#21517;&#21015;&#34920;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#23884;&#22871;&#19981;&#30830;&#23450;&#24615;&#65292;&#26088;&#22312;&#20351;&#29992;&#39640;&#26031;
&lt;/p&gt;
&lt;p&gt;
Text-guided image retrieval is to incorporate conditional text to better capture users' intent. Traditionally, the existing methods focus on minimizing the embedding distances between the source inputs and the targeted image, using the provided triplets $\langle$source image, source text, target image$\rangle$. However, such triplet optimization may limit the learned retrieval model to capture more detailed ranking information, e.g., the triplets are one-to-one correspondences and they fail to account for many-to-many correspondences arising from semantic diversity in feedback languages and images. To capture more ranking information, we propose a novel ranking-aware uncertainty approach to model many-to-many correspondences by only using the provided triplets. We introduce uncertainty learning to learn the stochastic ranking list of features. Specifically, our approach mainly comprises three components: (1) In-sample uncertainty, which aims to capture semantic diversity using a Gaussi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#31995;&#32479;&#21270;&#32534;&#30721;&#21644;&#21452;&#37325;&#36974;&#34109;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08128</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#36827;&#34892;&#36974;&#34109;&#65306;&#31995;&#32479;&#21270;&#19982;&#21452;&#37325;&#36974;&#34109;
&lt;/p&gt;
&lt;p&gt;
How to Mask in Error Correction Code Transformer: Systematic and Double Masking. (arXiv:2308.08128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08128
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#31995;&#32479;&#21270;&#32534;&#30721;&#21644;&#21452;&#37325;&#36974;&#34109;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#20449;&#21644;&#23384;&#20648;&#31995;&#32479;&#20013;&#65292;&#32416;&#38169;&#30721;&#65288;ECC&#65289;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#24191;&#27867;&#25193;&#23637;&#65292;&#31070;&#32463;&#32593;&#32476;&#35299;&#30721;&#22120;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#65292;&#36229;&#36234;&#20256;&#32479;&#35299;&#30721;&#31639;&#27861;&#12290;&#22312;&#36825;&#20123;&#31070;&#32463;&#35299;&#30721;&#22120;&#20013;&#65292;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#65288;ECCT&#65289;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22823;&#24133;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;ECCT&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;ECC&#30340;&#31995;&#32479;&#32534;&#30721;&#25216;&#26415;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36974;&#34109;&#30697;&#38453;&#26469;&#25913;&#21892;ECCT&#30340;&#24615;&#33021;&#24182;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ECCT&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#31216;&#20026;&#21452;&#37325;&#36974;&#34109;&#30340;ECCT&#12290;&#35813;&#26550;&#26500;&#20197;&#24182;&#34892;&#26041;&#24335;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#36974;&#34109;&#30697;&#38453;&#65292;&#20197;&#23398;&#20064;&#36974;&#34109;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#32534;&#30721;&#23383;&#20301;&#20043;&#38388;&#26356;&#22810;&#26679;&#30340;&#29305;&#24449;&#20851;&#31995;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In communication and storage systems, error correction codes (ECCs) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer (ECCT) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of ECCT, we propose two novel methods. First, leveraging the systematic encoding technique of ECCs, we introduce a new masking matrix for ECCT, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of ECCT called a double-masked ECCT. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniZoomer&#65292;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;M&#246;bius&#21464;&#25442;&#25972;&#21512;&#21040;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#22312;&#20840;&#21521;&#22270;&#20687;&#19978;&#36827;&#34892;&#31227;&#21160;&#21644;&#32553;&#25918;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#21464;&#25442;&#29305;&#24449;&#22270;&#65292;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#22686;&#21152;&#30340;&#36793;&#32536;&#26354;&#29575;&#24182;&#20943;&#36731;&#27169;&#31946;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#28151;&#21472;&#38382;&#39064;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.08114</link><description>&lt;p&gt;
OmniZoomer: &#23398;&#20064;&#22312;&#39640;&#20998;&#36776;&#29575;&#29699;&#38754;&#19978;&#31227;&#21160;&#21644;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution. (arXiv:2308.08114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniZoomer&#65292;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;M&#246;bius&#21464;&#25442;&#25972;&#21512;&#21040;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#22312;&#20840;&#21521;&#22270;&#20687;&#19978;&#36827;&#34892;&#31227;&#21160;&#21644;&#32553;&#25918;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#21464;&#25442;&#29305;&#24449;&#22270;&#65292;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#22686;&#21152;&#30340;&#36793;&#32536;&#26354;&#29575;&#24182;&#20943;&#36731;&#27169;&#31946;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#28151;&#21472;&#38382;&#39064;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#21521;&#22270;&#20687;&#65288;ODI&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22823;&#35270;&#37326;&#21487;&#20197;&#35753;&#35266;&#20247;&#22312;&#34394;&#25311;&#29616;&#23454;&#31561;&#27785;&#28024;&#24335;&#29615;&#22659;&#20013;&#33258;&#30001;&#36873;&#25321;&#35270;&#35282;&#12290;&#36890;&#24120;&#20351;&#29992;M&#246;bius&#21464;&#25442;&#22312;ODI&#19978;&#25552;&#20379;&#31227;&#21160;&#21644;&#32553;&#25918;&#30340;&#26426;&#20250;&#65292;&#20294;&#23558;&#20854;&#24212;&#29992;&#22312;&#22270;&#20687;&#32423;&#21035;&#19978;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#31946;&#25928;&#26524;&#21644;&#28151;&#21472;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;OmniZoomer&#65292;&#23558;M&#246;bius&#21464;&#25442;&#25972;&#21512;&#21040;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#22312;ODI&#19978;&#36827;&#34892;&#31227;&#21160;&#21644;&#32553;&#25918;&#12290;&#36890;&#36807;&#23398;&#20064;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#21508;&#31181;&#21464;&#25442;&#29305;&#24449;&#22270;&#65292;&#22686;&#24378;&#20102;&#32593;&#32476;&#22788;&#29702;&#22686;&#21152;&#30340;&#36793;&#32536;&#26354;&#29575;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#27169;&#31946;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#28151;&#21472;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#24357;&#34917;&#25551;&#36848;&#26354;&#32447;&#25152;&#38656;&#20687;&#32032;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#22270;&#65292;&#24182;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
Omnidirectional images (ODIs) have become increasingly popular, as their large field-of-view (FoV) can offer viewers the chance to freely choose the view directions in immersive environments such as virtual reality. The M\"obius transformation is typically employed to further provide the opportunity for movement and zoom on ODIs, but applying it to the image level often results in blurry effect and aliasing problem. In this paper, we propose a novel deep learning-based approach, called \textbf{OmniZoomer}, to incorporate the M\"obius transformation into the network for movement and zoom on ODIs. By learning various transformed feature maps under different conditions, the network is enhanced to handle the increasing edge curvatures, which alleviates the blurry effect. Moreover, to address the aliasing problem, we propose two key components. Firstly, to compensate for the lack of pixels for describing curves, we enhance the feature maps in the high-resolution (HR) space and calculate the
&lt;/p&gt;</description></item><item><title>S-Mixup&#26159;&#19968;&#31181;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#32467;&#26500;&#20449;&#24687;&#30340;&#26032;&#22411;Mixup&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#21644;&#36793;&#26799;&#24230;&#26469;&#26500;&#24314;Mixup&#27744;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.08097</link><description>&lt;p&gt;
S-Mixup: &#32467;&#26500;Mixup&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
S-Mixup: Structural Mixup for Graph Neural Networks. (arXiv:2308.08097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08097
&lt;/p&gt;
&lt;p&gt;
S-Mixup&#26159;&#19968;&#31181;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#32467;&#26500;&#20449;&#24687;&#30340;&#26032;&#22411;Mixup&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#21644;&#36793;&#26799;&#24230;&#26469;&#26500;&#24314;Mixup&#27744;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24212;&#29992;Mixup&#25216;&#26415;&#20110;&#22270;&#20013;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#32780;&#33410;&#28857;&#20998;&#31867;&#30340;&#30740;&#31350;&#20173;&#26410;&#28145;&#20837;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33410;&#28857;&#20998;&#31867;Mixup&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;&#32467;&#26500;Mixup&#65288;S-Mixup&#65289;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#28151;&#21512;&#33410;&#28857;&#26102;&#32771;&#34385;&#32467;&#26500;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;S-Mixup&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20998;&#31867;&#22120;&#33719;&#24471;&#22270;&#20013;&#26410;&#26631;&#35760;&#33410;&#28857;&#30340;&#20266;&#26631;&#31614;&#21644;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;&#36825;&#20123;&#20266;&#26631;&#31614;&#21644;&#32622;&#20449;&#24230;&#20316;&#20026;&#26500;&#25104;&#36328;&#31867;&#21644;&#20869;&#31867;Mixup&#30340;Mixup&#27744;&#30340;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;GNN&#35757;&#32451;&#20013;&#33719;&#24471;&#30340;&#36793;&#26799;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36793;&#36873;&#25321;&#31574;&#30053;&#65292;&#29992;&#20110;&#36873;&#25321;&#36830;&#25509;&#21040;Mixup&#29983;&#25104;&#30340;&#33410;&#28857;&#30340;&#36793;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;S-Mixup&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;S-Mixup&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing studies for applying the mixup technique on graphs mainly focus on graph classification tasks, while the research in node classification is still under-explored. In this paper, we propose a novel mixup augmentation for node classification called Structural Mixup (S-Mixup). The core idea is to take into account the structural information while mixing nodes. Specifically, S-Mixup obtains pseudo-labels for unlabeled nodes in a graph along with their prediction confidence via a Graph Neural Network (GNN) classifier. These serve as the criteria for the composition of the mixup pool for both inter and intra-class mixups. Furthermore, we utilize the edge gradient obtained from the GNN training and propose a gradient-based edge selection strategy for selecting edges to be attached to the nodes generated by the mixup. Through extensive experiments on real-world benchmark datasets, we demonstrate the effectiveness of S-Mixup evaluated on the node classification task. We observe that S-M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGREC&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#65292;&#20854;&#20013;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#20844;&#24320;&#20182;&#20204;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22270;&#26500;&#24314;&#12289;&#23616;&#37096;&#26799;&#24230;&#35745;&#31639;&#21644;&#20840;&#23616;&#26799;&#24230;&#20256;&#36882;&#19977;&#20010;&#38454;&#27573;&#23454;&#29616;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21517;&#20026;&#23433;&#20840;&#26799;&#24230;&#20849;&#20139;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#20445;&#25252;&#29992;&#25143;&#30340;&#31169;&#23494;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08072</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Decentralized Graph Neural Network for Privacy-Preserving Recommendation. (arXiv:2308.08072v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGREC&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#65292;&#20854;&#20013;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#20844;&#24320;&#20182;&#20204;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22270;&#26500;&#24314;&#12289;&#23616;&#37096;&#26799;&#24230;&#35745;&#31639;&#21644;&#20840;&#23616;&#26799;&#24230;&#20256;&#36882;&#19977;&#20010;&#38454;&#27573;&#23454;&#29616;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21517;&#20026;&#23433;&#20840;&#26799;&#24230;&#20849;&#20139;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#20445;&#25252;&#29992;&#25143;&#30340;&#31169;&#23494;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#36829;&#21453;&#29992;&#25143;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#32852;&#37030;GNN&#21644;&#21435;&#20013;&#24515;&#21270;GNN&#20004;&#31181;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#38382;&#39064;&#65292;&#22914;&#36890;&#20449;&#25928;&#29575;&#20302;&#21644;&#38544;&#31169;&#27844;&#38706;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#20013;&#24515;&#21270;GNN&#26694;&#26550;&#65292;&#21517;&#20026;DGREC&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#20844;&#24320;&#20182;&#20204;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65292;&#21363;&#22270;&#26500;&#24314;&#12289;&#23616;&#37096;&#26799;&#24230;&#35745;&#31639;&#21644;&#20840;&#23616;&#26799;&#24230;&#20256;&#36882;&#12290;&#31532;&#19968;&#38454;&#27573;&#20026;&#27599;&#20010;&#29992;&#25143;&#26500;&#24314;&#20102;&#19968;&#20010;&#26412;&#22320;&#20869;&#37096;&#29289;&#21697;&#36229;&#22270;&#21644;&#19968;&#20010;&#20840;&#23616;&#29992;&#25143;&#38388;&#22270;&#12290;&#31532;&#20108;&#38454;&#27573;&#23545;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#22312;&#27599;&#20010;&#26412;&#22320;&#35774;&#22791;&#19978;&#35745;&#31639;&#26799;&#24230;&#12290;&#31532;&#19977;&#38454;&#27573;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;&#26799;&#24230;&#20849;&#20139;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#31169;&#23494;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#36143;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a graph neural network (GNN)-based recommender system without violating user privacy proves challenging. Existing methods can be divided into federated GNNs and decentralized GNNs. But both methods have undesirable effects, i.e., low communication efficiency and privacy leakage. This paper proposes DGREC, a novel decentralized GNN for privacy-preserving recommendations, where users can choose to publicize their interactions. It includes three stages, i.e., graph construction, local gradient calculation, and global gradient passing. The first stage builds a local inner-item hypergraph for each user and a global inter-user graph. The second stage models user preference and calculates gradients on each local device. The third stage designs a local differential privacy mechanism named secure gradient-sharing, which proves strong privacy-preserving of users' private data. We conduct extensive experiments on three public datasets to validate the consistent superiority of our framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26032;&#40092;&#24230;&#21644;&#26631;&#31614;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.08071</link><description>&lt;p&gt;
&#40092;&#24230;&#25110;&#20934;&#30830;&#24615;&#65292;&#20026;&#20160;&#20040;&#19981;&#33021;&#20004;&#32773;&#20860;&#24471;&#65311;&#36890;&#36807;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks. (arXiv:2308.08071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26032;&#40092;&#24230;&#21644;&#26631;&#31614;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#21830;&#19994;&#31995;&#32479;&#20013;&#65292;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;&#26159;&#39044;&#27979;&#36716;&#21270;&#29575;&#38754;&#20020;&#30340;&#26368;&#32039;&#36843;&#25361;&#25112;&#20043;&#19968;&#65292;&#22240;&#20026;&#29992;&#25143;&#30340;&#36716;&#21270;&#24635;&#26159;&#24310;&#36831;&#21457;&#29983;&#12290;&#34429;&#28982;&#26032;&#25968;&#25454;&#26377;&#30410;&#20110;&#25345;&#32493;&#35757;&#32451;&#65292;&#20294;&#26159;&#27809;&#26377;&#23436;&#25972;&#30340;&#21453;&#39304;&#20449;&#24687;&#65292;&#21363;&#36716;&#21270;&#26631;&#31614;&#65292;&#35757;&#32451;&#31639;&#27861;&#21487;&#33021;&#20250;&#36973;&#21463;&#22823;&#37327;&#30340;&#20551;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25110;&#35774;&#35745;&#25968;&#25454;&#27969;&#31243;&#26469;&#35299;&#20915;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25968;&#25454;&#26032;&#40092;&#24230;&#21644;&#26631;&#31614;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#25240;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24310;&#36831;&#21453;&#39304;&#24314;&#27169; &#65288;DGDFEM&#65289;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65292;&#21363;&#20934;&#22791;&#25968;&#25454;&#27969;&#31243;&#12289;&#26500;&#24314;&#21160;&#24577;&#22270;&#20197;&#21450;&#35757;&#32451;CVR&#39044;&#27979;&#27169;&#22411;&#12290;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HLGCN&#30340;&#26032;&#39062;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39640;&#36890;&#21644;&#20302;&#36890;&#28388;&#27874;&#22120;&#26469;&#22788;&#29702;&#36716;&#21270;&#21644;&#38750;&#36716;&#21270;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#25454;&#26032;&#40092;&#24230;&#21644;&#26631;&#31614;&#20934;&#30830;&#24615;&#30340;&#21452;&#37325;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The delayed feedback problem is one of the most pressing challenges in predicting the conversion rate since users' conversions are always delayed in online commercial systems. Although new data are beneficial for continuous training, without complete feedback information, i.e., conversion labels, training algorithms may suffer from overwhelming fake negatives. Existing methods tend to use multitask learning or design data pipelines to solve the delayed feedback problem. However, these methods have a trade-off between data freshness and label accuracy. In this paper, we propose Delayed Feedback Modeling by Dynamic Graph Neural Network (DGDFEM). It includes three stages, i.e., preparing a data pipeline, building a dynamic graph, and training a CVR prediction model. In the model training, we propose a novel graph convolutional method named HLGCN, which leverages both high-pass and low-pass filters to deal with conversion and non-conversion relationships. The proposed method achieves both 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#19988;&#25928;&#26524;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#23384;&#22312;&#19968;&#20010;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08055</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#36827;&#34892;&#31616;&#21333;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple online learning with consistency oracle. (arXiv:2308.08055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#19988;&#25928;&#26524;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#23384;&#22312;&#19968;&#20010;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#8212;&#8212;&#22312;&#20219;&#20309;&#26102;&#21051;&#65292;&#39044;&#35328;&#26426;&#37117;&#33021;&#32473;&#20986;&#19982;&#30446;&#21069;&#20026;&#27490;&#30475;&#21040;&#30340;&#25152;&#26377;&#31034;&#20363;&#19968;&#33268;&#30340;&#31867;&#20989;&#25968;&#12290;&#35813;&#27169;&#22411;&#26368;&#36817;&#30001;Assos&#31561;&#20154;&#65288;COLT'23&#65289;&#32771;&#34385;&#12290;&#36825;&#20010;&#27169;&#22411;&#30340;&#21160;&#26426;&#26159;&#26631;&#20934;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#23376;&#31867;&#30340;Littlestone&#32500;&#24230;&#65292;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;Assos&#31561;&#20154;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#32473;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23545;&#20110;Littlestone&#32500;&#24230;&#20026;d&#30340;&#31867;&#65292;&#26368;&#22810;&#20250;&#29359;C^d&#20010;&#38169;&#35823;&#65292;&#20854;&#20013;C&#26159;&#19968;&#20010;&#26410;&#25351;&#23450;&#30340;&#32477;&#23545;&#24120;&#25968;&#19988;&#22823;&#20110;0&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#65292;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#26356;&#31616;&#21333;&#65292;&#21482;&#20351;&#29992;&#20102;Littlestone&#32500;&#24230;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#19981;&#23384;&#22312;&#19968;&#20010;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#65288;&#20197;&#21450;Assos&#31561;&#20154;&#30340;&#31639;&#27861;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online learning in the model where a learning algorithm can access the class only via the consistency oracle -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al. (COLT'23). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a problem that is computationally intractable. Assos et al. gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C &gt; 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also observe that there exists no algorithm in this model that makes at most $2^{d+1}-2$ mistakes. We also observe that our algorithm (as well as the algorithm of Assos et al.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38134;&#34892;&#36151;&#27454;&#38382;&#39064;&#20013;&#35757;&#32451;&#38598;&#20559;&#24046;&#38382;&#39064;&#65292;&#26088;&#22312;&#23398;&#20064;&#26080;&#20559;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#36807;&#21435;&#25968;&#25454;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.08051</link><description>&lt;p&gt;
&#20943;&#23569;&#21518;&#24724;&#30340;&#26080;&#20559;&#20915;&#31574;&#65306;&#38754;&#21521;&#38134;&#34892;&#36151;&#27454;&#38382;&#39064;&#30340;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem. (arXiv:2308.08051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08051
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38134;&#34892;&#36151;&#27454;&#38382;&#39064;&#20013;&#35757;&#32451;&#38598;&#20559;&#24046;&#38382;&#39064;&#65292;&#26088;&#22312;&#23398;&#20064;&#26080;&#20559;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#36807;&#21435;&#25968;&#25454;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#26223;&#20013;&#65292;&#22522;&#20110;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#20915;&#31574;&#26159;&#22522;&#20110;&#36817;&#23454;&#26102;&#30340;&#65292;&#20363;&#22914;&#22312;&#23457;&#25209;&#36151;&#27454;&#30003;&#35831;&#26102;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31867;&#20855;&#26377;&#20849;&#21516;&#29305;&#24449;&#30340;&#38382;&#39064;&#65306;&#21482;&#26377;&#22312;&#25968;&#25454;&#28857;&#34987;&#25351;&#27966;&#20026;&#27491;&#26631;&#31614;&#26102;&#25165;&#33021;&#35266;&#23519;&#21040;&#30495;&#23454;&#26631;&#31614;&#65292;&#20363;&#22914;&#65292;&#21482;&#26377;&#22312;&#25105;&#20204;&#25509;&#21463;&#36151;&#27454;&#30003;&#35831;&#20043;&#21518;&#25165;&#33021;&#21457;&#29616;&#30003;&#35831;&#20154;&#26159;&#21542;&#36829;&#32422;&#12290;&#22240;&#27492;&#65292;&#38169;&#35823;&#25298;&#32477;&#20250;&#21464;&#24471;&#33258;&#25105;&#24378;&#21270;&#65292;&#24182;&#23548;&#33268;&#30001;&#27169;&#22411;&#20915;&#31574;&#19981;&#26029;&#26356;&#26032;&#30340;&#26631;&#35760;&#35757;&#32451;&#38598;&#32047;&#31215;&#20559;&#24046;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#20048;&#35266;&#20027;&#20041;&#27880;&#20837;&#27169;&#22411;&#26469;&#20943;&#36731;&#36825;&#31181;&#25928;&#24212;&#65292;&#20294;&#36825;&#20250;&#22686;&#21152;&#38169;&#35823;&#25509;&#21463;&#29575;&#30340;&#20195;&#20215;&#12290;&#25105;&#20204;&#24341;&#20837;&#23545;&#25239;&#20048;&#35266;&#20027;&#20041;&#65288;AdOpt&#65289;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#39046;&#22495;&#36866;&#24212;&#30452;&#25509;&#35299;&#20915;&#35757;&#32451;&#38598;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;AdOpt&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20943;&#23569;&#34987;&#25509;&#21463;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#20559;&#31227;&#26469;&#23398;&#20064;&#36807;&#21435;&#25968;&#25454;&#30340;&#26080;&#20559;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real world settings binary classification decisions are made based on limited data in near real-time, e.g. when assessing a loan application. We focus on a class of these problems that share a common feature: the true label is only observed when a data point is assigned a positive label by the principal, e.g. we only find out whether an applicant defaults if we accepted their loan application. As a consequence, the false rejections become self-reinforcing and cause the labelled training set, that is being continuously updated by the model decisions, to accumulate bias. Prior work mitigates this effect by injecting optimism into the model, however this comes at the cost of increased false acceptance rate. We introduce adversarial optimism (AdOpt) to directly address bias in the training set using adversarial domain adaptation. The goal of AdOpt is to learn an unbiased but informative representation of past data, by reducing the distributional shift between the set of accepted da
&lt;/p&gt;</description></item><item><title>DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08043</link><description>&lt;p&gt;
DiagGPT:&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08043
&lt;/p&gt;
&lt;p&gt;
DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;AI&#27169;&#22411;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36741;&#21161;&#20154;&#31867;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;AI&#20316;&#20026;&#32842;&#22825;&#20195;&#29702;&#20154;&#30340;&#37325;&#35201;&#24212;&#29992;&#26159;&#22238;&#31572;&#20154;&#31867;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;LLMs&#22312;&#22238;&#31572;&#19968;&#33324;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#29087;&#32451;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#35786;&#26029;&#22330;&#26223;(&#22914;&#27861;&#24459;&#25110;&#21307;&#30103;&#21672;&#35810;)&#20013;&#65292;&#22522;&#26412;&#30340;&#38382;&#31572;&#23545;&#35805;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;(TOD)&#65292;&#20854;&#20013;AI&#32842;&#22825;&#20195;&#29702;&#38656;&#35201;&#20027;&#21160;&#25552;&#38382;&#24182;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;TOD&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24403;&#21069;&#30340;LLMs&#24182;&#26410;&#22266;&#26377;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiagGPT (Diagnosis GPT)&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLMs&#25512;&#24191;&#21040;TOD&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#21033;&#29992;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#21644;&#21487;&#29992;&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#32534;&#35793;&#12289;&#26131;&#35835;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.08033</link><description>&lt;p&gt;
&#20351;&#29992;&#20195;&#30721;&#27169;&#22411;&#21644;&#39046;&#22495;&#36866;&#24212;&#24615;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automated Test Case Generation Using Code Models and Domain Adaptation. (arXiv:2308.08033v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#21033;&#29992;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#21644;&#21487;&#29992;&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#32534;&#35793;&#12289;&#26131;&#35835;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29983;&#25104;&#25216;&#26415;&#65292;&#20363;&#22914;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#65292;&#36890;&#24120;&#23545;&#24320;&#21457;&#20154;&#21592;&#21019;&#24314;&#30340;&#27979;&#35797;&#29992;&#20363;&#19968;&#26080;&#25152;&#30693;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#36890;&#24120;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#19981;&#26131;&#38405;&#35835;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#26816;&#27979;&#25152;&#26377;&#22797;&#26434;&#32570;&#38519;&#65292;&#32780;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#29992;&#20363;&#21017;&#21487;&#20197;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#20197;&#34917;&#20805;&#22522;&#20110;&#25628;&#32034;&#27979;&#35797;&#29983;&#25104;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;CodeT5&#65292;&#21363;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#65292;&#24182;&#23545;&#27979;&#35797;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;Methods2test&#25968;&#25454;&#38598;&#23545;CodeT5&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;Defects4j&#36827;&#34892;&#39033;&#30446;&#32423;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#21033;&#29992;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#21644;&#21487;&#29992;&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#32534;&#35793;&#12289;&#26131;&#35835;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26032;&#30340;&#27979;&#35797;&#29992;&#20363;&#65292;&#35206;&#30422;&#20102;&#24050;&#32463;&#34987;&#27979;&#35797;&#36807;&#30340;&#20195;&#30721;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art automated test generation techniques, such as search-based testing, are usually ignorant about what a developer would create as a test case. Therefore, they typically create tests that are not human-readable and may not necessarily detect all types of complex bugs developer-written tests would do. In this study, we leverage Transformer-based code models to generate unit tests that can complement search-based test generation. Specifically, we use CodeT5, i.e., a state-of-the-art large code model, and fine-tune it on the test generation downstream task. For our analysis, we use the Methods2test dataset for fine-tuning CodeT5 and Defects4j for project-level domain adaptation and evaluation. The main contribution of this study is proposing a fully automated testing framework that leverages developer-written tests and available code models to generate compilable, human-readable unit tests. Results show that our approach can generate new test cases that cover lines that were
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;SI SL&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#35268;&#21010;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#19982;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#30340;&#27604;&#36739;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08029</link><description>&lt;p&gt;
&#35268;&#21010;&#23398;&#20064;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning. (arXiv:2308.08029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;SI SL&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#35268;&#21010;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#19982;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#30340;&#27604;&#36739;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25512;&#29702;&#26159;&#19968;&#31181;&#36817;&#26399;&#30340;&#23545;&#19981;&#30830;&#23450;&#24615;&#24773;&#22659;&#19979;&#35268;&#21010;&#24314;&#27169;&#30340;&#26694;&#26550;&#12290;&#29616;&#22312;&#20154;&#20204;&#24050;&#32463;&#24320;&#22987;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#23427;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#25299;&#23637;-&#22797;&#26434;&#27169;&#22411;&#20248;&#21270;&#31639;&#27861;&#36890;&#36807;&#36882;&#24402;&#20915;&#31574;&#26641;&#25628;&#32034;&#22312;&#22810;&#27493;&#35268;&#21010;&#38382;&#39064;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#24456;&#23569;&#26377;&#24037;&#20316;&#23545;&#27604;SI&#19982;&#20854;&#20182;&#24050;&#24314;&#31435;&#30340;&#35268;&#21010;&#31639;&#27861;&#12290;SI&#31639;&#27861;&#20063;&#20027;&#35201;&#20851;&#27880;&#25512;&#29702;&#32780;&#19981;&#26159;&#23398;&#20064;&#12290;&#26412;&#25991;&#26377;&#20004;&#20010;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27604;&#36739;SI&#19982;&#26088;&#22312;&#35299;&#20915;&#30456;&#20284;&#38382;&#39064;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SI&#22797;&#26434;&#23398;&#20064;&#65288;SL&#65289;&#30340;&#25299;&#23637;&#65292;&#35813;&#25299;&#23637;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#26356;&#21152;&#20805;&#20998;&#22320;&#24341;&#20837;&#20102;&#20027;&#21160;&#23398;&#20064;&#12290;SL&#32500;&#25345;&#23545;&#26410;&#26469;&#35266;&#27979;&#19979;&#27599;&#20010;&#31574;&#30053;&#19979;&#27169;&#22411;&#21442;&#25968;&#22914;&#20309;&#21464;&#21270;&#30340;&#20449;&#24565;&#12290;&#36825;&#20801;&#35768;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#30340;&#22238;&#39038;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Inference is a recent framework for modeling planning under uncertainty. Empirical and theoretical work have now begun to evaluate the strengths and weaknesses of this approach and how it might be improved. A recent extension - the sophisticated inference (SI) algorithm - improves performance on multi-step planning problems through recursive decision tree search. However, little work to date has been done to compare SI to other established planning algorithms. SI was also developed with a focus on inference as opposed to learning. The present paper has two aims. First, we compare performance of SI to Bayesian reinforcement learning (RL) schemes designed to solve similar problems. Second, we present an extension of SI sophisticated learning (SL) - that more fully incorporates active learning during planning. SL maintains beliefs about how model parameters would change under the future observations expected under each policy. This allows a form of counterfactual retrospective in
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#35745;&#31639;&#22312;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#30408;&#21033;&#21644;&#33021;&#28304;&#25928;&#29575;&#19978;&#36229;&#36234;&#32463;&#20856;&#35745;&#31639;&#12290;&#36825;&#20351;&#24471;&#37327;&#23376;&#35745;&#31639;&#25104;&#20026;&#35745;&#31639;&#34892;&#19994;&#26356;&#21487;&#25345;&#32493;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.08025</link><description>&lt;p&gt;
&#37327;&#23376;&#32463;&#27982;&#30340;&#21183;&#33021;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Potential Energy Advantage of Quantum Economy. (arXiv:2308.08025v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08025
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#22312;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#30408;&#21033;&#21644;&#33021;&#28304;&#25928;&#29575;&#19978;&#36229;&#36234;&#32463;&#20856;&#35745;&#31639;&#12290;&#36825;&#20351;&#24471;&#37327;&#23376;&#35745;&#31639;&#25104;&#20026;&#35745;&#31639;&#34892;&#19994;&#26356;&#21487;&#25345;&#32493;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#33021;&#28304;&#25104;&#26412;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#23545;&#20110;&#25552;&#20379;&#35745;&#31639;&#26381;&#21153;&#30340;&#20844;&#21496;&#26469;&#35828;&#65292;&#20302;&#33021;&#32791;&#23545;&#20110;&#24066;&#22330;&#22686;&#38271;&#21644;&#25919;&#24220;&#27861;&#35268;&#26469;&#35828;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#35745;&#31639;&#19982;&#32463;&#20856;&#35745;&#31639;&#20043;&#38388;&#30340;&#33021;&#28304;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#33021;&#28304;&#25928;&#29575;&#30340;&#32972;&#26223;&#19979;&#37325;&#26032;&#23450;&#20041;&#20248;&#21183;&#65292;&#19982;&#20165;&#22522;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#20256;&#32479;&#37327;&#23376;&#20248;&#21183;&#19981;&#21516;&#12290;&#36890;&#36807;&#19968;&#20010;&#20197;&#33021;&#37327;&#20351;&#29992;&#20026;&#32422;&#26463;&#26465;&#20214;&#30340;Cournot&#31454;&#20105;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#37327;&#23376;&#35745;&#31639;&#20844;&#21496;&#22312;Nash&#22343;&#34913;&#28857;&#19978;&#22312;&#30408;&#21033;&#33021;&#21147;&#21644;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#37117;&#33021;&#36229;&#36234;&#32463;&#20856;&#23545;&#25163;&#12290;&#22240;&#27492;&#65292;&#37327;&#23376;&#35745;&#31639;&#21487;&#33021;&#20195;&#34920;&#35745;&#31639;&#34892;&#19994;&#26356;&#21487;&#25345;&#32493;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#35745;&#31639;&#32463;&#27982;&#30340;&#33021;&#28304;&#21033;&#30410;&#21462;&#20915;&#20110;&#22823;&#35268;&#27169;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy cost is increasingly crucial in the modern computing industry with the wide deployment of large-scale machine learning models and language models. For the firms that provide computing services, low energy consumption is important both from the perspective of their own market growth and the government's regulations. In this paper, we study the energy benefits of quantum computing vis-a-vis classical computing. Deviating from the conventional notion of quantum advantage based solely on computational complexity, we redefine advantage in an energy efficiency context. Through a Cournot competition model constrained by energy usage, we demonstrate quantum computing firms can outperform classical counterparts in both profitability and energy efficiency at Nash equilibrium. Therefore quantum computing may represent a more sustainable pathway for the computing industry. Moreover, we discover that the energy benefits of quantum computing economies are contingent on large-scale computation
&lt;/p&gt;</description></item><item><title>GRINN&#26159;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#23548;&#21521;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#33258;&#37325;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#27714;&#35299;&#19977;&#32500;&#27969;&#20307;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#23427;&#22312;&#27169;&#25311;&#37325;&#21147;&#19981;&#31283;&#23450;&#24615;&#21644;&#27874;&#20256;&#25773;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08010</link><description>&lt;p&gt;
GRINN:&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#23548;&#21521;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#33258;&#37325;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#27714;&#35299;&#27969;&#20307;&#21160;&#21147;&#23398;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity. (arXiv:2308.08010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08010
&lt;/p&gt;
&lt;p&gt;
GRINN&#26159;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#23548;&#21521;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#33258;&#37325;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#27714;&#35299;&#19977;&#32500;&#27969;&#20307;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#23427;&#22312;&#27169;&#25311;&#37325;&#21147;&#19981;&#31283;&#23450;&#24615;&#21644;&#27874;&#20256;&#25773;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#33258;&#37325;&#27668;&#20307;&#27969;&#21160;&#23545;&#20110;&#22238;&#31572;&#22825;&#20307;&#29289;&#29702;&#23398;&#20013;&#30340;&#35768;&#22810;&#22522;&#26412;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#28041;&#21450;&#20247;&#22810;&#35805;&#39064;&#65292;&#21253;&#25324;&#34892;&#26143;&#24418;&#25104;&#30424;&#12289;&#26143;&#20113;&#24418;&#25104;&#12289;&#26143;&#31995;&#24418;&#25104;&#20197;&#21450;&#23431;&#23449;&#20013;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#37325;&#21147;&#19982;&#27969;&#20307;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#23545;&#27714;&#35299;&#25152;&#24471;&#21040;&#30340;&#19977;&#32500;&#26102;&#21464;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26080;&#32593;&#26684;&#26694;&#26550;&#20013;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#65292;&#29289;&#29702;&#20449;&#24687;&#23548;&#21521;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;PINN&#30340;&#30721;&#65292;&#21517;&#20026;GRINN&#65292;&#29992;&#20110;&#27169;&#25311;&#19977;&#32500;&#33258;&#37325;&#27969;&#20307;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#19968;&#20010;&#31561;&#28201;&#27668;&#20307;&#20013;&#30340;&#37325;&#21147;&#19981;&#31283;&#23450;&#24615;&#21644;&#27874;&#20256;&#25773;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#32447;&#24615;&#21306;&#22495;&#20869;&#19982;&#32447;&#24615;&#35299;&#30456;&#21305;&#37197;&#65292;&#35823;&#24046;&#22312;1%&#20197;&#20869;&#65292;&#19982;&#20256;&#32479;&#32593;&#26684;&#30721;&#27714;&#35299;&#30340;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling self-gravitating gas flows is essential to answering many fundamental questions in astrophysics. This spans many topics including planet-forming disks, star-forming clouds, galaxy formation, and the development of large-scale structures in the Universe. However, the nonlinear interaction between gravity and fluid dynamics offers a formidable challenge to solving the resulting time-dependent partial differential equations (PDEs) in three dimensions (3D). By leveraging the universal approximation capabilities of a neural network within a mesh-free framework, physics informed neural networks (PINNs) offer a new way of addressing this challenge. We introduce the gravity-informed neural network (GRINN), a PINN-based code, to simulate 3D self-gravitating hydrodynamic systems. Here, we specifically study gravitational instability and wave propagation in an isothermal gas. Our results match a linear analytic solution to within 1\% in the linear regime and a conventional grid code solu
&lt;/p&gt;</description></item><item><title>APACE&#26159;&#19968;&#20010;&#23558;AlphaFold2&#21644;&#20808;&#36827;&#35745;&#31639;&#20316;&#20026;&#26381;&#21153;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#20195;&#36229;&#32423;&#35745;&#31639;&#29615;&#22659;&#20013;&#21152;&#36895;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#20998;&#26512;&#12290;&#30740;&#31350;&#32773;&#22312;Delta&#36229;&#32423;&#35745;&#31639;&#26426;&#20013;&#37096;&#32626;&#20102;APACE&#65292;&#22312;&#20934;&#30830;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.07954</link><description>&lt;p&gt;
APACE: AlphaFold2&#21644;&#20808;&#36827;&#35745;&#31639;&#20316;&#20026;&#21152;&#36895;&#29983;&#29289;&#29289;&#29702;&#23398;&#21457;&#29616;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
APACE: AlphaFold2 and advanced computing as a service for accelerated discovery in biophysics. (arXiv:2308.07954v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07954
&lt;/p&gt;
&lt;p&gt;
APACE&#26159;&#19968;&#20010;&#23558;AlphaFold2&#21644;&#20808;&#36827;&#35745;&#31639;&#20316;&#20026;&#26381;&#21153;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#20195;&#36229;&#32423;&#35745;&#31639;&#29615;&#22659;&#20013;&#21152;&#36895;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#20998;&#26512;&#12290;&#30740;&#31350;&#32773;&#22312;Delta&#36229;&#32423;&#35745;&#31639;&#26426;&#20013;&#37096;&#32626;&#20102;APACE&#65292;&#22312;&#20934;&#30830;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#27688;&#22522;&#37240;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;3D&#32467;&#26500;&#26159;&#29983;&#29289;&#29289;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#35745;&#31639;&#37325;&#22823;&#25361;&#25112;&#65292;&#23427;&#22312;&#31283;&#20581;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#33647;&#29289;&#21457;&#29616;&#21040;&#22522;&#22240;&#32452;&#35299;&#35835;&#37117;&#31163;&#19981;&#24320;&#36825;&#20010;&#25216;&#26415;&#12290;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#27604;&#22914;AlphaFold&#65292;&#27491;&#22312;&#38761;&#26032;&#20381;&#36182;&#31283;&#20581;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#36825;&#20123;&#26032;&#22411;AI&#24037;&#20855;&#30340;&#24433;&#21709;&#21147;&#21644;&#26131;&#29992;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;APACE&#65292;AlphaFold2&#21644;&#20808;&#36827;&#35745;&#31639;&#20316;&#20026;&#26381;&#21153;&#30340;&#26032;&#22411;&#35745;&#31639;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20010;AI&#27169;&#22411;&#21450;&#20854;TB&#32423;&#22823;&#23567;&#30340;&#25968;&#25454;&#24211;&#65292;&#20197;&#22312;&#29616;&#20195;&#36229;&#32423;&#35745;&#31639;&#29615;&#22659;&#20013;&#21152;&#36895;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;APACE&#37096;&#32626;&#22312;Delta&#36229;&#32423;&#35745;&#31639;&#26426;&#20013;&#65292;&#24182;&#20351;&#29992;&#22235;&#20010;&#31034;&#20363;&#34507;&#30333;&#36136;&#65288;6AWO&#65292;6OAN&#65292;7MEZ&#21644;6D6U&#65289;&#26469;&#37327;&#21270;&#20854;&#22312;&#20934;&#30830;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#22312;Delta&#30340;50&#20010;&#33410;&#28857;&#19978;&#20998;&#24067;&#20102;&#22810;&#36798;200&#20010;&#21512;&#38598;&#65292;&#30456;&#24403;&#20110;200&#20010;A100 NVIDIA GPU&#65292;&#25105;&#20204;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
The prediction of protein 3D structure from amino acid sequence is a computational grand challenge in biophysics, and plays a key role in robust protein structure prediction algorithms, from drug discovery to genome interpretation. The advent of AI models, such as AlphaFold, is revolutionizing applications that depend on robust protein structure prediction algorithms. To maximize the impact, and ease the usability, of these novel AI tools we introduce APACE, AlphaFold2 and advanced computing as a service, a novel computational framework that effectively handles this AI model and its TB-size database to conduct accelerated protein structure prediction analyses in modern supercomputing environments. We deployed APACE in the Delta supercomputer, and quantified its performance for accurate protein structure predictions using four exemplar proteins: 6AWO, 6OAN, 7MEZ, and 6D6U. Using up to 200 ensembles, distributed across 50 nodes in Delta, equivalent to 200 A100 NVIDIA GPUs, we found that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#21697;&#25644;&#36816;&#20219;&#21153;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31561;&#21464;Transporter Net&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31561;&#21464;&#31070;&#32463;&#27169;&#22411;&#25429;&#25417;&#20102;&#25152;&#26377;&#23545;&#31216;&#24615;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25644;&#36816;&#20301;&#32622;&#25512;&#24191;&#25644;&#36816;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2308.07948</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#31216;&#24615;&#22312;&#29289;&#21697;&#25644;&#36816;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging Symmetries in Pick and Place. (arXiv:2308.07948v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#21697;&#25644;&#36816;&#20219;&#21153;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31561;&#21464;Transporter Net&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31561;&#21464;&#31070;&#32463;&#27169;&#22411;&#25429;&#25417;&#20102;&#25152;&#26377;&#23545;&#31216;&#24615;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25644;&#36816;&#20301;&#32622;&#25512;&#24191;&#25644;&#36816;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#29289;&#21697;&#25644;&#36816;&#20219;&#21153;&#22312;&#29289;&#21697;&#21644;&#30446;&#26631;&#25918;&#32622;&#20301;&#32622;&#30340;&#24179;&#31227;&#21644;&#26059;&#36716;&#19979;&#20855;&#26377;&#23545;&#31216;&#24615;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#29289;&#21697;&#34987;&#26059;&#36716;&#25110;&#24179;&#31227;&#65292;&#26368;&#20339;&#25644;&#36816;&#21160;&#20316;&#20063;&#24212;&#35813;&#26059;&#36716;&#25110;&#24179;&#31227;&#12290;&#23545;&#20110;&#25918;&#32622;&#20301;&#32622;&#20063;&#26159;&#22914;&#27492;&#65292;&#22914;&#26524;&#30446;&#26631;&#25918;&#32622;&#20301;&#32622;&#21457;&#29983;&#21464;&#21270;&#65292;&#25918;&#32622;&#21160;&#20316;&#20063;&#24212;&#30456;&#24212;&#25913;&#21464;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#25644;&#36816;&#26694;&#26550;Transporter Net&#25429;&#25417;&#20102;&#37096;&#20998;&#36825;&#20123;&#23545;&#31216;&#24615;&#65292;&#20294;&#19981;&#23436;&#20840;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#24179;&#38754;&#26426;&#22120;&#20154;&#29289;&#21697;&#25644;&#36816;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31561;&#21464;&#31070;&#32463;&#27169;&#22411;&#25972;&#21512;&#21040;Transporter Net&#20013;&#20197;&#25429;&#25417;&#25152;&#26377;&#23545;&#31216;&#24615;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#26032;&#27169;&#22411;&#34987;&#31216;&#20026;&#31561;&#21464;Transporter Net&#65292;&#23545;&#20110;&#29289;&#21697;&#30340;&#25644;&#36816;&#21644;&#25918;&#32622;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#21487;&#20197;&#31435;&#21363;&#23558;&#25644;&#36816;&#30693;&#35782;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#25644;&#36816;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#26032;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#26174;&#31034;&#23427;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic pick and place tasks are symmetric under translations and rotations of both the object to be picked and the desired place pose. For example, if the pick object is rotated or translated, then the optimal pick action should also rotate or translate. The same is true for the place pose; if the desired place pose changes, then the place action should also transform accordingly. A recently proposed pick and place framework known as Transporter Net captures some of these symmetries, but not all. This paper analytically studies the symmetries present in planar robotic pick and place and proposes a method of incorporating equivariant neural models into Transporter Net in a way that captures all symmetries. The new model, which we call Equivariant Transporter Net, is equivariant to both pick and place symmetries and can immediately generalize pick and place knowledge to different pick and place poses. We evaluate the new model empirically and show that it is much more sample efficient t
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30740;&#31350;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#27809;&#26377;&#25490;&#21517;&#21644;&#21482;&#32771;&#34385;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#26159;&#24433;&#21709;&#22240;&#32032;&#12290;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;NBFNet&#12290;&#36825;&#20123;&#21464;&#20307;&#20165;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.07942</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis. (arXiv:2308.07942v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07942
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30740;&#31350;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#27809;&#26377;&#25490;&#21517;&#21644;&#21482;&#32771;&#34385;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#26159;&#24433;&#21709;&#22240;&#32032;&#12290;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;NBFNet&#12290;&#36825;&#20123;&#21464;&#20307;&#20165;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#20174;&#35757;&#32451;&#22270;&#35889;&#20013;&#23398;&#20064;&#25512;&#29702;&#27169;&#24335;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#26469;&#22312;&#20998;&#31163;&#30340;&#27979;&#35797;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#20284;&#20046;&#24456;&#36866;&#21512;&#36825;&#20010;&#20219;&#21153;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#26126;&#26174;&#19981;&#22914;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22914;NBFNet&#12290;&#25105;&#20204;&#20551;&#35774;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#26159;&#30001;&#20110;&#20004;&#20010;&#22240;&#32032;&#65306;&#65288;i&#65289;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#26681;&#26412;&#27809;&#26377;&#25490;&#21517;&#65292;&#65288;ii&#65289;&#22312;&#30830;&#23450;&#32473;&#23450;&#38142;&#25509;&#39044;&#27979;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#26102;&#65292;&#21482;&#32771;&#34385;&#20102;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#12290;&#20026;&#20102;&#20998;&#26512;&#36825;&#20123;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#38024;&#23545;&#19978;&#36848;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#25509;&#36817;NBFNet&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#32771;&#34385;&#21040;&#30340;&#21464;&#20307;&#21482;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;GPT-2&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#32534;&#30721;&#30340;&#26102;&#31354;&#25968;&#25454;&#65292;&#29983;&#25104;&#21463;&#29615;&#22659;&#22240;&#32032;&#21644;&#20010;&#20307;&#23646;&#24615;&#24433;&#21709;&#30340;&#20010;&#20307;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.07940</link><description>&lt;p&gt;
&#20174;&#22836;&#24320;&#22987;&#23545;&#32534;&#30721;&#30340;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#20351;&#29992;GPT-2&#29983;&#25104;&#20010;&#20307;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data. (arXiv:2308.07940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;GPT-2&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#32534;&#30721;&#30340;&#26102;&#31354;&#25968;&#25454;&#65292;&#29983;&#25104;&#21463;&#29615;&#22659;&#22240;&#32032;&#21644;&#20010;&#20307;&#23646;&#24615;&#24433;&#21709;&#30340;&#20010;&#20307;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27492;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Mizuno&#12289;Fujimoto&#21644;Ishikawa&#30340;&#26041;&#27861;&#23558;&#22320;&#29702;&#22352;&#26631;&#36716;&#25442;&#20026;&#20855;&#26377;&#19981;&#21516;&#31354;&#38388;&#23610;&#24230;&#20301;&#32622;&#29305;&#24449;&#30340;&#29420;&#29305;&#20301;&#32622;&#31526;&#21495;&#12290;&#25105;&#20204;&#20351;&#29992;&#29420;&#29305;&#30340;&#26102;&#38388;&#38388;&#38548;&#31526;&#21495;&#23558;&#20301;&#32622;&#31526;&#21495;&#32452;&#21512;&#25104;&#20010;&#20307;&#27599;&#26085;&#36712;&#36857;&#30340;&#24207;&#21015;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;GPT-2&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#36825;&#20010;&#31526;&#21495;&#24207;&#21015;&#65292;&#20174;&#32780;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#39034;&#24207;&#29983;&#25104;&#20010;&#20307;&#27599;&#26085;&#36712;&#36857;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#31526;&#21495;&#34920;&#31034;&#27668;&#35937;&#26465;&#20214;&#21644;&#20010;&#20307;&#23646;&#24615;&#65292;&#27604;&#22914;&#24615;&#21035;&#21644;&#24180;&#40836;&#65292;&#24182;&#22312;GPT-2&#26550;&#26500;&#19978;&#35757;&#32451;&#36825;&#20123;&#31526;&#21495;&#21644;&#36712;&#36857;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#21516;&#26102;&#21463;&#29615;&#22659;&#22240;&#32032;&#21644;&#20010;&#20307;&#23646;&#24615;&#24433;&#21709;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following Mizuno, Fujimoto, and Ishikawa's research (Front. Phys. 2022), we transpose geographical coordinates expressed in latitude and longitude into distinctive location tokens that embody positions across varied spatial scales. We encapsulate an individual daily trajectory as a sequence of tokens by adding unique time interval tokens to the location tokens. Using the architecture of an autoregressive language model, GPT-2, this sequence of tokens is trained from scratch, allowing us to construct a deep learning model that sequentially generates an individual daily trajectory. Environmental factors such as meteorological conditions and individual attributes such as gender and age are symbolized by unique special tokens, and by training these tokens and trajectories on the GPT-2 architecture, we can generate trajectories that are influenced by both environmental factors and individual attributes.
&lt;/p&gt;</description></item><item><title>Ada-QPacknet&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#30340;&#39640;&#25928;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#37327;&#21270;&#25216;&#26415;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#65292;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07939</link><description>&lt;p&gt;
Ada-QPacknet -- &#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20250;&#36951;&#24536;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting. (arXiv:2308.07939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07939
&lt;/p&gt;
&lt;p&gt;
Ada-QPacknet&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#30340;&#39640;&#25928;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#37327;&#21270;&#25216;&#26415;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#65292;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32487;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#20010;&#36807;&#31243;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#20173;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#12290;&#26368;&#36817;&#35774;&#35745;&#20102;&#35768;&#22810;CL&#31639;&#27861;&#65292;&#22823;&#37096;&#20998;&#37117;&#23384;&#22312;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#26550;&#26500;&#30340;&#26041;&#27861;Ada-QPacknet&#12290;&#23427;&#36890;&#36807;&#21098;&#26525;&#25552;&#21462;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#12290;&#22522;&#20110;&#26550;&#26500;&#30340;CL&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#23481;&#37327;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37327;&#21270;&#26041;&#27861;&#20943;&#23567;&#20102;&#27169;&#22411;&#30340;&#35268;&#27169;&#12290;&#35813;&#26041;&#27861;&#20943;&#23567;&#20102;&#26435;&#37325;&#26684;&#24335;&#30340;&#20301;&#23485;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#28151;&#21512;8&#20301;&#21644;4&#20301;&#37327;&#21270;&#22312;&#33879;&#21517;&#30340;CL&#22330;&#26223;&#19978;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#21098;&#26525;&#21644;&#37327;&#21270;&#36825;&#20004;&#31181;&#21387;&#32553;&#25216;&#26415;&#24212;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#30340;CL&#31574;&#30053;&#12290;&#35813;&#31639;&#27861;&#22312;&#33879;&#21517;&#30340;&#24773;&#33410;&#32452;&#21512;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combination
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;ChatGPT 3.5&#26469;&#36827;&#34892;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#65292;&#29305;&#21035;&#20851;&#27880;&#22806;&#27719;&#24066;&#22330;&#65292;&#36890;&#36807;&#38646;-shot&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;ChatGPT&#22312;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#32422;35&#65285;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.07935</link><description>&lt;p&gt;
&#29992;ChatGPT&#21464;&#38761;&#37329;&#34701;&#39046;&#22495;&#30340;&#24773;&#32490;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Transforming Sentiment Analysis in the Financial Domain with ChatGPT. (arXiv:2308.07935v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;ChatGPT 3.5&#26469;&#36827;&#34892;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#65292;&#29305;&#21035;&#20851;&#27880;&#22806;&#27719;&#24066;&#22330;&#65292;&#36890;&#36807;&#38646;-shot&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;ChatGPT&#22312;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#32422;35&#65285;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#22312;&#35299;&#35835;&#24066;&#22330;&#36235;&#21183;&#21644;&#25351;&#23548;&#25112;&#30053;&#20132;&#26131;&#20915;&#31574;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#24050;&#32463;&#20351;&#29992;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#65292;&#20294;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;ChatGPT 3.5&#65289;&#22312;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#24378;&#35843;&#22806;&#27719;&#24066;&#22330;&#65288;forex&#65289;&#65292;&#24320;&#21019;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#37319;&#29992;&#38646;-shot&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;&#19968;&#20221;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#22806;&#27719;&#30456;&#20851;&#26032;&#38395;&#26631;&#39064;&#25968;&#25454;&#38598;&#19978;&#26816;&#39564;&#22810;&#20010;ChatGPT&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#24471;&#20998;&#21644;&#24773;&#32490;&#20998;&#31867;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#31561;&#25351;&#26631;&#35780;&#20272;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#39044;&#27979;&#24773;&#32490;&#21644;&#24066;&#22330;&#22238;&#25253;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20316;&#20026;&#19968;&#31181;&#39069;&#22806;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#19982;FinBERT&#30456;&#27604;&#65292;ChatGPT&#22312;&#24773;&#32490;&#20998;&#26512;&#26041;&#38754;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#32422;35&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial sentiment analysis plays a crucial role in decoding market trends and guiding strategic trading decisions. Despite the deployment of advanced deep learning techniques and language models to refine sentiment analysis in finance, this study breaks new ground by investigating the potential of large language models, particularly ChatGPT 3.5, in financial sentiment analysis, with a strong emphasis on the foreign exchange market (forex). Employing a zero-shot prompting approach, we examine multiple ChatGPT prompts on a meticulously curated dataset of forex-related news headlines, measuring performance using metrics such as precision, recall, f1-score, and Mean Absolute Error (MAE) of the sentiment class. Additionally, we probe the correlation between predicted sentiment and market returns as an additional evaluation approach. ChatGPT, compared to FinBERT, a well-established sentiment analysis model for financial texts, exhibited approximately 35\% enhanced performance in sentiment 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20301;&#32763;&#36716;&#25915;&#20987;&#21644;&#27169;&#22411;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#23545;&#25163;&#26500;&#24314;&#39640;&#39118;&#38505;&#27169;&#22411;&#65292;&#22312;&#21482;&#36827;&#34892;&#23569;&#37327;&#20301;&#32763;&#36716;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#27491;&#24120;&#27169;&#22411;&#36716;&#21270;&#20026;&#24694;&#24847;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#36867;&#36991;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.07934</link><description>&lt;p&gt;
&#21482;&#38656;&#19968;&#27425;&#20301;&#32763;&#36716;&#65306;&#24403;&#20301;&#32763;&#36716;&#25915;&#20987;&#36935;&#21040;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training. (arXiv:2308.07934v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20301;&#32763;&#36716;&#25915;&#20987;&#21644;&#27169;&#22411;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#23545;&#25163;&#26500;&#24314;&#39640;&#39118;&#38505;&#27169;&#22411;&#65292;&#22312;&#21482;&#36827;&#34892;&#23569;&#37327;&#20301;&#32763;&#36716;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#27491;&#24120;&#27169;&#22411;&#36716;&#21270;&#20026;&#24694;&#24847;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#36867;&#36991;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#35774;&#22791;&#19978;&#12290;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#20851;&#27880;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#20462;&#25913;&#25915;&#20987;&#31216;&#20026;&#20301;&#32763;&#36716;&#25915;&#20987;&#65288;BFA&#65289;&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#20869;&#23384;&#25925;&#38556;&#27880;&#20837;&#25216;&#26415;&#65292;&#22914;&#34892;&#38180;&#20987;&#65292;&#26469;&#25915;&#20987;&#37096;&#32626;&#38454;&#27573;&#30340;&#37327;&#21270;&#27169;&#22411;&#12290;&#20165;&#36890;&#36807;&#23569;&#37327;&#30340;&#20301;&#32763;&#36716;&#65292;&#30446;&#26631;&#27169;&#22411;&#21487;&#20197;&#34987;&#28210;&#26579;&#20026;&#26080;&#29992;&#30340;&#38543;&#26426;&#29468;&#27979;&#32773;&#65292;&#29978;&#33267;&#21487;&#20197;&#26893;&#20837;&#24694;&#24847;&#21151;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36827;&#19968;&#27493;&#38477;&#20302;&#20301;&#32763;&#36716;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#36741;&#21161;&#30340;&#20301;&#32763;&#36716;&#25915;&#20987;&#65292;&#22312;&#20854;&#20013;&#65292;&#23545;&#25163;&#21442;&#19982;&#21040;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#24314;&#31435;&#19968;&#20010;&#39640;&#39118;&#38505;&#30340;&#37322;&#25918;&#27169;&#22411;&#12290;&#36825;&#20010;&#39640;&#39118;&#38505;&#27169;&#22411;&#19982;&#30456;&#24212;&#30340;&#24694;&#24847;&#27169;&#22411;&#32467;&#21512;&#65292;&#34920;&#29616;&#27491;&#24120;&#65292;&#24182;&#19988;&#21487;&#20197;&#36867;&#36991;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#36825;&#20010;&#39640;&#39118;&#38505;&#20294;&#27491;&#24120;&#30340;&#27169;&#22411;&#36716;&#21270;&#20026;&#21463;&#23475;&#32773;&#36825;&#36793;&#30340;&#24694;&#24847;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are widely deployed on real-world devices. Concerns regarding their security have gained great attention from researchers. Recently, a new weight modification attack called bit flip attack (BFA) was proposed, which exploits memory fault inject techniques such as row hammer to attack quantized models in the deployment stage. With only a few bit flips, the target model can be rendered useless as a random guesser or even be implanted with malicious functionalities. In this work, we seek to further reduce the number of bit flips. We propose a training-assisted bit flip attack, in which the adversary is involved in the training stage to build a high-risk model to release. This high-risk model, obtained coupled with a corresponding malicious model, behaves normally and can escape various detection methods. The results on benchmark datasets show that an adversary can easily convert this high-risk but normal model to a malicious one on victim's side by \textbf{flipp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340;3D&#20960;&#20309;&#19982;2D&#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07931</link><description>&lt;p&gt;
&#31934;&#31616;&#29305;&#24449;&#22330;&#20351;&#24471;&#35821;&#35328;&#24341;&#23548;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340;3D&#20960;&#20309;&#19982;2D&#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21644;&#35821;&#35328;&#30417;&#30563;&#30340;&#22270;&#20687;&#27169;&#22411;&#21253;&#21547;&#20102;&#19990;&#30028;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#23545;&#20110;&#27867;&#21270;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#38656;&#35201;&#23545; 3D &#20960;&#20309;&#30340;&#35814;&#32454;&#29702;&#35299;&#65292;&#36825;&#22312; 2D &#22270;&#20687;&#29305;&#24449;&#20013;&#24448;&#24448;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340; 3D &#20960;&#20309;&#19982; 2D &#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#26469;&#24357;&#21512;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340; 2D &#21040; 3D &#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545; 6 &#33258;&#30001;&#24230;&#25235;&#21462;&#21644;&#25918;&#32622;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20808;&#39564;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#33258;&#28982;&#27867;&#21270;&#12290;&#36890;&#36807;&#20174;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; CLIP &#20013;&#31934;&#31616;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#30001;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#26032;&#39062;&#23545;&#35937;&#36827;&#34892;&#25805;&#20316;&#30340;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#26410;&#35265;&#36807;&#30340;&#34920;&#36798;&#21644;&#26032;&#39062;&#31867;&#21035;&#30340;&#29289;&#20307;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25506;&#32034;&#20102;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#30340;&#24320;&#22987;&#21644;&#25345;&#32493;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.07927</link><description>&lt;p&gt;
"&#26376;&#32463;&#21608;&#26399;&#38271;&#24230;&#30340;&#39044;&#27979;&#24314;&#27169;&#65306;&#19968;&#31181;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;"
&lt;/p&gt;
&lt;p&gt;
Predictive Modeling of Menstrual Cycle Length: A Time Series Forecasting Approach. (arXiv:2308.07927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25506;&#32034;&#20102;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#30340;&#24320;&#22987;&#21644;&#25345;&#32493;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30830;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#23545;&#22899;&#24615;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#35753;&#20010;&#20307;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#26469;&#20943;&#23569;&#19982;&#21608;&#26399;&#30456;&#20851;&#30340;&#19981;&#36866;&#12290;&#27492;&#22806;&#65292;&#31934;&#30830;&#30340;&#39044;&#27979;&#23545;&#20110;&#35268;&#21010;&#22899;&#24615;&#29983;&#27963;&#20013;&#30340;&#37325;&#35201;&#20107;&#20214;&#65292;&#22914;&#35745;&#21010;&#29983;&#32946;&#65292;&#20063;&#26159;&#26377;&#29992;&#30340;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#39044;&#27979;&#35268;&#24459;&#21644;&#19981;&#35268;&#24459;&#26376;&#32463;&#21608;&#26399;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20123;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31639;&#27861;&#65292;&#22914;&#33258;&#22238;&#24402;&#32508;&#21512;&#31227;&#21160;&#24179;&#22343;&#12289;Huber&#22238;&#24402;&#12289;Lasso&#22238;&#24402;&#12289;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#30340;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#30340;&#24320;&#22987;&#21644;&#25345;&#32493;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
A proper forecast of the menstrual cycle is meaningful for women's health, as it allows individuals to take preventive actions to minimize cycle-associated discomforts. In addition, precise prediction can be useful for planning important events in a woman's life, such as family planning. In this work, we explored the use of machine learning techniques to predict regular and irregular menstrual cycles. We implemented some time series forecasting algorithm approaches, such as AutoRegressive Integrated Moving Average, Huber Regression, Lasso Regression, Orthogonal Matching Pursuit, and Long Short-Term Memory Network. Moreover, we generated synthetic data to achieve our purposes. The results showed that it is possible to accurately predict the onset and duration of menstrual cycles using machine learning techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffGuard&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DiffGuard&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.07687</link><description>&lt;p&gt;
DiffGuard&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models. (arXiv:2308.07687v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffGuard&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DiffGuard&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#35821;&#20041;&#24102;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#19982;&#21512;&#27861;&#31867;&#21035;&#20869;&#23481;&#22312;&#35821;&#20041;&#19978;&#30340;&#19981;&#21305;&#37197;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;DiffGuard&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;DiffGuard&#30452;&#25509;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#65292;&#30456;&#36739;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#25193;&#25955;&#27169;&#22411;&#26356;&#26131;&#20110;&#35757;&#32451;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#26465;&#20214;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#65292;DiffGuard&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20351;&#29992;&#22270;&#20687;&#21644;&#26631;&#31614;&#20316;&#20026;&#26465;&#20214;&#35757;&#32451;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#22256;&#38590;&#24615;&#65292;DiffGuard&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#20010;&#24615;&#21270;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.07439</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#20114;&#24863;&#30693;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks. (arXiv:2308.07439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#20010;&#24615;&#21270;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#23545;&#20110;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#30340;&#36890;&#29992;&#36712;&#36857;&#39044;&#27979;&#65292;&#24573;&#35270;&#20102;&#20010;&#21035;&#39550;&#39542;&#21592;&#30340;&#20010;&#24615;&#21270;&#39550;&#39542;&#27169;&#24335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#12290;&#20026;&#20102;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#27969;&#31243;&#65306;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#35268;&#27169;&#36712;&#36857;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#23450;&#39550;&#39542;&#25968;&#25454;&#38024;&#23545;&#27599;&#20010;&#39550;&#39542;&#21592;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#37319;&#29992;&#20154;&#26426;&#21327;&#21516;&#20223;&#30495;&#26469;&#25910;&#38598;&#20010;&#24615;&#21270;&#30340;&#33258;&#28982;&#39550;&#39542;&#36712;&#36857;&#21450;&#20854;&#30456;&#24212;&#30340;&#21608;&#22260;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of vehicle trajectories is vital for advanced driver assistance systems and autonomous vehicles. Existing methods mainly rely on generic trajectory predictions derived from large datasets, overlooking the personalized driving patterns of individual drivers. To address this gap, we propose an approach for interaction-aware personalized vehicle trajectory prediction that incorporates temporal graph neural networks. Our method utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to model the spatio-temporal interactions between target vehicles and their surrounding traffic. To personalize the predictions, we establish a pipeline that leverages transfer learning: the model is initially pre-trained on a large-scale trajectory dataset and then fine-tuned for each driver using their specific driving data. We employ human-in-the-loop simulation to collect personalized naturalistic driving trajectories and corresponding surrounding vehicle trajectories
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#30340;&#20301;&#23485;&#21644;&#23618;&#23485;&#26469;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#20248;&#21270;&#20102;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06422</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26641;&#29366;Parzen&#20272;&#35745;&#30340;&#25935;&#24863;&#24615;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21644;&#23485;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation. (arXiv:2308.06422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#30340;&#20301;&#23485;&#21644;&#23618;&#23485;&#26469;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#20248;&#21270;&#20102;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#25552;&#39640;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#26377;&#25928;&#20248;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25628;&#32034;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#36873;&#25321;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26368;&#20339;&#20301;&#23485;&#21644;&#23618;&#23485;&#12290;&#36825;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25928;&#29575;&#30340;&#26126;&#26174;&#25552;&#39640;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;Hessian&#30340;&#21098;&#26525;&#31574;&#30053;&#65292;&#26377;&#36873;&#25321;&#22320;&#20943;&#23569;&#25628;&#32034;&#22495;&#65292;&#30830;&#20445;&#31227;&#38500;&#38750;&#20851;&#38190;&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#24320;&#21457;&#26377;&#21033;&#21644;&#19981;&#21033;&#32467;&#26524;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#36825;&#31181;&#31574;&#30053;&#20801;&#35768;&#23545;&#26550;&#26500;&#21487;&#33021;&#24615;&#36827;&#34892;&#31616;&#21270;&#30340;&#25506;&#32034;&#65292;&#24182;&#36805;&#36895;&#30830;&#23450;&#34920;&#29616;&#26368;&#22909;&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#30693;&#21517;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35777;&#26126;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#26126;&#26174;&#20248;&#21183;&#12290;&#19982;&#39046;&#20808;&#30340;&#21387;&#32553;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#22686;&#24378;&#30340;&#30456;&#20851;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#26597;&#35810;-&#39033;&#30446;&#21305;&#37197;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05379</link><description>&lt;p&gt;
&#36229;&#36234;&#35821;&#20041;&#65306;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#34892;&#20026;&#22686;&#24378;&#30456;&#20851;&#27169;&#22411;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Semantics: Learning a Behavior Augmented Relevance Model with Self-supervised Learning. (arXiv:2308.05379v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05379
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#22686;&#24378;&#30340;&#30456;&#20851;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#26597;&#35810;-&#39033;&#30446;&#21305;&#37197;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24314;&#27169;&#26088;&#22312;&#23450;&#20301;&#19982;&#23545;&#24212;&#26597;&#35810;&#30456;&#20851;&#30340;&#29702;&#24819;&#39033;&#30446;&#65292;&#36825;&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#30830;&#20445;&#29992;&#25143;&#20307;&#39564;&#38750;&#24120;&#37325;&#35201;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#20256;&#32479;&#26041;&#27861;&#36890;&#36807;&#35780;&#20272;&#26597;&#35810;&#19982;&#39033;&#30446;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32431;&#35821;&#20041;&#21305;&#37197;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#26041;&#27861;&#12290;&#23454;&#38469;&#19978;&#65292;&#20174;&#29992;&#25143;&#25628;&#32034;&#35760;&#24405;&#30340;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#21487;&#20197;&#25552;&#20379;&#36827;&#19968;&#27493;&#25581;&#31034;&#29992;&#25143;&#25628;&#32034;&#24847;&#22270;&#30340;&#32447;&#32034;&#12290;&#24471;&#30410;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#34892;&#20026;&#22686;&#24378;&#30456;&#20851;&#23398;&#20064;&#27169;&#22411;&#30340;&#25903;&#20184;&#23453;&#25628;&#32034;&#27169;&#22411;&#65288;BARL-ASe&#65289;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#30446;&#26631;&#39033;&#30446;&#30340;&#30456;&#37051;&#26597;&#35810;&#21644;&#30446;&#26631;&#26597;&#35810;&#30340;&#30456;&#37051;&#39033;&#30446;&#26469;&#34917;&#20805;&#30446;&#26631;&#26597;&#35810;-&#39033;&#30446;&#30340;&#35821;&#20041;&#21305;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#22810;&#23618;&#20849;&#21516;&#27880;&#24847;&#21147;&#65292;&#20174;&#30456;&#37051;&#21644;&#30446;&#26631;&#35270;&#22270;&#20013;&#25552;&#21462;&#20102;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#27169;&#22411;&#38543;&#21518;&#37319;&#29992;&#37051;&#23621;-&#30446;&#26631;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26469;&#25552;&#39640;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance modeling aims to locate desirable items for corresponding queries, which is crucial for search engines to ensure user experience. Although most conventional approaches address this problem by assessing the semantic similarity between the query and item, pure semantic matching is not everything. In reality, auxiliary query-item interactions extracted from user historical behavior data of the search log could provide hints to reveal users' search intents further. Drawing inspiration from this, we devise a novel Behavior Augmented Relevance Learning model for Alipay Search (BARL-ASe) that leverages neighbor queries of target item and neighbor items of target query to complement target query-item semantic matching. Specifically, our model builds multi-level co-attention for distilling coarse-grained and fine-grained semantic representations from both neighbor and target views. The model subsequently employs neighbor-target self-supervised learning to improve the accuracy and robu
&lt;/p&gt;</description></item><item><title>&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.05342</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05342
&lt;/p&gt;
&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20013;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#19968;&#30452;&#22312;&#19981;&#26029;&#25552;&#39640;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#25552;&#31034;&#30340;&#30740;&#31350;&#22686;&#24378;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034; (MP)&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#20154;&#31867;&#20869;&#30465;&#25512;&#29702;&#36807;&#31243;&#21551;&#21457;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;MP&#65292;LLMs&#32463;&#21382;&#19968;&#31995;&#21015;&#26377;&#32467;&#26500;&#12289;&#33258;&#25105;&#24847;&#35782;&#30340;&#35780;&#20272;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#22312;&#30693;&#35782;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#20116;&#20010;&#24120;&#35265;&#30340;LLMs&#65306;Llama2&#12289;Vicuna&#12289;PaLM&#12289;GPT-3.5&#21644;GPT-4&#65292;&#23427;&#20204;&#37117;&#28085;&#30422;&#20102;&#26469;&#33258;GLUE&#21644;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#21508;&#31181;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37197;&#22791;MP&#30340;PaLM&#25509;&#36817;&#20854;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#36328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;MP&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce metacognitive prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SSL-Auth&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36873;&#25321;&#30340;&#20851;&#38190;&#26679;&#26412;&#20316;&#20026;&#27700;&#21360;&#20449;&#24687;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#39564;&#35777;&#32593;&#32476;&#26469;&#37325;&#26500;&#27700;&#21360;&#20449;&#24687;&#65292;&#20174;&#32780;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.04673</link><description>&lt;p&gt;
SSL-Auth:&#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SSL-Auth: An Authentication Framework by Fragile Watermarking for Pre-trained Encoders in Self-supervised Learning. (arXiv:2308.04673v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SSL-Auth&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36873;&#25321;&#30340;&#20851;&#38190;&#26679;&#26412;&#20316;&#20026;&#27700;&#21360;&#20449;&#24687;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#39564;&#35777;&#32593;&#32476;&#26469;&#37325;&#26500;&#27700;&#21360;&#20449;&#24687;&#65292;&#20174;&#32780;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20026;&#39044;&#35757;&#32451;&#30340;&#24378;&#22823;&#32534;&#30721;&#22120;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#36825;&#20123;&#32534;&#30721;&#22120;&#24120;&#34987;&#29992;&#20316;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20854;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22312;&#21830;&#19994;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#65292;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32534;&#30721;&#22120;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12289;&#23545;&#25239;&#25915;&#20987;&#31561;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#39564;&#35777;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#23436;&#25972;&#24615;&#30340;&#26041;&#26696;&#26469;&#20445;&#25252;&#29992;&#25143;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SSL-Auth&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#30340;&#26131;&#30862;&#27700;&#21360;&#36523;&#20221;&#39564;&#35777;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36873;&#25321;&#30340;&#20851;&#38190;&#26679;&#26412;&#20316;&#20026;&#27700;&#21360;&#20449;&#24687;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#39564;&#35777;&#32593;&#32476;&#26469;&#37325;&#26500;&#27700;&#21360;&#20449;&#24687;&#65292;&#20174;&#32780;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) which leverages unlabeled datasets for pre-training powerful encoders has achieved significant success in recent years. These encoders are commonly used as feature extractors for various downstream tasks, requiring substantial data and computing resources for their training process. With the deployment of pre-trained encoders in commercial use, protecting the intellectual property of model owners and ensuring the trustworthiness of the models becomes crucial. Recent research has shown that encoders are threatened by backdoor attacks, adversarial attacks, etc. Therefore, a scheme to verify the integrity of pre-trained encoders is needed to protect users. In this paper, we propose SSL-Auth, the first fragile watermarking scheme for verifying the integrity of encoders without compromising model performance. Our method utilizes selected key samples as watermark information and trains a verification network to reconstruct the watermark information, thereby ver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20132;&#20114;&#22240;&#26524;&#24207;&#21015;&#30340;&#20219;&#21153;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#20013;&#25191;&#34892;&#20219;&#21153;&#24182;&#35780;&#20272;&#20854;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#8220;&#24868;&#24594;&#30340;&#23567;&#40479;&#8221;&#28216;&#25103;&#20316;&#20026;&#31034;&#20363;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25351;&#26631;&#23545;&#29983;&#25104;&#30340;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.02835</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20132;&#20114;&#22240;&#26524;&#24207;&#21015;&#30340;&#20219;&#21153;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Physics-Based Task Generation Through Causal Sequence of Physical Interactions. (arXiv:2308.02835v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20132;&#20114;&#22240;&#26524;&#24207;&#21015;&#30340;&#20219;&#21153;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#20013;&#25191;&#34892;&#20219;&#21153;&#24182;&#35780;&#20272;&#20854;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#8220;&#24868;&#24594;&#30340;&#23567;&#40479;&#8221;&#28216;&#25103;&#20316;&#20026;&#31034;&#20363;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25351;&#26631;&#23545;&#29983;&#25104;&#30340;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#23545;&#20110;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26469;&#25191;&#34892;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#21364;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29289;&#29702;&#27169;&#25311;&#20219;&#21153;&#36890;&#24120;&#34987;&#29992;&#26469;&#20419;&#36827;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#20307;&#20043;&#38388;&#30340;&#29289;&#29702;&#20132;&#20114;&#30340;&#22240;&#26524;&#24207;&#21015;&#26469;&#23450;&#20041;&#29289;&#29702;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20123;&#23450;&#20041;&#30340;&#22330;&#26223;&#20316;&#20026;&#36755;&#20837;&#65292;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#20013;&#29983;&#25104;&#20219;&#21153;&#30340;&#26041;&#27861;&#23398;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35299;&#20915;&#22522;&#20110;&#29289;&#29702;&#30340;&#20219;&#21153;&#25152;&#38656;&#30340;&#24494;&#35266;&#21147;&#23398;&#65292;&#20174;&#32780;&#20026;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#25552;&#20379;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;&#30410;&#26234;&#28216;&#25103;&#8220;&#24868;&#24594;&#30340;&#23567;&#40479;&#8221;&#26469;&#28436;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#20219;&#21153;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#25351;&#26631;&#23545;&#29983;&#25104;&#30340;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#29289;&#29702;&#31283;&#23450;&#24615;&#12289;&#20351;&#29992;&#39044;&#26399;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#30340;&#21487;&#35299;&#24615;&#20197;&#21450;&#20351;&#29992;&#24847;&#22806;&#30456;&#20114;&#20316;&#29992;&#30340;&#20598;&#28982;&#21487;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performing tasks in a physical environment is a crucial yet challenging problem for AI systems operating in the real world. Physics simulation-based tasks are often employed to facilitate research that addresses this challenge. In this paper, first, we present a systematic approach for defining a physical scenario using a causal sequence of physical interactions between objects. Then, we propose a methodology for generating tasks in a physics-simulating environment using these defined scenarios as inputs. Our approach enables a better understanding of the granular mechanics required for solving physics-based tasks, thereby facilitating accurate evaluation of AI systems' physical reasoning capabilities. We demonstrate our proposed task generation methodology using the physics-based puzzle game Angry Birds and evaluate the generated tasks using a range of metrics, including physical stability, solvability using intended physical interactions, and accidental solvability using unintended s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37325;&#24314;&#35266;&#23519;&#21040;&#30340;&#22270;&#20687;&#26469;&#25581;&#31034;&#20154;&#33041;&#23545;&#35270;&#35273;&#21050;&#28608;&#30340;&#30693;&#35273;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroImagen&#30340;&#32508;&#21512;&#31649;&#36947;&#65292;&#21033;&#29992;&#33041;&#30005;&#22270;&#20449;&#21495;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.02510</link><description>&lt;p&gt;
&#31397;&#35270;&#22823;&#33041;&#65306;&#36890;&#36807;&#20154;&#33041;&#20449;&#21495;&#37325;&#24314;&#35270;&#35273;&#30693;&#35273;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals. (arXiv:2308.02510v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37325;&#24314;&#35266;&#23519;&#21040;&#30340;&#22270;&#20687;&#26469;&#25581;&#31034;&#20154;&#33041;&#23545;&#35270;&#35273;&#21050;&#28608;&#30340;&#30693;&#35273;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroImagen&#30340;&#32508;&#21512;&#31649;&#36947;&#65292;&#21033;&#29992;&#33041;&#30005;&#22270;&#20449;&#21495;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35265;&#21040;&#23601;&#20449;&#65292;&#28982;&#32780;&#20154;&#31867;&#35270;&#35273;&#30693;&#35273;&#19982;&#35748;&#30693;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#26159;&#19968;&#20010;&#35868;&#12290;&#30001;&#20110;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#25105;&#20204;&#33021;&#22815;&#35760;&#24405;&#21040;&#35270;&#35273;&#35825;&#21457;&#30340;&#33041;&#27963;&#21160;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#27169;&#25311;&#35270;&#35273;&#24863;&#30693;&#33021;&#21147;&#12290;&#26412;&#25991;&#20851;&#27880;&#36890;&#36807;&#37325;&#24314;&#35266;&#23519;&#21040;&#30340;&#22270;&#20687;&#26469;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#65292;&#22522;&#20110;&#26131;&#20110;&#33719;&#24471;&#30340;&#33041;&#20449;&#21495;&#65292;&#21363;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#12290;&#30001;&#20110;&#33041;&#30005;&#22270;&#20449;&#21495;&#26159;&#21160;&#24577;&#30340;&#26102;&#38388;&#24207;&#21015;&#26684;&#24335;&#65292;&#21516;&#26102;&#20063;&#22240;&#26377;&#22122;&#38899;&#32780;&#33261;&#21517;&#26157;&#33879;&#65292;&#22788;&#29702;&#21644;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#38656;&#35201;&#26356;&#22810;&#30340;&#19987;&#38376;&#24037;&#20316;&#65307;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#31649;&#36947;&#65292;&#21517;&#20026;NeuroImagen&#65292;&#29992;&#20110;&#20174;&#33041;&#30005;&#22270;&#20449;&#21495;&#20013;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#22270;&#20687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#26032;&#39062;&#30340;&#22810;&#23618;&#24863;&#30693;&#20449;&#24687;&#35299;&#30721;&#26041;&#27861;&#65292;&#20197;&#20174;&#32473;&#23450;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#33719;&#21462;&#22810;&#31890;&#24230;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seeing is believing, however, the underlying mechanism of how human visual perceptions are intertwined with our cognitions is still a mystery. Thanks to the recent advances in both neuroscience and artificial intelligence, we have been able to record the visually evoked brain activities and mimic the visual perception ability through computational approaches. In this paper, we pay attention to visual stimuli reconstruction by reconstructing the observed images based on portably accessible brain signals, i.e., electroencephalography (EEG) data. Since EEG signals are dynamic in the time-series format and are notorious to be noisy, processing and extracting useful information requires more dedicated efforts; In this paper, we propose a comprehensive pipeline, named NeuroImagen, for reconstructing visual stimuli images from EEG signals. Specifically, we incorporate a novel multi-level perceptual information decoding to draw multi-grained outputs from the given EEG data. A latent diffusion 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;PeRP&#65292;&#29992;&#20110;&#32531;&#35299;&#25317;&#22581;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#26500;&#21270;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26681;&#25454;&#39550;&#39542;&#21592;&#30340;&#29305;&#24449;&#20026;&#20854;&#25552;&#20379;&#34892;&#21160;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;</title><link>http://arxiv.org/abs/2308.00864</link><description>&lt;p&gt;
PeRP&#65306;&#36890;&#36807;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;&#23454;&#29616;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#20197;&#32531;&#35299;&#25317;&#22581;
&lt;/p&gt;
&lt;p&gt;
PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems. (arXiv:2308.00864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;PeRP&#65292;&#29992;&#20110;&#32531;&#35299;&#25317;&#22581;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#26500;&#21270;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26681;&#25454;&#39550;&#39542;&#21592;&#30340;&#29305;&#24449;&#20026;&#20854;&#25552;&#20379;&#34892;&#21160;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#34892;&#21160;&#26469;&#32531;&#35299;&#25317;&#22581;&#65292;&#20174;&#32780;&#25913;&#21892;&#36890;&#21220;&#26102;&#38388;&#21644;&#29123;&#27833;&#25104;&#26412;&#31561;&#20247;&#22810;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20551;&#35774;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#38431;&#20855;&#26377;&#31934;&#30830;&#30340;&#25511;&#21046;&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#20013;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#21040;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20998;&#27573;&#24120;&#25968;&#65288;PC&#65289;&#31574;&#30053;&#36890;&#36807;&#32467;&#26500;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#26469;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#65292;&#20197;&#25552;&#20379;&#32473;&#20154;&#31867;&#39550;&#39542;&#21592;&#36981;&#24490;&#30340;&#34892;&#21160;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;PC&#31574;&#30053;&#20551;&#35774;&#25152;&#26377;&#39550;&#39542;&#21592;&#34892;&#20026;&#30456;&#20284;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;PC&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31181;&#26032;&#22411;&#30340;&#39550;&#39542;&#21592;&#29305;&#24449;&#30456;&#20851;&#30340;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#65292;&#21363;PeRP&#12290;PeRP&#24314;&#35758;&#39550;&#39542;&#21592;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#30340;&#26041;&#24335;&#34892;&#39542;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26080;&#30417;&#30563;&#22320;&#25512;&#26029;&#39550;&#39542;&#21592;&#22914;&#20309;&#36981;&#24490;&#25351;&#20196;&#30340;&#20869;&#22312;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#31574;&#30053;&#19982;&#39550;&#39542;&#21592;&#29305;&#24449;&#26465;&#20214;&#21270;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#34892;&#21160;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent driving systems can be used to mitigate congestion through simple actions, thus improving many socioeconomic factors such as commute time and gas costs. However, these systems assume precise control over autonomous vehicle fleets, and are hence limited in practice as they fail to account for uncertainty in human behavior. Piecewise Constant (PC) Policies address these issues by structurally modeling the likeness of human driving to reduce traffic congestion in dense scenarios to provide action advice to be followed by human drivers. However, PC policies assume that all drivers behave similarly. To this end, we develop a co-operative advisory system based on PC policies with a novel driver trait conditioned Personalized Residual Policy, PeRP. PeRP advises drivers to behave in ways that mitigate traffic congestion. We first infer the driver's intrinsic traits on how they follow instructions in an unsupervised manner with a variational autoencoder. Then, a policy conditioned o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#12290;&#35813;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.16706</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD-Learning&#30340;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;ODE&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes. (arXiv:2307.16706v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#12290;&#35813;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#21644;&#20998;&#24067;&#24335;&#26102;&#24207;&#24046;&#20998;&#65288;TD&#65289;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#20854;&#20013;&#21508;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35775;&#38382;&#33258;&#24049;&#30340;&#22870;&#21169;&#65292;&#32570;&#20047;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#22870;&#21169;&#30340;&#20102;&#35299;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#33021;&#36890;&#36807;&#19968;&#20010;&#30001;&#22270;&#34920;&#31034;&#30340;&#36890;&#20449;&#32593;&#32476;&#19982;&#30456;&#37051;&#26234;&#33021;&#20307;&#20849;&#20139;&#20854;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21487;&#20197;&#24635;&#32467;&#20026;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;1&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21463;&#36830;&#32493;&#26102;&#38388;&#21306;&#38388;&#20869;&#30340;&#24179;&#22343;&#19968;&#33268;&#24615;&#26041;&#27861;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;DP&#12290;&#36890;&#36807;&#25511;&#21046;&#29702;&#35770;&#30340;&#35270;&#35282;&#35780;&#20272;&#20102;&#35813;DP&#30340;&#25910;&#25947;&#24615;&#12290;2&#65289;&#22522;&#20110;&#19978;&#36848;DP&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#24067;&#24335;DP&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary objective of this paper is to investigate distributed dynamic programming (DP) and distributed temporal difference (TD) learning algorithms for networked multi-agent Markov decision problems (MAMDPs). In our study, we adopt a distributed multi-agent framework where individual agents have access only to their own rewards, lacking insights into the rewards of other agents. Additionally, each agent has the ability to share its parameters with neighboring agents through a communication network, represented by a graph. Our contributions can be summarized in two key points: 1) We introduce a novel distributed DP, inspired by the averaging consensus method in the continuous-time domain. The convergence of this DP is assessed through control theory perspectives. 2) Building upon the aforementioned DP, we devise a new distributed TD-learning algorithm and prove its convergence. A standout feature of our proposed distributed DP is its incorporation of two independent dynamic systems,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16082</link><description>&lt;p&gt;
EnrichEvent: &#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#20026;&#26032;&#20986;&#29616;&#30340;&#20107;&#20214;&#25552;&#20379;&#20016;&#23500;&#30340;&#31038;&#20132;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#24179;&#21488;&#24050;&#25104;&#20026;&#20256;&#25773;&#21644;&#35752;&#35770;&#30495;&#23454;&#20107;&#20214;&#20449;&#24687;&#30340;&#20851;&#38190;&#24179;&#21488;&#65292;&#20026;&#21450;&#26089;&#21457;&#29616;&#26377;&#26032;&#38395;&#20215;&#20540;&#30340;&#20107;&#20214;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#20165;&#21033;&#29992;&#20851;&#38190;&#35789;&#31361;&#21457;&#24615;&#25110;&#32593;&#32476;&#32467;&#26500;&#26469;&#26816;&#27979;&#28909;&#28857;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20107;&#20214;&#21644;&#31038;&#20132;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35328;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#22312;&#36798;&#21040;&#36235;&#21183;&#29366;&#24577;&#20043;&#21069;&#35782;&#21035;&#20986;&#26032;&#20986;&#29616;&#30340;&#31038;&#20132;&#20107;&#20214;&#12290;&#31038;&#20132;&#25968;&#25454;&#65292;&#20363;&#22914;&#25512;&#25991;&#65292;&#20855;&#26377;&#25340;&#20889;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#12289;&#27495;&#20041;&#24615;&#21644;&#35821;&#35328;&#19981;&#35268;&#33539;&#24615;&#65292;&#20197;&#21450;&#24847;&#35265;&#26041;&#38754;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#23398;&#20064;&#20107;&#20214;&#30340;&#28436;&#21464;&#29305;&#24449;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20960;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#27969;&#24335;&#31038;&#20132;&#25968;&#25454;&#30340;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15780</link><description>&lt;p&gt;
LLM-Rec: &#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;LLM-Rec&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65306;&#65288;1&#65289;&#22522;&#30784;&#24341;&#23548;&#65292;&#65288;2&#65289;&#25512;&#33616;&#39537;&#21160;&#24341;&#23548;&#65292;&#65288;3&#65289;&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#65292;&#21644;&#65288;4&#65289;&#25512;&#33616;&#39537;&#21160;+&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#21407;&#22987;&#20869;&#23481;&#25551;&#36848;&#19982;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#36755;&#20837;&#25991;&#26412;&#32467;&#21512;&#36215;&#26469;&#65292;&#37319;&#29992;&#36825;&#20123;&#24341;&#23548;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that combining the original content description with the augmented input text generated by LLM using these prompting strategies leads to improved recommendation performance. This finding highlights the importance of incorporating diverse prompts and input augmentation techniques to enhance the recommendation capabilities with large language models for personalized content recommendation.
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.13565</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65306;&#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#22522;&#20934;&#21644;&#26410;&#26469;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13565
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#35757;&#32451;&#27169;&#22411;&#20197;&#20248;&#21270;&#20915;&#31574;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20010;&#33539;&#24335;&#26377;&#26395;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20915;&#31574;&#21046;&#23450;&#65292;&#36825;&#20123;&#24212;&#29992;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20123;&#20915;&#31574;&#27169;&#22411;&#20013;&#20272;&#35745;&#26410;&#30693;&#21442;&#25968;&#32463;&#24120;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#23545;DFL&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#23427;&#23545;&#21508;&#31181;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26681;&#25454;&#20854;&#29420;&#29305;&#29305;&#24449;&#26469;&#21306;&#20998;DFL&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;DFL&#30340;&#21512;&#36866;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;DFL&#30740;&#31350;&#20013;&#24403;&#21069;&#21644;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.12267</link><description>&lt;p&gt;
&#38754;&#21521;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#35770;&#25991;&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#33021;&#22815;&#22312;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27969;&#30021;&#22238;&#31572;&#12290;&#23613;&#31649;&#25215;&#35748;&#25216;&#26415;&#36827;&#27493;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#25945;&#32946;&#32773;&#20063;&#25285;&#24515;&#23398;&#29983;&#21487;&#33021;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#24182;&#23558;&#20854;&#20551;&#20882;&#20026;&#33258;&#24049;&#30340;&#21407;&#21019;&#20316;&#21697;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;AI&#20869;&#23481;&#26816;&#27979;&#30740;&#31350;&#26159;&#22522;&#20110;&#36825;&#20123;&#25285;&#24551;&#36827;&#34892;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;AI&#20869;&#23481;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#20551;&#35774;&#19968;&#20010;&#25991;&#26412;&#35201;&#20040;&#23436;&#20840;&#30001;&#20154;&#31867;&#32534;&#20889;&#65292;&#35201;&#20040;&#23436;&#20840;&#30001;AI&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#22312;&#19968;&#20010;&#23569;&#26377;&#25506;&#32034;&#20294;&#21364;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26816;&#27979;&#30340;&#25991;&#26412;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#28151;&#21512;&#25991;&#26412;&#65289;&#21327;&#20316;&#32534;&#20889;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#26816;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20174;&#32473;&#23450;&#30340;&#28151;&#21512;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#31867;&#32534;&#20889;&#20869;&#23481;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#36716;&#25442;&#28857;&#65288;&#36793;&#30028;&#26816;&#27979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2307.11787</link><description>&lt;p&gt;
LLM&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#26377;&#25152;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#20225;&#19994;&#21644;&#28040;&#36153;&#32773;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#34429;&#28982;&#36825;&#31867;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#23427;&#20204;&#20316;&#20026;&#35748;&#30693;&#20027;&#20307;&#30340;&#35843;&#26597;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#23545;GPT-3&#21644;ChatGPT&#22312;&#19968;&#20010;&#26469;&#33258;&#35748;&#30693;&#31185;&#23398;&#25991;&#29486;&#30340;&#26377;&#38480;&#25968;&#25454;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#20462;&#21098;&#30456;&#20851;&#23454;&#20307;&#65292;&#20174;&#32780;&#24341;&#23548;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#39046;&#22495;&#21644;&#24322;&#36136;&#31181;&#23376;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24212;&#29992;&#31867;&#27604;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.16296</link><description>&lt;p&gt;
&#30456;&#20851;&#23454;&#20307;&#36873;&#25321;&#65306;&#36890;&#36807;&#38646;&#26679;&#26412;&#31867;&#27604;&#20462;&#21098;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical Pruning. (arXiv:2306.16296v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#20462;&#21098;&#30456;&#20851;&#23454;&#20307;&#65292;&#20174;&#32780;&#24341;&#23548;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#39046;&#22495;&#21644;&#24322;&#36136;&#31181;&#23376;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24212;&#29992;&#31867;&#27604;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#39640;&#36136;&#37327;&#30340;&#26680;&#24515;&#24320;&#22987;&#65292;&#36890;&#36807;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#19981;&#26029;&#25913;&#36827;&#12290;&#36825;&#26679;&#30340;&#26680;&#24515;&#21487;&#20197;&#20174;&#20687;Wikidata&#36825;&#26679;&#30340;&#24320;&#25918;&#24335;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35268;&#27169;&#65292;&#23558;&#20854;&#20316;&#20026;&#25972;&#20307;&#38598;&#25104;&#21487;&#33021;&#20250;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#20174;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#24863;&#20852;&#36259;&#31181;&#23376;&#23454;&#20307;&#24320;&#22987;&#65292;&#24182;&#20445;&#30041;&#25110;&#20462;&#21098;&#20854;&#30456;&#37051;&#23454;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598; &#22312;Wikidata&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#39046;&#22495;&#21516;&#36136;&#25110;&#24322;&#36136;&#30340;&#31181;&#23376;&#23454;&#20307;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#20248;&#20110;LSTM&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#19988;&#21442;&#25968;&#25968;&#37327;&#22823;&#22823;&#20943;&#23569;&#12290;&#25105;&#20204;&#36824;&#22312;&#36801;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#36827;&#19968;&#27493;&#23558;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#38598;&#25104;&#21040;&#30456;&#20851;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Construction (KGC) can be seen as an iterative process starting from a high quality nucleus that is refined by knowledge extraction approaches in a virtuous loop. Such a nucleus can be obtained from knowledge existing in an open KG like Wikidata. However, due to the size of such generic KGs, integrating them as a whole may entail irrelevant content and scalability issues. We propose an analogy-based approach that starts from seed entities of interest in a generic KG, and keeps or prunes their neighboring entities. We evaluate our approach on Wikidata through two manually labeled datasets that contain either domain-homogeneous or -heterogeneous seed entities. We empirically show that our analogy-based approach outperforms LSTM, Random Forest, SVM, and MLP, with a drastically lower number of parameters. We also evaluate its generalization potential in a transfer learning setting. These results advocate for the further integration of analogy-based inference in tasks relate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Instruct-NeuralTalker&#65292;&#19968;&#31181;&#21033;&#29992;&#25351;&#20196;&#32534;&#36753;&#38899;&#39057;&#39537;&#21160;&#30340;&#23545;&#35805;&#36752;&#23556;&#22330;&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#20010;&#24615;&#21270;&#30340;&#23545;&#35805;&#38754;&#37096;&#29983;&#25104;&#65292;&#24182;&#22312;&#32534;&#36753;&#36807;&#31243;&#20013;&#20445;&#25345;&#38899;&#39057;&#21767;&#21516;&#27493;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#36731;&#37327;&#32423;&#30340;&#32454;&#21270;&#32593;&#32476;&#26469;&#23454;&#29616;&#21487;&#25511;&#30340;&#32454;&#33410;&#29983;&#25104;&#65292;&#24182;&#19988;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#21487;&#20197;&#36798;&#21040;&#26368;&#39640;30FPS&#30340;&#23454;&#26102;&#28210;&#26579;&#12290;</title><link>http://arxiv.org/abs/2306.10813</link><description>&lt;p&gt;
Instruct-NeuralTalker: &#21033;&#29992;&#25351;&#20196;&#32534;&#36753;&#38899;&#39057;&#39537;&#21160;&#30340;&#23545;&#35805;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions. (arXiv:2306.10813v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Instruct-NeuralTalker&#65292;&#19968;&#31181;&#21033;&#29992;&#25351;&#20196;&#32534;&#36753;&#38899;&#39057;&#39537;&#21160;&#30340;&#23545;&#35805;&#36752;&#23556;&#22330;&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#20010;&#24615;&#21270;&#30340;&#23545;&#35805;&#38754;&#37096;&#29983;&#25104;&#65292;&#24182;&#22312;&#32534;&#36753;&#36807;&#31243;&#20013;&#20445;&#25345;&#38899;&#39057;&#21767;&#21516;&#27493;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#36731;&#37327;&#32423;&#30340;&#32454;&#21270;&#32593;&#32476;&#26469;&#23454;&#29616;&#21487;&#25511;&#30340;&#32454;&#33410;&#29983;&#25104;&#65292;&#24182;&#19988;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#21487;&#20197;&#36798;&#21040;&#26368;&#39640;30FPS&#30340;&#23454;&#26102;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#23545;&#35805;&#36752;&#23556;&#22330;&#26041;&#27861;&#22312;&#36924;&#30495;&#30340;&#38899;&#39057;&#39537;&#21160;&#30340;&#23545;&#35805;&#38754;&#37096;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#25351;&#20196;&#26469;&#32534;&#36753;&#36825;&#31181;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65292;&#23454;&#29616;&#23454;&#26102;&#20010;&#24615;&#21270;&#30340;&#23545;&#35805;&#38754;&#37096;&#29983;&#25104;&#12290;&#32473;&#23450;&#19968;&#20010;&#30701;&#35270;&#39057;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#39640;&#25928;&#30340;&#23545;&#35805;&#36752;&#23556;&#22330;&#65292;&#28982;&#21518;&#26681;&#25454;&#32473;&#23450;&#30340;&#25351;&#20196;&#21644;&#24341;&#23548;&#30340;&#38544;&#24335;&#34920;&#31034;&#20248;&#21270;&#65292;&#24212;&#29992;&#26368;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#20197;&#23454;&#29616;&#32534;&#36753;&#30446;&#26631;&#12290;&#20026;&#20102;&#30830;&#20445;&#32534;&#36753;&#36807;&#31243;&#20013;&#30340;&#38899;&#39057;&#21767;&#21516;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#25968;&#25454;&#38598;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#21767;&#27839;&#25439;&#22833;&#32422;&#26463;&#21767;&#37096;&#21306;&#22495;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#32454;&#21270;&#32593;&#32476;&#65292;&#29992;&#20110;&#34917;&#20805;&#22270;&#20687;&#32454;&#33410;&#65292;&#24182;&#22312;&#26368;&#32456;&#28210;&#26579;&#22270;&#20687;&#20013;&#23454;&#29616;&#21487;&#25511;&#30340;&#32454;&#33410;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#23454;&#29616;&#26368;&#39640;30FPS&#30340;&#23454;&#26102;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent neural talking radiance field methods have shown great success in photorealistic audio-driven talking face synthesis. In this paper, we propose a novel interactive framework that utilizes human instructions to edit such implicit neural representations to achieve real-time personalized talking face generation. Given a short speech video, we first build an efficient talking radiance field, and then apply the latest conditional diffusion model for image editing based on the given instructions and guiding implicit representation optimization towards the editing target. To ensure audio-lip synchronization during the editing process, we propose an iterative dataset updating strategy and utilize a lip-edge loss to constrain changes in the lip region. We also introduce a lightweight refinement network for complementing image details and achieving controllable detail generation in the final rendered image. Our method also enables real-time rendering at up to 30FPS on consumer hardware. M
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10698</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#35760;&#24518;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10698
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21463;&#21040;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38480;&#21046;&#65292;&#20005;&#37325;&#20381;&#36182;&#19982;&#29615;&#22659;&#30340;&#22810;&#27425;&#20132;&#20114;&#25165;&#33021;&#33719;&#24471;&#20934;&#30830;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20284;&#20046;&#20381;&#36182;&#28023;&#39532;&#20307;&#20174;&#36807;&#21435;&#26377;&#20851;&#20219;&#21153;&#30340;&#32463;&#21382;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#25351;&#23548;&#20854;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20026;&#20195;&#29702;&#35774;&#35745;&#31867;&#20284;&#28023;&#39532;&#20307;&#30340;&#27169;&#22359;&#20197;&#23558;&#36807;&#21435;&#30340;&#32463;&#21382;&#34701;&#20837;&#26082;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#28041;&#21450;&#36873;&#25321;&#24403;&#21069;&#20219;&#21153;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#36825;&#20123;&#32463;&#39564;&#19982;&#20915;&#31574;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#26816;&#32034;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01631</link><description>&lt;p&gt;
Gode -- &#23558;&#29983;&#29289;&#21270;&#23398;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21040;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20419;&#36827;&#21019;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#21457;&#23637;&#21644;&#29702;&#35299;&#21270;&#23398;&#29289;&#36136;&#21644;&#29983;&#29289;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20010;&#20998;&#23376;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889; (KG) &#30340;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#36827;&#34892;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#20998;&#23376;&#32423;&#21644; KG &#32423;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312; 11 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18477</link><description>&lt;p&gt;
&#36229;&#36234;&#20803;&#25968;&#25454;&#65306;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#36827;&#34892;&#36328;&#29256;&#26412;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics. (arXiv:2305.18477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#26159;&#20840;&#29699;&#28216;&#25103;&#24066;&#22330;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#26159;&#22686;&#38271;&#26368;&#24555;&#30340;&#28216;&#25103;&#32454;&#20998;&#39046;&#22495;&#12290;&#36825;&#23548;&#33268;&#20102;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#30340;&#39046;&#22495;&#20135;&#29983;&#65292;&#20854;&#20351;&#29992;&#28216;&#25103;&#25552;&#21462;&#30340;&#36965;&#27979;&#25968;&#25454;&#26469;&#20026;&#29609;&#23478;&#12289;&#25945;&#32451;&#12289;&#25773;&#38899;&#21592;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#30340;&#20307;&#32946;&#27604;&#36187;&#30456;&#27604;&#65292;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#30340;&#26426;&#21046;&#21644;&#35268;&#21017;&#32463;&#24120;&#21457;&#29983;&#24555;&#36895;&#21464;&#21270;&#12290;&#30001;&#20110;&#28216;&#25103;&#21442;&#25968;&#30340;&#39057;&#32321;&#26356;&#25913;&#65292;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#27169;&#22411;&#30340;&#20351;&#29992;&#23551;&#21629;&#21487;&#33021;&#24456;&#30701;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#30053;&#20102;&#12290;&#26412;&#25991;&#25552;&#21462;&#28216;&#25103;&#35774;&#35745;&#20449;&#24687;&#65288;&#21363;&#34917;&#19969;&#35828;&#26126;&#65289;&#65292;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#12290;&#20197;Dota 2&#28216;&#25103;&#20013;&#20987;&#26432;&#27425;&#25968;&#30340;&#39044;&#27979;&#20026;&#26696;&#20363;&#65292;&#21033;&#29992;&#36825;&#31181;&#21019;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#25216;&#26415;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#27492;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#21253;&#25324;&#24120;&#35268;&#25216;&#26415;&#22312;&#20869;&#30340;&#20004;&#20010;&#19981;&#21516;&#22522;&#32447;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20010;&#27169;&#22411;&#19981;&#20165;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#27700;&#24179;&#65292;&#36824;&#20811;&#26381;&#20102;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#20013;&#29256;&#26412;&#26356;&#36845;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Esport games comprise a sizeable fraction of the global games market, and is the fastest growing segment in games. This has given rise to the domain of esports analytics, which uses telemetry data from games to inform players, coaches, broadcasters and other stakeholders. Compared to traditional sports, esport titles change rapidly, in terms of mechanics as well as rules. Due to these frequent changes to the parameters of the game, esport analytics models can have a short life-spam, a problem which is largely ignored within the literature. This paper extracts information from game design (i.e. patch notes) and utilises clustering techniques to propose a new form of character representation. As a case study, a neural network model is trained to predict the number of kills in a Dota 2 match utilising this novel character representation technique. The performance of this model is then evaluated against two distinct baselines, including conventional techniques. Not only did the model signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11095</link><description>&lt;p&gt;
&#28608;&#21457;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#20197;&#23454;&#29616;&#38646;-shot&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;Whisper&#30340;&#26032;&#20852;&#21151;&#33021;&#65292;&#22312;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;&#27169;&#22411;&#21518;&#65292;&#36866;&#24212;&#20102;&#26410;&#35265;&#36807;&#30340;AVSR&#65292;CS-ASR&#21644;ST&#19977;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#35201;&#20040;&#21033;&#29992;&#21478;&#19968;&#20010;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25805;&#20316;&#40664;&#35748;&#25552;&#31034;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#20351;&#36825;&#19977;&#20010;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;10%&#21040;45&#65285;&#65292;&#29978;&#33267;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;SotA&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;Whisper&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#21253;&#25324;&#20854;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#21475;&#38899;&#30340;&#20559;&#22909;&#20197;&#21450;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/jasonppy/PromptingWhisper&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
&lt;/p&gt;</description></item><item><title>Echoes&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#38598;&#20013;&#20559;&#24046;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#22788;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04043</link><description>&lt;p&gt;
Echoes: &#22522;&#20110;&#20266;&#20559;&#24046;&#26631;&#35760;&#30340;&#27169;&#20223;&#24335;&#22238;&#22768;&#23460;&#26080;&#30417;&#30563;&#21435;&#20559;
&lt;/p&gt;
&lt;p&gt;
Echoes: Unsupervised Debiasing via Pseudo-bias Labeling in an Echo Chamber. (arXiv:2305.04043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04043
&lt;/p&gt;
&lt;p&gt;
Echoes&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#38598;&#20013;&#20559;&#24046;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#22788;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#26292;&#38706;&#20110;&#26377;&#20559;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#36890;&#24120;&#20250;&#23398;&#20064;&#21040;&#19981;&#27491;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#25299;&#23637;&#39046;&#22495;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;Echoes&#8221;&#30340;&#31616;&#21333;&#39640;&#25928;&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#20197;&#24378;&#21046;&#20351;&#20266;&#26631;&#31614;&#19982;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#24046;&#29305;&#24449;&#19968;&#33268;&#65292;&#24182;&#29992;&#20110;&#21435;&#20559;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Echoes&#23454;&#29616;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often learn spurious correlations when exposed to biased training data, leading to poor performance on out-of-distribution data. A biased dataset can be divided, according to biased features, into bias-aligned samples (i.e., with biased features) and bias-conflicting samples (i.e., without biased features). Recent debiasing works typically assume that no bias label is available during the training phase, as obtaining such information is challenging and labor-intensive. Following this unsupervised assumption, existing methods usually train two models: a biased model specialized to learn biased features and a target model that uses information from the biased model for debiasing. This paper first presents experimental analyses revealing that the existing biased models overfit to bias-conflicting samples in the training data, which negatively impacts the debiasing performance of the target models. To address this issue, we propose a straightforward and effective method cal
&lt;/p&gt;</description></item><item><title>STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08503
&lt;/p&gt;
&lt;p&gt;
STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;(STO)&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#26088;&#22312;&#21033;&#29992;&#20648;&#23384;&#22312;&#25968;&#25454;&#24211;&#20013;&#20197;&#21069;&#27714;&#35299;&#30340;&#20248;&#21270;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31639;&#27861;&#35774;&#35745;&#24050;&#26377;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;STO&#20013;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#24182;&#19981;&#23436;&#21892;&#12290;&#23427;&#20204;&#24448;&#24448;&#26159;&#30001;&#20854;&#20182;&#22522;&#20934;&#20989;&#25968;&#38543;&#26426;&#32452;&#21512;&#32780;&#25104;&#65292;&#36825;&#20123;&#22522;&#20934;&#20989;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20339;&#20540;&#65292;&#25110;&#32773;&#29983;&#25104;&#33258;&#34920;&#29616;&#20986;&#26377;&#38480;&#21464;&#21270;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#25163;&#21160;&#37197;&#32622;&#30340;&#65292;&#22240;&#27492;&#21333;&#35843;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#34920;&#24449;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#21462;&#24471;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#20855;&#26377;&#39640;&#24230;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#38382;&#39064;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20123;&#34920;&#24449;STO&#38382;&#39064;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
&lt;/p&gt;</description></item><item><title>&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16166</link><description>&lt;p&gt;
&#27809;&#26377;&#27491;&#30830;&#24615;&#30340;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#37325;&#35201;&#65306;&#22312;NLP&#39046;&#22495;&#20013;&#27979;&#35797;&#20195;&#30721;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16166
&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20854;&#22312;&#30740;&#31350;&#23454;&#39564;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20195;&#30721;&#27491;&#30830;&#24615;&#24448;&#24448;&#20165;&#22522;&#20110;&#32467;&#26524;&#30340;&#24863;&#30693;&#36136;&#37327;&#32780;&#34987;&#20551;&#23450;&#12290;&#36825;&#24102;&#26469;&#20102;&#38169;&#35823;&#32467;&#26524;&#21644;&#28508;&#22312;&#35823;&#23548;&#24615;&#21457;&#29616;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#20851;&#27880;&#32467;&#26524;&#37325;&#29616;&#24212;&#35813;&#19982;&#24378;&#35843;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#25903;&#25345;&#25105;&#20204;&#21521;NLP&#31038;&#21306;&#21457;&#20986;&#30340;&#21495;&#21484;&#65292;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#24182;&#32416;&#27491;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;Conformer&#26550;&#26500;&#30340;&#24320;&#28304;&#23454;&#29616;&#20013;&#30340;&#19977;&#20010;Bug&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Bug&#30340;&#23384;&#22312;&#24182;&#19981;&#20250;&#22952;&#30861;&#33719;&#24471;&#33391;&#22909;&#30340;&#21644;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#65292;&#21453;&#32780;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#33021;&#25552;&#20379;&#38169;&#35823;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#37319;&#29992;&#26088;&#22312;&#20419;&#36827;NLP&#30740;&#31350;&#20013;&#27491;&#30830;&#24615;&#30340;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#21644;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#20013;&#12290;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#20132;&#36890;&#35268;&#21017;&#30340;&#36981;&#23432;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11888</link><description>&lt;p&gt;
&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36328;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving. (arXiv:2303.11888v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#21644;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#20013;&#12290;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#20132;&#36890;&#35268;&#21017;&#30340;&#36981;&#23432;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#24335;&#35782;&#21035;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#31561;&#20219;&#21153;&#30340;&#20934;&#30830;&#24230;&#24050;&#32463;&#36229;&#36807;&#20154;&#31867;&#12290;&#33258;&#21160;&#39550;&#39542;&#20316;&#20026;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#24443;&#24213;&#25913;&#21464;&#26410;&#26469;&#30340;&#20132;&#36890;&#21644;&#20986;&#34892;&#26041;&#24335;&#12290;&#20256;&#24863;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#34892;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#30001;&#20110;&#20854;&#22810;&#32500;&#24863;&#30693;&#21644;&#38598;&#25104;&#33021;&#21147;&#30340;&#28508;&#21147;&#32780;&#25104;&#20026;&#24403;&#21069;&#30740;&#31350;&#30340;&#28909;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#21644;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20027;&#35201;&#20851;&#27880;&#20110;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#30340;&#33021;&#21147;&#24182;&#32479;&#19968;&#27169;&#20223;&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of Pattern Recognition and Computer Vision technologies, tasks like object detection or semantic segmentation have achieved even better accuracy than human beings. Based on these solid foundations, autonomous driving is becoming an important research direction, aiming to revolute the future of transportation and mobility. Sensors are critical to autonomous driving's security and feasibility to perceive the surrounding environment. Multi-Sensor fusion has become a current research hot spot because of its potential for multidimensional perception and integration ability. In this paper, we propose a novel feature-level multi-sensor fusion technology for end-to-end autonomous driving navigation with imitation learning. Our paper mainly focuses on fusion technologies for Lidar and RGB information. We also provide a brand-new penalty-based imitation learning method to reinforce the model's compliance with traffic rules and unify the objective of imitation learning 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#38480;&#26597;&#35810;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#30340;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#65292;&#30446;&#26631;&#26159;&#29992;&#26368;&#23567;&#21270;&#26597;&#35810;&#27425;&#25968;&#30830;&#23450;&#22270;&#30340;s-t&#36830;&#36890;&#24615;&#65292;&#20027;&#35201;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#29992;&#20363;&#20013;&#30830;&#23450;&#25915;&#20987;&#36335;&#24452;&#30340;&#23384;&#22312;&#19982;&#21542;&#12290;</title><link>http://arxiv.org/abs/2302.13036</link><description>&lt;p&gt;
&#26377;&#38480;&#26597;&#35810;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Limited Query Graph Connectivity Test. (arXiv:2302.13036v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13036
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#38480;&#26597;&#35810;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#30340;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#65292;&#30446;&#26631;&#26159;&#29992;&#26368;&#23567;&#21270;&#26597;&#35810;&#27425;&#25968;&#30830;&#23450;&#22270;&#30340;s-t&#36830;&#36890;&#24615;&#65292;&#20027;&#35201;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#29992;&#20363;&#20013;&#30830;&#23450;&#25915;&#20987;&#36335;&#24452;&#30340;&#23384;&#22312;&#19982;&#21542;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#26597;&#35810;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20855;&#26377;&#20004;&#31181;&#21487;&#33021;&#29366;&#24577;&#65288;&#24320;/&#20851;&#65289;&#30340;&#22270;&#12290;&#36793;&#32536;&#30340;&#29366;&#24577;&#26368;&#21021;&#26159;&#38544;&#34255;&#30340;&#12290;&#25105;&#20204;&#21487;&#20197;&#26597;&#35810;&#36793;&#32536;&#20197;&#25581;&#31034;&#20854;&#29366;&#24577;&#12290;&#32473;&#23450;&#19968;&#20010;&#28304;&#28857;s&#21644;&#19968;&#20010;&#30446;&#26631;&#28857;t&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#35782;&#21035;&#36335;&#24452;&#65288;&#20165;&#30001;&#24320;&#36793;&#32452;&#25104;&#65289;&#25110;&#20999;&#21106;&#65288;&#20165;&#30001;&#20851;&#36793;&#32452;&#25104;&#65289;&#26469;&#27979;&#35797;s-t&#36830;&#36890;&#24615;&#12290;&#26080;&#35770;&#26159;&#21542;&#24314;&#31435;&#20102;&#22270;&#30340;&#36830;&#36890;&#24615;&#65292;&#25105;&#20204;&#37117;&#38480;&#21046;&#26597;&#35810;&#27425;&#25968;&#20026;B&#27425;&#21518;&#20572;&#27490;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#26597;&#35810;&#31574;&#30053;&#65292;&#20351;&#26399;&#26395;&#26597;&#35810;&#27425;&#25968;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#26159;&#22522;&#20110;&#19968;&#20010;&#32593;&#32476;&#20013;&#26159;&#21542;&#23384;&#22312;&#25915;&#20987;&#36335;&#24452;&#30340;&#32593;&#32476;&#23433;&#20840;&#29992;&#20363;&#32780;&#25552;&#20986;&#30340;&#12290;&#36793;&#32536;&#26597;&#35810;&#30001;IT&#31649;&#29702;&#21592;&#30340;&#25163;&#21160;&#24037;&#20316;&#35299;&#20915;&#65292;&#36825;&#26159;&#26368;&#23567;&#21270;&#26597;&#35810;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#21333;&#35843;&#38543;&#26426;&#24067;&#23572;&#20989;&#25968;&#27714;&#20540;&#65288;SBFE&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a combinatorial optimisation model called Limited Query Graph Connectivity Test. We consider a graph whose edges have two possible states (On/Off). The edges' states are hidden initially. We could query an edge to reveal its state. Given a source s and a destination t, we aim to test s-t connectivity by identifying either a path (consisting of only On edges) or a cut (consisting of only Off edges). We are limited to B queries, after which we stop regardless of whether graph connectivity is established. We aim to design a query policy that minimizes the expected number of queries.  Our model is mainly motivated by a cyber security use case where we need to establish whether an attack path exists in a network, between a source and a destination. Edge query is resolved by manual effort from the IT admin, which is the motivation behind query minimization.  Our model is highly related to monotone Stochastic Boolean Function Evaluation (SBFE). There are two existing exact algorith
&lt;/p&gt;</description></item><item><title>&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11562</link><description>&lt;p&gt;
&#39044;&#27979;&#26159;&#21542;&#38543;&#24847;&#65311;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#35780;&#20272;&#33258;&#27965;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290; &#23454;&#35777;&#34920;&#26126;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30340;&#26041;&#24046;&#24046;&#24322;&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#20915;&#31574;&#23454;&#38469;&#19978;&#26159;&#38543;&#24847;&#30340;&#12290; &#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#20570;&#20986;&#20102;&#22235;&#20010;&#24635;&#20307;&#36129;&#29486;&#65306;&#25105;&#20204;1&#65289;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#33258;&#27965;&#24615;&#65292;&#22312;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#26102;&#20351;&#29992;&#65307; 2&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#24403;&#39044;&#27979;&#26080;&#27861;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#21487;&#20197;&#25918;&#24323;&#20998;&#31867;&#65307; 3&#65289;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26377;&#20851;&#20844;&#24179;&#20998;&#31867;&#20013;&#26041;&#24046;&#65288;&#30456;&#23545;&#20110;&#33258;&#27965;&#24615;&#21644;&#38543;&#24847;&#24615;&#65289;&#20316;&#29992;&#30340;&#26368;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65307; 4&#65289;&#25512;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#20351;&#32654;&#22269;&#20303;&#25151;&#25269;&#25276;&#36151;&#27454;&#25259;&#38706;&#27861;&#26696;&#65288;HMDA&#65289;&#25968;&#25454;&#38598;&#26131;&#20110;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#12290; &#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#25581;&#31034;&#20102;&#20851;&#20110;&#21487;&#37325;&#22797;&#24615;&#30340;&#20196;&#20154;&#38663;&#24778;&#30340;&#35265;&#35299;&#12290;&#24403;&#32771;&#34385;&#21040;&#26041;&#24046;&#21644;&#38543;&#24847;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#26102;&#65292;&#22823;&#22810;&#25968;&#20844;&#24179;&#20998;&#31867;&#22522;&#20934;&#25509;&#36817;&#20844;&#24179;&#12290; &#20294;&#26159;&#65292;&#19968;&#23567;&#37096;&#20998;&#23454;&#20363;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#38543;&#24847;&#24615;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
&lt;/p&gt;</description></item><item><title>Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.11118</link><description>&lt;p&gt;
Box$^2$EL: EL++&#25551;&#36848;&#36923;&#36753;&#20013;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#26041;&#27861;&#21450;&#20854;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++. (arXiv:2301.11118v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11118
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#36923;&#36753;&#26412;&#20307;&#35770;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#35889;&#19982;&#27010;&#24565;&#20449;&#24687;&#21644;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#36825;&#31181;&#26412;&#20307;&#35770;&#30340;&#24402;&#32435;&#25512;&#29702;&#25216;&#26415;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#36825;&#20123;&#25216;&#26415;&#26377;&#26395;&#34917;&#20805;&#20256;&#32479;&#30340;&#28436;&#32462;&#25512;&#29702;&#31639;&#27861;&#12290;&#31867;&#20284;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23436;&#21892;&#65292;&#29616;&#26377;&#30340;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#26412;&#20307;&#35770;&#23884;&#20837;&#65292;&#21516;&#26102;&#30830;&#20445;&#36825;&#20123;&#23884;&#20837;&#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#24213;&#23618;&#25551;&#36848;&#36923;&#36753;&#30340;&#36923;&#36753;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21463;&#38480;&#30340;&#35282;&#33394;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Box$^2$EL&#26041;&#27861;&#65292;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#37117;&#34920;&#31034;&#20026;&#30418;&#23376;&#65288;&#21363;&#36724;&#23545;&#40784;&#36229;&#30697;&#24418;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20811;&#26381;&#20043;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;&#20316;&#20026;&#25105;&#20204;&#35780;&#20272;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Description logic (DL) ontologies extend knowledge graphs (KGs) with conceptual information and logical background knowledge. In recent years, there has been growing interest in inductive reasoning techniques for such ontologies, which promise to complement classical deductive reasoning algorithms. Similar to KG completion, several existing approaches learn ontology embeddings in a latent space, while additionally ensuring that they faithfully capture the logical semantics of the underlying DL. However, they suffer from several shortcomings, mainly due to a limiting role representation. We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles) and demonstrate how it overcomes the limitations of previous methods. We theoretically prove the soundness of our model and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets. As part of our evaluation, we introduce a novel benchmark for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;660&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#24182;&#19981;&#22343;&#21248;&#20998;&#24067;&#22312;&#20854;&#21508;&#20010;&#32452;&#20214;&#19978;&#12290;&#36890;&#36807;&#31227;&#38500;&#32422;70%&#30340;&#27880;&#24847;&#21147;&#22836;&#21644;&#32422;20%&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#20219;&#21153;&#25191;&#34892;&#34920;&#29616;&#20165;&#26377;&#36731;&#24494;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#22312;OPT-66B&#20013;&#65292;&#23384;&#22312;&#19968;&#23567;&#37096;&#20998;&#27880;&#24847;&#21147;&#22836;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#24402;&#32435;&#25805;&#20316;&#20855;&#26377;&#39640;&#25928;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09095</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#35268;&#27169;&#30340;&#20316;&#29992;: &#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;660&#20159;&#23610;&#24230;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale. (arXiv:2212.09095v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;660&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#24182;&#19981;&#22343;&#21248;&#20998;&#24067;&#22312;&#20854;&#21508;&#20010;&#32452;&#20214;&#19978;&#12290;&#36890;&#36807;&#31227;&#38500;&#32422;70%&#30340;&#27880;&#24847;&#21147;&#22836;&#21644;&#32422;20%&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#20219;&#21153;&#25191;&#34892;&#34920;&#29616;&#20165;&#26377;&#36731;&#24494;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#22312;OPT-66B&#20013;&#65292;&#23384;&#22312;&#19968;&#23567;&#37096;&#20998;&#27880;&#24847;&#21147;&#22836;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#24402;&#32435;&#25805;&#20316;&#20855;&#26377;&#39640;&#25928;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#27169;&#22686;&#21152;&#26102;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;660&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;OPT-66B&#65289;&#22312;14&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#26159;&#21542;&#22343;&#21248;&#20998;&#24067;&#22312;&#20854;&#25152;&#26377;&#30340;&#32452;&#20214;&#19978;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#32422;70%&#30340;&#27880;&#24847;&#21147;&#22836;&#21644;&#32422;20%&#30340;&#21069;&#39304;&#32593;&#36335;&#21487;&#20197;&#31227;&#38500;&#32780;&#20219;&#21153;&#34920;&#29616;&#20165;&#26377;&#36731;&#24494;&#19979;&#38477;&#12290;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#19978;&#19979;&#25991;&#31034;&#20363;&#25968;&#37327;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#19981;&#37325;&#35201;&#30340;&#27880;&#24847;&#21147;&#22836;&#30340;&#38598;&#21512;&#23384;&#22312;&#36739;&#22823;&#30340;&#37325;&#21472;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21457;&#29616;OPT-66B&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#27880;&#24847;&#21147;&#22836;&#22312;&#25191;&#34892;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#20851;&#30340;&#22522;&#30784;&#24402;&#32435;&#25805;&#20316;&#65288;&#21363;&#21069;&#32512;&#21305;&#37197;&#21644;&#22797;&#21046;&#65289;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: $\sim$70% of attention heads and $\sim$20% of feed forward networks can be removed with minimal decline in task performance. We find substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, prefix matching and copying. These in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3D-TOGO&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#36328;&#31867;&#21035;&#30340;&#25991;&#26412;&#24341;&#23548;&#30340;&#19977;&#32500;&#29289;&#20307;&#29983;&#25104;&#12290;&#27169;&#22411;&#21253;&#25324;&#25991;&#26412;&#21040;&#35270;&#22270;&#29983;&#25104;&#27169;&#22359;&#21644;&#35270;&#22270;&#21040;&#19977;&#32500;&#29983;&#25104;&#27169;&#22359;&#65292;&#20351;&#29992;&#20808;&#39564;&#24341;&#23548;&#12289;&#26631;&#39064;&#24341;&#23548;&#21644;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#31561;&#26041;&#27861;&#25552;&#39640;&#35270;&#22270;&#19968;&#33268;&#24615;&#21644;&#26631;&#39064;&#30456;&#20284;&#24615;&#12290;&#37319;&#29992;pixelNeRF&#27169;&#22411;&#36827;&#34892;&#35270;&#22270;&#21040;&#19977;&#32500;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2212.01103</link><description>&lt;p&gt;
3D-TOGO: &#36328;&#31867;&#21035;&#25991;&#26412;&#24341;&#23548;&#30340;&#19977;&#32500;&#29289;&#20307;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
3D-TOGO: Towards Text-Guided Cross-Category 3D Object Generation. (arXiv:2212.01103v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3D-TOGO&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#36328;&#31867;&#21035;&#30340;&#25991;&#26412;&#24341;&#23548;&#30340;&#19977;&#32500;&#29289;&#20307;&#29983;&#25104;&#12290;&#27169;&#22411;&#21253;&#25324;&#25991;&#26412;&#21040;&#35270;&#22270;&#29983;&#25104;&#27169;&#22359;&#21644;&#35270;&#22270;&#21040;&#19977;&#32500;&#29983;&#25104;&#27169;&#22359;&#65292;&#20351;&#29992;&#20808;&#39564;&#24341;&#23548;&#12289;&#26631;&#39064;&#24341;&#23548;&#21644;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#31561;&#26041;&#27861;&#25552;&#39640;&#35270;&#22270;&#19968;&#33268;&#24615;&#21644;&#26631;&#39064;&#30456;&#20284;&#24615;&#12290;&#37319;&#29992;pixelNeRF&#27169;&#22411;&#36827;&#34892;&#35270;&#22270;&#21040;&#19977;&#32500;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#19977;&#32500;&#29289;&#20307;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#30001;&#29992;&#25143;&#23450;&#20041;&#30340;&#26631;&#39064;&#25551;&#36848;&#30340;&#19977;&#32500;&#29289;&#20307;&#65292;&#20026;&#25105;&#20204;&#35774;&#24819;&#20013;&#30340;&#20107;&#29289;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#21487;&#35270;&#21270;&#26041;&#24335;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#35201;&#20040;&#21033;&#29992;&#19968;&#20123;&#26174;&#24335;&#30340;&#19977;&#32500;&#34920;&#31034;&#65288;&#20363;&#22914;&#32593;&#26684;&#65289;&#65292;&#32570;&#20047;&#32441;&#29702;&#24182;&#38656;&#35201;&#21518;&#22788;&#29702;&#20197;&#28210;&#26579;&#29031;&#29255;&#33324;&#36924;&#30495;&#30340;&#35270;&#22270;&#65307;&#35201;&#20040;&#38656;&#35201;&#20010;&#21035;&#32791;&#26102;&#30340;&#20248;&#21270;&#26469;&#22788;&#29702;&#27599;&#20010;&#21333;&#29420;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#26032;&#30340;3D-TOGO&#27169;&#22411;&#23454;&#29616;&#36890;&#29992;&#30340;&#36328;&#31867;&#21035;&#25991;&#26412;&#24341;&#23548;&#30340;&#19977;&#32500;&#29289;&#20307;&#29983;&#25104;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#25991;&#26412;&#21040;&#35270;&#22270;&#29983;&#25104;&#27169;&#22359;&#21644;&#35270;&#22270;&#21040;&#19977;&#32500;&#29983;&#25104;&#27169;&#22359;&#12290;&#25991;&#26412;&#21040;&#35270;&#22270;&#29983;&#25104;&#27169;&#22359;&#19987;&#38376;&#29992;&#20110;&#26681;&#25454;&#36755;&#20837;&#26631;&#39064;&#29983;&#25104;&#30446;&#26631;&#19977;&#32500;&#29289;&#20307;&#30340;&#19981;&#21516;&#35270;&#22270;&#12290;&#25552;&#20986;&#20102;&#20808;&#39564;&#24341;&#23548;&#12289;&#26631;&#39064;&#24341;&#23548;&#21644;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35270;&#22270;&#19968;&#33268;&#24615;&#21644;&#26631;&#39064;&#30456;&#20284;&#24615;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;pixelNeRF&#27169;&#22411;&#36827;&#34892;&#35270;&#22270;&#21040;&#19977;&#32500;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided 3D object generation aims to generate 3D objects described by user-defined captions, which paves a flexible way to visualize what we imagined. Although some works have been devoted to solving this challenging task, these works either utilize some explicit 3D representations (e.g., mesh), which lack texture and require post-processing for rendering photo-realistic views; or require individual time-consuming optimization for every single case. Here, we make the first attempt to achieve generic text-guided cross-category 3D object generation via a new 3D-TOGO model, which integrates a text-to-views generation module and a views-to-3D generation module. The text-to-views generation module is designed to generate different views of the target 3D object given an input caption. prior-guidance, caption-guidance and view contrastive learning are proposed for achieving better view-consistency and caption similarity. Meanwhile, a pixelNeRF model is adopted for the views-to-3D generati
&lt;/p&gt;</description></item><item><title>&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.11695</link><description>&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning. (arXiv:2211.11695v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11695
&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65288;DRL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#21487;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#22240;&#32032;&#30340;&#27169;&#22411;&#12290;&#23558;&#21464;&#21270;&#30340;&#28508;&#22312;&#35201;&#32032;&#20998;&#31163;&#25104;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#21464;&#37327;&#30340;&#36807;&#31243;&#26377;&#21161;&#20110;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#27169;&#20223;&#20154;&#31867;&#35266;&#23519;&#23545;&#35937;&#25110;&#20851;&#31995;&#26102;&#30340;&#26377;&#24847;&#20041;&#29702;&#35299;&#36807;&#31243;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;DRL&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25968;&#25454;&#25366;&#25496;&#31561;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;DRL&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#23450;&#20041;&#12289;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#12289;&#24212;&#29992;&#21644;&#27169;&#22411;&#35774;&#35745;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#20004;&#20010;&#20844;&#35748;&#23450;&#20041;&#65288;&#30452;&#35266;&#23450;&#20041;&#21644;&#32676;&#35770;&#23450;&#20041;&#65289;&#30340;DRL&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;DRL&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#23039;&#21183;&#24322;&#24120;&#26816;&#27979;&#30340;&#26631;&#20934;&#21270;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#23039;&#21183;&#24322;&#24120;&#26816;&#27979;&#20943;&#23569;&#20102;&#24178;&#25200;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20855;&#26377;&#20943;&#23569;&#20559;&#35265;&#30340;&#20248;&#21183;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#39640;&#24230;&#32039;&#20945;&#30340;&#23039;&#21183;&#34920;&#31034;&#65292;&#22312;&#35299;&#20915;&#26102;&#31354;&#23039;&#21183;&#25968;&#25454;&#30340;&#29305;&#27530;&#29305;&#24449;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20934;&#21017;&#35774;&#32622;&#21644;&#38750;&#20934;&#21017;&#35774;&#32622;&#19979;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.10946</link><description>&lt;p&gt;
&#26631;&#20934;&#21270;&#27969;&#29992;&#20110;&#20154;&#20307;&#23039;&#21183;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows for Human Pose Anomaly Detection. (arXiv:2211.10946v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#23039;&#21183;&#24322;&#24120;&#26816;&#27979;&#30340;&#26631;&#20934;&#21270;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#23039;&#21183;&#24322;&#24120;&#26816;&#27979;&#20943;&#23569;&#20102;&#24178;&#25200;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20855;&#26377;&#20943;&#23569;&#20559;&#35265;&#30340;&#20248;&#21183;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#39640;&#24230;&#32039;&#20945;&#30340;&#23039;&#21183;&#34920;&#31034;&#65292;&#22312;&#35299;&#20915;&#26102;&#31354;&#23039;&#21183;&#25968;&#25454;&#30340;&#29305;&#27530;&#29305;&#24449;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20934;&#21017;&#35774;&#32622;&#21644;&#38750;&#20934;&#21017;&#35774;&#32622;&#19979;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#19981;&#30830;&#23450;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#35768;&#22810;&#21442;&#25968;&#65292;&#22914;&#22806;&#35266;&#12289;&#23039;&#21183;&#12289;&#25668;&#20687;&#26426;&#35282;&#24230;&#12289;&#32972;&#26223;&#31561;&#31561;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#20154;&#20307;&#23039;&#21183;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#22806;&#35266;&#31561;&#24433;&#21709;&#32467;&#26524;&#30340;&#24178;&#25200;&#21442;&#25968;&#30340;&#39118;&#38505;&#12290;&#20165;&#20851;&#27880;&#23039;&#21183;&#36824;&#21487;&#20197;&#20943;&#23569;&#23545;&#29305;&#23450;&#23569;&#25968;&#32676;&#20307;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30452;&#25509;&#22312;&#20154;&#20307;&#23039;&#21183;&#22270;&#24207;&#21015;&#19978;&#36816;&#34892;&#65292;&#38750;&#24120;&#36731;&#37327;&#32423;&#65288;&#32422;1K&#20010;&#21442;&#25968;&#65289;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#33021;&#22815;&#36816;&#34892;&#23039;&#21183;&#20272;&#35745;&#30340;&#26426;&#22120;&#19978;&#36816;&#34892;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#36164;&#28304;&#12290;&#25105;&#20204;&#21033;&#29992;&#39640;&#24230;&#32039;&#20945;&#30340;&#23039;&#21183;&#34920;&#31034;&#22312;&#26631;&#20934;&#21270;&#27969;&#26694;&#26550;&#20013;&#36827;&#34892;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26102;&#31354;&#23039;&#21183;&#25968;&#25454;&#30340;&#29305;&#27530;&#29305;&#24449;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#29992;&#20363;&#20013;&#30340;&#20248;&#21183;&#12290;&#35813;&#31639;&#27861;&#38750;&#24120;&#36890;&#29992;&#65292;&#26082;&#21487;&#20197;&#22788;&#29702;&#21482;&#26377;&#27491;&#24120;&#31034;&#20363;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#22788;&#29702;&#21253;&#21547;&#26631;&#35760;&#30340;&#27491;&#24120;&#21644;&#24322;&#24120;&#31034;&#20363;&#30340;&#30417;&#30563;&#35774;&#32622;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26368;&#20808;&#36827;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video anomaly detection is an ill-posed problem because it relies on many parameters such as appearance, pose, camera angle, background, and more. We distill the problem to anomaly detection of human pose, thus decreasing the risk of nuisance parameters such as appearance affecting the result. Focusing on pose alone also has the side benefit of reducing bias against distinct minority groups. Our model works directly on human pose graph sequences and is exceptionally lightweight (~1K parameters), capable of running on any machine able to run the pose estimation with negligible additional resources. We leverage the highly compact pose representation in a normalizing flows framework, which we extend to tackle the unique characteristics of spatio-temporal pose data and show its advantages in this use case. The algorithm is quite general and can handle training data of only normal examples as well as a supervised setting that consists of labeled normal and abnormal examples. We report state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#20808;&#35753;&#27169;&#22411;&#23398;&#20064;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#19981;&#26029;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2211.09703</link><description>&lt;p&gt;
&#39640;&#25928;&#35757;&#32451;&#65306;&#25506;&#32034;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26469;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones. (arXiv:2211.09703v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#20808;&#35753;&#27169;&#22411;&#23398;&#20064;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#19981;&#26029;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#30340;&#21331;&#36234;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#26114;&#36149;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65288;&#20363;&#22914;&#35270;&#35273;Transformer&#65289;&#12290;&#26412;&#25991;&#21551;&#21457;&#20110;&#28145;&#24230;&#32593;&#32476;&#30340;&#20869;&#22312;&#23398;&#20064;&#21160;&#21147;&#23398;&#65306;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#22312;&#36739;&#26089;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#27169;&#22411;&#20027;&#35201;&#23398;&#20064;&#22312;&#27599;&#20010;&#31034;&#20363;&#20013;&#35782;&#21035;&#19968;&#20123;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#21028;&#21035;&#27169;&#24335;&#65292;&#20363;&#22914;&#22270;&#20687;&#30340;&#20302;&#39057;&#25104;&#20998;&#21644;&#25968;&#25454;&#22686;&#24191;&#20043;&#21069;&#30340;&#21407;&#22987;&#20449;&#24687;&#12290;&#22522;&#20110;&#27492;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#65292;&#20854;&#20013;&#27169;&#22411;&#24635;&#26159;&#22312;&#27599;&#20010;&#26102;&#26399;&#21033;&#29992;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#35838;&#31243;&#22987;&#20110;&#20165;&#26292;&#38706;&#27599;&#20010;&#31034;&#20363;&#30340;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#24182;&#36880;&#28176;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;1&#65289;&#22312;&#36755;&#20837;&#30340;&#20613;&#37324;&#21494;&#35889;&#20013;&#24341;&#20837;&#19968;&#20010;&#35009;&#21098;&#25805;&#20316;&#65292;&#20351;&#27169;&#22411;&#21482;&#33021;&#20174;&#20302;&#39057;&#32452;&#20998;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The superior performance of modern deep networks usually comes with a costly training procedure. This paper presents a new curriculum learning approach for the efficient training of visual backbones (e.g., vision Transformers). Our work is inspired by the inherent learning dynamics of deep networks: we experimentally show that at an earlier training stage, the model mainly learns to recognize some 'easier-to-learn' discriminative patterns within each example, e.g., the lower-frequency components of images and the original information before data augmentation. Driven by this phenomenon, we propose a curriculum where the model always leverages all the training data at each epoch, while the curriculum starts with only exposing the 'easier-to-learn' patterns of each example, and introduces gradually more difficult patterns. To implement this idea, we 1) introduce a cropping operation in the Fourier spectrum of the inputs, which enables the model to learn from only the lower-frequency compo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SDAM&#30340;&#26032;&#39062;&#22522;&#20110;&#35821;&#27861;&#25351;&#23548;&#30340;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#39046;&#22495;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;SDAM&#21033;&#29992;&#21477;&#27861;&#32467;&#26500;&#30456;&#20284;&#24615;&#26500;&#24314;&#20266;&#35757;&#32451;&#23454;&#20363;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.05457</link><description>&lt;p&gt;
&#35821;&#27861;&#25351;&#23548;&#30340;&#39046;&#22495;&#36866;&#24212;&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Syntax-Guided Domain Adaptation for Aspect-based Sentiment Analysis. (arXiv:2211.05457v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SDAM&#30340;&#26032;&#39062;&#22522;&#20110;&#35821;&#27861;&#25351;&#23548;&#30340;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#39046;&#22495;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;SDAM&#21033;&#29992;&#21477;&#27861;&#32467;&#26500;&#30456;&#20284;&#24615;&#26500;&#24314;&#20266;&#35757;&#32451;&#23454;&#20363;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26088;&#22312;&#20174;&#35780;&#35770;&#25991;&#26412;&#20013;&#25552;&#21462;&#20855;&#26377;&#24847;&#35265;&#30340;&#26041;&#38754;&#35789;&#27719;&#24182;&#30830;&#23450;&#23427;&#20204;&#30340;&#24773;&#24863;&#26497;&#24615;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#20316;&#20026;&#19968;&#39033;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#26631;&#27880;&#25104;&#26412;&#38750;&#24120;&#39640;&#12290;&#39046;&#22495;&#36866;&#24212;&#26159;&#35299;&#20915;&#26032;&#39046;&#22495;&#25968;&#25454;&#19981;&#36275;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#20043;&#38388;&#36716;&#31227;&#20849;&#26377;&#30693;&#35782;&#26469;&#20943;&#23567;&#24046;&#36317;&#12290;&#22823;&#22810;&#25968;&#36328;&#39046;&#22495;ABSA&#30740;&#31350;&#22522;&#20110;&#32467;&#26500;&#23545;&#24212;&#23398;&#20064;&#65288;SCL&#65289;&#65292;&#24182;&#20351;&#29992;&#20013;&#20171;&#29305;&#24449;&#26500;&#24314;&#36741;&#21161;&#20219;&#21153;&#20197;&#32553;&#23567;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22522;&#20110;&#20013;&#20171;&#30340;&#36741;&#21161;&#20219;&#21153;&#21482;&#33021;&#20256;&#36882;&#20851;&#20110;&#26041;&#38754;&#35789;&#27719;&#32780;&#19981;&#26159;&#24773;&#24863;&#30340;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#27861;&#25351;&#23548;&#30340;&#39046;&#22495;&#36866;&#24212;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;SDAM&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#39046;&#22495;ABSA&#12290;SDAM&#21033;&#29992;&#21477;&#27861;&#32467;&#26500;&#30456;&#20284;&#24615;&#26469;&#26500;&#24314;&#20266;&#35757;&#32451;&#23454;&#20363;&#65292;&#20854;&#20013;&#30446;&#26631;&#39046;&#22495;&#30340;&#26041;&#38754;&#35789;&#27719;&#19982;&#28304;&#39046;&#22495;&#30340;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis (ABSA) aims at extracting opinionated aspect terms in review texts and determining their sentiment polarities, which is widely studied in both academia and industry. As a fine-grained classification task, the annotation cost is extremely high. Domain adaptation is a popular solution to alleviate the data deficiency issue in new domains by transferring common knowledge across domains. Most cross-domain ABSA studies are based on structure correspondence learning (SCL), and use pivot features to construct auxiliary tasks for narrowing down the gap between domains. However, their pivot-based auxiliary tasks can only transfer knowledge of aspect terms but not sentiment, limiting the performance of existing models. In this work, we propose a novel Syntax-guided Domain Adaptation Model, named SDAM, for more effective cross-domain ABSA. SDAM exploits syntactic structure similarities for building pseudo training instances, during which aspect terms of target doma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#21160;&#24577;&#22270;&#30340;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;DyTed&#65292;&#36890;&#36807;&#35774;&#35745;&#26102;&#38388;&#29255;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#21644;&#32467;&#26500;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#26102;&#38388;&#19981;&#21464;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20998;&#31163;&#24863;&#30693;&#21028;&#21035;&#22120;&#20197;&#22686;&#24378;&#20998;&#31163;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.10592</link><description>&lt;p&gt;
DyTed:&#31163;&#25955;&#26102;&#21160;&#24577;&#22270;&#30340;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DyTed: Disentangled Representation Learning for Discrete-time Dynamic Graph. (arXiv:2210.10592v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#21160;&#24577;&#22270;&#30340;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;DyTed&#65292;&#36890;&#36807;&#35774;&#35745;&#26102;&#38388;&#29255;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#21644;&#32467;&#26500;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#26102;&#38388;&#19981;&#21464;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20998;&#31163;&#24863;&#30693;&#21028;&#21035;&#22120;&#20197;&#22686;&#24378;&#20998;&#31163;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26080;&#30417;&#30563;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#20851;&#27880;&#12290;&#19982;&#38745;&#24577;&#22270;&#30456;&#27604;&#65292;&#21160;&#24577;&#22270;&#26082;&#20307;&#29616;&#20102;&#33410;&#28857;&#30340;&#20869;&#22312;&#31283;&#23450;&#29305;&#24449;&#65292;&#21448;&#20307;&#29616;&#20102;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#21160;&#24577;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;&#36825;&#20004;&#31181;&#20449;&#24687;&#28151;&#21512;&#21040;&#19968;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35299;&#37322;&#24615;&#24046;&#12289;&#40065;&#26834;&#24615;&#24046;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#21160;&#24577;&#22270;&#30340;&#26032;&#22411;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;DyTed&#12290;&#25105;&#20204;&#29305;&#21035;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#29255;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#20102;&#32467;&#26500;&#23545;&#27604;&#23398;&#20064;&#65292;&#20998;&#21035;&#26377;&#25928;&#22320;&#35782;&#21035;&#26102;&#38388;&#19981;&#21464;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#20004;&#31181;&#34920;&#31034;&#30340;&#20998;&#31163;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31163;&#24863;&#30693;&#21028;&#21035;&#22120;...
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning for dynamic graphs has attracted a lot of research attention in recent years. Compared with static graph, the dynamic graph is a comprehensive embodiment of both the intrinsic stable characteristics of nodes and the time-related dynamic preference. However, existing methods generally mix these two types of information into a single representation space, which may lead to poor explanation, less robustness, and a limited ability when applied to different downstream tasks. To solve the above problems, in this paper, we propose a novel disenTangled representation learning framework for discrete-time Dynamic graphs, namely DyTed. We specially design a temporal-clips contrastive learning task together with a structure contrastive learning to effectively identify the time-invariant and time-varying representations respectively. To further enhance the disentanglement of these two types of representation, we propose a disentanglement-aware discriminator unde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-former&#30340;&#26032;&#22411;transformer&#26550;&#26500;&#65292;&#29992;&#20110;COVID-19&#26399;&#38388;&#30340;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#23458;&#27969;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#33258;&#36866;&#24212;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#23458;&#27969;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21644;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.09043</link><description>&lt;p&gt;
ST-former&#22312;COVID-19&#26399;&#38388;&#29992;&#20110;&#30701;&#26399;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#23458;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ST-former for short-term passenger flow prediction during COVID-19 in urban rail transit system. (arXiv:2210.09043v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-former&#30340;&#26032;&#22411;transformer&#26550;&#26500;&#65292;&#29992;&#20110;COVID-19&#26399;&#38388;&#30340;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#23458;&#27969;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#33258;&#36866;&#24212;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#23458;&#27969;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21644;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#30340;&#23458;&#27969;&#23545;&#20110;&#25552;&#39640;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#24615;&#33021;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#30123;&#24773;&#26399;&#38388;&#12290;&#22914;&#20309;&#21160;&#24577;&#24314;&#27169;&#23458;&#27969;&#30340;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26159;&#23454;&#29616;&#20934;&#30830;&#23458;&#27969;&#39044;&#27979;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;ST-former&#65292;&#19987;&#38376;&#29992;&#20110;COVID-19&#26399;&#38388;&#30340;&#23458;&#27969;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21629;&#21517;&#20026;&#22240;&#26524;&#21367;&#31215;ProbSparse&#33258;&#27880;&#24847;&#65288;CPSA&#65289;&#65292;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#24314;&#27169;&#23458;&#27969;&#30340;&#22810;&#20010;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#25429;&#25417;&#22797;&#26434;&#19988;&#21160;&#24577;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;AMGCN&#65289;&#65292;&#36890;&#36807;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#21033;&#29992;&#22810;&#20010;&#22270;&#26469;&#36827;&#34892;&#24314;&#27169;&#12290;&#21478;&#22806;&#65292;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#27169;&#22359;&#23558;&#23458;&#27969;&#25968;&#25454;&#12289;COVID-19&#30830;&#35786;&#30149;&#20363;&#31561;&#22810;&#31181;&#25968;&#25454;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate passenger flow prediction of urban rail transit is essential for improving the performance of intelligent transportation systems, especially during the epidemic. How to dynamically model the complex spatiotemporal dependencies of passenger flow is the main issue in achieving accurate passenger flow prediction during the epidemic. To solve this issue, this paper proposes a brand-new transformer-based architecture called STformer under the encoder-decoder framework specifically for COVID-19. Concretely, we develop a modified self-attention mechanism named Causal-Convolution ProbSparse Self-Attention (CPSA) to model the multiple temporal dependencies of passenger flow with low computational costs. To capture the complex and dynamic spatial dependencies, we introduce a novel Adaptive Multi-Graph Convolution Network (AMGCN) by leveraging multiple graphs in a self-adaptive manner. Additionally, the Multi-source Data Fusion block fuses the passenger flow data, COVID-19 confirmed case
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#20984;&#21644;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Kullback Leibler&#25955;&#24230;&#32422;&#26463;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#30340;&#22797;&#26434;&#24230;&#65292;&#27599;&#27425;&#36845;&#20195;&#21482;&#38656;&#35201;&#24658;&#23450;&#30340;&#25209;&#27425;&#22823;&#23567;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#38750;&#20984;&#21644;&#20984;&#32422;&#26463;DRO&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05740</link><description>&lt;p&gt;
&#38543;&#26426;&#32422;&#26463;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#30340;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Stochastic Constrained DRO with a Complexity Independent of Sample Size. (arXiv:2210.05740v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#20984;&#21644;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Kullback Leibler&#25955;&#24230;&#32422;&#26463;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#30340;&#22797;&#26434;&#24230;&#65292;&#27599;&#27425;&#36845;&#20195;&#21482;&#38656;&#35201;&#24658;&#23450;&#30340;&#25209;&#27425;&#22823;&#23567;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#38750;&#20984;&#21644;&#20984;&#32422;&#26463;DRO&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;(DRO)&#20316;&#20026;&#19968;&#31181;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#36827;&#34892;&#20998;&#24067;&#20559;&#31227;&#35757;&#32451;&#40065;&#26834;&#27169;&#22411;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;Kullback Leibler&#25955;&#24230;&#32422;&#26463;DRO&#38382;&#39064;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#38750;&#20984;&#21644;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38543;&#26426;&#31639;&#27861;&#19981;&#20165;&#20855;&#26377;&#19982;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#30340;&#31454;&#20105;&#24615;&#29978;&#33267;&#26356;&#22909;&#30340;&#22797;&#26434;&#24230;&#65292;&#32780;&#19988;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#21482;&#38656;&#35201;&#24658;&#23450;&#30340;&#25209;&#27425;&#22823;&#23567;&#65292;&#36825;&#23545;&#20110;&#24191;&#27867;&#24212;&#29992;&#26356;&#21152;&#23454;&#29992;&#12290;&#25105;&#20204;&#20026;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#25214;&#21040;&#20102;&#19968;&#20010;$\epsilon$&#31283;&#23450;&#35299;&#30340;&#36817;&#20046;&#26368;&#20248;&#30340;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#20026;&#20984;&#25439;&#22833;&#20989;&#25968;&#25214;&#21040;&#20102;&#19968;&#20010;$\epsilon$&#26368;&#20248;&#35299;&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#25152;&#25552;&#31639;&#27861;&#22312;&#35299;&#20915;&#38750;&#20984;&#21644;&#20984;&#32422;&#26463;DRO&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Optimization (DRO), as a popular method to train robust models against distribution shift between training and test sets, has received tremendous attention in recent years. In this paper, we propose and analyze stochastic algorithms that apply to both non-convex and convex losses for solving Kullback Leibler divergence constrained DRO problem. Compared with existing methods solving this problem, our stochastic algorithms not only enjoy competitive if not better complexity independent of sample size but also just require a constant batch size at every iteration, which is more practical for broad applications. We establish a nearly optimal complexity bound for finding an $\epsilon$ stationary solution for non-convex losses and an optimal complexity for finding an $\epsilon$ optimal solution for convex losses. Empirical studies demonstrate the effectiveness of the proposed algorithms for solving non-convex and convex constrained DRO problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#35299;&#37322;&#34920;&#31034;&#20026;&#20174;&#32473;&#23450;&#26597;&#35810;&#29983;&#25104;&#30340;&#31243;&#24207;&#65292;&#36890;&#36807;&#22810;&#27425;&#23637;&#24320;&#31867;&#21464;&#25442;&#26469;&#29983;&#25104;&#31243;&#24207;&#65292;&#20174;&#32780;&#26174;&#24335;&#23637;&#31034;&#20986;&#35777;&#26126;&#32473;&#23450;&#26597;&#35810;&#30340;&#25512;&#29702;&#38142;&#12290;</title><link>http://arxiv.org/abs/2210.03021</link><description>&lt;p&gt;
&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#20013;&#20316;&#20026;&#31243;&#24207;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explanations as Programs in Probabilistic Logic Programming. (arXiv:2210.03021v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#35299;&#37322;&#34920;&#31034;&#20026;&#20174;&#32473;&#23450;&#26597;&#35810;&#29983;&#25104;&#30340;&#31243;&#24207;&#65292;&#36890;&#36807;&#22810;&#27425;&#23637;&#24320;&#31867;&#21464;&#25442;&#26469;&#29983;&#25104;&#31243;&#24207;&#65292;&#20174;&#32780;&#26174;&#24335;&#23637;&#31034;&#20986;&#35777;&#26126;&#32473;&#23450;&#26597;&#35810;&#30340;&#25512;&#29702;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20197;&#29983;&#25104;&#26131;&#29702;&#35299;&#30340;&#35299;&#37322;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#19968;&#39033;&#37325;&#35201;&#21151;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#25193;&#23637;&#20102;&#36923;&#36753;&#32534;&#31243;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20855;&#26377;&#20851;&#31995;&#32467;&#26500;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#35299;&#37322;&#34920;&#31034;&#20026;&#20174;&#32473;&#23450;&#26597;&#35810;&#29983;&#25104;&#30340;&#31243;&#24207;&#65292;&#36890;&#36807;&#22810;&#27425;&#23637;&#24320;&#31867;&#21464;&#25442;&#26469;&#29983;&#25104;&#31243;&#24207;&#12290;&#36825;&#26679;&#65292;&#35777;&#26126;&#32473;&#23450;&#26597;&#35810;&#30340;&#25512;&#29702;&#38142;&#34987;&#26174;&#24335;&#22320;&#23637;&#31034;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of comprehensible explanations is an essential feature of modern artificial intelligence systems. In this work, we consider probabilistic logic programming, an extension of logic programming which can be useful to model domains with relational structure and uncertainty. Essentially, a program specifies a probability distribution over possible worlds (i.e., sets of facts). The notion of explanation is typically associated with that of a world, so that one often looks for the most probable world as well as for the worlds where the query is true. Unfortunately, such explanations exhibit no causal structure. In particular, the chain of inferences required for a specific prediction (represented by a query) is not shown. In this paper, we propose a novel approach where explanations are represented as programs that are generated from a given query by a number of unfolding-like transformations. Here, the chain of inferences that proves a given query is made explicit. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;&#24341;&#21147;&#27874;&#20449;&#21495;&#26816;&#27979;&#21644;&#25552;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#22122;&#22768;&#20013;&#35782;&#21035;&#21508;&#31181;&#28304;&#30340;&#20449;&#21495;&#65292;&#36798;&#21040;&#20102;&#36229;&#36807;99%&#30340;&#26816;&#27979;&#29575;&#21644;&#33267;&#23569;95%&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#34394;&#35686;&#29575;&#21644;&#24378;&#22823;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2207.07414</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;&#24341;&#21147;&#27874;&#20449;&#21495;&#26816;&#27979;&#21644;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Space-based gravitational wave signal detection and extraction with deep neural network. (arXiv:2207.07414v3 [gr-qc] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;&#24341;&#21147;&#27874;&#20449;&#21495;&#26816;&#27979;&#21644;&#25552;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#22122;&#22768;&#20013;&#35782;&#21035;&#21508;&#31181;&#28304;&#30340;&#20449;&#21495;&#65292;&#36798;&#21040;&#20102;&#36229;&#36807;99%&#30340;&#26816;&#27979;&#29575;&#21644;&#33267;&#23569;95%&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#34394;&#35686;&#29575;&#21644;&#24378;&#22823;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#24341;&#21147;&#27874;&#25506;&#27979;&#22120;&#23558;&#33021;&#22815;&#35266;&#27979;&#21040;&#22320;&#38754;&#25506;&#27979;&#22120;&#20960;&#20046;&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#20449;&#21495;&#28304;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#20449;&#21495;&#26816;&#27979;&#26041;&#27861;&#65292;&#21305;&#37197;&#28388;&#27874;&#65292;&#23558;&#38656;&#35201;&#19968;&#20010;&#22797;&#26434;&#30340;&#27169;&#26495;&#24211;&#65292;&#23548;&#33268;&#20102;&#23454;&#38469;&#19978;&#36807;&#20110;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#29992;&#20110;&#25152;&#26377;&#31354;&#38388;&#24341;&#21147;&#27874;&#28304;&#30340;&#20449;&#21495;&#26816;&#27979;&#21644;&#25552;&#21462;&#26041;&#27861;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31185;&#23398;&#39537;&#21160;&#21644;&#32479;&#19968;&#30340;&#22810;&#38454;&#27573;&#33258;&#27880;&#24847;&#21147;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#35782;&#21035;&#22312;&#39640;&#26031;&#22122;&#22768;&#20013;&#28508;&#22312;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#26469;&#33258;&#19981;&#21516;&#28304;&#30340;&#20449;&#21495;&#26102;&#34920;&#29616;&#20986;&#36229;&#36807;99%&#30340;&#26816;&#27979;&#29575;&#65292;&#20449;&#22122;&#27604;&#20026;50&#65292;&#34394;&#35686;&#29575;&#20026;1%&#65292;&#19982;&#30446;&#26631;&#20449;&#21495;&#30456;&#27604;&#33267;&#23569;&#36798;&#21040;95%&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#20960;&#20010;&#25193;&#23637;&#22330;&#26223;&#19979;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24378;&#22823;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Space-based gravitational wave (GW) detectors will be able to observe signals from sources that are otherwise nearly impossible from current ground-based detection. Consequently, the well established signal detection method, matched filtering, will require a complex template bank, leading to a computational cost that is too expensive in practice. Here, we develop a high-accuracy GW signal detection and extraction method for all space-based GW sources. As a proof of concept, we show that a science-driven and uniform multi-stage self-attention-based deep neural network can identify synthetic signals that are submerged in Gaussian noise. Our method exhibits a detection rate exceeding 99% in identifying signals from various sources, with the signal-to-noise ratio at 50, at a false alarm rate of 1%. while obtaining at least 95% similarity compared with target signals. We further demonstrate the interpretability and strong generalization behavior for several extended scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35748;&#35777;&#23545;&#31216;&#24615;&#21644;&#20248;&#20808;&#32423;&#31361;&#30772;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#39564;&#35777;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#27714;&#35299;&#20013;&#30340;&#19968;&#33324;&#23545;&#31216;&#24615;&#31361;&#30772;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#26368;&#22823;&#22242;&#27714;&#35299;&#21644;&#32422;&#26463;&#32534;&#31243;&#31561;&#32452;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.12275</link><description>&lt;p&gt;
&#35748;&#35777;&#30340;&#23545;&#31216;&#24615;&#21644;&#20248;&#20808;&#32423;&#31361;&#30772;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Certified Symmetry and Dominance Breaking for Combinatorial Optimisation. (arXiv:2203.12275v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35748;&#35777;&#23545;&#31216;&#24615;&#21644;&#20248;&#20808;&#32423;&#31361;&#30772;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#39564;&#35777;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#27714;&#35299;&#20013;&#30340;&#19968;&#33324;&#23545;&#31216;&#24615;&#31361;&#30772;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#26368;&#22823;&#22242;&#27714;&#35299;&#21644;&#32422;&#26463;&#32534;&#31243;&#31561;&#32452;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#21644;&#20248;&#20808;&#32423;&#31361;&#30772;&#23545;&#20110;&#35299;&#20915;&#22256;&#38590;&#30340;&#32452;&#21512;&#25628;&#32034;&#21644;&#20248;&#21270;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#30340;&#27491;&#30830;&#24615;&#26377;&#26102;&#20381;&#36182;&#20110;&#24494;&#22937;&#30340;&#35770;&#35777;&#12290;&#22240;&#27492;&#65292;&#20135;&#29983;&#39640;&#25928;&#30340;&#26426;&#22120;&#21487;&#39564;&#35777;&#35777;&#20070;&#26469;&#35777;&#26126;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#30830;&#35745;&#31639;&#26159;&#21487;&#21462;&#30340;&#12290;&#25105;&#20204;&#22522;&#20110;&#20999;&#21106;&#24179;&#38754;&#35777;&#26126;&#31995;&#32479;&#65292;&#20026;&#21487;&#34920;&#36798;&#23545;&#31216;&#24615;&#21644;&#20248;&#20808;&#32423;&#31361;&#30772;&#30340;&#20248;&#21270;&#38382;&#39064;&#24320;&#21457;&#20102;&#19968;&#31181;&#35748;&#35777;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#25928;&#22320;&#39564;&#35777;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#27714;&#35299;&#20013;&#30340;&#23436;&#20840;&#19968;&#33324;&#23545;&#31216;&#24615;&#31361;&#30772;&#65292;&#20174;&#32780;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#35748;&#35777;&#19968;&#31995;&#21015;&#39640;&#32423;SAT&#25216;&#26415;&#65292;&#21253;&#25324;&#24322;&#25110;&#21644;&#22522;&#25968;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#22823;&#22242;&#27714;&#35299;&#21644;&#32422;&#26463;&#32534;&#31243;&#65292;&#20316;&#20026;&#23545;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#32452;&#21512;&#38382;&#39064;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry and dominance breaking can be crucial for solving hard combinatorial search and optimisation problems, but the correctness of these techniques sometimes relies on subtle arguments. For this reason, it is desirable to produce efficient, machine-verifiable certificates that solutions have been computed correctly. Building on the cutting planes proof system, we develop a certification method for optimisation problems in which symmetry and dominance breaking are easily expressible. Our experimental evaluation demonstrates that we can efficiently verify fully general symmetry breaking in Boolean satisfiability (SAT) solving, thus providing, for the first time, a unified method to certify a range of advanced SAT techniques that also includes XOR and cardinality reasoning. In addition, we apply our method to maximum clique solving and constraint programming as a proof of concept that the approach applies to a wider range of combinatorial problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#20998;&#36776;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;SMGRL&#65289;&#65292;&#36890;&#36807;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#24212;&#29992;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#22810;&#20998;&#36776;&#29575;&#33410;&#28857;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2201.12670</link><description>&lt;p&gt;
SMGRL&#65306;&#21487;&#25193;&#23637;&#30340;&#22810;&#20998;&#36776;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SMGRL: Scalable Multi-resolution Graph Representation Learning. (arXiv:2201.12670v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#20998;&#36776;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;SMGRL&#65289;&#65292;&#36890;&#36807;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#24212;&#29992;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#22810;&#20998;&#36776;&#29575;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#25299;&#25169;&#24863;&#30693;&#33021;&#21147;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23545;&#20110;&#20998;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#38500;&#38750;&#28155;&#21152;&#39069;&#22806;&#30340;&#23618;&#27425;&#8212;&#8212;&#32780;&#36825;&#21448;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#21644;&#26102;&#38388;&#31354;&#38388;&#22797;&#26434;&#24230;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#20351;&#24471;&#23567;&#25209;&#37327;&#22788;&#29702;&#21464;&#24471;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#22411;&#22270;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#20998;&#36776;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;SMGRL&#65289;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#22810;&#20998;&#36776;&#29575;&#33410;&#28857;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;GCN&#27169;&#22411;&#12290;&#36890;&#36807;&#20165;&#22312;&#21407;&#22987;&#22270;&#30340;&#38477;&#32500;&#31895;&#21270;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#24212;&#29992;&#25152;&#24471;&#31639;&#27861;&#65292;&#25105;&#20204;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;&#26368;&#32456;&#30340;&#22810;&#20998;&#36776;&#29575;&#23884;&#20837;&#21487;&#20197;&#32858;&#21512;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) allow us to learn topologically-aware node embeddings, which can be useful for classification or link prediction. However, they are unable to capture long-range dependencies between nodes without adding additional layers -- which in turn leads to over-smoothing and increased time and space complexity. Further, the complex dependencies between nodes make mini-batching challenging, limiting their applicability to large graphs. We propose a Scalable Multi-resolution Graph Representation Learning (SMGRL) framework that enables us to learn multi-resolution node embeddings efficiently. Our framework is model-agnostic and can be applied to any existing GCN model. We dramatically reduce training costs by training only on a reduced-dimension coarsening of the original graph, then exploit self-similarity to apply the resulting algorithm at multiple resolutions. The resulting multi-resolution embeddings can be aggregated to yield high-quality node embeddings th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#33258;&#30417;&#30563;&#22312;&#22788;&#29702;&#35821;&#35328;&#20559;&#35265;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#22235;&#20010;&#20559;&#35265;&#20219;&#21153;&#65288;&#35786;&#26029;&#12289;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#25913;&#20889;&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#21035;&#30340;&#20219;&#21153;&#25551;&#36848;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2112.08637</link><description>&lt;p&gt;
&#20998;&#26512;&#33258;&#30417;&#30563;&#22312;&#22788;&#29702;&#35821;&#35328;&#20559;&#35265;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Limits of Self-Supervision in Handling Bias in Language. (arXiv:2112.08637v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#33258;&#30417;&#30563;&#22312;&#22788;&#29702;&#35821;&#35328;&#20559;&#35265;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#22235;&#20010;&#20559;&#35265;&#20219;&#21153;&#65288;&#35786;&#26029;&#12289;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#25913;&#20889;&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#21035;&#30340;&#20219;&#21153;&#25551;&#36848;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#36755;&#20837;&#24050;&#25104;&#20026;&#20174;&#22823;&#35268;&#27169;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#30456;&#23545;&#20934;&#30830;&#36755;&#20986;&#30340;&#27969;&#34892;&#26426;&#21046;&#65292;&#32780;&#21516;&#26102;&#21448;&#20960;&#20046;&#27809;&#26377;&#19978;&#19979;&#25991;&#30417;&#30563;&#12290;&#36825;&#20063;&#26377;&#21161;&#20110;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#20174;&#26080;&#26631;&#35760;&#25991;&#26412;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#32431;&#31929;&#25429;&#25417;&#19979;&#28216;&#20219;&#21153;&#30340;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#33258;&#28982;&#20063;&#26292;&#38706;&#20110;&#35768;&#22810;&#19981;&#24076;&#26395;&#30340;&#20869;&#23481;&#65292;&#22914;&#31181;&#26063;&#20027;&#20041;&#21644;&#24615;&#21035;&#27495;&#35270;&#30340;&#35821;&#35328;&#65292;&#30446;&#21069;&#23545;&#27169;&#22411;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#24847;&#35782;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#20840;&#38754;&#35780;&#20272;&#36825;&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;&#22235;&#20010;&#20559;&#35265;&#20219;&#21153;&#65288;&#35786;&#26029;&#12289;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#25913;&#20889;&#65289;&#20013;&#25429;&#25417;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19977;&#31867;&#20219;&#21153;&#25551;&#36848;&#65306;&#38472;&#36848;&#12289;&#38382;&#39064;&#21644;&#23436;&#25104;&#65292;&#24182;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#20351;&#29992;&#20102;&#35768;&#22810;&#35789;&#27719;&#21464;&#20307;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#36825;&#20123;&#20219;&#21153;&#25551;&#36848;&#30340;&#25552;&#31034;&#23545;&#27599;&#20010;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting inputs with natural language task descriptions has emerged as a popular mechanism to elicit reasonably accurate outputs from large-scale generative language models with little to no in-context supervision. This also helps gain insight into how well language models capture the semantics of a wide range of downstream tasks purely from self-supervised pre-training on massive corpora of unlabeled text. Such models have naturally also been exposed to a lot of undesirable content like racist and sexist language and there is limited work on awareness of models along these dimensions. In this paper, we define and comprehensively evaluate how well such language models capture the semantics of four tasks for bias: diagnosis, identification, extraction and rephrasing. We define three broad classes of task descriptions for these tasks: statement, question, and completion, with numerous lexical variants within each class. We study the efficacy of prompting for each task using these classe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23494;&#38598;&#26862;&#26519;&#26519;&#20896;&#19979;&#36827;&#34892;&#22823;&#35268;&#27169;&#33258;&#20027;&#39134;&#34892;&#21644;&#23454;&#26102;&#35821;&#20041;&#22320;&#22270;&#26500;&#24314;&#12290;&#31995;&#32479;&#20351;&#29992;LiDAR&#25968;&#25454;&#26816;&#27979;&#21644;&#24314;&#27169;&#26641;&#24178;&#21644;&#22320;&#38754;&#24179;&#38754;&#65292;&#24182;&#21033;&#29992;&#22810;&#32423;&#35268;&#21010;&#21644;&#22320;&#22270;&#26500;&#24314;&#26694;&#26550;&#35745;&#31639;&#21160;&#24577;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#20197;&#26500;&#24314;&#29992;&#25143;&#23450;&#20041;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#24182;&#36890;&#36807;&#35821;&#20041;SLAM&#26469;&#26368;&#23567;&#21270;&#37324;&#31243;&#35745;&#28418;&#31227;&#12290;</title><link>http://arxiv.org/abs/2109.06479</link><description>&lt;p&gt;
&#23494;&#38598;&#26862;&#26519;&#26519;&#20896;&#19979;&#30340;&#22823;&#35268;&#27169;&#33258;&#20027;&#39134;&#34892;&#19982;&#23454;&#26102;&#35821;&#20041;SLAM
&lt;/p&gt;
&lt;p&gt;
Large-scale Autonomous Flight with Real-time Semantic SLAM under Dense Forest Canopy. (arXiv:2109.06479v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.06479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23494;&#38598;&#26862;&#26519;&#26519;&#20896;&#19979;&#36827;&#34892;&#22823;&#35268;&#27169;&#33258;&#20027;&#39134;&#34892;&#21644;&#23454;&#26102;&#35821;&#20041;&#22320;&#22270;&#26500;&#24314;&#12290;&#31995;&#32479;&#20351;&#29992;LiDAR&#25968;&#25454;&#26816;&#27979;&#21644;&#24314;&#27169;&#26641;&#24178;&#21644;&#22320;&#38754;&#24179;&#38754;&#65292;&#24182;&#21033;&#29992;&#22810;&#32423;&#35268;&#21010;&#21644;&#22320;&#22270;&#26500;&#24314;&#26694;&#26550;&#35745;&#31639;&#21160;&#24577;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#20197;&#26500;&#24314;&#29992;&#25143;&#23450;&#20041;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#24182;&#36890;&#36807;&#35821;&#20041;SLAM&#26469;&#26368;&#23567;&#21270;&#37324;&#31243;&#35745;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22320;&#22270;&#20351;&#29992;&#19968;&#32452;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#23545;&#35937;&#34920;&#31034;&#29615;&#22659;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#24335;&#22312;&#23384;&#20648;&#25928;&#29575;&#12289;&#27495;&#20041;&#24615;&#20943;&#23569;&#21644;&#20449;&#24687;&#20016;&#23500;&#24230;&#26041;&#38754;&#26356;&#22909;&#65292;&#20026;&#22312;&#39640;&#24230;&#26080;&#32467;&#26500;&#12289;&#26080;GPS&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#33258;&#27835;&#39134;&#34892;&#21644;&#33719;&#21462;&#21487;&#25805;&#20316;&#20449;&#24687;&#25552;&#20379;&#26041;&#20415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26519;&#20896;&#19979;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#33258;&#20027;&#39134;&#34892;&#21644;&#23454;&#26102;&#35821;&#20041;&#22320;&#22270;&#26500;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;LiDAR&#25968;&#25454;&#26816;&#27979;&#21644;&#24314;&#27169;&#26641;&#24178;&#21644;&#22320;&#38754;&#24179;&#38754;&#65292;&#36825;&#20123;&#20449;&#24687;&#19982;&#25195;&#25551;&#25968;&#25454;&#30456;&#20851;&#32852;&#65292;&#24182;&#29992;&#20110;&#32422;&#26463;&#26426;&#22120;&#20154;&#23039;&#24577;&#21644;&#26641;&#24178;&#27169;&#22411;&#12290;&#33258;&#20027;&#23548;&#33322;&#27169;&#22359;&#21033;&#29992;&#22810;&#32423;&#35268;&#21010;&#21644;&#22320;&#22270;&#26500;&#24314;&#26694;&#26550;&#65292;&#20197;&#35745;&#31639;&#21160;&#24577;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#20351;&#26080;&#20154;&#26426;&#20197;&#35745;&#31639;&#21644;&#23384;&#20648;&#26377;&#25928;&#30340;&#26041;&#24335;&#26500;&#24314;&#29992;&#25143;&#23450;&#20041;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#35821;&#20041;&#22320;&#22270;&#12290;&#35774;&#35745;&#20102;&#28418;&#31227;&#34917;&#20607;&#26426;&#21046;&#65292;&#36890;&#36807;&#35821;&#20041;SLAM&#26469;&#26368;&#23567;&#21270;&#37324;&#31243;&#35745;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic maps represent the environment using a set of semantically meaningful objects. This representation is storage-efficient, less ambiguous, and more informative, thus facilitating large-scale autonomy and the acquisition of actionable information in highly unstructured, GPS-denied environments. In this letter, we propose an integrated system that can perform large-scale autonomous flights and real-time semantic mapping in challenging under-canopy environments. We detect and model tree trunks and ground planes from LiDAR data, which are associated across scans and used to constrain robot poses as well as tree trunk models. The autonomous navigation module utilizes a multi-level planning and mapping framework and computes dynamically feasible trajectories that lead the UAV to build a semantic map of the user-defined region of interest in a computationally and storage efficient manner. A drift-compensation mechanism is designed to minimize the odometry drift using semantic SLAM outp
&lt;/p&gt;</description></item></channel></rss>