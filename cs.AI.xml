<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#25105;&#26159;&#25105;&#33402;&#26415;&#30340;&#36873;&#25321;"&#65288;MAMC&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#25239;&#25163;&#27573;&#20445;&#25252;&#33402;&#26415;&#20316;&#21697;&#30340;&#29256;&#26435;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#24615;&#25200;&#21160;&#21518;&#30340;&#22270;&#20687;&#26469;"&#30772;&#35299;"&#25193;&#25955;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24179;&#34913;&#20869;&#23481;&#30340;&#22833;&#30495;&#21644;&#20445;&#25252;&#65292;&#24182;&#19988;&#21487;&#20197;&#25915;&#20987;&#40657;&#30418;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.03198</link><description>&lt;p&gt;
&#25105;&#30340;&#33402;&#26415;&#25105;&#30340;&#36873;&#25321;&#65306;&#23545;&#25239;&#19981;&#23432;&#35268;&#30697;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
My Art My Choice: Adversarial Protection Against Unruly AI. (arXiv:2309.03198v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03198
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#25105;&#26159;&#25105;&#33402;&#26415;&#30340;&#36873;&#25321;"&#65288;MAMC&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#25239;&#25163;&#27573;&#20445;&#25252;&#33402;&#26415;&#20316;&#21697;&#30340;&#29256;&#26435;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#24615;&#25200;&#21160;&#21518;&#30340;&#22270;&#20687;&#26469;"&#30772;&#35299;"&#25193;&#25955;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24179;&#34913;&#20869;&#23481;&#30340;&#22833;&#30495;&#21644;&#20445;&#25252;&#65292;&#24182;&#19988;&#21487;&#20197;&#25915;&#20987;&#40657;&#30418;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23835;&#36215;&#20351;&#24471;&#27599;&#20010;&#20154;&#37117;&#33021;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#25509;&#21475;&#20135;&#29983;&#36924;&#30495;&#30340;&#20869;&#23481;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#24341;&#23548;&#24335;&#22270;&#20687;&#29983;&#25104;&#65292;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#20135;&#29983;&#39640;&#36136;&#37327;&#20302;&#25104;&#26412;&#30340;&#20869;&#23481;&#25913;&#21464;&#20102;&#21019;&#20316;&#32773;&#32463;&#27982;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#33402;&#26415;&#23478;&#20204;&#27491;&#22312;&#23545;&#25239;&#19981;&#23432;&#35268;&#30697;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#22240;&#20026;&#20182;&#20204;&#30340;&#20316;&#21697;&#34987;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#21033;&#29992;&#12289;&#20998;&#21457;&#21644;&#20266;&#35013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;"&#25105;&#26159;&#25105;&#33402;&#26415;&#30340;&#36873;&#25321;"&#65288;MAMC&#65289;&#26088;&#22312;&#36890;&#36807;&#23545;&#25239;&#25163;&#27573;&#20445;&#25252;&#29256;&#26435;&#26448;&#26009;&#65292;&#20351;&#20869;&#23481;&#25152;&#26377;&#32773;&#33021;&#22815;&#33719;&#24471;&#26356;&#22810;&#26435;&#21147;&#12290;MAMC&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#24615;&#25200;&#21160;&#21518;&#30340;"&#21463;&#20445;&#25252;"&#29256;&#26412;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;"&#30772;&#35299;"&#25193;&#25955;&#27169;&#22411;&#12290;&#25200;&#21160;&#37327;&#30001;&#33402;&#26415;&#23478;&#20915;&#23450;&#65292;&#20197;&#24179;&#34913;&#20869;&#23481;&#30340;&#22833;&#30495;&#21644;&#20445;&#25252;&#12290;MAMC&#37319;&#29992;&#22522;&#20110;UNet&#30340;&#31616;&#21333;&#29983;&#25104;&#22120;&#65292;&#25915;&#20987;&#40657;&#30418;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#22810;&#20010;&#25439;&#22833;&#20989;&#25968;&#21019;&#24314;&#21407;&#22987;&#33402;&#26415;&#20316;&#21697;&#30340;&#23545;&#25239;&#24615;&#21452;&#32990;&#32974;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI is on the rise, enabling everyone to produce realistic content via publicly available interfaces. Especially for guided image generation, diffusion models are changing the creator economy by producing high quality low cost content. In parallel, artists are rising against unruly AI, since their artwork are leveraged, distributed, and dissimulated by large generative models. Our approach, My Art My Choice (MAMC), aims to empower content owners by protecting their copyrighted materials from being utilized by diffusion models in an adversarial fashion. MAMC learns to generate adversarially perturbed "protected" versions of images which can in turn "break" diffusion models. The perturbation amount is decided by the artist to balance distortion vs. protection of the content. MAMC is designed with a simple UNet-based generator, attacking black box diffusion models, combining several losses to create adversarial twins of the original artwork. We experiment on three datasets for v
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#21106;&#22686;&#24378;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#33258;&#21160;&#21253;&#21547;&#27491;&#21017;&#21270;&#34892;&#20026;&#65292;&#38477;&#20302;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.03167</link><description>&lt;p&gt;
&#20998;&#21106;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Split-Boost Neural Networks. (arXiv:2309.03167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#21106;&#22686;&#24378;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#33258;&#21160;&#21253;&#21547;&#27491;&#21017;&#21270;&#34892;&#20026;&#65292;&#38477;&#20302;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#21644;&#35757;&#32451;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20851;&#38190;&#38556;&#30861;&#26159;&#38656;&#35201;&#36873;&#25321;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#21069;&#39304;&#26550;&#26500;&#35757;&#32451;&#31574;&#30053;&#65292;&#31216;&#20026;&#20998;&#21106;&#22686;&#24378;&#65288;split-boost&#65289;&#65292;&#23427;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#24182;&#33258;&#21160;&#21253;&#21547;&#19968;&#31181;&#27491;&#21017;&#21270;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#22320;&#24314;&#27169;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26368;&#32456;&#20351;&#25105;&#20204;&#33021;&#22815;&#36991;&#20813;&#26174;&#24335;&#24314;&#27169;&#27491;&#21017;&#21270;&#39033;&#65292;&#20943;&#23569;&#24635;&#30340;&#36229;&#21442;&#25968;&#25968;&#37327;&#24182;&#21152;&#36895;&#35843;&#20248;&#38454;&#27573;&#12290;&#35813;&#31574;&#30053;&#22312;&#19968;&#20010;&#21311;&#21517;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24212;&#29992;&#20110;&#22522;&#20934;&#21307;&#30103;&#20445;&#38505;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The calibration and training of a neural network is a complex and time-consuming procedure that requires significant computational resources to achieve satisfactory results. Key obstacles are a large number of hyperparameters to select and the onset of overfitting in the face of a small amount of data. In this framework, we propose an innovative training strategy for feed-forward architectures - called split-boost - that improves performance and automatically includes a regularizing behaviour without modeling it explicitly. Such a novel approach ultimately allows us to avoid explicitly modeling the regularization term, decreasing the total number of hyperparameters and speeding up the tuning phase. The proposed strategy is tested on a real-world (anonymized) dataset within a benchmark medical insurance design problem.
&lt;/p&gt;</description></item><item><title>J-Guard&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;AI&#29983;&#25104;&#26032;&#38395;&#20013;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#38395;&#23646;&#24615;&#21644;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#21487;&#38752;&#24615;&#21644;&#35823;&#25253;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03164</link><description>&lt;p&gt;
J-Guard&#65306;&#26032;&#38395;&#25351;&#23548;&#30340;&#23545;&#25239;&#40065;&#26834;AI&#29983;&#25104;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
J-Guard: Journalism Guided Adversarially Robust Detection of AI-generated News. (arXiv:2309.03164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03164
&lt;/p&gt;
&lt;p&gt;
J-Guard&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;AI&#29983;&#25104;&#26032;&#38395;&#20013;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#38395;&#23646;&#24615;&#21644;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#21487;&#38752;&#24615;&#21644;&#35823;&#25253;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#22312;&#32593;&#32476;&#19978;&#30340;&#36805;&#36895;&#25193;&#24352;&#27491;&#22312;&#28145;&#21051;&#22320;&#25913;&#21464;&#20449;&#24687;&#26684;&#23616;&#12290;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#20013;&#65292;AI&#29983;&#25104;&#26032;&#38395;&#20316;&#20026;&#19968;&#31181;&#26174;&#33879;&#30340;&#26469;&#28304;&#65292;&#23545;&#32593;&#32476;&#19978;&#30340;&#35823;&#23548;&#20449;&#24687;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#19968;&#20123;&#21162;&#21147;&#33268;&#21147;&#20110;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#20294;&#32771;&#34385;&#21040;&#20854;&#38754;&#20020;&#30340;&#31616;&#21333;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#26356;&#39640;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26032;&#38395;&#20889;&#20316;&#30340;&#29305;&#24322;&#24615;&#65292;&#23558;&#36825;&#20123;&#26816;&#27979;&#26041;&#27861;&#24212;&#29992;&#20110;AI&#29983;&#25104;&#26032;&#38395;&#21487;&#33021;&#20250;&#20135;&#29983;&#35823;&#25253;&#65292;&#28508;&#22312;&#22320;&#30772;&#22351;&#26032;&#38395;&#26426;&#26500;&#30340;&#22768;&#35465;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#36328;&#23398;&#31185;&#22242;&#38431;&#30340;&#19987;&#38376;&#30693;&#35782;&#26469;&#24320;&#21457;&#19968;&#20010;&#21517;&#20026;J-Guard&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#24341;&#23548;&#29616;&#26377;&#30340;&#22522;&#20110;&#30417;&#30563;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#20197;&#26816;&#27979;AI&#29983;&#25104;&#26032;&#38395;&#65292;&#24182;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#21463;&#29420;&#29305;&#26032;&#38395;&#23646;&#24615;&#21551;&#21457;&#30340;&#25991;&#20307;&#26263;&#31034;&#65292;J-Guard&#33021;&#26377;&#25928;&#25269;&#21046;&#23545;&#25239;&#25915;&#20987;&#24182;&#38477;&#20302;&#35823;&#25253;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid proliferation of AI-generated text online is profoundly reshaping the information landscape. Among various types of AI-generated text, AI-generated news presents a significant threat as it can be a prominent source of misinformation online. While several recent efforts have focused on detecting AI-generated text in general, these methods require enhanced reliability, given concerns about their vulnerability to simple adversarial attacks. Furthermore, due to the eccentricities of news writing, applying these detection methods for AI-generated news can produce false positives, potentially damaging the reputation of news organizations. To address these challenges, we leverage the expertise of an interdisciplinary team to develop a framework, J-Guard, capable of steering existing supervised AI text detectors for detecting AI-generated news while boosting adversarial robustness. By incorporating stylistic cues inspired by the unique journalistic attributes, J-Guard effectively dis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#38477;&#20302;&#35774;&#35745;&#21644;&#36816;&#33829;&#24037;&#20855;&#21253;&#65288;RDOT&#65289;&#20013;&#30340;90&#31181;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#25552;&#20379;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#36825;&#20123;&#31574;&#30053;&#21253;&#25324;&#23558;&#31283;&#20581;&#24615;&#32435;&#20837;&#35774;&#35745;&#12289;&#20107;&#21518;&#39044;&#38450;&#25514;&#26045;&#31561;&#65292;&#33021;&#22815;&#24110;&#21161;&#24037;&#31243;&#24072;&#12289;&#20844;&#20849;&#35268;&#21010;&#32773;&#21644;&#20854;&#20182;&#20915;&#31574;&#32773;&#24212;&#23545;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03133</link><description>&lt;p&gt;
&#39118;&#38505;&#38477;&#20302;&#35774;&#35745;&#21644;&#36816;&#33829;&#24037;&#20855;&#21253;: &#31649;&#29702;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#39118;&#38505;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;90&#31181;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Risk-reducing design and operations toolkit: 90 strategies for managing risk and uncertainty in decision problems. (arXiv:2309.03133v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#38477;&#20302;&#35774;&#35745;&#21644;&#36816;&#33829;&#24037;&#20855;&#21253;&#65288;RDOT&#65289;&#20013;&#30340;90&#31181;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#25552;&#20379;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#36825;&#20123;&#31574;&#30053;&#21253;&#25324;&#23558;&#31283;&#20581;&#24615;&#32435;&#20837;&#35774;&#35745;&#12289;&#20107;&#21518;&#39044;&#38450;&#25514;&#26045;&#31561;&#65292;&#33021;&#22815;&#24110;&#21161;&#24037;&#31243;&#24072;&#12289;&#20844;&#20849;&#35268;&#21010;&#32773;&#21644;&#20854;&#20182;&#20915;&#31574;&#32773;&#24212;&#23545;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#26159;&#20915;&#31574;&#20998;&#26512;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#20915;&#31574;&#29702;&#35770;&#25215;&#35748;&#20004;&#31867;&#35299;&#20915;&#26041;&#26696;: &#27010;&#29575;&#27169;&#22411;&#21644;&#35748;&#30693;&#21551;&#21457;&#24335;&#12290;&#28982;&#32780;&#65292;&#24037;&#31243;&#24072;&#12289;&#20844;&#20849;&#35268;&#21010;&#32773;&#21644;&#20854;&#20182;&#20915;&#31574;&#32773;&#20351;&#29992;&#30340;&#26159;&#21478;&#19968;&#31867;&#34987;&#31216;&#20026;RDOT&#65288;&#39118;&#38505;&#38477;&#20302;&#35774;&#35745;&#21644;&#36816;&#33829;&#24037;&#20855;&#21253;&#65289;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#21253;&#25324;&#23558;&#31283;&#20581;&#24615;&#32435;&#20837;&#35774;&#35745;&#12289;&#20107;&#21518;&#39044;&#38450;&#25514;&#26045;&#31561;&#65292;&#24182;&#19981;&#23646;&#20110;&#27010;&#29575;&#27169;&#22411;&#25110;&#35748;&#30693;&#21551;&#21457;&#24335;&#30340;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#31574;&#30053;&#20986;&#29616;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#23398;&#31185;&#20013;&#65292;&#25351;&#21521;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20849;&#20139;&#24037;&#20855;&#21253;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#24320;&#21457;&#36825;&#20123;&#31574;&#30053;&#30340;&#30446;&#24405;&#24182;&#20026;&#20854;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102;&#36229;&#36807;90&#20010;&#23646;&#20110;&#20845;&#20010;&#24191;&#27867;&#31867;&#21035;&#30340;&#36825;&#26679;&#30340;&#31574;&#30053;&#65292;&#24182;&#35748;&#20026;&#23427;&#20204;&#23545;&#20110;&#30001;&#20110;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#32780;&#20284;&#20046;&#26840;&#25163;&#30340;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#21709;&#24212;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#23427;&#20204;&#32435;&#20837;&#20915;&#31574;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty is a pervasive challenge in decision analysis, and decision theory recognizes two classes of solutions: probabilistic models and cognitive heuristics. However, engineers, public planners and other decision-makers instead use a third class of strategies that could be called RDOT (Risk-reducing Design and Operations Toolkit). These include incorporating robustness into designs, contingency planning, and others that do not fall into the categories of probabilistic models or cognitive heuristics. Moreover, identical strategies appear in several domains and disciplines, pointing to an important shared toolkit.  The focus of this paper is to develop a catalog of such strategies and develop a framework for them. The paper finds more than 90 examples of such strategies falling into six broad categories and argues that they provide an efficient response to decision problems that are seemingly intractable due to high uncertainty. It then proposes a framework to incorporate them into 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20154;&#31867;&#28789;&#24039;&#25805;&#20316;&#30340;&#22810;&#26679;&#20808;&#21069;&#32463;&#39564;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24320;&#21457;&#20102;&#33021;&#22815;&#21033;&#29992;&#20197;&#21069;&#30340;&#32463;&#39564;&#24555;&#36895;&#33719;&#24471;&#26032;&#34892;&#20026;&#30340;&#20195;&#29702;&#32773;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25429;&#25417;&#36890;&#29992;&#30340;&#34892;&#20026;&#20808;&#39564;&#65288;MyoDex&#65289;&#65292;&#24182;&#20351;&#29992;&#20102;&#22522;&#20110;&#29983;&#29702;&#23398;&#30340;&#20154;&#25163;&#27169;&#22411; - MyoHand&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;MyoDex&#30340;&#20195;&#29702;&#32773;&#21487;&#20197;&#35299;&#20915;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03130</link><description>&lt;p&gt;
MyoDex&#65306;&#28789;&#24039;&#25805;&#20316;&#30340;&#36890;&#29992;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
MyoDex: A Generalizable Prior for Dexterous Manipulation. (arXiv:2309.03130v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20154;&#31867;&#28789;&#24039;&#25805;&#20316;&#30340;&#22810;&#26679;&#20808;&#21069;&#32463;&#39564;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24320;&#21457;&#20102;&#33021;&#22815;&#21033;&#29992;&#20197;&#21069;&#30340;&#32463;&#39564;&#24555;&#36895;&#33719;&#24471;&#26032;&#34892;&#20026;&#30340;&#20195;&#29702;&#32773;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25429;&#25417;&#36890;&#29992;&#30340;&#34892;&#20026;&#20808;&#39564;&#65288;MyoDex&#65289;&#65292;&#24182;&#20351;&#29992;&#20102;&#22522;&#20110;&#29983;&#29702;&#23398;&#30340;&#20154;&#25163;&#27169;&#22411; - MyoHand&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;MyoDex&#30340;&#20195;&#29702;&#32773;&#21487;&#20197;&#35299;&#20915;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#28789;&#24039;&#25805;&#20316;&#26159;&#36816;&#21160;&#25511;&#21046;&#30340;&#26631;&#24535;&#12290;&#23613;&#31649;&#39592;&#39612;&#32908;&#24863;&#35273;&#36816;&#21160;&#22238;&#36335;&#30340;&#22797;&#26434;&#24615;&#65288;&#22810;&#20851;&#33410;&#21644;&#22810;&#21160;&#20316;&#65292;&#30001;40&#22810;&#22359;&#32908;&#32905;&#25511;&#21046;&#30340;23&#20010;&#20851;&#33410;&#65289; &#65292;&#20294;&#25105;&#20204;&#30340;&#25163;&#21487;&#20197;&#36805;&#36895;&#21512;&#25104;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#20154;&#31867;&#28789;&#24039;&#25805;&#20316;&#22914;&#20309;&#24314;&#31435;&#22312;&#22810;&#26679;&#30340;&#20808;&#21069;&#32463;&#39564;&#20043;&#19978;&#30340;&#21551;&#21457;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#21333;&#19968;&#20219;&#21153;&#33719;&#24471;&#12290;&#21463;&#27492;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30528;&#25163;&#24320;&#21457;&#33021;&#22815;&#21033;&#29992;&#20197;&#21069;&#30340;&#32463;&#39564;&#24555;&#36895;&#33719;&#24471;&#26032;&#30340;&#65288;&#20197;&#21069;&#26080;&#27861;&#36798;&#21040;&#30340;&#65289;&#34892;&#20026;&#30340;&#20195;&#29702;&#32773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#38544;&#24335;&#22320;&#25429;&#25417;&#38754;&#21521;&#20219;&#21153;&#30340;&#34892;&#20026;&#20808;&#39564;&#65288;MyoDex&#65289; &#65292;&#29992;&#29983;&#29702;&#23454;&#38469;&#30340;&#20154;&#25163;&#27169;&#22411; - MyoHand&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MyoDex&#22312;&#23569;&#26679;&#26412;&#27867;&#21270;&#21644;&#23545;&#19968;&#22823;&#25209;&#26410;&#35265;&#30340;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#30340;&#31215;&#26497;&#36801;&#31227;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#21033;&#29992;MyoDex&#30340;&#20195;&#29702;&#32773;&#21487;&#20197;&#35299;&#20915;&#22823;&#32422;3&#20493;&#30340;&#20219;&#21153;&#65292;4&#20493;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Human dexterity is a hallmark of motor control. Our hands can rapidly synthesize new behaviors despite the complexity (multi-articular and multi-joints, with 23 joints controlled by more than 40 muscles) of musculoskeletal sensory-motor circuits. In this work, we take inspiration from how human dexterity builds on a diversity of prior experiences, instead of being acquired through a single task. Motivated by this observation, we set out to develop agents that can build upon their previous experience to quickly acquire new (previously unattainable) behaviors. Specifically, our approach leverages multi-task learning to implicitly capture task-agnostic behavioral priors (MyoDex) for human-like dexterity, using a physiologically realistic human hand model - MyoHand. We demonstrate MyoDex's effectiveness in few-shot generalization as well as positive transfer to a large repertoire of unseen dexterous manipulation tasks. Agents leveraging MyoDex can solve approximately 3x more tasks, and 4x 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#38177;&#33167;&#26816;&#26597;&#29305;&#24449;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;PCB&#21046;&#36896;&#20013;&#30340;&#19977;&#20010;&#38454;&#27573;&#26816;&#27979;&#32570;&#38519;&#65292;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#21644;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2309.03113</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#38177;&#33167;&#26816;&#26597;&#29305;&#24449;&#19978;&#26816;&#27979;PCB&#21046;&#36896;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Solder Paste Inspection Features. (arXiv:2309.03113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03113
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#38177;&#33167;&#26816;&#26597;&#29305;&#24449;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;PCB&#21046;&#36896;&#20013;&#30340;&#19977;&#20010;&#38454;&#27573;&#26816;&#27979;&#32570;&#38519;&#65292;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#21644;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38177;&#33167;&#26816;&#26597;&#65288;SPI&#65289;&#21644;&#33258;&#21160;&#20809;&#23398;&#26816;&#26597;&#65288;AOI&#65289;&#26426;&#22120;&#33258;&#21160;&#26816;&#27979;&#21360;&#21046;&#30005;&#36335;&#26495;&#65288;PCB&#65289;&#21046;&#36896;&#20013;&#30340;&#32570;&#38519;&#21487;&#20197;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#65292;&#26174;&#33879;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#35201;&#12290;&#26412;&#25991;&#21033;&#29992;&#20174;SPI&#25552;&#21462;&#30340;600&#19975;&#20010;&#24341;&#33050;&#30340;&#29305;&#24449;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20197;&#22312;PCB&#21046;&#36896;&#30340;&#19977;&#20010;&#38454;&#27573;&#26816;&#27979;&#32570;&#38519;&#12290;&#36825;600&#19975;&#20010;PCB&#24341;&#33050;&#23545;&#24212;&#20110;&#23646;&#20110;15,387&#20010;PCB&#30340;200&#19975;&#20010;&#32452;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#30340;ML&#27169;&#22411;&#65292;&#36845;&#20195;&#25968;&#25454;&#39044;&#22788;&#29702;&#27493;&#39588;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#32452;&#20214;&#21644;PCB ID&#32452;&#21512;&#30340;&#24341;&#33050;&#32423;SPI&#29305;&#24449;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#32452;&#20214;&#21644;PCB&#32423;&#21035;&#30340;&#35757;&#32451;&#23454;&#20363;&#12290;&#36825;&#20351;&#24471;ML&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#22312;&#24341;&#33050;&#32423;&#21035;&#21487;&#33021;&#19981;&#26126;&#26174;&#30340;&#24341;&#33050;&#38388;&#12289;&#32452;&#20214;&#38388;&#25110;&#31354;&#38388;&#25928;&#24212;&#12290;&#27169;&#22411;&#22312;&#24341;&#33050;&#12289;&#32452;&#20214;&#21644;PCB&#32423;&#21035;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated detection of defects in Printed Circuit Board (PCB) manufacturing using Solder Paste Inspection (SPI) and Automated Optical Inspection (AOI) machines can help improve operational efficiency and significantly reduce the need for manual intervention. In this paper, using SPI-extracted features of 6 million pins, we demonstrate a data-centric approach to train Machine Learning (ML) models to detect PCB defects at three stages of PCB manufacturing. The 6 million PCB pins correspond to 2 million components that belong to 15,387 PCBs. Using a base extreme gradient boosting (XGBoost) ML model, we iterate on the data pre-processing step to improve detection performance. Combining pin-level SPI features using component and PCB IDs, we developed training instances also at the component and PCB level. This allows the ML model to capture any inter-pin, inter-component, or spatial effects that may not be apparent at the pin level. Models are trained at the pin, component, and PCB levels, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31232;&#30095;&#21152;&#26435;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#22411;&#21333;&#24490;&#29615;&#24179;&#28369;ADMM&#31639;&#27861;&#65292;&#21517;&#20026;SIAD&#65292;&#23427;&#22312;&#23384;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#31232;&#30095;&#24809;&#32602;&#26465;&#20214;&#19979;&#33021;&#22815;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03094</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#20984;&#24809;&#32602;&#30340;&#31232;&#30095;&#21152;&#26435;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#24179;&#28369;ADMM
&lt;/p&gt;
&lt;p&gt;
Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties. (arXiv:2309.03094v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31232;&#30095;&#21152;&#26435;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#26032;&#22411;&#21333;&#24490;&#29615;&#24179;&#28369;ADMM&#31639;&#27861;&#65292;&#21517;&#20026;SIAD&#65292;&#23427;&#22312;&#23384;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#31232;&#30095;&#24809;&#32602;&#26465;&#20214;&#19979;&#33021;&#22815;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#31232;&#30095;&#24809;&#32602;&#26465;&#20214;&#19979;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#65292;&#22914;&#26368;&#23567;&#26368;&#22823;&#20985;&#24809;&#32602;&#65288;MCP&#65289;&#21644;&#24179;&#28369;&#21098;&#20999;&#32477;&#23545;&#20559;&#24046;&#65288;SCAD&#65289;&#12290;&#36825;&#20123;&#38382;&#39064;&#30340;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#29305;&#24615;&#32463;&#24120;&#23548;&#33268;&#35768;&#22810;&#31639;&#27861;&#30340;&#25910;&#25947;&#22256;&#38590;&#12290;&#34429;&#28982;&#36845;&#20195;&#25216;&#26415;&#22914;&#22352;&#26631;&#19979;&#38477;&#21644;&#23616;&#37096;&#32447;&#24615;&#36817;&#20284;&#21487;&#20197;&#20419;&#36827;&#25910;&#25947;&#65292;&#20294;&#36807;&#31243;&#36890;&#24120;&#24456;&#24930;&#12290;&#36825;&#31181;&#32531;&#24930;&#30340;&#36895;&#24230;&#20027;&#35201;&#26159;&#22240;&#20026;&#38656;&#35201;&#22312;&#27599;&#19968;&#27493;&#36816;&#34892;&#36825;&#20123;&#36817;&#20284;&#25216;&#26415;&#30452;&#21040;&#23436;&#20840;&#25910;&#25947;&#65292;&#36825;&#26159;&#25105;&#20204;&#31216;&#20043;&#20026;\emph{&#20108;&#27425;&#25910;&#25947;&#36845;&#20195;}&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#65288;ADMM&#65289;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#36882;&#22686;&#24809;&#32602;&#21442;&#25968;&#30340;&#21333;&#24490;&#29615;&#24179;&#28369;ADMM&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;SIAD&#65292;&#19987;&#38376;&#29992;&#20110;&#31232;&#30095;&#21152;&#26435;&#20998;&#20301;&#25968;&#22238;&#24402;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#30340;SIAD&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#21644;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates quantile regression in the presence of non-convex and non-smooth sparse penalties, such as the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). The non-smooth and non-convex nature of these problems often leads to convergence difficulties for many algorithms. While iterative techniques like coordinate descent and local linear approximation can facilitate convergence, the process is often slow. This sluggish pace is primarily due to the need to run these approximation techniques until full convergence at each step, a requirement we term as a \emph{secondary convergence iteration}. To accelerate the convergence speed, we employ the alternating direction method of multipliers (ADMM) and introduce a novel single-loop smoothing ADMM algorithm with an increasing penalty parameter, named SIAD, specifically tailored for sparse-penalized quantile regression. We first delve into the convergence properties of the proposed SIAD algorithm and est
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24490;&#29615;&#26377;&#21521;&#22270;&#20013;&#24314;&#31435;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20877;&#38656;&#35201;&#23545;d-&#20998;&#31163;&#36827;&#34892;&#27979;&#35797;&#65292;&#22823;&#22823;&#20943;&#23567;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#37325;&#35201;&#30340;&#29702;&#35770;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.03092</link><description>&lt;p&gt;
&#22312;&#24490;&#29615;&#26377;&#21521;&#22270;&#20013;&#24314;&#31435;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Establishing Markov Equivalence in Cyclic Directed Graphs. (arXiv:2309.03092v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24490;&#29615;&#26377;&#21521;&#22270;&#20013;&#24314;&#31435;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20877;&#38656;&#35201;&#23545;d-&#20998;&#31163;&#36827;&#34892;&#27979;&#35797;&#65292;&#22823;&#22823;&#20943;&#23567;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#37325;&#35201;&#30340;&#29702;&#35770;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#22312;&#21487;&#33021;&#21253;&#21547;&#24490;&#29615;&#30340;&#26377;&#21521;&#22270;&#19978;&#24314;&#31435;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Thomas Richardson&#22312;90&#24180;&#20195;&#20013;&#26399;&#20851;&#20110;&#24490;&#29615;&#27169;&#22411;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20013;&#30340;&#24490;&#29615;&#31561;&#20215;&#23450;&#29702;(CET)&#65292;&#20294;&#26159;&#29616;&#22312;&#20174;&#19968;&#20010;&#31062;&#20808;&#30340;&#35282;&#24230;&#37325;&#26032;&#34920;&#36848;&#12290;&#24471;&#21040;&#30340;&#29305;&#24449;&#23548;&#33268;&#20102;&#19968;&#31181;&#24314;&#31435;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#20851;&#31995;&#30340;&#36807;&#31243;&#65292;&#19981;&#20877;&#38656;&#35201;&#23545;d-&#20998;&#31163;&#36827;&#34892;&#27979;&#35797;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23567;&#20102;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#27010;&#24565;&#19978;&#31616;&#21270;&#30340;&#29305;&#24449;&#21487;&#33021;&#26377;&#21161;&#20110;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#37325;&#26032;&#28608;&#21457;&#23545;&#24490;&#29615;&#21457;&#29616;&#30340;&#29702;&#35770;&#30740;&#31350;&#30340;&#20852;&#36259;&#12290;&#35813;&#29256;&#26412;&#21253;&#25324;&#20102;&#23450;&#29702;1&#20013;&#35268;&#21017;(iv)&#30340;&#20462;&#27491;&#65292;&#20197;&#21450;&#31639;&#27861;2&#31532;2&#37096;&#20998;&#30340;&#30456;&#20851;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new, efficient procedure to establish Markov equivalence between directed graphs that may or may not contain cycles under the \textit{d}-separation criterion. It is based on the Cyclic Equivalence Theorem (CET) in the seminal works on cyclic models by Thomas Richardson in the mid '90s, but now rephrased from an ancestral perspective. The resulting characterization leads to a procedure for establishing Markov equivalence between graphs that no longer requires tests for d-separation, leading to a significantly reduced algorithmic complexity. The conceptually simplified characterization may help to reinvigorate theoretical research towards sound and complete cyclic discovery in the presence of latent confounders. This version includes a correction to rule (iv) in Theorem 1, and the subsequent adjustment in part 2 of Algorithm 2.
&lt;/p&gt;</description></item><item><title>&#22312;&#39640;&#31561;&#29289;&#29702;&#25945;&#32946;&#20013;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#36741;&#21161;&#38382;&#39064;&#35299;&#20915;&#30340;&#23398;&#29983;&#24448;&#24448;&#36807;&#20110;&#20381;&#36182;&#27169;&#22411;&#65292;&#23548;&#33268;&#36817;&#19968;&#21322;&#30340;&#35299;&#31572;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#26159;&#27491;&#30830;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.03087</link><description>&lt;p&gt;
&#26410;&#32463;&#21453;&#24605;&#30340;&#25509;&#21463; &#8212;&#8212; &#23545;ChatGPT&#36741;&#21161;&#29289;&#29702;&#25945;&#32946;&#20013;&#36127;&#38754;&#21518;&#26524;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Unreflected Acceptance -- Investigating the Negative Consequences of ChatGPT-Assisted Problem Solving in Physics Education. (arXiv:2309.03087v1 [physics.ed-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03087
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#31561;&#29289;&#29702;&#25945;&#32946;&#20013;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#36741;&#21161;&#38382;&#39064;&#35299;&#20915;&#30340;&#23398;&#29983;&#24448;&#24448;&#36807;&#20110;&#20381;&#36182;&#27169;&#22411;&#65292;&#23548;&#33268;&#36817;&#19968;&#21322;&#30340;&#35299;&#31572;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#26159;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;ChatGPT&#22312;&#25945;&#32946;&#31561;&#25935;&#24863;&#39046;&#22495;&#30340;&#26222;&#36941;&#21487;&#29992;&#24615;&#23545;&#20110;&#24050;&#26377;&#30340;&#25945;&#32946;&#26041;&#27861;&#30340;&#31038;&#20250;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23398;&#29983;&#21644;&#25945;&#32946;&#32773;&#24050;&#32463;&#22312;&#24050;&#26377;&#30340;&#25945;&#32946;&#26041;&#27861;&#26041;&#38754;&#20307;&#39564;&#21040;&#20102;&#31038;&#20250;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#39640;&#31561;&#29289;&#29702;&#25945;&#32946;&#65292;&#24182;&#30740;&#31350;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#22312;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#20855;&#26377;&#29289;&#29702;&#32972;&#26223;&#30340;&#23398;&#29983;&#34987;&#20998;&#37197;&#35299;&#20915;&#29289;&#29702;&#20064;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#32452;&#26377;&#20351;&#29992;&#20114;&#32852;&#32593;&#25628;&#32034;&#24341;&#25806;&#30340;&#26435;&#21033;&#65288;N=12&#65289;&#65292;&#32780;&#21478;&#19968;&#20010;&#32452;&#20801;&#35768;&#20351;&#29992;ChatGPT&#65288;N=27&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20182;&#20204;&#30340;&#34920;&#29616;&#12289;&#31574;&#30053;&#21644;&#19982;&#25552;&#20379;&#30340;&#24037;&#20855;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20960;&#20046;&#19968;&#21322;&#30340;&#36890;&#36807;ChatGPT&#25903;&#25345;&#25552;&#20379;&#30340;&#35299;&#31572;&#34987;&#23398;&#29983;&#38169;&#35823;&#22320;&#35748;&#20026;&#26159;&#27491;&#30830;&#30340;&#65292;&#36825;&#34920;&#26126;&#20182;&#20204;&#36807;&#20998;&#20449;&#20219;ChatGPT&#65292;&#21363;&#20351;&#22312;&#20182;&#20204;&#30340;&#19987;&#19994;&#39046;&#22495;&#20869;&#20063;&#26159;&#22914;&#27492;&#12290;&#21516;&#26679;&#22320;&#65292;&#22312;42%&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#20351;&#29992;&#20102;&#22797;&#21046;&#31896;&#36148;&#26469;&#36827;&#34892;&#26597;&#35810;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently gained popularity. However, the impact of their general availability through ChatGPT on sensitive areas of everyday life, such as education, remains unclear. Nevertheless, the societal impact on established educational methods is already being experienced by both students and educators. Our work focuses on higher physics education and examines problem solving strategies. In a study, students with a background in physics were assigned to solve physics exercises, with one group having access to an internet search engine (N=12) and the other group being allowed to use ChatGPT (N=27). We evaluated their performance, strategies, and interaction with the provided tools. Our results showed that nearly half of the solutions provided with the support of ChatGPT were mistakenly assumed to be correct by the students, indicating that they overly trusted ChatGPT even in their field of expertise. Likewise, in 42% of cases, students used copy &amp; paste to quer
&lt;/p&gt;</description></item><item><title>&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03084</link><description>&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03084
&lt;/p&gt;
&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#30446;&#21069;&#35299;&#20915;&#22823;&#35268;&#27169;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;&#26412;&#25991;&#22312;CFR&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32431;CFR&#65288;PCFR&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;PCFR&#21487;&#20197;&#30475;&#20316;&#26159;CFR&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#30340;&#32467;&#21512;&#65292;&#32487;&#25215;&#20102;CFR&#30340;&#21453;&#20107;&#23454;&#36951;&#25022;&#65288;&#20540;&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#20351;&#29992;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#32780;&#19981;&#26159;&#36951;&#25022;&#21305;&#37197;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;PCFR&#21487;&#20197;&#23454;&#29616;Blackwell&#21487;&#36798;&#24615;&#65292;&#20351;PCFR&#33021;&#22815;&#19982;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#22312;&#20869;&#30340;&#20219;&#20309;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#32431;MCCFR&#65288;PMCCFR&#65289;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;PMCCFR&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#27604;MCCFR&#24555;&#19977;&#20493;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PMCCFR&#19981;&#36890;&#36807;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21160;&#31639;&#27861;&#65292;&#21463;&#21040;&#20102;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;Twitter&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#23454;&#39564;&#20102;&#22810;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#24433;&#21709;&#32773;&#20869;&#23481;&#20013;&#30340;&#21830;&#19994;&#25512;&#24191;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.03064</link><description>&lt;p&gt;
&#22312;Twitter&#19978;&#23545;&#24433;&#21709;&#32773;&#20869;&#23481;&#30340;&#22810;&#27169;&#24577;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Analysis of Influencer Content on Twitter. (arXiv:2309.03064v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;Twitter&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#23454;&#39564;&#20102;&#22810;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#24433;&#21709;&#32773;&#20869;&#23481;&#20013;&#30340;&#21830;&#19994;&#25512;&#24191;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#32773;&#33829;&#38144;&#28041;&#21450;&#19968;&#31995;&#21015;&#30340;&#31574;&#30053;&#65292;&#21697;&#29260;&#19982;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#65288;&#21363;&#24433;&#21709;&#32773;&#65289;&#21512;&#20316;&#65292;&#21033;&#29992;&#20182;&#20204;&#30340;&#24433;&#21709;&#21147;&#12289;&#20449;&#20219;&#24230;&#21644;&#23545;&#20182;&#20204;&#30340;&#21463;&#20247;&#30340;&#24433;&#21709;&#21147;&#65292;&#25512;&#24191;&#21644;&#32972;&#20070;&#20135;&#21697;&#25110;&#26381;&#21153;&#12290;&#30001;&#20110;&#24433;&#21709;&#32773;&#30340;&#31881;&#19997;&#22312;&#25509;&#25910;&#21040;&#30495;&#23454;&#30340;&#20135;&#21697;&#35748;&#21487;&#21518;&#26356;&#26377;&#21487;&#33021;&#36141;&#20080;&#20135;&#21697;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#30340;&#30452;&#25509;&#20135;&#21697;&#25512;&#24191;&#65292;&#20010;&#20154;&#35266;&#28857;&#19982;&#21830;&#19994;&#20869;&#23481;&#25512;&#24191;&#20043;&#38388;&#30340;&#30028;&#38480;&#32463;&#24120;&#27169;&#31946;&#12290;&#36825;&#20351;&#24471;&#33258;&#21160;&#26816;&#27979;&#19982;&#24433;&#21709;&#32773;&#24191;&#21578;&#30456;&#20851;&#30340;&#30417;&#31649;&#21512;&#35268;&#36829;&#35268;&#34892;&#20026;&#65288;&#20363;&#22914;&#35823;&#23548;&#24615;&#24191;&#21578;&#25110;&#38544;&#34255;&#36190;&#21161;&#65289;&#23588;&#20026;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#65288;1&#65289;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;Twitter&#65288;&#29616;&#22312;&#26159;X&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;15,998&#20010;&#24433;&#21709;&#32773;&#30340;&#24086;&#23376;&#65292;&#20998;&#20026;&#21830;&#19994;&#21644;&#38750;&#21830;&#19994;&#31867;&#21035;&#65292;&#20197;&#21327;&#21161;&#33258;&#21160;&#26816;&#27979;&#21830;&#19994;&#24433;&#21709;&#32773;&#20869;&#23481;&#65307;&#65288;2&#65289;&#23581;&#35797;&#20102;&#19968;&#31995;&#21015;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#39044;&#27979;&#27169;&#22411;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influencer marketing involves a wide range of strategies in which brands collaborate with popular content creators (i.e., influencers) to leverage their reach, trust, and impact on their audience to promote and endorse products or services. Because followers of influencers are more likely to buy a product after receiving an authentic product endorsement rather than an explicit direct product promotion, the line between personal opinions and commercial content promotion is frequently blurred. This makes automatic detection of regulatory compliance breaches related to influencer advertising (e.g., misleading advertising or hidden sponsorships) particularly difficult. In this work, we (1) introduce a new Twitter (now X) dataset consisting of 15,998 influencer posts mapped into commercial and non-commercial categories for assisting in the automatic detection of commercial influencer content; (2) experiment with an extensive set of predictive models that combine text and visual information 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hide and Seek (HaS)&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#26412;&#22320;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36820;&#22238;&#30340;&#32467;&#26524;&#36827;&#34892;&#35299;&#21311;&#21517;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.03057</link><description>&lt;p&gt;
Hide and Seek (HaS): &#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#38544;&#31169;&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection. (arXiv:2309.03057v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hide and Seek (HaS)&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#26412;&#22320;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36820;&#22238;&#30340;&#32467;&#26524;&#36827;&#34892;&#35299;&#21311;&#21517;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20844;&#21496;&#24050;&#24320;&#22987;&#25552;&#20379;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#26381;&#21153;&#65292;&#22914; ChatGPT&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#24341;&#36215;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#29992;&#25143;&#30340;&#25552;&#31034;&#26292;&#38706;&#32473;&#20102;&#27169;&#22411;&#25552;&#20379;&#32773;&#12290;&#20043;&#21069;&#20851;&#20110;&#20351;&#29992;&#22810;&#26041;&#35745;&#31639; (MPC) &#36827;&#34892;&#23433;&#20840;&#25512;&#29702;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#23545;&#20110; LLM &#24212;&#29992;&#26469;&#35828;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#32791;&#26102;&#19988;&#36890;&#20449;&#23494;&#38598;&#12290;&#34429;&#28982;&#36731;&#37327;&#32423;&#30340;&#21311;&#21517;&#21270;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#26367;&#25442;&#25110;&#25513;&#30422;&#26469;&#20445;&#25252;&#25552;&#31034;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#24674;&#22797; LLM &#29983;&#25104;&#30340;&#32467;&#26524;&#20013;&#26367;&#25442;&#30340;&#25935;&#24863;&#25968;&#25454;&#12290;&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#26412;&#22320;&#27169;&#22411;&#26469;&#35299;&#21311;&#21517;&#21270; LLM &#36820;&#22238;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#21311;&#21517;&#21270;&#25216;&#26415;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102; Hide and Seek (HaS) &#26694;&#26550;&#65292;&#20854;&#20013; "Hide" &#21644; "Seek" &#20998;&#21035;&#20195;&#34920;&#20854;&#20004;&#20010;&#26680;&#24515;&#36807;&#31243;&#65306;&#38544;&#34255;&#31169;&#26377;&#23454;&#20307;&#20197;&#36827;&#34892;&#21311;&#21517;&#21270;&#65292;&#23547;&#25214;&#31169;&#26377;&#23454;&#20307;&#20197;&#36827;&#34892;&#35299;&#21311;&#21517;&#21270;&#12290;&#20026;&#20102;&#23450;&#37327;&#35780;&#20272; HaS &#30340;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
Numerous companies have started offering services based on large language models (LLM), such as ChatGPT, which inevitably raises privacy concerns as users' prompts are exposed to the model provider. Previous research on secure reasoning using multi-party computation (MPC) has proven to be impractical for LLM applications due to its time-consuming and communication-intensive nature. While lightweight anonymization techniques can protect private information in prompts through substitution or masking, they fail to recover sensitive data replaced in the LLM-generated results. In this paper, we expand the application scenarios of anonymization techniques by training a small local model to de-anonymize the LLM's returned results with minimal computational overhead. We introduce the HaS framework, where "H(ide)" and "S(eek)" represent its two core processes: hiding private entities for anonymization and seeking private entities for de-anonymization, respectively. To quantitatively assess HaS'
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;Vision Transformers&#21644;CIDER&#26041;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#22495;&#22806;&#26816;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22312;&#24320;&#31665;&#21363;&#29992;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;ViT&#21644;CNN&#19982;CIDER&#26041;&#27861;&#32467;&#21512;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;Transformer&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20026;&#35813;&#20219;&#21153;&#35774;&#31435;&#20102;&#26356;&#24378;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.03047</link><description>&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21644;CIDER&#32467;&#21512;&#36827;&#34892;&#22495;&#22806;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combining pre-trained Vision Transformers and CIDER for Out Of Domain Detection. (arXiv:2309.03047v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;Vision Transformers&#21644;CIDER&#26041;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#22495;&#22806;&#26816;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22312;&#24320;&#31665;&#21363;&#29992;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;ViT&#21644;CNN&#19982;CIDER&#26041;&#27861;&#32467;&#21512;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;Transformer&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20026;&#35813;&#20219;&#21153;&#35774;&#31435;&#20102;&#26356;&#24378;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#26159;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#24110;&#21161;&#35782;&#21035;&#27169;&#22411;&#36935;&#21040;&#30340;&#36229;&#20986;&#35757;&#32451;&#20998;&#24067;&#30340;&#36755;&#20837;&#12290;&#22823;&#22810;&#25968;&#24037;&#19994;&#27969;&#31243;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;CNN&#25110;Vision Transformers&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#22495;&#22806;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22312;&#24320;&#31665;&#21363;&#29992;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;ViT&#21644;CNN&#19982;CIDER&#31561;&#25913;&#36827;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#22312;&#35768;&#22810;&#24773;&#22659;&#19979;&#20026;&#36825;&#20010;&#20219;&#21153;&#24314;&#31435;&#20102;&#26356;&#24378;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-domain (OOD) detection is a crucial component in industrial applications as it helps identify when a model encounters inputs that are outside the training distribution. Most industrial pipelines rely on pre-trained models for downstream tasks such as CNN or Vision Transformers. This paper investigates the performance of those models on the task of out-of-domain detection. Our experiments demonstrate that pre-trained transformers models achieve higher detection performance out of the box. Furthermore, we show that pre-trained ViT and CNNs can be combined with refinement methods such as CIDER to improve their OOD detection performance even more. Our results suggest that transformers are a promising approach for OOD detection and set a stronger baseline for this task in many contexts
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#39539;&#26021;&#20102;Shapley Values&#22312;&#35268;&#21017;&#35299;&#37322;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23384;&#22312;&#24067;&#23572;&#20989;&#25968;&#65292;&#20351;&#24471;Shapley&#20540;&#32473;&#20986;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20449;&#24687;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#34542;&#21147;&#26041;&#27861;&#26469;&#35782;&#21035;&#36825;&#31181;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#29305;&#24449;&#25968;&#37327;&#36739;&#22823;&#30340;&#24067;&#23572;&#20989;&#25968;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03041</link><description>&lt;p&gt;
&#39539;&#26021;Shapley Values&#29992;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#35770;&#35777;
&lt;/p&gt;
&lt;p&gt;
A Refutation of Shapley Values for Explainability. (arXiv:2309.03041v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03041
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#39539;&#26021;&#20102;Shapley Values&#22312;&#35268;&#21017;&#35299;&#37322;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23384;&#22312;&#24067;&#23572;&#20989;&#25968;&#65292;&#20351;&#24471;Shapley&#20540;&#32473;&#20986;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20449;&#24687;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#34542;&#21147;&#26041;&#27861;&#26469;&#35782;&#21035;&#36825;&#31181;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#29305;&#24449;&#25968;&#37327;&#36739;&#22823;&#30340;&#24067;&#23572;&#20989;&#25968;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#20013;&#65292;Shapley&#20540;&#23545;&#29305;&#24449;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#25552;&#20379;&#20102;&#35823;&#23548;&#24615;&#20449;&#24687;&#30340;&#24067;&#23572;&#20989;&#25968;&#30340;&#23384;&#22312;&#12290;&#36825;&#20123;&#35823;&#23548;&#24615;&#20449;&#24687;&#34987;&#24191;&#27867;&#20998;&#31867;&#20026;&#20960;&#20010;&#21487;&#33021;&#30340;&#38382;&#39064;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#28041;&#21450;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#25110;&#26080;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#20013;&#65292;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#37117;&#19982;Shapley&#20540;&#30340;&#19981;&#36275;&#26377;&#20851;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#34542;&#21147;&#26041;&#27861;&#26469;&#35782;&#21035;&#20165;&#21253;&#21547;&#23569;&#25968;&#29305;&#24449;&#21644;&#30456;&#20851;&#23454;&#20363;&#30340;&#24067;&#23572;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;Shapley&#20540;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#20316;&#20026;&#35777;&#26126;&#22312;&#35268;&#21017;&#35299;&#37322;&#20013;Shapley&#20540;&#30340;&#19981;&#36275;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#36825;&#31181;Shapley&#20540;&#19981;&#36275;&#30340;&#38382;&#39064;&#22312;&#20855;&#26377;&#20219;&#24847;&#22823;&#25968;&#37327;&#29305;&#24449;&#30340;&#24067;&#23572;&#20989;&#25968;&#20013;&#26377;&#22810;&#39057;&#32321;&#20986;&#29616;&#12290;&#24456;&#26174;&#28982;&#65292;&#34542;&#21147;&#26041;&#27861;&#19981;&#22826;&#21487;&#33021;&#25552;&#20379;&#27934;&#23519;&#22914;&#20309;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#22823;&#25968;&#37327;&#29305;&#24449;&#30340;&#38382;&#39064;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work demonstrated the existence of Boolean functions for which Shapley values provide misleading information about the relative importance of features in rule-based explanations. Such misleading information was broadly categorized into a number of possible issues. Each of those issues relates with features being relevant or irrelevant for a prediction, and all are significant regarding the inadequacy of Shapley values for rule-based explainability. This earlier work devised a brute-force approach to identify Boolean functions, defined on small numbers of features, and also associated instances, which displayed such inadequacy-revealing issues, and so served as evidence to the inadequacy of Shapley values for rule-based explainability. However, an outstanding question is how frequently such inadequacy-revealing issues can occur for Boolean functions with arbitrary large numbers of features. It is plain that a brute-force approach would be unlikely to provide insights on how to ta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#20020;&#26102;Deepfake&#20301;&#32622;&#26041;&#27861;&#65288;TDL&#65289;&#29992;&#20110;&#37096;&#20998;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#65292;&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#27169;&#22359;&#21644;&#26102;&#38388;&#21367;&#31215;&#25805;&#20316;&#65292;&#33021;&#26377;&#25928;&#25429;&#25417;&#38899;&#39057;&#30340;&#29305;&#24449;&#21644;&#20301;&#32622;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;ASVspoof2019 Partial Spoof&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.03036</link><description>&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#30340;&#39640;&#25928;&#20020;&#26102;Deepfake&#20301;&#32622;&#26041;&#27861;&#29992;&#20110;&#37096;&#20998;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection. (arXiv:2309.03036v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#20020;&#26102;Deepfake&#20301;&#32622;&#26041;&#27861;&#65288;TDL&#65289;&#29992;&#20110;&#37096;&#20998;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#65292;&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#27169;&#22359;&#21644;&#26102;&#38388;&#21367;&#31215;&#25805;&#20316;&#65292;&#33021;&#26377;&#25928;&#25429;&#25417;&#38899;&#39057;&#30340;&#29305;&#24449;&#21644;&#20301;&#32622;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;ASVspoof2019 Partial Spoof&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#24103;&#32423;&#21035;&#20934;&#30830;&#22320;&#23450;&#20301;&#38899;&#39057;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#21270;&#30340;&#37096;&#20998;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;&#20020;&#26102;Deepfake&#20301;&#32622;&#65288;TDL&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#29305;&#24449;&#21644;&#20301;&#32622;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#26032;&#39062;&#30340;&#37096;&#20998;&#65306;&#23884;&#20837;&#30456;&#20284;&#24615;&#27169;&#22359;&#21644;&#26102;&#38388;&#21367;&#31215;&#25805;&#20316;&#12290;&#20026;&#20102;&#22686;&#24378;&#30495;&#23454;&#29305;&#24449;&#21644;&#20266;&#36896;&#29305;&#24449;&#20043;&#38388;&#30340;&#21306;&#20998;&#24230;&#65292;&#23884;&#20837;&#30456;&#20284;&#24615;&#27169;&#22359;&#34987;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#21487;&#20197;&#23558;&#30495;&#23454;&#24103;&#19982;&#20266;&#36896;&#24103;&#20998;&#31163;&#24320;&#26469;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20851;&#27880;&#20301;&#32622;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#21367;&#31215;&#25805;&#20316;&#65292;&#29992;&#20110;&#35745;&#31639;&#30456;&#37051;&#24103;&#20043;&#38388;&#30340;&#29305;&#23450;&#24103;&#30456;&#20284;&#24615;&#65292;&#24182;&#21160;&#24577;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#37051;&#23621;&#36827;&#34892;&#21367;&#31215;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ASVspoof2019 Partial Spoof&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially spoofed audio detection is a challenging task, lying in the need to accurately locate the authenticity of audio at the frame level. To address this issue, we propose a fine-grained partially spoofed audio detection method, namely Temporal Deepfake Location (TDL), which can effectively capture information of both features and locations. Specifically, our approach involves two novel parts: embedding similarity module and temporal convolution operation. To enhance the identification between the real and fake features, the embedding similarity module is designed to generate an embedding space that can separate the real frames from fake frames. To effectively concentrate on the position information, temporal convolution operation is proposed to calculate the frame-specific similarities among neighboring frames, and dynamically select informative neighbors to convolution. Extensive experiments show that our method outperform baseline models in ASVspoof2019 Partial Spoof dataset and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#36890;&#29992;&#39044;&#22788;&#29702;&#25805;&#20316;&#31526;&#65292;&#29992;&#20110;&#23558;&#20855;&#26377;&#25968;&#20540;&#12289;&#26102;&#38388;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#36716;&#25442;&#20026;&#33021;&#22815;&#20351;&#29992;&#20219;&#20309;&#23884;&#20837;&#26041;&#27861;&#30340;&#24418;&#24335;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03023</link><description>&lt;p&gt;
&#29992;&#20110;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;&#25991;&#23383;&#23884;&#20837;&#30340;&#36890;&#29992;&#39044;&#22788;&#29702;&#25805;&#20316;&#31526;
&lt;/p&gt;
&lt;p&gt;
Universal Preprocessing Operators for Embedding Knowledge Graphs with Literals. (arXiv:2309.03023v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#36890;&#29992;&#39044;&#22788;&#29702;&#25805;&#20316;&#31526;&#65292;&#29992;&#20110;&#23558;&#20855;&#26377;&#25968;&#20540;&#12289;&#26102;&#38388;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#36716;&#25442;&#20026;&#33021;&#22815;&#20351;&#29992;&#20219;&#20309;&#23884;&#20837;&#26041;&#27861;&#30340;&#24418;&#24335;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26159;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#23494;&#38598;&#25968;&#20540;&#34920;&#31034;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#20851;&#27880;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#36739;&#23569;&#26377;&#26041;&#27861;&#23558;&#25991;&#23383;&#25551;&#36848;&#25110;&#25968;&#20540;&#20449;&#24687;&#31561;&#30693;&#35782;&#20063;&#32771;&#34385;&#22312;&#20869;&#12290;&#24050;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#25991;&#23383;&#31867;&#22411;&#21644;&#23884;&#20837;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#32452;&#36890;&#29992;&#39044;&#22788;&#29702;&#25805;&#20316;&#31526;&#65292;&#21487;&#29992;&#20110;&#23558;&#24102;&#26377;&#25968;&#20540;&#12289;&#26102;&#38388;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#36716;&#25442;&#20026;&#33021;&#22815;&#20351;&#29992;&#20219;&#20309;&#26041;&#27861;&#36827;&#34892;&#23884;&#20837;&#30340;&#24418;&#24335;&#12290;&#22312;kgbench&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embeddings are dense numerical representations of entities in a knowledge graph (KG). While the majority of approaches concentrate only on relational information, i.e., relations between entities, fewer approaches exist which also take information about literal values (e.g., textual descriptions or numerical information) into account. Those which exist are typically tailored towards a particular modality of literal and a particular embedding method. In this paper, we propose a set of universal preprocessing operators which can be used to transform KGs with literals for numerical, temporal, textual, and image information, so that the transformed KGs can be embedded with any method. The results on the kgbench dataset with three different embedding methods show promising results.
&lt;/p&gt;</description></item><item><title>EdgeFL&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#22312;&#36793;&#32536;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#27719;&#24635;&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#29992;&#20363;&#30340;&#26080;&#32541;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#23450;&#21046;&#27719;&#24635;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02936</link><description>&lt;p&gt;
EdgeFL&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EdgeFL: A Lightweight Decentralized Federated Learning Framework. (arXiv:2309.02936v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02936
&lt;/p&gt;
&lt;p&gt;
EdgeFL&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#22312;&#36793;&#32536;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#27719;&#24635;&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#29992;&#20363;&#30340;&#26080;&#32541;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#23450;&#21046;&#27719;&#24635;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FL&#24179;&#21488;&#21644;&#26694;&#26550;&#22312;&#36719;&#20214;&#24037;&#31243;&#24072;&#26041;&#38754;&#23384;&#22312;&#22797;&#26434;&#24615;&#12289;&#26377;&#38480;&#30340;&#23450;&#21046;&#36873;&#39033;&#21644;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EdgeFL&#65292;&#19968;&#31181;&#20165;&#22312;&#36793;&#32536;&#37096;&#32626;&#30340;&#36731;&#37327;&#32423;&#20998;&#25955;&#24335;FL&#26694;&#26550;&#65292;&#26088;&#22312;&#20811;&#26381;&#38598;&#20013;&#24335;&#27719;&#24635;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#37319;&#29992;&#20165;&#22312;&#36793;&#32536;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#27719;&#24635;&#30340;&#26041;&#27861;&#65292;EdgeFL&#28040;&#38500;&#20102;&#38656;&#35201;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#29992;&#20363;&#30340;&#26080;&#32541;&#21487;&#25193;&#23637;&#24615;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#38598;&#25104;&#36807;&#31243;&#65292;&#20165;&#38656;&#35201;&#22235;&#34892;&#20195;&#30721;&#65288;LOC&#65289;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#21487;&#20197;&#36731;&#26494;&#23558;FL&#21151;&#33021;&#34701;&#20837;&#20854;AI&#20135;&#21697;&#20013;&#12290;&#27492;&#22806;&#65292;EdgeFL&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#23450;&#21046;&#27719;&#24635;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#20351;&#24037;&#31243;&#24072;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#38656;&#27714;&#36827;&#34892;&#35843;&#25972;&#12290;&#26681;&#25454;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;EdgeFL&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising approach for collaborative machine learning, addressing data privacy concerns. However, existing FL platforms and frameworks often present challenges for software engineers in terms of complexity, limited customization options, and scalability limitations. In this paper, we introduce EdgeFL, an edge-only lightweight decentralized FL framework, designed to overcome the limitations of centralized aggregation and scalability in FL deployments. By adopting an edge-only model training and aggregation approach, EdgeFL eliminates the need for a central server, enabling seamless scalability across diverse use cases. With a straightforward integration process requiring just four lines of code (LOC), software engineers can easily incorporate FL functionalities into their AI products. Furthermore, EdgeFL offers the flexibility to customize aggregation functions, empowering engineers to adapt them to specific needs. Based on the results, we demons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21387;&#21147;&#25968;&#25454;&#20272;&#35745;&#26410;&#30693;&#30340;&#19981;&#35268;&#21017;&#29992;&#27700;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#31616;&#21270;&#27844;&#28431;&#26816;&#27979;&#38382;&#39064;&#23454;&#29616;&#32447;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.02935</link><description>&lt;p&gt;
&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#19981;&#35268;&#21017;&#29992;&#27700;&#38656;&#27714;&#20197;&#25903;&#25345;&#27844;&#28431;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Estimating irregular water demands with physics-informed machine learning to inform leakage detection. (arXiv:2309.02935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21387;&#21147;&#25968;&#25454;&#20272;&#35745;&#26410;&#30693;&#30340;&#19981;&#35268;&#21017;&#29992;&#27700;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#31616;&#21270;&#27844;&#28431;&#26816;&#27979;&#38382;&#39064;&#23454;&#29616;&#32447;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39278;&#29992;&#27700;&#20379;&#24212;&#32593;&#32476;&#20013;&#30340;&#28431;&#27700;&#38382;&#39064;&#32473;&#27700;&#21153;&#20844;&#21496;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#20102;&#22522;&#30784;&#35774;&#26045;&#25925;&#38556;&#12289;&#36816;&#33829;&#20013;&#26029;&#12289;&#29615;&#22659;&#39118;&#38505;&#12289;&#36130;&#20135;&#25439;&#22833;&#21644;&#32463;&#27982;&#25439;&#22833;&#12290;&#21450;&#26102;&#35782;&#21035;&#21644;&#20934;&#30830;&#23450;&#20301;&#36825;&#20123;&#27844;&#28431;&#23545;&#20110;&#27700;&#21153;&#20844;&#21496;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#27844;&#28431;&#26816;&#27979;&#31639;&#27861;&#30340;&#23454;&#26045;&#22312;&#23454;&#36341;&#20013;&#21463;&#21040;&#27700;&#21147;&#27169;&#22411;&#25110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21033;&#29992;&#27700;&#21147;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21387;&#21147;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20272;&#35745;&#26410;&#30693;&#30340;&#19981;&#35268;&#21017;&#29992;&#27700;&#38656;&#27714;&#65292;&#26368;&#32456;&#21033;&#29992;&#20271;&#21162;&#21033;&#26041;&#31243;&#26377;&#25928;&#32447;&#24615;&#21270;&#27844;&#28431;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;L-Town&#22522;&#20934;&#32593;&#32476;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Leakages in drinking water distribution networks pose significant challenges to water utilities, leading to infrastructure failure, operational disruptions, environmental hazards, property damage, and economic losses. The timely identification and accurate localisation of such leakages is paramount for utilities to mitigate these unwanted effects. However, implementation of algorithms for leakage detection is limited in practice by requirements of either hydraulic models or large amounts of training data. Physics-informed machine learning can utilise hydraulic information thereby circumventing both limitations. In this work, we present a physics-informed machine learning algorithm that analyses pressure data and therefrom estimates unknown irregular water demands via a fully connected neural network, ultimately leveraging the Bernoulli equation and effectively linearising the leakage detection problem. Our algorithm is tested on data from the L-Town benchmark network, and results indic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#26469;&#25351;&#23548;&#26410;&#26469;&#26500;&#24314;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.02912</link><description>&lt;p&gt;
&#20851;&#20110;&#26500;&#24314;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On the Challenges of Building Datasets for Hate Speech Detection. (arXiv:2309.02912v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#26469;&#25351;&#23548;&#26410;&#26469;&#26500;&#24314;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#30340;&#26816;&#27979;&#34987;&#25552;&#20986;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20010;&#29420;&#31435;&#24212;&#29992;&#65292;&#24182;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#30446;&#26631;&#32676;&#20307;&#12289;&#33719;&#21462;&#21407;&#22987;&#25968;&#25454;&#12289;&#23450;&#20041;&#26631;&#35760;&#36807;&#31243;&#12289;&#36873;&#25321;&#26816;&#27979;&#31639;&#27861;&#20197;&#21450;&#35780;&#20272;&#25152;&#38656;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#19981;&#21516;&#65292;&#30001;&#20110;&#20219;&#21153;&#30340;&#39640;&#24230;&#20027;&#35266;&#24615;&#65292;&#20167;&#24680;&#35328;&#35770;&#30340;&#25968;&#25454;&#38598;&#32570;&#20047;&#22823;&#35268;&#27169;&#12289;&#31934;&#24515;&#31579;&#36873;&#12289;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#39318;&#20808;&#20998;&#26512;&#20102;&#22260;&#32469;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#20197;&#20167;&#24680;&#35328;&#35770;&#23545;&#24615;&#23569;&#25968;&#32676;&#20307;&#30340;&#29305;&#23450;&#31034;&#20363;&#20026;&#20363;&#65292;&#27010;&#25324;&#20102;&#28085;&#30422;&#19971;&#20010;&#26041;&#38754;&#30340;&#25968;&#25454;&#21019;&#24314;&#27969;&#31243;&#12290;&#25105;&#20204;&#35748;&#20026;&#20174;&#26410;&#26469;&#26500;&#24314;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#23454;&#36341;&#35282;&#24230;&#20986;&#21457;&#65292;&#20174;&#19994;&#32773;&#23558;&#21463;&#30410;&#20110;&#36981;&#24490;&#36825;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of hate speech has been formulated as a standalone application of NLP and different approaches have been adopted for identifying the target groups, obtaining raw data, defining the labeling process, choosing the detection algorithm, and evaluating the performance in the desired setting. However, unlike other downstream tasks, hate speech suffers from the lack of large-sized, carefully curated, generalizable datasets owing to the highly subjective nature of the task. In this paper, we first analyze the issues surrounding hate speech detection through a data-centric lens. We then outline a holistic framework to encapsulate the data creation pipeline across seven broad dimensions by taking the specific example of hate speech towards sexual minorities. We posit that practitioners would benefit from following this framework as a form of best practice when creating hate speech datasets in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02908</link><description>&lt;p&gt;
DECODE: &#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#25968;&#25454;&#39537;&#21160;&#33021;&#32791;&#39044;&#27979;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings. (arXiv:2309.02908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#20013;&#30340;&#33021;&#32791;&#39044;&#27979;&#22312;&#26377;&#25928;&#30340;&#33021;&#28304;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#31934;&#30830;&#30340;&#39044;&#27979;&#23545;&#20110;&#23454;&#29616;&#20248;&#21270;&#30340;&#33021;&#32791;&#21644;&#30005;&#32593;&#20998;&#37197;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#33021;&#28304;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#12290;&#19982;&#29616;&#26377;&#30340;&#39044;&#27979;&#27169;&#22411;&#30456;&#27604;&#65292;LSTM&#27169;&#22411;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#30701;&#26399;&#12289;&#20013;&#26399;&#21644;&#38271;&#26399;&#33021;&#32791;&#39044;&#27979;&#65292;&#36866;&#29992;&#20110;&#20303;&#23429;&#21644;&#21830;&#19994;&#24314;&#31569;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;LSTM&#27169;&#22411;&#19982;&#32447;&#24615;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#31561;&#24050;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#25552;&#20986;&#30340;LSTM&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#23427;&#20855;&#26377;&#20986;&#33394;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;R2&#24471;&#20998;&#20026;0.97&#65292;&#26368;&#20302;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.007&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#20248;&#28857;&#26159;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy prediction in buildings plays a crucial role in effective energy management. Precise predictions are essential for achieving optimal energy consumption and distribution within the grid. This paper introduces a Long Short-Term Memory (LSTM) model designed to forecast building energy consumption using historical energy data, occupancy patterns, and weather conditions. The LSTM model provides accurate short, medium, and long-term energy predictions for residential and commercial buildings compared to existing prediction models. We compare our LSTM model with established prediction methods, including linear regression, decision trees, and random forest. Encouragingly, the proposed LSTM model emerges as the superior performer across all metrics. It demonstrates exceptional prediction accuracy, boasting the highest R2 score of 0.97 and the most favorable mean absolute error (MAE) of 0.007. An additional advantage of our developed model is its capacity to achieve efficient energy consu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#30446;&#26631;&#35821;&#35328;&#21477;&#23376;&#23545;&#20043;&#38388;&#25512;&#29702;&#20851;&#31995;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#29305;&#23450;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23454;&#20363;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#65292;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.02887</link><description>&lt;p&gt;
&#19968;&#31181;&#27809;&#26377;&#35821;&#35328;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
A deep Natural Language Inference predictor without language-specific training data. (arXiv:2309.02887v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#30446;&#26631;&#35821;&#35328;&#21477;&#23376;&#23545;&#20043;&#38388;&#25512;&#29702;&#20851;&#31995;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#29305;&#23450;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23454;&#20363;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#65292;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#30446;&#26631;&#35821;&#35328;&#21477;&#23376;&#23545;&#20043;&#38388;&#25512;&#29702;&#20851;&#31995;&#65288;NLI&#65289;&#38382;&#39064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#26080;&#38656;&#35821;&#35328;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#25163;&#21160;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#21516;&#19968;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20004;&#20010;&#23454;&#20363;&#8212;&#8212;&#31532;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#31532;&#20108;&#20010;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#27169;&#20223;&#31532;&#19968;&#20010;&#23454;&#20363;&#12290;&#36825;&#31181;&#25216;&#26415;&#31216;&#20026;&#30693;&#35782;&#33976;&#39311;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#30340;&#26031;&#22374;&#31119;NLI&#27979;&#35797;&#25968;&#25454;&#38598;&#12289;&#26426;&#22120;&#32763;&#35793;&#30340;&#22810;&#31867;&#22411;NLI&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#25163;&#21160;&#32763;&#35793;&#30340;RTE3-ITA&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#23454;&#35777;&#22320;&#23637;&#31034;NLI&#20219;&#21153;&#30340;&#36890;&#29992;&#24615;&#12290;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#26412;&#22320;&#30340;ABSITA&#25968;&#25454;&#38598;&#19978;&#30340;&#24773;&#24863;&#20998;&#26512;&#12289;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#35782;&#21035;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a technique of NLP to tackle the problem of inference relation (NLI) between pairs of sentences in a target language of choice without a language-specific training dataset. We exploit a generic translation dataset, manually translated, along with two instances of the same pre-trained model - the first to generate sentence embeddings for the source language, and the second fine-tuned over the target language to mimic the first. This technique is known as Knowledge Distillation. The model has been evaluated over machine translated Stanford NLI test dataset, machine translated Multi-Genre NLI test dataset, and manually translated RTE3-ITA test dataset. We also test the proposed architecture over different tasks to empirically demonstrate the generality of the NLI task. The model has been evaluated over the native Italian ABSITA dataset, on the tasks of Sentiment Analysis, Aspect-Based Sentiment Analysis, and Topic Recognition. We emphasise the generality and explo
&lt;/p&gt;</description></item><item><title>MAD&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#23398;&#20064;&#22270;&#20687;&#22266;&#26377;&#20960;&#20309;&#29305;&#24615;&#30340;&#28145;&#24230;&#22270;&#20687;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22810;&#27169;&#24577;&#22270;&#20687;&#37197;&#20934;&#20013;&#30340;&#22806;&#35266;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02875</link><description>&lt;p&gt;
MAD: &#22270;&#20687;&#37197;&#20934;&#30340;&#27169;&#24577;&#26080;&#20851;&#36317;&#31163;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
MAD: Modality Agnostic Distance Measure for Image Registration. (arXiv:2309.02875v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02875
&lt;/p&gt;
&lt;p&gt;
MAD&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#23398;&#20064;&#22270;&#20687;&#22266;&#26377;&#20960;&#20309;&#29305;&#24615;&#30340;&#28145;&#24230;&#22270;&#20687;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22810;&#27169;&#24577;&#22270;&#20687;&#37197;&#20934;&#20013;&#30340;&#22806;&#35266;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#20687;&#37197;&#20934;&#26159;&#35768;&#22810;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#25104;&#20687;&#27169;&#24577;&#20043;&#38388;&#22797;&#26434;&#30340;&#24378;&#24230;&#20851;&#31995;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#23548;&#33268;&#22270;&#20687;&#22806;&#35266;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#26080;&#35770;&#26159;&#20256;&#32479;&#30340;&#36824;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#22270;&#20687;&#37197;&#20934;&#30340;&#25104;&#21151;&#65292;&#37117;&#21462;&#20915;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;&#36317;&#31163;&#65288;&#25110;&#30456;&#20284;&#24230;&#65289;&#24230;&#37327;&#12290;&#29305;&#21035;&#26159;&#65292;&#28145;&#24230;&#23398;&#20064;&#37197;&#20934;&#31639;&#27861;&#22312;&#23581;&#35797;&#23545;&#26469;&#33258;&#8220;&#26410;&#35265;&#36807;&#8221;&#27169;&#24577;&#30340;&#25968;&#25454;&#36827;&#34892;&#37197;&#20934;&#26102;&#31934;&#24230;&#19981;&#36275;&#29978;&#33267;&#23436;&#20840;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Modality Agnostic Distance&#65288;MAD&#65289;&#30340;&#28145;&#24230;&#22270;&#20687;&#36317;&#31163;&#24230;&#37327;&#65292;&#23427;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#26469;&#23398;&#20064;&#22270;&#20687;&#30340;&#22266;&#26377;&#20960;&#20309;&#29305;&#24615;&#65292;&#21516;&#26102;&#23545;&#22823;&#24133;&#24230;&#22806;&#35266;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#38543;&#26426;&#21367;&#31215;&#26159;&#20445;&#25345;&#20960;&#20309;&#24615;&#36136;&#30340;&#27169;&#22359;&#65292;&#25105;&#20204;&#29992;&#23427;&#20204;&#26469;&#27169;&#25311;&#26080;&#38480;&#25968;&#37327;&#30340;&#21512;&#25104;&#27169;&#24577;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#20102;&#23545;&#40784;&#37197;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal image registration is a crucial pre-processing step in many medical applications. However, it is a challenging task due to the complex intensity relationships between different imaging modalities, which can result in large discrepancy in image appearance. The success of multi-modal image registration, whether it is conventional or learning based, is predicated upon the choice of an appropriate distance (or similarity) measure. Particularly, deep learning registration algorithms lack in accuracy or even fail completely when attempting to register data from an "unseen" modality. In this work, we present Modality Agnostic Distance (MAD), a deep image distance}] measure that utilises random convolutions to learn the inherent geometry of the images while being robust to large appearance changes. Random convolutions are geometry-preserving modules which we use to simulate an infinite number of synthetic modalities alleviating the need for aligned paired data during training. We c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;OCL&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.02870</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#37325;&#26032;&#24605;&#32771;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Rethinking Momentum Knowledge Distillation in Online Continual Learning. (arXiv:2309.02870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02870
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;OCL&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65288;OCL&#65289;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#19978;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#25353;&#39034;&#24207;&#20986;&#29616;&#12290;&#19982;&#31163;&#32447;&#36830;&#32493;&#23398;&#20064;&#30456;&#27604;&#65292;&#22312;OCL&#20013;&#21482;&#33021;&#30475;&#21040;&#25968;&#25454;&#19968;&#27425;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22238;&#25918;&#30340;&#31574;&#30053;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#20005;&#37325;&#20381;&#36182;&#23427;&#20204;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#22312;&#31163;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#24050;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;OCL&#20013;&#20173;&#28982;&#26410;&#20805;&#20998;&#21033;&#29992;&#20854;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#23558;KD&#24212;&#29992;&#20110;OCL&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#21160;&#37327;&#30693;&#35782;&#33976;&#39311;&#65288;MKD&#65289;&#24212;&#29992;&#20110;&#35768;&#22810;&#26071;&#33328;OCL&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#22686;&#24378;&#29616;&#26377;&#26041;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#38500;&#20102;&#23558;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;ImageNet100&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;&#36229;&#36807;10&#20010;&#30334;&#20998;&#28857;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#38416;&#26126;&#20102;MKD&#22312;OCL&#20013;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#37096;&#26426;&#21046;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Continual Learning (OCL) addresses the problem of training neural networks on a continuous data stream where multiple classification tasks emerge in sequence. In contrast to offline Continual Learning, data can be seen only once in OCL. In this context, replay-based strategies have achieved impressive results and most state-of-the-art approaches are heavily depending on them. While Knowledge Distillation (KD) has been extensively used in offline Continual Learning, it remains under-exploited in OCL, despite its potential. In this paper, we theoretically analyze the challenges in applying KD to OCL. We introduce a direct yet effective methodology for applying Momentum Knowledge Distillation (MKD) to many flagship OCL methods and demonstrate its capabilities to enhance existing approaches. In addition to improving existing state-of-the-arts accuracy by more than $10\%$ points on ImageNet100, we shed light on MKD internal mechanics and impacts during training in OCL. We argue that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#29992;&#20114;&#20449;&#24687;&#65288;GEMINI&#65289;&#20316;&#20026;&#19968;&#31181;&#36776;&#21035;&#32858;&#31867;&#30340;&#26694;&#26550;&#65292;&#30456;&#27604;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#65292;GEMINI&#22312;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;&#65292;&#20854;&#21487;&#20197;&#36873;&#25321;&#21512;&#36866;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.02858</link><description>&lt;p&gt;
&#36890;&#29992;&#20114;&#20449;&#24687;&#65306;&#19968;&#31181;&#36776;&#21035;&#32858;&#31867;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Generalised Mutual Information: a Framework for Discriminative Clustering. (arXiv:2309.02858v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#29992;&#20114;&#20449;&#24687;&#65288;GEMINI&#65289;&#20316;&#20026;&#19968;&#31181;&#36776;&#21035;&#32858;&#31867;&#30340;&#26694;&#26550;&#65292;&#30456;&#27604;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#65292;GEMINI&#22312;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;&#65292;&#20854;&#21487;&#20197;&#36873;&#25321;&#21512;&#36866;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#32858;&#31867;&#30340;&#26368;&#26032;&#25104;&#26524;&#20027;&#35201;&#28041;&#21450;&#20316;&#20026;&#26080;&#30417;&#30563;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#23458;&#35266;&#20989;&#25968;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#65292;&#24182;&#22686;&#21152;&#20102;&#27491;&#21017;&#39033;&#12290;&#23613;&#31649;&#27491;&#21017;&#21270;&#30340;&#36136;&#37327;&#24050;&#32463;&#34987;&#24191;&#27867;&#35752;&#35770;&#20197;&#36827;&#34892;&#25913;&#36827;&#65292;&#20294;&#23545;&#20110;MI&#20316;&#20026;&#32858;&#31867;&#30446;&#26631;&#30340;&#30456;&#20851;&#24615;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#20808;&#24378;&#35843;&#20102;&#26368;&#22823;&#21270;MI&#24182;&#19981;&#33021;&#24471;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#24211;&#23572;&#24052;&#20811;-&#33713;&#24067;&#21202;&#25955;&#24230;&#26159;&#36825;&#19968;&#34892;&#20026;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#20854;&#26680;&#24515;&#24046;&#24322;&#65292;&#24341;&#20837;&#36890;&#29992;&#20114;&#20449;&#24687;&#65288;GEMINI&#65289;&#26469;&#25512;&#24191;&#20114;&#20449;&#24687;&#65306;&#19968;&#32452;&#29992;&#20110;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;&#19982;MI&#19981;&#21516;&#30340;&#26159;&#65292;&#19968;&#20123;GEMINI&#22312;&#35757;&#32451;&#26102;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#20855;&#26377;&#20960;&#20309;&#24847;&#35782;&#30340;&#36317;&#31163;&#25110;&#26680;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;GEMINI&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#25968;&#37327;&#30340;&#32858;&#31867;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, recent successes in deep clustering majorly involved the Mutual Information (MI) as an unsupervised objective for training neural networks with increasing regularisations. While the quality of the regularisations have been largely discussed for improvements, little attention has been dedicated to the relevance of MI as a clustering objective. In this paper, we first highlight how the maximisation of MI does not lead to satisfying clusters. We identified the Kullback-Leibler divergence as the main reason of this behaviour. Hence, we generalise the mutual information by changing its core distance, introducing the Generalised Mutual Information (GEMINI): a set of metrics for unsupervised neural network training. Unlike MI, some GEMINIs do not require regularisations when training as they are geometry-aware thanks to distances or kernels in the data space. Finally, we highlight that GEMINIs can automatically select a relevant number of clusters, a property that has been
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#32447;&#33258;&#36866;&#24212;&#31639;&#27861;&#20013;&#29305;&#24449;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38656;&#35201;&#23398;&#20064;&#26368;&#20339;&#25805;&#20316;&#26102;&#65292;&#20010;&#24615;&#21270;&#23398;&#20064;&#33021;&#22815;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#21253;&#21547;&#19981;&#24517;&#35201;&#30340;&#23398;&#29983;&#29305;&#24449;&#21487;&#33021;&#20250;&#38477;&#20302;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02856</link><description>&lt;p&gt;
&#22826;&#36807;&#20010;&#24615;&#21270;&#65306;&#22312;&#32447;&#33258;&#36866;&#24212;&#31639;&#27861;&#20013;&#29305;&#24449;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Getting too personal(ized): The importance of feature choice in online adaptive algorithms. (arXiv:2309.02856v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02856
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#32447;&#33258;&#36866;&#24212;&#31639;&#27861;&#20013;&#29305;&#24449;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38656;&#35201;&#23398;&#20064;&#26368;&#20339;&#25805;&#20316;&#26102;&#65292;&#20010;&#24615;&#21270;&#23398;&#20064;&#33021;&#22815;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#21253;&#21547;&#19981;&#24517;&#35201;&#30340;&#23398;&#29983;&#29305;&#24449;&#21487;&#33021;&#20250;&#38477;&#20302;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#25945;&#32946;&#25216;&#26415;&#20855;&#26377;&#20010;&#24615;&#21270;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#26681;&#25454;&#23398;&#29983;&#30340;&#29305;&#28857;&#36827;&#34892;&#23450;&#21046;&#21270;&#25945;&#32946;&#65292;&#25552;&#39640;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20010;&#24615;&#21270;&#23398;&#20064;&#26159;&#21542;&#23384;&#22312;&#25104;&#26412;&#65292;&#20363;&#22914;&#26159;&#21542;&#20250;&#24310;&#36831;&#21463;&#30410;&#20110;&#25152;&#26377;&#23398;&#29983;&#30340;&#25919;&#31574;&#30340;&#37319;&#29992;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#23398;&#20064;&#25945;&#32946;&#25216;&#26415;&#23637;&#31034;&#29256;&#26412;&#30340;&#25919;&#31574;&#26102;&#65292;&#25506;&#35752;&#20102;&#23398;&#29983;&#29305;&#24449;&#21644;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#31639;&#27861;&#26159;&#21542;&#24847;&#35782;&#21040;&#36825;&#20123;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38656;&#35201;&#23398;&#20064;&#26368;&#20339;&#25805;&#20316;&#26102;&#65292;&#21253;&#21547;&#23398;&#29983;&#29305;&#24449;&#36827;&#34892;&#20010;&#24615;&#21270;&#33021;&#22815;&#24102;&#26469;&#30410;&#22788;&#12290;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#21253;&#21547;&#20250;&#38477;&#20302;&#36172;&#21338;&#26426;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#32780;&#19988;&#65292;&#21253;&#21547;&#19981;&#24517;&#35201;&#30340;&#23398;&#29983;&#29305;&#24449;&#36824;&#26377;&#21487;&#33021;&#22686;&#21152;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital educational technologies offer the potential to customize students' experiences and learn what works for which students, enhancing the technology as more students interact with it. We consider whether and when attempting to discover how to personalize has a cost, such as if the adaptation to personal information can delay the adoption of policies that benefit all students. We explore these issues in the context of using multi-armed bandit (MAB) algorithms to learn a policy for what version of an educational technology to present to each student, varying the relation between student characteristics and outcomes and also whether the algorithm is aware of these characteristics. Through simulations, we demonstrate that the inclusion of student characteristics for personalization can be beneficial when those characteristics are needed to learn the optimal action. In other scenarios, this inclusion decreases performance of the bandit algorithm. Moreover, including unneeded student ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#21644;&#22238;&#22797;&#20043;&#38388;&#30340;&#38544;&#24335;&#27169;&#24335;&#20449;&#24687;&#25552;&#39640;&#20102;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#37319;&#29992;&#20102;&#25913;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#21010;&#37319;&#26679;&#26041;&#27861;&#65292;&#20351;&#29983;&#25104;&#30340;&#22238;&#22797;&#26356;&#21152;&#29983;&#21160;&#21644;&#20449;&#24687;&#20016;&#23500;&#12290;</title><link>http://arxiv.org/abs/2309.02823</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#21644;&#22238;&#22797;&#20043;&#38388;&#30340;&#27169;&#24335;&#20449;&#24687;&#20419;&#36827;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Promoting Open-domain Dialogue Generation through Learning Pattern Information between Contexts and Responses. (arXiv:2309.02823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#21644;&#22238;&#22797;&#20043;&#38388;&#30340;&#38544;&#24335;&#27169;&#24335;&#20449;&#24687;&#25552;&#39640;&#20102;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#37319;&#29992;&#20102;&#25913;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#21010;&#37319;&#26679;&#26041;&#27861;&#65292;&#20351;&#29983;&#25104;&#30340;&#22238;&#22797;&#26356;&#21152;&#29983;&#21160;&#21644;&#20449;&#24687;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#27169;&#22411;&#24050;&#25104;&#20026;&#28909;&#38376;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#22797;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#22914;&#32570;&#20047;&#19978;&#19979;&#25991;&#21270;&#21644;&#23481;&#26131;&#29983;&#25104;&#32570;&#20047;&#20449;&#24687;&#20869;&#23481;&#30340;&#36890;&#29992;&#22238;&#22797;&#65292;&#20005;&#37325;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#35797;&#22270;&#24341;&#20837;&#26356;&#22810;&#20449;&#24687;&#21040;&#23545;&#35805;&#27169;&#22411;&#20013;&#65292;&#20351;&#29983;&#25104;&#30340;&#22238;&#22797;&#26356;&#21152;&#29983;&#21160;&#21644;&#20449;&#24687;&#20016;&#23500;&#12290;&#19982;&#23427;&#20204;&#19981;&#21516;&#65292;&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#26679;&#26412;&#20013;&#19978;&#19979;&#25991;&#21644;&#22238;&#22797;&#20043;&#38388;&#30340;&#38544;&#24335;&#27169;&#24335;&#20449;&#24687;&#26469;&#25552;&#39640;&#29983;&#25104;&#30340;&#22238;&#22797;&#36136;&#37327;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;GPT-2&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#21010;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#30340;&#22238;&#22797;&#26469;&#25351;&#23548;&#22238;&#22797;&#29983;&#25104;&#65292;&#21516;&#26102;&#36991;&#20813;&#26292;&#38706;&#20559;&#24046;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, utilizing deep neural networks to build the opendomain dialogue models has become a hot topic. However, the responses generated by these models suffer from many problems such as responses not being contextualized and tend to generate generic responses that lack information content, damaging the user's experience seriously. Therefore, many studies try introducing more information into the dialogue models to make the generated responses more vivid and informative. Unlike them, this paper improves the quality of generated responses by learning the implicit pattern information between contexts and responses in the training samples. In this paper, we first build an open-domain dialogue model based on the pre-trained language model (i.e., GPT-2). And then, an improved scheduled sampling method is proposed for pre-trained models, by which the responses can be used to guide the response generation in the training phase while avoiding the exposure bias problem. More importantly, we de
&lt;/p&gt;</description></item><item><title>Roulette&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#30340;&#35821;&#20041;&#38544;&#31169;&#20445;&#25252;&#30340;&#35774;&#22791;&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#28151;&#28102;&#21644;&#21152;&#22122;&#22768;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02820</link><description>&lt;p&gt;
Roulette&#65306;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#30340;&#35821;&#20041;&#38544;&#31169;&#20445;&#25252;&#30340;&#35774;&#22791;&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative Inference Framework for Deep Learning Classification Tasks. (arXiv:2309.02820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02820
&lt;/p&gt;
&lt;p&gt;
Roulette&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#30340;&#35821;&#20041;&#38544;&#31169;&#20445;&#25252;&#30340;&#35774;&#22791;&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#28151;&#28102;&#21644;&#21152;&#22122;&#22768;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#12290;&#35774;&#22791;&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#20316;&#20026;&#22312;&#29289;&#32852;&#32593;&#21644;5G/6G&#32593;&#32476;&#20013;&#25512;&#24191;&#20854;&#24212;&#29992;&#30340;&#39640;&#25928;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20998;&#24067;&#21644;&#38544;&#31169;&#27844;&#38706;&#26041;&#38754;&#37117;&#23384;&#22312;&#31934;&#24230;&#19979;&#38477;&#38382;&#39064;&#12290;&#23545;&#20110;&#31934;&#24230;&#19979;&#38477;&#65292;&#30452;&#25509;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#20998;&#21106;&#23398;&#20064;&#25104;&#26412;&#36739;&#39640;&#65292;&#38544;&#31169;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#23545;&#20110;&#38544;&#31169;&#27844;&#38706;&#65292;&#22522;&#20110;&#23494;&#30721;&#23398;&#30340;&#26041;&#27861;&#20250;&#23548;&#33268;&#24040;&#22823;&#30340;&#24320;&#38144;&#12290;&#20854;&#20182;&#36731;&#37327;&#32423;&#26041;&#27861;&#20551;&#35774;&#30495;&#23454;&#26631;&#31614;&#26159;&#38750;&#25935;&#24863;&#30340;&#24182;&#19988;&#21487;&#20197;&#34987;&#20844;&#24320;&#12290;&#20294;&#26159;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#26469;&#35828;&#65292;&#30495;&#23454;&#26631;&#31614;&#26159;&#29992;&#25143;&#30340;&#20851;&#38190;&#25935;&#24863;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Roulette&#30340;&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#30340;&#35821;&#20041;&#38544;&#31169;&#20445;&#25252;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#21327;&#21516;&#25512;&#29702;&#26694;&#26550;&#12290;&#38500;&#20102;&#36755;&#20837;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#35270;&#20026;&#31169;&#23494;&#20449;&#24687;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#28102;&#21644;&#21152;&#22122;&#22768;&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#32467;&#26524;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning classifiers are crucial in the age of artificial intelligence. The device-edge-based collaborative inference has been widely adopted as an efficient framework for promoting its applications in IoT and 5G/6G networks. However, it suffers from accuracy degradation under non-i.i.d. data distribution and privacy disclosure. For accuracy degradation, direct use of transfer learning and split learning is high cost and privacy issues remain. For privacy disclosure, cryptography-based approaches lead to a huge overhead. Other lightweight methods assume that the ground truth is non-sensitive and can be exposed. But for many applications, the ground truth is the user's crucial privacy-sensitive information. In this paper, we propose a framework of Roulette, which is a task-oriented semantic privacy-preserving collaborative inference framework for deep learning classifiers. More than input data, we treat the ground truth of the data as private information. We develop a novel paradig
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#28909;&#21147;&#23398;&#27169;&#22411;&#21644;&#20027;&#21160;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;Active-CompDesign&#26694;&#26550;&#29992;&#20110;&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#20248;&#21270;&#35774;&#35745;&#12290;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.02818</link><description>&lt;p&gt;
&#32467;&#21512;&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#28909;&#21147;&#23398;&#27169;&#22411;&#21644;&#20027;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#22686;&#24378;&#24037;&#19994;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Thermodynamics-based Model of the Centrifugal Compressors and Active Machine Learning for Enhanced Industrial Design Optimization. (arXiv:2309.02818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02818
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#28909;&#21147;&#23398;&#27169;&#22411;&#21644;&#20027;&#21160;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;Active-CompDesign&#26694;&#26550;&#29992;&#20110;&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#20248;&#21270;&#35774;&#35745;&#12290;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#24515;&#21387;&#32553;&#26426;&#30340;&#35774;&#35745;&#36807;&#31243;&#38656;&#35201;&#24212;&#29992;&#19968;&#20010;&#20248;&#21270;&#36807;&#31243;&#65292;&#30001;&#20110;&#35813;&#36807;&#31243;&#19979;&#38754;&#30340;&#20998;&#26512;&#26041;&#31243;&#65292;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#12290;&#34429;&#28982;&#22238;&#24402;&#26367;&#20195;&#27169;&#22411;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#36825;&#20010;&#36807;&#31243;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#25112;&#30053;&#24615;&#22320;&#21033;&#29992;&#26631;&#35760;&#26679;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Active-CompDesign&#26694;&#26550;&#65292;&#20854;&#20013;&#22312;&#21487;&#37096;&#32626;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#29615;&#22659;&#20013;&#23558;&#22522;&#20110;&#28909;&#21147;&#23398;&#30340;&#21387;&#32553;&#26426;&#27169;&#22411;&#65288;&#21363;&#25105;&#20204;&#20869;&#37096;&#30340;&#21387;&#32553;&#26426;&#35774;&#35745;&#36719;&#20214;&#65289;&#19982;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#26367;&#20195;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#20854;&#25193;&#23637;&#21040;&#22312;&#32447;AL&#26694;&#26550;&#65292;&#20854;&#20013;&#19982;&#22522;&#20110;&#28909;&#21147;&#23398;&#30340;&#21387;&#32553;&#26426;&#27169;&#22411;&#30340;&#23454;&#26102;&#20132;&#20114;&#20801;&#35768;&#22312;&#29983;&#20135;&#20013;&#37096;&#32626;&#12290;ActiveCompDesign&#22312;&#26367;&#20195;&#24314;&#27169;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design process of centrifugal compressors requires applying an optimization process which is computationally expensive due to complex analytical equations underlying the compressor's dynamical equations. Although the regression surrogate models could drastically reduce the computational cost of such a process, the major challenge is the scarcity of data for training the surrogate model. Aiming to strategically exploit the labeled samples, we propose the Active-CompDesign framework in which we combine a thermodynamics-based compressor model (i.e., our internal software for compressor design) and Gaussian Process-based surrogate model within a deployable Active Learning (AL) setting. We first conduct experiments in an offline setting and further, extend it to an online AL framework where a real-time interaction with the thermodynamics-based compressor's model allows the deployment in production. ActiveCompDesign shows a significant performance improvement in surrogate modeling by lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20046;&#36830;&#32493;&#26102;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#36830;&#32493;&#29366;&#24577;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#26410;&#30693;&#21160;&#21147;&#31995;&#32479;&#65292;&#22312;&#21333;&#20010;&#36712;&#36857;&#19978;&#26368;&#22823;&#21270;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#27850;&#26494;&#26102;&#38047;&#23545;&#20132;&#20114;&#26102;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#31163;&#25955;&#21040;&#36830;&#32493;&#26102;&#38388;&#25429;&#25417;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.02815</link><description>&lt;p&gt;
&#36830;&#32493;&#29366;&#24577;&#21160;&#20316;&#31354;&#38388;&#30340;&#20960;&#20046;&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Near-continuous time Reinforcement Learning for continuous state-action spaces. (arXiv:2309.02815v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20046;&#36830;&#32493;&#26102;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#36830;&#32493;&#29366;&#24577;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#26410;&#30693;&#21160;&#21147;&#31995;&#32479;&#65292;&#22312;&#21333;&#20010;&#36712;&#36857;&#19978;&#26368;&#22823;&#21270;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#27850;&#26494;&#26102;&#38047;&#23545;&#20132;&#20114;&#26102;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#31163;&#25955;&#21040;&#36830;&#32493;&#26102;&#38388;&#25429;&#25417;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25511;&#21046;&#26410;&#30693;&#21160;&#21147;&#31995;&#32479;&#20197;&#22312;&#21333;&#20010;&#36712;&#36857;&#19978;&#26368;&#22823;&#21270;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#25991;&#29486;&#32771;&#34385;&#30340;&#26159;&#22312;&#31163;&#25955;&#26102;&#38388;&#21644;&#31163;&#25955;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#21457;&#29983;&#30340;&#31995;&#32479;&#20132;&#20114;&#12290;&#23613;&#31649;&#36825;&#31181;&#35266;&#28857;&#36866;&#29992;&#20110;&#28216;&#25103;&#65292;&#20294;&#23545;&#20110;&#20132;&#20114;&#39057;&#29575;&#39640;&#65288;&#22914;&#26524;&#19981;&#26159;&#36830;&#32493;&#26102;&#38388;&#65289;&#19988;&#29366;&#24577;&#31354;&#38388;&#22823;&#65288;&#22914;&#26524;&#19981;&#26159;&#22266;&#26377;&#36830;&#32493;&#30340;&#65289;&#30340;&#26426;&#26800;&#25110;&#25968;&#23383;&#31995;&#32479;&#26469;&#35828;&#65292;&#36890;&#24120;&#26159;&#19981;&#22815;&#30340;&#12290;&#20063;&#35768;&#21807;&#19968;&#30340;&#20363;&#22806;&#26159;&#32447;&#24615;&#20108;&#27425;&#26694;&#26550;&#65292;&#23427;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#20013;&#37117;&#26377;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22788;&#29702;&#36830;&#32493;&#29366;&#24577;&#30340;&#33021;&#21147;&#24102;&#26469;&#20102;&#21160;&#24577;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#21018;&#24615;&#32570;&#28857;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#39057;&#29575;&#20026;&#949;&#30340;&#27850;&#26494;&#26102;&#38047;&#23545;&#24314;&#27169;&#20132;&#20114;&#26102;&#38388;&#65292;&#20174;&#31163;&#25955;&#65288;&#949;=1&#65289;&#21040;&#36830;&#32493;&#26102;&#38388;&#65288;&#949;&#8595;0&#65289;&#25429;&#25417;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#65292;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the Reinforcement Learning problem of controlling an unknown dynamical system to maximise the long-term average reward along a single trajectory. Most of the literature considers system interactions that occur in discrete time and discrete state-action spaces. Although this standpoint is suitable for games, it is often inadequate for mechanical or digital systems in which interactions occur at a high frequency, if not in continuous time, and whose state spaces are large if not inherently continuous. Perhaps the only exception is the Linear Quadratic framework for which results exist both in discrete and continuous time. However, its ability to handle continuous states comes with the drawback of a rigid dynamic and reward structure. This work aims to overcome these shortcomings by modelling interaction times with a Poisson clock of frequency $\varepsilon^{-1}$, which captures arbitrary time scales: from discrete ($\varepsilon=1$) to continuous time ($\varepsilon\downarrow0$)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02784</link><description>&lt;p&gt;
Norm&#35843;&#25972;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#20302;&#27604;&#29305;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Norm Tweaking: High-performance Low-bit Quantization of Large Language Models. (arXiv:2309.02784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23610;&#23544;&#19981;&#26029;&#22686;&#22823;&#65292;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#24050;&#25104;&#20026;&#37096;&#32626;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#34429;&#28982;&#19968;&#20123;&#37327;&#21270;&#26041;&#27861;&#65292;&#22914;GPTQ&#65292;&#22312;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;4&#27604;&#29305;&#26435;&#37325;&#37327;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23581;&#35797;&#26356;&#20302;&#20301;&#30340;&#37327;&#21270;&#24448;&#24448;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24403;&#21069;PTQ&#26041;&#27861;&#30340;&#25554;&#20214;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#25104;&#26412;&#39640;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#19968;&#39033;&#35266;&#23519;&#30340;&#21551;&#31034;&#65292;&#21363;&#20351;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#20197;&#19982;&#20854;&#28014;&#28857;&#23545;&#24212;&#29289;&#21305;&#37197;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#35843;&#25972;&#31574;&#30053;&#65292;&#21253;&#25324;&#29983;&#25104;&#26657;&#20934;&#25968;&#25454;&#21644;&#36890;&#36947;&#36317;&#31163;&#32422;&#26463;&#65292;&#20197;&#26356;&#26032;&#24402;&#19968;&#21270;&#23618;&#30340;&#26435;&#37325;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20960;&#20010;&#24320;&#28304;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment. While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower bit quantization often result in severe performance degradation. In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient. Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs. To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization. We conduct extensive experiments on various datasets using several open-sourced LLMs. Our me
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#33539;&#22260;&#23457;&#26597;&#24635;&#32467;&#20102;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#32954;&#30284;&#25104;&#20687;&#24212;&#29992;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#32954;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#65292;&#20197;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.02783</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#25552;&#39640;&#32954;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#65306;&#19968;&#39033;&#33539;&#22260;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Improving diagnosis and prognosis of lung cancer using vision transformers: A scoping review. (arXiv:2309.02783v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#33539;&#22260;&#23457;&#26597;&#24635;&#32467;&#20102;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#32954;&#30284;&#25104;&#20687;&#24212;&#29992;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#32954;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#65292;&#20197;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#22312;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#21644;&#30284;&#30151;&#25104;&#20687;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#21253;&#25324;&#32954;&#30284;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#35786;&#26029;&#21644;&#39044;&#27979;&#32954;&#30284;&#12290;&#26412;&#33539;&#22260;&#23457;&#26597;&#26088;&#22312;&#30830;&#23450;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#32954;&#30284;&#25104;&#20687;&#24212;&#29992;&#26041;&#38754;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#23427;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23558;&#35270;&#35273;&#21464;&#25442;&#22120;&#19982;&#20154;&#24037;&#26234;&#33021;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#32954;&#30284;&#24615;&#33021;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#35813;&#23457;&#26597;&#36824;&#30830;&#23450;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#21457;&#23637;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#26816;&#32034;&#21040;&#30340;314&#39033;&#30740;&#31350;&#20013;&#65292;&#26412;&#23457;&#26597;&#21253;&#25324;&#20102;&#20174;2020&#24180;&#21040;2022&#24180;&#21457;&#34920;&#30340;34&#39033;&#30740;&#31350;&#12290;&#36825;&#20123;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#20219;&#21153;&#26159;&#23545;&#32954;&#30284;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#32954;&#40158;&#29366;&#32454;&#32990;&#30284;&#19982;&#32954;&#33146;&#30284;&#65292;&#24182;&#37492;&#21035;&#33391;&#24615;&#19982;&#24694;&#24615;&#32954;&#32467;&#33410;&#12290;&#20854;&#20182;&#24212;&#29992;&#21253;&#25324;&#32954;&#30284;&#30340;&#29983;&#23384;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformer-based methods are advancing the field of medical artificial intelligence and cancer imaging, including lung cancer applications. Recently, many researchers have developed vision transformer-based AI methods for lung cancer diagnosis and prognosis. This scoping review aims to identify the recent developments on vision transformer-based AI methods for lung cancer imaging applications. It provides key insights into how vision transformers complemented the performance of AI and deep learning methods for lung cancer. Furthermore, the review also identifies the datasets that contributed to advancing the field. Of the 314 retrieved studies, this review included 34 studies published from 2020 to 2022. The most commonly addressed task in these studies was the classification of lung cancer types, such as lung squamous cell carcinoma versus lung adenocarcinoma, and identifying benign versus malignant pulmonary nodules. Other applications included survival prediction of lung can
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;SWAP&#65292;&#36890;&#36807;&#25552;&#39640;&#27425;&#32423;logits&#30340;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#20854;&#20182;logits&#30340;&#24178;&#25200;&#26469;&#23454;&#29616;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ASR&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02752</link><description>&lt;p&gt;
SWAP:&#21033;&#29992;&#27425;&#32423;logits&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series. (arXiv:2309.02752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;SWAP&#65292;&#36890;&#36807;&#25552;&#39640;&#27425;&#32423;logits&#30340;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#20854;&#20182;logits&#30340;&#24178;&#25200;&#26469;&#23454;&#29616;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ASR&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#22312;TSC&#20219;&#21153;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#23545;&#25239;&#26041;&#27861;&#32463;&#24120;&#38754;&#20020;&#30528;&#36229;&#21442;&#25968;&#21270;&#25110;&#38543;&#26426;logit&#25200;&#21160;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#36890;&#24120;&#38656;&#35201;&#29983;&#25104;&#26356;&#22810;&#30340;&#22122;&#22768;&#65292;&#20351;&#24471;&#25915;&#20987;&#26356;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;TSC&#27169;&#22411;&#30340;&#25915;&#20987;&#26041;&#27861;SWAP&#12290;SWAP&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#27425;&#32423;logits&#30340;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#20854;&#20182;logits&#30340;&#24178;&#25200;&#12290;&#36825;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;logit&#20998;&#24067;&#21644;&#39044;&#27979;logit&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SWAP&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;ASR&#36798;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Time series classification (TSC) has emerged as a critical task in various domains, and deep neural models have shown superior performance in TSC tasks. However, these models are vulnerable to adversarial attacks, where subtle perturbations can significantly impact the prediction results. Existing adversarial methods often suffer from over-parameterization or random logit perturbation, hindering their effectiveness. Additionally, increasing the attack success rate (ASR) typically involves generating more noise, making the attack more easily detectable. To address these limitations, we propose SWAP, a novel attacking method for TSC models. SWAP focuses on enhancing the confidence of the second-ranked logits while minimizing the manipulation of other logits. This is achieved by minimizing the Kullback-Leibler divergence between the target logit distribution and the predictive logit distribution. Experimental results demonstrate that SWAP achieves state-of-the-art performance, with an ASR
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLN-net&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38598;&#32676;&#24494;&#38041;&#21270;&#30340;&#20934;&#30830;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#28304;&#22270;&#20687;&#26469;&#20934;&#30830;&#22320;&#20998;&#21106;&#22810;&#28304;&#22270;&#20687;&#65292;&#36890;&#36807;&#22810;&#23618;&#24402;&#19968;&#21270;&#23618;&#32467;&#26500;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#20687;&#20998;&#21106;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02742</link><description>&lt;p&gt;
MLN-net&#65306;&#19968;&#31181;&#21033;&#29992;&#22810;&#23618;&#24402;&#19968;&#21270;&#36827;&#34892;&#38598;&#32676;&#24494;&#38041;&#21270;&#30340;&#22810;&#28304;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MLN-net: A multi-source medical image segmentation method for clustered microcalcifications using multiple layer normalization. (arXiv:2309.02742v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02742
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLN-net&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38598;&#32676;&#24494;&#38041;&#21270;&#30340;&#20934;&#30830;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#28304;&#22270;&#20687;&#26469;&#20934;&#30830;&#22320;&#20998;&#21106;&#22810;&#28304;&#22270;&#20687;&#65292;&#36890;&#36807;&#22810;&#23618;&#24402;&#19968;&#21270;&#23618;&#32467;&#26500;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#20687;&#20998;&#21106;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20083;&#33146;X&#32447;&#25668;&#24433;&#20013;&#38598;&#32676;&#24494;&#38041;&#21270;&#30340;&#20934;&#30830;&#20998;&#21106;&#23545;&#20110;&#20083;&#33146;&#30284;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#19987;&#23478;&#32423;&#20934;&#30830;&#24615;&#65292;&#20294;&#30001;&#20110;&#24739;&#32773;&#20307;&#20301;&#12289;&#20010;&#20307;&#33146;&#20307;&#23494;&#24230;&#21644;&#20083;&#33146;X&#32447;&#25668;&#24433;&#25104;&#20687;&#27169;&#24335;&#31561;&#26041;&#38754;&#30340;&#24046;&#24322;&#36896;&#25104;&#20102;&#39046;&#22495;&#36716;&#31227;&#65292;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36129;&#29486;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLN-net&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20165;&#20351;&#29992;&#21333;&#19968;&#28304;&#22270;&#20687;&#21363;&#21487;&#20934;&#30830;&#22320;&#20998;&#21106;&#22810;&#28304;&#22270;&#20687;&#65292;&#29992;&#20110;&#38598;&#32676;&#24494;&#38041;&#21270;&#20998;&#21106;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28304;&#22495;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#26469;&#29983;&#25104;&#22810;&#28304;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#37319;&#29992;&#20102;&#22810;&#23618;&#24402;&#19968;&#21270;&#65288;LN&#65289;&#23618;&#30340;&#32467;&#26500;&#26469;&#26500;&#24314;&#20998;&#21106;&#32593;&#32476;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23545;&#20110;&#38598;&#32676;&#24494;&#38041;&#21270;&#20998;&#21106;&#20855;&#26377;&#33391;&#22909;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#36335;&#36873;&#25321;&#31574;&#30053;&#26469;&#20248;&#21270;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate segmentation of clustered microcalcifications in mammography is crucial for the diagnosis and treatment of breast cancer. Despite exhibiting expert-level accuracy, recent deep learning advancements in medical image segmentation provide insufficient contribution to practical applications, due to the domain shift resulting from differences in patient postures, individual gland density, and imaging modalities of mammography etc. In this paper, a novel framework named MLN-net, which can accurately segment multi-source images using only single source images, is proposed for clustered microcalcification segmentation. We first propose a source domain image augmentation method to generate multi-source images, leading to improved generalization. And a structure of multiple layer normalization (LN) layers is used to construct the segmentation network, which can be found efficient for clustered microcalcification segmentation in different domains. Additionally, a branch selection strateg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#30340;&#32454;&#20998;&#26631;&#20934;&#29305;&#23450;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#22521;&#35757;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#23398;&#20064;&#20043;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#29305;&#24449;&#21644;&#21151;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02740</link><description>&lt;p&gt;
&#29992;&#32454;&#20998;&#26631;&#20934;&#29305;&#23450;&#26041;&#27861;&#36827;&#34892;&#22686;&#24378;&#22521;&#35757;&#30340;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Rubric-Specific Approach to Automated Essay Scoring with Augmentation Training. (arXiv:2309.02740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#30340;&#32454;&#20998;&#26631;&#20934;&#29305;&#23450;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#22521;&#35757;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#23398;&#20064;&#20043;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#29305;&#24449;&#21644;&#21151;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#35266;&#31572;&#26696;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24182;&#27809;&#26377;&#22949;&#21892;&#32771;&#34385;&#21040;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#39564;&#35777;&#36807;&#31243;&#20013;&#23545;&#20110;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#33267;&#20851;&#37325;&#35201;&#30340;&#32454;&#20998;&#26631;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#19968;&#20010;&#33258;&#21160;&#35780;&#20998;&#27169;&#22411;&#65292;&#23398;&#20064;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#29305;&#24449;&#21644;&#21151;&#33021;&#65292;&#21516;&#26102;&#22312;&#33258;&#21160;&#21270;&#23398;&#29983;&#35780;&#20272;&#22870;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural based approaches to automatic evaluation of subjective responses have shown superior performance and efficiency compared to traditional rule-based and feature engineering oriented solutions. However, it remains unclear whether the suggested neural solutions are sufficient replacements of human raters as we find recent works do not properly account for rubric items that are essential for automated essay scoring during model training and validation. In this paper, we propose a series of data augmentation operations that train and test an automated scoring model to learn features and functions overlooked by previous works while still achieving state-of-the-art performance in the Automated Student Assessment Prize dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.02731</link><description>&lt;p&gt;
HC3 Plus&#65306;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus. (arXiv:2309.02731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#26816;&#27979;&#36234;&#26469;&#36234;&#20851;&#27880;&#65292;&#36825;&#23545;&#26410;&#32463;&#35757;&#32451;&#30340;&#20154;&#31867;&#26469;&#35828;&#24448;&#24448;&#24456;&#38590;&#35782;&#21035;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#38382;&#31572;&#26041;&#38754;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#20855;&#26377;&#35821;&#20041;&#19981;&#21464;&#24615;&#30340;&#20219;&#21153;&#65292;&#22914;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#19978;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#12289;&#26356;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#32463;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#20197;&#21069;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#23548;&#24494;&#35843;&#20102;Tk-instruct&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has gained significant interest due to its impressive performance, but people are increasingly concerned about its potential risks, particularly around the detection of AI-generated content (AIGC), which is often difficult for untrained humans to identify. Current datasets utilized for detecting ChatGPT-generated text primarily center around question-answering, yet they tend to disregard tasks that possess semantic-invariant properties, such as summarization, translation, and paraphrasing. Our primary studies demonstrate that detecting model-generated text on semantic-invariant tasks is more difficult. To fill this gap, we introduce a more extensive and comprehensive dataset that considers more types of tasks than previous work, including semantic-invariant tasks. In addition, the model after a large number of task instruction fine-tuning shows a strong powerful performance. Owing to its previous success, we further instruct fine-tuning Tk-instruct and built a more powerful det
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363; Stylebook&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20174;&#30446;&#26631;&#35821;&#38899;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#39640;&#25928;&#22320;&#36716;&#31227;&#21040;&#28304;&#35821;&#38899;&#20869;&#23481;&#19978;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#25110;&#35828;&#35805;&#32773;&#26631;&#35760;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#26679;&#24335;&#25163;&#20876;&#65292;&#21487;&#20197;&#23454;&#29616;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#24544;&#23454;&#22797;&#21046;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2309.02730</link><description>&lt;p&gt;
Stylebook: &#22312;&#21482;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#30340;&#20219;&#24847;-&#20219;&#24847;&#35821;&#38899;&#36716;&#25442;&#20013;&#36827;&#34892;&#20381;&#36182;&#20869;&#23481;&#30340;&#35828;&#35805;&#39118;&#26684;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Stylebook: Content-Dependent Speaking Style Modeling for Any-to-Any Voice Conversion using Only Speech Data. (arXiv:2309.02730v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363; Stylebook&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20174;&#30446;&#26631;&#35821;&#38899;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#39640;&#25928;&#22320;&#36716;&#31227;&#21040;&#28304;&#35821;&#38899;&#20869;&#23481;&#19978;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#25110;&#35828;&#35805;&#32773;&#26631;&#35760;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#26679;&#24335;&#25163;&#20876;&#65292;&#21487;&#20197;&#23454;&#29616;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#24544;&#23454;&#22797;&#21046;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#20219;&#24847;-&#20219;&#24847;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#25104;&#21151;&#22320;&#23558;&#19968;&#20123;&#30446;&#26631;&#35821;&#38899;&#30340;&#39118;&#26684;&#20449;&#24687;&#36716;&#31227;&#21040;&#36716;&#25442;&#30340;&#35821;&#38899;&#20013;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#24544;&#23454;&#22320;&#22797;&#21046;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#35828;&#35805;&#39118;&#26684;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30446;&#26631;&#35821;&#38899;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#39640;&#25928;&#22320;&#36716;&#31227;&#21040;&#28304;&#35821;&#38899;&#20869;&#23481;&#19978;&#65292;&#32780;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#25110;&#35828;&#35805;&#32773;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27169;&#22411;&#25910;&#38598;&#19982;&#19981;&#21516;&#38899;&#32032;&#20869;&#23481;&#30456;&#23545;&#24212;&#30340;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#35828;&#35805;&#39118;&#26684;&#12290;&#36825;&#20123;&#39118;&#26684;&#29992;&#19968;&#32452;&#31216;&#20026;&#26679;&#24335;&#25163;&#20876;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#19979;&#19968;&#27493;&#20013;&#65292;&#26679;&#24335;&#25163;&#20876;&#19982;&#28304;&#35821;&#38899;&#30340;&#38899;&#32032;&#20869;&#23481;&#19968;&#36215;&#21442;&#19982;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#28304;&#20869;&#23481;&#30340;&#26368;&#32456;&#30446;&#26631;&#39118;&#26684;&#12290;&#26368;&#21518;&#65292;&#20174;&#28304;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20869;&#23481;&#20449;&#24687;&#21644;&#20381;&#36182;&#20869;&#23481;&#30340;&#30446;&#26631;&#39118;&#26684;&#23884;&#20837;&#34987;&#36755;&#20837;&#21040;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
While many recent any-to-any voice conversion models succeed in transferring some target speech's style information to the converted speech, they still lack the ability to faithfully reproduce the speaking style of the target speaker. In this work, we propose a novel method to extract rich style information from target utterances and to efficiently transfer it to source speech content without requiring text transcriptions or speaker labeling. Our proposed approach introduces an attention mechanism utilizing a self-supervised learning (SSL) model to collect the speaking styles of a target speaker each corresponding to the different phonetic content. The styles are represented with a set of embeddings called stylebook. In the next step, the stylebook is attended with the source speech's phonetic content to determine the final target style for each source content. Finally, content information extracted from the source speech and content-dependent target style embeddings are fed into a dif
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.02726</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20551;&#35774;&#21457;&#29616;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. (arXiv:2309.02726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31185;&#23398;&#23478;&#35266;&#23519;&#19990;&#30028;&#24182;&#35797;&#22270;&#25552;&#20986;&#35299;&#37322;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#30340;&#20551;&#35774;&#26102;&#65292;&#20551;&#35774;&#24402;&#32435;&#34987;&#35748;&#20026;&#26159;&#20027;&#35201;&#30340;&#25512;&#29702;&#31867;&#22411;&#12290;&#36807;&#21435;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;&#65288;1&#65289;&#25968;&#25454;&#38598;&#30340;&#35266;&#23519;&#27880;&#37322;&#19981;&#26159;&#21407;&#22987;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#32780;&#26159;&#25163;&#21160;&#36873;&#25321;&#30340;&#21477;&#23376;&#65288;&#23548;&#33268;&#20102;&#19968;&#20010;&#23553;&#38381;&#39046;&#22495;&#30340;&#35774;&#32622;&#65289;&#65307;&#65288;2&#65289;&#23454;&#38469;&#30340;&#20551;&#35774;&#27880;&#37322;&#20027;&#35201;&#26159;&#24120;&#35782;&#30693;&#35782;&#65292;&#20351;&#24471;&#20219;&#21153;&#19981;&#22826;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;50&#31687;&#21457;&#34920;&#22312;&#39030;&#32423;&#31038;&#20250;&#31185;&#23398;&#26399;&#21002;&#19978;&#30340;&#26368;&#26032;&#35770;&#25991;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#25910;&#38598;&#20102;&#24320;&#21457;&#35770;&#25991;&#20013;&#30340;&#20551;&#35774;&#25152;&#38656;&#30340;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#20165;&#36890;&#36807;&#19968;&#22534;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#23601;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#21487;&#20197;&#35299;&#20915;&#20197;&#21069;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#25152;&#38754;&#20020;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction has a limited setting that (1) the observation annotations of the dataset are not raw web corpus but are manually selected sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses annotations are mostly commonsense knowledge, making the task less challenging. In this work, we propose the first NLP dataset for social science academic hypotheses discovery, consisting of 50 recent papers published in top social science journals. Raw web corpora that are necessary for developing hypotheses in the published papers are also collected in the dataset, with the final goal of creating a system that automatically generates valid, novel, and helpful (to human researchers) hypotheses, given only a pile of raw web corpora. The new dataset can tackle the previou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#24076;&#20271;&#26469;&#35821;BERT&#27169;&#22411;&#65288;HeBERT&#21644;AlephBERT&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#21512;D_OLaH&#21487;&#20197;&#25552;&#39640;HeBERT&#27169;&#22411;&#30340;&#24615;&#33021;2%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#23545;AlephBERT&#27169;&#22411;&#20063;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02724</link><description>&lt;p&gt;
&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#21450;BERT&#27169;&#22411;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Offensive Hebrew Corpus and Detection using BERT. (arXiv:2309.02724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#24076;&#20271;&#26469;&#35821;BERT&#27169;&#22411;&#65288;HeBERT&#21644;AlephBERT&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#21512;D_OLaH&#21487;&#20197;&#25552;&#39640;HeBERT&#27169;&#22411;&#30340;&#24615;&#33021;2%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#23545;AlephBERT&#27169;&#22411;&#20063;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20398;&#36785;&#24615;&#35821;&#35328;&#26816;&#27979;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;(&#22914;&#24076;&#20271;&#26469;&#35821;)&#20013;&#20173;&#26377;&#25152;&#28382;&#21518;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#24076;&#20271;&#26469;&#35821;&#20398;&#36785;&#24615;&#35821;&#26009;&#24211;&#65292;&#20174;Twitter&#19978;&#25910;&#38598;&#20102;15881&#26465;&#25512;&#25991;&#12290;&#27599;&#26465;&#25512;&#25991;&#37117;&#30001;&#38463;&#25289;&#20271;-&#24076;&#20271;&#26469;&#21452;&#35821;&#20154;&#22763;&#26631;&#35760;&#20026;&#20116;&#20010;&#31867;&#21035;(&#36785;&#39554;&#12289;&#20167;&#24680;&#12289;&#26292;&#21147;&#12289;&#33394;&#24773;&#25110;&#38750;&#20398;&#36785;&#24615;)&#12290;&#26631;&#27880;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#26631;&#27880;&#32773;&#37117;&#38656;&#35201;&#29087;&#24713;&#20197;&#33394;&#21015;&#30340;&#25991;&#21270;&#12289;&#25919;&#27835;&#21644;&#23454;&#36341;&#65292;&#20197;&#29702;&#35299;&#27599;&#26465;&#25512;&#25991;&#30340;&#32972;&#26223;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#21644;&#21478;&#19968;&#20010;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#23545;&#20004;&#20010;&#24076;&#20271;&#26469;&#35821;BERT&#27169;&#22411;(HeBERT&#21644;AlephBERT)&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#19982;D_OLaH&#32467;&#21512;&#21518;&#65292;&#25552;&#39640;&#20102;HeBERT&#27169;&#22411;2%&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#23545;AlephBERT&#36827;&#34892;&#24494;&#35843;&#24182;&#22312;D_OLaH&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;69%&#65292;&#32780;&#22312;D_OLaH&#19978;&#36827;&#34892;&#24494;&#35843;&#24182;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#20934;&#30830;&#29575;&#20026;57%&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25105;&#20204;&#30340;&#25968;&#25454;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offensive language detection has been well studied in many languages, but it is lagging behind in low-resource languages, such as Hebrew. In this paper, we present a new offensive language corpus in Hebrew. A total of 15,881 tweets were retrieved from Twitter. Each was labeled with one or more of five classes (abusive, hate, violence, pornographic, or none offensive) by Arabic-Hebrew bilingual speakers. The annotation process was challenging as each annotator is expected to be familiar with the Israeli culture, politics, and practices to understand the context of each tweet. We fine-tuned two Hebrew BERT models, HeBERT and AlephBERT, using our proposed dataset and another published dataset. We observed that our data boosts HeBERT performance by 2% when combined with D_OLaH. Fine-tuning AlephBERT on our data and testing on D_OLaH yields 69% accuracy, while fine-tuning on D_OLaH and testing on our data yields 57% accuracy, which may be an indication to the generalizability our data offer
&lt;/p&gt;</description></item><item><title>SlAction&#21033;&#29992;&#32418;&#22806;&#35270;&#39057;&#36827;&#34892;&#26080;&#24178;&#25200;&#12289;&#36731;&#37327;&#32423;&#30340;OSA&#26816;&#27979;&#65292;&#36890;&#36807;&#20998;&#26512;&#30561;&#30496;&#26399;&#38388;&#30340;&#20154;&#20307;&#36816;&#21160;&#19982;OSA&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#29992;&#20110;&#26085;&#24120;&#30561;&#30496;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#36991;&#20813;&#20256;&#32479;&#30340;&#22810;&#23548;&#30561;&#30496;&#22270;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02713</link><description>&lt;p&gt;
SlAction&#65306;&#20351;&#29992;&#32418;&#22806;&#35270;&#39057;&#36827;&#34892;&#26080;&#24178;&#25200;&#12289;&#36731;&#37327;&#32423;&#30340;&#38459;&#22622;&#24615;&#30561;&#30496;&#21628;&#21560;&#26242;&#20572;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SlAction: Non-intrusive, Lightweight Obstructive Sleep Apnea Detection using Infrared Video. (arXiv:2309.02713v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02713
&lt;/p&gt;
&lt;p&gt;
SlAction&#21033;&#29992;&#32418;&#22806;&#35270;&#39057;&#36827;&#34892;&#26080;&#24178;&#25200;&#12289;&#36731;&#37327;&#32423;&#30340;OSA&#26816;&#27979;&#65292;&#36890;&#36807;&#20998;&#26512;&#30561;&#30496;&#26399;&#38388;&#30340;&#20154;&#20307;&#36816;&#21160;&#19982;OSA&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#29992;&#20110;&#26085;&#24120;&#30561;&#30496;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#36991;&#20813;&#20256;&#32479;&#30340;&#22810;&#23548;&#30561;&#30496;&#22270;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38459;&#22622;&#24615;&#30561;&#30496;&#21628;&#21560;&#26242;&#20572;&#65288;OSA&#65289;&#26159;&#19968;&#31181;&#24433;&#21709;&#20840;&#29699;&#32422;&#21313;&#20159;&#20154;&#21475;&#30340;&#24120;&#35265;&#30561;&#30496;&#38556;&#30861;&#12290;&#30446;&#21069;&#35786;&#26029;OSA&#30340;&#40644;&#37329;&#26631;&#20934;&#26159;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#65292;&#23427;&#38656;&#35201;&#22312;&#21307;&#38498;&#36807;&#22812;&#24182;&#36830;&#25509;&#22810;&#20010;&#20256;&#24863;&#22120;&#65292;&#20174;&#32780;&#21487;&#33021;&#22240;&#39318;&#22812;&#25928;&#24212;&#32780;&#23548;&#33268;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SlAction&#30340;&#38750;&#24178;&#25200;&#24615;OSA&#26816;&#27979;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#26085;&#24120;&#30561;&#30496;&#29615;&#22659;&#20013;&#20351;&#29992;&#32418;&#22806;&#35270;&#39057;&#12290;&#30001;&#20110;&#30561;&#30496;&#35270;&#39057;&#20013;&#23637;&#29616;&#20986;&#30340;&#36816;&#21160;&#26497;&#23569;&#65292;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#8220;&#22312;&#30561;&#30496;&#26399;&#38388;&#65292;&#21628;&#21560;&#20107;&#20214;&#26159;&#21542;&#36866;&#24403;&#22320;&#21453;&#26144;&#22312;&#20154;&#20307;&#36816;&#21160;&#20013;&#65311;&#8221;&#36890;&#36807;&#20998;&#26512;&#26368;&#22823;&#30340;&#30561;&#30496;&#35270;&#39057;&#25968;&#25454;&#38598;&#65288;&#24635;&#35745;5098&#23567;&#26102;&#65289;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;OSA&#20107;&#20214;&#19982;&#30561;&#30496;&#26399;&#38388;&#20154;&#20307;&#36816;&#21160;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#20302;&#24103;&#29575;&#65288;2.5 FPS&#65289;&#12289;&#22823;&#23610;&#23544;&#65288;60&#31186;&#65289;&#21644;&#28369;&#21160;&#31383;&#21475;&#20998;&#26512;&#30340;&#27493;&#38271;&#65288;30&#31186;&#65289;&#65292;&#20197;&#25429;&#25417;&#19982;OSA&#30456;&#20851;&#30340;&#32531;&#24930;&#21644;&#38271;&#26399;&#36816;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21628;&#21560;&#20107;&#20214;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obstructive sleep apnea (OSA) is a prevalent sleep disorder affecting approximately one billion people world-wide. The current gold standard for diagnosing OSA, Polysomnography (PSG), involves an overnight hospital stay with multiple attached sensors, leading to potential inaccuracies due to the first-night effect. To address this, we present SlAction, a non-intrusive OSA detection system for daily sleep environments using infrared videos. Recognizing that sleep videos exhibit minimal motion, this work investigates the fundamental question: "Are respiratory events adequately reflected in human motions during sleep?" Analyzing the largest sleep video dataset of 5,098 hours, we establish correlations between OSA events and human motions during sleep. Our approach uses a low frame rate (2.5 FPS), a large size (60 seconds) and step (30 seconds) for sliding window analysis to capture slow and long-term motions related to OSA. Furthermore, we utilize a lightweight deep neural network for res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24191;&#27867;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.02712</link><description>&lt;p&gt;
&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#30340;&#21069;&#27839;&#65306;&#22609;&#36896;&#22810;&#20010;&#39046;&#22495;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
Unveiling the frontiers of deep learning: innovations shaping diverse domains. (arXiv:2309.02712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24191;&#27867;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20351;&#24471;&#24320;&#21457;&#33021;&#22815;&#23398;&#20064;&#12289;&#21487;&#35270;&#21270;&#12289;&#20248;&#21270;&#12289;&#25913;&#36827;&#21644;&#39044;&#27979;&#25968;&#25454;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;DL&#24050;&#32463;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#22788;&#29702;&#12289;&#20892;&#19994;&#12289;&#20132;&#36890;&#39044;&#27979;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#29983;&#29289;&#21307;&#23398;&#12289;&#28798;&#23475;&#31649;&#29702;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#33647;&#29289;&#35774;&#35745;&#12289;&#22522;&#22240;&#32452;&#23398;&#12289;&#20154;&#33080;&#35782;&#21035;&#21644;&#29983;&#24577;&#23398;&#12290;&#20026;&#20102;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#26377;&#24517;&#35201;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20123;&#23398;&#31185;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#32570;&#20047;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#25152;&#26377;&#28508;&#22312;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#30340;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24191;&#27867;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#25152;&#26377;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;&#27491;&#22914;&#25991;&#29486;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;DL&#22312;&#39044;&#27979;&#21644;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#20986;&#20934;&#30830;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#35745;&#31639;&#24037;&#20855;&#65292;&#24182;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) enables the development of computer models that are capable of learning, visualizing, optimizing, refining, and predicting data. In recent years, DL has been applied in a range of fields, including audio-visual data processing, agriculture, transportation prediction, natural language, biomedicine, disaster management, bioinformatics, drug design, genomics, face recognition, and ecology. To explore the current state of deep learning, it is necessary to investigate the latest developments and applications of deep learning in these disciplines. However, the literature is lacking in exploring the applications of deep learning in all potential sectors. This paper thus extensively investigates the potential applications of deep learning across all major fields of study as well as the associated benefits and challenges. As evidenced in the literature, DL exhibits accuracy in prediction and analysis, makes it a powerful computational tool, and has the ability to articulate i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#39640;&#25928;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.02711</link><description>&lt;p&gt;
&#35299;&#20915;&#19981;&#23436;&#20840;&#23545;&#31216;&#24615;&#65306;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Addressing Imperfect Symmetry: a Novel Symmetry-Learning Actor-Critic Extension. (arXiv:2309.02711v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#39640;&#25928;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#26159;&#29702;&#35299;&#25105;&#20204;&#30340;&#29615;&#22659;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20294;&#24448;&#24448;&#20174;&#25968;&#23398;&#30340;&#35282;&#24230;&#36807;&#20110;&#31616;&#21270;&#20102;&#29616;&#23454;&#12290;&#20154;&#31867;&#26159;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#65292;&#22806;&#35980;&#21644;&#35748;&#30693;&#20559;&#35265;&#65288;&#20363;&#22914;&#26377;&#19968;&#21482;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#25163;&#65289;&#37117;&#19981;&#23436;&#32654;&#22320;&#20559;&#31163;&#20102;&#23545;&#31216;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#30340;&#22823;&#33041;&#24456;&#23481;&#26131;&#20811;&#26381;&#36825;&#20123;&#19981;&#23436;&#32654;&#24182;&#39640;&#25928;&#22320;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#30340;&#39537;&#21160;&#21160;&#26426;&#22312;&#20110;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25429;&#25417;&#36825;&#31181;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;-&#19968;&#31181;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#25193;&#23637;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#19968;&#20010;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#19968;&#20010;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#22312;&#25152;&#26377;&#29366;&#24577;&#20013;&#24378;&#21046;&#23454;&#26045;&#20849;&#21516;&#30340;&#23545;&#31216;&#20851;&#31995;&#65292;&#24182;&#36866;&#24212;&#20102;&#25152;&#23398;&#31574;&#30053;&#12290;&#23558;ASL&#30340;&#24615;&#33021;&#19982;&#29616;&#26377;&#30340;&#23545;&#31216;&#22686;&#24378;&#26041;&#27861;&#22312;&#19968;&#20010;&#28041;&#21450;&#22235;&#36275;&#34434;&#34433;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry, a fundamental concept to understand our environment, often oversimplifies reality from a mathematical perspective. Humans are a prime example, deviating from perfect symmetry in terms of appearance and cognitive biases (e.g. having a dominant hand). Nevertheless, our brain can easily overcome these imperfections and efficiently adapt to symmetrical tasks. The driving motivation behind this work lies in capturing this ability through reinforcement learning. To this end, we introduce Adaptive Symmetry Learning (ASL) $\unicode{x2013}$ a model-minimization actor-critic extension that addresses incomplete or inexact symmetry descriptions by adapting itself during the learning process. ASL consists of a symmetry fitting component and a modular loss function that enforces a common symmetric relation across all states while adapting to the learned policy. The performance of ASL is compared to existing symmetry-enhanced methods in a case study involving a four-legged ant model for mul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;</title><link>http://arxiv.org/abs/2309.02705</link><description>&lt;p&gt;
&#35777;&#26126;LLM&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#23433;&#20840;&#65292;&#20844;&#24320;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#8220;&#27169;&#22411;&#23545;&#40784;&#8221;&#38450;&#25252;&#25514;&#26045;&#12290;&#19968;&#20010;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#25932;&#23545;&#25552;&#31034;&#21253;&#21547;&#24694;&#24847;&#35774;&#35745;&#30340;&#26631;&#35760;&#24207;&#21015;&#65292;&#20197;&#35268;&#36991;&#27169;&#22411;&#30340;&#23433;&#20840;&#38450;&#25252;&#24182;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#31532;&#19968;&#20010;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#12290;&#25105;&#20204;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#12290;&#22914;&#26524;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#27979;&#21040;&#20219;&#20309;&#23376;&#24207;&#21015;&#25110;&#36755;&#20837;&#25552;&#31034;&#26377;&#23475;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#23558;&#36755;&#20837;&#25552;&#31034;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#36825;&#20445;&#35777;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#22823;&#23567;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#30340;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#20063;&#23558;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#23545;&#25239;&#19977;&#31181;&#25915;&#20987;&#27169;&#24335;&#65306;i)&#25932;&#23545;&#21518;&#32512;&#65292;&#21363;&#38468;&#21152;&#25932;&#23545;&#24207;&#21015;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02685</link><description>&lt;p&gt;
Diffusion-EDFs: &#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#39564;&#35777;&#20102;&#31561;&#21464;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#34892;&#20026;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#23558;&#31354;&#38388;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#21363;SE(3)&#31561;&#21464;&#24615;&#24341;&#20837;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;SE(3)&#31561;&#21464;&#24615;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#22312;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#26102;&#21482;&#38656;5&#21040;10&#20010;&#20219;&#21153;&#28436;&#31034;&#21363;&#21487;&#12290;&#27492;&#22806;&#65292;&#19982;&#20043;&#21069;&#22522;&#20110;&#25193;&#25955;&#30340;&#25805;&#20316;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have verified that equivariant methods can significantly improve the data efficiency, generalizability, and robustness in robot learning. Meanwhile, denoising diffusion-based generative modeling has recently gained significant attention as a promising approach for robotic manipulation learning from demonstrations with stochastic behaviors. In this paper, we present Diffusion-EDFs, a novel approach that incorporates spatial roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative modeling. By integrating SE(3)-equivariance into our model architectures, we demonstrate that our proposed method exhibits remarkable data efficiency, requiring only 5 to 10 task demonstrations for effective end-to-end training. Furthermore, our approach showcases superior generalizability compared to previous diffusion-based manipulation methods.
&lt;/p&gt;</description></item><item><title>RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.02671</link><description>&lt;p&gt;
RLSynC: &#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#26041;&#27861;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
RLSynC: Offline-Online Reinforcement Learning for Synthon Completion. (arXiv:2309.02671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02671
&lt;/p&gt;
&lt;p&gt;
RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26159;&#30830;&#23450;&#33021;&#22815;&#21453;&#24212;&#24418;&#25104;&#25152;&#38656;&#20135;&#29289;&#30340;&#19968;&#32452;&#21453;&#24212;&#29289;&#20998;&#23376;&#30340;&#36807;&#31243;&#12290;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#39318;&#20808;&#39044;&#27979;&#20135;&#29289;&#20013;&#30340;&#21453;&#24212;&#20013;&#24515;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#21512;&#25104;&#29289;&#37325;&#26032;&#34917;&#20840;&#25104;&#21453;&#24212;&#29289;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#24517;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39640;&#23454;&#29992;&#24615;&#65292;&#20197;&#25351;&#23548;&#21512;&#25104;&#35268;&#21010;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;RLSynC&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#26041;&#27861;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;RLSynC&#20026;&#27599;&#20010;&#21512;&#25104;&#29289;&#20998;&#37197;&#19968;&#20010;&#20195;&#29702;&#65292;&#25152;&#26377;&#20195;&#29702;&#37117;&#36890;&#36807;&#21516;&#27493;&#36827;&#34892;&#36880;&#27493;&#34892;&#21160;&#65292;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#12290;RLSynC&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#20132;&#20114;&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#20174;&#32780;&#21487;&#20197;&#25506;&#32034;&#26032;&#30340;&#21453;&#24212;&#31354;&#38388;&#12290;RLSynC&#20351;&#29992;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#21453;&#24212;&#29289;&#22312;&#21512;&#25104;&#20135;&#29289;&#26102;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the process of determining the set of reactant molecules that can react to form a desired product. Semi-template-based retrosynthesis methods, which imitate the reverse logic of synthesis reactions, first predict the reaction centers in the products, and then complete the resulting synthons back into reactants. These methods enable necessary interpretability and high practical utility to inform synthesis planning. We develop a new offline-online reinforcement learning method RLSynC for synthon completion in semi-template-based methods. RLSynC assigns one agent to each synthon, all of which complete the synthons by conducting actions step by step in a synchronized fashion. RLSynC learns the policy from both offline training episodes and online interactions which allow RLSynC to explore new reaction spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the predicted reactants in synthesizing a product, and thus guides the action search. We compare 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31354;&#38388;&#31890;&#23376;&#30340;&#23376;&#38598;&#27979;&#24230;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#31895;&#31934;&#20851;&#31995;&#21644;&#25805;&#20316;&#12290;&#36890;&#36807;&#36825;&#20123;&#27010;&#24565;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#31895;&#31961;&#38598;&#27169;&#22411;&#21644;&#31354;&#38388;&#31895;&#31961;&#31890;&#23376;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#32467;&#26500;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.02662</link><description>&lt;p&gt;
&#31354;&#38388;&#31890;&#23376;&#30340;&#23376;&#38598;&#27979;&#24230;
&lt;/p&gt;
&lt;p&gt;
Subsethood Measures of Spatial Granules. (arXiv:2309.02662v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31354;&#38388;&#31890;&#23376;&#30340;&#23376;&#38598;&#27979;&#24230;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#31895;&#31934;&#20851;&#31995;&#21644;&#25805;&#20316;&#12290;&#36890;&#36807;&#36825;&#20123;&#27010;&#24565;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#31895;&#31961;&#38598;&#27169;&#22411;&#21644;&#31354;&#38388;&#31895;&#31961;&#31890;&#23376;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#32467;&#26500;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#38598;&#27979;&#24230;&#29992;&#20110;&#27979;&#37327;&#38598;&#21512;&#21253;&#21547;&#20851;&#31995;&#30340;&#31243;&#24230;&#65292;&#22312;&#27169;&#31946;&#38598;&#21512;&#29702;&#35770;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31354;&#38388;&#31890;&#23376;&#12289;&#31895;&#31934;&#20851;&#31995;&#20197;&#21450;meet&#12289;join&#12289;quotient meet&#21644;quotient join&#31561;&#25805;&#20316;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;&#25152;&#26377;&#21407;&#23376;&#31890;&#23376;&#21487;&#20197;&#36890;&#36807;&#38598;&#21512;&#21253;&#21547;&#20851;&#31995;&#36827;&#34892;&#23618;&#27425;&#21270;&#65292;&#25152;&#26377;&#31890;&#23376;&#21487;&#20197;&#36890;&#36807;&#31895;&#31934;&#20851;&#31995;&#36827;&#34892;&#23618;&#27425;&#21270;&#12290;&#36890;&#36807;&#20174;&#24494;&#35266;&#21644;&#23439;&#35266;&#30340;&#35270;&#35282;&#30475;&#24453;&#20449;&#24687;&#31995;&#32479;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#24494;&#35266;&#30693;&#35782;&#31354;&#38388;&#21644;&#23439;&#35266;&#30693;&#35782;&#31354;&#38388;&#65292;&#20174;&#20013;&#20998;&#21035;&#33719;&#24471;&#19968;&#20010;&#31895;&#31961;&#38598;&#27169;&#22411;&#21644;&#19968;&#20010;&#31354;&#38388;&#31895;&#31961;&#31890;&#23376;&#27169;&#22411;&#12290;&#32463;&#36807;&#20174;&#24494;&#35266;&#30693;&#35782;&#31354;&#38388;&#23548;&#20986;&#30340;&#31895;&#31961;&#38598;&#27169;&#22411;&#65292;&#26159;&#32463;&#20856;&#31895;&#31961;&#38598;&#27169;&#22411;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#32780;&#31354;&#38388;&#31895;&#31961;&#31890;&#23376;&#27169;&#22411;&#23558;&#22312;&#32467;&#26500;&#38382;&#39064;&#30340;&#35299;&#20915;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21313;&#20108;&#20010;&#21333;&#35843;&#36882;&#22686;&#30340;&#23376;&#38598;&#27979;&#24230;&#20844;&#29702;&#21644;&#21313;&#20108;&#20010;&#30456;&#24212;&#30340;&#21333;&#35843;&#36882;&#20943;&#30340;&#36229;&#38598;&#27979;&#24230;&#20844;&#29702;&#65292;&#24182;&#25512;&#24191;&#20102;&#23376;&#38598;&#27979;&#24230;&#21644;&#36229;&#38598;&#27979;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subsethood, which is to measure the degree of set inclusion relation, is predominant in fuzzy set theory. This paper introduces some basic concepts of spatial granules, coarse-fine relation, and operations like meet, join, quotient meet and quotient join. All the atomic granules can be hierarchized by set-inclusion relation and all the granules can be hierarchized by coarse-fine relation. Viewing an information system from the micro and the macro perspectives, we can get a micro knowledge space and a micro knowledge space, from which a rough set model and a spatial rough granule model are respectively obtained. The classical rough set model is the special case of the rough set model induced from the micro knowledge space, while the spatial rough granule model will be play a pivotal role in the problem-solving of structures. We discuss twelve axioms of monotone increasing subsethood and twelve corresponding axioms of monotone decreasing supsethood, and generalize subsethood and supsetho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#26041;&#38754;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#25925;&#38556;&#39044;&#27979;&#65292;&#26377;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#26085;&#24535;&#21644;&#25552;&#20379;&#39044;&#27979;&#20540;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.02641</link><description>&lt;p&gt;
TFBEST: &#20855;&#26377;&#21487;&#23398;&#20064;&#20301;&#32622;&#32534;&#30721;&#30340;&#21452;&#37325;&#26041;&#38754;Transformer&#29992;&#20110;&#25925;&#38556;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for Failure Prediction. (arXiv:2309.02641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#26041;&#38754;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#25925;&#38556;&#39044;&#27979;&#65292;&#26377;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#26085;&#24535;&#21644;&#25552;&#20379;&#39044;&#27979;&#20540;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20013;&#30340;&#30828;&#30424;&#25925;&#38556;&#26159;&#26114;&#36149;&#30340; - &#20174;&#28798;&#38590;&#24615;&#30340;&#25968;&#25454;&#20002;&#22833;&#21040;&#21830;&#19994;&#20449;&#35465;&#30340;&#38382;&#39064;&#65292;&#21033;&#30410;&#30456;&#20851;&#32773;&#24076;&#26395;&#20687;&#30239;&#30123;&#19968;&#26679;&#36991;&#20813;&#23427;&#12290;&#31215;&#26497;&#30417;&#27979;&#30828;&#30424;&#25925;&#38556;&#30340;&#37325;&#35201;&#24037;&#20855;&#26159;&#21450;&#26102;&#20272;&#35745;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#20026;&#27492;&#65292;&#30828;&#30424;&#39537;&#21160;&#22120;&#20869;&#37096;&#20351;&#29992;&#30340;&#33258;&#25105;&#30417;&#27979;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#25216;&#26415;&#65288;S.M.A.R.T.&#65289;&#20026;&#36825;&#20123;&#37325;&#35201;&#25968;&#25454;&#23384;&#20648;&#35774;&#22791;&#30340;&#38271;&#26399;&#32500;&#25252;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#26085;&#24535;&#12290;&#36807;&#21435;&#30340;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#32463;&#24120;&#20351;&#29992;&#36825;&#20123;S.M.A.R.T.&#26085;&#24535;&#21644;&#22522;&#20110;CNN/RNN&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25552;&#20379;&#39044;&#27979;&#30340;RUL&#20540;&#30340;&#32622;&#20449;&#21306;&#38388;&#20197;&#21450;&#22788;&#29702;&#38750;&#24120;&#38271;&#30340;&#26085;&#24535;&#24207;&#21015;&#26041;&#38754;&#36935;&#21040;&#20102;&#37325;&#22823;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#20351;&#29992;LSTM&#31561;&#26041;&#27861;&#30340;&#30740;&#31350;&#22312;&#35757;&#32451;&#36895;&#24230;&#19978;&#24456;&#24930;&#65292;&#24182;&#19988;&#38656;&#35201;&#32321;&#29712;&#30340;&#29305;&#24449;&#24037;&#31243;&#24320;&#38144;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#23398;&#20064;&#20301;&#32622;&#32534;&#30721;&#30340;&#21452;&#37325;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hard Disk Drive (HDD) failures in datacenters are costly - from catastrophic data loss to a question of goodwill, stakeholders want to avoid it like the plague. An important tool in proactively monitoring against HDD failure is timely estimation of the Remaining Useful Life (RUL). To this end, the Self-Monitoring, Analysis and Reporting Technology employed within HDDs (S.M.A.R.T.) provide critical logs for long-term maintenance of the security and dependability of these essential data storage devices. Data-driven predictive models in the past have used these S.M.A.R.T. logs and CNN/RNN based architectures heavily. However, they have suffered significantly in providing a confidence interval around the predicted RUL values as well as in processing very long sequences of logs. In addition, some of these approaches, such as those based on LSTMs, are inherently slow to train and have tedious feature engineering overheads. To overcome these challenges, in this work we propose a novel transfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02632</link><description>&lt;p&gt;
&#20174;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#20013;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning from Hierarchical Weak Preference Feedback. (arXiv:2309.02632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#35774;&#35745;&#26159;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20363;&#22914;&#20351;&#29992;&#33509;&#24178;&#20010;&#22870;&#21169;&#22240;&#23376;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22870;&#21169;&#24037;&#31243;&#21463;&#21040;&#36817;&#20284;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35843;&#20248;&#25104;&#26412;&#65292;&#24182;&#19988;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#22797;&#26434;&#20219;&#21153;&#25152;&#38656;&#30340;&#32454;&#31890;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#22256;&#38590;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#36716;&#21521;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20174;&#36712;&#36857;&#24207;&#21015;&#23545;&#20043;&#38388;&#30340;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#24314;&#27169;&#65292;RLHF&#23398;&#20064;&#21040;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#65292;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;RLHF&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#33719;&#24471;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#37492;&#20110;&#36825;&#20010;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20351;&#29992;&#26356;&#23569;&#20154;&#21147;&#25237;&#20837;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward design is a fundamental, yet challenging aspect of practical reinforcement learning (RL). For simple tasks, researchers typically handcraft the reward function, e.g., using a linear combination of several reward factors. However, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#20026;&#24868;&#24594;&#30340;&#23567;&#40479;&#29983;&#25104;&#22797;&#26434;&#19988;&#31283;&#23450;&#30340;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;GANs&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#24868;&#24594;&#30340;&#23567;&#40479;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.02614</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20026;&#24868;&#24594;&#30340;&#23567;&#40479;&#29983;&#25104;&#31283;&#23450;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds. (arXiv:2309.02614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#20026;&#24868;&#24594;&#30340;&#23567;&#40479;&#29983;&#25104;&#22797;&#26434;&#19988;&#31283;&#23450;&#30340;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;GANs&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#24868;&#24594;&#30340;&#23567;&#40479;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#29289;&#29702;&#22522;&#30784;&#25340;&#22270;&#28216;&#25103;&#24868;&#24594;&#30340;&#23567;&#40479;&#20013;&#31283;&#23450;&#32467;&#26500;&#30340;&#36866;&#29992;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#23545;&#20110;&#20851;&#21345;&#29983;&#25104;&#30340;GANs&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#22522;&#20110;&#29926;&#29255;&#30340;&#34920;&#31034;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#21019;&#24314;&#30001;&#22810;&#20010;&#36739;&#23567;&#22359;&#32452;&#25104;&#30340;&#31283;&#23450;&#32467;&#26500;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#21253;&#25324;&#20102;&#35814;&#32454;&#30340;&#32534;&#30721;/&#35299;&#30721;&#36807;&#31243;&#65292;&#23558;&#24868;&#24594;&#30340;&#23567;&#40479;&#20851;&#21345;&#25551;&#36848;&#36716;&#25442;&#20026;&#36866;&#21512;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#34920;&#31034;&#65292;&#20197;&#21450;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;GAN&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#30340;&#32467;&#26500;&#35774;&#35745;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GANs&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#25104;&#21508;&#31181;&#22797;&#26434;&#19988;&#31283;&#23450;&#30340;&#24868;&#24594;&#30340;&#23567;&#40479;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the suitability of using Generative Adversarial Networks (GANs) to generate stable structures for the physics-based puzzle game Angry Birds. While previous applications of GANs for level generation have been mostly limited to tile-based representations, this paper explores their suitability for creating stable structures made from multiple smaller blocks. This includes a detailed encoding/decoding process for converting between Angry Birds level descriptions and a suitable grid-based representation, as well as utilizing state-of-the-art GAN architectures and training methods to produce new structure designs. Our results show that GANs can be successfully applied to generate a varied range of complex and stable Angry Birds structures.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#23433;&#20840;&#20851;&#38190;&#30340;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#30340;&#25805;&#20316;&#36755;&#20986;&#29305;&#24615;&#30340;&#38543;&#26426;&#31526;&#21512;&#24615;&#65292;&#20174;&#32780;&#21457;&#29616;&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;&#24182;&#35780;&#20272;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.02603</link><description>&lt;p&gt;
&#20351;&#29992;&#32479;&#35745;&#23398;&#19982;&#29289;&#29702;&#24341;&#23548;&#30340;&#36807;&#31243;&#27169;&#22411;&#22312;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#20013;&#26816;&#27979;&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Detection of Unknown-Unknowns in Cyber-Physical Systems using Statistical Conformance with Physics Guided Process Models. (arXiv:2309.02603v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#23433;&#20840;&#20851;&#38190;&#30340;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#30340;&#25805;&#20316;&#36755;&#20986;&#29305;&#24615;&#30340;&#38543;&#26426;&#31526;&#21512;&#24615;&#65292;&#20174;&#32780;&#21457;&#29616;&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;&#24182;&#35780;&#20272;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;&#26159;&#25351;&#22312;&#35774;&#35745;&#21644;&#27979;&#35797;&#38454;&#27573;&#26410;&#32771;&#34385;&#21040;&#30340;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#20013;&#30340;&#25805;&#20316;&#22330;&#26223;&#12290;&#22312;&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;&#19979;&#65292;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#30340;&#25805;&#20316;&#34892;&#20026;&#19981;&#33021;&#20445;&#35777;&#28385;&#36275;&#36890;&#36807;&#36755;&#20986;&#36712;&#36857;&#19978;&#30340;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#25351;&#23450;&#30340;&#23433;&#20840;&#24615;&#21644;&#26377;&#25928;&#24615;&#31561;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#23433;&#20840;&#20851;&#38190;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#25805;&#20316;&#36755;&#20986;&#29305;&#24615;&#30340;&#38543;&#26426;&#31526;&#21512;&#24615;&#65292;&#21487;&#20197;&#21457;&#29616;&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;&#24182;&#35780;&#20272;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#21147;&#23398;&#35825;&#23548;&#30340;&#28151;&#21512;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;DiH-RNN&#65289;&#65292;&#29992;&#20110;&#25366;&#25496;&#29289;&#29702;&#24341;&#23548;&#30340;&#20195;&#29702;&#27169;&#22411;&#65288;PGSM&#65289;&#65292;&#24182;&#20351;&#29992;STL&#23545;&#27169;&#22411;&#31995;&#25968;&#36827;&#34892;&#27169;&#22411;&#19968;&#33268;&#24615;&#26816;&#26597;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36890;&#36807;&#26410;&#30693;&#33008;&#23707;&#32032;&#21345;&#24102;&#38169;&#35823;&#23548;&#33268;&#20154;&#24037;&#33008;&#33146;(AP)&#25805;&#20316;&#21464;&#21270;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unknown unknowns are operational scenarios in a cyber-physical system that are not accounted for in the design and test phase. As such under unknown-unknown scenarios, the operational behavior of the CPS is not guaranteed to meet requirements such as safety and efficacy specified using Signal Temporal Logic (STL) on the output trajectories. We propose a novel framework for analyzing the stochastic conformance of operational output characteristics of safety-critical cyber-physical systems that can discover unknown-unknown scenarios and evaluate potential safety hazards. We propose dynamics-induced hybrid recurrent neural networks (DiH-RNN) to mine a physics-guided surrogate model (PGSM) which is used to check the model conformance using STL on the model coefficients. We demonstrate the detection of operational changes in an Artificial Pancreas(AP) due to unknown insulin cartridge errors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#35780;&#20272;&#20102;&#30701;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;&#36951;&#20256;&#31639;&#27861;&#12289;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#21644;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#65289;&#22312;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#20840;&#23616;&#20248;&#21270;&#20013;&#30340;&#20248;&#21183;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02600</link><description>&lt;p&gt;
&#30701;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comparative Evaluation of Metaheuristic Algorithms for Hyperparameter Selection in Short-Term Weather Forecasting. (arXiv:2309.02600v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#35780;&#20272;&#20102;&#30701;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;&#36951;&#20256;&#31639;&#27861;&#12289;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#21644;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#65289;&#22312;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#20840;&#23616;&#20248;&#21270;&#20013;&#30340;&#20248;&#21183;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#25253;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#22825;&#27668;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#20173;&#28982;&#26159;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#38500;&#20102;&#33258;&#22238;&#24402;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65288;&#22914;ARIMA&#65289;&#20043;&#22806;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#26222;&#36890;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;LSTM&#21644;GRU&#32593;&#32476;&#65289;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#26174;&#31034;&#20986;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24212;&#29992;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21363;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#12289;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#65288;DE&#65289;&#21644;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#65288;PSO&#65289;&#65292;&#33258;&#21160;&#25628;&#32034;&#36825;&#20123;&#27169;&#22411;&#32467;&#26500;&#20013;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#12290;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20840;&#23616;&#20248;&#21270;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#20855;&#26377;&#22788;&#29702;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#40065;&#26834;&#24615;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#23545;&#38598;&#25104;&#20102;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#30340;&#19981;&#21516;&#27169;&#22411;&#32467;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#34920;&#29616;&#65292;&#22522;&#20110;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#31561;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of weather systems remains a challenge for traditional statistical models. Apart from Auto Regressive time forecasting models like ARIMA, deep learning techniques (Vanilla ANNs, LSTM and GRU networks), have shown promise in improving forecasting accuracy by capturing temporal dependencies. This paper explores the application of metaheuristic algorithms, namely Genetic Algorithm (GA), Differential Evolution (DE), and Particle Swarm Optimization (PSO), to automate the search for optimal hyperparameters in these model architectures. Metaheuristic algorithms excel in global optimization, offering robustness, versatility, and scalability in handling non-linear problems. We present a comparative analysis of different model architectures integrated with metaheuristic optimization, evaluating their performance in weather forecasting based on metrics such as Mean Squared Error (MSE) and Mea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35745;&#31639;&#26356;&#39640;&#32500;&#24230;&#20013;&#30340;&#26368;&#23567;&#26354;&#38754;&#30340;&#25968;&#20540;&#36924;&#36817;&#65292;&#35299;&#20915;&#20102;&#32500;&#25968;&#35781;&#21650;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#27809;&#26377;GPU&#30340;&#31508;&#35760;&#26412;&#30005;&#33041;&#19978;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.02589</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#26356;&#39640;&#32500;&#24230;&#20013;&#30340;&#26368;&#23567;&#26354;&#38754;
&lt;/p&gt;
&lt;p&gt;
Using Physics-Informed Neural Networks to Calculate Minimal Surfaces in Higher Dimensions. (arXiv:2309.02589v1 [math.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35745;&#31639;&#26356;&#39640;&#32500;&#24230;&#20013;&#30340;&#26368;&#23567;&#26354;&#38754;&#30340;&#25968;&#20540;&#36924;&#36817;&#65292;&#35299;&#20915;&#20102;&#32500;&#25968;&#35781;&#21650;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#27809;&#26377;GPU&#30340;&#31508;&#35760;&#26412;&#30005;&#33041;&#19978;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#26356;&#39640;&#32500;&#24230;&#20013;&#26368;&#23567;&#26354;&#38754;&#30340;&#25968;&#20540;&#36924;&#36817;&#65292;&#36825;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#31867;&#22411;&#12290;&#30001;&#20110;&#32500;&#25968;&#30340;&#35781;&#21650;&#23548;&#33268;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#20250;&#38543;&#32500;&#25968;&#22686;&#21152;&#32780;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#36828;&#36828;&#36229;&#20986;&#20219;&#20309;&#29616;&#20195;&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#21482;&#26377;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#25165;&#33021;&#22815;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#36873;&#25321;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#19968;&#31181;&#31216;&#20026;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;Physics-Informed Neural Network&#65292;PINN&#65289;&#30340;&#27169;&#22411;&#65292;&#23427;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;Deep Neural Network&#65292;DNN&#65289;&#26469;&#35299;&#20915;&#26368;&#23567;&#26354;&#38754;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#23427;&#21487;&#20197;&#22312;&#26356;&#39640;&#32500;&#24230;&#19978;&#25193;&#23637;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#27809;&#26377;GPU&#30340;&#31508;&#35760;&#26412;&#30005;&#33041;&#19978;&#20063;&#33021;&#30456;&#23545;&#24555;&#36895;&#22320;&#35757;&#32451;&#12290;&#30001;&#20110;&#26080;&#27861;&#26597;&#30475;&#39640;&#32500;&#24230;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#20197;&#36275;&#22815;&#30340;&#22266;&#23450;&#36724;&#30340;&#29255;&#27573;&#24418;&#24335;&#21576;&#29616;&#65292;&#20197;&#20415;&#36890;&#36807;3D&#22270;&#24418;&#36827;&#34892;&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we compute numerical approximations of the minimal surfaces, an essential type of Partial Differential Equation (PDE), in higher dimensions. Classical methods cannot handle it in this case because of the Curse of Dimensionality, where the computational cost of these methods increases exponentially fast in response to higher problem dimensions, far beyond the computing capacity of any modern supercomputers. Only in the past few years have machine learning researchers been able to mitigate this problem. The solution method chosen here is a model known as a Physics-Informed Neural Network (PINN) which trains a deep neural network (DNN) to solve the minimal surface PDE. It can be scaled up into higher dimensions and trained relatively quickly even on a laptop with no GPU. Due to the inability to view the high-dimension output, our data is presented as snippets of a higher-dimension shape with enough fixed axes so that it is viewable with 3-D graphs. Not only will the functio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.02583</link><description>&lt;p&gt;
&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Sequential Volumetric Design Tasks. (arXiv:2309.02583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#31215;&#35774;&#35745;&#65292;&#20063;&#31216;&#20026;&#36136;&#37327;&#35774;&#35745;&#65292;&#26159;&#19987;&#19994;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#31532;&#19968;&#27493;&#20851;&#38190;&#24615;&#20219;&#21153;&#65292;&#20855;&#26377;&#39034;&#24207;&#24615;&#12290;&#30001;&#20110;&#20307;&#31215;&#35774;&#35745;&#36807;&#31243;&#22797;&#26434;&#65292;&#39034;&#24207;&#21270;&#35774;&#35745;&#36807;&#31243;&#20013;&#21253;&#21547;&#20102;&#23545;&#35774;&#35745;&#24072;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#34987;&#25237;&#20837;&#21040;&#33258;&#21160;&#29983;&#25104;&#21512;&#29702;&#30340;&#20307;&#31215;&#35774;&#35745;&#19978;&#65292;&#20294;&#29983;&#25104;&#30340;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#35780;&#20272;&#19968;&#20010;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#38656;&#35201;&#19968;&#22871;&#36807;&#20110;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#19987;&#19994;&#30693;&#35782;&#12290;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23398;&#20064;&#26368;&#32456;&#35774;&#35745;&#65292;&#32780;&#19981;&#26159;&#39034;&#24207;&#35774;&#35745;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19987;&#23478;&#25110;&#39640;&#24615;&#33021;&#35774;&#35745;&#24207;&#21015;&#30340;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25152;&#23398;&#30340;&#34920;&#31034;&#22312;&#20851;&#38190;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#22914;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;prefer
&lt;/p&gt;
&lt;p&gt;
Volumetric design, also called massing design, is the first and critical step in professional building design which is sequential in nature. As the volumetric design process is complex, the underlying sequential design process encodes valuable information for designers. Many efforts have been made to automatically generate reasonable volumetric designs, but the quality of the generated design solutions varies, and evaluating a design solution requires either a prohibitively comprehensive set of metrics or expensive human expertise. While previous approaches focused on learning only the final design instead of sequential design tasks, we propose to encode the design knowledge from a collection of expert or high-performing design sequences and extract useful representations using transformer-based models. Later we propose to utilize the learned representations for crucial downstream applications such as design preference evaluation and procedural design generation. We develop the prefere
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20799;&#31461;&#24739;&#32773;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#38590;&#27835;&#24615;&#30315;&#30187;&#30340;&#33041;&#32593;&#32476;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#30315;&#30187;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.02580</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25581;&#31034;&#38590;&#27835;&#24615;&#30315;&#30187;&#33041;&#32593;&#32476;&#65306;&#19968;&#31181;&#38024;&#23545;&#20799;&#31461;&#24739;&#32773;&#21333;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#30315;&#30187;&#39044;&#27979;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling Intractable Epileptogenic Brain Networks with Deep Learning Algorithms: A Novel and Comprehensive Framework for Scalable Seizure Prediction with Unimodal Neuroimaging Data in Pediatric Patients. (arXiv:2309.02580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20799;&#31461;&#24739;&#32773;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#38590;&#27835;&#24615;&#30315;&#30187;&#30340;&#33041;&#32593;&#32476;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#30315;&#30187;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#20840;&#29699;&#26377;5000&#19975;&#20154;&#21463;&#21040;&#24433;&#21709;&#65292;&#32654;&#22269;&#26377;120&#19975;&#20154;&#21463;&#21040;&#24433;&#21709;&#12290;&#23384;&#22312;&#30528;&#22823;&#37327;&#20799;&#31461;&#24739;&#32773;&#24739;&#26377;&#38590;&#27835;&#24615;&#30315;&#30187;&#65292;&#21363;&#30315;&#30187;&#21457;&#20316;&#26080;&#27861;&#24471;&#21040;&#25511;&#21046;&#12290;&#30315;&#30187;&#21457;&#20316;&#21487;&#33021;&#23548;&#33268;&#36523;&#20307;&#20260;&#23475;&#12289;&#36855;&#22833;&#26041;&#21521;&#12289;&#22833;&#21435;&#24847;&#35782;&#65292;&#20197;&#21450;&#20854;&#20182;&#21487;&#33021;&#22952;&#30861;&#20799;&#31461;&#21442;&#19982;&#26085;&#24120;&#27963;&#21160;&#30340;&#30151;&#29366;&#12290;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#21487;&#20197;&#24110;&#21161;&#23478;&#38271;&#21644;&#21307;&#25252;&#20154;&#21592;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#65292;&#36991;&#20813;&#21361;&#38505;&#24773;&#20917;&#65292;&#24182;&#35753;&#20799;&#31461;&#22312;&#38754;&#23545;&#30315;&#30187;&#30340;&#19981;&#30830;&#23450;&#24615;&#26102;&#20943;&#23569;&#28966;&#34385;&#21644;&#32039;&#24352;&#24863;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20197;&#33041;&#30005;&#22270;&#20449;&#21495;&#20026;&#22522;&#30784;&#30340;&#21333;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#36827;&#34892;&#30315;&#30187;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy is a prevalent neurological disorder affecting 50 million individuals worldwide and 1.2 million Americans. There exist millions of pediatric patients with intractable epilepsy, a condition in which seizures fail to come under control. The occurrence of seizures can result in physical injury, disorientation, unconsciousness, and additional symptoms that could impede children's ability to participate in everyday tasks. Predicting seizures can help parents and healthcare providers take precautions, prevent risky situations, and mentally prepare children to minimize anxiety and nervousness associated with the uncertainty of a seizure. This research proposes a novel and comprehensive framework to predict seizures in pediatric patients by evaluating machine learning algorithms on unimodal neuroimaging data consisting of electroencephalogram signals. The bandpass filtering and independent component analysis proved to be effective in reducing the noise and artifacts from the dataset. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;Diffusion+&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#39640;&#25928;&#22320;&#25554;&#34917;&#32570;&#22833;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#24494;&#36719;365&#30340;&#25968;&#25454;&#36136;&#37327;&#65292;&#36827;&#32780;&#25913;&#21892;&#20102;&#19979;&#28216;&#25925;&#38556;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02564</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#24494;&#36719;365&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Time Series Data Imputation for Microsoft 365. (arXiv:2309.02564v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;Diffusion+&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#39640;&#25928;&#22320;&#25554;&#34917;&#32570;&#22833;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#24494;&#36719;365&#30340;&#25968;&#25454;&#36136;&#37327;&#65292;&#36827;&#32780;&#25913;&#21892;&#20102;&#19979;&#28216;&#25925;&#38556;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#24615;&#23545;&#20110;&#20687;&#24494;&#36719;365&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#20113;&#25925;&#38556;&#65288;&#22914;&#30913;&#30424;&#25925;&#38556;&#12289;&#33410;&#28857;&#25925;&#38556;&#31561;&#65289;&#23041;&#32961;&#21040;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#65292;&#23548;&#33268;&#22312;&#32447;&#26381;&#21153;&#20013;&#26029;&#21644;&#32463;&#27982;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#39044;&#27979;&#20113;&#25925;&#38556;&#24182;&#22312;&#25925;&#38556;&#21457;&#29983;&#20043;&#21069;&#37319;&#21462;&#31215;&#26497;&#34892;&#21160;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#39044;&#27979;&#20013;&#23384;&#22312;&#25968;&#25454;&#32570;&#22833;&#31561;&#25968;&#25454;&#36136;&#37327;&#24046;&#30340;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#30340;Diffusion+&#65288;&#19968;&#31181;&#26679;&#26412;&#25928;&#29575;&#39640;&#30340;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#39640;&#25928;&#22320;&#25554;&#34917;&#32570;&#22833;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21644;&#24212;&#29992;&#23454;&#36341;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#25925;&#38556;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability is extremely important for large-scale cloud systems like Microsoft 365. Cloud failures such as disk failure, node failure, etc. threaten service reliability, resulting in online service interruptions and economic loss. Existing works focus on predicting cloud failures and proactively taking action before failures happen. However, they suffer from poor data quality like data missing in model training and prediction, which limits the performance. In this paper, we focus on enhancing data quality through data imputation by the proposed Diffusion+, a sample-efficient diffusion model, to impute the missing data efficiently based on the observed data. Our experiments and application practice show that our model contributes to improving the performance of the downstream failure prediction task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#21462;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#21033;&#29992;&#35745;&#21010;CT&#22270;&#20687;&#26469;&#39044;&#27979;&#32923;&#38376;&#40158;&#29366;&#32454;&#32990;&#30284;&#21270;&#30103;&#25918;&#30103;&#21518;&#30340;&#26080;&#22797;&#21457;&#29983;&#23384;&#26399;&#12290;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#26174;&#33879;&#39044;&#27979;&#20102;RFS&#12290;</title><link>http://arxiv.org/abs/2309.02562</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35745;&#21010;CT&#30340;&#25918;&#23556;&#32452;&#23398;&#27169;&#22411;&#39044;&#27979;&#32923;&#38376;&#40158;&#29366;&#32454;&#32990;&#30284;&#21270;&#30103;&#25918;&#30103;&#30340;&#26080;&#22797;&#21457;&#29983;&#23384;&#26399;
&lt;/p&gt;
&lt;p&gt;
Recurrence-Free Survival Prediction for Anal Squamous Cell Carcinoma Chemoradiotherapy using Planning CT-based Radiomics Model. (arXiv:2309.02562v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02562
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#21033;&#29992;&#35745;&#21010;CT&#22270;&#20687;&#26469;&#39044;&#27979;&#32923;&#38376;&#40158;&#29366;&#32454;&#32990;&#30284;&#21270;&#30103;&#25918;&#30103;&#21518;&#30340;&#26080;&#22797;&#21457;&#29983;&#23384;&#26399;&#12290;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#26174;&#33879;&#39044;&#27979;&#20102;RFS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#32422;30%&#30340;&#38750;&#36716;&#31227;&#24615;&#32923;&#38376;&#40158;&#29366;&#32454;&#32990;&#30284;&#65288;ASCC&#65289;&#24739;&#32773;&#22312;&#21270;&#30103;&#25918;&#30103;&#21518;&#20250;&#20986;&#29616;&#22797;&#21457;&#65292;&#30446;&#21069;&#21487;&#29992;&#30340;&#20020;&#24202;&#21464;&#37327;&#23545;&#27835;&#30103;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#36739;&#24046;&#12290;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25918;&#23556;&#27835;&#30103;&#21069;&#30340;CT&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;ASCC&#24739;&#32773;&#22312;&#21270;&#30103;&#25918;&#30103;&#21518;&#30340;&#26080;&#22797;&#21457;&#29983;&#23384;&#26399;&#65288;RFS&#65289;&#12290;&#26041;&#27861;&#65306;&#20174;96&#21517;ASCC&#24739;&#32773;&#30340;&#35745;&#21010;CT&#22270;&#20687;&#20013;&#25552;&#21462;&#20102;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#12290;&#22312;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#20043;&#21069;&#65292;&#36890;&#36807;&#22810;&#21464;&#37327;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#30340;&#36880;&#27493;&#21069;&#21521;&#29305;&#24449;&#36873;&#25321;&#36873;&#20986;&#20102;&#26368;&#20339;&#29305;&#24449;&#38598;&#21512;&#12290;&#21033;&#29992;&#20116;&#27425;&#37325;&#22797;&#30340;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#65292;&#20174;&#19968;&#20010;&#22522;&#20110;&#26368;&#20339;&#29305;&#24449;&#38598;&#21512;&#30340;&#25918;&#23556;&#32452;&#23398;-&#20020;&#24202;&#32452;&#21512;&#27169;&#22411;&#20013;&#29983;&#25104;&#20102;RFS&#39044;&#27979;&#12290;&#37319;&#29992;Kaplan-Meier&#20998;&#26512;&#35780;&#20272;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#39118;&#38505;&#20998;&#23618;&#33021;&#21147;&#12290;&#32467;&#26524;&#65306;&#22522;&#20110;&#24418;&#29366;&#21644;&#32441;&#29702;&#30340;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#33021;&#22815;&#26174;&#33879;&#39044;&#27979;RFS&#12290;
&lt;/p&gt;
&lt;p&gt;
Objectives: Approximately 30% of non-metastatic anal squamous cell carcinoma (ASCC) patients will experience recurrence after chemoradiotherapy (CRT), and currently available clinical variables are poor predictors of treatment response. We aimed to develop a model leveraging information extracted from radiation pretreatment planning CT to predict recurrence-free survival (RFS) in ASCC patients after CRT. Methods: Radiomics features were extracted from planning CT images of 96 ASCC patients. Following pre-feature selection, the optimal feature set was selected via step-forward feature selection with a multivariate Cox proportional hazard model. The RFS prediction was generated from a radiomics-clinical combined model based on an optimal feature set with five repeats of five-fold cross validation. The risk stratification ability of the proposed model was evaluated with Kaplan-Meier analysis. Results: Shapeand texture-based radiomics features significantly predicted RFS. Compared to a c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29289;&#20307;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#29289;&#29702;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#22312;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02561</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physically Grounded Vision-Language Models for Robotic Manipulation. (arXiv:2309.02561v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29289;&#20307;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#29289;&#29702;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#22312;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#30740;&#31350;&#36827;&#23637;&#23548;&#33268;&#22312;&#35270;&#35273;&#38382;&#31572;&#21644;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#29616;&#22312;&#21487;&#20197;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VLMs&#22312;&#23545;&#24120;&#35265;&#29289;&#20307;&#30340;&#29289;&#29702;&#27010;&#24565;&#65288;&#20363;&#22914;&#26448;&#26009;&#12289;&#33030;&#24369;&#24615;&#65289;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#28041;&#21450;&#19982;&#36825;&#20123;&#29289;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#29289;&#29702;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhysObjects&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;36.9K&#20010;&#20247;&#21253;&#21644;417K&#20010;&#33258;&#21160;&#21270;&#30340;&#24120;&#35265;&#23478;&#23621;&#29289;&#21697;&#30340;&#29289;&#29702;&#27010;&#24565;&#27880;&#37322;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;PhysObjects&#19978;&#23545;VLM&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#23545;&#29289;&#29702;&#29289;&#20307;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25429;&#25417;&#36825;&#20123;&#27010;&#24565;&#30340;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23558;&#36825;&#20010;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;VLM&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PhysObjects, an object-centric dataset of 36.9K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically-grounded VLM in an interactive framework with a large languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.02553</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#32763;&#35793;&#30340;&#34892;&#20026;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#20013;&#30340;&#34892;&#20026;&#27979;&#35797;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#34892;&#20026;&#26469;&#32454;&#31890;&#24230;&#35780;&#20272;&#31995;&#32479;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26426;&#22120;&#32763;&#35793;&#20013;&#34892;&#20026;&#27979;&#35797;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#27979;&#35797;&#33539;&#22260;&#26377;&#38480;&#12289;&#28085;&#30422;&#30340;&#35821;&#35328;&#31181;&#31867;&#20063;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#28304;&#21477;&#23376;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22791;&#36873;&#38598;&#65292;&#20197;&#39564;&#35777;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#26159;&#21542;&#34920;&#29616;&#20986;&#39044;&#26399;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20351;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#34892;&#20026;&#27979;&#35797;&#23454;&#38469;&#21487;&#34892;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;&#22810;&#20010;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#24635;&#20307;&#19978;&#36890;&#36807;&#29575;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#21487;&#35266;&#23519;&#21040;&#30340;&#36235;&#21183;&#30456;&#31526;&#65292;&#20294;&#20173;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metri
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32447;&#24615;&#25628;&#32034;&#21644;&#31163;&#19968;&#26631;&#20934;&#20132;&#21449;&#39564;&#35777;&#33258;&#21160;&#36873;&#25321;&#38408;&#20540;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21160;&#24577;&#29615;&#22659;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.02551</link><description>&lt;p&gt;
&#25345;&#32493;&#25913;&#36827;&#22522;&#20110;&#38408;&#20540;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Continual Improvement of Threshold-Based Novelty Detection. (arXiv:2309.02551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02551
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32447;&#24615;&#25628;&#32034;&#21644;&#31163;&#19968;&#26631;&#20934;&#20132;&#21449;&#39564;&#35777;&#33258;&#21160;&#36873;&#25321;&#38408;&#20540;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21160;&#24577;&#29615;&#22659;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#24320;&#25918;&#30340;&#29615;&#22659;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#26816;&#27979;&#26410;&#35265;&#31867;&#21035;&#26102;&#38754;&#20020;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#20351;&#24471;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#37096;&#32626;&#25345;&#32493;&#23398;&#20064;&#21464;&#24471;&#22797;&#26434;&#65292;&#22240;&#20026;&#20195;&#29702;&#31243;&#24207;&#22312;&#36935;&#21040;&#26032;&#31867;&#22411;&#26102;&#24182;&#27809;&#26377;&#26126;&#30830;&#30340;&#36890;&#30693;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26816;&#27979;&#26032;&#39062;&#24615;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#35266;&#23519;&#25968;&#25454;&#28857;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#38408;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#25351;&#23450;&#36825;&#20123;&#38408;&#20540;&#30340;&#20540;&#65288;&#25552;&#21069;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#36866;&#24212;&#25968;&#25454;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32447;&#24615;&#25628;&#32034;&#21644;&#31163;&#19968;&#26631;&#20934;&#20132;&#21449;&#39564;&#35777;&#33258;&#21160;&#36873;&#25321;&#36825;&#20123;&#38408;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#36873;&#25321;&#38408;&#20540;&#30340;&#26032;&#26041;&#27861;&#22312;MNIST&#65292;&#26102;&#23578;MNIST&#21644;CIFAR-10&#19978;&#25552;&#39640;&#20102;&#24635;&#20307;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
When evaluated in dynamic, open-world situations, neural networks struggle to detect unseen classes. This issue complicates the deployment of continual learners in realistic environments where agents are not explicitly informed when novel categories are encountered. A common family of techniques for detecting novelty relies on thresholds of similarity between observed data points and the data used for training. However, these methods often require manually specifying (ahead of time) the value of these thresholds, and are therefore incapable of adapting to the nature of the data. We propose a new method for automatically selecting these thresholds utilizing a linear search and leave-one-out cross-validation on the ID classes. We demonstrate that this novel method for selecting thresholds results in improved total accuracy on MNIST, Fashion MNIST, and CIFAR-10.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#27010;&#24565;&#23398;&#20064;&#65288;SCL&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#22810;&#23618;&#27425;&#29289;&#20307;&#37325;&#32452;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#36866;&#24212;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#20381;&#36182;&#30340;&#26410;&#30693;&#22330;&#26223;&#65292;&#25512;&#26029;&#20986;&#29420;&#31435;&#30340;&#23376;&#32467;&#26500;&#20197;&#23454;&#29616;&#20219;&#21153;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02547</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#36827;&#34892;&#32467;&#26500;&#27010;&#24565;&#23398;&#20064;&#65292;&#23454;&#29616;&#22810;&#23618;&#27425;&#37325;&#32452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning. (arXiv:2309.02547v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#27010;&#24565;&#23398;&#20064;&#65288;SCL&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#22810;&#23618;&#27425;&#29289;&#20307;&#37325;&#32452;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#36866;&#24212;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#20381;&#36182;&#30340;&#26410;&#30693;&#22330;&#26223;&#65292;&#25512;&#26029;&#20986;&#29420;&#31435;&#30340;&#23376;&#32467;&#26500;&#20197;&#23454;&#29616;&#20219;&#21153;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#22914;&#29289;&#20307;&#37325;&#32452;&#65292;&#22312;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#19982;&#22797;&#26434;&#21644;&#20219;&#24847;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#23618;&#37325;&#32452;&#35268;&#21010;&#19978;&#65292;&#21363;&#20351;&#23384;&#22312;&#22810;&#20010;&#23618;&#27425;&#65292;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20960;&#20309;&#19978;&#20063;&#36739;&#31616;&#21333;&#65292;&#22914;&#22612;&#24335;&#22534;&#21472;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#27010;&#24565;&#23398;&#20064;&#65288;SCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#22810;&#23618;&#27425;&#29289;&#20307;&#37325;&#32452;&#35268;&#21010;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#30452;&#35266;&#30340;&#32467;&#26500;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#32467;&#26500;&#20381;&#36182;&#23618;&#27425;&#30340;&#26410;&#30693;&#22330;&#26223;&#65292;&#20855;&#26377;&#20219;&#24847;&#25968;&#37327;&#30340;&#23545;&#35937;&#21644;&#26356;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#25512;&#26029;&#20986;&#29420;&#31435;&#30340;&#23376;&#32467;&#26500;&#65292;&#36890;&#36807;&#22810;&#20010;&#25805;&#32437;&#22120;&#23454;&#29616;&#20219;&#21153;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#19968;&#31995;&#21015;&#32463;&#20856;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulation tasks, such as object rearrangement, play a crucial role in enabling robots to interact with complex and arbitrary environments. Existing work focuses primarily on single-level rearrangement planning and, even if multiple levels exist, dependency relations among substructures are geometrically simpler, like tower stacking. We propose Structural Concept Learning (SCL), a deep learning approach that leverages graph attention networks to perform multi-level object rearrangement planning for scenes with structural dependency hierarchies. It is trained on a self-generated simulation data set with intuitive structures, works for unseen scenes with an arbitrary number of objects and higher complexity of structures, infers independent substructures to allow for task parallelization over multiple manipulators, and generalizes to the real world. We compare our method with a range of classical and model-based baselines to show that our method leverages its scene understanding
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#36755;&#20986;Winograd&#27169;&#24335;&#30340;&#30828;&#24230;&#25351;&#25968;&#65292;&#21487;&#20197;&#22312;&#26410;&#26469;&#30340;&#25361;&#25112;&#25110;WSC CAPTCHA&#26381;&#21153;&#20013;&#23545;&#27169;&#24335;&#36827;&#34892;&#21306;&#20998;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.02534</link><description>&lt;p&gt;
&#32463;&#39564;&#19982;&#39044;&#27979;:&#19968;&#31181;&#26032;&#22411;&#30828;&#24230;&#24230;&#37327;&#30340;&#26631;&#20934;&#21270;&#35797;&#39564;
&lt;/p&gt;
&lt;p&gt;
Experience and Prediction: A Metric of Hardness for a Novel Litmus Test. (arXiv:2309.02534v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#36755;&#20986;Winograd&#27169;&#24335;&#30340;&#30828;&#24230;&#25351;&#25968;&#65292;&#21487;&#20197;&#22312;&#26410;&#26469;&#30340;&#25361;&#25112;&#25110;WSC CAPTCHA&#26381;&#21153;&#20013;&#23545;&#27169;&#24335;&#36827;&#34892;&#21306;&#20998;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;Winograd Schema Challenge (WSC) &#24050;&#25104;&#20026;&#30740;&#31350;&#30028;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#26631;&#20934;&#21270;&#35797;&#39564;&#12290;&#22240;&#27492;&#65292;WSC&#24341;&#36215;&#20102;&#30740;&#31350;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#34987;&#35270;&#20026;&#29702;&#35299;&#20154;&#31867;&#34892;&#20026;&#30340;&#25163;&#27573;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#20351;&#24471;Winograd&#27169;&#24335;&#33021;&#22815;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#65292;&#20363;&#22914;&#35774;&#35745;&#26032;&#22411;&#30340;CAPTCHAs&#24418;&#24335;&#12290;&#26089;&#20123;&#26102;&#20505;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#20026;&#20154;&#31867;&#25104;&#24180;&#20154;&#22312;WSC&#19978;&#30340;&#34920;&#29616;&#24314;&#31435;&#20102;&#22522;&#32447;&#65292;&#34920;&#26126;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#27169;&#24335;&#37117;&#26159;&#30456;&#21516;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#33021;&#26681;&#25454;&#20154;&#31867;&#30340;&#24863;&#30693;&#38590;&#24230;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#36825;&#31181;"&#30828;&#24230;&#24230;&#37327;"&#21487;&#20197;&#22312;&#26410;&#26469;&#30340;&#25361;&#25112;&#25110;WSC CAPTCHA&#26381;&#21153;&#20013;&#20351;&#29992;&#65292;&#20197;&#21306;&#20998;Winograd&#27169;&#24335;&#12290;&#25105;&#20204;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#36755;&#20986;Winograd&#27169;&#24335;&#30340;&#30828;&#24230;&#25351;&#25968;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, the Winograd Schema Challenge (WSC) has become a central aspect of the research community as a novel litmus test. Consequently, the WSC has spurred research interest because it can be seen as the means to understand human behavior. In this regard, the development of new techniques has made possible the usage of Winograd schemas in various fields, such as the design of novel forms of CAPTCHAs.  Work from the literature that established a baseline for human adult performance on the WSC has shown that not all schemas are the same, meaning that they could potentially be categorized according to their perceived hardness for humans. In this regard, this \textit{hardness-metric} could be used in future challenges or in the WSC CAPTCHA service to differentiate between Winograd schemas.  Recent work of ours has shown that this could be achieved via the design of an automated system that is able to output the hardness-indexes of Winograd schemas, albeit with limitations regar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#20204;&#23545;&#26469;&#33258;&#20154;&#31867;&#20316;&#32773;&#21644;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#24863;&#30693;&#65292;&#21457;&#29616;&#19981;&#35770;&#29992;&#25143;&#30028;&#38754;&#22914;&#20309;&#21576;&#29616;&#65292;&#21442;&#19982;&#32773;&#20542;&#21521;&#20110;&#36171;&#20104;&#30456;&#20284;&#27700;&#24179;&#30340;&#21487;&#20449;&#24230;&#12290;&#23613;&#31649;&#20154;&#20204;&#23545;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#21644;&#20540;&#24471;&#20449;&#36182;&#31243;&#24230;&#27809;&#26377;&#19981;&#21516;&#30340;&#24863;&#30693;&#65292;&#20294;&#20182;&#20204;&#35748;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#26356;&#21152;&#28165;&#26224;&#19988;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#25105;&#20204;&#22312;&#35780;&#20272;&#20449;&#24687;&#26469;&#28304;&#26102;&#26356;&#21152;&#23457;&#24910;&#65292;&#24182;&#40723;&#21169;&#29992;&#25143;&#20445;&#25345;&#35686;&#24789;&#21644;&#25209;&#21028;&#24615;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2309.02524</link><description>&lt;p&gt;
&#20320;&#30456;&#20449;ChatGPT&#21527;&#65311;-- &#20851;&#20110;&#20154;&#31867;&#19982;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Do You Trust ChatGPT? -- Perceived Credibility of Human and AI-Generated Content. (arXiv:2309.02524v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#20204;&#23545;&#26469;&#33258;&#20154;&#31867;&#20316;&#32773;&#21644;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#24863;&#30693;&#65292;&#21457;&#29616;&#19981;&#35770;&#29992;&#25143;&#30028;&#38754;&#22914;&#20309;&#21576;&#29616;&#65292;&#21442;&#19982;&#32773;&#20542;&#21521;&#20110;&#36171;&#20104;&#30456;&#20284;&#27700;&#24179;&#30340;&#21487;&#20449;&#24230;&#12290;&#23613;&#31649;&#20154;&#20204;&#23545;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#21644;&#20540;&#24471;&#20449;&#36182;&#31243;&#24230;&#27809;&#26377;&#19981;&#21516;&#30340;&#24863;&#30693;&#65292;&#20294;&#20182;&#20204;&#35748;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#26356;&#21152;&#28165;&#26224;&#19988;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#25105;&#20204;&#22312;&#35780;&#20272;&#20449;&#24687;&#26469;&#28304;&#26102;&#26356;&#21152;&#23457;&#24910;&#65292;&#24182;&#40723;&#21169;&#29992;&#25143;&#20445;&#25345;&#35686;&#24789;&#21644;&#25209;&#21028;&#24615;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#23519;&#20102;&#20010;&#20307;&#23545;&#20154;&#31867;&#20316;&#32773;&#25776;&#20889;&#20869;&#23481;&#21644;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;&#22914;ChatGPT&#25152;&#37319;&#29992;&#30340;GPT&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65289;&#30340;&#21487;&#20449;&#24230;&#24863;&#30693;&#65292;&#21516;&#26102;&#27604;&#36739;&#20102;&#19981;&#21516;&#29992;&#25143;&#30028;&#38754;&#29256;&#26412;&#19979;&#30340;&#24773;&#20917;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#35770;&#29992;&#25143;&#30028;&#38754;&#30340;&#21576;&#29616;&#24418;&#24335;&#22914;&#20309;&#65292;&#21442;&#19982;&#32773;&#20542;&#21521;&#20110;&#36171;&#20104;&#30456;&#20284;&#27700;&#24179;&#30340;&#21487;&#20449;&#24230;&#12290;&#23613;&#31649;&#21442;&#19982;&#32773;&#23545;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#21644;&#20540;&#24471;&#20449;&#36182;&#31243;&#24230;&#27809;&#26377;&#19981;&#21516;&#30340;&#24863;&#30693;&#65292;&#20294;&#20182;&#20204;&#35748;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#26356;&#21152;&#28165;&#26224;&#19988;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#21628;&#21505;&#25105;&#20204;&#22312;&#35780;&#20272;&#20449;&#24687;&#26469;&#28304;&#26102;&#37319;&#21462;&#26356;&#21152;&#23457;&#24910;&#30340;&#26041;&#24335;&#65292;&#40723;&#21169;&#29992;&#25143;&#22312;&#25509;&#35302;&#30001;AI&#31995;&#32479;&#29983;&#25104;&#30340;&#20869;&#23481;&#26102;&#20445;&#25345;&#35686;&#24789;&#21644;&#25209;&#21028;&#24615;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines how individuals perceive the credibility of content originating from human authors versus content generated by large language models, like the GPT language model family that powers ChatGPT, in different user interface versions. Surprisingly, our results demonstrate that regardless of the user interface presentation, participants tend to attribute similar levels of credibility. While participants also do not report any different perceptions of competence and trustworthiness between human and AI-generated content, they rate AI-generated content as being clearer and more engaging. The findings from this study serve as a call for a more discerning approach to evaluating information sources, encouraging users to exercise caution and critical thinking when engaging with content generated by AI systems.
&lt;/p&gt;</description></item><item><title>&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#36890;&#20449;&#65292;&#35299;&#20915;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#21644;&#22788;&#29702;&#36890;&#36947;&#24178;&#25200;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.02478</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#36890;&#20449; - ICASSP&#29305;&#21035;&#20250;&#35758;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Enhancing Semantic Communication with Deep Generative Models -- An ICASSP Special Session Overview. (arXiv:2309.02478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02478
&lt;/p&gt;
&lt;p&gt;
&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22686;&#24378;&#35821;&#20041;&#36890;&#20449;&#65292;&#35299;&#20915;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#21644;&#22788;&#29702;&#36890;&#36947;&#24178;&#25200;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#22312;&#22609;&#36896;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#36890;&#20449;&#31995;&#32479;&#20013;&#23558;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#25581;&#31034;&#20102;&#35821;&#20041;&#36890;&#20449;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25581;&#31034;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#12289;&#25552;&#21462;&#21644;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#20197;&#21450;&#23545;&#36890;&#36947;&#24178;&#25200;&#20855;&#26377;&#40065;&#26834;&#24615;&#26041;&#38754;&#26174;&#33879;&#22686;&#24378;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#12290;&#38500;&#20102;&#24314;&#31435;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#26412;&#25991;&#36824;&#20026;&#19979;&#19968;&#20195;&#29983;&#25104;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#30340;&#26032;&#30740;&#31350;&#36884;&#24452;&#20570;&#20102;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communication is poised to play a pivotal role in shaping the landscape of future AI-driven communication systems. Its challenge of extracting semantic information from the original complex content and regenerating semantically consistent data at the receiver, possibly being robust to channel corruptions, can be addressed with deep generative models. This ICASSP special session overview paper discloses the semantic communication challenges from the machine learning perspective and unveils how deep generative models will significantly enhance semantic communication frameworks in dealing with real-world complex data, extracting and exploiting semantic information, and being robust to channel corruptions. Alongside establishing this emerging field, this paper charts novel research pathways for the next generative semantic communication frameworks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#25351;&#20986;&#22312;&#22797;&#26434;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#34892;&#20026;&#26469;&#23398;&#20064;&#25152;&#38656;&#34892;&#20026;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02473</link><description>&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#32508;&#36848;&#65306;&#31639;&#27861;&#12289;&#26368;&#26032;&#36827;&#23637;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges. (arXiv:2309.02473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02473
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#25351;&#20986;&#22312;&#22797;&#26434;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#34892;&#20026;&#26469;&#23398;&#20064;&#25152;&#38656;&#34892;&#20026;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20196;&#20154;&#30633;&#30446;&#12290;&#38543;&#30528;&#36825;&#20123;&#31995;&#32479;&#30340;&#19981;&#26029;&#28436;&#36827;&#65292;&#23427;&#20204;&#36234;&#26469;&#36234;&#34987;&#24212;&#29992;&#20110;&#22797;&#26434;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#31354;&#20013;&#26426;&#22120;&#20154;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#30001;&#20110;&#36825;&#20123;&#29615;&#22659;&#38656;&#35201;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#27492;&#25163;&#21160;&#32534;&#31243;&#34892;&#20026;&#25110;&#36890;&#36807;&#22870;&#21169;&#20989;&#25968;&#26469;&#23450;&#20041;&#34892;&#20026;&#65288;&#22914;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20570;&#27861;&#65289;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#34892;&#20026;&#26469;&#23398;&#20064;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;&#36825;&#23601;&#26159;&#27169;&#20223;&#23398;&#20064;&#30340;&#20316;&#29992; - &#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#26469;&#23398;&#20064;&#25152;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#32780;&#36825;&#20123;&#34892;&#20026;&#26159;&#36890;&#36807;&#28436;&#31034;&#25552;&#20379;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the development of robotics and artificial intelligence (AI) systems has been nothing short of remarkable. As these systems continue to evolve, they are being utilized in increasingly complex and unstructured environments, such as autonomous driving, aerial robotics, and natural language processing. As a consequence, programming their behaviors manually or defining their behavior through reward functions (as done in reinforcement learning (RL)) has become exceedingly difficult. This is because such environments require a high degree of flexibility and adaptability, making it challenging to specify an optimal set of rules or reward signals that can account for all possible situations. In such environments, learning from an expert's behavior through imitation is often more appealing. This is where imitation learning (IL) comes into play - a process where desired behavior is learned by imitating an expert's behavior, which is provided through demonstrations.  This paper a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19977;&#32500;&#25171;&#21360;&#30340;G&#20195;&#30721;&#25991;&#20214;&#25552;&#20986;&#20102;&#20845;&#31181;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;&#23427;&#20204;&#22312;G&#20195;&#30721;&#35843;&#35797;&#21644;&#25805;&#20316;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#20197;&#21450;&#20960;&#20309;&#21464;&#25442;&#31561;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#20110;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.02465</link><description>&lt;p&gt;
&#38754;&#21521;&#22686;&#26448;&#21046;&#36896;&#30340;&#22522;&#30784;AI&#27169;&#22411;&#65306;&#29992;&#20110;G&#20195;&#30721;&#35843;&#35797;&#12289;&#25805;&#20316;&#21644;&#29702;&#35299;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension. (arXiv:2309.02465v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19977;&#32500;&#25171;&#21360;&#30340;G&#20195;&#30721;&#25991;&#20214;&#25552;&#20986;&#20102;&#20845;&#31181;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;&#23427;&#20204;&#22312;G&#20195;&#30721;&#35843;&#35797;&#21644;&#25805;&#20316;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38169;&#35823;&#26816;&#27979;&#21644;&#20462;&#27491;&#20197;&#21450;&#20960;&#20309;&#21464;&#25442;&#31561;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#20110;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#25171;&#21360;&#25110;&#22686;&#26448;&#21046;&#36896;&#26159;&#19968;&#39033;&#38761;&#21629;&#24615;&#30340;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#20174;&#25968;&#23383;&#27169;&#22411;&#20013;&#21019;&#24314;&#29289;&#29702;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#19977;&#32500;&#25171;&#21360;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;G&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;G&#20195;&#30721;&#26159;&#19968;&#31181;&#20302;&#32423;&#25968;&#25511;&#32534;&#31243;&#35821;&#35328;&#65292;&#25351;&#23548;&#19977;&#32500;&#25171;&#21360;&#26426;&#22914;&#20309;&#31227;&#21160;&#21644;&#25380;&#20986;&#26448;&#26009;&#12290;&#35843;&#35797;G&#20195;&#30721;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#38656;&#35201;&#23545;G&#20195;&#30721;&#26684;&#24335;&#21644;&#25152;&#25171;&#21360;&#38646;&#20214;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20845;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#39318;&#27425;&#24191;&#27867;&#35780;&#20272;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35843;&#35797;&#19977;&#32500;&#25171;&#21360;&#30340;G&#20195;&#30721;&#25991;&#20214;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#25928;&#30340;&#25552;&#31034;&#65292;&#20351;&#39044;&#35757;&#32451;&#30340;LLMs&#33021;&#22815;&#29702;&#35299;&#21644;&#25805;&#20316;G&#20195;&#30721;&#65292;&#24182;&#22312;G&#20195;&#30721;&#35843;&#35797;&#21644;&#25805;&#20316;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#26816;&#27979;&#21644;&#20462;&#27491;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#25191;&#34892;&#20960;&#20309;&#21464;&#25442;&#26041;&#38754;&#27979;&#35797;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#20248;&#28857;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D printing or additive manufacturing is a revolutionary technology that enables the creation of physical objects from digital models. However, the quality and accuracy of 3D printing depend on the correctness and efficiency of the G-code, a low-level numerical control programming language that instructs 3D printers how to move and extrude material. Debugging G-code is a challenging task that requires a syntactic and semantic understanding of the G-code format and the geometry of the part to be printed. In this paper, we present the first extensive evaluation of six state-of-the-art foundational large language models (LLMs) for comprehending and debugging G-code files for 3D printing. We design effective prompts to enable pre-trained LLMs to understand and manipulate G-code and test their performance on various aspects of G-code debugging and manipulation, including detection and correction of common errors and the ability to perform geometric transformations. We analyze their strength
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02460</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks. (arXiv:2309.02460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#37329;&#34701;&#24066;&#22330;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#12290;&#22312;&#21152;&#23494;&#36135;&#24065;&#19978;&#30340;&#38750;&#27861;&#27963;&#21160;&#28608;&#22686;&#23548;&#33268;&#20102;&#26222;&#36890;&#29992;&#25143;&#25968;&#21313;&#20159;&#30340;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20381;&#36182;&#20110;&#32321;&#29712;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#33719;&#24471;&#25163;&#24037;&#29305;&#24449;&#65292;&#35201;&#20040;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#20122;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#38382;&#39064;&#23450;&#20041;&#20026;&#24102;&#26377;&#36793;&#23646;&#24615;&#30340;&#26377;&#21521;&#22810;&#22270;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;DIAM&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#19978;&#26377;&#25928;&#22320;&#26816;&#27979;&#38750;&#27861;&#36134;&#25143;&#12290;&#39318;&#20808;&#65292;DIAM&#21253;&#21547;&#19968;&#20010;Edge2Seq&#27169;&#22359;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#36793;&#23646;&#24615;&#21644;&#26377;&#21521;&#36793;&#24207;&#21015;&#20381;&#36182;&#20851;&#31995;&#65292;&#33258;&#21160;&#23398;&#20064;&#26377;&#25928;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#12290;&#28982;&#21518;&#21033;&#29992;t
&lt;/p&gt;
&lt;p&gt;
We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21307;&#23398;&#24433;&#20687;&#24494;&#23567;&#24322;&#24120;&#26816;&#27979;&#30340;&#33410;&#20461;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#26469;&#20195;&#26367;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24230;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#26368;&#20248;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.02458</link><description>&lt;p&gt;
&#36208;&#21521;&#33410;&#20461;&#30340;&#26080;&#30417;&#30563;&#21307;&#23398;&#24433;&#20687;&#24494;&#23567;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards frugal unsupervised detection of subtle abnormalities in medical imaging. (arXiv:2309.02458v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21307;&#23398;&#24433;&#20687;&#24494;&#23567;&#24322;&#24120;&#26816;&#27979;&#30340;&#33410;&#20461;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#26469;&#20195;&#26367;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24230;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#26368;&#20248;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#24322;&#24120;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#21307;&#23398;&#24433;&#20687;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#19982;&#27491;&#24120;&#27169;&#22411;&#19981;&#21305;&#37197;&#30340;&#29305;&#24449;&#12290;&#34429;&#28982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#22312;&#20934;&#30830;&#24230;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#23454;&#29616;&#26368;&#20248;&#30340;&#26435;&#34913;&#12290;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27010;&#29575;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#21644;&#20219;&#21153;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#24471;&#21040;&#24191;&#27867;&#35748;&#21487;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#36807;&#22810;&#35774;&#35745;&#21644;&#35843;&#25972;&#12290;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#20351;&#23427;&#20204;&#25104;&#20026;&#35299;&#37322;&#22797;&#26434;&#22810;&#20803;&#21442;&#32771;&#27169;&#22411;&#30340;&#33391;&#22909;&#36873;&#25321;&#12290;&#23427;&#20204;&#30340;&#21442;&#25968;&#25968;&#37327;&#26356;&#23567;&#65292;&#26356;&#23481;&#26131;&#35299;&#37322;&#21644;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#20272;&#35745;&#31639;&#27861;&#65288;&#22914;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#65289;&#22312;&#22823;&#25968;&#25454;&#37327;&#19979;&#19981;&#26131;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in medical imaging is a challenging task in contexts where abnormalities are not annotated. This problem can be addressed through unsupervised anomaly detection (UAD) methods, which identify features that do not match with a reference model of normal profiles. Artificial neural networks have been extensively used for UAD but they do not generally achieve an optimal trade-o$\hookleftarrow$ between accuracy and computational demand. As an alternative, we investigate mixtures of probability distributions whose versatility has been widely recognized for a variety of data and tasks, while not requiring excessive design e$\hookleftarrow$ort or tuning. Their expressivity makes them good candidates to account for complex multivariate reference models. Their much smaller number of parameters makes them more amenable to interpretation and e cient learning. However, standard estimation procedures, such as the Expectation-Maximization algorithm, do not scale well to large data vo
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#31232;&#30095;&#30697;&#38453;&#32467;&#26500;&#30340;&#26694;&#26550;&#22312;&#21305;&#37197;&#25968;&#25454;&#26684;&#24335;&#21644;&#36991;&#20813;&#35835;&#21462;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2309.02442</link><description>&lt;p&gt;
&#35266;&#23519;&#23616;&#37096;&#65292;&#20840;&#23616;&#20998;&#31867;&#65306;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#31232;&#30095;&#30697;&#38453;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Observe Locally, Classify Globally: Using GNNs to Identify Sparse Matrix Structure. (arXiv:2309.02442v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02442
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#31232;&#30095;&#30697;&#38453;&#32467;&#26500;&#30340;&#26694;&#26550;&#22312;&#21305;&#37197;&#25968;&#25454;&#26684;&#24335;&#21644;&#36991;&#20813;&#35835;&#21462;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30697;&#38453;&#35745;&#31639;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#30697;&#38453;&#26684;&#24335;&#19982;&#35201;&#35745;&#31639;&#30340;&#25968;&#25454;&#30340;&#24213;&#23618;&#32467;&#26500;&#30340;&#21305;&#37197;&#12290;&#19981;&#21516;&#30340;&#31232;&#30095;&#30697;&#38453;&#26684;&#24335;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#25968;&#25454;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#39318;&#35201;&#25361;&#25112;&#26159;&#22312;&#35745;&#31639;&#20043;&#21069;&#35782;&#21035;&#30697;&#38453;&#32467;&#26500;&#65292;&#20197;&#23558;&#20854;&#19982;&#36866;&#24403;&#30340;&#25968;&#25454;&#26684;&#24335;&#30456;&#21305;&#37197;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#36991;&#20813;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#20043;&#21069;&#35835;&#21462;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#30697;&#38453;&#32467;&#26500;&#36890;&#36807;&#26679;&#26412;&#21450;&#20854;&#29305;&#24449;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#20840;&#23616;&#29305;&#24449;&#21487;&#33021;&#26080;&#27861;&#20174;&#19968;&#20010;&#37319;&#26679;&#38598;&#20013;&#30830;&#23450;&#65292;&#32780;&#24517;&#39035;&#20174;&#23616;&#37096;&#29305;&#24449;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#29983;&#25104;&#31232;&#30095;&#30697;&#38453;&#32467;&#26500;&#20998;&#31867;&#22120;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#29983;&#25104;&#22120;&#25193;&#23637;&#21040;&#20854;&#20182;&#30697;&#38453;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#32452;&#20195;&#34920;&#24615;&#31232;&#30095;&#30697;&#38453;&#19978;&#23454;&#29616;&#20102;97%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of sparse matrix computation highly depends on the matching of the matrix format with the underlying structure of the data being computed on. Different sparse matrix formats are suitable for different structures of data. Therefore, the first challenge is identifying the matrix structure before the computation to match it with an appropriate data format. The second challenge is to avoid reading the entire dataset before classifying it. This can be done by identifying the matrix structure through samples and their features. Yet, it is possible that global features cannot be determined from a sampling set and must instead be inferred from local features. To address these challenges, we develop a framework that generates sparse matrix structure classifiers using graph convolutional networks. The framework can also be extended to other matrix structures using user-provided generators. The approach achieves 97% classification accuracy on a set of representative sparse matrix 
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#20351;&#29992;&#25968;&#23398;&#32467;&#26500;&#31934;&#30830;&#22320;&#34920;&#31034;&#21644;&#25805;&#20316;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#31561;&#22810;&#31181;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02332</link><description>&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#22788;&#29702;&#65306;&#25968;&#25454;&#21644;&#25805;&#20316;&#30340;&#25968;&#23398;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations. (arXiv:2309.02332v1 [q-bio.NC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02332
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#20351;&#29992;&#25968;&#23398;&#32467;&#26500;&#31934;&#30830;&#22320;&#34920;&#31034;&#21644;&#25805;&#20316;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#31561;&#22810;&#31181;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21754;&#20083;&#21160;&#29289;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30340;&#22797;&#26434;&#32467;&#26500;&#20013;&#65292;&#31070;&#32463;&#20803;&#24418;&#25104;&#32676;&#20307;&#12290;&#36724;&#32034;&#26463;&#36890;&#36807;&#33033;&#20914;&#21015;&#20316;&#20026;&#23186;&#20171;&#22312;&#36825;&#20123;&#32676;&#38598;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31070;&#32463;&#32676;&#20307;&#30340;&#31934;&#30830;&#32534;&#30721;&#21644;&#25805;&#20316;&#36824;&#26377;&#24453;&#21457;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#20986;&#21457;&#28857;&#26159;&#19968;&#20010;&#20855;&#26377;&#21487;&#22609;&#24615;&#30340;&#36890;&#29992;&#31070;&#32463;&#20803;&#30340;&#20808;&#36827;&#30340;&#26426;&#26800;&#27169;&#22411;&#12290;&#20174;&#36825;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#20013;&#20986;&#29616;&#20102;&#19968;&#20010;&#28145;&#21051;&#30340;&#25968;&#23398;&#26500;&#36896;&#65306;&#36890;&#36807;&#26377;&#38480;&#20984;&#38181;&#30340;&#20195;&#25968;&#21487;&#20197;&#20934;&#30830;&#22320;&#25551;&#36848;&#20449;&#24687;&#30340;&#34920;&#31034;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31070;&#32463;&#32676;&#20307;&#19981;&#20165;&#20165;&#26159;&#34987;&#21160;&#20256;&#36755;&#32773;&#12290;&#23427;&#20204;&#22312;&#36825;&#20010;&#20195;&#25968;&#32467;&#26500;&#20013;&#25198;&#28436;&#30528;&#36816;&#31639;&#31526;&#30340;&#35282;&#33394;&#65292;&#21453;&#26144;&#20102;&#20302;&#32423;&#32534;&#31243;&#35821;&#35328;&#30340;&#21151;&#33021;&#12290;&#24403;&#36825;&#20123;&#32676;&#20307;&#20114;&#36830;&#26102;&#65292;&#23427;&#20204;&#20855;&#26377;&#31616;&#27905;&#32780;&#24378;&#22823;&#30340;&#20195;&#25968;&#34920;&#36798;&#24335;&#12290;&#36825;&#20123;&#32593;&#32476;&#20351;&#23427;&#20204;&#33021;&#22815;&#23454;&#29616;&#35768;&#22810;&#25805;&#20316;&#65292;&#22914;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#12289;&#32500;&#24230;&#38477;&#20302;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the intricate architecture of the mammalian central nervous system, neurons form populations. Axonal bundles communicate between these clusters using spike trains as their medium. However, these neuron populations' precise encoding and operations have yet to be discovered. In our analysis, the starting point is a state-of-the-art mechanistic model of a generic neuron endowed with plasticity. From this simple framework emerges a profound mathematical construct: The representation and manipulation of information can be precisely characterized by an algebra of finite convex cones. Furthermore, these neuron populations are not merely passive transmitters. They act as operators within this algebraic structure, mirroring the functionality of a low-level programming language. When these populations interconnect, they embody succinct yet potent algebraic expressions. These networks allow them to implement many operations, such as specialization, generalization, novelty detection, dimensiona
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#27468;&#26354;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#35757;&#32451;&#20110;&#35821;&#38899;&#30340;DeepFake&#26816;&#27979;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#19978;&#19981;&#36215;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.02232</link><description>&lt;p&gt;
FSD: &#19968;&#20221;&#29992;&#20110;&#27468;&#26354;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#21021;&#27493;&#20013;&#25991;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FSD: An Initial Chinese Dataset for Fake Song Detection. (arXiv:2309.02232v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#27468;&#26354;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#35757;&#32451;&#20110;&#35821;&#38899;&#30340;DeepFake&#26816;&#27979;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#19978;&#19981;&#36215;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#21644;&#36716;&#25442;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#38899;&#20048;&#20307;&#39564;&#24471;&#21040;&#20102;&#38761;&#21629;&#24615;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30001;&#36825;&#20123;&#25216;&#26415;&#29983;&#25104;&#30340;&#8220;Deepfake Songs&#8221;&#30340;&#20852;&#36215;&#24341;&#21457;&#20102;&#23545;&#30495;&#23454;&#24615;&#30340;&#20851;&#27880;&#12290;&#19982;&#38899;&#39057;DeepFake&#26816;&#27979;&#65288;ADD&#65289;&#19981;&#21516;&#65292;&#27468;&#26354;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#39046;&#22495;&#32570;&#20047;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#25110;&#26041;&#27861;&#29992;&#20110;&#27468;&#26354;&#30495;&#23454;&#24615;&#30340;&#39564;&#35777;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#27468;&#26354;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#39046;&#22495;&#30340;&#20013;&#25991;Fake Song Detection (FSD)&#25968;&#25454;&#38598;&#12290;FSD&#25968;&#25454;&#38598;&#20013;&#30340;&#20266;&#36896;&#27468;&#26354;&#30001;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#21644;&#36716;&#25442;&#26041;&#27861;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;FSD&#19978;&#30340;&#21021;&#27493;&#23454;&#39564;&#25581;&#31034;&#20102;&#29616;&#26377;&#35757;&#32451;&#20110;&#35821;&#38899;&#30340;ADD&#27169;&#22411;&#22312;&#27468;&#26354;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#19981;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;FSD&#25968;&#25454;&#38598;&#23545;ADD&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65306;&#19968;&#31181;&#26159;&#20351;&#29992;&#21407;&#22987;&#27468;&#26354;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#20998;&#31163;&#30340;&#20154;&#22768;&#38899;&#36712;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Singing voice synthesis and singing voice conversion have significantly advanced, revolutionizing musical experiences. However, the rise of "Deepfake Songs" generated by these technologies raises concerns about authenticity. Unlike Audio DeepFake Detection (ADD), the field of song deepfake detection lacks specialized datasets or methods for song authenticity verification. In this paper, we initially construct a Chinese Fake Song Detection (FSD) dataset to investigate the field of song deepfake detection. The fake songs in the FSD dataset are generated by five state-of-the-art singing voice synthesis and singing voice conversion methods. Our initial experiments on FSD revealed the ineffectiveness of existing speech-trained ADD models for the task of song deepFake detection. Thus, we employ the FSD dataset for the training of ADD models. We subsequently evaluate these models under two scenarios: one with the original songs and another with separated vocal tracks. Experiment results show 
&lt;/p&gt;</description></item><item><title>CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.01940</link><description>&lt;p&gt;
CodeApex&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. (arXiv:2309.01940v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01940
&lt;/p&gt;
&lt;p&gt;
CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#30340;&#32534;&#31243;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#26085;&#30410;&#22686;&#38271;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CodeApex&#65292;&#19968;&#31181;&#21452;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#19987;&#27880;&#20110;LLM&#30340;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;CodeApex&#21253;&#25324;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65306;&#27010;&#24565;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#36339;&#25512;&#29702;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#32534;&#31243;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;CodeApex&#21033;&#29992;&#31639;&#27861;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;&#27979;&#35797;&#29992;&#20363;&#26469;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#36136;&#37327;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;14&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#65292;&#21253;&#25324;&#36890;&#29992;&#21644;&#19987;&#38376;&#21270;&#27169;&#22411;&#12290;GPT&#23637;&#29616;&#20986;&#26368;&#20339;&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;&#32422;50%&#21644;56%&#12290;&#32534;&#31243;&#20219;&#21153;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#24076;&#26395;CodeApex&#33021;&#22815;&#20026;&#35780;&#20272;&#32534;&#31243;&#33021;&#21147;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. We propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension and code generation abilities of LLMs. CodeApex comprises three types of multiple-choice questions: conceptual understanding, commonsense reasoning, and multi-hop reasoning, designed to evaluate LLMs on programming comprehension tasks. Additionally, CodeApex utilizes algorithmic questions and corresponding test cases to assess the code quality generated by LLMs. We evaluate 14 state-of-the-art LLMs, including both general-purpose and specialized models. GPT exhibits the best programming capabilities, achieving approximate accuracies of 50% and 56% on the two tasks, respectively. There is still significant room for improvement in programming tasks. We hope that CodeApex can serve as a reference for evaluating the co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26080;&#30693;&#35782;&#29615;&#22659;&#19979;&#39640;&#25928;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#26694;&#26550;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#22411;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;&#23545;&#21508;&#31181;&#20027;&#27969;&#26041;&#27861;&#21644;&#26432;&#27602;&#36719;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#20855;&#26377;&#24378;&#22823;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.01866</link><description>&lt;p&gt;
&#22312;&#26080;&#30693;&#35782;&#29615;&#22659;&#19979;&#65292;&#23545;&#22522;&#20110;&#26597;&#35810;&#30340;&#39640;&#25928;&#25915;&#20987;&#26426;&#22120;&#23398;&#20064;&#22411;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting. (arXiv:2309.01866v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26080;&#30693;&#35782;&#29615;&#22659;&#19979;&#39640;&#25928;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#26694;&#26550;&#65292;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#22411;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;&#23545;&#21508;&#31181;&#20027;&#27969;&#26041;&#27861;&#21644;&#26432;&#27602;&#36719;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#20855;&#26377;&#24378;&#22823;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#21331;&#25805;&#20316;&#31995;&#32479;&#30340;&#24191;&#27867;&#24212;&#29992;&#20351;&#24471;&#24694;&#24847;&#23433;&#21331;&#24212;&#29992;&#25104;&#20026;&#25915;&#20987;&#32773;&#30340;&#21560;&#24341;&#30446;&#26631;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#33267;&#20851;&#37325;&#35201;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#33030;&#24369;&#24615;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#30446;&#21069;&#23545;ML-based AMD&#26041;&#27861;&#30340;&#25915;&#20987;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25915;&#20987;&#20381;&#36182;&#20110;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#29616;&#23454;&#30340;&#24378;&#20551;&#35774;&#65292;&#20363;&#22914;&#29305;&#24449;&#31354;&#38388;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AdvDroidZero&#65292;&#19968;&#31181;&#23545;ML-based AMD&#26041;&#27861;&#30340;&#39640;&#25928;&#26597;&#35810;&#24335;&#25915;&#20987;&#26694;&#26550;&#65292;&#22312;&#26080;&#30693;&#35782;&#29615;&#22659;&#19979;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;AdvDroidZero&#23545;&#21508;&#31181;&#20027;&#27969;ML-based AMD&#26041;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21644;&#23454;&#38469;&#30340;&#26432;&#27602;&#36719;&#20214;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of the Android operating system has made malicious Android applications an appealing target for attackers. Machine learning-based (ML-based) Android malware detection (AMD) methods are crucial in addressing this problem; however, their vulnerability to adversarial examples raises concerns. Current attacks against ML-based AMD methods demonstrate remarkable performance but rely on strong assumptions that may not be realistic in real-world scenarios, e.g., the knowledge requirements about feature space, model parameters, and training dataset. To address this limitation, we introduce AdvDroidZero, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting. Our extensive evaluation shows that AdvDroidZero is effective against various mainstream ML-based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#22855;&#24322;&#40657;&#22622;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#31070;&#32463;&#38544;&#24335;&#20989;&#25968;&#30340;&#40657;&#22622;&#30697;&#38453;&#22312;&#38752;&#36817;&#34920;&#38754;&#30340;&#28857;&#19978;&#20855;&#26377;&#38646;&#34892;&#21015;&#24335;&#65292;&#20174;&#36136;&#37327;&#36739;&#24046;&#30340;&#26080;&#23450;&#21521;&#28857;&#20113;&#20013;&#37325;&#26500;&#39640;&#20445;&#30495;&#30340;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2309.01793</link><description>&lt;p&gt;
&#31070;&#32463;&#22855;&#24322;&#40657;&#22622;&#30697;&#38453;&#65306;&#36890;&#36807;&#24378;&#21046;&#22855;&#24322;&#40657;&#22622;&#30697;&#38453;&#38544;&#24335;&#34920;&#31034;&#26080;&#23450;&#21521;&#28857;&#20113;
&lt;/p&gt;
&lt;p&gt;
Neural-Singular-Hessian: Implicit Neural Representation of Unoriented Point Clouds by Enforcing Singular Hessian. (arXiv:2309.01793v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#22855;&#24322;&#40657;&#22622;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#31070;&#32463;&#38544;&#24335;&#20989;&#25968;&#30340;&#40657;&#22622;&#30697;&#38453;&#22312;&#38752;&#36817;&#34920;&#38754;&#30340;&#28857;&#19978;&#20855;&#26377;&#38646;&#34892;&#21015;&#24335;&#65292;&#20174;&#36136;&#37327;&#36739;&#24046;&#30340;&#26080;&#23450;&#21521;&#28857;&#20113;&#20013;&#37325;&#26500;&#39640;&#20445;&#30495;&#30340;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26159;&#20174;&#28857;&#20113;&#37325;&#26500;&#34920;&#38754;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21508;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#22914;Eikonal&#21644;Laplacian&#33021;&#37327;&#39033;&#65292;&#20197;&#24378;&#21046;&#23398;&#20064;&#30340;&#31070;&#32463;&#20989;&#25968;&#20855;&#26377;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;(SDF)&#30340;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#36136;&#37327;&#36739;&#24046;&#30340;&#26080;&#23450;&#21521;&#28857;&#20113;&#20013;&#25512;&#26029;&#20986;&#28508;&#22312;&#34920;&#38754;&#30340;&#23454;&#38469;&#25299;&#25169;&#21644;&#20960;&#20309;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26681;&#25454;&#24494;&#20998;&#20960;&#20309;&#65292;&#22312;&#22260;&#32469;&#34920;&#38754;&#30340;&#24494;&#20998;&#34180;&#22771;&#31354;&#38388;&#20869;&#65292;SDF&#30340;&#40657;&#22622;&#30697;&#38453;&#26159;&#22855;&#24322;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#21046;&#31070;&#32463;&#38544;&#24335;&#20989;&#25968;&#30340;&#40657;&#22622;&#30697;&#38453;&#22312;&#38752;&#36817;&#34920;&#38754;&#30340;&#28857;&#19978;&#20855;&#26377;&#38646;&#34892;&#21015;&#24335;&#12290;&#36825;&#31181;&#25216;&#26415;&#23545;&#20110;&#36817;&#34920;&#38754;&#28857;&#21450;&#20854;&#22312;&#34920;&#38754;&#19978;&#25237;&#24433;&#28857;&#23545;&#40784;&#26799;&#24230;&#65292;&#20165;&#38656;&#20960;&#27425;&#36845;&#20195;&#21363;&#21487;&#29983;&#25104;&#31895;&#30053;&#20294;&#24544;&#23454;&#30340;&#24418;&#29366;&#12290;&#36890;&#36807;&#36880;&#28176;&#38477;&#20302;&#22855;&#24322;&#40657;&#22622;&#30697;&#38453;&#39033;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26368;&#32456;&#20135;&#29983;&#39640;&#20445;&#30495;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural implicit representation is a promising approach for reconstructing surfaces from point clouds. Existing methods combine various regularization terms, such as the Eikonal and Laplacian energy terms, to enforce the learned neural function to possess the properties of a Signed Distance Function (SDF). However, inferring the actual topology and geometry of the underlying surface from poor-quality unoriented point clouds remains challenging. In accordance with Differential Geometry, the Hessian of the SDF is singular for points within the differential thin-shell space surrounding the surface. Our approach enforces the Hessian of the neural implicit function to have a zero determinant for points near the surface. This technique aligns the gradients for a near-surface point and its on-surface projection point, producing a rough but faithful shape within just a few iterations. By annealing the weight of the singular-Hessian term, our approach ultimately produces a high-fidelity reconstr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#20301;&#23485;&#21387;&#32553;&#33267;4&#20301;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#30340;&#35814;&#32454;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#36817;&#20284;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#24182;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#35299;&#20915;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01507</link><description>&lt;p&gt;
&#20869;&#23384;&#39640;&#25928;&#30340;&#20855;&#26377;4&#20301;&#29366;&#24577;&#30340;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Memory Efficient Optimizers with 4-bit States. (arXiv:2309.01507v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#20301;&#23485;&#21387;&#32553;&#33267;4&#20301;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#30340;&#35814;&#32454;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#36817;&#20284;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#24182;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#35299;&#20915;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#22120;&#29366;&#24577;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#20027;&#35201;&#20869;&#23384;&#28040;&#32791;&#26469;&#28304;&#65292;&#38480;&#21046;&#20102;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#20869;&#21487;&#35757;&#32451;&#30340;&#26368;&#22823;&#27169;&#22411;&#12290;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#20174;32&#20301;&#28014;&#28857;&#25968;&#21387;&#32553;&#21040;&#26356;&#20302;&#30340;&#20301;&#23485;&#26377;&#26395;&#20943;&#23567;&#35757;&#32451;&#20869;&#23384;&#21344;&#29992;&#65292;&#32780;&#24403;&#21069;&#26368;&#20302;&#21487;&#36798;&#21040;&#30340;&#20301;&#23485;&#20026;8&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#30340;&#32463;&#39564;&#20998;&#26512;&#23558;&#20248;&#21270;&#22120;&#29366;&#24577;&#20301;&#23485;&#38477;&#33267;4&#20301;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#30697;&#20855;&#26377;&#22797;&#26434;&#30340;&#24322;&#24120;&#20540;&#27169;&#24335;&#65292;&#26080;&#27861;&#36890;&#36807;&#24403;&#21069;&#30340;&#22359;&#29366;&#37327;&#21270;&#26041;&#27861;&#20934;&#30830;&#36817;&#20284;&#12290;&#25105;&#20204;&#20351;&#29992;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#65292;&#24182;&#25552;&#20986;&#21516;&#26102;&#21033;&#29992;&#34892;&#19978;&#21644;&#21015;&#19978;&#30340;&#20449;&#24687;&#36827;&#34892;&#26356;&#22909;&#30340;&#37327;&#21270;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#37327;&#21270;&#31532;&#20108;&#38454;&#30697;&#26102;&#30340;&#38646;&#28857;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25490;&#38500;&#38646;&#28857;&#30340;&#32447;&#24615;&#37327;&#21270;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;4&#20301;&#20248;&#21270;&#22120;&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizer is evaluated on a wide variety of benchmarks including natural language understanding, machine translat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#30340;&#26102;&#38388;&#37329;&#23383;&#22612;&#21387;&#32553;&#21644;&#25918;&#22823;&#21464;&#25442;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#39057;&#24207;&#21015;&#20013;&#20934;&#30830;&#20272;&#35745;&#20154;&#20307;3D&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25193;&#23637;&#26102;&#38388;&#24314;&#27169;&#21644;&#32454;&#21270;&#29305;&#24449;&#20132;&#20114;&#26469;&#35299;&#20915;&#20854;&#20182;&#26041;&#27861;&#20013;&#30340;&#32454;&#33410;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.01365</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#26102;&#38388;&#37329;&#23383;&#22612;&#21387;&#32553;&#21644;&#25918;&#22823;&#21464;&#25442;&#22120;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation. (arXiv:2309.01365v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01365
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#30340;&#26102;&#38388;&#37329;&#23383;&#22612;&#21387;&#32553;&#21644;&#25918;&#22823;&#21464;&#25442;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#39057;&#24207;&#21015;&#20013;&#20934;&#30830;&#20272;&#35745;&#20154;&#20307;3D&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25193;&#23637;&#26102;&#38388;&#24314;&#27169;&#21644;&#32454;&#21270;&#29305;&#24449;&#20132;&#20114;&#26469;&#35299;&#20915;&#20854;&#20182;&#26041;&#27861;&#20013;&#30340;&#32454;&#33410;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#24207;&#21015;&#20013;&#20934;&#30830;&#20272;&#35745;&#20154;&#20307;&#30340;3D&#23039;&#21183;&#38656;&#35201;&#20934;&#30830;&#24615;&#21644;&#33391;&#22909;&#30340;&#32467;&#26500;&#21270;&#26550;&#26500;&#12290;&#22522;&#20110;transformer&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20248;&#21270;&#30340;&#26102;&#38388;&#37329;&#23383;&#22612;&#21387;&#32553;&#21644;&#25918;&#22823;&#65288;RTPCA&#65289;&#21464;&#25442;&#22120;&#12290;RTPCA&#36890;&#36807;&#20854;&#26102;&#38388;&#37329;&#23383;&#22612;&#21387;&#32553;&#21644;&#25918;&#22823;&#65288;TPCA&#65289;&#32467;&#26500;&#25193;&#23637;&#20102;&#22359;&#20869;&#26102;&#38388;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#23618;&#32454;&#21270;&#65288;XLR&#65289;&#27169;&#22359;&#32454;&#21270;&#22359;&#38388;&#29305;&#24449;&#20132;&#20114;&#12290;&#29305;&#21035;&#22320;&#65292;TPCA&#22359;&#21033;&#29992;&#26102;&#38388;&#37329;&#23383;&#22612;&#33539;&#20363;&#65292;&#22686;&#24378;&#20851;&#38190;&#21644;&#20540;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20174;&#36816;&#21160;&#24207;&#21015;&#20013;&#26080;&#32541;&#25552;&#21462;&#31354;&#38388;&#35821;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;TPCA&#22359;&#19982;XLR&#36830;&#25509;&#36215;&#26469;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#26597;&#35810;&#12289;&#20851;&#38190;&#23383;&#21644;&#20540;&#30340;&#30456;&#20114;&#20316;&#29992;&#20419;&#36827;&#20016;&#23500;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#36825;&#31181;&#31574;&#30053;&#36890;&#36807;&#24403;&#21069;&#27969;&#31243;&#20307;&#29616;&#20102;&#26089;&#26399;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#20854;&#20182;&#22522;&#20110;Transformer&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#32454;&#33410;&#21644;&#31283;&#23450;&#24615;&#19981;&#36275;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#21518;&#30340;&#27169;&#22411;&#22312;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately estimating the 3D pose of humans in video sequences requires both accuracy and a well-structured architecture. With the success of transformers, we introduce the Refined Temporal Pyramidal Compression-and-Amplification (RTPCA) transformer. Exploiting the temporal dimension, RTPCA extends intra-block temporal modeling via its Temporal Pyramidal Compression-and-Amplification (TPCA) structure and refines inter-block feature interaction with a Cross-Layer Refinement (XLR) module. In particular, TPCA block exploits a temporal pyramid paradigm, reinforcing key and value representation capabilities and seamlessly extracting spatial semantics from motion sequences. We stitch these TPCA blocks with XLR that promotes rich semantic representation through continuous interaction of queries, keys, and values. This strategy embodies early-stage information with current flows, addressing typical deficits in detail and stability seen in other transformer-based methods. We demonstrate the eff
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.01069</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Hamiltonian Neural Networks. (arXiv:2309.01069v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#35266;&#27979;&#25968;&#25454;&#24314;&#27169;&#21160;&#21147;&#31995;&#32479;&#26159;&#29616;&#20195;&#31185;&#23398;&#21644;&#24037;&#31243;&#25968;&#25454;&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290; &#21704;&#23494;&#39039;&#31995;&#32479;&#26159;&#19968;&#31867;&#22522;&#26412;&#19988;&#24191;&#27867;&#23384;&#22312;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290; &#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#27721;&#23494;&#23572;&#39039;&#26041;&#31243;&#30340;&#23398;&#20064;&#20559;&#24046;&#19979;&#65292;&#20174;&#31163;&#25955;&#35266;&#27979;&#30340;&#21521;&#37327;&#22330;&#20013;&#26080;&#30417;&#30563;&#22320;&#22238;&#24402;&#21160;&#21147;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#37327;&#12290;&#28982;&#32780;&#65292;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#36890;&#24120;&#24456;&#22797;&#26434;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20854;&#20013;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#29366;&#24577;&#31354;&#38388;&#30456;&#23545;&#20110;&#26679;&#26412;&#25968;&#37327;&#26159;&#24456;&#22823;&#30340;&#12290; &#26368;&#36817;&#21457;&#29616;&#30340;&#19968;&#31181;&#32531;&#35299;&#29366;&#24577;&#21464;&#37327;&#20043;&#38388;&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#65292;&#24182;&#23558;&#35813;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#23884;&#20837;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#26681;&#25454;&#29289;&#29702;&#23398;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26415;&#35821;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20998;&#31163;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#27169;&#22411;&#23884;&#20837;&#20102;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additi
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.00424</link><description>&lt;p&gt;
CPSP: &#20174;&#38899;&#32032;&#30417;&#30563;&#20013;&#23398;&#20064;&#35821;&#38899;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
CPSP: Learning Speech Concepts From Phoneme Supervision. (arXiv:2309.00424v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35832;&#22914;&#26368;&#23567;&#30417;&#30563;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#12289;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31561;&#32454;&#31890;&#24230;&#29983;&#25104;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20013;&#38388;&#34920;&#31034;&#24212;&#21253;&#21547;&#20171;&#20110;&#25991;&#26412;&#32534;&#30721;&#21644;&#22768;&#23398;&#32534;&#30721;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#20869;&#23481;&#31361;&#20986;&#65292;&#32780;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#31561;&#35821;&#38899;&#20449;&#24687;&#24212;&#35813;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#20013;&#38388;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20887;&#20313;&#24615;&#36807;&#39640;&#21644;&#32500;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#38899;&#39057;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#25552;&#21462;&#29992;&#20110;&#19979;&#28216;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#20840;&#23616;&#25551;&#36848;&#20449;&#24687;&#65292;&#19981;&#36866;&#21512;TTS&#12289;VC&#21644;ASR&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38899;&#32032;-&#35821;&#38899;&#39044;&#35757;&#32451;&#65288;CPSP&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19977;&#20010;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35299;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#38899;&#32032;&#21644;&#35821;&#38899;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representation extracted from speech should contain information that is between text coding and acoustic coding. The linguistic content is salient, while the paralinguistic information such as speaker identity and acoustic details should be removed. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Additionally, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named Contrastive Phoneme-Speech Pretraining (CPSP), which uses three encoders, one decoder, and contrastive learning to bring phoneme and speech
&lt;/p&gt;</description></item><item><title>StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16781</link><description>&lt;p&gt;
StratMed&#65306;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16781
&lt;/p&gt;
&lt;p&gt;
StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26377;&#38480;&#21307;&#30103;&#36164;&#28304;&#19982;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#20043;&#38388;&#30340;&#22833;&#34913;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20020;&#24202;&#20219;&#21153;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#33647;&#29289;&#25512;&#33616;&#26088;&#22312;&#23558;&#24739;&#32773;&#30340;&#32437;&#21521;&#21382;&#21490;&#19982;&#21307;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#23433;&#20840;&#12289;&#26356;&#20934;&#30830;&#22320;&#24320;&#20855;&#33647;&#29289;&#32452;&#21512;&#22788;&#26041;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#21307;&#30103;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#38271;&#23614;&#20998;&#24067;&#65292;&#32570;&#20047;&#22836;&#23614;&#25968;&#25454;&#20043;&#38388;&#30340;&#24179;&#34913;&#34920;&#31034;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StratMed&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21019;&#26032;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21327;&#35843;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26500;&#24314;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#33719;&#21462;&#23454;&#20307;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#37329;&#23383;&#22612;&#30340;&#25968;&#25454;&#20998;&#23618;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#36890;&#29992;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Socratis&#65292;&#19968;&#20010;&#26032;&#30340;&#31038;&#20250;&#21453;&#24212;&#22522;&#20934;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#22810;&#26679;&#21270;&#24773;&#32490;&#21453;&#24212;&#12290;&#26681;&#25454;&#20154;&#31867;&#30740;&#31350;&#32467;&#26524;&#65292;&#20154;&#20204;&#26356;&#21916;&#27426;&#20154;&#24037;&#25776;&#20889;&#30340;&#24773;&#24863;&#21407;&#22240;&#65292;&#27604;&#26426;&#22120;&#29983;&#25104;&#30340;&#35201;&#22810;2&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2308.16741</link><description>&lt;p&gt;
Socratis&#65306;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24773;&#32490;&#24847;&#35782;&#65311;
&lt;/p&gt;
&lt;p&gt;
Socratis: Are large multimodal models emotionally aware?. (arXiv:2308.16741v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16741
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Socratis&#65292;&#19968;&#20010;&#26032;&#30340;&#31038;&#20250;&#21453;&#24212;&#22522;&#20934;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#22810;&#26679;&#21270;&#24773;&#32490;&#21453;&#24212;&#12290;&#26681;&#25454;&#20154;&#31867;&#30740;&#31350;&#32467;&#26524;&#65292;&#20154;&#20204;&#26356;&#21916;&#27426;&#20154;&#24037;&#25776;&#20889;&#30340;&#24773;&#24863;&#21407;&#22240;&#65292;&#27604;&#26426;&#22120;&#29983;&#25104;&#30340;&#35201;&#22810;2&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24773;&#32490;&#39044;&#27979;&#22522;&#20934;&#21253;&#21547;&#31895;&#31961;&#30340;&#24773;&#32490;&#26631;&#31614;&#65292;&#19981;&#32771;&#34385;&#22270;&#20687;&#21644;&#25991;&#26412;&#22312;&#20154;&#31867;&#20013;&#24341;&#21457;&#22810;&#26679;&#21270;&#24773;&#32490;&#30340;&#21508;&#31181;&#21407;&#22240;&#12290;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#23545;&#20110;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#21453;&#24212;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#26234;&#33021;&#26426;&#22120;&#22312;&#29983;&#25104;&#21644;&#20256;&#36882;&#20869;&#23481;&#32473;&#31038;&#20250;&#20013;&#36215;&#21040;&#26680;&#24515;&#20316;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Socratis&#65292;&#19968;&#20010;&#31038;&#20250;&#21453;&#24212;&#22522;&#20934;&#65292;&#22312;&#20854;&#20013;&#27599;&#20010;&#22270;&#20687;-&#26631;&#39064;&#65288;IC&#65289;&#23545;&#37117;&#38468;&#24102;&#26377;&#22810;&#31181;&#24773;&#32490;&#21644;&#24863;&#21463;&#23427;&#20204;&#30340;&#21407;&#22240;&#30340;&#27880;&#37322;&#12290;Socratis&#21253;&#21547;&#20102;&#26469;&#33258;5&#20010;&#24191;&#27867;&#38405;&#35835;&#30340;&#26032;&#38395;&#21644;&#22270;&#20687;&#26631;&#39064;&#65288;IC&#65289;&#25968;&#25454;&#38598;&#30340;2075&#20010;&#22270;&#20687;-&#26631;&#39064;&#23545;&#30340;980&#20010;&#24773;&#32490;&#30340;18K&#20010;&#33258;&#30001;&#24418;&#24335;&#21453;&#24212;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;IC&#23545;&#30340;&#24773;&#24863;&#21407;&#22240;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26681;&#25454;&#19968;&#20010;&#21021;&#27493;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20154;&#20204;&#26356;&#21916;&#27426;&#20154;&#24037;&#25776;&#20889;&#30340;&#21407;&#22240;&#65292;&#27604;&#26426;&#22120;&#29983;&#25104;&#30340;&#35201;&#22810;2&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a \underline{soc}ietal \underline{r}e\underline{a}c\underline{ti}on\underline{s} benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-genera
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AutoDroid&#65292;&#19968;&#20010;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#33258;&#21160;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#21644;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#26469;&#23454;&#29616;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#21644;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.15272</link><description>&lt;p&gt;
&#35753;LLM&#33021;&#22815;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#26234;&#33021;&#20219;&#21153;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Empowering LLM to use Smartphone for Intelligent Task Automation. (arXiv:2308.15272v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AutoDroid&#65292;&#19968;&#20010;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#33258;&#21160;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#21644;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#26469;&#23454;&#29616;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#21644;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#23454;&#29616;&#22522;&#20110;&#35821;&#38899;&#30340;&#20813;&#25552;&#29992;&#25143;&#19982;&#26234;&#33021;&#25163;&#26426;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#30001;&#20110;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26377;&#38480;&#65292;&#20197;&#21450;&#24320;&#21457;&#20154;&#21592;&#25110;&#32456;&#31471;&#29992;&#25143;&#38656;&#35201;&#20184;&#20986;&#38750;&#24120;&#21162;&#21147;&#30340;&#25163;&#21160;&#24037;&#20316;&#32780;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#24046;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#28608;&#21457;&#20102;&#25105;&#20204;&#20174;&#27169;&#22411;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#20219;&#21153;&#20934;&#22791;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoDroid&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#26080;&#38656;&#25163;&#21160;&#24037;&#20316;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#30340;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#23558;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#19982;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#20027;&#35201;&#32452;&#20214;&#21253;&#25324;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#65292;&#26725;&#25509;&#20102;UI&#21644;LLM&#65292;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection t
&lt;/p&gt;</description></item><item><title>C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14781</link><description>&lt;p&gt;
&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conflict-Aware Active Automata Learning. (arXiv:2308.14781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14781
&lt;/p&gt;
&lt;p&gt;
C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65288;&#21516;&#19968;&#36755;&#20837;&#23545;&#24212;&#19981;&#21516;&#36755;&#20986;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#31181;&#22266;&#26377;&#30340;&#20914;&#31361;&#24674;&#22797;&#33021;&#21147;&#19981;&#36275;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#25110;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#21464;&#21270;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#65288;C3AL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#22788;&#29702;&#20914;&#31361;&#20449;&#24687;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#25152;&#35859;&#30340;&#35266;&#27979;&#26641;&#35270;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#65292;&#24182;&#22312;&#38754;&#23545;&#20914;&#31361;&#26102;&#26368;&#23567;&#21270;&#23545;&#27491;&#22312;&#23398;&#20064;&#30340;&#31995;&#32479;&#25191;&#34892;&#30340;&#27979;&#35797;&#27425;&#25968;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#23427;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;C3AL&#65292;&#28085;&#30422;&#20102;30&#22810;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#30446;&#26631;&#21644;18,000&#22810;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;C3AL&#26159;&#19968;&#20010;&#21512;&#36866;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active automata learning algorithms cannot easily handle \emph{conflict} in the observation data (different outputs observed for the same inputs). This inherent inability to recover after a conflict impairs their effective applicability in scenarios where noise is present or the system under learning is mutating.  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to enable handling conflicting information during the learning process. The core idea is to consider the so-called observation tree as a first-class citizen in the learning process. Though this idea is explored in recent work, we take it to its full effect by enabling its use with any existing learner and minimizing the number of tests performed on the system under learning, specially in the face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over 30 different realistic targets, and over 18,000 different scenarios. The results of the evaluation show that C3AL is a suitable alternati
&lt;/p&gt;</description></item><item><title>InstructME&#26159;&#19968;&#20010;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#38899;&#20048;&#32534;&#36753;&#21644;&#28151;&#38899;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#32858;&#21512;&#21644;&#24341;&#20837;&#21644;&#24358;&#36827;&#23637;&#30697;&#38453;&#26469;&#20445;&#25345;&#32534;&#36753;&#30340;&#19968;&#33268;&#24615;&#21644;&#25552;&#39640;&#26059;&#24459;&#21644;&#35856;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14360</link><description>&lt;p&gt;
InstructME:&#19968;&#20010;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#38899;&#20048;&#32534;&#36753;&#21644;&#28151;&#38899;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
InstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models. (arXiv:2308.14360v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14360
&lt;/p&gt;
&lt;p&gt;
InstructME&#26159;&#19968;&#20010;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#38899;&#20048;&#32534;&#36753;&#21644;&#28151;&#38899;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#32858;&#21512;&#21644;&#24341;&#20837;&#21644;&#24358;&#36827;&#23637;&#30697;&#38453;&#26469;&#20445;&#25345;&#32534;&#36753;&#30340;&#19968;&#33268;&#24615;&#21644;&#25552;&#39640;&#26059;&#24459;&#21644;&#35856;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#32534;&#36753;&#20027;&#35201;&#28041;&#21450;&#21040;&#20462;&#25913;&#20048;&#22120;&#36712;&#36947;&#25110;&#25972;&#20307;&#28151;&#38899;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25805;&#20316;&#20026;&#21407;&#22987;&#20316;&#21697;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#37325;&#26032;&#35808;&#37322;&#12290;&#36825;&#20123;&#38899;&#20048;&#22788;&#29702;&#26041;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#34429;&#28982;&#23545;&#22270;&#20687;&#21644;&#38899;&#39057;&#20462;&#25913;&#26377;&#25928;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#20110;&#38899;&#20048;&#26102;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#24402;&#22240;&#20110;&#38899;&#20048;&#30340;&#29420;&#29305;&#25968;&#25454;&#24615;&#36136;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#30772;&#22351;&#38899;&#20048;&#30340;&#20869;&#22312;&#21644;&#35856;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;InstructME&#65292;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#38899;&#20048;&#32534;&#36753;&#21644;&#28151;&#38899;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#32534;&#36753;&#20043;&#21069;&#21644;&#20043;&#21518;&#36890;&#36807;&#22810;&#23610;&#24230;&#32858;&#21512;&#24378;&#21270;&#20102;U-Net&#20197;&#20445;&#25345;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21644;&#24358;&#36827;&#23637;&#30697;&#38453;&#20316;&#20026;&#26465;&#20214;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#35821;&#20041;&#31354;&#38388;&#20197;&#25552;&#39640;&#32534;&#36753;&#26102;&#30340;&#26059;&#24459;&#21644;&#35856;&#24615;&#12290;&#20026;&#20102;&#36866;&#24212;&#22806;&#37096;&#38656;&#27714;&#65292;&#25105;&#20204;&#36824;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#28151;&#38899;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music editing primarily entails the modification of instrument tracks or remixing in the whole, which offers a novel reinterpretation of the original piece through a series of operations. These music processing methods hold immense potential across various applications but demand substantial expertise. Prior methodologies, although effective for image and audio modifications, falter when directly applied to music. This is attributed to music's distinctive data nature, where such methods can inadvertently compromise the intrinsic harmony and coherence of music. In this paper, we develop InstructME, an Instruction guided Music Editing and remixing framework based on latent diffusion models. Our framework fortifies the U-Net with multi-scale aggregation in order to maintain consistency before and after editing. In addition, we introduce chord progression matrix as condition information and incorporate it in the semantic space to improve melodic harmony while editing. For accommodating ext
&lt;/p&gt;</description></item><item><title>DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.10807</link><description>&lt;p&gt;
DynED: &#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
DynED: Dynamic Ensemble Diversification in Data Stream Classification. (arXiv:2308.10807v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10807
&lt;/p&gt;
&lt;p&gt;
DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#31361;&#21464;&#24615;&#21464;&#21270;&#65292;&#20063;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38598;&#21512;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290; &#22312;&#38598;&#21512;&#20869;&#37096;&#30340;&#26356;&#22823;&#22810;&#26679;&#24615;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#38598;&#21512;&#20869;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#24456;&#39640;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#32452;&#20214;&#37117;&#20687;&#39044;&#26399;&#30340;&#37027;&#26679;&#23545;&#25972;&#20307;&#24615;&#33021;&#26377;&#25152;&#36129;&#29486;&#12290;&#36825;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#36873;&#25321;&#23637;&#29616;&#20986;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MMR&#65288;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65289;&#30340;&#26032;&#22411;&#38598;&#21512;&#26500;&#24314;&#21644;&#32500;&#25252;&#26041;&#27861;&#65292;&#22312;&#32452;&#21512;&#38598;&#21512;&#30340;&#36807;&#31243;&#20013;&#21160;&#24577;&#22320;&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#21644;11&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65288;DynED&#65289;&#30456;&#27604;&#20110;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods are commonly used in classification due to their remarkable performance. Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift. A greater diversity of ensemble components is known to enhance prediction accuracy in such settings. Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance. This necessitates a method for selecting components that exhibit high performance and diversity. We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble. The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines
&lt;/p&gt;</description></item><item><title>&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#21152;&#36895;&#24320;&#21457;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09520</link><description>&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#35770;&#25991;&#38598;
&lt;/p&gt;
&lt;p&gt;
Proceedings of the 2nd International Workshop on Adaptive Cyber Defense. (arXiv:2308.09520v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09520
&lt;/p&gt;
&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#21152;&#36895;&#24320;&#21457;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#22312;&#20315;&#32599;&#37324;&#36798;&#29702;&#24037;&#23398;&#38498;&#20030;&#34892;&#65292;&#35813;&#30740;&#35752;&#20250;&#26088;&#22312;&#20998;&#20139;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#24403;&#21069;&#30340;&#32593;&#32476;&#39046;&#22495;&#26080;&#27861;&#21487;&#38752;&#26377;&#25928;&#22320;&#36827;&#34892;&#38450;&#24481;&#65292;&#24517;&#39035;&#24191;&#27867;&#20381;&#36182;&#20154;&#24037;&#19987;&#23478;&#12290;&#29087;&#32451;&#30340;&#32593;&#32476;&#38450;&#24481;&#20154;&#21592;&#20379;&#24212;&#19981;&#36275;&#65292;&#24448;&#24448;&#26080;&#27861;&#21450;&#26102;&#24212;&#23545;&#32593;&#32476;&#23041;&#32961;&#12290;&#20511;&#37492;AI&#21644;ML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#32593;&#32476;&#38450;&#24481;&#30740;&#31350;&#31038;&#21306;&#34987;&#28608;&#21169;&#30528;&#36890;&#36807;&#23558;AI&#21644;ML&#25216;&#26415;&#24212;&#29992;&#20110;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#24320;&#21457;&#26032;&#30340;&#21160;&#24577;&#21487;&#25345;&#32493;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#19982;&#23454;&#36341;&#32773;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#36317;&#21487;&#20197;&#21152;&#36895;&#21019;&#24314;&#33021;&#22815;&#23398;&#20064;&#35782;&#21035;&#21644;&#24212;&#23545;&#32593;&#32476;&#25915;&#20987;&#65292;&#25110;&#32773;&#21457;&#29616;&#21644;&#20943;&#36731;&#24369;&#28857;&#30340;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 2nd International Workshop on Adaptive Cyber Defense was held at the Florida Institute of Technology, Florida. This workshop was organized to share research that explores unique applications of Artificial Intelligence (AI) and Machine Learning (ML) as foundational capabilities for the pursuit of adaptive cyber defense. The cyber domain cannot currently be reliably and effectively defended without extensive reliance on human experts. Skilled cyber defenders are in short supply and often cannot respond fast enough to cyber threats.  Building on recent advances in AI and ML the Cyber defense research community has been motivated to develop new dynamic and sustainable defenses through the adoption of AI and ML techniques to cyber settings. Bridging critical gaps between AI and Cyber researchers and practitioners can accelerate efforts to create semi-autonomous cyber defenses that can learn to recognize and respond to cyber attacks or discover and mitigate weaknesses in cooperation with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25216;&#26415;&#22522;&#30784;&#21644;&#28508;&#22312;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#37319;&#29992;&#31574;&#30053;&#26694;&#26550;&#65292;&#24182;&#25351;&#20986;&#32852;&#37030;&#23398;&#20064;&#20026;&#21830;&#19994;&#21644;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#23398;&#30028;&#25552;&#20379;&#20102;&#36328;&#23398;&#31185;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2308.02219</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65306;&#26426;&#26500;&#26426;&#36935;&#12289;&#25361;&#25112;&#21644;&#37319;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Federated Learning: Organizational Opportunities, Challenges, and Adoption Strategies. (arXiv:2308.02219v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25216;&#26415;&#22522;&#30784;&#21644;&#28508;&#22312;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#37319;&#29992;&#31574;&#30053;&#26694;&#26550;&#65292;&#24182;&#25351;&#20986;&#32852;&#37030;&#23398;&#20064;&#20026;&#21830;&#19994;&#21644;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#23398;&#30028;&#25552;&#20379;&#20102;&#36328;&#23398;&#31185;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#65292;&#25968;&#25454;&#20849;&#20139;&#30340;&#38480;&#21046;&#24615;&#35268;&#21017;&#23548;&#33268;&#32852;&#37030;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20801;&#35768;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#21512;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#19982;&#20182;&#20154;&#20849;&#20139;&#20854;&#21508;&#33258;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#39318;&#20808;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25216;&#26415;&#22522;&#30784;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#23558;&#32452;&#32455;&#25353;&#29031;&#20854;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#21644;&#29615;&#22659;&#36827;&#34892;&#20102;&#26144;&#23556;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#19981;&#21516;&#34892;&#19994;&#30340;&#20856;&#22411;&#32452;&#32455;&#65292;&#21253;&#25324;&#34892;&#19994;&#32852;&#30431;&#12289;&#24314;&#31435;&#38134;&#34892;&#12289;&#20844;&#20849;&#26426;&#26500;&#21644;&#25968;&#25454;&#23494;&#38598;&#22411;&#20013;&#23567;&#20225;&#19994;&#21487;&#33021;&#32771;&#34385;&#19981;&#21516;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#32852;&#37030;&#23398;&#20064;&#20026;&#21830;&#19994;&#21644;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#23398;&#30028;&#25552;&#20379;&#20102;&#20805;&#28385;&#36328;&#23398;&#31185;&#30740;&#31350;&#26426;&#20250;&#30340;&#26426;&#26500;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restrictive rules for data sharing in many industries have led to the development of \ac{FL}. \ac{FL} is a \ac{ML} technique that allows distributed clients to train models collaboratively without the need to share their respective training data with others. In this article, we first explore the technical basics of FL and its potential applications. Second, we present a conceptual framework for the adoption of \ac{FL}, mapping organizations along the lines of their \ac{AI} capabilities and environment. We then discuss why exemplary organizations in different industries, including industry consortia, established banks, public authorities, and data-intensive SMEs might consider different approaches to \ac{FL}. To conclude, we argue that \ac{FL} presents an institutional shift with ample interdisciplinary research opportunities for the business and information systems engineering community.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22270;&#23884;&#20837;&#20013;&#24212;&#29992;&#20449;&#24687;&#20960;&#20309;&#26469;&#25551;&#36848;Ising&#27169;&#22411;&#30340;&#22522;&#24577;&#65292;&#36890;&#36807;&#21033;&#29992;&#29699;&#38754;&#21644;&#21452;&#26354;&#38754;&#25299;&#25169;&#19978;&#30340;&#32534;&#30721;&#65292;&#24314;&#31435;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#32416;&#38169;&#30721;&#21644;&#21457;&#23637;&#23884;&#20837;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15778</link><description>&lt;p&gt;
&#22312;&#22270;&#23884;&#20837;&#20013;&#22522;&#20110;&#29699;&#38754;&#21644;&#21452;&#26354;&#38754;&#25299;&#25169;&#30340;&#32534;&#30721;&#24212;&#29992;&#20110;Ising MRF&#27169;&#22411;&#65306;&#32463;&#20856;&#21644;&#37327;&#23376;&#25299;&#25169;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning. (arXiv:2307.15778v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22270;&#23884;&#20837;&#20013;&#24212;&#29992;&#20449;&#24687;&#20960;&#20309;&#26469;&#25551;&#36848;Ising&#27169;&#22411;&#30340;&#22522;&#24577;&#65292;&#36890;&#36807;&#21033;&#29992;&#29699;&#38754;&#21644;&#21452;&#26354;&#38754;&#25299;&#25169;&#19978;&#30340;&#32534;&#30721;&#65292;&#24314;&#31435;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#32416;&#38169;&#30721;&#21644;&#21457;&#23637;&#23884;&#20837;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#20449;&#24687;&#20960;&#20309;&#24212;&#29992;&#20110;&#25551;&#36848;Ising&#27169;&#22411;&#30340;&#22522;&#24577;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#25176;&#37324;&#20811;&#21644;&#29699;&#38754;&#25299;&#25169;&#19978;&#30340;&#24490;&#29615;&#21644;&#20934;&#24490;&#29615;&#30721;&#30340;&#22855;&#20598;&#26816;&#39564;&#30697;&#38453;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21516;&#26500;&#21644;&#20934;&#24490;&#29615;&#30721;&#24490;&#29615;&#30697;&#38453;&#30340;&#23610;&#23544;&#26041;&#38754;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#22522;&#20110;&#25429;&#33719;&#38598;&#30340;&#23884;&#20837;&#26041;&#27861;&#30340;&#21457;&#23637;&#20855;&#26377;&#24433;&#21709;&#12290;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#25968;&#23383;&#20960;&#20309;&#23398;&#26469;&#20248;&#21270;&#32416;&#38169;&#30721;&#65292;&#20174;&#32780;&#23548;&#33268;&#36825;&#20123;&#23884;&#20837;&#21644;&#31232;&#30095;&#22240;&#23376;&#21270;&#26041;&#27861;&#30340;&#20986;&#29616;&#12290;&#26412;&#25991;&#36890;&#36807;&#28436;&#31034;&#38271;&#36317;&#31163;&#39046;&#22495;&#30340;&#26368;&#26032;DNN&#26550;&#26500;&#65288;ChordMixer&#65292;Mega&#65292;Mega-chunk&#65292;CDIL&#65292;...&#65289;&#19982;&#29305;&#23450;&#31867;&#22411;&#65288;Cage-graph&#65292;Repeat Accumulate&#65289;&#30340;&#21306;&#22359;&#21644;&#21367;&#31215;LDPC&#30721;&#31561;&#20215;&#30340;&#26041;&#24335;&#65292;&#24314;&#31435;&#20102;DNN&#26550;&#26500;&#21644;&#32416;&#38169;&#32534;&#30721;&#20043;&#38388;&#30340;&#30452;&#25509;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper introduces the application of information geometry to describe the ground states of Ising models. This is achieved by utilizing parity-check matrices of cyclic and quasi-cyclic codes on toric and spherical topologies. The approach establishes a connection between machine learning and error-correcting coding, specifically in terms of automorphism and the size of the circulant of the quasi-cyclic code. This proposed approach has implications for the development of new embedding methods based on trapping sets. Statistical physics and number geometry are utilized to optimize error-correcting codes, leading to these embedding and sparse factorization methods. The paper establishes a direct connection between DNN architecture and error-correcting coding by demonstrating how state-of-the-art DNN architectures (ChordMixer, Mega, Mega-chunk, CDIL, ...) from the long-range arena can be equivalent to specific types (Cage-graph, Repeat Accumulate) of block and convolutional LDPC codes. Q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26397;&#30528;&#21487;&#35299;&#37322;&#30340;AI&#22312;&#31227;&#21160;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21453;&#20107;&#23454;&#26469;&#23398;&#20064;&#20174;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08461</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#35299;&#37322;&#30340;AI&#22312;&#31227;&#21160;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards eXplainable AI for Mobility Data Science. (arXiv:2307.08461v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26397;&#30528;&#21487;&#35299;&#37322;&#30340;AI&#22312;&#31227;&#21160;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21453;&#20107;&#23454;&#26469;&#23398;&#20064;&#20174;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#30340;&#20851;&#20110;&#21487;&#35299;&#37322;AI&#22312;&#31227;&#21160;&#25968;&#25454;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#24037;&#20316;&#65292;&#37325;&#28857;&#26159;&#33021;&#22815;&#20174;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#65288;&#22914;&#36710;&#36742;&#21644;&#33337;&#21482;&#30340;GPS&#36712;&#36857;&#65289;&#20013;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#20351;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;GeoXAI&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21487;&#29702;&#35299;&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#24182;&#21246;&#30011;&#20986;&#20102;&#26397;&#30528;&#21487;&#35299;&#37322;&#30340;AI&#22312;&#31227;&#21160;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#30740;&#31350;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our ongoing work towards XAI for Mobility Data Science applications, focusing on explainable models that can learn from dense trajectory data, such as GPS tracks of vehicles and vessels using temporal graph neural networks (GNNs) and counterfactuals. We review the existing GeoXAI studies, argue the need for comprehensible explanations with human-centered approaches, and outline a research path toward XAI for Mobility Data Science.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-CPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#22788;&#29702;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.04869</link><description>&lt;p&gt;
Fed-CPrompt: &#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#30340;&#23545;&#27604;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning. (arXiv:2307.04869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-CPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#22788;&#29702;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#65288;FCL&#65289;&#20174;&#20998;&#24067;&#22312;&#23458;&#25143;&#31471;&#19978;&#30340;&#26426;&#23494;&#25968;&#25454;&#38598;&#20013;&#36880;&#27493;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#26080;&#37325;&#22797;&#23398;&#20064;&#30340;FCL&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#23384;&#22312;&#22240;&#26080;&#27861;&#35775;&#38382;&#21382;&#21490;&#20219;&#21153;&#25968;&#25454;&#32780;&#23548;&#33268;&#20005;&#37325;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;Fed-CPrompt&#65292;&#20197;&#19968;&#31181;&#39640;&#25928;&#30340;&#36890;&#20449;&#26041;&#24335;&#33719;&#24471;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#12290;Fed-CPrompt&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#24322;&#27493;&#25552;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#25345;&#32493;&#25439;&#22833;&#65292;&#20197;&#20998;&#21035;&#22788;&#29702;FCL&#20013;&#30340;&#24322;&#27493;&#20219;&#21153;&#21040;&#36798;&#21644;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;Fed-CPrompt&#22312;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26080;&#37325;&#22797;&#23398;&#20064;FCL&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated continual learning (FCL) learns incremental tasks over time from confidential datasets distributed across clients. This paper focuses on rehearsal-free FCL, which has severe forgetting issues when learning new tasks due to the lack of access to historical task data. To address this issue, we propose Fed-CPrompt based on prompt learning techniques to obtain task-specific prompts in a communication-efficient way. Fed-CPrompt introduces two key components, asynchronous prompt learning, and contrastive continual loss, to handle asynchronous task arrival and heterogeneous data distributions in FCL, respectively. Extensive experiments demonstrate the effectiveness of Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#25299;&#23637;&#20102;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#22635;&#34917;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#35299;&#37322;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2307.02131</link><description>&lt;p&gt;
&#36229;&#36234;&#24050;&#30693;&#29616;&#23454;&#65306;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#34892;&#21307;&#23398;&#30740;&#31350; (arXiv&#65306;2307.02131v2 [cs.AI] &#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research. (arXiv:2307.02131v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#25299;&#23637;&#20102;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#22635;&#34917;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#35299;&#37322;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25506;&#32034;&#21307;&#23398;&#30740;&#31350;&#20013;&#30340;&#8220;&#20551;&#22914;&#8221;&#22330;&#26223;&#65292;&#26088;&#22312;&#25299;&#23637;&#25105;&#20204;&#23545;&#29616;&#26377;&#36793;&#30028;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#21033;&#29992;&#30913;&#20849;&#25391;&#25104;&#20687;&#29305;&#24449;&#26469;&#35786;&#26029;&#20799;&#31185;&#21518;&#39045;&#31389;&#33041;&#32959;&#30244;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#24050;&#32463;&#35265;&#35777;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#21644;&#23398;&#26415;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32467;&#26524;&#30340;&#20154;&#31867;&#21451;&#22909;&#35299;&#37322;&#30340;&#32570;&#20047;&#26174;&#33879;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#25509;&#21463;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34701;&#20837;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20026;&#26816;&#26597;&#26367;&#20195;&#20915;&#31574;&#22330;&#26223;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#35299;&#37322;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#29305;&#23450;&#30340;&#35265;&#35299;&#65292;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#39564;&#35777;&#39044;&#27979;&#24182;&#28548;&#28165;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#24046;&#24322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#20445;&#25345;&#20102;&#32479;&#35745;&#23398;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study employs counterfactual explanations to explore "what if?" scenarios in medical research, with the aim of expanding our understanding beyond existing boundaries. Specifically, we focus on utilizing MRI features for diagnosing pediatric posterior fossa brain tumors as a case study. The field of artificial intelligence and explainability has witnessed a growing number of studies and increasing scholarly interest. However, the lack of human-friendly interpretations in explaining the outcomes of machine learning algorithms has significantly hindered the acceptance of these methods by clinicians in their clinical practice. To address this, our approach incorporates counterfactual explanations, providing a novel way to examine alternative decision-making scenarios. These explanations offer personalized and context-specific insights, enabling the validation of predictions and clarification of variations under diverse circumstances. Importantly, our approach maintains both statistica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#30475;&#20284;&#26080;&#20851;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#37325;&#26032;&#26500;&#36896;&#20026;&#20849;&#21516;&#30340;&#20004;&#20010;&#20132;&#38169;&#27493;&#39588;&#65292;&#21363;&#20048;&#35266;&#31574;&#30053;&#25913;&#36827;&#21644;&#21518;&#35265;&#36866;&#24212;&#65292;&#32479;&#19968;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21152;&#36895;&#26041;&#27861;&#20013;&#30340;&#20048;&#35266;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20849;&#21516;&#29702;&#35770;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10587</link><description>&lt;p&gt;
&#31574;&#30053;&#20248;&#21270;&#20013;&#30340;&#20048;&#35266;&#24615;&#21644;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimism and Adaptivity in Policy Optimization. (arXiv:2306.10587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#30475;&#20284;&#26080;&#20851;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#37325;&#26032;&#26500;&#36896;&#20026;&#20849;&#21516;&#30340;&#20004;&#20010;&#20132;&#38169;&#27493;&#39588;&#65292;&#21363;&#20048;&#35266;&#31574;&#30053;&#25913;&#36827;&#21644;&#21518;&#35265;&#36866;&#24212;&#65292;&#32479;&#19968;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21152;&#36895;&#26041;&#27861;&#20013;&#30340;&#20048;&#35266;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20849;&#21516;&#29702;&#35770;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#36890;&#36807;&#8220;&#20048;&#35266;&#24615;&#8221;&#21644;&#8220;&#36866;&#24212;&#24615;&#8221;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#30340;&#32479;&#19968;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#31574;&#30053;&#36845;&#20195;&#21644;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20043;&#38388;&#30340;&#28145;&#21051;&#32852;&#31995;&#65292;&#25105;&#20204;&#23558;&#19968;&#20123;&#30475;&#20284;&#26080;&#20851;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#37325;&#26032;&#26500;&#36896;&#20026;&#20004;&#20010;&#20132;&#38169;&#27493;&#39588;&#65288;i&#65289;&#20048;&#35266;&#31574;&#30053;&#25913;&#36827;&#25805;&#20316;&#22120;&#20351;&#29992;&#8220;&#26799;&#24230;&#19978;&#21319;&#39044;&#27979;&#8221;&#23558;&#20808;&#21069;&#30340;&#31574;&#30053;$\pi_t$&#26144;&#23556;&#21040;&#19968;&#20010;&#20551;&#35774;$\pi_{t+1}$&#65292;&#28982;&#21518;&#65288;ii&#65289;&#23545;$\pi_{t+1}$&#30340;&#24615;&#33021;&#36827;&#34892;&#37096;&#20998;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#27492;&#36827;&#34892;&#8220;&#21518;&#35265;&#36866;&#24212;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20849;&#20139;&#30340;&#35270;&#35282;&#26469;&#20849;&#21516;&#34920;&#36798;&#20854;&#20182;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#36719;&#20214;&#21644;&#20048;&#35266;&#31574;&#30053;&#36845;&#20195;&#12289;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12289;&#22522;&#20110;&#21069;&#21521;&#25628;&#32034;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#25913;&#36827;&#21644;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20851;&#20110;&#36890;&#36807;&#20048;&#35266;&#24615;&#21644;&#36866;&#24212;&#24615;&#21152;&#36895;&#30340;&#20849;&#21516;&#29702;&#35770;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We work towards a unifying paradigm for accelerating policy optimization methods in reinforcement learning (RL) through \emph{optimism} \&amp; \emph{adaptivity}. Leveraging the deep connection between policy iteration and policy gradient methods, we recast seemingly unrelated policy optimization algorithms as the repeated application of two interleaving steps (i) an \emph{optimistic policy improvement operator} maps a prior policy $\pi_t$ to a hypothesis $\pi_{t+1}$ using a \emph{gradient ascent prediction}, followed by (ii) a \emph{hindsight adaptation} of the optimistic prediction based on a partial evaluation of the performance of $\pi_{t+1}$. We use this shared lens to jointly express other well-known algorithms, including soft and optimistic policy iteration, natural actor-critic methods, model-based policy improvement based on forward search, and meta-learning algorithms. By doing so, we shed light on collective theoretical properties related to acceleration via optimism \&amp; adaptivit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;</title><link>http://arxiv.org/abs/2306.03933</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#32622;&#25442;&#19981;&#21464;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23398;&#20064;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#22256;&#38590;&#65292;&#26032;&#29289;&#29702;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#20302;&#32500;&#31354;&#38388;&#12290;&#29305;&#21035;&#26159;&#22312;&#25104;&#20998;&#32423;&#21035;&#19978;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#31561;&#33391;&#22909;&#24615;&#36136;&#21512;&#24182;&#21040;&#27969;&#34892;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#32622;&#25442;&#19981;&#21464;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#29992;&#20316;&#32622;&#25442;&#19981;&#21464;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#30340;&#21487;&#33021;&#24615;&#36739;&#20302;&#30340;&#21943;&#27880;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#27604;&#19982;&#34987;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#33719;&#24471;&#30340;&#23494;&#24230;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01665</link><description>&lt;p&gt;
SourceP&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#26234;&#33021;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;
&lt;/p&gt;
&lt;p&gt;
SourceP: Smart Ponzi Schemes Detection on Ethereum Using Pre-training Model with Data Flow. (arXiv:2306.01665v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21306;&#22359;&#38142;&#25216;&#26415;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20856;&#22411;&#30340;&#37329;&#34701;&#39575;&#23616;&#24222;&#20857;&#39575;&#23616;&#20063;&#22312;&#21306;&#22359;&#38142;&#24179;&#21488;&#20197;&#22826;&#22346;&#19978;&#20986;&#29616;&#12290;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#37096;&#32626;&#30340;&#36825;&#31181;&#24222;&#20857;&#39575;&#23616;&#65292;&#20063;&#31216;&#20026;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#65292;&#24050;&#32463;&#36896;&#25104;&#20102;&#22823;&#37327;&#30340;&#32463;&#27982;&#25439;&#22833;&#21644;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20197;&#22826;&#22346;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#23383;&#33410;&#30721;&#29305;&#24449;&#12289;&#25805;&#20316;&#30721;&#29305;&#24449;&#12289;&#36134;&#25143;&#29305;&#24449;&#21644;&#20132;&#26131;&#34892;&#20026;&#29305;&#24449;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SourceP&#65292;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#22312;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#20316;&#20026;&#29305;&#24449;&#65292;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#25506;&#32034;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#21487;&#33021;&#24615;&#12290;SourceP&#38477;&#20302;&#20102;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#30340;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#38590;&#24230;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As blockchain technology becomes more and more popular, a typical financial scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum. This Ponzi scheme deployed through smart contracts, also known as the smart Ponzi scheme, has caused a lot of economic losses and negative impacts. Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on bytecode features, opcode features, account features, and transaction behavior features of smart contracts, and such methods lack interpretability and sustainability. In this paper, we propose SourceP, a method to detect smart Ponzi schemes on the Ethereum platform using pre-training models and data flow, which only requires using the source code of smart contracts as features to explore the possibility of detecting smart Ponzi schemes from another direction. SourceP reduces the difficulty of data acquisition and feature extraction of existing detection methods while increasing the interpretability of the model. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.00526</link><description>&lt;p&gt;
&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#38646;&#26679;&#26412;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#25351;&#23548;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering. (arXiv:2306.00526v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24067;&#23616;&#24863;&#30693;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#24494;&#35843;&#23545;&#20110;&#39069;&#22806;&#30340;&#35270;&#35273;&#12289;&#24067;&#23616;&#21644;&#20219;&#21153;&#27169;&#22359;&#38459;&#27490;&#20102;&#20854;&#30452;&#25509;&#21033;&#29992;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#26368;&#36817;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;&#19982;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#39046;&#22495;&#23545;&#40784;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#19982;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#23427;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#12290;&#21069;&#32773;&#36890;&#36807;&#36866;&#24403;&#30340;&#31354;&#26684;&#21644;&#25442;&#34892;&#31526;&#20174;OCR&#24037;&#20855;&#20013;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#12290;&#21518;&#32773;&#30830;&#20445;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pre-training-fine-tuning paradigm based on layout-aware multimodal pre-trained models has achieved significant progress on document image question answering. However, domain pre-training and task fine-tuning for additional visual, layout, and task modules prevent them from directly utilizing off-the-shelf instruction-tuning language foundation models, which have recently shown promising potential in zero-shot learning. Contrary to aligning language models to the domain of document image question answering, we align document image question answering to off-the-shell instruction-tuning language foundation models to utilize their zero-shot capability. Specifically, we propose layout and task aware instruction prompt called LATIN-Prompt, which consists of layout-aware document content and task-aware descriptions. The former recovers the layout information among text segments from OCR tools by appropriate spaces and line breaks. The latter ensures that the model generates answers that m
&lt;/p&gt;</description></item><item><title>AnoOnly&#26159;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#26469;&#35299;&#20915;&#21516;&#36136;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18798</link><description>&lt;p&gt;
AnoOnly:&#26080;&#38656;&#25439;&#22833;&#27491;&#24120;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AnoOnly: Semi-Supervised Anomaly Detection without Loss on Normal Data. (arXiv:2305.18798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18798
&lt;/p&gt;
&lt;p&gt;
AnoOnly&#26159;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#26469;&#35299;&#20915;&#21516;&#36136;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;(SSAD)&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#20294;&#26377;&#25351;&#23548;&#20316;&#29992;&#30340;&#24322;&#24120;&#23454;&#20363;&#65292;&#22686;&#24378;&#20102;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;(UAD)&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#21516;&#36136;&#27491;&#24120;&#25968;&#25454;&#23545;&#24322;&#24120;&#30340;&#32479;&#27835;&#20351;&#24471;SSAD&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22320;&#24863;&#30693;&#24322;&#24120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#22312;&#20005;&#37325;&#19981;&#24179;&#34913;&#30340;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;AnoOnly(&#20165;&#24322;&#24120;)&#30340;&#26032;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#30340;SSAD&#26041;&#27861;&#19981;&#21516;&#65292;AnoOnly&#26242;&#20572;&#20102;&#20005;&#26684;&#30340;&#25439;&#22833;&#30417;&#30563;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#24418;&#24335;&#12290;&#36825;&#31181;&#24369;&#30417;&#30563;&#36890;&#36807;&#25209;&#37327;&#24402;&#19968;&#21270;&#23454;&#29616;&#65292;&#38544;&#24335;&#22320;&#23545;&#27491;&#24120;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#23398;&#20064;&#12290;&#24403;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;SSAD&#26041;&#27861;&#20013;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;AnoOnly&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;A
&lt;/p&gt;
&lt;p&gt;
Semi-supervised anomaly detection (SSAD) methods have demonstrated their effectiveness in enhancing unsupervised anomaly detection (UAD) by leveraging few-shot but instructive abnormal instances. However, the dominance of homogeneous normal data over anomalies biases the SSAD models against effectively perceiving anomalies. To address this issue and achieve balanced supervision between heavily imbalanced normal and abnormal data, we develop a novel framework called AnoOnly (Anomaly Only). Unlike existing SSAD methods that resort to strict loss supervision, AnoOnly suspends it and introduces a form of weak supervision for normal data. This weak supervision is instantiated through the utilization of batch normalization, which implicitly performs cluster learning on normal data. When integrated into existing SSAD methods, the proposed AnoOnly demonstrates remarkable performance enhancements across various models and datasets, achieving new state-of-the-art performance. Additionally, our A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#20986;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.03859</link><description>&lt;p&gt;
&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#65306;&#20197;&#33521;&#22269;COVID-19&#20026;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Open problems in causal structure learning: A case study of COVID-19 in the UK. (arXiv:2305.03859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#20986;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#24674;&#22797;&#22270;&#24418;&#32467;&#26500;&#65292;&#20174;&#32780;&#25581;&#31034;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#20123;&#31639;&#27861;&#25552;&#20379;&#30340;&#22240;&#26524;&#34920;&#31034;&#20351;&#24471;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#24471;&#20197;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#19982;&#20851;&#32852;&#24615;&#26426;&#22120;&#23398;&#20064;&#30456;&#27604;&#65292;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#21508;&#31181;&#20844;&#20849;&#26469;&#28304;&#25972;&#21512;&#25968;&#25454;&#65292;&#24182;&#30740;&#31350;&#21508;&#31181;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#20102;&#27599;&#20010;&#31639;&#27861;&#21450;&#31639;&#27861;&#32452;&#20135;&#29983;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#22270;&#24418;&#32467;&#26500;&#12289;&#27169;&#22411;&#32500;&#24230;&#12289;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#28151;&#28102;&#21464;&#37327;&#12289;&#39044;&#27979;&#21644;&#24178;&#39044;&#25512;&#26029;&#31561;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#26469;&#31361;&#20986;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal machine learning (ML) algorithms recover graphical structures that tell us something about cause-and-effect relationships. The causal representation provided by these algorithms enables transparency and explainability, which is necessary in critical real-world problems. Yet, causal ML has had limited impact in practice compared to associational ML. This paper investigates the challenges of causal ML with application to COVID-19 UK pandemic data. We collate data from various public sources and investigate what the various structure learning algorithms learn from these data. We explore the impact of different data formats on algorithms spanning different classes of learning, and assess the results produced by each algorithm, and groups of algorithms, in terms of graphical structure, model dimensionality, sensitivity analysis, confounding variables, predictive and interventional inference. We use these results to highlight open problems in causal structure learning and directions f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#25104;&#24335;&#38544;&#20889;&#25193;&#25955;&#8221;&#65288;GSD&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#38544;&#20889;&#26041;&#26696;&#65292;&#21033;&#29992;&#19968;&#31181;&#21453;&#28436;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;100&#65285;&#22320;&#24674;&#22797;&#38544;&#34255;&#30340;&#31192;&#23494;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38544;&#20889;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.03472</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#38544;&#20889;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Generative Steganography Diffusion. (arXiv:2305.03472v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03472
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#25104;&#24335;&#38544;&#20889;&#25193;&#25955;&#8221;&#65288;GSD&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#38544;&#20889;&#26041;&#26696;&#65292;&#21033;&#29992;&#19968;&#31181;&#21453;&#28436;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;100&#65285;&#22320;&#24674;&#22797;&#38544;&#34255;&#30340;&#31192;&#23494;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38544;&#20889;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#38544;&#20889;&#26415;&#65288;GS&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#23427;&#30452;&#25509;&#20174;&#31192;&#23494;&#25968;&#25454;&#20013;&#29983;&#25104;&#38544;&#34255;&#20449;&#24687;&#22270;&#20687;&#12290;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#22522;&#20110;GAN&#25110;Flow&#30340;&#21508;&#31181;GS&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#30340;GS&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#32593;&#32476;&#21487;&#36870;&#24615;&#65292;&#26080;&#27861;&#23436;&#20840;&#24674;&#22797;&#38544;&#34255;&#30340;&#31192;&#23494;&#25968;&#25454;&#65292;&#32780;&#22522;&#20110;Flow&#30340;&#26041;&#27861;&#30001;&#20110;&#27599;&#20010;&#27169;&#22359;&#20013;&#20005;&#26684;&#30340;&#21487;&#36870;&#38480;&#21046;&#32780;&#20135;&#29983;&#36136;&#37327;&#36739;&#24046;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GS&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#29983;&#25104;&#24335;&#38544;&#20889;&#25193;&#25955;&#8221;&#65288;GSD&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21453;&#28436;&#25193;&#25955;&#27169;&#22411;&#8220;StegoDiffusion&#8221;&#26469;&#23454;&#29616;&#12290;&#23427;&#19981;&#20165;&#29983;&#25104;&#36924;&#30495;&#30340;&#38544;&#20889;&#22270;&#20687;&#65292;&#32780;&#19988;&#20801;&#35768;100&#65285;&#22320;&#24674;&#22797;&#38544;&#34255;&#30340;&#31192;&#23494;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;StegoDiffusion&#27169;&#22411;&#21033;&#29992;&#38750;&#39532;&#23572;&#21487;&#22827;&#38142;&#21644;&#24555;&#36895;&#37319;&#26679;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;&#30340;&#38544;&#20889;&#22270;&#20687;&#29983;&#25104;&#12290;&#36890;&#36807;&#26681;&#25454;StegoDiffusion&#20013;&#30340;&#29983;&#25104;&#36807;&#31243;&#30340;&#36716;&#31227;&#27010;&#29575;&#26500;&#36896;&#19968;&#20010;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65292;&#31192;&#23494;&#25968;&#25454;&#21644;&#38544;&#20889;&#20449;&#24687;&#21487;&#20197;&#34987;&#26377;&#25928;&#21644;&#21487;&#36870;&#22320;&#23884;&#20837;&#21040;&#38544;&#20889;&#22270;&#20687;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;GSD&#26041;&#26696;&#22312;&#38544;&#20889;&#22270;&#20687;&#36136;&#37327;&#21644;&#31192;&#23494;&#25968;&#25454;&#24674;&#22797;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative steganography (GS) is an emerging technique that generates stego images directly from secret data. Various GS methods based on GANs or Flow have been developed recently. However, existing GAN-based GS methods cannot completely recover the hidden secret data due to the lack of network invertibility, while Flow-based methods produce poor image quality due to the stringent reversibility restriction in each module. To address this issue, we propose a novel GS scheme called "Generative Steganography Diffusion" (GSD) by devising an invertible diffusion model named "StegoDiffusion". It not only generates realistic stego images but also allows for 100\% recovery of the hidden secret data. The proposed StegoDiffusion model leverages a non-Markov chain with a fast sampling technique to achieve efficient stego image generation. By constructing an ordinary differential equation (ODE) based on the transition probability of the generation process in StegoDiffusion, secret data and stego i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#20915;&#38750;&#32447;&#24615;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#65292;&#26080;&#38656;&#21021;&#22987;&#25968;&#25454;&#65292;&#36991;&#20813;&#37325;&#22797;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20316;&#20026;&#26377;&#38480;&#20803;&#31243;&#24207;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;&#35832;&#22810;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.06044</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38750;&#32447;&#24615;&#26412;&#26500;&#26448;&#26009;&#27169;&#22411;&#65306;COMM-PINN&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning solution of nonlinear constitutive material models using physics-informed neural networks: COMM-PINN. (arXiv:2304.06044v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06044
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#20915;&#38750;&#32447;&#24615;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#65292;&#26080;&#38656;&#21021;&#22987;&#25968;&#25454;&#65292;&#36991;&#20813;&#37325;&#22797;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20316;&#20026;&#26377;&#38480;&#20803;&#31243;&#24207;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;&#35832;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#12289;&#36335;&#24452;&#30456;&#20851;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#12290;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#19981;&#20165;&#28385;&#36275;&#25152;&#26377;&#28909;&#21147;&#23398;&#32422;&#26463;&#65292;&#32780;&#19988;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#21152;&#36733;&#24773;&#20917;&#19979;&#65292;&#31435;&#21363;&#25552;&#20379;&#20851;&#20110;&#24403;&#21069;&#26448;&#26009;&#29366;&#24577;&#65288;&#21363;&#33258;&#30001;&#33021;&#65292;&#24212;&#21147;&#21644;&#20869;&#37096;&#21464;&#37327;&#30340;&#28436;&#21464;&#65289;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#38656;&#35201;&#21021;&#22987;&#25968;&#25454;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#23427;&#35268;&#36991;&#20102;&#27714;&#35299;&#22797;&#26448;&#26009;&#27169;&#22411;&#20013;&#38750;&#32447;&#24615;&#26041;&#31243;&#25152;&#38656;&#30340;&#37325;&#22797;&#29275;&#39039;&#36845;&#20195;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20943;&#23569;&#33719;&#21462;&#20999;&#21521;&#31639;&#23376;&#25152;&#38656;&#30340;&#23548;&#25968;&#27425;&#24207;&#30340;&#31574;&#30053;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29992;&#20316;&#20219;&#20309;&#26377;&#38480;&#20803;&#31243;&#24207;&#65288;&#25110;&#20854;&#20182;&#25968;&#20540;&#26041;&#27861;&#65289;&#20013;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23450;&#20041;&#37197;&#28857;&#21644;&#25972;&#21512;&#21516;&#26102;&#28608;&#27963;&#25110;&#38750;&#28608;&#27963;&#30340;&#22810;&#20010;&#38750;&#30456;&#31561;&#32422;&#26463;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We applied physics-informed neural networks to solve the constitutive relations for nonlinear, path-dependent material behavior. As a result, the trained network not only satisfies all thermodynamic constraints but also instantly provides information about the current material state (i.e., free energy, stress, and the evolution of internal variables) under any given loading scenario without requiring initial data. One advantage of this work is that it bypasses the repetitive Newton iterations needed to solve nonlinear equations in complex material models. Additionally, strategies are provided to reduce the required order of derivation for obtaining the tangent operator. The trained model can be directly used in any finite element package (or other numerical methods) as a user-defined material model. However, challenges remain in the proper definition of collocation points and in integrating several non-equality constraints that become active or non-active simultaneously. We tested this
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#34987;&#25552;&#20986;&#29992;&#20110;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#34920;&#29616;&#21644;&#20808;&#36827;&#30340;&#24418;&#29366;/&#32441;&#29702;&#20559;&#24046;&#27979;&#35797;&#32467;&#26524;&#65292;&#33021;&#22815;&#25104;&#21151;&#25191;&#34892;&#23646;&#24615;&#32465;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.15233</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26159;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-Image Diffusion Models are Zero-Shot Classifiers. (arXiv:2303.15233v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15233
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#34987;&#25552;&#20986;&#29992;&#20110;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#34920;&#29616;&#21644;&#20808;&#36827;&#30340;&#24418;&#29366;/&#32441;&#29702;&#20559;&#24046;&#27979;&#35797;&#32467;&#26524;&#65292;&#33021;&#22815;&#25104;&#21151;&#25191;&#34892;&#23646;&#24615;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#20248;&#31168;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#23398;&#20064;&#20102;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#30340;&#20449;&#24687;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#25152;&#25429;&#25417;&#30340;&#30693;&#35782;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#65292;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23578;&#26410;&#36827;&#34892;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26681;&#25454;&#26631;&#31614;&#30340;&#25991;&#26412;&#25551;&#36848;&#21435;&#38500;&#22122;&#22768;&#22270;&#20687;&#30340;&#33021;&#21147;&#20316;&#20026;&#35813;&#26631;&#31614;&#27010;&#29575;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#31283;&#23450;&#25193;&#25955;&#21644;Imagen&#65292;&#24182;&#19982;CLIP&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#36827;&#34892;&#23545;&#27604;&#65292;&#25506;&#32034;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#12290;&#22312;&#24191;&#27867;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#20182;&#20204;&#19982;CLIP&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#22312;&#24418;&#29366;/&#32441;&#29702;&#20559;&#24046;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#25104;&#21151;&#25191;&#34892;&#23646;&#24615;&#32465;&#23450;&#65292;&#32780;CLIP&#19981;&#33021;&#12290;&#23613;&#31649;&#29983;&#25104;&#24615;&#39044;&#35757;&#32451;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24456;&#24120;&#35265;&#65292;v
&lt;/p&gt;
&lt;p&gt;
The excellent generative capabilities of text-to-image diffusion models suggest they learn informative representations of image-text data. However, what knowledge their representations capture is not fully understood, and they have not been thoroughly explored on downstream tasks. We investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers. The key idea is using a diffusion model's ability to denoise a noised image given a text description of a label as a proxy for that label's likelihood. We apply our method to Stable Diffusion and Imagen, using it to probe fine-grained aspects of the models' knowledge and comparing them with CLIP's zero-shot abilities. They perform competitively with CLIP on a wide range of zero-shot image classification datasets. Additionally, they achieve state-of-the-art results on shape/texture bias tests and can successfully perform attribute binding while CLIP cannot. Although generative pre-training is prevalent in NLP, v
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06053</link><description>&lt;p&gt;
TSMixer&#65306;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06053
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22810;&#21464;&#37327;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#12290;&#20026;&#20102;&#25429;&#33719;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#20687;&#24490;&#29615;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36825;&#26679;&#30340;&#39640;&#23481;&#37327;&#32467;&#26500;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#20010;&#24120;&#29992;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25193;&#23637;&#23427;&#20204;&#65292;&#26412;&#25991;&#30740;&#31350;&#32447;&#24615;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#24207;&#28151;&#21512;&#22120;&#65288;TSMixer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#12290; TSMixer&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#22312;&#27969;&#34892;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#31616;&#21333;&#26131;&#34892;&#30340;TSMixer&#19982;&#21033;&#29992;&#29305;&#23450;&#22522;&#20934;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22823;&#35268;&#27169;&#30340;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21363;&#19968;&#20010;&#23454;&#38469;&#30340;&#38646;&#21806;&#25968;&#25454;&#38598;&#19978;&#65292;TSMixer&#34920;&#29616;&#20986;&#38750;&#24120;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05382</link><description>&lt;p&gt;
ChatGPT&#24050;&#22312;&#22320;&#24179;&#32447;&#19978;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23601;&#26159;&#25105;&#20204;&#38656;&#35201;&#30340;&#26234;&#33021;&#20132;&#36890;&#35299;&#20915;&#26041;&#26696;&#65311;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?. (arXiv:2303.05382v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;&#20855;&#26377;60&#20159;&#21442;&#25968;&#30340;&#37325;&#35201;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#12290;ChatGPT&#23637;&#31034;&#20102;LLM&#30340;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#23545;&#35805;&#21709;&#24212;&#26041;&#38754;&#12290;&#38543;&#30528;LLM&#22312;&#21508;&#31181;&#30740;&#31350;&#25110;&#24037;&#31243;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#29616;&#22312;&#26159;&#26102;&#20505;&#35774;&#24819;LLM&#22914;&#20309;&#38761;&#26032;&#25105;&#20204;&#22788;&#29702;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#26041;&#24335;&#20102;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#20915;&#20851;&#38190;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#26234;&#33021;&#31995;&#32479;&#36824;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#36890;&#36807;LLM&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;LLM&#35013;&#22791;&#30340;&#36825;&#20123;&#28508;&#22312;&#30340;&#20132;&#36890;&#24212;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35777;&#26126;&#36825;&#31181;&#28508;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#12290;&#23613;&#31649;&#23384;&#22312;&#28508;&#22312;&#30340;&#30410;&#22788;&#65292;&#20294;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, developed by OpenAI, is one of the milestone large language models (LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive language understanding capability of LLM, particularly in generating conversational response. As LLMs start to gain more attention in various research or engineering domains, it is time to envision how LLM may revolutionize the way we approach intelligent transportation systems. This paper explores the future applications of LLM in addressing key transportation problems. By leveraging LLM with cross-modal encoder, an intelligent system can also process traffic data from different modalities and execute transportation operations through an LLM. We present and validate these potential transportation applications equipped by LLM. To further demonstrate this potential, we also provide a concrete smartphone-based crash report auto-generation and analysis framework as a use case. Despite the potential benefits, challenges related to data privac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20855;&#26377;&#27880;&#24847;&#34701;&#21512;&#30340;&#21160;&#24577;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#24378;&#26102;&#38388;&#29305;&#24449;&#32500;&#24230;&#30340;&#20132;&#20114;&#20316;&#29992;&#21644;&#25429;&#25417;&#22810;&#23610;&#24230;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#26377;&#25928;&#25429;&#25417;&#36828;&#36317;&#31163;&#12289;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#65292;&#23454;&#29616;&#31934;&#30830;&#19988;&#23454;&#26102;&#30340;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2302.12598</link><description>&lt;p&gt;
&#20855;&#26377;&#27880;&#24847;&#34701;&#21512;&#30340;&#21160;&#24577;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Convolutional Network with Attention Fusion for Traffic Flow Prediction. (arXiv:2302.12598v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12598
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20855;&#26377;&#27880;&#24847;&#34701;&#21512;&#30340;&#21160;&#24577;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#24378;&#26102;&#38388;&#29305;&#24449;&#32500;&#24230;&#30340;&#20132;&#20114;&#20316;&#29992;&#21644;&#25429;&#25417;&#22810;&#23610;&#24230;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#26377;&#25928;&#25429;&#25417;&#36828;&#36317;&#31163;&#12289;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#65292;&#23454;&#29616;&#31934;&#30830;&#19988;&#23454;&#26102;&#30340;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#23545;&#20110;&#22478;&#24066;&#20132;&#36890;&#25511;&#21046;&#21644;&#32593;&#32476;&#22320;&#22270;&#26381;&#21153;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#22312;&#22823;&#25968;&#25454;&#30340;&#25903;&#25345;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25429;&#25417;&#20132;&#36890;&#32593;&#32476;&#30340;&#22797;&#26434;&#26102;&#31354;&#27169;&#24335;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#22270;&#21644;&#31616;&#21333;&#30340;&#31354;&#38388;-&#26102;&#38388;&#32452;&#20214;&#65292;&#38590;&#20197;&#24314;&#27169;&#22810;&#23610;&#24230;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#27880;&#24847;&#34701;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#22686;&#24378;&#20102;&#26102;&#38388;&#29305;&#24449;&#32500;&#24230;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#28982;&#21518;&#23558;&#21160;&#24577;&#22270;&#23398;&#20064;&#22120;&#19982;GRU&#30456;&#32467;&#21512;&#65292;&#20849;&#21516;&#24314;&#27169;&#21516;&#27493;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#34701;&#20837;&#20102;&#31354;&#38388;-&#26102;&#38388;&#27880;&#24847;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#36828;&#36317;&#31163;&#12289;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and real-time traffic state prediction is of great practical importance for urban traffic control and web mapping services. With the support of massive data, deep learning methods have shown their powerful capability in capturing the complex spatialtemporal patterns of traffic networks. However, existing approaches use pre-defined graphs and a simple set of spatial-temporal components, making it difficult to model multi-scale spatial-temporal dependencies. In this paper, we propose a novel dynamic graph convolution network with attention fusion to tackle this gap. The method first enhances the interaction of temporal feature dimensions, and then it combines a dynamic graph learner with GRU to jointly model synchronous spatial-temporal correlations. We also incorporate spatial-temporal attention modules to effectively capture longrange, multifaceted domain spatial-temporal patterns. We conduct extensive experiments in four real-world traffic datasets to demonstrate that our met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#25163;&#21160;&#21162;&#21147;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#26159;&#26377;&#25928;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLMs&#32467;&#21512;&#20989;&#25968;&#30340;&#31614;&#21517;&#12289;&#23454;&#29616;&#21644;&#25991;&#26723;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#21487;&#20197;&#25104;&#21151;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25552;&#31034;&#27169;&#22411;&#26469;&#20462;&#22797;&#29983;&#25104;&#22833;&#36133;&#30340;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2302.06527</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#21270;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation. (arXiv:2302.06527v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#25163;&#21160;&#21162;&#21147;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#26159;&#26377;&#25928;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLMs&#32467;&#21512;&#20989;&#25968;&#30340;&#31614;&#21517;&#12289;&#23454;&#29616;&#21644;&#25991;&#26723;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#21487;&#20197;&#25104;&#21151;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25552;&#31034;&#27169;&#22411;&#26469;&#20462;&#22797;&#29983;&#25104;&#22833;&#36133;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20803;&#27979;&#35797;&#22312;&#30830;&#20445;&#36719;&#20214;&#27491;&#30830;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#21019;&#24314;&#21333;&#20803;&#27979;&#35797;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#65292;&#36825;&#20419;&#20351;&#33258;&#21160;&#21270;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#23545;&#29616;&#26377;&#27979;&#35797;&#26679;&#20363;&#30340;&#39069;&#22806;&#35757;&#32451;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#26412;&#25991;&#23545;LLMs&#22312;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#25163;&#21160;&#21162;&#21147;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#21270;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#35780;&#20272;&#65292;&#20026;LLM&#25552;&#20379;&#34987;&#27979;&#35797;&#20989;&#25968;&#30340;&#31614;&#21517;&#21644;&#23454;&#29616;&#20197;&#21450;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#20351;&#29992;&#31034;&#20363;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#36890;&#36807;&#37325;&#26032;&#25552;&#31034;&#27169;&#22411;&#20351;&#29992;&#22833;&#36133;&#30340;&#27979;&#35797;&#21644;&#38169;&#35823;&#28040;&#24687;&#26469;&#20462;&#22797;&#29983;&#25104;&#22833;&#36133;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#22312;JavaScript&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;TestPilot&#20316;&#20026;&#19968;&#20010;&#33258;&#21160;&#20026;npm&#36719;&#20214;&#21253;&#20013;&#30340;&#25152;&#26377;API&#20989;&#25968;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#30340;&#27979;&#35797;&#29983;&#25104;&#24037;&#20855;&#65292;&#24182;&#20351;&#29992;OpenAI&#30340;gpt3.5-turbo LLM&#22312;25&#20010;npm&#36719;&#20214;&#21253;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20849;&#35745;1,684&#20010;API&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to this problem, utilizing additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without additional training or manual effort, providing the LLM with the signature and implementation of the function under test, along with usage examples extracted from documentation. We also attempt to repair failed generated tests by re-prompting the model with the failing test and error message. We implement our approach in TestPilot, a test generation tool for JavaScript that automatically generates unit tests for all API functions in an npm package. We evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#28151;&#21512;&#24418;&#24335;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#29289;&#29702;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#31283;&#24577;&#28909;&#21147;&#23398;&#32806;&#21512;&#26041;&#31243;&#32452;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04954</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22810;&#20803;&#32032;&#32806;&#21512;&#31995;&#32479;&#21644;&#24322;&#36136;&#22495;&#30340;&#28151;&#21512;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
Mixed formulation of physics-informed neural networks for thermo-mechanically coupled systems and heterogeneous domains. (arXiv:2302.04954v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#28151;&#21512;&#24418;&#24335;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#29289;&#29702;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#31283;&#24577;&#28909;&#21147;&#23398;&#32806;&#21512;&#26041;&#31243;&#32452;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#36793;&#30028;&#20540;&#38382;&#39064;&#30340;&#26032;&#24037;&#20855;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;&#25511;&#21046;&#26041;&#31243;&#12289;&#36793;&#30028;&#26465;&#20214;&#21644;&#21021;&#22987;&#26465;&#20214;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#35774;&#35745;&#35768;&#22810;&#24037;&#31243;&#38382;&#39064;&#30340;&#25439;&#22833;&#20989;&#25968;&#26102;&#65292;&#20351;&#29992;&#19968;&#38454;&#23548;&#25968;&#24182;&#32467;&#21512;&#24378;&#24418;&#24335;&#21644;&#24369;&#24418;&#24335;&#30340;&#26041;&#31243;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#39046;&#22495;&#20013;&#23384;&#22312;&#24322;&#36136;&#24615;&#21644;&#21464;&#37327;&#36339;&#21464;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#34987;&#31216;&#20026;PINNs&#30340;&#28151;&#21512;&#24418;&#24335;&#65292;&#23427;&#20511;&#37492;&#20102;&#28151;&#21512;&#26377;&#38480;&#20803;&#26041;&#27861;&#30340;&#24605;&#24819;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;PDE&#34987;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#26041;&#31243;&#31995;&#32479;&#65292;&#20854;&#20013;&#20027;&#35201;&#26410;&#30693;&#37327;&#26159;&#35299;&#30340;&#36890;&#37327;&#25110;&#26799;&#24230;&#65292;&#32780;&#27425;&#35201;&#26410;&#30693;&#37327;&#26159;&#35299;&#26412;&#36523;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#28151;&#21512;&#24418;&#24335;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#29289;&#29702;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#31283;&#24577;&#28909;&#21147;&#23398;&#32806;&#21512;&#26041;&#31243;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) are a new tool for solving boundary value problems by defining loss functions of neural networks based on governing equations, boundary conditions, and initial conditions. Recent investigations have shown that when designing loss functions for many engineering problems, using first-order derivatives and combining equations from both strong and weak forms can lead to much better accuracy, especially when there are heterogeneity and variable jumps in the domain. This new approach is called the mixed formulation for PINNs, which takes ideas from the mixed finite element method. In this method, the PDE is reformulated as a system of equations where the primary unknowns are the fluxes or gradients of the solution, and the secondary unknowns are the solution itself. In this work, we propose applying the mixed formulation to solve multi-physical problems, specifically a stationary thermo-mechanically coupled system of equations. Additionally, we discus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;</title><link>http://arxiv.org/abs/2212.01071</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#39578;&#25200;&#34892;&#20026;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;&#36825;&#23548;&#33268;&#20102;&#34394;&#20551;&#26816;&#27979;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#20013;&#24341;&#20154;&#27880;&#30446;&#30340;&#39046;&#22495;&#12290;&#25968;&#25454;&#30340;&#22270;&#24418;&#29305;&#24615;&#20197;&#21450;&#22823;&#37327;&#33410;&#28857;&#23548;&#33268;&#20102;&#35768;&#22810;&#38556;&#30861;&#65292;&#21253;&#25324;&#30697;&#38453;&#20013;&#22823;&#37327;&#26080;&#20851;&#29305;&#24449;&#30340;&#39640;&#31163;&#25955;&#24230;&#21644;&#19981;&#24179;&#34913;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31639;&#27861;&#30340;&#32452;&#21512;&#65292;&#21363;SGAN&#12290;&#26412;&#25991;&#23558;&#23569;&#37327;&#26631;&#31614;&#24212;&#29992;&#20110;SGAN&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#34394;&#20551;&#36134;&#25143;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media grows faster, harassment becomes more prevalent which leads to considered fake detection a fascinating field among researchers. The graph nature of data with the large number of nodes caused different obstacles including a considerable amount of unrelated features in matrices as high dispersion and imbalance classes in the dataset. To deal with these issues Auto-encoders and a combination of semi-supervised learning and the GAN algorithm which is called SGAN were used. This paper is deploying a smaller number of labels and applying SGAN as a classifier. The result of this test showed that the accuracy had reached 91\% in detecting fake accounts using only 100 labeled samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#38544;&#31169;&#24863;&#30693;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;PC&#65288;FedPC&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#31574;&#30053;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2211.06919</link><description>&lt;p&gt;
&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#38754;&#21521;&#38544;&#31169;&#24863;&#30693;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Privacy-Aware Causal Structure Learning in Federated Setting. (arXiv:2211.06919v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#38544;&#31169;&#24863;&#30693;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;PC&#65288;FedPC&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#31574;&#30053;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#21508;&#31181;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#20026;&#20102;&#36798;&#21040;&#29702;&#24819;&#30340;&#24615;&#33021;&#65292;&#29616;&#26377;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#23558;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#20013;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#35774;&#32622;&#20013;&#65292;&#19981;&#21487;&#33021;&#23558;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#24182;&#25918;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#22312;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#21512;&#35774;&#32622;&#20013;&#30340;&#38544;&#31169;&#24863;&#30693;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;PC&#65288;FedPC&#65289;&#31639;&#27861;&#65292;&#37319;&#29992;&#20004;&#31181;&#26032;&#31574;&#30053;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#32780;&#19981;&#38598;&#20013;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36880;&#23618;&#32858;&#21512;&#31574;&#30053;&#65292;&#23558;PC&#31639;&#27861;&#24179;&#31283;&#22320;&#36866;&#24212;&#21040;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#20197;&#23454;&#29616;&#32852;&#21512;&#39592;&#26550;&#23398;&#20064;&#65292;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal structure learning has been extensively studied and widely used in machine learning and various applications. To achieve an ideal performance, existing causal structure learning algorithms often need to centralize a large amount of data from multiple data sources. However, in the privacy-preserving setting, it is impossible to centralize data from all sources and put them together as a single dataset. To preserve data privacy, federated learning as a new learning paradigm has attracted much attention in machine learning in recent years. In this paper, we study a privacy-aware causal structure learning problem in the federated setting and propose a novel Federated PC (FedPC) algorithm with two new strategies for preserving data privacy without centralizing data. Specifically, we first propose a novel layer-wise aggregation strategy for a seamless adaptation of the PC algorithm into the federated learning paradigm for federated skeleton learning, then we design an effective strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#23545;&#30340;&#26368;&#20248;&#35774;&#32622;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#30340;&#29305;&#27530;&#20043;&#22788;&#22312;&#20110;&#22823;&#37096;&#20998;&#21487;&#33021;&#30340;&#20132;&#26131;&#23545;&#20043;&#38388;&#30340;&#20132;&#26131;&#37327;&#26080;&#27861;&#30452;&#25509;&#35266;&#23519;&#65292;&#19988;&#38656;&#35201;&#28385;&#36275;&#36830;&#36890;&#24615;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2210.10971</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#23545;&#30340;&#26368;&#20248;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Optimal Settings for Cryptocurrency Trading Pairs. (arXiv:2210.10971v2 [q-fin.TR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#23545;&#30340;&#26368;&#20248;&#35774;&#32622;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#30340;&#29305;&#27530;&#20043;&#22788;&#22312;&#20110;&#22823;&#37096;&#20998;&#21487;&#33021;&#30340;&#20132;&#26131;&#23545;&#20043;&#38388;&#30340;&#20132;&#26131;&#37327;&#26080;&#27861;&#30452;&#25509;&#35266;&#23519;&#65292;&#19988;&#38656;&#35201;&#28385;&#36275;&#36830;&#36890;&#24615;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#30340;&#30446;&#26631;&#26159;&#21435;&#20013;&#24515;&#21270;&#65292;&#25152;&#26377;&#36135;&#24065;&#21407;&#21017;&#19978;&#22320;&#20301;&#30456;&#31561;&#12290;&#19982;&#20256;&#32479;&#32929;&#24066;&#19981;&#21516;&#65292;&#27809;&#26377;&#40664;&#35748;&#30340;&#36135;&#24065;&#21333;&#20301;&#65288;&#27861;&#24065;&#65289;&#65292;&#22240;&#27492;&#21487;&#20197;&#33258;&#30001;&#35774;&#32622;&#20132;&#26131;&#23545;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20004;&#31181;&#36135;&#24065;&#20043;&#38388;&#24314;&#31435;&#19968;&#20010;&#20132;&#26131;&#24066;&#22330;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#25511;&#21046;&#31649;&#29702;&#25104;&#26412;&#24182;&#30830;&#20445;&#36275;&#22815;&#30340;&#27969;&#21160;&#24615;&#65292;&#25105;&#20204;&#24517;&#39035;&#20248;&#20808;&#32771;&#34385;&#37027;&#20123;&#22823;&#37327;&#20132;&#26131;&#30340;&#20132;&#26131;&#23545;&#65292;&#24182;&#30830;&#20445;&#25152;&#26377;&#36135;&#24065;&#37117;&#26159;&#21487;&#20197;&#20132;&#26131;&#30340;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#36825;&#26159;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#29305;&#27530;&#20043;&#22788;&#22312;&#20110;&#65306;1&#65289;&#22823;&#37096;&#20998;&#65288;&gt;99.5%&#65289;&#21487;&#33021;&#30340;&#20132;&#26131;&#23545;&#20043;&#38388;&#30340;&#20132;&#26131;&#37327;&#26080;&#27861;&#30452;&#25509;&#35266;&#23519;&#12290;2&#65289;&#23427;&#28385;&#36275;&#36830;&#36890;&#24615;&#32422;&#26463;&#65292;&#21363;&#20445;&#35777;&#25152;&#26377;&#36135;&#24065;&#37117;&#21487;&#20197;&#20132;&#26131;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;1&#65289;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25130;&#26029;&#29305;&#24449;&#20540;&#20998;&#35299;&#22635;&#20805;&#32570;&#22833;&#20540;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#39033;&#29992;&#20110;&#25511;&#21046;&#32570;&#22833;&#20540;&#34987;&#38480;&#21046;&#20026;&#38646;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of cryptocurrencies is decentralization. In principle, all currencies have equal status. Unlike traditional stock markets, there is no default currency of denomination (fiat), thus the trading pairs can be set freely. However, it is impractical to set up a trading market between every two currencies. In order to control management costs and ensure sufficient liquidity, we must give priority to covering those large-volume trading pairs and ensure that all coins are reachable. We note that this is an optimization problem. Its particularity lies in: 1) the trading volume between most (&gt;99.5%) possible trading pairs cannot be directly observed. 2) It satisfies the connectivity constraint, that is, all currencies are guaranteed to be tradable.  To solve this problem, we use a two-stage process: 1) Fill in missing values based on a regularized, truncated eigenvalue decomposition, where the regularization term is used to control what extent missing values should be limited to zero. 2
&lt;/p&gt;</description></item><item><title>jsdp&#26159;&#19968;&#20010;Java&#24211;&#65292;&#36890;&#36807;&#21033;&#29992;Java&#20013;&#30340;lambda&#34920;&#36798;&#24335;&#12289;&#20989;&#25968;&#25509;&#21475;&#12289;&#38598;&#21512;&#21644;&#32858;&#21512;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#23545;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2209.09979</link><description>&lt;p&gt;
jsdp: &#19968;&#20010;Java&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
jsdp: a Java Stochastic Dynamic Programming Library. (arXiv:2209.09979v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09979
&lt;/p&gt;
&lt;p&gt;
jsdp&#26159;&#19968;&#20010;Java&#24211;&#65292;&#36890;&#36807;&#21033;&#29992;Java&#20013;&#30340;lambda&#34920;&#36798;&#24335;&#12289;&#20989;&#25968;&#25509;&#21475;&#12289;&#38598;&#21512;&#21644;&#32858;&#21512;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#23545;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#35268;&#21010;&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#21644;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26159;&#38543;&#26426;&#35268;&#21010;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#37319;&#29992;&#8220;&#20989;&#25968;&#26041;&#31243;&#8221;&#26041;&#27861;&#26469;&#21457;&#29616;&#26368;&#20248;&#31574;&#30053;&#12290;&#36890;&#36807;&#21033;&#29992;Java&#20013;&#23454;&#29616;&#30340;lambda&#34920;&#36798;&#24335;&#12289;&#20989;&#25968;&#25509;&#21475;&#12289;&#38598;&#21512;&#21644;&#32858;&#21512;&#36816;&#31639;&#31526;&#26469;&#25805;&#20316;MapReduce&#26694;&#26550;&#65292;jsdp&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24211;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Programming is a framework for modelling and solving problems of decision making under uncertainty. Stochastic Dynamic Programming is a branch of Stochastic Programming that takes a "functional equation" approach to the discovery of optimal policies. By leveraging constructs - lambda expressions, functional interfaces, collections and aggregate operators - implemented in Java to operationalise the MapReduce framework, jsdp provides a general purpose library for modelling and solving Stochastic Dynamic Programs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#26694;&#26550;&#35299;&#20915;&#22810;&#20934;&#21017;&#20915;&#31574;&#38382;&#39064;&#65292;&#22312;&#22242;&#20307;&#20915;&#31574;&#38382;&#39064;&#21644;&#20934;&#21017;&#30456;&#20851;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#32479;&#35745;&#20248;&#38597;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#24212;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#20915;&#31574;&#32773;&#20559;&#22909;&#65292;&#24320;&#21457;&#20102;&#35782;&#21035;&#20915;&#31574;&#32773;&#23376;&#32676;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#20934;&#21017;&#21644;&#22791;&#36873;&#26041;&#26696;&#30456;&#23545;&#37325;&#35201;&#24615;&#30340;&#27010;&#29575;&#25490;&#24207;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.13390</link><description>&lt;p&gt;
&#32479;&#19968;&#36125;&#21494;&#26031;&#26694;&#26550;&#29992;&#20110;&#22810;&#20934;&#21017;&#20915;&#31574;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Unified Bayesian Frameworks for Multi-criteria Decision-making Problems. (arXiv:2208.13390v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#26694;&#26550;&#35299;&#20915;&#22810;&#20934;&#21017;&#20915;&#31574;&#38382;&#39064;&#65292;&#22312;&#22242;&#20307;&#20915;&#31574;&#38382;&#39064;&#21644;&#20934;&#21017;&#30456;&#20851;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#32479;&#35745;&#20248;&#38597;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#24212;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#20915;&#31574;&#32773;&#20559;&#22909;&#65292;&#24320;&#21457;&#20102;&#35782;&#21035;&#20915;&#31574;&#32773;&#23376;&#32676;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#20934;&#21017;&#21644;&#22791;&#36873;&#26041;&#26696;&#30456;&#23545;&#37325;&#35201;&#24615;&#30340;&#27010;&#29575;&#25490;&#24207;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;&#20934;&#21017;&#20915;&#31574;&#38382;&#39064;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21033;&#29992;&#27010;&#29575;&#35299;&#37322;&#22810;&#20934;&#21017;&#20915;&#31574;&#26041;&#27861;&#21644;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#20026;&#22810;&#20934;&#21017;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#22914;&#22242;&#20307;&#20915;&#31574;&#38382;&#39064;&#21644;&#20934;&#21017;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#20102;&#32479;&#35745;&#20248;&#38597;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#20915;&#31574;&#32773;&#20559;&#22909;&#20013;&#21508;&#31181;&#24418;&#24335;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#27491;&#24577;&#20998;&#24067;&#12289;&#19977;&#35282;&#20998;&#24067;&#21644;&#21306;&#38388;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#22242;&#20307;&#22810;&#20934;&#21017;&#20915;&#31574;&#22330;&#26223;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#27010;&#29575;&#28151;&#21512;&#27169;&#22411;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#19968;&#33268;&#30340;&#20915;&#31574;&#32773;&#23376;&#32676;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#27010;&#29575;&#25490;&#24207;&#26041;&#26696;&#65292;&#26681;&#25454;&#20915;&#31574;&#32773;&#20559;&#22909;&#35780;&#20272;&#20934;&#21017;&#21644;&#22791;&#36873;&#26041;&#26696;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#25968;&#20540;&#31034;&#20363;&#19978;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Bayesian frameworks for tackling various aspects of multi-criteria decision-making (MCDM) problems, leveraging a probabilistic interpretation of MCDM methods and challenges. By harnessing the flexibility of Bayesian models, the proposed frameworks offer statistically elegant solutions to key challenges in MCDM, such as group decision-making problems and criteria correlation. Additionally, these models can accommodate diverse forms of uncertainty in decision makers' (DMs) preferences, including normal and triangular distributions, as well as interval preferences. To address large-scale group MCDM scenarios, a probabilistic mixture model is developed, enabling the identification of homogeneous subgroups of DMs. Furthermore, a probabilistic ranking scheme is devised to assess the relative importance of criteria and alternatives based on DM(s) preferences. Through experimentation on various numerical examples, the proposed frameworks are validated, demonstrating their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#21160;&#20316;&#25551;&#36848;&#25552;&#31034;&#65288;GAP&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21160;&#20316;&#30340;&#36523;&#20307;&#37096;&#20301;&#36816;&#21160;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#37319;&#29992;&#22810;&#27169;&#24577;&#35757;&#32451;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.05318</link><description>&lt;p&gt;
&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#21160;&#20316;&#25551;&#36848;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generative Action Description Prompts for Skeleton-based Action Recognition. (arXiv:2208.05318v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#21160;&#20316;&#25551;&#36848;&#25552;&#31034;&#65288;GAP&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21160;&#20316;&#30340;&#36523;&#20307;&#37096;&#20301;&#36816;&#21160;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#37319;&#29992;&#22810;&#27169;&#24577;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#21333;&#28909;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21160;&#20316;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#20363;&#22914;&#65292;&#8220;&#20570;&#32988;&#21033;&#25163;&#21183;&#8221;&#21644;&#8220;&#31446;&#36215;&#22823;&#25287;&#25351;&#8221;&#26159;&#25163;&#21183;&#30340;&#20004;&#31181;&#21160;&#20316;&#65292;&#20854;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#25163;&#37096;&#30340;&#36816;&#21160;&#12290;&#36825;&#20123;&#20449;&#24687;&#19982;&#21160;&#20316;&#31867;&#21035;&#30340;&#21333;&#28909;&#32534;&#30721;&#26080;&#20851;&#65292;&#20294;&#21487;&#20197;&#20174;&#21160;&#20316;&#25551;&#36848;&#20013;&#25581;&#31034;&#20986;&#26469;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#20013;&#21033;&#29992;&#21160;&#20316;&#25551;&#36848;&#21487;&#33021;&#26377;&#21161;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#21160;&#20316;&#25551;&#36848;&#25552;&#31034;&#65288;GAP&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#24341;&#25806;&#65292;&#33258;&#21160;&#29983;&#25104;&#21160;&#20316;&#30340;&#36523;&#20307;&#37096;&#20301;&#36816;&#21160;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35757;&#32451;&#26041;&#26696;&#65292;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skeleton-based action recognition has recently received considerable attention. Current approaches to skeleton-based action recognition are typically formulated as one-hot classification tasks and do not fully exploit the semantic relations between actions. For example, "make victory sign" and "thumb up" are two actions of hand gestures, whose major difference lies in the movement of hands. This information is agnostic from the categorical one-hot encoding of action classes but could be unveiled from the action description. Therefore, utilizing action description in training could potentially benefit representation learning. In this work, we propose a Generative Action-description Prompts (GAP) approach for skeleton-based action recognition. More specifically, we employ a pre-trained large-scale language model as the knowledge engine to automatically generate text descriptions for body parts movements of actions, and propose a multi-modal training scheme by utilizing the text encoder t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.07886</link><description>&lt;p&gt;
&#22522;&#20110;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07886
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#19981;&#26029;&#35775;&#38382;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#36807;&#31243;&#36890;&#24120;&#20250;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#65288;&#20363;&#22914;CPU&#65292;GPU&#65289;&#22312;&#20869;&#23384;&#21333;&#20803;&#21644;&#22788;&#29702;&#21333;&#20803;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#26041;&#38754;&#23384;&#22312;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#36825;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#33021;&#37327;&#21644;&#25191;&#34892;&#21608;&#26399;&#12290;&#20855;&#26377;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#21151;&#33021;&#30340;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20102;&#35299;&#29616;&#20195;&#36890;&#29992;PIM&#26550;&#26500;&#21152;&#36895;ML&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#65288;1&#65289;&#22312;&#23454;&#38469;&#36890;&#29992;PIM&#26550;&#26500;&#19978;&#23454;&#29616;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#20256;&#32479;ML&#31639;&#27861;&#65288;&#21363;&#32447;&#24615;&#22238;&#24402;&#65292;&#36923;&#36753;&#22238;&#24402;&#65292;&#20915;&#31574;&#26641;&#65292;K-Means&#32858;&#31867;&#65289;&#65292;&#65288;2&#65289;&#20005;&#26684;&#35780;&#20272;&#21644;&#34920;&#24449;&#36825;&#20123;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24615;&#33021;&#21644;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#65288;3&#65289;&#19982;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#30340;&#30456;&#24212;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#30456;&#24212;&#30340;CPU&#21644;GPU&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
&lt;/p&gt;</description></item><item><title>Neural-IMLS&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26410;&#23450;&#21521;&#30340;&#21407;&#22987;&#28857;&#20113;&#20013;&#23398;&#20064;&#25239;&#22122;&#22768;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#34920;&#38754;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2109.04398</link><description>&lt;p&gt;
Neural-IMLS: &#29992;&#20110;&#34920;&#38754;&#37325;&#24314;&#30340;&#33258;&#30417;&#30563;&#38544;&#24335;&#31227;&#21160;&#26368;&#23567;&#20108;&#20056;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Neural-IMLS: Self-supervised Implicit Moving Least-Squares Network for Surface Reconstruction. (arXiv:2109.04398v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04398
&lt;/p&gt;
&lt;p&gt;
Neural-IMLS&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26410;&#23450;&#21521;&#30340;&#21407;&#22987;&#28857;&#20113;&#20013;&#23398;&#20064;&#25239;&#22122;&#22768;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#36755;&#20837;&#30340;&#28857;&#20113;&#65292;&#29305;&#21035;&#26159;&#30495;&#23454;&#25195;&#25551;&#30340;&#28857;&#20113;&#23384;&#22312;&#22122;&#22768;&#19988;&#32570;&#20047;&#27861;&#32447;&#26102;&#65292;&#34920;&#38754;&#37325;&#24314;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#38544;&#24335;&#31227;&#21160;&#26368;&#23567;&#20108;&#20056;&#20989;&#25968;&#65288;IMLS&#65289;&#25552;&#20379;&#20102;&#24213;&#23618;&#34920;&#38754;&#30340;&#21452;&#37325;&#34920;&#31034;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;Neural-IMLS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#30452;&#25509;&#20174;&#26410;&#23450;&#21521;&#21407;&#22987;&#28857;&#20113;&#20013;&#33258;&#30417;&#30563;&#22320;&#23398;&#20064;&#25239;&#22122;&#22768;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;IMLS&#26469;&#27491;&#21017;&#21270;MLP&#25253;&#21578;&#30340;&#36317;&#31163;&#20540;&#65292;&#21516;&#26102;&#20351;&#29992;MLP&#26469;&#27491;&#21017;&#21270;&#25968;&#25454;&#28857;&#30340;&#27861;&#32447;&#20197;&#36816;&#34892;IMLS&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#22312;&#25910;&#25947;&#26102;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;MLP&#21644;IMLS&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#26426;&#21046;&#65292;&#33021;&#22815;&#20135;&#29983;&#19968;&#20010;&#24544;&#23454;&#30340;SDF&#65292;&#20854;&#38646;&#32423;&#38598;&#36817;&#20284;&#20110;&#24213;&#23618;&#34920;&#38754;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;&#21512;&#25104;&#25195;&#25551;&#21644;&#30495;&#23454;&#25195;&#25551;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Neural-IMLS&#21487;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surface reconstruction is very challenging when the input point clouds, particularly real scans, are noisy and lack normals. Observing that the Multilayer Perceptron (MLP) and the implicit moving least-square function (IMLS) provide a dual representation of the underlying surface, we introduce Neural-IMLS, a novel approach that directly learns the noise-resistant signed distance function (SDF) from unoriented raw point clouds in a self-supervised fashion. We use the IMLS to regularize the distance values reported by the MLP while using the MLP to regularize the normals of the data points for running the IMLS. We also prove that at the convergence, our neural network, benefiting from the mutual learning mechanism between the MLP and the IMLS, produces a faithful SDF whose zero-level set approximates the underlying surface. We conducted extensive experiments on various benchmarks, including synthetic scans and real scans. The experimental results show that {\em Neural-IMLS} can reconstru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#31639;&#27861;&#34892;&#20026;&#30340;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#22270;&#34920;&#31034;&#26469;&#25552;&#20379;&#22240;&#26524;&#35299;&#37322;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#21363;&#35299;&#37322;&#21333;&#20803;&#26356;&#21152;&#21487;&#35299;&#37322;&#19988;&#32771;&#34385;&#20102;&#23439;&#35266;&#32423;&#29305;&#24449;&#21644;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#12290;</title><link>http://arxiv.org/abs/2006.02482</link><description>&lt;p&gt;
&#29992;&#22240;&#26524;&#23398;&#20064;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#31639;&#27861;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Explaining the Behavior of Black-Box Prediction Algorithms with Causal Learning. (arXiv:2006.02482v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.02482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#31639;&#27861;&#34892;&#20026;&#30340;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#22270;&#34920;&#31034;&#26469;&#25552;&#20379;&#22240;&#26524;&#35299;&#37322;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#21363;&#35299;&#37322;&#21333;&#20803;&#26356;&#21152;&#21487;&#35299;&#37322;&#19988;&#32771;&#34385;&#20102;&#23439;&#35266;&#32423;&#29305;&#24449;&#21644;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#23398;&#26041;&#27861;&#22312;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#22522;&#20110;&#22270;&#20687;&#20687;&#32032;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#32570;&#28857;&#65306;&#65288;i&#65289;&#8220;&#35299;&#37322;&#21333;&#20803;&#8221;&#26159;&#30456;&#20851;&#39044;&#27979;&#27169;&#22411;&#30340;&#24494;&#35266;&#32423;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#20687;&#32032;&#65292;&#32780;&#19981;&#26159;&#26356;&#26377;&#29992;&#20110;&#29702;&#35299;&#22914;&#20309;&#21487;&#33021;&#25913;&#21464;&#31639;&#27861;&#34892;&#20026;&#30340;&#21487;&#35299;&#37322;&#30340;&#23439;&#35266;&#32423;&#29305;&#24449;&#65307;&#65288;ii&#65289;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#29305;&#24449;&#19982;&#30446;&#26631;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#19981;&#23384;&#22312;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#65292;&#36825;&#22312;&#35299;&#37322;&#21333;&#20803;&#26159;&#23439;&#35266;&#32423;&#21464;&#37327;&#26102;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#22312;&#20998;&#26512;&#20154;&#21592;&#26080;&#27861;&#35775;&#38382;&#30446;&#26631;&#39044;&#27979;&#31639;&#27861;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#30340;&#37325;&#35201;&#24773;&#20917;&#65292;&#32780;&#21482;&#33021;&#26681;&#25454;&#29305;&#23450;&#36755;&#20837;&#26597;&#35810;&#27169;&#22411;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#22240;&#26524;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#20801;&#35768;&#26356;&#22909;&#22320;&#29702;&#35299;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal approaches to post-hoc explainability for black-box prediction models (e.g., deep neural networks trained on image pixel data) have become increasingly popular. However, existing approaches have two important shortcomings: (i) the "explanatory units" are micro-level inputs into the relevant prediction model, e.g., image pixels, rather than interpretable macro-level features that are more useful for understanding how to possibly change the algorithm's behavior, and (ii) existing approaches assume there exists no unmeasured confounding between features and target model predictions, which fails to hold when the explanatory units are macro-level variables. Our focus is on the important setting where the analyst has no access to the inner workings of the target prediction algorithm, rather only the ability to query the output of the model in response to a particular input. To provide causal explanations in such a setting, we propose to learn causal graphical representations that allo
&lt;/p&gt;</description></item></channel></rss>