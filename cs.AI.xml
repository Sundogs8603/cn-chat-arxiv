<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01588</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#32422;&#26463;&#23431;&#23449;&#23398;&#21442;&#25968;&#30340;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#25688;&#35201;&#32479;&#35745;&#37327;&#65288;&#22914;&#21151;&#29575;&#35889;&#65289;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20174;&#22797;&#26434;&#23431;&#23449;&#23398;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#27169;&#25311;&#22871;&#20214;&#20013;&#30340;&#23376;&#32593;&#26684;&#29289;&#29702;&#23454;&#29616;&#21644;&#25968;&#20540;&#36924;&#36817;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#19968;&#20010;&#23431;&#23449;&#23398;&#27169;&#25311;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#21518;&#65292;&#22312;&#21478;&#19968;&#20010;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20250;&#19979;&#38477;&#12290;&#21516;&#26679;&#65292;&#23545;&#20110;&#20219;&#20309;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#24212;&#29992;&#20110;&#35266;&#27979;&#25968;&#25454;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#19981;&#21516;&#22871;&#20214;&#30340;CAMELS&#27700;&#21160;&#21147;&#23431;&#23449;&#23398;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DA-GNNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#25429;&#25417;&#26469;&#33258;&#26143;&#31995;&#20998;&#24067;&#30340;&#32467;&#26500;&#26080;&#26631;&#24230;&#23431;&#23449;&#23398;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21253;&#25324;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#36866;&#37197;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20004;&#20010;&#27169;&#25311;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have been shown to outperform methods that rely on summary statistics, like the power spectrum, in extracting information from complex cosmological data sets. However, due to differences in the subgrid physics implementation and numerical approximations across different simulation suites, models trained on data from one cosmological simulation show a drop in performance when tested on another. Similarly, models trained on any of the simulations would also likely experience a drop in performance when applied to observational data. Training on data from two different suites of the CAMELS hydrodynamic cosmological simulations, we examine the generalization capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing GNNs, we capitalize on their capacity to capture structured scale-free cosmological information from galaxy distributions. Moreover, by including unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable our models to ex
&lt;/p&gt;</description></item><item><title>FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01483</link><description>&lt;p&gt;
FedSN&#65306;&#19968;&#20010;&#36866;&#29992;&#20110;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01483
&lt;/p&gt;
&lt;p&gt;
FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#24050;&#32463;&#30001;&#21830;&#19994;&#20844;&#21496;&#25104;&#21151;&#22320;&#21457;&#23556;&#21644;&#37096;&#32626;&#21040;&#22826;&#31354;&#20013;&#65292;&#22914;SpaceX&#12290;&#30001;&#20110;LEO&#21355;&#26143;&#37197;&#22791;&#20102;&#22810;&#27169;&#20256;&#24863;&#22120;&#65292;&#23427;&#20204;&#19981;&#20165;&#29992;&#20110;&#36890;&#20449;&#65292;&#36824;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#31354;&#38388;&#35843;&#21046;&#35782;&#21035;&#12289;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19982;LEO&#21355;&#26143;&#30340;&#26377;&#38480;&#25509;&#35302;&#26102;&#38388;&#65288;&#20363;&#22914;5&#20998;&#38047;&#65289;&#65292;&#22320;&#38754;&#31449;&#65288;GS&#65289;&#21487;&#33021;&#26080;&#27861;&#19979;&#36733;&#22914;&#27492;&#22823;&#37327;&#30340;&#21407;&#22987;&#24863;&#27979;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35201;&#22312;LEO&#21355;&#26143;&#19978;&#20351;&#29992;FL&#65292;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;i&#65289;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#65292;ii&#65289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#65292;&#20197;&#21450;iii&#65289;&#27169;&#22411;&#38472;&#26087;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSN&#30340;&#36890;&#29992;FL&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#19968;
&lt;/p&gt;
&lt;p&gt;
Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#23454;&#38469;&#31639;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#32469;&#36807;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.12036</link><description>&lt;p&gt;
&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#29702;&#35299;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#19968;&#33324;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A General Theoretical Paradigm to Understand Learning from Human Preferences. (arXiv:2310.12036v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#23454;&#38469;&#31639;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#32469;&#36807;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#20381;&#36182;&#20110;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#65306;&#31532;&#19968;&#20551;&#35774;&#21487;&#20197;&#29992;&#36880;&#28857;&#22870;&#21169;&#26367;&#20195;&#25104;&#23545;&#20559;&#22909;&#12290;&#31532;&#20108;&#20010;&#20551;&#35774;&#26159;&#22312;&#36825;&#20123;&#36880;&#28857;&#22870;&#21169;&#19978;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#20174;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#27867;&#21270;&#21040;&#31574;&#30053;&#37319;&#26679;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32469;&#36807;&#20102;&#31532;&#20108;&#20010;&#36817;&#20284;&#65292;&#24182;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#31532;&#19968;&#20010;&#36817;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23545;&#36825;&#20123;&#23454;&#38469;&#31639;&#27861;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#31216;&#20026;&#936;PO&#65292;&#29992;&#20110;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#35813;&#30446;&#26631;&#20197;&#25104;&#23545;&#20559;&#22909;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#22240;&#27492;&#32469;&#36807;&#20102;&#36825;&#20004;&#20010;&#36817;&#20284;&#12290;&#36825;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#31181;&#26032;&#30340;&#20174;&#35757;&#32451;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#31574;&#30053;&#30340;&#26041;&#27861;&#32780;&#26080;&#38656;&#36827;&#34892;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.  In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2310.09886</link><description>&lt;p&gt;
&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation. (arXiv:2310.09886v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09886
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#65288;LSG&#65289;&#26159;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#35753;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#65292;&#20197;&#19981;&#26029;&#23398;&#20064;&#26032;&#30340;&#29983;&#25104;&#27169;&#24335;&#24182;&#36991;&#20813;&#36951;&#24536;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;LSG&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#32500;&#25345;&#26087;&#30693;&#35782;&#65292;&#32780;&#23545;&#36328;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#20851;&#27880;&#36739;&#23569;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#33719;&#21462;&#30340;&#31867;&#20284;&#20219;&#21153;&#30340;&#30693;&#35782;&#26356;&#22909;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#21463;&#20154;&#31867;&#23398;&#20064;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#65288;DMEA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#30830;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#20419;&#36827;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23398;&#20064;&#36807;&#31243;&#24456;&#23481;&#26131;&#20559;&#21521;&#20110;&#24403;&#21069;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model to dynamically determine the architecture for acquiring new knowledge based on task correlation and select the most similar previous tasks to facilitate adaptation to new tasks. In addition, as the learning process can easily be biased towards the current task which might cause more severe forgetting of previously learned knowledge, we propose dynamic gradient scaling to balance the lea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2310.01012</link><description>&lt;p&gt;
CCA&#23478;&#26063;&#30340;&#39640;&#25928;&#31639;&#27861;&#65306;&#26080;&#32422;&#26463;&#30446;&#26631;&#19982;&#26080;&#20559;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#32422;&#26463;&#30446;&#26631;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21040;CCA&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26041;&#27861;&#22312;&#22810;&#35270;&#35282;&#23398;&#20064;&#20013;&#20855;&#26377;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#27491;&#21017;&#21270;&#32447;&#24615;CCA&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#30340;&#25512;&#24191;&#65292;&#24182;&#19982;&#24191;&#20041;&#29305;&#24449;&#20540;&#38382;&#39064;&#65288;GEP&#65289;&#26694;&#26550;&#32479;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#24615;&#26041;&#27861;&#30340;&#20256;&#32479;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#28145;&#24230;CCA&#30340;&#25193;&#23637;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#35757;&#32451;&#36807;&#31243;&#32531;&#24930;&#19988;&#22797;&#26434;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#25551;&#36848;GEPs&#30340;&#39030;&#32423;&#23376;&#31354;&#38388;&#30340;&#26032;&#39062;&#26080;&#32422;&#26463;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#36129;&#29486;&#26159;&#19968;&#31995;&#21015;&#24555;&#36895;&#31639;&#27861;&#65292;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#24212;&#29992;&#20110;&#30456;&#24212;&#30340;CCA&#30446;&#26631;&#65292;&#20174;&#32780;&#33719;&#24471;&#38543;&#26426;PLS&#12289;&#38543;&#26426;CCA&#21644;&#28145;&#24230;CCA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#25152;&#26377;&#26631;&#20934;CCA&#21644;&#28145;&#24230;CCA&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#24674;&#22797;&#12290;&#36825;&#26679;&#30340;&#36895;&#24230;&#20351;&#25105;&#20204;&#33021;&#22815;&#39318;&#27425;&#36827;&#34892;&#22823;&#35268;&#27169;&#29983;&#29289;&#25968;&#25454;&#30340;PLS&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Canonical Correlation Analysis (CCA) family of methods is foundational in multi-view learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data. Extensions to Deep CCA show great promise, but current training procedures are slow and complicated. First we propose a novel unconstrained objective that characterizes the top subspace of GEPs. Our core contribution is a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives. These methods show far faster convergence and recover higher correlations than the previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows us to perform a first-of-its-kind PLS analysis of an extremely large bio
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#31867;&#28608;&#27963;&#22270;&#25216;&#26415;&#65292;USL-Net&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30382;&#32932;&#30149;&#21464;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.13289</link><description>&lt;p&gt;
USL-Net&#65306;&#29992;&#20110;&#26080;&#30417;&#30563;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion Segmentation. (arXiv:2309.13289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13289
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#31867;&#28608;&#27963;&#22270;&#25216;&#26415;&#65292;USL-Net&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30382;&#32932;&#30149;&#21464;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#20855;&#26377;&#22810;&#31181;&#22909;&#22788;&#65292;&#21253;&#25324;&#33410;&#32422;&#19987;&#23478;&#20154;&#21147;&#36164;&#28304;&#12289;&#20943;&#23569;&#20027;&#35266;&#20154;&#24037;&#26631;&#27880;&#24341;&#36215;&#30340;&#24046;&#24322;&#20197;&#21450;&#36866;&#24212;&#26032;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#20998;&#21106;&#30382;&#32932;&#38236;&#22270;&#20687;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#65292;&#22914;&#27611;&#21457;&#22122;&#22768;&#12289;&#27700;&#30129;&#22122;&#22768;&#21644;&#32454;&#24494;&#36793;&#32536;&#24046;&#24322;&#31561;&#30382;&#32932;&#38236;&#22270;&#20687;&#20266;&#24433;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#23398;&#20064;&#32593;&#32476;&#65288;USL-Net&#65289;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#12290;USL-Net&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30149;&#21464;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#29983;&#25104;&#31867;&#28608;&#27963;&#22270;&#65288;CAMs&#65289;&#20316;&#20026;&#26174;&#33879;&#22270;&#12290;&#19981;&#21516;&#30340;CAM&#20301;&#32622;&#23545;&#24212;&#20110;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#30149;&#21464;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#12290;&#22320;&#22270;&#20013;&#30340;&#39640;&#26174;&#33879;&#21306;&#22495;&#29992;&#20316;&#30149;&#21464;&#21306;&#22495;&#30340;&#20266;&#26631;&#31614;&#65292;&#32780;&#20302;&#26174;&#33879;&#21306;&#22495;&#29992;&#20316;&#38750;&#30149;&#21464;&#21306;&#22495;&#30340;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised skin lesion segmentation offers several benefits, including conserving expert human resources, reducing discrepancies due to subjective human labeling, and adapting to novel environments. However, segmenting dermoscopic images without manual labeling guidance presents significant challenges due to dermoscopic image artifacts such as hair noise, blister noise, and subtle edge differences. To address these challenges, we introduce an innovative Uncertainty Self-Learning Network (USL-Net) designed for skin lesion segmentation. The USL-Net can effectively segment a range of lesions, eliminating the need for manual labeling guidance. Initially, features are extracted using contrastive learning, followed by the generation of Class Activation Maps (CAMs) as saliency maps using these features. The different CAM locations correspond to the importance of the lesion region based on their saliency. High-saliency regions in the map serve as pseudo-labels for lesion regions while low-sa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.08549</link><description>&lt;p&gt;
&#22522;&#20110;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#30340;&#35757;&#32451;&#26469;&#25269;&#24481;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#26469;&#38450;&#27490;&#26469;&#33258;&#19981;&#21487;&#20449;&#25968;&#25454;&#28304;&#30340;&#28508;&#22312;&#27745;&#26579;&#25915;&#20987;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#65292;&#36825;&#32473;&#20102;&#25915;&#20987;&#32773;&#35768;&#22810;&#21487;&#21033;&#29992;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#26377;&#25928;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#22914;&#20960;&#31181;&#20043;&#21069;&#30340;&#26041;&#27861;&#20013;&#37027;&#26679;&#21521;&#25152;&#26377;&#31034;&#20363;&#28155;&#21152;&#22122;&#22768;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#30340;&#23454;&#38469;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#26368;&#26032;&#25915;&#20987;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;H
&lt;/p&gt;
&lt;p&gt;
While numerous defense methods have been proposed to prohibit potential poisoning attacks from untrusted data sources, most research works only defend against specific attacks, which leaves many avenues for an adversary to exploit. In this work, we propose an efficient and robust training approach to defend against data poisoning attacks based on influence functions, named Healthy Influential-Noise based Training. Using influence functions, we craft healthy noise that helps to harden the classification model against poisoning attacks without significantly affecting the generalization ability on test data. In addition, our method can perform effectively when only a subset of the training data is modified, instead of the current method of adding noise to all examples that has been used in several previous works. We conduct comprehensive evaluations over two image datasets with state-of-the-art poisoning attacks under different realistic attack scenarios. Our empirical results show that H
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BEVTrack&#30340;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#22522;&#32447;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#65292;BEVTrack&#33021;&#22815;&#32534;&#30721;&#31354;&#38388;&#37051;&#36817;&#24615;&#21644;&#25429;&#25417;&#36816;&#21160;&#32447;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36319;&#36394;&#12290;</title><link>http://arxiv.org/abs/2309.02185</link><description>&lt;p&gt;
BEVTrack&#65306;&#19968;&#31181;&#38024;&#23545;&#40479;&#30640;&#22270;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#31616;&#21333;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
BEVTrack: A Simple Baseline for 3D Single Object Tracking in Birds's-Eye-View. (arXiv:2309.02185v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BEVTrack&#30340;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#22522;&#32447;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#65292;BEVTrack&#33021;&#22815;&#32534;&#30721;&#31354;&#38388;&#37051;&#36817;&#24615;&#21644;&#25429;&#25417;&#36816;&#21160;&#32447;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22806;&#35266;&#21464;&#21270;&#12289;&#24178;&#25200;&#29289;&#21644;&#28857;&#20113;&#30340;&#39640;&#31232;&#30095;&#24615;&#65292;&#28857;&#20113;&#20013;&#30340;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#29289;&#20307;&#36890;&#24120;&#22312;&#36830;&#32493;&#24103;&#20013;&#20445;&#25345;&#31354;&#38388;&#37051;&#36817;&#24615;&#65292;&#20027;&#35201;&#27700;&#24179;&#31227;&#21160;&#12290;&#36825;&#31181;&#31354;&#38388;&#36830;&#32493;&#24615;&#20026;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36319;&#36394;&#22120;&#36890;&#24120;&#37319;&#29992;&#28857;&#32423;&#34920;&#31034;&#65292;&#38590;&#20197;&#26377;&#25928;&#21033;&#29992;&#36825;&#31181;&#30693;&#35782;&#65292;&#22240;&#20026;&#36825;&#31181;&#34920;&#31034;&#30340;&#19981;&#35268;&#21017;&#26684;&#24335;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#24182;&#35299;&#20915;&#22810;&#20010;&#23376;&#20219;&#21153;&#26469;&#24314;&#31435;&#31354;&#38388;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;BEVTrack&#65292;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#29992;&#20110;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#22522;&#32447;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#36830;&#32493;&#30340;&#28857;&#20113;&#36716;&#25442;&#20026;&#24120;&#35265;&#30340;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;BEVTrack&#36890;&#36807;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#31354;&#38388;&#37051;&#36817;&#24615;&#65292;&#24182;&#28789;&#27963;&#22320;&#25429;&#25417;&#20102;&#36319;&#36394;&#30340;&#36816;&#21160;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D single object tracking (SOT) in point clouds is still a challenging problem due to appearance variation, distractors, and high sparsity of point clouds. Notably, in autonomous driving scenarios, the target object typically maintains spatial adjacency across consecutive frames, predominantly moving horizontally. This spatial continuity offers valuable prior knowledge for target localization. However, existing trackers, which often employ point-wise representations, struggle to efficiently utilize this knowledge owing to the irregular format of such representations. Consequently, they require elaborate designs and solving multiple subtasks to establish spatial correspondence. In this paper, we introduce BEVTrack, a simple yet strong baseline framework for 3D SOT. After converting consecutive point clouds into the common Bird's-Eye-View representation, BEVTrack inherently encodes spatial proximity and adeptly captures motion cues for tracking via a simple element-wise operation and con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11494</link><description>&lt;p&gt;
&#39044;&#27979;&#12289;&#25913;&#36827;&#12289;&#21512;&#25104;&#65306;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#39044;&#27979;&#25110;&#22635;&#34917;&#20219;&#21153;&#30340;&#26465;&#20214;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38754;&#21521;&#22810;&#31181;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TSDiff&#65292;&#19968;&#31181;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#26080;&#26465;&#20214;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#33258;&#24341;&#23548;&#26426;&#21046;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#24471;TSDiff&#33021;&#22815;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#26465;&#20214;&#35774;&#32622;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#32593;&#32476;&#25110;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;TSDiff&#19982;&#20960;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26465;&#20214;&#39044;&#27979;&#26041;&#27861;&#30456;&#31454;&#20105;&#65288;&#39044;&#27979;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;TSDiff&#23398;&#21040;&#30340;&#38544;&#24615;&#27010;&#29575;&#23494;&#24230;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;p
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#30340;&#33258;&#21160;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#21644;&#19987;&#38376;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08430</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#36890;&#36807;&#28176;&#36827;&#25277;&#26679;&#36827;&#34892;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Long-range Meta-path Search through Progressive Sampling on Large-scale Heterogeneous Information Networks. (arXiv:2307.08430v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#30340;&#33258;&#21160;&#26694;&#26550;&#12290;&#36890;&#36807;&#21160;&#24577;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#21644;&#19987;&#38376;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31243;&#20381;&#36182;&#30340;&#21033;&#29992;&#22312;&#21516;&#36136;&#22270;&#20013;&#26377;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#24456;&#23569;&#30740;&#31350;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#39640;&#25104;&#26412;&#21644;&#26377;&#25928;&#20449;&#24687;&#21033;&#29992;&#30340;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#20803;&#36335;&#24452;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#21033;&#29992;&#38271;&#31243;&#20381;&#36182;&#30340;&#33258;&#21160;&#26694;&#26550;&#65292;&#31216;&#20026;&#38271;&#31243;&#20803;&#36335;&#24452;&#25628;&#32034;&#21644;&#28176;&#36827;&#25277;&#26679;&#65288;LMSPS&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#21508;&#31181;&#25968;&#25454;&#38598;&#25110;&#20219;&#21153;&#30340;&#20803;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#25152;&#26377;&#30446;&#26631;&#33410;&#28857;&#30456;&#20851;&#20803;&#36335;&#24452;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#36890;&#36807;&#28176;&#36827;&#25277;&#26679;&#31639;&#27861;&#65292;&#25105;&#20204;&#21160;&#24577;&#22320;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#20197;&#36339;&#25968;&#26080;&#20851;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#39537;&#21160;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#30001;&#24403;&#21069;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#21644;&#20219;&#21153;&#39537;&#21160;&#30340;&#32039;&#20945;&#25628;&#32034;&#31354;&#38388;&#12290;&#21033;&#29992;&#25277;&#26679;&#35780;&#20272;&#31574;&#30053;&#20316;&#20026;&#25351;&#23548;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19987;&#38376;&#21644;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#20803;&#36335;&#24452;&#36873;&#25321;&#12290;&#23545;&#20843;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LMSPS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing long-range dependency, though extensively studied in homogeneous graphs, is rarely studied in large-scale heterogeneous information networks (HINs), whose main challenge is the high costs and the difficulty in utilizing effective information. To this end, we investigate the importance of different meta-paths and propose an automatic framework for utilizing long-range dependency in HINs, called Long-range Meta-path Search through Progressive Sampling (LMSPS). Specifically, to discover meta-paths for various datasets or tasks without prior, we develop a search space with all target-node-related meta-paths. With a progressive sampling algorithm, we dynamically shrink the search space with hop-independent time complexity, leading to a compact search space driven by the current HIN and task. Utilizing a sampling evaluation strategy as the guidance, we conduct a specialized and expressive meta-path selection. Extensive experiments on eight heterogeneous datasets demonstrate that LM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#36328;&#39046;&#22495;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#31243;&#24207;&#21644;&#39537;&#21160;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.03817</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#25551;&#36848;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#24320;&#21457;&#21644;&#35843;&#35797;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Exploring and Characterizing Large Language Models For Embedded System Development and Debugging. (arXiv:2307.03817v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#36328;&#39046;&#22495;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#31243;&#24207;&#21644;&#39537;&#21160;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#26174;&#30528;&#33021;&#21147;&#65292;&#28982;&#32780;&#23427;&#20204;&#23545;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#24320;&#21457;&#25152;&#38656;&#30340;&#36328;&#39046;&#22495;&#30828;&#20214;&#21644;&#36719;&#20214;&#30693;&#35782;&#30340;&#33021;&#21147;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#65288;GPT-3.5&#12289;GPT-4&#12289;PaLM 2&#65289;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20154;&#31867;&#31243;&#24207;&#21592;&#22914;&#20309;&#19982;&#36825;&#20123;&#24037;&#20855;&#20132;&#20114;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#36719;&#20214;&#24037;&#31243;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#30828;&#20214;&#22312;&#22238;&#36335;&#35780;&#20272;&#24179;&#21488;&#65292;&#29992;&#20110;&#39564;&#35777;LLM&#29983;&#25104;&#30340;&#31243;&#24207;&#65292;&#20351;&#29992;&#20256;&#24863;&#22120;&#25191;&#34892;&#22120;&#23545;&#12290;&#25105;&#20204;&#36890;&#36807;N=450&#20010;&#23454;&#39564;&#27604;&#36739;&#20102;&#25152;&#26377;&#19977;&#20010;&#27169;&#22411;&#65292;&#20196;&#20154;&#24778;&#35766;&#22320;&#21457;&#29616;GPT-4&#29305;&#21035;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#36328;&#39046;&#22495;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20174;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#23436;&#20840;&#27491;&#30830;&#30340;&#31243;&#24207;&#12290;&#22312;N=50&#20010;&#35797;&#39564;&#20013;&#65292;GPT-4&#30340;I2C&#25509;&#21475;&#21151;&#33021;&#36798;&#21040;&#20102;66%&#12290;GPT-4&#36824;&#29983;&#25104;&#20102;&#23492;&#23384;&#22120;&#32423;&#39537;&#21160;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable abilities to generate code, however their ability to develop software for embedded systems, which requires cross-domain knowledge of hardware and software has not been studied. In this paper we systematically evaluate leading LLMs (GPT-3.5, GPT-4, PaLM 2) to assess their performance for embedded system development, study how human programmers interact with these tools, and develop an AI-based software engineering workflow for building embedded systems.  We develop an an end-to-end hardware-in-the-loop evaluation platform for verifying LLM generated programs using sensor actuator pairs. We compare all three models with N=450 experiments and find surprisingly that GPT-4 especially shows an exceptional level of cross-domain understanding and reasoning, in some cases generating fully correct programs from a single prompt. In N=50 trials, GPT-4 produces functional I2C interfaces 66% of the time. GPT-4 also produces register-level drivers, c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.06202</link><description>&lt;p&gt;
NeuroGraph:&#38754;&#21521;&#33041;&#36830;&#25509;&#32452;&#23398;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20026;&#20998;&#26512;&#39640;&#32500;&#21151;&#33021;&#24615;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#39044;&#27979;&#21508;&#31181;&#31070;&#32463;&#30142;&#30149;&#12289;&#31934;&#31070;&#38556;&#30861;&#21644;&#35748;&#30693;&#27169;&#24335;&#26377;&#25928;&#12290;&#22312;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#30740;&#31350;&#20013;&#65292;&#22823;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#12290;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#35777;&#23454;&#65292;&#26631;&#24535;&#30528;&#25968;&#25454;&#35299;&#37322;&#21644;&#39044;&#27979;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#36716;&#21464;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#22270;&#24418;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#24191;&#27867;&#39044;&#22788;&#29702;&#27969;&#27700;&#32447;&#21644;&#22823;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#20013;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#36716;&#25442;&#20173;&#28982;&#21463;&#21040;&#24847;&#22806;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeuroGraph(&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#38598;)&#65292;&#23427;&#28085;&#30422;&#20102;&#22810;&#20010;&#34892;&#20026;&#21644;&#35748;&#30693;&#29305;&#24449;&#31867;&#21035;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01665</link><description>&lt;p&gt;
SourceP&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#26234;&#33021;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;
&lt;/p&gt;
&lt;p&gt;
SourceP: Smart Ponzi Schemes Detection on Ethereum Using Pre-training Model with Data Flow. (arXiv:2306.01665v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21306;&#22359;&#38142;&#25216;&#26415;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20856;&#22411;&#30340;&#37329;&#34701;&#39575;&#23616;&#24222;&#20857;&#39575;&#23616;&#20063;&#22312;&#21306;&#22359;&#38142;&#24179;&#21488;&#20197;&#22826;&#22346;&#19978;&#20986;&#29616;&#12290;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#37096;&#32626;&#30340;&#36825;&#31181;&#24222;&#20857;&#39575;&#23616;&#65292;&#20063;&#31216;&#20026;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#65292;&#24050;&#32463;&#36896;&#25104;&#20102;&#22823;&#37327;&#30340;&#32463;&#27982;&#25439;&#22833;&#21644;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20197;&#22826;&#22346;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#23383;&#33410;&#30721;&#29305;&#24449;&#12289;&#25805;&#20316;&#30721;&#29305;&#24449;&#12289;&#36134;&#25143;&#29305;&#24449;&#21644;&#20132;&#26131;&#34892;&#20026;&#29305;&#24449;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SourceP&#65292;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#22312;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#20316;&#20026;&#29305;&#24449;&#65292;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#25506;&#32034;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#21487;&#33021;&#24615;&#12290;SourceP&#38477;&#20302;&#20102;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#30340;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#38590;&#24230;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As blockchain technology becomes more and more popular, a typical financial scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum. This Ponzi scheme deployed through smart contracts, also known as the smart Ponzi scheme, has caused a lot of economic losses and negative impacts. Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on bytecode features, opcode features, account features, and transaction behavior features of smart contracts, and such methods lack interpretability and sustainability. In this paper, we propose SourceP, a method to detect smart Ponzi schemes on the Ethereum platform using pre-training models and data flow, which only requires using the source code of smart contracts as features to explore the possibility of detecting smart Ponzi schemes from another direction. SourceP reduces the difficulty of data acquisition and feature extraction of existing detection methods while increasing the interpretability of the model. 
&lt;/p&gt;</description></item><item><title>DroidBot-GPT&#26159;&#19968;&#27454;&#21033;&#29992;GPT&#27169;&#22411;&#33258;&#21160;&#21270;Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#26377;&#26395;&#25552;&#39640;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07061</link><description>&lt;p&gt;
DroidBot-GPT&#65306;&#22522;&#20110;GPT&#30340;Android UI&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
DroidBot-GPT: GPT-powered UI Automation for Android. (arXiv:2304.07061v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07061
&lt;/p&gt;
&lt;p&gt;
DroidBot-GPT&#26159;&#19968;&#27454;&#21033;&#29992;GPT&#27169;&#22411;&#33258;&#21160;&#21270;Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#26377;&#26395;&#25552;&#39640;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DroidBot-GPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31867;&#20284;GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#21270;&#19982;Android&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20132;&#20114;&#30340;&#24037;&#20855;&#12290;&#32473;&#23450;&#25152;&#38656;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;DroidBot-GPT&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#23558;&#24212;&#29992;&#31243;&#24207;GUI&#29366;&#24577;&#20449;&#24687;&#21644;&#26234;&#33021;&#25163;&#26426;&#23631;&#24149;&#19978;&#21487;&#29992;&#30340;&#25805;&#20316;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#24182;&#35201;&#27714;LLM&#36873;&#25321;&#21160;&#20316;&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;LLM&#36890;&#24120;&#21463;&#36807;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#21253;&#25324;&#21508;&#31181;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#25805;&#20316;&#25351;&#21335;&#65292;&#22240;&#27492;&#23427;&#20855;&#26377;&#26681;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#20316;&#20986;&#21512;&#29702;&#21160;&#20316;&#36873;&#25321;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#33258;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#23545;DroidBot-GPT&#36827;&#34892;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;10&#20010;&#31867;&#21035;&#30340;17&#20010;Android&#24212;&#29992;&#31243;&#24207;&#30340;33&#20010;&#20219;&#21153;&#12290;&#23427;&#21487;&#20197;&#25104;&#21151;&#23436;&#25104;39.39%&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#24179;&#22343;&#37096;&#20998;&#23436;&#25104;&#36827;&#24230;&#32422;&#20026;66.76%&#12290;&#37492;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23436;&#20840;&#33258;&#21160;&#30340;&#65292;&#24182;&#19988;&#29992;&#20110;&#35757;&#32451;LLM&#30340;&#25968;&#25454;&#26159;&#24191;&#27867;&#21487;&#29992;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;DroidBot-GPT&#22312;&#25913;&#21892;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large language models (LLMs) to automate the interactions with Android mobile applications. Given a natural language description of a desired task, DroidBot-GPT can automatically generate and execute actions that navigate the app to complete the task. It works by translating the app GUI state information and the available actions on the smartphone screen to natural language prompts and asking the LLM to make a choice of actions. Since the LLM is typically trained on a large amount of data including the how-to manuals of diverse software applications, it has the ability to make reasonable choices of actions based on the provided information. We evaluate DroidBot-GPT with a self-created dataset that contains 33 tasks collected from 17 Android applications spanning 10 categories. It can successfully complete 39.39% of the tasks, and the average partial completion progress is about 66.76%. Given the fact that our method is f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#20351;&#24471;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20998;&#21035;&#26159;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#12290;&#19982;&#20854;&#20182;&#21487;&#25193;&#23637;&#31639;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#65292;&#32780;&#26159;&#36890;&#36807;&#26435;&#34913;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2302.05326</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#36830;&#25509;&#21644;&#36873;&#25321;&#24615;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Real-Time Recurrent Learning Using Sparse Connections and Selective Learning. (arXiv:2302.05326v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#20351;&#24471;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20998;&#21035;&#26159;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#12290;&#19982;&#20854;&#20182;&#21487;&#25193;&#23637;&#31639;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#65292;&#32780;&#26159;&#36890;&#36807;&#26435;&#34913;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24863;&#30693;&#35266;&#23519;&#20013;&#26500;&#24314;&#29366;&#24577;&#26159;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#19968;&#31181;&#29992;&#20110;&#29366;&#24577;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290; BPTT&#21644;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#65288;RTRL&#65289;&#26159;&#20004;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24490;&#29615;&#23398;&#20064;&#26041;&#27861;&#12290; BPTT&#22312;&#35745;&#31639;&#26799;&#24230;&#20043;&#21069;&#38656;&#35201;&#23436;&#25972;&#30340;&#35266;&#23519;&#24207;&#21015;&#65292;&#19981;&#36866;&#21512;&#22312;&#32447;&#23454;&#26102;&#26356;&#26032;&#12290; RTRL&#21487;&#20197;&#36827;&#34892;&#22312;&#32447;&#26356;&#26032;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#32593;&#32476;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#65292;&#20351;RTRL&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;RTRL&#19982;&#21442;&#25968;&#25968;&#37327;&#21576;&#32447;&#24615;&#27604;&#20363;&#20851;&#31995;&#12290;&#19982;&#20808;&#21069;&#30340;&#21487;&#25193;&#23637;&#26799;&#24230;&#20272;&#35745;&#31639;&#27861;&#65288;&#20363;&#22914;UORO&#21644;Truncated-BPTT&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#26435;&#34913;&#20102;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
State construction from sensory observations is an important component of a reinforcement learning agent. One solution for state construction is to use recurrent neural networks. Back-propagation through time (BPTT), and real-time recurrent learning (RTRL) are two popular gradient-based methods for recurrent learning. BPTT requires the complete sequence of observations before computing gradients and is unsuitable for online real-time updates. RTRL can do online updates but scales poorly to large networks. In this paper, we propose two constraints that make RTRL scalable. We show that by either decomposing the network into independent modules, or learning the network incrementally, we can make RTRL scale linearly with the number of parameters. Unlike prior scalable gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms do not add noise or bias to the gradient estimate. Instead, they trade-off the functional capacity of the network to achieve scalable learning. W
&lt;/p&gt;</description></item></channel></rss>