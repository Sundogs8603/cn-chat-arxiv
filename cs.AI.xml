<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.12963</link><description>&lt;p&gt;
AutoRT&#65306;&#22823;&#35268;&#27169;&#32534;&#25490;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12963
&lt;/p&gt;
&lt;p&gt;
AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#34892;&#21160;&#31561;&#21151;&#33021;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#21033;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#26469;&#25512;&#29702;&#26377;&#29992;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32570;&#20047;&#22522;&#20110;&#29289;&#29702;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoRT&#65292;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#23436;&#20840;&#26410;&#30693;&#22330;&#26223;&#20013;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#24037;&#30417;&#30563;&#12290;AutoRT&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#23454;&#29616;&#22330;&#26223;&#29702;&#35299;&#21644;&#22522;&#30784;&#32465;&#23450;&#65292;&#24182;&#36827;&#19968;&#27493;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#20379;&#19968;&#32452;&#26426;&#22120;&#20154;&#25191;&#34892;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#25351;&#23548;&#25968;&#25454;&#25910;&#38598;&#65292;AutoRT&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#26174;&#33879;&#25193;&#22823;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;AutoRT&#21521;20&#22810;&#20010;&#26426;&#22120;&#20154;&#25552;&#35758;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multi
&lt;/p&gt;</description></item><item><title>&#20803;&#25552;&#31034;&#26159;&#19968;&#31181;&#25903;&#25345;&#25216;&#26415;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#22810;&#38754;&#25163;&#25351;&#25381;&#32773;&#65292;&#36890;&#36807;&#39640;&#32423;&#25351;&#20196;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#30001;&#19981;&#21516;&#30340;&#19987;&#23478;&#27169;&#22411;&#22788;&#29702;&#65292;&#26368;&#32456;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12954</link><description>&lt;p&gt;
&#20803;&#25552;&#31034;&#65306;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#26080;&#20851;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding. (arXiv:2401.12954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12954
&lt;/p&gt;
&lt;p&gt;
&#20803;&#25552;&#31034;&#26159;&#19968;&#31181;&#25903;&#25345;&#25216;&#26415;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#22810;&#38754;&#25163;&#25351;&#25381;&#32773;&#65292;&#36890;&#36807;&#39640;&#32423;&#25351;&#20196;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#30001;&#19981;&#21516;&#30340;&#19987;&#23478;&#27169;&#22411;&#22788;&#29702;&#65292;&#26368;&#32456;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#25552;&#31034;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#25903;&#25345;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#21151;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#21333;&#20010;LM&#36716;&#21270;&#20026;&#22810;&#38754;&#25163;&#25351;&#25381;&#32773;&#65292;&#21892;&#20110;&#31649;&#29702;&#21644;&#25972;&#21512;&#22810;&#20010;&#29420;&#31435;&#30340;LM&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#32423;&#25351;&#20196;&#65292;&#20803;&#25552;&#31034;&#25351;&#23548;LM&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#21487;&#31649;&#29702;&#30340;&#23376;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#23376;&#20219;&#21153;&#30001;&#30456;&#21516;LM&#30340;&#19981;&#21516;&#8220;&#19987;&#23478;&#8221;&#23454;&#20363;&#22788;&#29702;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#22312;&#29305;&#23450;&#30340;&#12289;&#23450;&#21046;&#21270;&#30340;&#25351;&#23548;&#19979;&#25805;&#20316;&#12290;&#35813;&#36807;&#31243;&#30340;&#26680;&#24515;&#26159;LM&#26412;&#36523;&#65292;&#23427;&#20316;&#20026;&#25351;&#25381;&#32773;&#30830;&#20445;&#36825;&#20123;&#19987;&#23478;&#27169;&#22411;&#30340;&#36755;&#20986;&#26080;&#32541;&#27807;&#36890;&#21644;&#26377;&#25928;&#38598;&#25104;&#12290;&#23427;&#36824;&#21033;&#29992;&#20854;&#22266;&#26377;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#24378;&#22823;&#30340;&#39564;&#35777;&#36807;&#31243;&#26469;&#23436;&#21892;&#21644;&#39564;&#35777;&#26368;&#32456;&#32467;&#26524;&#12290;&#36825;&#31181;&#21327;&#20316;&#25552;&#31034;&#26041;&#27861;&#20351;&#21333;&#20010;LM&#33021;&#22815;&#21516;&#26102;&#20805;&#24403;&#20840;&#38754;&#30340;&#32534;&#25490;&#32773;&#21644;&#22810;&#26679;&#21270;&#30340;&#19987;&#23478;&#23567;&#32452;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#23398;&#20064;&#32467;&#26500;&#36882;&#24402;&#30340;&#33021;&#21147;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#25277;&#35937;&#27010;&#24565;&#19982;&#20855;&#20307;&#30340;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#21644;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30456;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2401.12947</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#27169;&#25311;&#32467;&#26500;&#36882;&#24402;&#26041;&#38754;&#23578;&#26410;&#23436;&#32654;
&lt;/p&gt;
&lt;p&gt;
Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion. (arXiv:2401.12947v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#23398;&#20064;&#32467;&#26500;&#36882;&#24402;&#30340;&#33021;&#21147;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#25277;&#35937;&#27010;&#24565;&#19982;&#20855;&#20307;&#30340;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#21644;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30456;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#23398;&#20064;&#32467;&#26500;&#36882;&#24402;&#30340;&#33021;&#21147;&#12290;&#36882;&#24402;&#26159;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#20013;&#30340;&#19968;&#31181;&#36890;&#29992;&#27010;&#24565;&#12290;&#32467;&#26500;&#36882;&#24402;&#22312;&#32534;&#31243;&#35821;&#35328;&#21644;&#24418;&#24335;&#25968;&#23398;&#20219;&#21153;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#31526;&#21495;&#24037;&#20855;&#30446;&#21069;&#22312;&#31070;&#32463;&#27169;&#22411;&#20043;&#19978;&#26377;&#20248;&#21183;&#65292;&#27604;&#22914;&#25512;&#26029;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#21644;&#27169;&#25311;&#31243;&#24207;&#34892;&#20026;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#25277;&#35937;&#27010;&#24565;&#19982;&#20855;&#20307;&#30340;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#21644;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30456;&#36830;&#25509;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25429;&#25417;&#32467;&#26500;&#36882;&#24402;&#19968;&#33324;"&#35821;&#27861;"&#30340;&#34920;&#31034;&#65292;&#20197;&#21450;&#20004;&#31181;&#19981;&#21516;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#23427;&#20204;&#30340;"&#35821;&#20041;"&#8212;&#8212;&#19968;&#31181;&#26356;&#31526;&#21512;&#32534;&#31243;&#35821;&#35328;&#35270;&#35282;&#30340;&#33258;&#28982;&#26041;&#24335;&#65292;&#20197;&#21450;&#19968;&#31181;&#26377;&#21161;&#20110;&#23558;&#35813;&#35270;&#35282;&#19982;&#24213;&#23618;Transformer&#27169;&#22411;&#30340;&#26426;&#26800;&#29702;&#35299;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the ability of transformer-based models to learn structural recursion from examples. Recursion is a universal concept in both natural and formal languages. Structural recursion is central to the programming language and formal mathematics tasks where symbolic tools currently excel beyond neural models, such as inferring semantic relations between datatypes and emulating program behavior. We introduce a general framework that nicely connects the abstract concepts of structural recursion in the programming language domain to concrete sequence modeling problems and learned models' behavior. The framework includes a representation that captures the general \textit{syntax} of structural recursion, coupled with two different frameworks for understanding their \textit{semantics} -- one that is more natural from a programming languages perspective and one that helps bridge that perspective with a mechanistic understanding of the underlying transformer architecture.  Wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Regional Temporal Graph Neural Network (RegT-GCN)&#20316;&#20026;&#19968;&#20010;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25972;&#20010;&#24030;&#30340;&#21345;&#36710;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20572;&#36710;&#20449;&#24687;&#24182;&#32531;&#35299;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12920</link><description>&lt;p&gt;
&#29992;&#20998;&#35299;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21345;&#36710;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Truck Parking Usage Prediction with Decomposed Graph Neural Networks. (arXiv:2401.12920v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12920
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Regional Temporal Graph Neural Network (RegT-GCN)&#20316;&#20026;&#19968;&#20010;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25972;&#20010;&#24030;&#30340;&#21345;&#36710;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20572;&#36710;&#20449;&#24687;&#24182;&#32531;&#35299;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36135;&#36816;&#36208;&#24266;&#19978;&#30340;&#21345;&#36710;&#20572;&#36710;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#20572;&#36710;&#20301;&#19981;&#36275;&#21644;&#36981;&#23432;&#24037;&#26102;&#35268;&#23450;&#12290;&#36825;&#20123;&#38480;&#21046;&#24448;&#24448;&#23548;&#33268;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#34892;&#20026;&#65292;&#24341;&#21457;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#36135;&#36816;&#20316;&#19994;&#30340;&#23433;&#20840;&#24615;&#65292;&#25552;&#20379;&#20934;&#30830;&#30340;&#20572;&#36710;&#20351;&#29992;&#39044;&#27979;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;&#21333;&#20010;&#21345;&#36710;&#20572;&#36710;&#22330;&#20351;&#29992;&#24773;&#20917;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#36739;&#39640;&#65292;&#20294;&#23545;&#22810;&#20010;&#21345;&#36710;&#20572;&#36710;&#22330;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20351;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#24456;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21306;&#22495;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;RegT-GCN&#65289;&#20316;&#20026;&#19968;&#20010;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25972;&#20010;&#24030;&#30340;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#21345;&#36710;&#20572;&#36710;&#20449;&#24687;&#21644;&#32531;&#35299;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#21345;&#36710;&#20572;&#36710;&#22330;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#21382;&#21490;&#20572;&#36710;&#25968;&#25454;&#26469;&#39044;&#27979;&#25972;&#20010;&#24030;&#30340;&#21344;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Truck parking on freight corridors faces various challenges, such as insufficient parking spaces and compliance with Hour-of-Service (HOS) regulations. These constraints often result in unauthorized parking practices, causing safety concerns. To enhance the safety of freight operations, providing accurate parking usage prediction proves to be a cost-effective solution. Despite the existing research demonstrating satisfactory accuracy for predicting individual truck parking site usage, few approaches have been proposed for predicting usage with spatial dependencies of multiple truck parking sites. We present the Regional Temporal Graph Neural Network (RegT-GCN) as a predictive framework for assessing parking usage across the entire state to provide better truck parking information and mitigate unauthorized parking. The framework leverages the topological structures of truck parking site distributions and historical parking data to predict occupancy rates across a state. To achieve this,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#20027;&#21160;&#25512;&#26029;&#20316;&#20026;&#35268;&#33539;&#24615;&#36125;&#21494;&#26031;&#26694;&#26550;&#26469;&#27169;&#25311;&#21644;&#24314;&#27169;&#26426;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39118;&#38505;&#21644;&#23545;&#19990;&#30028;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25972;&#21512;&#25506;&#32034;&#19982;&#24320;&#21457;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#24191;&#27867;&#24212;&#29992;&#20110;&#34892;&#20026;&#31070;&#32463;&#31185;&#23398;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12917</link><description>&lt;p&gt;
&#20027;&#21160;&#25512;&#26029;&#20316;&#20026;&#26426;&#26500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Active Inference as a Model of Agency. (arXiv:2401.12917v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#20027;&#21160;&#25512;&#26029;&#20316;&#20026;&#35268;&#33539;&#24615;&#36125;&#21494;&#26031;&#26694;&#26550;&#26469;&#27169;&#25311;&#21644;&#24314;&#27169;&#26426;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39118;&#38505;&#21644;&#23545;&#19990;&#30028;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25972;&#21512;&#25506;&#32034;&#19982;&#24320;&#21457;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#24191;&#27867;&#24212;&#29992;&#20110;&#34892;&#20026;&#31070;&#32463;&#31185;&#23398;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#22870;&#21169;&#26368;&#22823;&#21270;&#20043;&#22806;&#65292;&#26159;&#21542;&#26377;&#19968;&#31181;&#35268;&#33539;&#30340;&#26041;&#24335;&#26469;&#24605;&#32771;&#26426;&#26500;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;&#31526;&#21512;&#20851;&#20110;&#23439;&#35266;&#29983;&#29289;&#20307;&#22914;&#20309;&#19982;&#19990;&#30028;&#20114;&#21160;&#30340;&#29289;&#29702;&#21512;&#29702;&#20551;&#35774;&#30340;&#34892;&#20026;&#37117;&#20197;&#26368;&#23567;&#21270;&#39118;&#38505;&#21644;&#23545;&#19990;&#30028;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24847;&#20041;&#19978;&#65292;&#35268;&#33539;&#22320;&#25972;&#21512;&#20102;&#25506;&#32034;&#19982;&#24320;&#21457;&#12290;&#36825;&#31181;&#25551;&#36848;&#34987;&#31216;&#20026;&#20027;&#21160;&#25512;&#26029;&#65292;&#23427;&#31934;&#32454;&#21270;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#65292;&#36825;&#26159;&#31070;&#32463;&#31185;&#23398;&#20013;&#34892;&#21160;&#21644;&#30693;&#35273;&#30340;&#19968;&#20010;&#27969;&#34892;&#30340;&#25551;&#36848;&#24615;&#26694;&#26550;&#12290;&#20027;&#21160;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#31181;&#35268;&#33539;&#24615;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#24314;&#27169;&#26426;&#26500;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#34892;&#20026;&#31070;&#32463;&#31185;&#23398;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#39046;&#22495;&#12290;&#20027;&#21160;&#25512;&#26029;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#29992;&#22788;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;a) &#20027;&#21160;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#29983;&#29289;&#26426;&#26500;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;b) &#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#37197;&#26041;&#26469;&#27169;&#25311;&#34892;&#20026;&#65292;&#34892;&#20026;&#38543;&#20043;&#25104;&#20026;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is there a canonical way to think of agency beyond reward maximisation? In this paper, we show that any type of behaviour complying with physically sound assumptions about how macroscopic biological agents interact with the world canonically integrates exploration and exploitation in the sense of minimising risk and ambiguity about states of the world. This description, known as active inference, refines the free energy principle, a popular descriptive framework for action and perception originating in neuroscience. Active inference provides a normative Bayesian framework to simulate and model agency that is widely used in behavioural neuroscience, reinforcement learning (RL) and robotics. The usefulness of active inference for RL is three-fold. \emph{a}) Active inference provides a principled solution to the exploration-exploitation dilemma that usefully simulates biological agency. \emph{b}) It provides an explainable recipe to simulate behaviour, whence behaviour follows as an expla
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#32418;&#38431;&#25968;&#25454;&#38598;RTVLM&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;VLMs&#22312;&#24544;&#23454;&#24230;&#12289;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;GPT-4V&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;10&#20010;&#30693;&#21517;&#30340;VLMs&#22312;&#32418;&#38431;&#34892;&#21160;&#20013;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.12915</link><description>&lt;p&gt;
&#32418;&#38431;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Visual Language Models. (arXiv:2401.12915v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#32418;&#38431;&#25968;&#25454;&#38598;RTVLM&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;VLMs&#22312;&#24544;&#23454;&#24230;&#12289;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;GPT-4V&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;10&#20010;&#30693;&#21517;&#30340;VLMs&#22312;&#32418;&#38431;&#34892;&#21160;&#20013;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#25193;&#23637;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25509;&#21463;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#24050;&#32463;&#39564;&#35777;LLMs&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#27979;&#35797;&#29992;&#20363;&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#20934;&#30830;&#30340;&#20869;&#23481;(&#31216;&#20026;&#32418;&#38431;&#34892;&#21160;)&#65292;VLMs&#22312;&#31867;&#20284;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#21644;&#35270;&#35273;&#36755;&#20837;&#30340;&#32452;&#21512;&#20013;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32418;&#38431;&#25968;&#25454;&#38598;RTVLM&#65292;&#21253;&#21547;4&#20010;&#20027;&#35201;&#26041;&#38754;(&#24544;&#23454;&#24230;&#12289;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;)&#19979;&#30340;10&#20010;&#23376;&#20219;&#21153;(&#22914;&#22270;&#20687;&#35823;&#23548;&#12289;&#22810;&#27169;&#24577;&#36234;&#29425;&#12289;&#33080;&#37096;&#20844;&#24179;&#31561;)&#12290;&#25105;&#20204;&#30340;RTVLM&#26159;&#31532;&#19968;&#20010;&#20174;&#36825;&#22235;&#20010;&#19981;&#21516;&#26041;&#38754;&#35780;&#20272;&#24403;&#21069;VLMs&#30340;&#32418;&#38431;&#25968;&#25454;&#38598;&#12290;&#35814;&#32454;&#20998;&#26512;&#34920;&#26126;&#65292;10&#20010;&#30693;&#21517;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;VLMs&#22312;&#32418;&#38431;&#34892;&#21160;&#20013;&#36935;&#21040;&#19981;&#21516;&#31243;&#24230;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#19982;GPT-4V&#30456;&#27604;&#65292;&#24615;&#33021;&#24046;&#36317;&#39640;&#36798;31%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;RT&#31616;&#21333;&#22320;&#23558;&#32418;&#38431;&#34892;&#21160;&#23545;&#40784;&#21040;LLaVA-v1.5&#19978;&#65292;&#20351;&#29992;&#30417;&#30563;&#24494;&#35843;(SFT)&#12290;
&lt;/p&gt;
&lt;p&gt;
VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#24037;&#19994;&#29289;&#32852;&#32593;&#20013;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#30340;&#26032;&#22411;&#36890;&#20449;&#21327;&#35758;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#20449;&#36947;&#25509;&#20837;&#25104;&#21151;&#29575;&#21644;&#25104;&#21151;&#35745;&#31639;&#20219;&#21153;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#32988;&#36807;&#29616;&#26377;&#30340;&#36828;&#31243;&#35745;&#31639;&#21644;&#26412;&#22320;&#35745;&#31639;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12914</link><description>&lt;p&gt;
&#24037;&#19994;&#29289;&#32852;&#32593;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#30340;&#26032;&#22411;&#36890;&#20449;&#21327;&#35758;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Emergent Communication Protocol Learning for Task Offloading in Industrial Internet of Things. (arXiv:2401.12914v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#24037;&#19994;&#29289;&#32852;&#32593;&#20013;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#30340;&#26032;&#22411;&#36890;&#20449;&#21327;&#35758;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#20449;&#36947;&#25509;&#20837;&#25104;&#21151;&#29575;&#21644;&#25104;&#21151;&#35745;&#31639;&#20219;&#21153;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#32988;&#36807;&#29616;&#26377;&#30340;&#36828;&#31243;&#35745;&#31639;&#21644;&#26412;&#22320;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#32852;&#21512;&#23398;&#20064;&#35745;&#31639;&#21368;&#36733;&#20915;&#31574;&#21644;&#22810;&#20449;&#36947;&#25509;&#20837;&#31574;&#30053;&#20197;&#21450;&#23545;&#24212;&#30340;&#20449;&#20196;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#31449;&#21644;&#24037;&#19994;&#29289;&#32852;&#32593;&#31227;&#21160;&#35774;&#22791;&#26159;&#38656;&#35201;&#21512;&#20316;&#20197;&#22312;&#25130;&#27490;&#26102;&#38388;&#38480;&#21046;&#20869;&#25191;&#34892;&#35745;&#31639;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#37319;&#29992;&#26032;&#22411;&#30340;&#36890;&#20449;&#21327;&#35758;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#31454;&#20105;&#12289;&#26080;&#31454;&#20105;&#21644;&#26080;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#26032;&#22411;&#30340;&#36890;&#20449;&#21327;&#35758;&#33021;&#22815;&#25552;&#39640;&#20449;&#36947;&#25509;&#20837;&#25104;&#21151;&#29575;&#21644;&#25104;&#21151;&#35745;&#31639;&#20219;&#21153;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#32988;&#36807;&#36828;&#31243;&#35745;&#31639;&#21644;&#26412;&#22320;&#35745;&#31639;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we leverage a multi-agent reinforcement learning (MARL) framework to jointly learn a computation offloading decision and multichannel access policy with corresponding signaling. Specifically, the base station and industrial Internet of Things mobile devices are reinforcement learning agents that need to cooperate to execute their computation tasks within a deadline constraint. We adopt an emergent communication protocol learning framework to solve this problem. The numerical results illustrate the effectiveness of emergent communication in improving the channel access success rate and the number of successfully computed tasks compared to contention-based, contention-free, and no-communication approaches. Moreover, the proposed task offloading policy outperforms remote and local computation baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#35813;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.12874</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#21040;&#24212;&#29992;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From Understanding to Utilization: A Survey on Explainability for Large Language Models. (arXiv:2401.12874v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#35813;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;LLMs&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20854;&#8220;&#40657;&#30418;&#8221;&#24615;&#36136;&#24341;&#21457;&#20102;&#23545;&#36879;&#26126;&#24615;&#21644;&#20262;&#29702;&#20351;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#24191;&#22823;&#20844;&#20247;&#23545;&#20854;&#20449;&#20219;&#21644;&#25216;&#26415;&#30028;&#23545;&#36825;&#20123;&#27169;&#22411;&#26356;&#28145;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;LLMs&#65292;&#22914;LLaMA&#65292;&#20854;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#20351;&#20854;&#38754;&#20020;&#29420;&#29305;&#30340;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20195;&#34920;&#24615;&#30340;&#35780;&#20215;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#26412;&#32508;&#36848;&#30340;&#30446;&#26631;&#26159;&#24357;&#21512;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20379;&#20174;&#25216;&#26415;&#35282;&#24230;&#24635;&#32467;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper delves into the burgeoning field of explainability for Large Language Models (LLMs), a critical yet challenging aspect of natural language processing. With LLMs playing a pivotal role in various applications, their "black-box" nature raises concerns about transparency and ethical use. This paper emphasizes the necessity for enhanced explainability in LLMs, addressing both the general public's trust and the technical community's need for a deeper understanding of these models. We concentrate on pre-trained Transformer-based LLMs, such as LLaMA, which present unique interpretability challenges due to their scale and complexity. Our review categorizes existing explainability methods and discusses their application in improving model transparency and reliability. We also discuss representative evaluation methods, highlighting their strengths and limitations. The goal of this survey is to bridge the gap between theoretical understanding and practical application, offering 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12873</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#30340;&#21453;&#39304;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;: &#23558;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model. (arXiv:2401.12873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#20197;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#21453;&#39304;&#35757;&#32451;&#23384;&#22312;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#24182;&#23545;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#36827;&#34892;&#24809;&#32602;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#20805;&#20998;&#24314;&#27169;&#20154;&#31867;&#20559;&#22909;&#23548;&#33268;&#22870;&#21169;&#27169;&#22411;&#22312;&#21033;&#29992;&#20154;&#30340;&#21453;&#39304;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36136;&#37327;&#20272;&#35745;(QE)&#22312;&#36807;&#21435;&#20004;&#24180;&#20013;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#23601;&#33021;&#20934;&#30830;&#39044;&#27979;&#32473;&#23450;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;QE&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;(&#22522;&#20110;QE&#30340;&#22870;&#21169;&#27169;&#22411;)&#26469;&#39044;&#27979;&#20154;&#30340;&#20559;&#22909;&#20197;&#36827;&#34892;&#21453;&#39304;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#20102;&#22312;&#22522;&#20110;QE&#30340;&#21453;&#39304;&#35757;&#32451;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#34920;&#29616;&#20026;&#22870;&#21169;&#30340;&#22686;&#21152;&#32780;&#32763;&#35793;&#36136;&#37327;&#19979;&#38477;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35748;&#20026;QE&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#32763;&#35793;&#30340;&#39640;&#22870;&#21169;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#20248;&#21270;&#21644;&#38169;&#35823;&#20256;&#25773;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#26816;&#27979;&#38169;&#35823;&#32763;&#35793;&#65292;&#24182;&#20026;QE&#27169;&#22411;&#28155;&#21152;&#20102;&#19968;&#20010;&#24809;&#32602;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the Q
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TROVE&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#12289;&#20351;&#29992;&#12289;&#25193;&#23637;&#21644;&#23450;&#26399;&#20462;&#21098;&#24037;&#20855;&#31665;&#65292;&#24341;&#23548;&#29983;&#25104;&#21487;&#39564;&#35777;&#21644;&#39640;&#25928;&#30340;&#20989;&#25968;&#24037;&#20855;&#31665;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;TROVE&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26102;&#33021;&#22815;&#20135;&#29983;&#26356;&#31616;&#21333;&#19988;&#26356;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#24037;&#20855;&#31665;&#30340;&#20307;&#31215;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#36895;&#21644;&#26356;&#20934;&#30830;&#30340;&#20154;&#24037;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;TROVE&#36824;&#33021;&#22815;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#21019;&#24314;&#22810;&#26679;&#21270;&#30340;&#20989;&#25968;&#65292;&#25552;&#20379;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.12869</link><description>&lt;p&gt;
TroVE:&#24341;&#23548;&#21487;&#39564;&#35777;&#21644;&#39640;&#25928;&#24037;&#20855;&#31665;&#35299;&#20915;&#31243;&#24207;&#21270;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks. (arXiv:2401.12869v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TROVE&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#12289;&#20351;&#29992;&#12289;&#25193;&#23637;&#21644;&#23450;&#26399;&#20462;&#21098;&#24037;&#20855;&#31665;&#65292;&#24341;&#23548;&#29983;&#25104;&#21487;&#39564;&#35777;&#21644;&#39640;&#25928;&#30340;&#20989;&#25968;&#24037;&#20855;&#31665;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;TROVE&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26102;&#33021;&#22815;&#20135;&#29983;&#26356;&#31616;&#21333;&#19988;&#26356;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#24037;&#20855;&#31665;&#30340;&#20307;&#31215;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#36895;&#21644;&#26356;&#20934;&#30830;&#30340;&#20154;&#24037;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;TROVE&#36824;&#33021;&#22815;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#21019;&#24314;&#22810;&#26679;&#21270;&#30340;&#20989;&#25968;&#65292;&#25552;&#20379;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#36890;&#36807;&#32534;&#20889;&#31243;&#24207;&#26469;&#35299;&#20915;&#35832;&#22914;&#22238;&#31572;&#34920;&#26684;&#25110;&#22270;&#20687;&#30456;&#20851;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22522;&#26412;&#20989;&#25968;&#36890;&#24120;&#20250;&#23548;&#33268;&#20887;&#38271;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#31243;&#24207;&#65292;&#32780;&#39640;&#32423;&#20989;&#25968;&#38656;&#35201;&#19987;&#23478;&#35774;&#35745;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#38656;&#20154;&#21147;&#30340;&#26356;&#22909;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#35201;&#27714;&#20195;&#30721;LMs&#31574;&#21010;&#21487;&#37325;&#29992;&#30340;&#39640;&#32423;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#32534;&#20889;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TROVE&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#12289;&#20351;&#29992;&#12289;&#25193;&#23637;&#21644;&#23450;&#26399;&#20462;&#21098;&#24037;&#20855;&#31665;&#65292;&#24341;&#23548;&#29983;&#25104;&#21487;&#39564;&#35777;&#21644;&#39640;&#25928;&#30340;&#20989;&#25968;&#24037;&#20855;&#31665;&#12290;&#22312;&#20174;&#25968;&#23398;&#12289;&#34920;&#26684;&#38382;&#31572;&#21644;&#22270;&#20687;&#25512;&#29702;&#20219;&#21153;&#20013;&#24471;&#21040;&#30340;11&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;TROVE&#22987;&#32456;&#27604;&#20351;&#29992;CODELLAMA&#21644;&#20808;&#21069;&#20351;&#29992;GPT&#30340;&#22522;&#20934;&#26041;&#27861;&#20135;&#29983;&#26356;&#31616;&#21333;&#19988;&#26356;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20351;&#29992;79-98%&#26356;&#23567;&#30340;&#24037;&#20855;&#31665;&#12290;TROVE&#36824;&#20351;&#24471;&#20154;&#24037;&#39564;&#35777;&#27604;&#22522;&#20934;&#26041;&#27861;&#24555;31&#65285;&#19988;&#20934;&#30830;&#24615;&#26356;&#39640;13&#65285;&#12290;&#22312;&#30456;&#21516;&#30340;&#27969;&#31243;&#19979;&#65292;&#23427;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#22810;&#26679;&#21270;&#30340;&#20989;&#25968;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) can solve tasks such as answering questions about tables or images by writing programs. However, using primitive functions often leads to verbose and error-prone programs, and higher-level functions require expert design. To enable better solutions without human labor, we ask code LMs to curate reusable high-level functions, and use them to write solutions. We present TROVE, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox. On 11 datasets from math, table question answering, and image reasoning tasks, TROVE consistently yields simpler solutions with higher accuracy than baselines using CODELLAMA and previous methods using GPT, while using 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more accurate human verification than baselines. With the same pipeline, it creates diverse functions for varied tasks and datasets, providing insights into 
&lt;/p&gt;</description></item><item><title>&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#20013;&#20219;&#21153;&#20998;&#37197;&#21644;&#36136;&#37327;&#38382;&#39064;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#27969;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#20219;&#21153;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#36716;&#31227;&#26469;&#35299;&#20915;&#20219;&#21153;&#20998;&#37197;&#20013;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26426;&#21046;&#26377;&#21161;&#20110;&#25552;&#39640;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.12866</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#27969;&#25903;&#25345;&#30340;&#31227;&#21160;&#20247;&#21253;&#21327;&#35843;&#20013;&#35780;&#20272;&#21327;&#20316;&#21644;&#33258;&#27835;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported Coordination of Mobile Crowdsourcing. (arXiv:2401.12866v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12866
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#20013;&#20219;&#21153;&#20998;&#37197;&#21644;&#36136;&#37327;&#38382;&#39064;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#27969;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#20219;&#21153;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#36716;&#31227;&#26469;&#35299;&#20915;&#20219;&#21153;&#20998;&#37197;&#20013;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26426;&#21046;&#26377;&#21161;&#20110;&#25552;&#39640;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20247;&#21253;&#26159;&#25351;&#23436;&#25104;&#20219;&#21153;&#38656;&#35201;&#20247;&#21253;&#24037;&#20316;&#32773;&#22312;&#25353;&#38656;&#21171;&#21160;&#21147;&#20013;&#36827;&#34892;&#29289;&#29702;&#31227;&#21160;&#30340;&#31995;&#32479;&#12290;Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. &#25552;&#39640;&#26381;&#21153;&#36136;&#37327;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#19981;&#26029;&#36866;&#24212;&#20219;&#21153;&#20998;&#37197;&#65292;&#24182;&#22312;&#20986;&#29616;&#23548;&#33268;&#22833;&#36133;&#30340;&#20107;&#20214;&#26102;&#23558;&#20219;&#21153;&#36716;&#31227;&#32473;&#26356;&#21512;&#36866;&#30340;&#24037;&#20316;&#32773;&#65292;&#20182;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#36335;&#32447;&#25110;&#36710;&#36742;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#20247;&#21253;&#20013;&#23454;&#29616;&#20219;&#21153;&#36716;&#31227;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#24037;&#20316;&#32773;&#26159;&#33258;&#27835;&#30340;&#65292;&#21487;&#33021;&#25298;&#32477;&#36716;&#31227;&#35831;&#27714;&#12290;&#27492;&#22806;&#65292;&#20219;&#21153;&#32467;&#26524;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#38656;&#35201;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26426;&#21046;&#26469;&#23454;&#29616;&#31227;&#21160;&#20247;&#21253;&#20013;&#30340;&#32467;&#26524;&#39044;&#27979;&#21644;&#20219;&#21153;&#21327;&#35843;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#27969;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#20219;&#21153;&#32467;&#26524;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;p
&lt;/p&gt;
&lt;p&gt;
Mobile crowdsourcing refers to systems where the completion of tasks necessarily requires physical movement of crowdworkers in an on-demand workforce. Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. A promising solution to ensure higher quality of service is to continuously adapt the assignment and respond to failure-causing events by transferring tasks to better-suited workers who use different routes or vehicles. However, implementing task transfers in mobile crowdsourcing is difficult because workers are autonomous and may reject transfer requests. Moreover, task outcomes are uncertain and need to be predicted. In this paper, we propose different mechanisms to achieve outcome prediction and task coordination in mobile crowdsourcing. First, we analyze different data stream learning approaches for the prediction of task outcomes. Second, based on
&lt;/p&gt;</description></item><item><title>KAM-CoT&#26159;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#27169;&#24335;&#24605;&#32500;&#38142;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#22810;&#27169;&#24335;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.12863</link><description>&lt;p&gt;
KAM-CoT: &#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#27169;&#24335;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning. (arXiv:2401.12863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12863
&lt;/p&gt;
&lt;p&gt;
KAM-CoT&#26159;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#27169;&#24335;&#24605;&#32500;&#38142;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#22810;&#27169;&#24335;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#23454;&#29616;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#23558;LLMs&#25193;&#23637;&#20026;&#22810;&#27169;&#24335;&#33021;&#21147;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#21521;&#65292;&#20294;&#20250;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#24182;&#38656;&#35201;&#22823;&#37327;&#30828;&#20214;&#36164;&#28304;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KAM-CoT&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#34701;&#21512;&#20102;CoT&#25512;&#29702;&#12289;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#21644;&#22810;&#31181;&#27169;&#24335;&#65292;&#20197;&#20840;&#38754;&#29702;&#35299;&#22810;&#27169;&#24335;&#20219;&#21153;&#12290;KAM-CoT&#37319;&#29992;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#65292;&#36890;&#36807;KG&#22522;&#30784;&#29983;&#25104;&#26377;&#25928;&#30340;&#29702;&#30001;&#21644;&#31572;&#26696;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24341;&#20837;&#26469;&#33258;&#30693;&#35782;&#22270;&#35889;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#27169;&#22411;&#33719;&#24471;&#20102;&#26356;&#28145;&#20837;&#30340;&#35821;&#22659;&#29702;&#35299;&#65292;&#20943;&#23569;&#20102;&#34394;&#26500;&#21644;&#25913;&#21892;&#20102;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;CoT&#25512;&#29702;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#38656;&#35201;&#22806;&#37096;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#26356;&#26377;&#26681;&#25454;&#30340;&#31572;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;KAM-CoT&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive performance in natural language processing tasks by leveraging chain of thought (CoT) that enables step-by-step thinking. Extending LLMs with multimodal capabilities is the recent interest, but incurs computational cost and requires substantial hardware resources. To address these challenges, we propose KAM-CoT a framework that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a two-stage training process with KG grounding to generate effective rationales and answers. By incorporating external knowledge from KGs during reasoning, the model gains a deeper contextual understanding reducing hallucinations and enhancing the quality of answers. This knowledge-augmented CoT reasoning empowers the model to handle questions requiring external context, providing more informed answers. Experimental findings show KAM-CoT outperforms the state-of-t
&lt;/p&gt;</description></item><item><title>FedRSU&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#23427;&#36890;&#36807;&#25972;&#21512;RSU&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#24490;&#29615;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12862</link><description>&lt;p&gt;
FedRSU: &#22522;&#20110;RSU&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#22330;&#26223;&#27969;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units. (arXiv:2401.12862v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12862
&lt;/p&gt;
&lt;p&gt;
FedRSU&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#23427;&#36890;&#36807;&#25972;&#21512;RSU&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#24490;&#29615;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RSU&#65288;&#36335;&#36793;&#21333;&#20803;&#65289;&#36890;&#36807;&#36710;&#32852;&#32593;&#65288;V2X&#65289;&#36890;&#20449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#30446;&#21069;&#65292;&#21333;&#20010;RSU&#30340;&#20351;&#29992;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#26102;&#25512;&#26029;&#21644;V2X&#21327;&#20316;&#19978;&#65292;&#32780;&#24573;&#35270;&#20102;RSU&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#28508;&#22312;&#20215;&#20540;&#12290;&#25972;&#21512;&#22823;&#37327;&#26469;&#33258;&#22810;&#20010;RSU&#30340;&#25968;&#25454;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20016;&#23500;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#21644;&#20256;&#36755;&#24222;&#22823;&#25968;&#25454;&#37327;&#30340;&#22256;&#38590;&#26159;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#38544;&#34255;&#20215;&#20540;&#30340;&#20004;&#20010;&#19981;&#21487;&#36991;&#20813;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FedRSU&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#30417;&#30563;&#22330;&#26223;&#27969;&#20272;&#35745;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;FedRSU&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#23545;&#20110;&#27599;&#20010;RSU&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#21518;&#32493;&#26410;&#26469;&#30340;&#22810;&#27169;&#24577;&#35266;&#27979;&#23545;&#27599;&#20010;&#26102;&#38388;&#25139;&#30340;&#28857;&#30340;&#22330;&#26223;&#27969;&#39044;&#27979;&#36827;&#34892;&#30417;&#30563;&#12290;FedRSU&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#32852;&#37030;
&lt;/p&gt;
&lt;p&gt;
Roadside unit (RSU) can significantly improve the safety and robustness of autonomous vehicles through Vehicle-to-Everything (V2X) communication. Currently, the usage of a single RSU mainly focuses on real-time inference and V2X collaboration, while neglecting the potential value of the high-quality data collected by RSU sensors. Integrating the vast amounts of data from numerous RSUs can provide a rich source of data for model training. However, the absence of ground truth annotations and the difficulty of transmitting enormous volumes of data are two inevitable barriers to fully exploiting this hidden value. In this paper, we introduce FedRSU, an innovative federated learning framework for self-supervised scene flow estimation. In FedRSU, we present a recurrent self-supervision training paradigm, where for each RSU, the scene flow prediction of points at every timestamp can be supervised by its subsequent future multi-modality observation. Another key component of FedRSU is federated
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#20154;&#26426;&#39640;&#20809;&#35889;&#25104;&#20687;&#23545;17&#31181;&#32418;&#30333;&#33889;&#33796;&#21697;&#31181;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#30772;&#22351;&#24615;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644; less prohibitive&#65292;&#33021;&#22815;&#32416;&#27491;&#21644;&#38477;&#20302;&#22823;&#37327;&#25968;&#25454;&#30340;&#37319;&#26679;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#33889;&#33796;&#21697;&#31181;&#39640;&#24230;&#30456;&#20284;&#30340;&#39640;&#20809;&#35889;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.12851</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#20154;&#26426;&#39640;&#20809;&#35889;&#25104;&#20687;&#23545;&#33889;&#33796;&#21697;&#31181;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of grapevine varieties using UAV hyperspectral imaging. (arXiv:2401.12851v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12851
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#20154;&#26426;&#39640;&#20809;&#35889;&#25104;&#20687;&#23545;17&#31181;&#32418;&#30333;&#33889;&#33796;&#21697;&#31181;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#30772;&#22351;&#24615;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644; less prohibitive&#65292;&#33021;&#22815;&#32416;&#27491;&#21644;&#38477;&#20302;&#22823;&#37327;&#25968;&#25454;&#30340;&#37319;&#26679;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#33889;&#33796;&#21697;&#31181;&#39640;&#24230;&#30456;&#20284;&#30340;&#39640;&#20809;&#35889;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#33889;&#33796;&#21697;&#31181;&#30340;&#20998;&#31867;&#26159;&#31934;&#20934;&#33889;&#33796;&#26685;&#22521;&#20013;&#19968;&#20010;&#30456;&#20851;&#30340;&#34920;&#22411;&#20998;&#26512;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20272;&#35745;&#19981;&#21516;&#21697;&#31181;&#30340;&#33889;&#33796;&#22253;&#34892;&#30340;&#29983;&#38271;&#24773;&#20917;&#65292;&#24182;&#28041;&#21450;&#21040;&#33889;&#33796;&#37202;&#34892;&#19994;&#30340;&#20854;&#20182;&#24212;&#29992;&#12290;&#36825;&#39033;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#30772;&#22351;&#24615;&#30340;&#26041;&#27861;&#26469;&#23436;&#25104;&#65292;&#38656;&#35201;&#32791;&#26102;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#23454;&#39564;&#23460;&#20998;&#26512;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26080;&#20154;&#26426;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#12289; less prohibitive&#30340;&#26041;&#27861;&#26469;&#25910;&#38598;&#39640;&#20809;&#35889;&#25968;&#25454;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#33719;&#24471;&#26356;&#22024;&#26434;&#30340;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#31532;&#19968;&#20010;&#20219;&#21153;&#26159;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#20197;&#32416;&#27491;&#21644;&#38477;&#20302;&#22823;&#37327;&#25968;&#25454;&#30340;&#37319;&#26679;&#29575;&#12290;&#27492;&#22806;&#65292;&#33889;&#33796;&#21697;&#31181;&#30340;&#39640;&#20809;&#35889;&#29305;&#24449;&#38750;&#24120;&#30456;&#20284;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#29992;&#20110;&#23545;17&#31181;&#32418;&#30333;&#33889;&#33796;&#21697;&#31181;&#36827;&#34892;&#20998;&#31867;&#12290;&#19982;&#20998;&#31867;&#21333;&#20010;&#26679;&#26412;&#19981;&#21516;&#65292;&#36825;&#20123;&#26679;&#26412;&#19982;&#20854;&#21608;&#22260;&#29615;&#22659;&#19968;&#36215;&#36827;&#34892;&#22788;&#29702;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#36827;&#34892;&#31354;&#38388;&#29305;&#24449;&#30340;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classification of different grapevine varieties is a relevant phenotyping task in Precision Viticulture since it enables estimating the growth of vineyard rows dedicated to different varieties, among other applications concerning the wine industry. This task can be performed with destructive methods that require time-consuming tasks, including data collection and analysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a more efficient and less prohibitive approach to collecting hyperspectral data, despite acquiring noisier data. Therefore, the first task is the processing of these data to correct and downsample large amounts of data. In addition, the hyperspectral signatures of grape varieties are very similar. In this work, a Convolutional Neural Network (CNN) is proposed for classifying seventeen varieties of red and white grape variants. Rather than classifying single samples, these are processed together with their neighbourhood. Hence, the extraction of spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28436;&#35762;&#32773;&#20998;&#21106;&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12289;&#24230;&#37327;&#23398;&#20064;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#22806;&#37096;&#37325;&#21472;&#26816;&#27979;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2401.12850</link><description>&lt;p&gt;
&#38024;&#23545;&#28436;&#35762;&#32773;&#20998;&#21106;&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#22270;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Overlap-aware End-to-End Supervised Hierarchical Graph Clustering for Speaker Diarization. (arXiv:2401.12850v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28436;&#35762;&#32773;&#20998;&#21106;&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12289;&#24230;&#37327;&#23398;&#20064;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#22806;&#37096;&#37325;&#21472;&#26816;&#27979;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#35762;&#32773;&#20998;&#21106;&#26159;&#22522;&#20110;&#35828;&#35805;&#32773;&#36523;&#20221;&#23545;&#38899;&#39057;&#24405;&#38899;&#36827;&#34892;&#20998;&#21106;&#30340;&#37325;&#35201;&#35821;&#38899;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#19979;&#28216;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#20998;&#21106;&#26041;&#27861;&#28041;&#21450;&#22810;&#27425;&#23884;&#20837;&#25552;&#21462;&#21644;&#32858;&#31867;&#27493;&#39588;&#65292;&#36890;&#24120;&#20197;&#23396;&#31435;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#12290;&#34429;&#28982;&#31471;&#21040;&#31471;&#30340;&#20998;&#21106;&#31995;&#32479;&#35797;&#22270;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26469;&#23436;&#25104;&#20219;&#21153;&#65292;&#20294;&#36890;&#24120;&#35757;&#32451;&#22797;&#26434;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#30417;&#30563;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#32858;&#31867;&#31639;&#27861;&#65292;&#31216;&#20026;E-SHARC&#12290;E-SHARC&#26041;&#27861;&#20351;&#29992;&#21069;&#31471;mel-filterbank&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#32852;&#21512;&#23398;&#20064;&#23884;&#20837;&#25552;&#21462;&#22120;&#21644;GNN&#32858;&#31867;&#27169;&#22359;&#65292;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12289;&#24230;&#37327;&#23398;&#20064;&#21644;&#31471;&#21040;&#31471;&#20248;&#21270;&#30340;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;E-SHARC&#36824;&#36890;&#36807;&#22806;&#37096;&#37325;&#21472;&#26816;&#27979;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker diarization, the task of segmenting an audio recording based on speaker identity, constitutes an important speech pre-processing step for several downstream applications. The conventional approach to diarization involves multiple steps of embedding extraction and clustering, which are often optimized in an isolated fashion. While end-to-end diarization systems attempt to learn a single model for the task, they are often cumbersome to train and require large supervised datasets. In this paper, we propose an end-to-end supervised hierarchical clustering algorithm based on graph neural networks (GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). The E-SHARC approach uses front-end mel-filterbank features as input and jointly learns an embedding extractor and the GNN clustering module, performing representation learning, metric learning, and clustering with end-to-end optimization. Further, with additional inputs from an external overlap detector, the E-SHARC app
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#19994;&#21153;&#27969;&#31243;&#35299;&#37322;&#30340;SAX4BPM&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38598;&#25104;&#26469;&#32508;&#21512;&#21508;&#31181;&#36755;&#20837;&#35201;&#32032;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24773;&#22659;&#24863;&#30693;&#35299;&#37322;&#65288;SAX&#65289;&#12290;</title><link>http://arxiv.org/abs/2401.12846</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22914;&#20309;&#35299;&#37322;&#19994;&#21153;&#27969;&#31243;&#65311;
&lt;/p&gt;
&lt;p&gt;
How well can large language models explain business processes?. (arXiv:2401.12846v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#19994;&#21153;&#27969;&#31243;&#35299;&#37322;&#30340;SAX4BPM&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38598;&#25104;&#26469;&#32508;&#21512;&#21508;&#31181;&#36755;&#20837;&#35201;&#32032;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24773;&#22659;&#24863;&#30693;&#35299;&#37322;&#65288;SAX&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#22312;&#26410;&#26469;&#30340;AI&#36741;&#21161;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#31995;&#32479;&#65288;ABPMSs&#65289;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20854;&#21151;&#33021;&#28085;&#30422;&#31995;&#32479;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#12290;&#20854;&#20013;&#19968;&#20010;&#31995;&#32479;&#21151;&#33021;&#26159;&#24773;&#22659;&#24863;&#30693;&#35299;&#37322;&#65288;SAX&#65289;&#65292;&#23427;&#28041;&#21450;&#29983;&#25104;&#22312;&#32771;&#34385;&#25152;&#35299;&#37322;&#26465;&#20214;&#20986;&#29616;&#30340;&#27969;&#31243;&#19978;&#19979;&#25991;&#30340;&#21069;&#25552;&#19979;&#26082;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#21448;&#21487;&#20154;&#31867;&#35299;&#35835;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24320;&#21457;&#29992;&#20110;&#29983;&#25104;SAX&#35299;&#37322;&#30340;SAX4BPM&#26694;&#26550;&#12290;SAX4BPM&#22871;&#20214;&#21253;&#25324;&#19968;&#32452;&#26381;&#21153;&#21644;&#19968;&#20010;&#20013;&#22830;&#30693;&#35782;&#24211;&#12290;&#36825;&#20123;&#26381;&#21153;&#30340;&#21151;&#33021;&#26159;&#33719;&#21462;&#26500;&#25104;SAX&#35299;&#37322;&#30340;&#21508;&#31181;&#30693;&#35782;&#35201;&#32032;&#12290;&#20854;&#20013;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#22240;&#26524;&#36807;&#31243;&#25191;&#34892;&#35270;&#22270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#19982;LLM&#38598;&#25104;&#65292;&#20197;&#21033;&#29992;&#20854;&#32508;&#21512;&#21508;&#31181;&#36755;&#20837;&#35201;&#32032;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;SAX&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are likely to play a prominent role in future AI-augmented business process management systems (ABPMSs) catering functionalities across all system lifecycle stages. One such system's functionality is Situation-Aware eXplainability (SAX), which relates to generating causally sound and yet human-interpretable explanations that take into account the process context in which the explained condition occurred. In this paper, we present the SAX4BPM framework developed to generate SAX explanations. The SAX4BPM suite consists of a set of services and a central knowledge repository. The functionality of these services is to elicit the various knowledge ingredients that underlie SAX explanations. A key innovative component among these ingredients is the causal process execution view. In this work, we integrate the framework with an LLM to leverage its power to synthesize the various input ingredients for the sake of improved SAX explanations. Since the use of LLMs for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31471;&#21040;&#31471;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20108;&#20998;&#22270;&#26500;&#24314;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23454;&#20307;&#21644;&#35859;&#35789;&#20505;&#36873;&#38598;&#30340;&#29983;&#25104;&#21644;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25512;&#26029;&#65292;&#24182;&#35774;&#35745;&#20102;&#22270;&#32452;&#35013;&#27169;&#22359;&#25512;&#26029;&#20108;&#20998;&#22270;&#30340;&#36830;&#36890;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22330;&#26223;&#22270;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.12835</link><description>&lt;p&gt;
SGTR+: &#22522;&#20110;Transformer&#30340;&#31471;&#21040;&#31471;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SGTR+: End-to-end Scene Graph Generation with Transformer. (arXiv:2401.12835v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31471;&#21040;&#31471;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20108;&#20998;&#22270;&#26500;&#24314;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23454;&#20307;&#21644;&#35859;&#35789;&#20505;&#36873;&#38598;&#30340;&#29983;&#25104;&#21644;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25512;&#26029;&#65292;&#24182;&#35774;&#35745;&#20102;&#22270;&#32452;&#35013;&#27169;&#22359;&#25512;&#26029;&#20108;&#20998;&#22270;&#30340;&#36830;&#36890;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22330;&#26223;&#22270;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#22240;&#20854;&#32452;&#21512;&#24615;&#36136;&#32780;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#29702;&#35299;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#26041;&#27861;&#37319;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#20004;&#38454;&#27573;&#25110;&#22522;&#20110;&#28857;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21463;&#21040;&#39640;&#26102;&#38388;&#22797;&#26434;&#24230;&#25110;&#27425;&#20248;&#35774;&#35745;&#30340;&#22256;&#25200;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SGG&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#23558;&#35813;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20108;&#20998;&#22270;&#26500;&#24314;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#26469;&#29983;&#25104;&#23454;&#20307;&#21644;&#23454;&#20307;&#24863;&#30693;&#35859;&#35789;&#20505;&#36873;&#38598;&#65292;&#24182;&#25512;&#26029;&#20986;&#24418;&#25104;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#26377;&#21521;&#36793;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#32452;&#35013;&#27169;&#22359;&#26469;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#20307;&#24863;&#30693;&#32467;&#26500;&#25512;&#26029;&#20986;&#20108;&#20998;&#22270;&#30340;&#36830;&#36890;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#29983;&#25104;&#22330;&#26223;&#22270;&#12290;&#22522;&#20110;&#20108;&#20998;&#22270;&#32452;&#35013;&#33539;&#20363;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#35774;&#35745;&#26469;&#35299;&#20915;&#23454;&#20307;&#24863;&#30693;&#24314;&#27169;&#30340;&#26377;&#25928;&#24615;&#21644;&#22270;&#32452;&#35013;&#30340;&#20248;&#21270;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene Graph Generation (SGG) remains a challenging visual understanding task due to its compositional property. Most previous works adopt a bottom-up, two-stage or point-based, one-stage approach, which often suffers from high time complexity or suboptimal designs. In this work, we propose a novel SGG method to address the aforementioned issues, formulating the task as a bipartite graph construction problem. To address the issues above, we create a transformer-based end-to-end framework to generate the entity and entity-aware predicate proposal set, and infer directed edges to form relation triplets. Moreover, we design a graph assembling module to infer the connectivity of the bipartite scene graph based on our entity-aware structure, enabling us to generate the scene graph in an end-to-end manner. Based on bipartite graph assembling paradigm, we further propose a new technical design to address the efficacy of entity-aware modeling and optimization stability of graph assembling. Equi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#25552;&#21319;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#20248;&#21270;&#23458;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.12830</link><description>&lt;p&gt;
&#25552;&#21319;&#19979;&#19968;&#20010;&#30446;&#30340;&#22320;&#39044;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#33322;&#31354;&#25968;&#25454;&#30340;&#26032;&#39062;LSTM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Next Destination Prediction: A Novel LSTM Approach Using Real-World Airline Data. (arXiv:2401.12830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12830
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#25552;&#21319;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#20248;&#21270;&#23458;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#20132;&#36890;&#34892;&#19994;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#26053;&#34892;&#32773;&#30340;&#19979;&#19968;&#20010;&#30446;&#30340;&#22320;&#20026;&#20844;&#21496;&#24102;&#26469;&#24456;&#22810;&#22909;&#22788;&#65292;&#20363;&#22914;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#21644;&#23450;&#21521;&#33829;&#38144;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#30340;&#26032;&#39062;&#27169;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#20132;&#36890;&#19994;&#20013;&#30340;&#30446;&#30340;&#22320;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#29616;&#21644;&#39640;&#20998;&#25968;&#12290;&#26412;&#30740;&#31350;&#22312;&#25512;&#36827;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#24182;&#20248;&#21270;&#21160;&#24577;&#26053;&#34892;&#29615;&#22659;&#20013;&#30340;&#23458;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the modern transportation industry, accurate prediction of travelers' next destinations brings multiple benefits to companies, such as customer satisfaction and targeted marketing. This study focuses on developing a precise model that captures the sequential patterns and dependencies in travel data, enabling accurate predictions of individual travelers' future destinations. To achieve this, a novel model architecture with a sliding window approach based on Long Short-Term Memory (LSTM) is proposed for destination prediction in the transportation industry. The experimental results highlight satisfactory performance and high scores achieved by the proposed model across different data sizes and performance metrics. This research contributes to advancing destination prediction methods, empowering companies to deliver personalized recommendations and optimize customer experiences in the dynamic travel landscape.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21019;&#24314;&#20102;&#30967;&#21435;&#38500;&#36807;&#31243;&#25511;&#21046;&#30340;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#35797;&#38169;&#26469;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#22120;&#30340;&#24615;&#33021;&#22312;&#26356;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#21463;&#21040;&#27169;&#22411;&#35823;&#24046;&#32047;&#31215;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.12822</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#25311;&#22120;&#29992;&#20110;&#24223;&#27700;&#22788;&#29702;&#20013;&#30967;&#21435;&#38500;&#36807;&#31243;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Simulators for the Phosphorus Removal Process Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms. (arXiv:2401.12822v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21019;&#24314;&#20102;&#30967;&#21435;&#38500;&#36807;&#31243;&#25511;&#21046;&#30340;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#35797;&#38169;&#26469;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#22120;&#30340;&#24615;&#33021;&#22312;&#26356;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#21463;&#21040;&#27169;&#22411;&#35823;&#24046;&#32047;&#31215;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30967;&#21435;&#38500;&#23545;&#20110;&#24223;&#27700;&#22788;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20943;&#23569;&#23545;&#26377;&#38480;&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#26469;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#20248;&#21270;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#24223;&#27700;&#22788;&#29702;&#21378;&#30340;&#22788;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#21270;&#23398;&#21644;&#29983;&#29289;&#36807;&#31243;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38656;&#35201;&#20934;&#30830;&#30340;&#27169;&#25311;&#22120;&#12290;&#26412;&#30740;&#31350;&#35757;&#32451;&#20102;&#20845;&#20010;&#27169;&#22411;&#26469;&#35782;&#21035;&#30967;&#21435;&#38500;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#30340;&#27169;&#25311;&#22120;&#12290;&#34429;&#28982;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24456;&#39640;&#65288;&gt;97%&#65289;&#65292;&#20294;&#19981;&#30830;&#23450;&#24615;&#21644;&#38169;&#35823;&#39044;&#27979;&#34892;&#20026;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#27169;&#25311;&#22120;&#22312;&#26356;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#12290;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#30340;&#32047;&#31215;&#34987;&#30830;&#23450;&#20026;&#36825;&#20010;&#38382;&#39064;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#36825;&#31181;&#25913;&#36827;&#36807;&#31243;&#25511;&#21046;&#30340;&#26041;&#27861;&#28041;&#21450;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21019;&#24314;&#27169;&#25311;&#29615;&#22659;&#65292;&#20351;&#29992;&#30417;&#25511;&#19982;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#65288;SCADA&#65289;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phosphorus removal is vital in wastewater treatment to reduce reliance on limited resources. Deep reinforcement learning (DRL) is a machine learning technique that can optimize complex and nonlinear systems, including the processes in wastewater treatment plants, by learning control policies through trial and error. However, applying DRL to chemical and biological processes is challenging due to the need for accurate simulators. This study trained six models to identify the phosphorus removal process and used them to create a simulator for the DRL environment. Although the models achieved high accuracy (&gt;97%), uncertainty and incorrect prediction behavior limited their performance as simulators over longer horizons. Compounding errors in the models' predictions were identified as one of the causes of this problem. This approach for improving process control involves creating simulation environments for DRL algorithms, using data from supervisory control and data acquisition (SCADA) sys
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23618;&#32465;&#23450;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;&#36873;&#25321;&#23618;&#24182;&#23558;&#23427;&#20204;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#28145;&#24230;Transformer&#32593;&#32476;&#20013;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12819</link><description>&lt;p&gt;
&#21160;&#24577;&#23618;&#32465;&#23450;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Dynamic Layer Tying for Parameter-Efficient Transformers. (arXiv:2401.12819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23618;&#32465;&#23450;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;&#36873;&#25321;&#23618;&#24182;&#23558;&#23427;&#20204;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#26469;&#20943;&#23569;&#28145;&#24230;Transformer&#32593;&#32476;&#20013;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20943;&#23569;&#28145;&#24230;Transformer&#32593;&#32476;&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#35757;&#32451;&#26399;&#38388;&#21160;&#24577;&#36873;&#25321;&#23618;&#24182;&#23558;&#23427;&#20204;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#27599;&#38548;&#19968;&#27573;&#26102;&#38388;&#65292;RL agent&#20250;&#34987;&#35810;&#38382;&#26159;&#21542;&#29420;&#31435;&#35757;&#32451;&#27599;&#20010;&#23618;$i$&#65292;&#36824;&#26159;&#22797;&#21046;&#21069;&#19968;&#20010;&#23618;$j&lt;i$&#30340;&#26435;&#37325;&#12290;&#36825;&#26679;&#20570;&#26377;&#21161;&#20110;&#20849;&#20139;&#26435;&#37325;&#65292;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20063;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#23454;&#39564;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#30053;&#20248;&#20110;&#22522;&#20934;Transformer&#27169;&#22411;&#65292;&#24182;&#19988;&#26174;&#33879;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#30340;&#20869;&#23384;&#28040;&#32791;&#27604;&#24120;&#35268;&#35757;&#32451;&#26041;&#27861;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j&lt;i$. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#26469;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.12806</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Binary structured physics-informed neural networks for solving equations with rapidly changing solutions. (arXiv:2401.12806v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#26469;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;PDEs&#25551;&#36848;&#30340;&#29289;&#29702;&#20449;&#24687;&#23884;&#20837;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;PINNs&#34987;&#35757;&#32451;&#20026;&#26367;&#20195;&#27169;&#22411;&#65292;&#20197;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#32780;&#26080;&#38656;&#26631;&#31614;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;PINNs&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#20123;&#22256;&#38590;&#21253;&#25324;&#25910;&#25947;&#36895;&#24230;&#24930;&#12289;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#35299;&#20915;&#31934;&#24230;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(BsPINN)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;(BsNN)&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#65292;BsPINNs&#22312;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs), rooted in deep learning, have emerged as a promising approach for solving partial differential equations (PDEs). By embedding the physical information described by PDEs into feedforward neural networks, PINNs are trained as surrogate models to approximate solutions without the need for label data. Nevertheless, even though PINNs have shown remarkable performance, they can face difficulties, especially when dealing with equations featuring rapidly changing solutions. These difficulties encompass slow convergence, susceptibility to becoming trapped in local minima, and reduced solution accuracy. To address these issues, we propose a binary structured physics-informed neural network (BsPINN) framework, which employs binary structured neural network (BsNN) as the neural network component. By leveraging a binary structure that reduces inter-neuron connections compared to fully connected neural networks, BsPINNs excel in capturing the local features 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;5G NR PRACH&#25509;&#25910;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20998;&#21035;&#20272;&#35745;RAPID&#21644;TA&#65292;&#19982;&#20256;&#32479;&#30456;&#20851;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12803</link><description>&lt;p&gt;
5G NR PRACH&#25509;&#25910;&#30340;&#22686;&#24378;&#65306;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancements for 5G NR PRACH Reception: An AI/ML Approach. (arXiv:2401.12803v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12803
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;5G NR PRACH&#25509;&#25910;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20998;&#21035;&#20272;&#35745;RAPID&#21644;TA&#65292;&#19982;&#20256;&#32479;&#30456;&#20851;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#25509;&#20837;&#26159;&#20351;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#19982;&#22522;&#31449;&#65288;gNB&#65289;&#36827;&#34892;&#26368;&#21021;&#36830;&#25509;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;UE&#36890;&#36807;&#23884;&#20837;&#22312;&#24050;&#30693;&#22522;&#24207;&#21015;&#30340;&#30456;&#20301;&#26059;&#36716;&#20013;&#30340;&#21069;&#23548;&#32034;&#24341;&#65288;RAPID&#65289;&#26469;&#36827;&#34892;&#33258;&#25105;&#35782;&#21035;&#65292;&#24182;&#23558;&#20854;&#21457;&#36865;&#21040;&#29289;&#29702;&#38543;&#26426;&#25509;&#20837;&#36890;&#36947;&#65288;PRACH&#65289;&#12290;PRACH&#19978;&#30340;&#20449;&#21495;&#36824;&#33021;&#22815;&#20272;&#35745;&#30001;UE&#20301;&#32622;&#24341;&#36215;&#30340;&#20256;&#25773;&#26102;&#24310;&#65288;&#24120;&#31216;&#20026;&#26102;&#38388;&#25552;&#21069;&#65292;TA&#65289;&#12290;&#20256;&#32479;&#25509;&#25910;&#22120;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;RAPID&#21644;TA&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;AI/ML&#27169;&#22411;&#30340;&#26367;&#20195;&#25509;&#25910;&#22120;&#26041;&#27861;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#19968;&#20010;&#29992;&#20110;RAPID&#65292;&#19968;&#20010;&#29992;&#20110;TA&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#19981;&#21516;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#21487;&#20197;&#24182;&#34892;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#39034;&#24207;&#36816;&#34892;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#21644;&#23454;&#38469;&#30828;&#20214;&#25429;&#33719;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;AI/ML&#30340;&#25216;&#26415;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#30456;&#20851;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Access is an important step in enabling the initial attachment of a User Equipment (UE) to a Base Station (gNB). The UE identifies itself by embedding a Preamble Index (RAPID) in the phase rotation of a known base sequence, which it transmits on the Physical Random Access Channel (PRACH). The signal on the PRACH also enables the estimation of propagation delay, often known as Timing Advance (TA), which is induced by virtue of the UE's position. Traditional receivers estimate the RAPID and TA using correlation-based techniques. This paper presents an alternative receiver approach that uses AI/ML models, wherein two neural networks are proposed, one for the RAPID and one for the TA. Different from other works, these two models can run in parallel as opposed to sequentially. Experiments with both simulated data and over-the-air hardware captures highlight the improved performance of the proposed AI/ML-based techniques compared to conventional correlation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#33258;2017&#24180;&#33267;2023&#24180;&#26399;&#38388;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22788;&#29702;&#20809;&#30005;&#23481;&#31215;&#27861;&#25968;&#25454;&#30340;&#35770;&#25991;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20010;&#20154;&#20581;&#24247;&#31649;&#29702;&#21644;&#20854;&#20182;&#24212;&#29992;&#20013;&#20855;&#26377;&#26174;&#33879;&#25104;&#26524;&#12290;&#26681;&#25454;&#20219;&#21153;&#30340;&#19981;&#21516;&#65292;&#36825;&#20123;&#35770;&#25991;&#34987;&#20998;&#20026;&#21307;&#23398;&#30456;&#20851;&#21644;&#38750;&#21307;&#23398;&#30456;&#20851;&#20004;&#22823;&#31867;&#21035;&#65292;&#21307;&#23398;&#30456;&#20851;&#21448;&#32454;&#20998;&#20026;&#19971;&#20010;&#23376;&#32452;&#65292;&#21253;&#25324;&#34880;&#21387;&#20998;&#26512;...</title><link>http://arxiv.org/abs/2401.12783</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20809;&#30005;&#23481;&#31215;&#27861;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Deep Learning Methods for Photoplethysmography Data. (arXiv:2401.12783v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#33258;2017&#24180;&#33267;2023&#24180;&#26399;&#38388;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22788;&#29702;&#20809;&#30005;&#23481;&#31215;&#27861;&#25968;&#25454;&#30340;&#35770;&#25991;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20010;&#20154;&#20581;&#24247;&#31649;&#29702;&#21644;&#20854;&#20182;&#24212;&#29992;&#20013;&#20855;&#26377;&#26174;&#33879;&#25104;&#26524;&#12290;&#26681;&#25454;&#20219;&#21153;&#30340;&#19981;&#21516;&#65292;&#36825;&#20123;&#35770;&#25991;&#34987;&#20998;&#20026;&#21307;&#23398;&#30456;&#20851;&#21644;&#38750;&#21307;&#23398;&#30456;&#20851;&#20004;&#22823;&#31867;&#21035;&#65292;&#21307;&#23398;&#30456;&#20851;&#21448;&#32454;&#20998;&#20026;&#19971;&#20010;&#23376;&#32452;&#65292;&#21253;&#25324;&#34880;&#21387;&#20998;&#26512;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#31215;&#27861;&#65288;PPG&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#21069;&#26223;&#30340;&#35774;&#22791;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#20415;&#25658;&#24615;&#12289;&#29992;&#25143;&#21451;&#22909;&#25805;&#20316;&#21644;&#38750;&#20405;&#20837;&#24615;&#27979;&#37327;&#22810;&#31181;&#29983;&#29702;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#21033;&#29992;PPG&#20449;&#21495;&#65292;&#23637;&#31034;&#20102;&#22312;&#20010;&#20154;&#20581;&#24247;&#31649;&#29702;&#21644;&#20854;&#20182;&#22810;&#26041;&#38754;&#24212;&#29992;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#33258;2017&#24180;1&#26376;1&#26085;&#33267;2023&#24180;7&#26376;31&#26085;&#26399;&#38388;&#22312;Google&#23398;&#26415;&#12289;PubMed&#21644;Dimensions&#21457;&#34920;&#30340;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22788;&#29702;PPG&#25968;&#25454;&#30340;&#35770;&#25991;&#12290;&#27599;&#31687;&#35770;&#25991;&#20174;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#19977;&#20010;&#20851;&#38190;&#35282;&#24230;&#36827;&#34892;&#20998;&#26512;&#12290;&#26368;&#32456;&#25552;&#21462;&#20102;193&#31687;&#35770;&#25991;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#22788;&#29702;PPG&#20449;&#21495;&#12290;&#26681;&#25454;&#36825;&#20123;&#35770;&#25991;&#25152;&#28041;&#21450;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20998;&#20026;&#20004;&#22823;&#31867;&#21035;&#65306;&#21307;&#23398;&#30456;&#20851;&#21644;&#38750;&#21307;&#23398;&#30456;&#20851;&#12290;&#21307;&#23398;&#30456;&#20851;&#20219;&#21153;&#36827;&#19968;&#27493;&#20998;&#20026;&#19971;&#20010;&#23376;&#32452;&#65292;&#21253;&#25324;&#34880;&#21387;&#20998;&#26512;...
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) is a highly promising device due to its advantages in portability, user-friendly operation, and non-invasive capabilities to measure a wide range of physiological information. Recent advancements in deep learning have demonstrated remarkable outcomes by leveraging PPG signals for tasks related to personal health management and other multifaceted applications. In this review, we systematically reviewed papers that applied deep learning models to process PPG data between January 1st of 2017 and July 31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed from three key perspectives: tasks, models, and data. We finally extracted 193 papers where different deep learning frameworks were used to process PPG signals. Based on the tasks addressed in these papers, we categorized them into two major groups: medical-related, and non-medical-related. The medical-related tasks were further divided into seven subgroups, including blood pressure anal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26415;&#20013;&#30913;&#20849;&#25391;&#25104;&#20687;&#37325;&#24314;&#26041;&#27861;&#22312;&#33041;&#32959;&#30244;&#25163;&#26415;&#20013;&#30340;&#36136;&#37327;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#22312;&#36136;&#37327;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.12771</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26415;&#20013;&#30913;&#20849;&#25391;&#25104;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Intraoperative MRI Reconstruction. (arXiv:2401.12771v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26415;&#20013;&#30913;&#20849;&#25391;&#25104;&#20687;&#37325;&#24314;&#26041;&#27861;&#22312;&#33041;&#32959;&#30244;&#25163;&#26415;&#20013;&#30340;&#36136;&#37327;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#22312;&#36136;&#37327;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#29992;&#20110;&#26415;&#20013;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;iMRI&#65289;&#22312;&#33041;&#32959;&#30244;&#25163;&#26415;&#20013;&#30340;&#36136;&#37327;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#22312;&#33041;&#25163;&#26415;&#20013;&#20351;&#29992;&#25918;&#32622;&#22312;&#20999;&#38500;&#21306;&#22495;&#21608;&#22260;&#30340;&#21452;&#34920;&#38754;&#32447;&#22280;&#36827;&#34892;&#21152;&#36895;iMRI&#12290;&#20351;&#29992;fastMRI&#31070;&#32463;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#20197;&#27169;&#25311;iMRI&#21327;&#35758;&#30340;&#25968;&#25454;&#12290;&#35780;&#20272;&#20102;40&#21517;&#22312;2021&#24180;11&#26376;1&#26085;&#33267;2023&#24180;6&#26376;1&#26085;&#26399;&#38388;&#36827;&#34892;&#33041;&#32959;&#30244;&#20999;&#38500;&#25163;&#26415;&#26399;&#38388;&#36827;&#34892;iMRI&#30340;&#24739;&#32773;&#30340;&#25104;&#20687;&#26448;&#26009;&#12290;&#22312;&#20256;&#32479;&#30340;&#21387;&#32553;&#24863;&#30693;&#65288;CS&#65289;&#26041;&#27861;&#21644;&#35757;&#32451;&#36807;&#30340;DL&#37325;&#24314;&#26041;&#27861;&#20043;&#38388;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#20004;&#20301;&#31070;&#32463;&#25918;&#23556;&#31185;&#21307;&#24072;&#21644;&#19968;&#20301;&#31070;&#32463;&#22806;&#31185;&#21307;&#29983;&#20197;1&#33267;5&#30340;Likert&#37327;&#34920;&#65288;1=&#38750;&#35786;&#26029;&#12289;2=&#24046;&#12289;3=&#21487;&#25509;&#21463;&#12289;4=&#22909;&#12289;5=&#20248;&#31168;&#65289;&#36827;&#34892;&#20102;&#30450;&#35780;&#20272;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#37325;&#24314;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To evaluate the quality of deep learning reconstruction for prospectively accelerated intraoperative magnetic resonance imaging (iMRI) during resective brain tumor surgery.  Materials and Methods: Accelerated iMRI was performed during brain surgery using dual surface coils positioned around the area of resection. A deep learning (DL) model was trained on the fastMRI neuro dataset to mimic the data from the iMRI protocol. Evaluation was performed on imaging material from 40 patients imaged between 01.11.2021 - 01.06.2023 that underwent iMRI during tumor resection surgery. A comparative analysis was conducted between the conventional compressed sense (CS) method and the trained DL reconstruction method. Blinded evaluation of multiple image quality metrics was performed by two working neuro-radiologists and a working neurosurgeon on a 1 to 5 Likert scale (1=non diagnostic, 2=poor, 3=acceptable, 4=good, 5=excellent), and the favored reconstruction variant.  Results: The DL reconst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21508;&#31181;&#21464;&#21270;&#12290;&#20197;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#20026;&#22330;&#26223;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.12756</link><description>&lt;p&gt;
What the Weight?! &#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition. (arXiv:2401.12756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21508;&#31181;&#21464;&#21270;&#12290;&#20197;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#20026;&#22330;&#26223;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20013;&#25152;&#23553;&#35013;&#30340;&#30693;&#35782;&#26159;&#30830;&#23450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26368;&#32456;&#24615;&#33021;&#30340;&#26680;&#24515;&#22240;&#32032;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35768;&#22810;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#23384;&#20648;&#21644;&#35843;&#25972;&#19981;&#21516;&#31867;&#22411;&#30693;&#35782;&#30340;&#26377;&#25928;&#26041;&#27861;&#19978;&#65292;&#20363;&#22914;&#22312;&#19987;&#29992;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#39069;&#22806;&#30340;&#21442;&#25968;&#26469;&#26377;&#25928;&#22320;&#32452;&#21512;&#36825;&#20123;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#23384;&#22312;&#35768;&#22810;&#21487;&#33021;&#30340;&#36873;&#39033;&#65292;&#23545;&#20110;&#36825;&#20123;&#32452;&#21512;&#20013;&#28041;&#21450;&#30340;&#26426;&#21046;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#22240;&#27492;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#24212;&#35813;&#20351;&#29992;&#21738;&#20123;&#31574;&#30053;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#23427;&#28085;&#30422;&#20102;&#29616;&#26377;&#30340;&#19968;&#20123;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21464;&#21270;&#65292;&#32479;&#19968;&#20102;&#36825;&#20123;&#27010;&#24565;&#12290;&#22312;&#32858;&#28966;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#30340;&#24773;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#39318;&#27425;&#20840;&#38754;&#30340;&#21508;&#31181;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The knowledge encapsulated in a model is the core factor determining its final performance on downstream tasks. Much research in NLP has focused on efficient methods for storing and adapting different types of knowledge, e.g., in dedicated modularized structures, and on how to effectively combine these, e.g., by learning additional parameters. However, given the many possible options, a thorough understanding of the mechanisms involved in these compositions is missing, and hence it remains unclear which strategies to utilize. To address this research gap, we propose a novel framework for zero-shot module composition, which encompasses existing and some novel variations for selecting, weighting, and combining parameter modules under a single unified notion. Focusing on the scenario of domain knowledge and adapter layers, our framework provides a systematic unification of concepts, allowing us to conduct the first comprehensive benchmarking study of various zero-shot knowledge compositio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.12731</link><description>&lt;p&gt;
SHAP&#35780;&#20998;&#22312;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Distributional Uncertainty of the SHAP score in Explainable Machine Learning. (arXiv:2401.12731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#23646;&#20998;&#25968;&#21453;&#26144;&#20102;&#36755;&#20837;&#23454;&#20307;&#20013;&#30340;&#29305;&#24449;&#20540;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#35780;&#20998;&#20043;&#19968;&#26159;SHAP&#35780;&#20998;&#65292;&#23427;&#26159;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;&#35813;&#35780;&#20998;&#30340;&#23450;&#20041;&#20381;&#36182;&#20110;&#23454;&#20307;&#32676;&#20307;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#36890;&#24120;&#19981;&#30693;&#36947;&#31934;&#30830;&#30340;&#20998;&#24067;&#65292;&#22240;&#27492;&#38656;&#35201;&#20027;&#35266;&#22320;&#36827;&#34892;&#20998;&#37197;&#25110;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#30693;&#36947;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#30340;SHAP&#35780;&#20998;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21253;&#21547;&#28508;&#22312;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#32780;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#25104;&#20026;&#22312;&#35813;&#21306;&#22495;&#19978;&#23450;&#20041;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25214;&#21040;&#35813;&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attribution scores reflect how important the feature values in an input entity are for the output of a machine learning model. One of the most popular attribution scores is the SHAP score, which is an instantiation of the general Shapley value used in coalition game theory. The definition of this score relies on a probability distribution on the entity population. Since the exact distribution is generally unknown, it needs to be assigned subjectively or be estimated from data, which may lead to misleading feature scores. In this paper, we propose a principled framework for reasoning on SHAP scores under unknown entity population distributions. In our framework, we consider an uncertainty region that contains the potential distributions, and the SHAP score of a feature becomes a function defined over this region. We study the basic problems of finding maxima and minima of this function, which allows us to determine tight ranges for the SHAP scores of all features. In particular, we pinp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#21487;&#32500;&#25252;&#24615;&#65292;&#21457;&#29616;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27010;&#29575;&#19982;&#24403;&#21069;&#20195;&#30721;&#21487;&#33021;&#20855;&#26377;&#30340;&#27010;&#29575;&#21487;&#20197;&#25351;&#31034;&#28508;&#22312;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#19988;&#20132;&#21449;&#29109;&#26159;&#21487;&#32500;&#25252;&#24615;&#30340;&#39044;&#27979;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2401.12714</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#21487;&#32500;&#25252;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluation of large language models for assessing code maintainability. (arXiv:2401.12714v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#21487;&#32500;&#25252;&#24615;&#65292;&#21457;&#29616;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27010;&#29575;&#19982;&#24403;&#21069;&#20195;&#30721;&#21487;&#33021;&#20855;&#26377;&#30340;&#27010;&#29575;&#21487;&#20197;&#25351;&#31034;&#28508;&#22312;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#19988;&#20132;&#21449;&#29109;&#26159;&#21487;&#32500;&#25252;&#24615;&#30340;&#39044;&#27979;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#36719;&#20214;&#20179;&#24211;&#30340;&#21487;&#29992;&#24615;&#22686;&#21152;&#20197;&#21450;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20195;&#30721;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#24341;&#21457;&#20102;&#19968;&#31995;&#21015;&#23581;&#35797;&#33258;&#21160;&#21270;&#20043;&#21069;&#38750;&#24120;&#38590;&#20197;&#33258;&#21160;&#21270;&#30340;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#30340;&#26032;&#24037;&#20316;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#65292;&#20551;&#35774;&#36890;&#36807;&#27604;&#36739;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27010;&#29575;&#19982;&#24403;&#21069;&#20195;&#30721;&#21487;&#33021;&#20855;&#26377;&#30340;&#27010;&#29575;&#21487;&#20197;&#25351;&#31034;&#28508;&#22312;&#30340;&#36136;&#37327;&#38382;&#39064;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#21313;&#31181;&#19981;&#21516;&#27169;&#22411;&#65288;&#22522;&#20110;GPT2&#21644;Llama2&#65289;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#20132;&#21449;&#29109;&#19982;&#21487;&#35835;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#12289;&#22797;&#26434;&#24615;&#12289;&#27169;&#22359;&#21270;&#20197;&#21450;&#30001;&#19987;&#23478;&#35780;&#20272;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#21487;&#29992;&#30340;&#24635;&#20307;&#21487;&#32500;&#25252;&#24615;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25511;&#21046;&#36923;&#36753;&#20195;&#30721;&#34892;&#25968;&#65288;LLOC&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;LLMs&#35745;&#31639;&#24471;&#21040;&#30340;&#20132;&#21449;&#29109;&#30830;&#23454;&#26159;&#31867;&#32423;&#21035;&#19978;&#21487;&#32500;&#25252;&#24615;&#30340;&#39044;&#27979;&#22240;&#23376;&#65288;&#20132;&#21449;&#29109;&#36234;&#39640;&#65292;&#21487;&#32500;&#25252;&#24615;&#36234;&#20302;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increased availability of open-source software repositories and recent advances in code analysis using large language models (LLMs) has triggered a wave of new work to automate software engineering tasks that were previously very difficult to automate. In this paper, we investigate a recent line of work that hypothesises that comparing the probability of code generated by LLMs with the probability the current code would have had can indicate potential quality problems. We investigate the association between the cross-entropy of code generated by ten different models (based on GPT2 and Llama2) and the following quality aspects: readability, understandability, complexity, modularisation, and overall maintainability assessed by experts and available in an benchmark dataset. Our results show that, controlling for the number of logical lines of codes (LLOC), cross-entropy computed by LLMs is indeed a predictor of maintainability on a class level (the higher the cross-entropy the lower the m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#30340;&#26159;&#35774;&#35745;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#26469;&#24179;&#34913;&#34987;&#25298;&#32477;&#30340;&#39044;&#27979;&#27604;&#20363;&#21644;&#25152;&#36873;&#39044;&#27979;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12708</link><description>&lt;p&gt;
&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Network Benchmarks for Selective Classification. (arXiv:2401.12708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#30340;&#26159;&#35774;&#35745;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#26469;&#24179;&#34913;&#34987;&#25298;&#32477;&#30340;&#39044;&#27979;&#27604;&#20363;&#21644;&#25152;&#36873;&#39044;&#27979;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20855;&#26377;&#31038;&#20250;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#30340;&#37096;&#32626;&#22686;&#21152;&#65292;&#23545;&#21487;&#38752;&#21644;&#21487;&#20449;&#39044;&#27979;&#30340;&#38656;&#27714;&#20063;&#26085;&#30410;&#22686;&#38271;&#12290;&#23454;&#29616;&#36825;&#20123;&#35201;&#27714;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20801;&#35768;&#27169;&#22411;&#22312;&#23384;&#22312;&#39640;&#38169;&#35823;&#39118;&#38505;&#26102;&#25918;&#24323;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#38656;&#35201;&#20026;&#27169;&#22411;&#28155;&#21152;&#36873;&#25321;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#36873;&#25321;&#27169;&#22411;&#23558;&#25552;&#20379;&#39044;&#27979;&#30340;&#20363;&#23376;&#12290;&#36873;&#25321;&#24615;&#20998;&#31867;&#26694;&#26550;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#24179;&#34913;&#34987;&#25298;&#32477;&#39044;&#27979;&#27604;&#20363;&#65288;&#21363;&#27169;&#22411;&#19981;&#36827;&#34892;&#39044;&#27979;&#30340;&#20363;&#23376;&#27604;&#20363;&#65289;&#19982;&#22312;&#25152;&#36873;&#39044;&#27979;&#19978;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#20043;&#38388;&#30340;&#26426;&#21046;&#12290;&#23384;&#22312;&#22810;&#20010;&#36873;&#25321;&#24615;&#20998;&#31867;&#26694;&#26550;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#20173;&#23616;&#38480;&#20110;&#37096;&#20998;&#26041;&#27861;&#21644;&#35774;&#32622;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#32473;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing deployment of machine learning models in many socially-sensitive tasks, there is a growing demand for reliable and trustworthy predictions. One way to accomplish these requirements is to allow a model to abstain from making a prediction when there is a high risk of making an error. This requires adding a selection mechanism to the model, which selects those examples for which the model will provide a prediction. The selective classification framework aims to design a mechanism that balances the fraction of rejected predictions (i.e., the proportion of examples for which the model does not make a prediction) versus the improvement in predictive performance on the selected predictions. Multiple selective classification frameworks exist, most of which rely on deep neural network architectures. However, the empirical evaluation of the existing approaches is still limited to partial comparisons among methods and settings, providing practitioners with little insight into 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#25968;&#25454;&#22788;&#29702;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#29616;&#26377;&#25915;&#20987;&#23041;&#32961;&#19981;&#36275;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2401.12700</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#35757;&#32451;&#26469;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Securing Recommender System via Cooperative Training. (arXiv:2401.12700v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#25968;&#25454;&#22788;&#29702;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#29616;&#26377;&#25915;&#20987;&#23041;&#32961;&#19981;&#36275;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#31934;&#24515;&#21046;&#20316;&#30340;&#34394;&#20551;&#20010;&#20154;&#36164;&#26009;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#23384;&#22312;&#20559;&#35265;&#12290;&#22312;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#25968;&#25454;&#22788;&#29702;&#30340;&#26041;&#27861;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#25490;&#38500;&#27491;&#24120;&#26679;&#26412;&#65292;&#32780;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#21017;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#22788;&#29702;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;Triple Cooperative Defense&#65288;TCD&#65289;&#65292;&#23427;&#37319;&#29992;&#19977;&#20010;&#21327;&#21516;&#27169;&#22411;&#30456;&#20114;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#29616;&#26377;&#25915;&#20987;&#26041;&#27861;&#22312;&#24179;&#34913;&#21452;&#23618;&#20248;&#21270;&#21644;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25915;&#20987;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;Co-training Attack&#65288;Co-Attack&#65289;&#65292;&#35813;&#31574;&#30053;&#22312;&#32771;&#34385;&#21452;&#23618;&#35774;&#32622;&#30340;&#21516;&#26102;&#65292;&#21327;&#21516;&#20248;&#21270;&#25915;&#20987;&#20248;&#21270;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#20445;&#25345;&#25915;&#20987;&#30340;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29616;&#26377;&#25915;&#20987;&#23041;&#32961;&#19981;&#36275;&#30340;&#19968;&#20010;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are often susceptible to well-crafted fake profiles, leading to biased recommendations. Among existing defense methods, data-processing-based methods inevitably exclude normal samples, while model-based methods struggle to enjoy both generalization and robustness. To this end, we suggest integrating data processing and the robust model to propose a general framework, Triple Cooperative Defense (TCD), which employs three cooperative models that mutually enhance data and thereby improve recommendation robustness. Furthermore, Considering that existing attacks struggle to balance bi-level optimization and efficiency, we revisit poisoning attacks in recommender systems and introduce an efficient attack strategy, Co-training Attack (Co-Attack), which cooperatively optimizes the attack optimization and model training, considering the bi-level setting while maintaining attack efficiency. Moreover, we reveal a potential reason for the insufficient threat of existing attacks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12689</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#12289;&#20551;&#35774;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#24182;&#19981;&#24120;&#35265;&#12290;&#33258;&#21160;&#27169;&#22411;&#35780;&#20272;&#65288;AutoEval&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#24037;&#20316;&#27969;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#19968;&#20010;&#25509;&#36817;&#39044;&#27979;&#24615;&#33021;&#30340;&#27979;&#35797;&#31649;&#32447;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#31614;&#30340;&#23384;&#22312;&#12290;&#23613;&#31649;AutoEval&#26694;&#26550;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26041;&#24335;&#8212;&#8212;&#20803;&#20998;&#24067;&#33021;&#37327;&#65288;MDE&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;AutoEval&#26694;&#26550;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;MDE&#30340;&#26680;&#24515;&#26159;&#24314;&#31435;&#19968;&#20010;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#65288;&#33021;&#37327;&#65289;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#33021;&#37327;&#30340;&#23398;&#20064;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;MDE&#19982;&#20998;&#31867;&#25439;&#22833;&#30456;&#36830;&#25509;&#65292;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#25454;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#25193;&#23637;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#31232;&#30095;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.12686</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65306;&#19968;&#31181;&#28151;&#21512;&#22270;&#24418;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach. (arXiv:2401.12686v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12686
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22270;&#19978;&#23398;&#20064;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#25193;&#23637;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#31232;&#30095;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22823;&#35268;&#27169;&#20195;&#29702;&#32676;&#20307;&#30340;&#34892;&#20026;&#26159;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#34429;&#28982;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#22312;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#20195;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#19988;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65288;MFGs&#65289;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#20195;&#29702;&#20043;&#38388;&#30340;&#32593;&#32476;&#32467;&#26500;&#30340;&#22270;&#24418;&#24179;&#22343;&#22330;&#23545;&#23616;&#21338;&#24328;&#65288;GMFGs&#65289;&#12290;&#23613;&#31649;&#20855;&#26377;&#35832;&#22810;&#20248;&#28857;&#65292;&#20294;GMFGs&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#21463;&#21040;&#22270;&#24418;&#21482;&#33021;&#25429;&#25417;&#23494;&#38598;&#22270;&#30340;&#38480;&#21046;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#23454;&#39564;&#35777;&#26126;&#30340;&#32593;&#32476;&#26174;&#31034;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#31232;&#30095;&#24615;&#65292;&#20363;&#22914;&#24130;&#24459;&#22270;&#65292;&#22240;&#27492;GMFG&#26694;&#26550;&#26080;&#27861;&#25429;&#25417;&#36825;&#20123;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#23545;&#23616;&#21338;&#24328;&#65288;GXMFGs&#65289;&#30340;&#27010;&#24565;&#65292;&#23427;&#24314;&#31435;&#22312;&#22270;&#35770;&#27010;&#24565;&#22270;&#24418;&#25193;&#23637;&#65288;graphexes&#65289;&#22522;&#30784;&#19978;&#12290;&#22270;&#24418;&#25193;&#23637;&#26159;&#31232;&#30095;&#22270;&#24207;&#21015;&#30340;&#26497;&#38480;&#23545;&#35937;&#65292;&#36824;&#20855;&#26377;&#20854;&#20182;&#19968;&#20123;&#29702;&#24819;&#29305;&#24615;&#65292;&#22914;sma
&lt;/p&gt;
&lt;p&gt;
Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the sma
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#24615;&#20856;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;Kriging&#36807;&#31243;&#20013;&#37051;&#23621;&#21644;&#38750;&#37051;&#23621;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#23646;&#24615;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12681</link><description>&lt;p&gt;
Kriging &#30340;&#25193;&#23637;:&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#24615;&#20856;&#22411;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical Learning. (arXiv:2401.12681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12681
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#24615;&#20856;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;Kriging&#36807;&#31243;&#20013;&#37051;&#23621;&#21644;&#38750;&#37051;&#23621;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#23646;&#24615;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kriging&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#31354;&#38388;&#37051;&#36817;&#25110;&#29289;&#29702;&#36830;&#25509;&#20013;&#30340;&#35266;&#27979;&#20540;&#26469;&#20272;&#35745;&#26410;&#37319;&#26679;&#22320;&#29702;&#20301;&#32622;&#30340;&#23646;&#24615;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20943;&#36731;&#30001;&#20110;&#37096;&#32626;&#19981;&#36275;&#30340;&#20256;&#24863;&#22120;&#24341;&#36215;&#30340;&#30417;&#27979;&#20559;&#24046;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20551;&#35774;&#37051;&#23621;&#30340;&#20449;&#24687;&#20026;&#20272;&#35745;&#26410;&#35266;&#23519;&#21040;&#30340;&#30446;&#26631;&#30340;&#23646;&#24615;&#25552;&#20379;&#22522;&#30784;&#65292;&#32780;&#24573;&#30053;&#20102;&#38750;&#37051;&#23621;&#12290;&#28982;&#32780;&#65292;&#38750;&#37051;&#23621;&#20063;&#21487;&#20197;&#25552;&#20379;&#26377;&#30410;&#30340;&#20449;&#24687;&#65292;&#32780;&#37051;&#23621;&#20063;&#21487;&#33021;&#26159;&#35823;&#23548;&#24615;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23545;&#27604;&#24615;&#20856;&#22411;&#8221;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#37051;&#23621;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#24182;&#22238;&#25910;&#38750;&#37051;&#23621;&#30340;&#20449;&#24687;&#12290;&#20316;&#20026;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#25105;&#20204;&#20174;&#34920;&#31034;&#30340;&#26032;&#35270;&#35282;&#36827;&#34892;Kriging&#20219;&#21153;&#65306;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20174;&#34920;&#31034;&#20013;&#24674;&#22797;&#23646;&#24615;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#37051;&#23621;&#23545;&#27604;&#27169;&#22359;&#65292;&#36890;&#36807;&#32553;&#23567;&#30446;&#26631;&#19982;&#37051;&#23621;&#30340;&#34920;&#31034;&#36317;&#31163;&#65292;&#31895;&#30053;&#22320;&#23398;&#20064;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kriging aims at estimating the attributes of unsampled geo-locations from observations in the spatial vicinity or physical connections, which helps mitigate skewed monitoring caused by under-deployed sensors. Existing works assume that neighbors' information offers the basis for estimating the attributes of the unobserved target while ignoring non-neighbors. However, non-neighbors could also offer constructive information, and neighbors could also be misleading. To this end, we propose ``Contrastive-Prototypical'' self-supervised learning for Kriging (KCP) to refine valuable information from neighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we conduct the Kriging task from a new perspective of representation: we aim to first learn robust and general representations and then recover attributes from representations. A neighboring contrastive module is designed that coarsely learns the representations by narrowing the representation distance between the target a
&lt;/p&gt;</description></item><item><title>ChatGraph&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#22270;&#36827;&#34892;&#20132;&#20114;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#31616;&#21333;&#21644;&#28789;&#27963;&#12290;ChatGraph&#36890;&#36807;&#29983;&#25104;&#22270;&#20998;&#26512;API&#38142;&#26469;&#23454;&#29616;&#23545;&#29992;&#25143;&#36755;&#20837;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.12672</link><description>&lt;p&gt;
ChatGraph: &#19982;&#22270;&#36827;&#34892;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
ChatGraph: Chat with Your Graphs. (arXiv:2401.12672v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12672
&lt;/p&gt;
&lt;p&gt;
ChatGraph&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#22270;&#36827;&#34892;&#20132;&#20114;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#31616;&#21333;&#21644;&#28789;&#27963;&#12290;ChatGraph&#36890;&#36807;&#29983;&#25104;&#22270;&#20998;&#26512;API&#38142;&#26469;&#23454;&#29616;&#23545;&#29992;&#25143;&#36755;&#20837;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#26512;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#26159;&#22522;&#30784;&#24615;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#31867;&#20284;SPARQL&#30340;&#35821;&#35328;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#28857;&#20987;&#21644;&#25302;&#21160;&#24335;&#30340;&#30028;&#38754;&#19982;&#22270;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#35201;&#27714;&#29992;&#25143;&#20855;&#22791;&#39640;&#32423;&#32534;&#31243;&#25216;&#33021;&#65292;&#35201;&#20040;&#20165;&#25903;&#25345;&#26377;&#38480;&#33539;&#22260;&#30340;&#22270;&#20998;&#26512;&#21151;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;ChatGraph&#12290;&#36890;&#36807;ChatGraph&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#22270;&#36827;&#34892;&#20132;&#20114;&#65292;&#20351;&#24471;&#20351;&#29992;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#31616;&#21333;&#21644;&#28789;&#27963;&#12290;ChatGraph&#30340;&#26680;&#24515;&#22312;&#20110;&#22522;&#20110;&#23545;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#29702;&#35299;&#29983;&#25104;&#22270;&#20998;&#26512;API&#38142;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;ChatGraph&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#65306;API&#26816;&#32034;&#27169;&#22359;&#29992;&#20110;&#25628;&#32034;&#30456;&#20851;API&#65292;&#22270;&#20998;&#26512;LLM&#27169;&#22359;&#20351;LLM&#33021;&#22815;&#29702;&#35299;&#22270;&#25968;&#25454;&#65292;&#20197;&#21450;API&#38142;&#23548;&#21521;&#30340;&#24494;&#35843;&#27169;&#22359;&#29992;&#20110;&#25351;&#23548;LLM&#29983;&#25104;API&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph analysis is fundamental in real-world applications. Traditional approaches rely on SPARQL-like languages or clicking-and-dragging interfaces to interact with graph data. However, these methods either require users to possess high programming skills or support only a limited range of graph analysis functionalities. To address the limitations, we propose a large language model (LLM)-based framework called ChatGraph. With ChatGraph, users can interact with graphs through natural language, making it easier to use and more flexible than traditional approaches. The core of ChatGraph lies in generating chains of graph analysis APIs based on the understanding of the texts and graphs inputted in the user prompts. To achieve this, ChatGraph consists of three main modules: an API retrieval module that searches for relevant APIs, a graph-aware LLM module that enables the LLM to comprehend graphs, and an API chain-oriented finetuning module that guides the LLM in generating API chains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;EL-VIT&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#31995;&#32479;&#65292;&#26088;&#22312;&#25506;&#27979;&#35270;&#35273;&#21464;&#21387;&#22120;&#24182;&#20419;&#36827;&#23545;&#20854;&#25805;&#20316;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.12666</link><description>&lt;p&gt;
EL-VIT: &#20351;&#29992;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#25506;&#27979;&#35270;&#35273;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
EL-VIT: Probing Vision Transformer with Interactive Visualization. (arXiv:2401.12666v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EL-VIT&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#31995;&#32479;&#65292;&#26088;&#22312;&#25506;&#27979;&#35270;&#35273;&#21464;&#21387;&#22120;&#24182;&#20419;&#36827;&#23545;&#20854;&#25805;&#20316;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViT&#65289;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;ViT&#30340;&#27169;&#22411;&#26550;&#26500;&#22797;&#26434;&#65292;&#24448;&#24448;&#38590;&#20197;&#29702;&#35299;&#65292;&#23548;&#33268;&#23398;&#20064;&#26354;&#32447;&#38497;&#23789;&#12290;ViT&#30340;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#32463;&#24120;&#36935;&#21040;&#35299;&#37322;&#20854;&#20869;&#37096;&#24037;&#20316;&#30340;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#21487;&#35270;&#21270;&#31995;&#32479;&#26469;&#24110;&#21161;ViT&#29992;&#25143;&#29702;&#35299;&#20854;&#21151;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EL-VIT&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#21487;&#35270;&#20998;&#26512;&#31995;&#32479;&#65292;&#26088;&#22312;&#25506;&#27979;&#35270;&#35273;&#21464;&#21387;&#22120;&#24182;&#20419;&#36827;&#23545;&#20854;&#25805;&#20316;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;&#35813;&#31995;&#32479;&#21253;&#25324;&#22235;&#20010;&#23618;&#27425;&#30340;&#21487;&#35270;&#21270;&#35270;&#22270;&#12290;&#21069;&#19977;&#20010;&#23618;&#27425;&#21253;&#25324;&#27169;&#22411;&#27010;&#36848;&#12289;&#30693;&#35782;&#32972;&#26223;&#22270;&#21644;&#27169;&#22411;&#35814;&#32454;&#35270;&#22270;&#12290;&#36825;&#19977;&#20010;&#23618;&#27425;&#20174;&#24635;&#20307;&#27169;&#22411;&#26550;&#26500;&#12289;&#35814;&#32454;&#35299;&#37322;&#21644;&#25968;&#23398;&#25805;&#20316;&#30340;&#19977;&#20010;&#35282;&#24230;&#38416;&#26126;&#20102;ViT&#30340;&#25805;&#20316;&#36807;&#31243;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#29702;&#35299;&#20854;&#22522;&#26412;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, Vision Transformer (ViT) is widely utilized in various computer vision tasks, owing to its unique self-attention mechanism. However, the model architecture of ViT is complex and often challenging to comprehend, leading to a steep learning curve. ViT developers and users frequently encounter difficulties in interpreting its inner workings. Therefore, a visualization system is needed to assist ViT users in understanding its functionality. This paper introduces EL-VIT, an interactive visual analytics system designed to probe the Vision Transformer and facilitate a better understanding of its operations. The system consists of four layers of visualization views. The first three layers include model overview, knowledge background graph, and model detail view. These three layers elucidate the operation process of ViT from three perspectives: the overall model architecture, detailed explanation, and mathematical operations, enabling users to understand the underlying principles and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClipSAM&#30340;CLIP&#21644;SAM&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;&#12290;ClipSAM&#21033;&#29992;CLIP&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#24322;&#24120;&#23450;&#20301;&#21644;&#31895;&#31961;&#20998;&#21106;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;SAM&#30340;&#25552;&#31034;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#24322;&#24120;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.12665</link><description>&lt;p&gt;
ClipSAM&#65306;CLIP&#21644;SAM&#30340;&#21512;&#20316;&#29992;&#20110;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation. (arXiv:2401.12665v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12665
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClipSAM&#30340;CLIP&#21644;SAM&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;&#12290;ClipSAM&#21033;&#29992;CLIP&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#24322;&#24120;&#23450;&#20301;&#21644;&#31895;&#31961;&#20998;&#21106;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;SAM&#30340;&#25552;&#31034;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#24322;&#24120;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;CLIP&#21644;SAM&#31561;&#22522;&#30784;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;&#65288;ZSAS&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26080;&#35770;&#26159;&#22522;&#20110;CLIP&#36824;&#26159;SAM&#30340;ZSAS&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#19981;&#21487;&#24573;&#35270;&#30340;&#20851;&#38190;&#32570;&#28857;&#65306;1&#65289;CLIP&#20027;&#35201;&#20851;&#27880;&#19981;&#21516;&#36755;&#20837;&#20043;&#38388;&#30340;&#20840;&#23616;&#29305;&#24449;&#23545;&#40784;&#65292;&#23548;&#33268;&#23545;&#23616;&#37096;&#24322;&#24120;&#37096;&#20998;&#30340;&#20998;&#21106;&#19981;&#20934;&#30830;&#65307;2&#65289;SAM&#20542;&#21521;&#20110;&#29983;&#25104;&#22823;&#37327;&#27809;&#26377;&#36866;&#24403;&#25552;&#31034;&#32422;&#26463;&#30340;&#20887;&#20313;&#25513;&#30721;&#65292;&#23548;&#33268;&#22797;&#26434;&#30340;&#21518;&#22788;&#29702;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClipSAM&#30340;CLIP&#21644;SAM&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;ZSAS&#12290;ClipSAM&#30340;&#24605;&#36335;&#26159;&#21033;&#29992;CLIP&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#24322;&#24120;&#23450;&#20301;&#21644;&#31895;&#31961;&#20998;&#21106;&#65292;&#36827;&#19968;&#27493;&#23558;&#20854;&#29992;&#20316;&#25552;&#20379;&#25552;&#31034;&#32422;&#26463;&#20197;&#25913;&#36827;SAM&#30340;&#24322;&#24120;&#20998;&#21106;&#32467;&#26524;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#32479;&#19968;&#22810;&#23610;&#24230;&#36328;&#27169;&#24577;&#20132;&#20114;&#65288;UMCI&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#30456;&#20114;&#20316;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple sc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#21644;&#24341;&#20837;&#26032;&#30340;&#25910;&#30410;&#20989;&#25968;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12662</link><description>&lt;p&gt;
&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#34701;&#20837;&#36830;&#32493;&#31354;&#38388;: &#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#20559;&#22909;&#39044;&#26399;&#25913;&#21892;&#30340;&#20132;&#20114;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Integrating Human Expertise in Continuous Spaces: A Novel Interactive Bayesian Optimization Framework with Preference Expected Improvement. (arXiv:2401.12662v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12662
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#21644;&#24341;&#20837;&#26032;&#30340;&#25910;&#30410;&#20989;&#25968;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;IML&#65289;&#26088;&#22312;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#19982;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#19981;&#33021;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#29366;&#24577;&#31354;&#38388;&#21644;/&#25110;&#34892;&#20026;&#31354;&#38388;&#20165;&#38480;&#20110;&#31163;&#25955;&#20540;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#20043;&#38388;&#30340;&#20132;&#20114;&#21463;&#21040;&#22312;&#22810;&#20010;&#24314;&#35758;&#20043;&#38388;&#20570;&#20986;&#20915;&#31574;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#20132;&#20114;&#24335;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;IBO&#65289;&#23454;&#29616;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;&#35813;&#26694;&#26550;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#30028;&#38754;&#32473;&#29992;&#25143;&#25163;&#21160;&#35843;&#25972;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21152;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#30410;&#20989;&#25968;&#65292;&#20559;&#22909;&#39044;&#26399;&#25913;&#21892;&#65288;PEI&#65289;&#65292;&#36890;&#36807;&#29992;&#25143;&#20559;&#22909;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#30830;&#20445;&#26426;&#22120;&#33021;&#20174;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#20013;&#21463;&#30410;&#65292;&#20197;&#23454;&#29616;&#26356;&#21152;&#21327;&#35843;&#21644;&#26377;&#25928;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive Machine Learning (IML) seeks to integrate human expertise into machine learning processes. However, most existing algorithms cannot be applied to Realworld Scenarios because their state spaces and/or action spaces are limited to discrete values. Furthermore, the interaction of all existing methods is restricted to deciding between multiple proposals. We therefore propose a novel framework based on Bayesian Optimization (BO). Interactive Bayesian Optimization (IBO) enables collaboration between machine learning algorithms and humans. This framework captures user preferences and provides an interface for users to shape the strategy by hand. Additionally, we've incorporated a new acquisition function, Preference Expected Improvement (PEI), to refine the system's efficiency using a probabilistic model of the user preferences. Our approach is geared towards ensuring that machines can benefit from human expertise, aiming for a more aligned and effective learning process. In the c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#30340;&#28608;&#21169;&#19968;&#33268;&#24615;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20154;&#22312;&#19981;&#39057;&#32321;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435;&#21512;&#20316;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#30830;&#23450;&#24615;&#26126;&#26174;&#38477;&#20302;&#20102;&#20195;&#29702;&#20154;&#20174;&#20107;&#21512;&#20316;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.12646</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#28608;&#21169;&#23545;&#26032;&#20852;&#21512;&#20316;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emergent Cooperation under Uncertain Incentive Alignment. (arXiv:2401.12646v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#30340;&#28608;&#21169;&#19968;&#33268;&#24615;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20154;&#22312;&#19981;&#39057;&#32321;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435;&#21512;&#20316;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#30830;&#23450;&#24615;&#26126;&#26174;&#38477;&#20302;&#20102;&#20195;&#29702;&#20154;&#20174;&#20107;&#21512;&#20316;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35745;&#31639;&#20195;&#29702;&#31995;&#32479;&#20013;&#21512;&#20316;&#30340;&#20986;&#29616;&#23545;&#20110;&#24320;&#21457;&#26377;&#25928;&#30340;&#21512;&#20316;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#30495;&#23454;&#19990;&#30028;&#20013;&#20010;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#36890;&#24120;&#31232;&#23569;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#28608;&#21169;&#33539;&#22260;&#20869;&#21457;&#29983;&#65292;&#36825;&#20123;&#28608;&#21169;&#36890;&#24120;&#21482;&#26377;&#37096;&#20998;&#30693;&#26195;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36935;&#21040;&#19981;&#39057;&#32321;&#30340;&#24773;&#20917;&#21644;&#20195;&#29702;&#20154;&#38754;&#20020;&#23545;&#20854;&#28608;&#21169;&#19982;&#20182;&#20154;&#28608;&#21169;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20154;&#20043;&#38388;&#24314;&#31435;&#21512;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#19981;&#21516;&#29615;&#22659;&#19979;&#35757;&#32451;&#20195;&#29702;&#20154;&#65292;&#20174;&#23436;&#20840;&#31454;&#20105;&#21040;&#23436;&#20840;&#21512;&#20316;&#20877;&#21040;&#28151;&#21512;&#21160;&#26426;&#12290;&#22312;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#29992;&#20110;&#22312;&#28151;&#21512;&#21160;&#26426;&#29615;&#22659;&#20013;&#20419;&#36827;&#21512;&#20316;&#30340;&#26426;&#21046;&#65292;&#22914;&#22768;&#35465;&#21644;&#20869;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;&#65292;&#19981;&#30830;&#23450;&#24615;&#26174;&#33879;&#38477;&#20302;&#20102;&#20195;&#29702;&#20154;&#20174;&#20107;&#21512;&#20316;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#24403;&#36825;&#31181;&#21512;&#20316;&#34892;&#20026;&#38656;&#35201;&#22242;&#32467;&#19968;&#33268;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the emergence of cooperation in systems of computational agents is crucial for the development of effective cooperative AI. Interaction among individuals in real-world settings are often sparse and occur within a broad spectrum of incentives, which often are only partially known. In this work, we explore how cooperation can arise among reinforcement learning agents in scenarios characterised by infrequent encounters, and where agents face uncertainty about the alignment of their incentives with those of others. To do so, we train the agents under a wide spectrum of environments ranging from fully competitive, to fully cooperative, to mixed-motives. Under this type of uncertainty we study the effects of mechanisms, such as reputation and intrinsic rewards, that have been proposed in the literature to foster cooperation in mixed-motives environments. Our findings show that uncertainty substantially lowers the agents' ability to engage in cooperative behaviour, when that wou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#30896;&#25758;&#21270;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#36935;&#21040;&#19981;&#21033;&#20107;&#20214;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12632</link><description>&lt;p&gt;
&#30896;&#25758;&#21270;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#38887;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Resilience of Collaborative AI Systems. (arXiv:2401.12632v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#30896;&#25758;&#21270;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#36935;&#21040;&#19981;&#21033;&#20107;&#20214;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30896;&#25758;&#21270;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;CAIS&#65289;&#19982;&#20154;&#31867;&#21512;&#20316;&#25191;&#34892;&#34892;&#21160;&#20197;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#12290; CAIS&#21487;&#20197;&#20351;&#29992;&#32463;&#36807;&#35757;&#32451;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26469;&#25511;&#21046;&#20154;-&#31995;&#32479;&#20132;&#20114;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#20154;&#31867;&#20132;&#20114;&#20197;&#22312;&#32447;&#26041;&#24335;&#21160;&#24577;&#22320;&#20174;&#20154;&#31867;&#20013;&#23398;&#20064;&#12290;&#22312;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#31995;&#32479;&#20256;&#24863;&#22120;&#22312;&#23398;&#20064;&#29366;&#24577;&#19979;&#30417;&#27979;&#20154;&#31867;&#20132;&#20114;&#65292;AI&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#22312;&#25805;&#20316;&#29366;&#24577;&#19979;&#28608;&#27963;CAIS&#30340;&#33258;&#20027;&#32452;&#20214;&#12290;&#22240;&#27492;&#65292;&#20219;&#20309;&#24433;&#21709;&#36825;&#20123;&#20256;&#24863;&#22120;&#30340;&#19981;&#21033;&#20107;&#20214;&#21487;&#33021;&#20250;&#24433;&#21709;AI&#27169;&#22411;&#20570;&#20986;&#20934;&#30830;&#20915;&#31574;&#30340;&#33021;&#21147;&#24182;&#38477;&#20302;CAIS&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;CAIS&#31649;&#29702;&#32773;&#33021;&#22815;&#33258;&#21160;&#36319;&#36394;&#31995;&#32479;&#24615;&#33021;&#20197;&#20102;&#35299;CAIS&#22312;&#27492;&#31867;&#19981;&#21033;&#20107;&#20214;&#20013;&#30340;&#38887;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31995;&#32479;&#36935;&#21040;&#19981;&#21033;&#20107;&#20214;&#26102;&#24314;&#27169;CAIS&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Collaborative Artificial Intelligence System (CAIS) performs actions in collaboration with the human to achieve a common goal. CAISs can use a trained AI model to control human-system interaction, or they can use human interaction to dynamically learn from humans in an online fashion. In online learning with human feedback, the AI model evolves by monitoring human interaction through the system sensors in the learning state, and actuates the autonomous components of the CAIS based on the learning in the operational state. Therefore, any disruptive event affecting these sensors may affect the AI model's ability to make accurate decisions and degrade the CAIS performance. Consequently, it is of paramount importance for CAIS managers to be able to automatically track the system performance to understand the resilience of the CAIS upon such disruptive events. In this paper, we provide a new framework to model CAIS performance when the system experiences a disruptive event. With our frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#24212;&#20102;Makelov&#31561;&#20154;(2023)&#30340;&#35770;&#25991;&#65292;&#35813;&#35770;&#25991;&#35780;&#36848;&#20102;&#23376;&#31354;&#38388;&#20132;&#25442;&#24178;&#39044;&#26041;&#27861;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#38382;&#39064;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#25152;&#35859;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#21487;&#20197;&#21253;&#25324;&#30452;&#35266;&#21644;&#21487;&#21462;&#30340;&#35299;&#37322;&#65292;&#32780;Makelov&#31561;&#20154;(2023)&#21457;&#29616;&#30340;"&#38169;&#35273;"&#26159;&#20182;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#33539;&#20363;&#30340;&#20135;&#29289;&#12290;&#23613;&#31649;&#25105;&#20204;&#19981;&#21516;&#24847;&#20182;&#20204;&#30340;&#26680;&#24515;&#34920;&#36848;&#65292;&#20294;&#20182;&#20204;&#30340;&#20363;&#23376;&#21644;&#35752;&#35770;&#25512;&#21160;&#20102;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.12631</link><description>&lt;p&gt;
&#23545;Makelov&#31561;&#20154;(2023)&#30340;&#12298;&#21487;&#35299;&#37322;&#24615;&#38169;&#35273;&#12299;&#35770;&#28857;&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments. (arXiv:2401.12631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#24212;&#20102;Makelov&#31561;&#20154;(2023)&#30340;&#35770;&#25991;&#65292;&#35813;&#35770;&#25991;&#35780;&#36848;&#20102;&#23376;&#31354;&#38388;&#20132;&#25442;&#24178;&#39044;&#26041;&#27861;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#38382;&#39064;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#25152;&#35859;&#30340;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#21487;&#20197;&#21253;&#25324;&#30452;&#35266;&#21644;&#21487;&#21462;&#30340;&#35299;&#37322;&#65292;&#32780;Makelov&#31561;&#20154;(2023)&#21457;&#29616;&#30340;"&#38169;&#35273;"&#26159;&#20182;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#33539;&#20363;&#30340;&#20135;&#29289;&#12290;&#23613;&#31649;&#25105;&#20204;&#19981;&#21516;&#24847;&#20182;&#20204;&#30340;&#26680;&#24515;&#34920;&#36848;&#65292;&#20294;&#20182;&#20204;&#30340;&#20363;&#23376;&#21644;&#35752;&#35770;&#25512;&#21160;&#20102;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22238;&#24212;&#20102;Makelov&#31561;&#20154;(2023)&#30340;&#26368;&#26032;&#35770;&#25991;&#65292;&#35813;&#35770;&#25991;&#35780;&#36848;&#20102;&#35832;&#22914;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;(DAS; Geiger&#31561;&#20154;&#65292;2023)&#36825;&#26679;&#30340;&#23376;&#31354;&#38388;&#20132;&#25442;&#24178;&#39044;&#26041;&#27861;&#65292;&#24182;&#22768;&#31216;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#24341;&#36215;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;Makelov&#31561;&#20154;(2023)&#23545;"&#35299;&#37322;&#24615;&#38169;&#35273;"&#30340;&#25216;&#26415;&#27010;&#24565;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#21363;&#20351;&#30452;&#35266;&#21644;&#21487;&#21462;&#30340;&#35299;&#37322;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#20063;&#21487;&#33021;&#25104;&#20026;&#38169;&#35273;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#21457;&#29616;"&#38169;&#35273;"&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#25298;&#32477;&#20182;&#20204;&#35748;&#20026;"&#38750;&#38169;&#35273;"&#30340;&#35299;&#37322;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35748;&#20026;Makelov&#31561;&#20154;(2023)&#22312;&#23454;&#36341;&#20013;&#30475;&#21040;&#30340;"&#38169;&#35273;"&#26159;&#20182;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#33539;&#20363;&#30340;&#20135;&#29289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;&#23613;&#31649;&#25105;&#20204;&#19981;&#21516;&#24847;&#20182;&#20204;&#30340;&#26680;&#24515;&#34920;&#36848;&#65292;&#20294;Makelov&#31561;&#20154;(2023)&#30340;&#20363;&#23376;&#21644;&#35752;&#35770;&#26080;&#30097;&#25512;&#21160;&#20102;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We respond to the recent paper by Makelov et al. (2023), which reviews subspace interchange intervention methods like distributed alignment search (DAS; Geiger et al. 2023) and claims that these methods potentially cause "interpretability illusions". We first review Makelov et al. (2023)'s technical notion of what an "interpretability illusion" is, and then we show that even intuitive and desirable explanations can qualify as illusions in this sense. As a result, their method of discovering "illusions" can reject explanations they consider "non-illusory". We then argue that the illusions Makelov et al. (2023) see in practice are artifacts of their training and evaluation paradigms. We close by emphasizing that, though we disagree with their core characterization, Makelov et al. (2023)'s examples and discussion have undoubtedly pushed the field of interpretability forward.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23558;&#35821;&#35328;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#19982;&#26032;&#20852;&#36890;&#20449;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.12624</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#22522;&#20110;&#35821;&#35328;&#21040;&#26032;&#20852;&#36890;&#20449;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control. (arXiv:2401.12624v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12624
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23558;&#35821;&#35328;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#19982;&#26032;&#20852;&#36890;&#20449;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#30340;&#26032;&#20852;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#30001;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#36890;&#20449;&#65288;LSC&#65289;&#12290;&#22312;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#23548;&#33322;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#21253;&#21547;&#20301;&#32622;&#21644;&#36890;&#36947;&#22320;&#22270;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;EC&#22312;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#26102;&#20250;&#20135;&#29983;&#39640;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#22256;&#38590;&#65292;&#32780;LSC&#30001;&#20110;LLM&#23610;&#23544;&#36739;&#22823;&#65292;&#20250;&#23548;&#33268;&#39640;&#30340;&#25512;&#29702;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#23427;&#20204;&#21508;&#33258;&#30340;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#24341;&#23548;EC&#35757;&#32451;&#20351;&#29992;LSC&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;&#35821;&#35328;&#24341;&#23548;&#30340;EC&#65288;LEC&#65289;&#12290;&#27169;&#25311;&#39564;&#35777;&#20102;LEC&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#65292;&#36991;&#20813;&#20102;&#20449;&#36947;&#36136;&#37327;&#24046;&#30340;&#21306;&#22495;&#65292;&#24182;&#19988;&#22312;&#19982;EC&#30456;&#27604;&#33021;&#22815;&#21152;&#36895;MADRL&#35757;&#32451;&#25910;&#25947;&#36798;&#21040;61.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we compare emergent communication (EC) built upon multi-agent deep reinforcement learning (MADRL) and language-oriented semantic communication (LSC) empowered by a pre-trained large language model (LLM) using human language. In a multi-agent remote navigation task, with multimodal input data comprising location and channel maps, it is shown that EC incurs high training cost and struggles when using multimodal data, whereas LSC yields high inference computing cost due to the LLM's large size. To address their respective bottlenecks, we propose a novel framework of language-guided EC (LEC) by guiding the EC training using LSC via knowledge distillation (KD). Simulations corroborate that LEC achieves faster travel time while avoiding areas with poor channel conditions, as well as speeding up the MADRL training convergence by up to 61.8% compared to EC.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#22686;&#24378;&#30340;PDF&#32467;&#26500;&#35782;&#21035;&#26469;&#38761;&#26032;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19987;&#19994;&#25991;&#26723;&#20013;PDF&#35299;&#26512;&#30340;&#20302;&#20934;&#30830;&#24615;&#23545;&#22522;&#20110;&#19987;&#19994;&#30693;&#35782;&#38382;&#31572;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12599</link><description>&lt;p&gt;
&#29992;&#22686;&#24378;&#30340;PDF&#32467;&#26500;&#35782;&#21035;&#26469;&#38761;&#26032;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition. (arXiv:2401.12599v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#22686;&#24378;&#30340;PDF&#32467;&#26500;&#35782;&#21035;&#26469;&#38761;&#26032;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19987;&#19994;&#25991;&#26723;&#20013;PDF&#35299;&#26512;&#30340;&#20302;&#20934;&#30830;&#24615;&#23545;&#22522;&#20110;&#19987;&#19994;&#30693;&#35782;&#38382;&#31572;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#25104;&#20026;&#19987;&#19994;&#30693;&#35782;&#38382;&#31572;&#39046;&#22495;&#20013;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#30446;&#21069;&#65292;&#20027;&#35201;&#30340;&#22522;&#30784;&#27169;&#22411;&#20844;&#21496;&#24050;&#32463;&#24320;&#25918;&#20102;&#23884;&#20837;&#24335;&#21644;&#32842;&#22825;API&#25509;&#21475;&#65292;&#32780;LangChain&#31561;&#26694;&#26550;&#24050;&#32463;&#25972;&#21512;&#20102;RAG&#27969;&#31243;&#12290;&#30475;&#20284;RAG&#20013;&#30340;&#20851;&#38190;&#27169;&#22411;&#21644;&#27493;&#39588;&#24050;&#32463;&#24471;&#21040;&#35299;&#20915;&#65292;&#36825;&#23601;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#19987;&#19994;&#30693;&#35782;&#38382;&#31572;&#31995;&#32479;&#26159;&#21542;&#25509;&#36817;&#23436;&#32654;&#20102;&#65311;&#26412;&#25991;&#21457;&#29616;&#24403;&#21069;&#30340;&#20027;&#35201;&#26041;&#27861;&#20381;&#36182;&#20110;&#35775;&#38382;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#21069;&#25552;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19987;&#19994;&#25991;&#26723;&#20027;&#35201;&#20197;PDF&#24418;&#24335;&#23384;&#20648;&#65292;PDF&#35299;&#26512;&#30340;&#20302;&#20934;&#30830;&#24615;&#26174;&#33879;&#24433;&#21709;&#20102;&#22522;&#20110;&#19987;&#19994;&#30693;&#35782;&#30340;&#38382;&#31572;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#25968;&#30334;&#20010;&#26469;&#33258;&#30456;&#24212;&#30495;&#23454;&#19990;&#30028;&#19987;&#19994;&#25991;&#26723;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#30340;RAG&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatDOC&#26159;&#19968;&#20010;&#37197;&#22791;&#20102;&#20840;&#26223;&#35270;&#22270;&#21151;&#33021;&#30340;RAG&#31995;&#32479;&#65292;&#33021;&#22815;&#25552;&#39640;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) has become a predominant method in the field of professional knowledge-based question answering. Presently, major foundation model companies have opened up Embedding and Chat API interfaces, and frameworks like LangChain have already integrated the RAG process. It appears that the key models and steps in RAG have been resolved, leading to the question: are professional knowledge QA systems now approaching perfection? This article discovers that current primary methods depend on the premise of accessing high-quality text corpora. However, since professional documents are mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts the effectiveness of professional knowledge-based QA. We conducted an empirical RAG experiment across hundreds of questions from the corresponding real-world professional documents. The results show that, ChatDOC, a RAG system equipped with a panopt
&lt;/p&gt;</description></item><item><title>MOReGIn&#26159;&#19968;&#31181;&#22810;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22312;&#20840;&#23616;&#21644;&#20010;&#20307;&#23618;&#38754;&#19978;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#12290;&#36890;&#36807;&#35843;&#33410;&#25512;&#33616;&#21015;&#34920;&#65292;MORS&#21487;&#20197;&#20445;&#35777;&#25512;&#33616;&#30340;&#27969;&#27966;&#26657;&#20934;&#21644;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12593</link><description>&lt;p&gt;
MOReGIn: &#22810;&#30446;&#26631;&#25512;&#33616;&#22312;&#20840;&#23616;&#21644;&#20010;&#20307;&#23618;&#38754;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MOReGIn: Multi-Objective Recommendation at the Global and Individual Levels. (arXiv:2401.12593v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12593
&lt;/p&gt;
&lt;p&gt;
MOReGIn&#26159;&#19968;&#31181;&#22810;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22312;&#20840;&#23616;&#21644;&#20010;&#20307;&#23618;&#38754;&#19978;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#12290;&#36890;&#36807;&#35843;&#33410;&#25512;&#33616;&#21015;&#34920;&#65292;MORS&#21487;&#20197;&#20445;&#35777;&#25512;&#33616;&#30340;&#27969;&#27966;&#26657;&#20934;&#21644;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#65288;MORS&#65289;&#20316;&#20026;&#19968;&#31181;&#20445;&#35777;&#22810;&#20010;&#30446;&#26631;&#30340;&#33539;&#24335;&#20986;&#29616;&#12290;&#38500;&#20102;&#20934;&#30830;&#24615;&#22806;&#65292;MORS&#21487;&#20197;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#24037;&#20316;&#65292;&#20026;&#25972;&#20010;&#31995;&#32479;&#23454;&#29616;&#38500;&#20934;&#30830;&#24615;&#20043;&#22806;&#30340;&#20854;&#20182;&#30446;&#26631;&#65292;&#20063;&#21487;&#20197;&#22312;&#20010;&#20307;&#23618;&#38754;&#19978;&#24037;&#20316;&#65292;&#36825;&#24847;&#21619;&#30528;&#25512;&#33616;&#26159;&#26681;&#25454;&#27599;&#20010;&#29992;&#25143;&#30340;&#38656;&#27714;&#23450;&#21046;&#30340;&#12290;&#29616;&#26377;&#30340;MORS&#35201;&#20040;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#24037;&#20316;&#65292;&#35201;&#20040;&#22312;&#20010;&#20307;&#23618;&#38754;&#19978;&#24037;&#20316;&#65292;&#32780;&#19981;&#20551;&#35774;&#36825;&#20004;&#31181;&#35266;&#28857;&#30340;&#20849;&#23384;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#20840;&#23616;&#21644;&#20010;&#20307;&#30446;&#26631;&#20849;&#23384;&#26102;&#65292;MORS&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#36825;&#20004;&#31181;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#33410;&#25512;&#33616;&#21015;&#34920;&#26469;&#20445;&#35777;&#20840;&#23616;&#21644;&#20010;&#20307;&#30340;&#35266;&#28857;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20316;&#20026;&#20010;&#20307;&#35282;&#24230;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27969;&#27966;&#26657;&#20934;&#38382;&#39064;&#65292;&#20316;&#20026;&#20840;&#23616;&#35282;&#24230;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19982;&#26412;&#25991;&#19968;&#21516;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Recommender Systems (MORSs) emerged as a paradigm to guarantee multiple (often conflicting) goals. Besides accuracy, a MORS can operate at the global level, where additional beyond-accuracy goals are met for the system as a whole, or at the individual level, meaning that the recommendations are tailored to the needs of each user. The state-of-the-art MORSs either operate at the global or individual level, without assuming the co-existence of the two perspectives. In this study, we show that when global and individual objectives co-exist, MORSs are not able to meet both types of goals. To overcome this issue, we present an approach that regulates the recommendation lists so as to guarantee both global and individual perspectives, while preserving its effectiveness. Specifically, as individual perspective, we tackle genre calibration and, as global perspective, provider fairness. We validate our approach on two real-world datasets, publicly released with this paper.
&lt;/p&gt;</description></item><item><title>LLMCheckup&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#25552;&#20379;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.12576</link><description>&lt;p&gt;
LLMCheckup&#65306;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#24335;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools. (arXiv:2401.12576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12576
&lt;/p&gt;
&lt;p&gt;
LLMCheckup&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#25552;&#20379;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20197;&#23545;&#35805;&#24418;&#24335;&#36827;&#34892;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#24050;&#32463;&#35777;&#26126;&#22312;&#22686;&#24378;&#29992;&#25143;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#25928;&#26524;&#65292;&#22240;&#20026;&#19968;&#27425;&#24615;&#35299;&#37322;&#26377;&#26102;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;&#23545;&#35805;&#30340;&#35299;&#37322;&#26041;&#26696;&#38656;&#35201;&#35768;&#22810;&#20381;&#36182;&#39033;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#36716;&#31227;&#21040;&#23427;&#20204;&#26410;&#35774;&#35745;&#30340;&#20219;&#21153;&#19978;&#12290;&#36890;&#36807;LLMCheckup&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#24037;&#20855;&#65292;&#20801;&#35768;&#29992;&#25143;&#19982;&#20219;&#20309;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#23545;&#35805;&#20197;&#20102;&#35299;&#20854;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;LLMs&#33021;&#22815;&#33258;&#34892;&#29983;&#25104;&#25152;&#26377;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#19982;&#19968;&#31995;&#21015;&#21487;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#24037;&#20855;&#65288;&#20363;&#22914;&#29305;&#24449;&#24402;&#22240;&#12289;&#22522;&#20110;&#23884;&#20837;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#21453;&#20107;&#23454;&#21644;&#22522;&#20110;&#29702;&#30001;&#29983;&#25104;&#30340;&#25552;&#31034;&#31574;&#30053;&#65289;&#36830;&#25509;&#65292;&#20197;&#23436;&#25104;&#24847;&#22270;&#35782;&#21035;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;LLM&#65288;&#33258;&#25105;&#65289;&#35299;&#37322;&#20197;&#20132;&#20114;&#23545;&#35805;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#25903;&#25345;&#21518;&#32493;&#38382;&#39064;&#21644;&#29983;&#25104;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability tools that offer explanations in the form of a dialogue have demonstrated their efficacy in enhancing users' understanding, as one-off explanations may occasionally fall short in providing sufficient information to the user. Current solutions for dialogue-based explanations, however, require many dependencies and are not easily transferable to tasks they were not designed for. With LLMCheckup, we present an easily accessible tool that allows users to chat with any state-of-the-art large language model (LLM) about its behavior. We enable LLMs to generate all explanations by themselves and take care of intent recognition without fine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI) tools, e.g. feature attributions, embedding-based similarity, and prompting strategies for counterfactual and rationale generation. LLM (self-)explanations are presented as an interactive dialogue that supports follow-up questions and generates suggestions. LLMCheckup p
&lt;/p&gt;</description></item><item><title>DiffMoog&#26159;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#27169;&#22359;&#21270;&#21512;&#25104;&#22120;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#22768;&#38899;&#21305;&#37197;&#12290;&#23427;&#20855;&#26377;&#35843;&#21046;&#33021;&#21147;&#12289;&#20302;&#39057;&#25391;&#33633;&#22120;&#12289;&#28388;&#27874;&#22120;&#21644;&#21253;&#32476;&#24418;&#29366;&#22120;&#31561;&#21151;&#33021;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#21019;&#24314;&#33258;&#23450;&#20041;&#20449;&#21495;&#38142;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20449;&#21495;&#38142;&#25439;&#22833;&#21644;&#32534;&#30721;&#22120;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#22768;&#38899;&#21305;&#37197;&#26694;&#26550;&#65292;&#20026;&#20351;&#29992;&#21487;&#24494;&#20998;&#21512;&#25104;&#36827;&#34892;&#22768;&#38899;&#21305;&#37197;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;</title><link>http://arxiv.org/abs/2401.12570</link><description>&lt;p&gt;
DiffMoog: &#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#27169;&#22359;&#21270;&#21512;&#25104;&#22120;&#29992;&#20110;&#22768;&#38899;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
DiffMoog: a Differentiable Modular Synthesizer for Sound Matching. (arXiv:2401.12570v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12570
&lt;/p&gt;
&lt;p&gt;
DiffMoog&#26159;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#27169;&#22359;&#21270;&#21512;&#25104;&#22120;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#22768;&#38899;&#21305;&#37197;&#12290;&#23427;&#20855;&#26377;&#35843;&#21046;&#33021;&#21147;&#12289;&#20302;&#39057;&#25391;&#33633;&#22120;&#12289;&#28388;&#27874;&#22120;&#21644;&#21253;&#32476;&#24418;&#29366;&#22120;&#31561;&#21151;&#33021;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#21019;&#24314;&#33258;&#23450;&#20041;&#20449;&#21495;&#38142;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20449;&#21495;&#38142;&#25439;&#22833;&#21644;&#32534;&#30721;&#22120;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#22768;&#38899;&#21305;&#37197;&#26694;&#26550;&#65292;&#20026;&#20351;&#29992;&#21487;&#24494;&#20998;&#21512;&#25104;&#36827;&#34892;&#22768;&#38899;&#21305;&#37197;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffMoog - &#19968;&#20010;&#20855;&#26377;&#21830;&#19994;&#20048;&#22120;&#20013;&#24120;&#35265;&#27169;&#22359;&#30340;&#21487;&#24494;&#20998;&#27169;&#22359;&#21270;&#21512;&#25104;&#22120;&#12290;&#20316;&#20026;&#21487;&#24494;&#20998;&#30340;&#65292;&#23427;&#21487;&#20197;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#22768;&#38899;&#21305;&#37197;&#65292;&#20197;&#22797;&#21046;&#32473;&#23450;&#30340;&#38899;&#39057;&#36755;&#20837;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DiffMoog&#20855;&#26377;&#35843;&#21046;&#33021;&#21147;&#65288;FM / AM&#65289;&#12289;&#20302;&#39057;&#25391;&#33633;&#22120;&#65288;LFOs&#65289;&#12289;&#28388;&#27874;&#22120;&#12289;&#21253;&#32476;&#24418;&#29366;&#22120;&#20197;&#21450;&#29992;&#25143;&#21019;&#24314;&#33258;&#23450;&#20041;&#20449;&#21495;&#38142;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#21253;&#25324;&#20102;DiffMoog&#21644;&#31471;&#21040;&#31471;&#30340;&#22768;&#38899;&#21305;&#37197;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#21495;&#38142;&#25439;&#22833;&#21644;&#19968;&#20010;&#32534;&#30721;&#22120;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#27169;&#22359;&#21270;&#26550;&#26500;&#26469;&#33258;&#21160;&#32534;&#31243;&#20854;&#36755;&#20986;&#65292;&#20197;&#39044;&#27979;DiffMoog&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#21512;&#25104;&#36827;&#34892;&#22768;&#38899;&#21305;&#37197;&#30340;&#35265;&#35299;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;&#22768;&#38899;&#21151;&#33021;&#19982;&#20840;&#38754;&#30340;&#24179;&#21488;&#32467;&#21512;&#65292;DiffMoog&#25104;&#20026;&#21152;&#24555;&#38899;&#39057;&#21512;&#25104;&#30740;&#31350;&#30340;&#39318;&#36873;&#36164;&#20135;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents DiffMoog - a differentiable modular synthesizer with a comprehensive set of modules typically found in commercial instruments. Being differentiable, it allows integration into neural networks, enabling automated sound matching, to replicate a given audio input. Notably, DiffMoog facilitates modulation capabilities (FM/AM), low-frequency oscillators (LFOs), filters, envelope shapers, and the ability for users to create custom signal chains. We introduce an open-source platform that comprises DiffMoog and an end-to-end sound matching framework. This framework utilizes a novel signal-chain loss and an encoder network that self-programs its outputs to predict DiffMoogs parameters based on the user-defined modular architecture. Moreover, we provide insights and lessons learned towards sound matching using differentiable synthesis. Combining robust sound capabilities with a holistic platform, DiffMoog stands as a premier asset for expediting research in audio synthesis an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21518;&#24724;&#21305;&#37197;+&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#24179;&#34913;&#33258;&#21338;&#24328;&#35757;&#32451;&#20013;&#19981;&#21516;&#35282;&#33394;&#30340;AI&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.12557</link><description>&lt;p&gt;
&#24179;&#34913;&#33258;&#21338;&#24328;&#35757;&#32451;&#20013;&#35282;&#33394;&#30340;AI&#33021;&#21147;&#19982;&#21518;&#24724;&#21305;&#37197;+&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Balancing the AI Strength of Roles in Self-Play Training with Regret Matching+. (arXiv:2401.12557v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12557
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21518;&#24724;&#21305;&#37197;+&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#24179;&#34913;&#33258;&#21338;&#24328;&#35757;&#32451;&#20013;&#19981;&#21516;&#35282;&#33394;&#30340;AI&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20026;&#28085;&#30422;&#22810;&#20010;&#35282;&#33394;&#30340;&#28216;&#25103;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#26102;&#65292;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#25511;&#21046;&#28216;&#25103;&#20869;&#20219;&#24847;&#35282;&#33394;&#30340;&#36890;&#29992;&#27169;&#22411;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;&#36825;&#31181;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#22312;&#35757;&#32451;&#38454;&#27573;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#65292;&#32780;&#19988;&#21487;&#20197;&#20943;&#23569;&#37096;&#32626;&#26102;&#30340;&#36164;&#28304;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#25511;&#21046;&#19981;&#21516;&#35282;&#33394;&#26102;&#33021;&#21147;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#24724;&#21305;&#37197;+&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21487;&#20197;&#20419;&#36827;&#27169;&#22411;&#22312;&#25511;&#21046;&#19981;&#21516;&#35282;&#33394;&#26102;&#26356;&#24179;&#34913;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training artificial intelligence for games encompassing multiple roles, the development of a generalized model capable of controlling any character within the game presents a viable option. This strategy not only conserves computational resources and time during the training phase but also reduces resource requirements during deployment. training such a generalized model often encounters challenges related to uneven capabilities when controlling different roles. A simple method is introduced based on Regret Matching+, which facilitates a more balanced performance of strength by the model when controlling various roles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;PCGBench&#65292;&#24182;&#20351;&#29992;&#26032;&#25351;&#26631;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#21644;&#35745;&#31639;&#38382;&#39064;&#31867;&#22411;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12554</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#32534;&#20889;&#24182;&#34892;&#20195;&#30721;?
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Write Parallel Code?. (arXiv:2401.12554v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;PCGBench&#65292;&#24182;&#20351;&#29992;&#26032;&#25351;&#26631;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#21644;&#35745;&#31639;&#38382;&#39064;&#31867;&#22411;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#36719;&#20214;&#24320;&#21457;&#32773;&#30340;&#27426;&#36814;&#12290;&#23427;&#20204;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#23637;&#31034;&#20102;&#23545;&#28304;&#20195;&#30721;&#30340;&#24314;&#27169;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21253;&#25324;&#20195;&#30721;&#34917;&#20840;&#12289;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#26597;&#25214;&#31561;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#20026;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#29983;&#25104;&#20195;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PCGBench&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;420&#20010;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#21644;&#38381;&#28304;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#29992;&#20110;&#27604;&#36739;&#24182;&#34892;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#30340;&#26032;&#25351;&#26631;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#26469;&#25506;&#31350;&#27599;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#21644;&#35745;&#31639;&#38382;&#39064;&#31867;&#22411;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are becoming an increasingly popular tool for software development. Their ability to model and generate source code has been demonstrated in a variety of contexts, including code completion, summarization, translation, and lookup. However, they often struggle to generate code for more complex tasks. In this paper, we explore the ability of state-of-the-art language models to generate parallel code. We propose a benchmark, PCGBench, consisting of a set of 420 tasks for evaluating the ability of language models to generate parallel code, and we evaluate the performance of several state-of-the-art open- and closed-source language models on these tasks. We introduce novel metrics for comparing parallel code generation performance and use them to explore how well each LLM performs on various parallel programming models and computational problem types.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UR4NNV&#39564;&#35777;&#26694;&#26550;&#65292;&#21033;&#29992;&#27424;&#20272;&#35745;&#21487;&#36798;&#24615;&#20998;&#26512;&#36827;&#34892;DNN&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#23545;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;DNN&#36827;&#34892;&#27424;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#35797;&#38169;&#26041;&#27861;&#26377;&#25928;&#22320;&#35777;&#20266;DNN&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12550</link><description>&lt;p&gt;
UR4NNV: &#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#22522;&#20110;&#27424;&#20272;&#35745;&#21487;&#36798;&#24615;&#30340;&#26041;&#27861;&#65281;
&lt;/p&gt;
&lt;p&gt;
UR4NNV: Neural Network Verification, Under-approximation Reachability Works!. (arXiv:2401.12550v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UR4NNV&#39564;&#35777;&#26694;&#26550;&#65292;&#21033;&#29992;&#27424;&#20272;&#35745;&#21487;&#36798;&#24615;&#20998;&#26512;&#36827;&#34892;DNN&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#23545;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;DNN&#36827;&#34892;&#27424;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#35797;&#38169;&#26041;&#27861;&#26377;&#25928;&#22320;&#35777;&#20266;DNN&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24418;&#24335;&#39564;&#35777;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22522;&#20110;&#36807;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#22240;&#20854;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#22312;&#35299;&#20915;&#28041;&#21450;&#30830;&#20999;&#36755;&#20986;&#21306;&#22495;&#25110;&#24341;&#20837;&#30340;&#36817;&#20284;&#35823;&#24046;&#26159;&#21542;&#36829;&#21453;&#25152;&#35752;&#35770;&#23646;&#24615;&#30340;&#8220;&#26410;&#30693;&#22256;&#22659;&#8221;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;UR4NNV&#39564;&#35777;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#27424;&#20272;&#35745;&#21487;&#36798;&#24615;&#20998;&#26512;&#36827;&#34892;DNN&#39564;&#35777;&#12290;UR4NNV&#19987;&#27880;&#20110;&#20855;&#26377;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#30340;DNN&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20108;&#21449;&#26641;&#20998;&#25903;&#30340;&#27424;&#20272;&#35745;&#31639;&#27861;&#12290;&#22312;&#27599;&#20010;&#21608;&#26399;&#20013;&#65292;UR4NNV&#23545;&#21487;&#36798;&#38598;&#21512;&#30340;&#23376;&#22810;&#38754;&#20307;&#36827;&#34892;&#27424;&#20272;&#35745;&#65292;&#24182;&#38024;&#23545;&#32473;&#23450;&#30340;&#23646;&#24615;&#39564;&#35777;&#35813;&#22810;&#38754;&#20307;&#12290;&#36890;&#36807;&#35797;&#38169;&#26041;&#27861;&#65292;UR4NNV&#22312;&#36798;&#21040;&#39564;&#35777;&#21608;&#26399;&#36793;&#30028;&#21644;&#22833;&#36133;&#26102;&#25552;&#20379;&#20102;&#26377;&#25928;&#22320;&#35777;&#20266;DNN&#23646;&#24615;&#30340;&#20449;&#24515;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, formal verification of deep neural networks (DNNs) has garnered considerable attention, and over-approximation based methods have become popular due to their effectiveness and efficiency. However, these strategies face challenges in addressing the "unknown dilemma" concerning whether the exact output region or the introduced approximation error violates the property in question. To address this, this paper introduces the UR4NNV verification framework, which utilizes under-approximation reachability analysis for DNN verification for the first time. UR4NNV focuses on DNNs with Rectified Linear Unit (ReLU) activations and employs a binary tree branch-based under-approximation algorithm. In each epoch, UR4NNV under-approximates a sub-polytope of the reachable set and verifies this polytope against the given property. Through a trial-and-error approach, UR4NNV effectively falsifies DNN properties while providing confidence levels when reaching verification epoch bounds and failing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12533</link><description>&lt;p&gt;
&#26377;&#25928;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;k&#20013;&#24515;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24515;&#20026;&#22522;&#30784;&#30340;&#32858;&#31867;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#37117;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36755;&#20837;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#32858;&#31867;&#32467;&#26524;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;k&#20013;&#24515;&#32858;&#31867;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#32972;&#26223;&#30693;&#35782;&#24314;&#27169;&#20026;&#24517;&#36830;&#65288;ML&#65289;&#21644;&#19981;&#36830;&#65288;CL&#65289;&#32422;&#26463;&#38598;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21253;&#25324;k&#20013;&#24515;&#22312;&#20869;&#30340;&#32858;&#31867;&#38382;&#39064;&#26412;&#36136;&#19978;&#37117;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#21463;&#32422;&#26463;&#21464;&#20307;&#34987;&#35748;&#20026;&#21463;&#21040;&#26356;&#20005;&#37325;&#30340;&#36817;&#20284;&#21644;&#35745;&#31639;&#38556;&#30861;&#30340;&#38480;&#21046;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#21453;&#25903;&#37197;&#38598;&#65292;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#25972;&#25968;&#24179;&#38754;&#21644;LP&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#32422;&#26463;k&#20013;&#24515;&#30340;&#39640;&#25928;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#31454;&#20105;&#22522;&#20934;&#31639;&#27861;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#36817;&#20284;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Center-based clustering has attracted significant research interest from both theory and practice. In many practical applications, input data often contain background knowledge that can be used to improve clustering results. In this work, we build on widely adopted $k$-center clustering and model its input background knowledge as must-link (ML) and cannot-link (CL) constraint sets. However, most clustering problems including $k$-center are inherently $\mathcal{NP}$-hard, while the more complex constrained variants are known to suffer severer approximation and computation barriers that significantly limit their applicability. By employing a suite of techniques including reverse dominating sets, linear programming (LP) integral polyhedron, and LP duality, we arrive at the first efficient approximation algorithm for constrained $k$-center with the best possible ratio of 2. We also construct competitive baseline algorithms and empirically evaluate our approximation algorithm against them o
&lt;/p&gt;</description></item><item><title>DAFA&#36890;&#36807;&#32771;&#34385;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#25439;&#22833;&#26435;&#37325;&#21644;&#23545;&#25239;&#36793;&#30028;&#65292;&#24182;&#35843;&#25972;&#23427;&#20204;&#20197;&#25552;&#39640;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12532</link><description>&lt;p&gt;
DAFA&#65306;&#36317;&#31163;&#24863;&#30693;&#20844;&#24179;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DAFA: Distance-Aware Fair Adversarial Training. (arXiv:2401.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12532
&lt;/p&gt;
&lt;p&gt;
DAFA&#36890;&#36807;&#32771;&#34385;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#25439;&#22833;&#26435;&#37325;&#21644;&#23545;&#25239;&#36793;&#30028;&#65292;&#24182;&#35843;&#25972;&#23427;&#20204;&#20197;&#25552;&#39640;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#35757;&#32451;&#20013;&#31867;&#21035;&#38388;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#34987;&#25918;&#22823;&#65292;&#36825;&#34987;&#31216;&#20026;&#40065;&#26834;&#20844;&#24179;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20026;&#22686;&#24378;&#40065;&#26834;&#20844;&#24179;&#24615;&#32780;&#29306;&#29298;&#27169;&#22411;&#23545;&#26131;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#21892;&#23545;&#38590;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#65292;&#27169;&#22411;&#23545;&#26368;&#24046;&#31867;&#21035;&#26679;&#26412;&#30340;&#39044;&#27979;&#22823;&#22810;&#20559;&#21521;&#20110;&#19982;&#26368;&#24046;&#31867;&#21035;&#30456;&#20284;&#30340;&#31867;&#21035;&#65292;&#32780;&#19981;&#26159;&#26131;&#31867;&#21035;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#30528;&#31867;&#21035;&#20043;&#38388;&#36317;&#31163;&#30340;&#20943;&#23567;&#65292;&#40065;&#26834;&#20844;&#24179;&#24615;&#20250;&#24694;&#21270;&#12290;&#21463;&#21040;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36317;&#31163;&#24863;&#30693;&#20844;&#24179;&#23545;&#25239;&#35757;&#32451;&#65288;DAFA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#35299;&#20915;&#40065;&#26834;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#20998;&#37197;&#19981;&#21516;&#30340;&#25439;&#22833;&#26435;&#37325;&#21644;&#23545;&#25239;&#36793;&#30028;&#65292;&#24182;&#35843;&#25972;&#23427;&#20204;&#20197;&#20419;&#36827;&#19968;&#31181;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The disparity in accuracy between classes in standard training is amplified during adversarial training, a phenomenon termed the robust fairness problem. Existing methodologies aimed to enhance robust fairness by sacrificing the model's performance on easier classes in order to improve its performance on harder ones. However, we observe that under adversarial attacks, the majority of the model's predictions for samples from the worst class are biased towards classes similar to the worst class, rather than towards the easy classes. Through theoretical and empirical analysis, we demonstrate that robust fairness deteriorates as the distance between classes decreases. Motivated by these insights, we introduce the Distance-Aware Fair Adversarial training (DAFA) methodology, which addresses robust fairness by taking into account the similarities between classes. Specifically, our method assigns distinct loss weights and adversarial margins to each class and adjusts them to encourage a trade-
&lt;/p&gt;</description></item><item><title>BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12522</link><description>&lt;p&gt;
BiTA: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#25439;&#21152;&#36895;&#30340;&#21452;&#21521;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12522
&lt;/p&gt;
&lt;p&gt;
BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#23548;&#33268;&#39640;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#21644;&#24310;&#36831;&#24310;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#8212;&#8212;&#21452;&#21521;&#35843;&#25972;&#20197;&#23454;&#29616;&#26080;&#25439;&#21152;&#36895;&#65288;BiTA&#65289;&#65292;&#36890;&#36807;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#26469;&#21152;&#36895;LLMs&#12290;&#21463;&#21551;&#21457;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#35774;&#35745;&#65292;&#31216;&#20026;&#21452;&#21521;&#35843;&#25972;&#65292;&#26469;&#22686;&#24378;LLMs&#22312;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#37319;&#29992;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#65292;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#33609;&#31295;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#30830;&#20445;&#36755;&#20986;&#32467;&#26524;&#19982;&#23427;&#20204;&#30340;&#33258;&#22238;&#24402;&#23545;&#24212;&#29289;&#22312;&#36138;&#23146;&#25277;&#26679;&#19979;&#23436;&#20840;&#30456;&#21516;&#12290;BiTA&#20316;&#20026;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#32541;&#22686;&#24378;&#29616;&#26377;LLMs&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#25215;&#25285;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#25552;&#20986;&#30340;BiTA&#65292;LLaMA-2-70B-Chat&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;YOLOv8&#12289;DeiT&#21644;SimCLR&#22312;&#24076;&#33098;&#32440;&#33609;&#20013;&#36827;&#34892;&#23383;&#31526;&#26816;&#27979;&#21644;&#35782;&#21035;&#31454;&#36187;&#65292;&#22312;&#35782;&#21035;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;42.2%&#30340;mAP&#65292;&#24182;&#22312;&#26816;&#27979;&#25361;&#25112;&#20013;&#20197;51.4%&#30340;&#24179;&#22343;&#31934;&#24230;&#33719;&#24471;&#20102;&#20122;&#20891;&#12290;</title><link>http://arxiv.org/abs/2401.12513</link><description>&lt;p&gt;
&#20351;&#29992;YOLOv8&#12289;DeiT&#21644;SimCLR&#22312;&#24076;&#33098;&#32440;&#33609;&#36825;&#26816;&#27979;&#21644;&#35782;&#21035;&#23383;&#31526;
&lt;/p&gt;
&lt;p&gt;
Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT and SimCLR. (arXiv:2401.12513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;YOLOv8&#12289;DeiT&#21644;SimCLR&#22312;&#24076;&#33098;&#32440;&#33609;&#20013;&#36827;&#34892;&#23383;&#31526;&#26816;&#27979;&#21644;&#35782;&#21035;&#31454;&#36187;&#65292;&#22312;&#35782;&#21035;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;42.2%&#30340;mAP&#65292;&#24182;&#22312;&#26816;&#27979;&#25361;&#25112;&#20013;&#20197;51.4%&#30340;&#24179;&#22343;&#31934;&#24230;&#33719;&#24471;&#20102;&#20122;&#20891;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32440;&#33609;&#25163;&#31295;&#30340;&#22270;&#20687;&#20013;&#20998;&#31163;&#21644;&#35782;&#21035;&#21333;&#20010;&#23383;&#31526;&#30340;&#33021;&#21147;&#20026;&#25968;&#23383;&#20998;&#26512;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#8220;ICDAR 2023&#24180;&#24076;&#33098;&#32440;&#33609;&#19978;&#23383;&#31526;&#26816;&#27979;&#21644;&#35782;&#21035;&#31454;&#36187;&#8221;&#20316;&#20026;&#31532;17&#23626;&#22269;&#38469;&#25991;&#20214;&#20998;&#26512;&#21644;&#35782;&#21035;&#20250;&#35758;&#30340;&#19968;&#37096;&#20998;&#20030;&#34892;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#25105;&#20204;&#22312;&#27604;&#36187;&#20013;&#30340;&#25552;&#20132;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#32452;YOLOv8&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#20998;&#31867;&#21333;&#20010;&#23383;&#31526;&#65292;&#24182;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#23436;&#21892;&#23383;&#31526;&#39044;&#27979;&#65292;&#21253;&#25324;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;DeiT&#26041;&#27861;&#21644;&#20351;&#29992;SimCLR&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;ResNet-50&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#22312;&#35782;&#21035;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;42.2%&#30340;mAP&#65292;&#24182;&#22312;&#26816;&#27979;&#25361;&#25112;&#20013;&#20197;51.4%&#30340;&#24179;&#22343;&#31934;&#24230;&#33719;&#24471;&#20102;&#20122;&#20891;&#12290;&#22312;&#26356;&#23485;&#26494;&#30340;iou&#38408;&#20540;&#20026;0.5&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#31934;&#24230;&#21644;&#24179;&#22343;.
&lt;/p&gt;
&lt;p&gt;
The capacity to isolate and recognize individual characters from facsimile images of papyrus manuscripts yields rich opportunities for digital analysis. For this reason the `ICDAR 2023 Competition on Detection and Recognition of Greek Letters on Papyri' was held as part of the 17th International Conference on Document Analysis and Recognition. This paper discusses our submission to the competition. We used an ensemble of YOLOv8 models to detect and classify individual characters and employed two different approaches for refining the character predictions, including a transformer based DeiT approach and a ResNet-50 model trained on a large corpus of unlabelled data using SimCLR, a self-supervised learning method. Our submission won the recognition challenge with a mAP of 42.2%, and was runner-up in the detection challenge with a mean average precision (mAP) of 51.4%. At the more relaxed intersection over union threshold of 0.5, we achieved the highest mean average precision and mean ave
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Causal Bisimulation Modeling (CBM)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#26469;&#26500;&#24314;&#26368;&#23567;&#21644;&#21487;&#37325;&#29992;&#30340;&#20219;&#21153;&#29305;&#23450;&#25277;&#35937;&#12290;&#23454;&#35777;&#39564;&#35777;&#34920;&#26126;&#65292;CBM&#23398;&#20064;&#21040;&#30340;&#38544;&#24335;&#21160;&#24577;&#27169;&#22411;&#27604;&#26174;&#24335;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#24213;&#23618;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29366;&#24577;&#25277;&#35937;&#12290;</title><link>http://arxiv.org/abs/2401.12497</link><description>&lt;p&gt;
&#26500;&#24314;&#26368;&#23567;&#21644;&#21487;&#37325;&#29992;&#30340;&#22240;&#26524;&#29366;&#24577;&#25277;&#35937;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Building Minimal and Reusable Causal State Abstractions for Reinforcement Learning. (arXiv:2401.12497v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Causal Bisimulation Modeling (CBM)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#26469;&#26500;&#24314;&#26368;&#23567;&#21644;&#21487;&#37325;&#29992;&#30340;&#20219;&#21153;&#29305;&#23450;&#25277;&#35937;&#12290;&#23454;&#35777;&#39564;&#35777;&#34920;&#26126;&#65292;CBM&#23398;&#20064;&#21040;&#30340;&#38544;&#24335;&#21160;&#24577;&#27169;&#22411;&#27604;&#26174;&#24335;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#24213;&#23618;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29366;&#24577;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20004;&#20010;&#26399;&#26395;&#26159;&#33021;&#22815;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#24182;&#23398;&#20064;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#38382;&#39064;&#35268;&#33539;&#30340;&#31574;&#30053;&#12290;&#22312;&#22240;&#32032;&#21270;&#29366;&#24577;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#23398;&#20064;&#29366;&#24577;&#25277;&#35937;&#65292;&#21482;&#20445;&#30041;&#23398;&#20064;&#20219;&#21153;&#25152;&#38656;&#30340;&#21464;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Causal Bisimulation Modeling (CBM)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#30340;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#26368;&#23567;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#25277;&#35937;&#12290;CBM&#21033;&#29992;&#21644;&#25913;&#36827;&#20102;&#38544;&#24335;&#24314;&#27169;&#25216;&#26415;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#39640;&#20445;&#30495;&#24230;&#30340;&#22240;&#26524;&#21160;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#20026;&#25152;&#26377;&#20219;&#21153;&#37325;&#22797;&#20351;&#29992;&#12290;&#22312;&#25805;&#20316;&#29615;&#22659;&#21644;Deepmind Control Suite&#19978;&#30340;&#23454;&#35777;&#39564;&#35777;&#34920;&#26126;&#65292;CBM&#23398;&#20064;&#21040;&#30340;&#38544;&#24335;&#21160;&#24577;&#27169;&#22411;&#27604;&#26174;&#24335;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#24213;&#23618;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#29366;&#24577;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two desiderata of reinforcement learning (RL) algorithms are the ability to learn from relatively little experience and the ability to learn policies that generalize to a range of problem specifications. In factored state spaces, one approach towards achieving both goals is to learn state abstractions, which only keep the necessary variables for learning the tasks at hand. This paper introduces Causal Bisimulation Modeling (CBM), a method that learns the causal relationships in the dynamics and reward functions for each task to derive a minimal, task-specific abstraction. CBM leverages and improves implicit modeling to train a high-fidelity causal dynamics model that can be reused for all tasks in the same environment. Empirical validation on manipulation environments and Deepmind Control Suite reveals that CBM's learned implicit dynamics models identify the underlying causal relationships and state abstractions more accurately than explicit ones. Furthermore, the derived state abstrac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12492</link><description>&lt;p&gt;
&#27604;&#36739;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35821;&#35328;&#24314;&#27169;&#65306;&#27169;&#25311;&#32676;&#20307;&#12289;&#20010;&#20307;&#29305;&#28857;&#36824;&#26159;&#20004;&#32773;&#20860;&#39038;&#65311;
&lt;/p&gt;
&lt;p&gt;
Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#23558;&#20154;&#30340;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#65288;&#22914;45&#23681;&#20197;&#19978;&#30340;&#20154;&#32676;&#65289;&#36824;&#26159;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#26356;&#26377;&#25928;&#30340;&#38382;&#39064;&#23578;&#26410;&#30830;&#23450;&#12290;&#32676;&#20307;&#23646;&#24615;&#22312;&#25216;&#26415;&#19978;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#20294;&#26159;&#36807;&#20110;&#31895;&#31961;&#65306;&#24182;&#38750;&#25152;&#26377;45&#23681;&#20197;&#19978;&#30340;&#20154;&#37117;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#20070;&#20889;&#12290;&#30456;&#21453;&#65292;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#33021;&#22815;&#25429;&#25417;&#27599;&#20010;&#20154;&#36523;&#20221;&#30340;&#22797;&#26434;&#24615;&#65292;&#20801;&#35768;&#26356;&#20010;&#24615;&#21270;&#30340;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#27169;&#25311;&#26080;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#24182;&#19988;&#38656;&#35201;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#23558;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#30340;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#65288;&#22914;&#24180;&#40836;&#20272;&#35745;&#25110;&#20010;&#24615;&#35780;&#20272;&#65289;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#31435;&#22330;&#21644;&#20027;&#39064;&#26816;&#27979;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21019;&#36896;&#21147;&#27700;&#24179;&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#21644;&#19968;&#20010;&#21253;&#21547;700&#20010;&#38382;&#39064;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#21457;&#29616;&#21019;&#36896;&#21147;&#27700;&#24179;&#21463;&#21040;&#20219;&#21153;&#24046;&#24322;&#21644;LLM&#27169;&#22411;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.12491</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21019;&#36896;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing and Understanding Creativity in Large Language Models. (arXiv:2401.12491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21019;&#36896;&#21147;&#27700;&#24179;&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#21644;&#19968;&#20010;&#21253;&#21547;700&#20010;&#38382;&#39064;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#21457;&#29616;&#21019;&#36896;&#21147;&#27700;&#24179;&#21463;&#21040;&#20219;&#21153;&#24046;&#24322;&#21644;LLM&#27169;&#22411;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#27700;&#24179;&#30340;&#21019;&#36896;&#21147;&#65292;&#20294;&#35780;&#20272;&#36825;&#31181;&#21019;&#36896;&#21147;&#30340;&#26041;&#27861;&#23578;&#19981;&#23436;&#21892;&#12290;&#35780;&#20272;LLM&#30340;&#21019;&#36896;&#21147;&#38656;&#35201;&#32771;&#34385;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#32500;&#24230;&#30340;&#27979;&#37327;&#65292;&#21516;&#26102;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#35780;&#20272;LLM&#21019;&#36896;&#21147;&#27700;&#24179;&#30340;&#39640;&#25928;&#26694;&#26550;&#12290;&#36890;&#36807;&#25913;&#36827;&#30340;&#25176;&#20848;&#26031;&#21019;&#36896;&#24615;&#24605;&#32500;&#27979;&#35797;&#30340;&#25913;&#32534;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;7&#20010;&#20219;&#21153;&#20013;&#30340;&#21019;&#36896;&#24615;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#27969;&#30021;&#24230;&#12289;&#28789;&#27963;&#24615;&#12289;&#29420;&#21019;&#24615;&#21644;&#20016;&#23500;&#24615;&#31561;4&#20010;&#26631;&#20934;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;700&#20010;&#38382;&#39064;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#23545;LLM&#23545;&#21508;&#31181;&#25552;&#31034;&#21644;&#35282;&#33394;&#25198;&#28436;&#24773;&#22659;&#30340;&#21453;&#24212;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#21019;&#36896;&#21147;&#30340;&#27700;&#24179;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#19981;&#21516;&#65292;&#21516;&#26102;&#20063;&#21463;&#21040;LLM&#30340;&#27169;&#22411;&#21644;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of natural language processing, the rapid development of large language model (LLM) has attracted more and more attention. LLMs have shown a high level of creativity in various tasks, but the methods for assessing such creativity are inadequate. The assessment of LLM creativity needs to consider differences from humans, requiring multi-dimensional measurement while balancing accuracy and efficiency. This paper aims to establish an efficient framework for assessing the level of creativity in LLMs. By adapting the modified Torrance Tests of Creative Thinking, the research evaluates the creative performance of various LLMs across 7 tasks, emphasizing 4 criteria including Fluency, Flexibility, Originality, and Elaboration. In this context, we develop a comprehensive dataset of 700 questions for testing and an LLM-based evaluation method. In addition, this study presents a novel analysis of LLMs' responses to diverse prompts and role-play situations. We found that the creativit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#30340;&#27874;&#21160;&#26041;&#31243;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#21644;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20256;&#32479;&#30340;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.12489</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#25439;&#22833;&#30340;&#27874;&#21160;&#26041;&#31243;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning Method for the Wave Equation Based on Finite Difference Residual Constraints Loss. (arXiv:2401.12489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#30340;&#27874;&#21160;&#26041;&#31243;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#21644;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20256;&#32479;&#30340;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#21160;&#26041;&#31243;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#29289;&#29702;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#21152;&#36895;&#25110;&#26367;&#20195;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#20013;&#35299;&#20915;&#27874;&#21160;&#26041;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#30528;&#39640;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#12289;&#20302;&#35757;&#32451;&#25928;&#29575;&#20197;&#21450;&#23545;&#36793;&#30028;&#26465;&#20214;&#30340;&#19981;&#20805;&#20998;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#30340;&#27874;&#21160;&#26041;&#31243;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#32593;&#26684;&#21644;&#26377;&#38480;&#24046;&#20998;&#26041;&#27861;&#30340;&#26032;&#22411;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#65292;&#20197;&#21450;&#19968;&#31181;&#26080;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#26080;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#24182;&#39044;&#27979;&#27874;&#30340;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;&#38480;&#24046;&#20998;&#27531;&#24046;&#32422;&#26463;&#30456;&#23545;&#20110;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#32422;&#26463;&#30340;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#20302;&#30340;&#25311;&#21512;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wave equation is an important physical partial differential equation, and in recent years, deep learning has shown promise in accelerating or replacing traditional numerical methods for solving it. However, existing deep learning methods suffer from high data acquisition costs, low training efficiency, and insufficient generalization capability for boundary conditions. To address these issues, this paper proposes an unsupervised learning method for the wave equation based on finite difference residual constraints. We construct a novel finite difference residual constraint based on structured grids and finite difference methods, as well as an unsupervised training strategy, enabling convolutional neural networks to train without data and predict the forward propagation process of waves. Experimental results show that finite difference residual constraints have advantages over physics-informed neural networks (PINNs) type physical information constraints, such as easier fitting, lowe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#32477;&#28909;&#37327;&#23376;&#26041;&#27861;&#65292;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#19978;&#21462;&#24471;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#24403;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12485</link><description>&lt;p&gt;
&#32477;&#28909;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Adiabatic Quantum Support Vector Machines. (arXiv:2401.12485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#32477;&#28909;&#37327;&#23376;&#26041;&#27861;&#65292;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#19978;&#21462;&#24471;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#24403;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#26426;&#21487;&#20197;&#35299;&#20915;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65288;&#20363;&#22914;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#38382;&#39064;&#65289;&#65292;&#24182;&#19988;&#23427;&#20204;&#20284;&#20046;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#32477;&#28909;&#37327;&#23376;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#37327;&#23376;&#26041;&#27861;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#27604;&#32463;&#20856;&#26041;&#27861;&#22909;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;Iris&#65292;Wisconsin&#20083;&#33146;&#30284;&#65288;WBC&#65289;&#65292;Wine&#65292;Digits&#21644;Lambeq&#65289;&#19978;&#23558;&#25105;&#20204;&#30340;&#37327;&#23376;&#26041;&#27861;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#19982;&#20351;&#29992;Python&#20013;&#30340;Scikit-learn&#24211;&#30340;&#32463;&#20856;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#37327;&#23376;&#26041;&#27861;&#33719;&#24471;&#20102;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;&#65292;&#20854;&#20013;&#25105;&#20204;&#35745;&#31639;&#20102;&#37327;&#23376;&#26041;&#27861;&#21644;&#32463;&#20856;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#29305;&#24449;&#25968;&#37327;&#21644;&#25968;&#25454;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#24635;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#21487;&#25193;&#23637;&#24615;&#32467;&#26524;&#26174;&#31034;&#65292;&#37327;&#23376;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adiabatic quantum computers can solve difficult optimization problems (e.g., the quadratic unconstrained binary optimization problem), and they seem well suited to train machine learning models. In this paper, we describe an adiabatic quantum approach for training support vector machines. We show that the time complexity of our quantum approach is an order of magnitude better than the classical approach. Next, we compare the test accuracy of our quantum approach against a classical approach that uses the Scikit-learn library in Python across five benchmark datasets (Iris, Wisconsin Breast Cancer (WBC), Wine, Digits, and Lambeq). We show that our quantum approach obtains accuracies on par with the classical approach. Finally, we perform a scalability study in which we compute the total training times of the quantum approach and the classical approach with increasing number of features and number of data points in the training dataset. Our scalability results show that the quantum approa
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23567;&#25209;&#37327;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#38750;&#36127;&#21333;&#35843;&#21487;&#20998;&#35299;&#30340;&#23376;&#27169;&#20989;&#25968;F&#65292;&#35813;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#27604;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#30340;&#20570;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.12478</link><description>&lt;p&gt;
&#23567;&#25209;&#37327;&#23376;&#27169;&#26368;&#22823;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mini-batch Submodular Maximization. (arXiv:2401.12478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12478
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23567;&#25209;&#37327;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#38750;&#36127;&#21333;&#35843;&#21487;&#20998;&#35299;&#30340;&#23376;&#27169;&#20989;&#25968;F&#65292;&#35813;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#27604;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#30340;&#20570;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#22312;&#19968;&#32452;&#32422;&#26463;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#19968;&#20010;&#38750;&#36127;&#21333;&#35843;&#21487;&#20998;&#35299;&#30340;&#23376;&#27169;&#20989;&#25968;F&#30340;&#23567;&#25209;&#37327;&#31639;&#27861;&#65292;&#20854;&#20013;F&#31561;&#20110;$f^i$&#30340;&#21644;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#37117;&#36229;&#36234;&#20102;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#30340;&#20570;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#29983;&#25104;&#30340;&#35299;&#27604;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#35201;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first mini-batch algorithm for maximizing a non-negative monotone decomposable submodular function, $F=\sum_{i=1}^N f^i$, under a set of constraints. We improve over the sparsifier based approach both in theory and in practice. We experimentally observe that our algorithm generates solutions that are far superior to those generated by the sparsifier based approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#23492;&#23384;&#22120;&#20998;&#37197;&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;Proximal Policy Optimization&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#22270;&#30528;&#33394;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22270;&#30340;&#26631;&#35760;&#23545;&#27169;&#22411;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20855;&#26377;&#26631;&#31614;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.12470</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30528;&#33394;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29702;&#35299;&#38750;&#26631;&#31614;&#19981;&#21464;&#34920;&#31034;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Graph Coloring: Understanding the Power and Limits of Non-Label Invariant Representations. (arXiv:2401.12470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#23492;&#23384;&#22120;&#20998;&#37197;&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;Proximal Policy Optimization&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#22270;&#30528;&#33394;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22270;&#30340;&#26631;&#35760;&#23545;&#27169;&#22411;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20855;&#26377;&#26631;&#31614;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23492;&#23384;&#22120;&#20998;&#37197;&#26159;&#29616;&#20195;&#32534;&#35793;&#22120;&#20013;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#22312;&#25317;&#26377;&#20960;&#20046;&#26080;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#21464;&#37327;&#21644;&#23569;&#37327;CPU&#23492;&#23384;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#21464;&#37327;&#20998;&#37197;&#32473;&#23492;&#23384;&#22120;&#20197;&#36991;&#20813;&#20914;&#31361;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#23558;&#23492;&#23384;&#22120;&#20998;&#37197;&#38382;&#39064;&#36716;&#21270;&#20026;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;PyTorch&#21644;OpenAI Gymnasium Environments&#31561;&#25216;&#26415;&#23637;&#31034;&#20102;Proximal Policy Optimization&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#35299;&#20915;&#22270;&#30528;&#33394;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22270;&#30340;&#26631;&#35760;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#33719;&#21462;&#22270;&#30340;&#30697;&#38453;&#34920;&#31034;&#24182;&#23545;&#20854;&#36827;&#34892;&#25490;&#21015;&#26469;&#36827;&#34892;&#27979;&#35797;&#12290;&#28982;&#21518;&#27979;&#35797;&#27169;&#22411;&#22312;&#27599;&#20010;&#25490;&#21015;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#24403;&#32473;&#20986;&#21516;&#19968;&#22270;&#30340;&#37325;&#26032;&#26631;&#35760;&#26102;&#20854;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#34920;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20855;&#26377;&#26631;&#31614;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#20197;&#23454;&#29616;&#19968;&#33268;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Register allocation is one of the most important problems for modern compilers. With a practically unlimited number of user variables and a small number of CPU registers, assigning variables to registers without conflicts is a complex task. This work demonstrates the use of casting the register allocation problem as a graph coloring problem. Using technologies such as PyTorch and OpenAI Gymnasium Environments we will show that a Proximal Policy Optimization model can learn to solve the graph coloring problem. We will also show that the labeling of a graph is critical to the performance of the model by taking the matrix representation of a graph and permuting it. We then test the model's effectiveness on each of these permutations and show that it is not effective when given a relabeling of the same graph. Our main contribution lies in showing the need for label reordering invariant representations of graphs for machine learning models to achieve consistent performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25910;&#38598;&#20102;&#21476;&#20195;&#23383;&#31526;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#30002;&#39592;&#25991;&#23383;&#31526;&#22312;&#20845;&#20010;&#21382;&#21490;&#38454;&#27573;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#20026;&#35299;&#35835;&#30002;&#39592;&#25991;&#38125;&#25991;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.12467</link><description>&lt;p&gt;
&#29992;&#20110;&#30002;&#39592;&#25991;&#23383;&#31526;&#28436;&#21464;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65306;EVOBC
&lt;/p&gt;
&lt;p&gt;
An open dataset for the evolution of oracle bone characters: EVOBC. (arXiv:2401.12467v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25910;&#38598;&#20102;&#21476;&#20195;&#23383;&#31526;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#30002;&#39592;&#25991;&#23383;&#31526;&#22312;&#20845;&#20010;&#21382;&#21490;&#38454;&#27573;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#20026;&#35299;&#35835;&#30002;&#39592;&#25991;&#38125;&#25991;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26089;&#30340;&#20013;&#25991;&#23383;&#31526;&#28304;&#33258;&#30002;&#39592;&#25991;&#38125;&#25991;&#65292;&#19982;&#20854;&#20182;&#19996;&#20122;&#35821;&#35328;&#23494;&#20999;&#30456;&#20851;&#12290;&#36825;&#20123;&#38125;&#25991;&#23545;&#20154;&#31867;&#23398;&#21644;&#32771;&#21476;&#23398;&#20855;&#26377;&#24040;&#22823;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#35299;&#35835;&#30002;&#39592;&#25991;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#36804;&#20170;&#20026;&#27490;&#21482;&#26377;&#32422;1600&#20010;4500&#22810;&#20010;&#29616;&#23384;&#23383;&#31526;&#24471;&#21040;&#35808;&#37322;&#12290;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#23398;&#26415;&#30740;&#31350;&#65292;&#20840;&#38754;&#20102;&#35299;&#36825;&#31181;&#21476;&#20195;&#20070;&#20889;&#31995;&#32479;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35299;&#35835;&#30002;&#39592;&#25991;&#23383;&#31526;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#23383;&#31526;&#28436;&#21464;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#26144;&#23556;&#36825;&#20123;&#23383;&#31526;&#28436;&#21464;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25910;&#38598;&#20102;&#21476;&#20195;&#23383;&#31526;&#65292;&#28085;&#30422;&#20102;&#30002;&#39592;&#25991;&#65288;&#20844;&#20803;&#21069;15&#19990;&#32426;&#65289;&#12289;&#37329;&#25991;&#65288;&#20844;&#20803;&#21069;13&#19990;&#32426;&#33267;&#20844;&#20803;221&#24180;&#65289;&#12289;&#31686;&#20070;&#65288;&#20844;&#20803;&#21069;11&#33267;8&#19990;&#32426;&#65289;&#12289;&#31206;&#31616;&#65288;&#20844;&#20803;&#21069;221&#33267;206&#24180;&#65289;&#12289;&#23567;&#31686;&#65288;&#20844;&#20803;&#21069;206&#33267;8&#19990;&#32426;&#65289;&#12289;&#26999;&#20070;&#65288;&#20844;&#20803;2&#33267;5&#19990;&#32426;&#65289;&#36825;&#20845;&#20010;&#21382;&#21490;&#38454;&#27573;&#30340;&#25991;&#23383;&#12290;
&lt;/p&gt;
&lt;p&gt;
The earliest extant Chinese characters originate from oracle bone inscriptions, which are closely related to other East Asian languages. These inscriptions hold immense value for anthropology and archaeology. However, deciphering oracle bone script remains a formidable challenge, with only approximately 1,600 of the over 4,500 extant characters elucidated to date. Further scholarly investigation is required to comprehensively understand this ancient writing system. Artificial Intelligence technology is a promising avenue for deciphering oracle bone characters, particularly concerning their evolution. However, one of the challenges is the lack of datasets mapping the evolution of these characters over time. In this study, we systematically collected ancient characters from authoritative texts and websites spanning six historical stages: Oracle Bone Characters - OBC (15th century B.C.), Bronze Inscriptions - BI (13th to 221 B.C.), Seal Script - SS (11th to 8th centuries B.C.), Spring and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#36981;&#24490;&#31038;&#20250;&#21644;&#36947;&#24503;&#35268;&#33539;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#23433;&#20840;&#25506;&#32034;&#65292;&#24182;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26524;&#20316;&#20026;&#30452;&#25509;&#22870;&#21169;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2401.12459</link><description>&lt;p&gt;
&#26397;&#30528;&#20855;&#26377;&#31038;&#20250;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65306;&#20351;&#29992;LLM&#36827;&#34892;&#22870;&#21169;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Towards Socially and Morally Aware RL agent: Reward Design With LLM. (arXiv:2401.12459v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#36981;&#24490;&#31038;&#20250;&#21644;&#36947;&#24503;&#35268;&#33539;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#23433;&#20840;&#25506;&#32034;&#65292;&#24182;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26524;&#20316;&#20026;&#30452;&#25509;&#22870;&#21169;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#21644;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#26102;&#65292;&#22870;&#21169;&#20989;&#25968;&#28608;&#21169;&#26234;&#33021;&#20307;&#23454;&#29616;&#19968;&#20010;&#30446;&#26631;&#12290;&#30446;&#26631;&#30340;&#19981;&#27491;&#30830;&#25110;&#19981;&#23436;&#25972;&#30340;&#35268;&#33539;&#21487;&#33021;&#23548;&#33268;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#34892;&#20026;&#65292;&#19981;&#36981;&#23432;&#27169;&#31946;&#21644;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#31038;&#20250;&#21644;&#36947;&#24503;&#35268;&#33539;&#65292;&#24182;&#23548;&#33268;&#36127;&#38754;&#21103;&#20316;&#29992;&#21644;&#19981;&#23433;&#20840;&#30340;&#25506;&#32034;&#31561;&#19981;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#25163;&#21160;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#26469;&#36991;&#20813;&#36127;&#38754;&#21103;&#20316;&#29992;&#65292;&#20351;&#29992;&#20154;&#31867;&#30417;&#30563;&#36827;&#34892;&#23433;&#20840;&#25506;&#32034;&#65292;&#25110;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#35268;&#21010;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#23433;&#20840;&#25506;&#32034;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#36947;&#24503;&#21644;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19982;&#20154;&#31867;&#21453;&#39304;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30452;&#25509;&#22870;&#21169;&#20449;&#21495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When we design and deploy an Reinforcement Learning (RL) agent, reward functions motivates agents to achieve an objective. An incorrect or incomplete specification of the objective can result in behavior that does not align with human values - failing to adhere with social and moral norms that are ambiguous and context dependent, and cause undesired outcomes such as negative side effects and exploration that is unsafe. Previous work have manually defined reward functions to avoid negative side effects, use human oversight for safe exploration, or use foundation models as planning tools. This work studies the ability of leveraging Large Language Models (LLM)' understanding of morality and social norms on safe exploration augmented RL methods. This work evaluates language model's result against human feedbacks and demonstrates language model's capability as direct reward signals.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;NeRF&#30340;3D&#22330;&#26223;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#20854;&#20182;&#27169;&#22411;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;NeRF&#30340;&#32534;&#36753;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#32534;&#36753;&#21453;&#39304;&#12289;&#22810;&#27169;&#24577;&#32534;&#36753;&#12289;4D&#21512;&#25104;&#24615;&#33021;&#31561;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12456</link><description>&lt;p&gt;
&#22522;&#20110;NeRF&#30340;3D&#22330;&#26223;&#32534;&#36753;&#25216;&#26415;&#30340;&#25506;&#32034;&#19982;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Exploration and Improvement of Nerf-based 3D Scene Editing Techniques. (arXiv:2401.12456v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;NeRF&#30340;3D&#22330;&#26223;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#20854;&#20182;&#27169;&#22411;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;NeRF&#30340;&#32534;&#36753;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#32534;&#36753;&#21453;&#39304;&#12289;&#22810;&#27169;&#24577;&#32534;&#36753;&#12289;4D&#21512;&#25104;&#24615;&#33021;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NeRF&#30340;&#39640;&#36136;&#37327;&#22330;&#26223;&#21512;&#25104;&#33021;&#21147;&#22312;&#25552;&#20986;&#21518;&#30340;&#20960;&#24180;&#20013;&#36805;&#36895;&#34987;&#23398;&#32773;&#20204;&#25509;&#21463;&#65292;&#24182;&#22312;3D&#22330;&#26223;&#34920;&#31034;&#21644;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#22330;&#26223;&#30452;&#35266;&#39640;&#25928;&#30340;&#32534;&#36753;&#65292;&#20351;&#24471;NeRF&#22312;&#22330;&#26223;&#32534;&#36753;&#39046;&#22495;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#36817;&#24180;&#26469;&#23398;&#32773;&#20204;&#22312;NeRF&#22312;&#22330;&#26223;&#25110;&#29289;&#20307;&#32534;&#36753;&#39046;&#22495;&#30340;&#21021;&#27493;&#25506;&#32034;&#65292;&#20027;&#35201;&#21253;&#25324;&#22312;&#26032;&#21512;&#25104;&#22330;&#26223;&#20013;&#25913;&#21464;&#22330;&#26223;&#25110;&#29289;&#20307;&#30340;&#24418;&#29366;&#21644;&#32441;&#29702;&#65307;&#36890;&#36807;&#23558;GaN&#21644;Transformer&#31561;&#27531;&#24046;&#27169;&#22411;&#19982;NeRF&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;NeRF&#22330;&#26223;&#32534;&#36753;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21253;&#25324;&#23454;&#26102;&#26032;&#35270;&#35282;&#32534;&#36753;&#21453;&#39304;&#12289;&#25991;&#26412;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#32534;&#36753;&#12289;4D&#21512;&#25104;&#24615;&#33021;&#20197;&#21450;&#22312;&#20809;&#24433;&#32534;&#36753;&#26041;&#38754;&#30340;&#28145;&#20837;&#25506;&#32034;&#65292;&#21021;&#27493;&#23454;&#29616;&#20102;&#38388;&#25509;&#35302;&#25720;&#32534;&#36753;&#21644;&#32454;&#33410;&#34920;&#31034;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
NeRF's high-quality scene synthesis capability was quickly accepted by scholars in the years after it was proposed, and significant progress has been made in 3D scene representation and synthesis. However, the high computational cost limits intuitive and efficient editing of scenes, making NeRF's development in the scene editing field facing many challenges. This paper reviews the preliminary explorations of scholars on NeRF in the scene or object editing field in recent years, mainly changing the shape and texture of scenes or objects in new synthesized scenes; through the combination of residual models such as GaN and Transformer with NeRF, the generalization ability of NeRF scene editing has been further expanded, including realizing real-time new perspective editing feedback, multimodal editing of text synthesized 3D scenes, 4D synthesis performance, and in-depth exploration in light and shadow editing, initially achieving optimization of indirect touch editing and detail represent
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#20013;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#30340;&#22810;Agent&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#31649;&#29702;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#31995;&#32479;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#24230;&#31354;&#38388;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#26102;&#33021;&#22815;&#38477;&#20302;&#38271;&#26399;&#39118;&#38505;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.12455</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#20013;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#30340;&#22810;Agent&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent deep reinforcement learning with centralized training and decentralized execution for transportation infrastructure management. (arXiv:2401.12455v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12455
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#20013;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#30340;&#22810;Agent&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#31649;&#29702;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#31995;&#32479;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#24230;&#31354;&#38388;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#26102;&#33021;&#22815;&#38477;&#20302;&#38271;&#26399;&#39118;&#38505;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20869;&#36827;&#34892;&#31649;&#29702;&#12290;&#36825;&#31181;&#24037;&#31243;&#31995;&#32479;&#30340;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#26159;&#19968;&#20010;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36866;&#24403;&#30340;&#39034;&#24207;&#26816;&#26597;&#21644;&#32500;&#25252;&#20915;&#31574;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#26102;&#38477;&#20302;&#38271;&#26399;&#39118;&#38505;&#21644;&#25104;&#26412;&#65292;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#23384;&#22312;&#20110;&#39640;&#32500;&#31354;&#38388;&#20013;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#38745;&#24577;&#30340;&#22522;&#20110;&#24180;&#40836;&#25110;&#26465;&#20214;&#30340;&#32500;&#25252;&#26041;&#27861;&#21644;&#22522;&#20110;&#39118;&#38505;&#25110;&#23450;&#26399;&#26816;&#26597;&#35745;&#21010;&#20027;&#35201;&#35299;&#20915;&#20102;&#36825;&#31867;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#26041;&#27861;&#19979;&#65292;&#20248;&#21270;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#32463;&#24120;&#26174;&#29616;&#20986;&#26469;&#12290;&#26412;&#24037;&#20316;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#20197;&#32422;&#26463;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDPs)&#26694;&#26550;&#20026;&#22522;&#30784;&#65292;&#20026;&#20855;&#26377;&#35266;&#23519;&#19981;&#30830;&#23450;&#24615;&#12289;&#39118;&#38505;&#32771;&#34385;&#21644;&#38543;&#26426;&#39034;&#24207;&#20915;&#31574;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#32508;&#21512;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a multi-agent Deep Reinforcement Learning (DRL) framework for managing large transportation infrastructure systems over their life-cycle. Life-cycle management of such engineering systems is a computationally intensive task, requiring appropriate sequential inspection and maintenance decisions able to reduce long-term risks and costs, while dealing with different uncertainties and constraints that lie in high-dimensional spaces. To date, static age- or condition-based maintenance methods and risk-based or periodic inspection plans have mostly addressed this class of optimization problems. However, optimality, scalability, and uncertainty limitations are often manifested under such approaches. The optimization problem in this work is cast in the framework of constrained Partially Observable Markov Decision Processes (POMDPs), which provides a comprehensive mathematical basis for stochastic sequential decision settings with observation uncertainties, risk considerations, and l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#25216;&#26415;&#22312;&#25552;&#39640;&#22270;&#20687;&#28210;&#26579;&#36136;&#37327;&#26041;&#38754;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#31574;&#30053;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#28436;&#21270;&#26041;&#21521;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;NeRF&#25216;&#26415;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.12451</link><description>&lt;p&gt;
&#25552;&#39640;&#31070;&#32463;&#36752;&#23556;&#22330;&#26032;&#35270;&#22270;&#21512;&#25104;&#36136;&#37327;&#30340;&#26041;&#27861;&#21644;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Methods and strategies for improving the novel view synthesis quality of neural radiation field. (arXiv:2401.12451v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#25216;&#26415;&#22312;&#25552;&#39640;&#22270;&#20687;&#28210;&#26579;&#36136;&#37327;&#26041;&#38754;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#31574;&#30053;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#28436;&#21270;&#26041;&#21521;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;NeRF&#25216;&#26415;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#25216;&#26415;&#21487;&#20197;&#20174;2D&#22270;&#20687;&#20013;&#23398;&#20064;&#22330;&#26223;&#30340;3D&#38544;&#24335;&#27169;&#22411;&#65292;&#24182;&#21512;&#25104;&#36924;&#30495;&#30340;&#26032;&#35270;&#22270;&#22270;&#20687;&#12290;&#35813;&#25216;&#26415;&#21463;&#21040;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;NeRF&#22270;&#20687;&#28210;&#26579;&#36136;&#37327;&#38656;&#35201;&#25552;&#39640;&#30340;&#38382;&#39064;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#22312;&#36807;&#21435;&#19977;&#24180;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#28210;&#26579;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#23545;&#26368;&#26032;&#30340;&#30456;&#20851;&#35770;&#25991;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#23457;&#26597;&#65292;&#20998;&#26512;&#20102;&#36136;&#37327;&#25913;&#36827;&#32972;&#21518;&#30340;&#25216;&#26415;&#21407;&#29702;&#65292;&#24182;&#35752;&#35770;&#20102;&#36136;&#37327;&#25913;&#36827;&#26041;&#27861;&#30340;&#26410;&#26469;&#28436;&#21270;&#26041;&#21521;&#12290;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#25216;&#26415;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#28436;&#21270;&#32972;&#26223;&#65292;&#26377;&#21161;&#20110;&#28608;&#21457;&#24320;&#21457;&#26356;&#39640;&#25928;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#24182;&#25512;&#21160;NeRF&#25216;&#26415;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a scene from 2D images and synthesize realistic novel view images. This technology has received widespread attention from the industry and has good application prospects. In response to the problem that the rendering quality of NeRF images needs to be improved, many researchers have proposed various methods to improve the rendering quality in the past three years. The latest relevant papers are classified and reviewed, the technical principles behind quality improvement are analyzed, and the future evolution direction of quality improvement methods is discussed. This study can help researchers quickly understand the current state and evolutionary context of technology in this field, which is helpful in inspiring the development of more efficient algorithms and promoting the application of NeRF technology in related fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23545;&#20998;&#23376;&#20256;&#36755;&#24418;&#24335;&#19981;&#28165;&#26970;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35745;&#31639;&#25193;&#25955;&#31995;&#25968;&#21644;&#20998;&#23376;&#36895;&#24230;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12435</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#30340;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network. (arXiv:2401.12435v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23545;&#32454;&#32990;&#22806;&#38388;&#38553;&#20013;&#20998;&#23376;&#20256;&#36755;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23545;&#20998;&#23376;&#20256;&#36755;&#24418;&#24335;&#19981;&#28165;&#26970;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35745;&#31639;&#25193;&#25955;&#31995;&#25968;&#21644;&#20998;&#23376;&#36895;&#24230;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30340;&#32454;&#32990;&#22806;&#38388;&#38553; (ECS)&#26159;&#20301;&#20110;&#32454;&#32990;&#20043;&#38388;&#25110;&#32454;&#32990;&#19982;&#34880;&#31649;&#20043;&#38388;&#30340;&#19981;&#35268;&#21017;&#12289;&#26497;&#20854;&#36802;&#22238;&#30340;&#32435;&#31859;&#32423;&#31354;&#38388;&#65292;&#23545;&#31070;&#32463;&#32454;&#32990;&#30340;&#29983;&#23384;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#22312;&#35760;&#24518;&#12289;&#24773;&#32490;&#21644;&#24863;&#35273;&#31561;&#39640;&#32423;&#33041;&#21151;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;ECS&#20869;&#20998;&#23376;&#20256;&#36755;&#30340;&#20855;&#20307;&#24418;&#24335;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476; (PINN) &#35299;&#20915;&#20174;&#23545;&#27969;-&#25193;&#25955;&#26041;&#31243; (ADE) &#23548;&#20986;&#30340;&#19968;&#20010;&#36870;&#38382;&#39064;&#65292;&#23450;&#37327;&#20998;&#26512;ECS&#20869;&#30340;&#20998;&#23376;&#20256;&#36755;&#12290;PINN&#20026;ADE&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#25968;&#23398;&#20844;&#24335;&#25110;&#32593;&#26684;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;PINN&#30340;&#20248;&#21270;&#21151;&#33021;&#21487;&#33258;&#21160;&#35745;&#31639;&#20915;&#23450;&#38271;&#26399;&#20998;&#23376;&#20256;&#36755;&#30340;&#25193;&#25955;&#31995;&#25968;&#21644;&#30001;&#23545;&#27969;&#39537;&#21160;&#30340;&#20998;&#23376;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain extracellular space (ECS), an irregular, extremely tortuous nanoscale space located between cells or between cells and blood vessels, is crucial for nerve cell survival. It plays a pivotal role in high-level brain functions such as memory, emotion, and sensation. However, the specific form of molecular transport within the ECS remain elusive. To address this challenge, this paper proposes a novel approach to quantitatively analyze the molecular transport within the ECS by solving an inverse problem derived from the advection-diffusion equation (ADE) using a physics-informed neural network (PINN). PINN provides a streamlined solution to the ADE without the need for intricate mathematical formulations or grid settings. Additionally, the optimization of PINN facilitates the automatic computation of the diffusion coefficient governing long-term molecule transport and the velocity of molecules driven by advection. Consequently, the proposed method allows for the quantitative analy
&lt;/p&gt;</description></item><item><title>AdaEmbed&#26159;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#30340;&#23884;&#20837;&#31354;&#38388;&#20197;&#21450;&#29983;&#25104;&#20934;&#30830;&#19988;&#32479;&#19968;&#30340;&#20266;&#26631;&#31614;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12421</link><description>&lt;p&gt;
AdaEmbed: &#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AdaEmbed: Semi-supervised Domain Adaptation in the Embedding Space. (arXiv:2401.12421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12421
&lt;/p&gt;
&lt;p&gt;
AdaEmbed&#26159;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#30340;&#23884;&#20837;&#31354;&#38388;&#20197;&#21450;&#29983;&#25104;&#20934;&#30830;&#19988;&#32479;&#19968;&#30340;&#20266;&#26631;&#31614;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;SSDA&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#38590;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#26631;&#35760;&#25968;&#25454;&#24120;&#24120;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#31232;&#32570;&#24120;&#23548;&#33268;&#22312;&#26032;&#39046;&#22495;&#24212;&#29992;&#22522;&#30784;&#27169;&#22411;&#26102;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;AdaEmbed&#26159;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20026;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;AdaEmbed&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#20419;&#36827;&#20174;&#26377;&#26631;&#31614;&#28304;&#39046;&#22495;&#21040;&#26080;&#26631;&#31614;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#36890;&#36807;&#26681;&#25454;&#24314;&#31435;&#30340;&#23884;&#20837;&#31354;&#38388;&#29983;&#25104;&#20934;&#30830;&#19988;&#32479;&#19968;&#30340;&#20266;&#26631;&#31614;&#65292;&#35813;&#27169;&#22411;&#20811;&#26381;&#20102;&#20256;&#32479;SSDA&#30340;&#23616;&#38480;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;DomainNet&#12289;Office-Home&#21644;VisDA-C&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;AdaEmbed&#22312;&#25152;&#26377;&#22522;&#32447;&#19978;&#22343;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21019;&#36896;&#20102;&#39046;&#22495;&#20013;&#30340;&#26032;&#32426;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised domain adaptation (SSDA) presents a critical hurdle in computer vision, especially given the frequent scarcity of labeled data in real-world settings. This scarcity often causes foundation models, trained on extensive datasets, to underperform when applied to new domains. AdaEmbed, our newly proposed methodology for SSDA, offers a promising solution to these challenges. Leveraging the potential of unlabeled data, AdaEmbed facilitates the transfer of knowledge from a labeled source domain to an unlabeled target domain by learning a shared embedding space. By generating accurate and uniform pseudo-labels based on the established embedding space, the model overcomes the limitations of conventional SSDA, thus enhancing performance significantly. Our method's effectiveness is validated through extensive experiments on benchmark datasets such as DomainNet, Office-Home, and VisDA-C, where AdaEmbed consistently outperforms all the baselines, setting a new state of the art for S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#65288;LinC&#65289;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#19978;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12406</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing In-context Learning via Linear Probe Calibration. (arXiv:2401.12406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#65288;LinC&#65289;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#19978;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33539;&#24335;&#65292;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31561;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#21253;&#21547;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#25552;&#31034;&#26469;&#20026;&#26032;&#30340;&#26597;&#35810;&#36755;&#20837;&#29983;&#25104;&#30456;&#24212;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24212;&#29992;ICL&#26080;&#27861;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;&#30340;&#25552;&#31034;&#27169;&#26495;&#21644;&#28436;&#31034;&#25490;&#21015;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#20351;&#29992;ICL&#30340;GPT&#27169;&#22411;&#22522;&#20110;&#22522;&#20110;&#39321;&#20892;&#29109;&#30340;&#26032;&#24230;&#37327;&#32780;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32447;&#24615;&#25506;&#27979;&#26657;&#20934;&#65288;LinC&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26657;&#20934;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#20174;&#32780;&#24471;&#21040;&#21487;&#38752;&#30340;&#39044;&#27979;&#21644;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#19988;&#20165;&#38656;&#35201;&#26497;&#23569;&#37327;&#30340;&#39069;&#22806;&#26679;&#26412;&#65288;&#20165;&#38656;&#20116;&#20010;&#24050;&#26631;&#35760;&#30340;&#25968;&#25454;&#26679;&#26412;&#65289;&#12290;LinC&#26174;&#33879;&#25552;&#39640;&#20102;GPT&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;ICL&#27979;&#35797;&#24615;&#33021;&#65292;&#24179;&#22343;&#25913;&#21892;&#25928;&#26524;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)-like models. This approach uses prompts that include in-context demonstrations to generate the corresponding output for a new query input. However, applying ICL in real cases does not scale with the number of samples, and lacks robustness to different prompt templates and demonstration permutations. In this paper, we first show that GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy. Then, to solve this problem, we propose a new technique called the Linear Probe Calibration (LinC), a method that calibrates the model's output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples (as few as five labeled data samples). LinC significantly enhances the ICL test performance of GPT models on various benchmark datasets, with an average improve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#22768;&#26126;&#24615;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;Differentially-Private Stochastic Gradient Descent&#65288;DP-SGD&#65289;&#31639;&#27861;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26367;&#20195;&#37096;&#20998;&#23454;&#38469;&#25968;&#25454;&#26469;&#22238;&#31572;&#26597;&#35810;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#35201;&#20445;&#25252;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#26694;&#26550;&#36824;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#36716;&#25442;&#35745;&#21010;&#21644;&#36229;&#21442;&#25968;&#65292;&#24182;&#20801;&#35768;&#20154;&#24037;&#19987;&#23478;&#23457;&#26680;&#21644;&#35843;&#25972;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.12393</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#22768;&#26126;&#24615;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#32852;&#37030;&#31649;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Learning-based Declarative Privacy-Preserving Framework for Federated Data Management. (arXiv:2401.12393v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#22768;&#26126;&#24615;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;Differentially-Private Stochastic Gradient Descent&#65288;DP-SGD&#65289;&#31639;&#27861;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26367;&#20195;&#37096;&#20998;&#23454;&#38469;&#25968;&#25454;&#26469;&#22238;&#31572;&#26597;&#35810;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#35201;&#20445;&#25252;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#26694;&#26550;&#36824;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#36716;&#25442;&#35745;&#21010;&#21644;&#36229;&#21442;&#25968;&#65292;&#24182;&#20801;&#35768;&#20154;&#24037;&#19987;&#23478;&#23457;&#26680;&#21644;&#35843;&#25972;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#31169;&#26377;&#25968;&#25454;&#23396;&#23707;&#19978;&#36827;&#34892;&#32852;&#37030;&#26597;&#35810;&#22788;&#29702;&#26102;&#65292;&#24179;&#34913;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#28436;&#31034;&#19968;&#31181;&#33258;&#21160;&#21270;&#26032;&#20852;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#31471;&#21040;&#31471;&#24037;&#20316;&#27969;&#65292;&#35813;&#25216;&#26415;&#20351;&#29992;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#31639;&#27861;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26367;&#25442;&#23454;&#38469;&#25968;&#25454;&#30340;&#37096;&#20998;&#26469;&#22238;&#31572;&#26597;&#35810;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26032;&#39062;&#22768;&#26126;&#24615;&#38544;&#31169;&#20445;&#25252;&#24037;&#20316;&#27969;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#8220;&#35201;&#20445;&#25252;&#30340;&#31169;&#20154;&#20449;&#24687;&#8221;&#32780;&#19981;&#26159;&#8220;&#22914;&#20309;&#20445;&#25252;&#8221;&#12290;&#22312;&#24213;&#23618;&#65292;&#31995;&#32479;&#33258;&#21160;&#36873;&#25321;&#26597;&#35810;-&#27169;&#22411;&#36716;&#25442;&#35745;&#21010;&#20197;&#21450;&#36229;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#24037;&#20316;&#27969;&#36824;&#20801;&#35768;&#20154;&#24037;&#19987;&#23478;&#23457;&#26680;&#21644;&#35843;&#25972;&#36873;&#25321;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#65292;&#29992;&#20110;&#23457;&#35745;/&#21512;&#35268;&#21644;&#20248;&#21270;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is challenging to balance the privacy and accuracy for federated query processing over multiple private data silos. In this work, we will demonstrate an end-to-end workflow for automating an emerging privacy-preserving technique that uses a deep learning model trained using the Differentially-Private Stochastic Gradient Descent (DP-SGD) algorithm to replace portions of actual data to answer a query. Our proposed novel declarative privacy-preserving workflow allows users to specify "what private information to protect" rather than "how to protect". Under the hood, the system automatically chooses query-model transformation plans as well as hyper-parameters. At the same time, the proposed workflow also allows human experts to review and tune the selected privacy-preserving mechanism for audit/compliance, and optimization purposes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#36335;&#36793;&#24863;&#30693;&#31995;&#32479;&#24615;&#33021;&#30340;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#22320;&#27979;&#35797;&#26469;&#30830;&#20445;&#23454;&#38469;&#36866;&#29992;&#24615;&#65292;&#24182;&#22312;&#25511;&#21046;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#21508;&#31181;&#29616;&#25104;&#30340;&#24863;&#30693;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.12392</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36335;&#36793;&#24863;&#30693;&#30340;&#35780;&#20272;&#65306;&#26469;&#33258;&#29616;&#22330;&#27979;&#35797;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Evaluating Roadside Perception for Autonomous Vehicles: Insights from Field Testing. (arXiv:2401.12392v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#36335;&#36793;&#24863;&#30693;&#31995;&#32479;&#24615;&#33021;&#30340;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#22320;&#27979;&#35797;&#26469;&#30830;&#20445;&#23454;&#38469;&#36866;&#29992;&#24615;&#65292;&#24182;&#22312;&#25511;&#21046;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#21508;&#31181;&#29616;&#25104;&#30340;&#24863;&#30693;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#36793;&#24863;&#30693;&#31995;&#32479;&#22312;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#21644;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#21512;&#20316;&#39550;&#39542;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23613;&#31649;&#25216;&#26415;&#19981;&#26029;&#36827;&#27493;&#65292;&#20294;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20173;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#26469;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#12290;&#36825;&#31181;&#38480;&#21046;&#38459;&#30861;&#20102;&#26377;&#25928;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#36825;&#20010;&#37325;&#35201;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#36335;&#36793;&#24863;&#30693;&#31995;&#32479;&#24615;&#33021;&#30340;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#27979;&#37327;&#25216;&#26415;&#12289;&#25351;&#26631;&#36873;&#25321;&#21644;&#23454;&#39564;&#35797;&#39564;&#35774;&#35745;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#24314;&#31435;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#22320;&#27979;&#35797;&#22522;&#30784;&#19978;&#65292;&#20197;&#30830;&#20445;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#22312;&#25511;&#21046;&#27979;&#35797;&#29615;&#22659;Mcity&#20013;&#24212;&#29992;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#29616;&#25104;&#30340;&#24863;&#30693;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Roadside perception systems are increasingly crucial in enhancing traffic safety and facilitating cooperative driving for autonomous vehicles. Despite rapid technological advancements, a major challenge persists for this newly arising field: the absence of standardized evaluation methods and benchmarks for these systems. This limitation hampers the ability to effectively assess and compare the performance of different systems, thus constraining progress in this vital field. This paper introduces a comprehensive evaluation methodology specifically designed to assess the performance of roadside perception systems. Our methodology encompasses measurement techniques, metric selection, and experimental trial design, all grounded in real-world field testing to ensure the practical applicability of our approach.  We applied our methodology in Mcity\footnote{\url{https://mcity.umich.edu/}}, a controlled testing environment, to evaluate various off-the-shelf perception systems. This approach al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#36716;SQL&#21512;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23545;&#27604;&#20004;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#24494;&#35843;gpt-3.5-turbo-16k + gpt-4-turbo&#27169;&#22411;&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#36798;&#21040;82.1%&#12290;&#21516;&#26102;&#65292;&#23545;&#38169;&#35823;&#26597;&#35810;&#36827;&#34892;&#20998;&#31867;&#20998;&#26512;&#65292;&#25214;&#20986;&#38169;&#35823;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2401.12379</link><description>&lt;p&gt;
&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#36716;SQL&#21512;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Effectiveness of Large Language Models on Text-to-SQL Synthesis. (arXiv:2401.12379v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#36716;SQL&#21512;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23545;&#27604;&#20004;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#24494;&#35843;gpt-3.5-turbo-16k + gpt-4-turbo&#27169;&#22411;&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#36798;&#21040;82.1%&#12290;&#21516;&#26102;&#65292;&#23545;&#38169;&#35823;&#26597;&#35810;&#36827;&#34892;&#20998;&#31867;&#20998;&#26512;&#65292;&#25214;&#20986;&#38169;&#35823;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25991;&#26412;&#36716;SQL&#31243;&#24207;&#21512;&#25104;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20135;&#29983;&#30340;&#32467;&#26524;&#21644;&#27934;&#23519;&#12290;&#20351;&#29992;&#27969;&#34892;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;spider&#65292;&#30446;&#26631;&#26159;&#36755;&#20837;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#21644;&#25968;&#25454;&#24211;&#27169;&#24335;&#65292;&#36755;&#20986;&#27491;&#30830;&#30340;SQL SELECT&#26597;&#35810;&#12290;&#26368;&#21021;&#30340;&#26041;&#27861;&#26159;&#23545;&#26412;&#22320;&#21644;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;SELECT&#26597;&#35810;&#12290;&#22312;spider&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;QLoRa&#24494;&#35843;WizardLM&#30340;WizardCoder-15B&#27169;&#22411;&#21518;&#65292;&#29983;&#25104;&#26597;&#35810;&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#36798;&#21040;61%&#30340;&#39640;&#27700;&#24179;&#12290;&#36890;&#36807;&#31532;&#20108;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#24494;&#35843;&#30340;gpt-3.5-turbo-16k&#65288;Few-shot&#65289;+ gpt-4-turbo&#65288;Zero-shot&#38169;&#35823;&#20462;&#27491;&#65289;&#65292;&#25191;&#34892;&#20934;&#30830;&#29575;&#36798;&#21040;82.1%&#30340;&#39640;&#27700;&#24179;&#12290;&#22312;&#25152;&#26377;&#38169;&#35823;&#30340;&#26597;&#35810;&#20013;&#65292;&#22823;&#37096;&#20998;&#21487;&#20197;&#20998;&#20026;&#19971;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#35828;&#26126;&#20102;&#20986;&#38169;&#30340;&#21407;&#22240;&#65306;&#36873;&#25321;&#38169;&#35823;&#30340;&#21015;&#25110;&#21015;&#30340;&#39034;&#24207;&#19981;&#27491;&#30830;&#65292;&#25353;&#38169;&#35823;&#30340;&#21015;&#36827;&#34892;&#20998;&#32452;&#65292;&#39044;&#27979;&#26465;&#20214;&#20013;&#30340;&#20540;&#38169;&#35823;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;...
&lt;/p&gt;
&lt;p&gt;
This study investigates various approaches to using Large Language Models (LLMs) for Text-to-SQL program synthesis, focusing on the outcomes and insights derived. Employing the popular Text-to-SQL dataset, spider, the goal was to input a natural language question along with the database schema and output the correct SQL SELECT query. The initial approach was to fine-tune a local and open-source model to generate the SELECT query. After QLoRa fine-tuning WizardLM's WizardCoder-15B model on the spider dataset, the execution accuracy for generated queries rose to a high of 61%. With the second approach, using the fine-tuned gpt-3.5-turbo-16k (Few-shot) + gpt-4-turbo (Zero-shot error correction), the execution accuracy reached a high of 82.1%. Of all the incorrect queries, most can be categorized into a seven different categories of what went wrong: selecting the wrong columns or wrong order of columns, grouping by the wrong column, predicting the wrong values in conditionals, using differ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#24320;&#21457;&#30340;&#22522;&#20110;NLP&#30340;&#35745;&#31639;&#26426;&#21270;&#27979;&#35797;&#25351;&#21335;&#65292;&#21033;&#29992;&#35821;&#38899;&#25216;&#26415;&#21644;NLP&#25216;&#26415;&#23454;&#26102;&#25552;&#20379;&#25903;&#25345;&#21644;&#24110;&#21161;&#65292;&#24182;&#23558;&#25991;&#26412;&#38382;&#39064;&#21644;&#36873;&#39033;&#36716;&#21270;&#20026;&#26426;&#22120;&#21487;&#35835;&#30340;&#26684;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.12375</link><description>&lt;p&gt;
&#20026;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#24320;&#21457;&#22522;&#20110;NLP&#30340;&#35745;&#31639;&#26426;&#21270;&#27979;&#35797;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Development of an NLP-driven computer-based test guide for visually impaired students. (arXiv:2401.12375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#24320;&#21457;&#30340;&#22522;&#20110;NLP&#30340;&#35745;&#31639;&#26426;&#21270;&#27979;&#35797;&#25351;&#21335;&#65292;&#21033;&#29992;&#35821;&#38899;&#25216;&#26415;&#21644;NLP&#25216;&#26415;&#23454;&#26102;&#25552;&#20379;&#25903;&#25345;&#21644;&#24110;&#21161;&#65292;&#24182;&#23558;&#25991;&#26412;&#38382;&#39064;&#21644;&#36873;&#39033;&#36716;&#21270;&#20026;&#26426;&#22120;&#21487;&#35835;&#30340;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#36827;&#27493;&#25913;&#21464;&#20102;&#26080;&#38556;&#30861;&#21644;&#29420;&#21344;&#24615;&#27979;&#35797;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#65288;VIS&#65289;&#12290;&#35745;&#31639;&#26426;&#21270;&#27979;&#35797;&#65288;CBT&#65289;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#22312;&#30005;&#23376;&#21270;&#32771;&#35797;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20351;&#32771;&#35797;&#36807;&#31243;&#26356;&#21152;&#31616;&#20415;&#65292;&#25552;&#20379;&#26356;&#24555;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#32771;&#29983;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#28982;&#32780;&#65292;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#26080;&#27861;&#35775;&#38382;&#21360;&#21047;&#25991;&#20214;&#65292;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#30340;&#22522;&#20110;NLP&#30340;&#35745;&#31639;&#26426;&#21270;&#27979;&#35797;&#25351;&#21335;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#35821;&#38899;&#25216;&#26415;&#39044;&#20808;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20026;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#23454;&#26102;&#25552;&#20379;&#25903;&#25345;&#21644;&#24110;&#21161;&#12290;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#23558;&#22522;&#20110;&#25991;&#26412;&#30340;&#38382;&#39064;&#21644;&#30456;&#20851;&#36873;&#39033;&#36716;&#21270;&#20026;&#26426;&#22120;&#21487;&#35835;&#30340;&#26684;&#24335;&#12290;&#38543;&#21518;&#65292;&#35821;&#38899;&#25216;&#26415;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#22788;&#29702;&#36716;&#21270;&#21518;&#30340;&#25991;&#26412;&#65292;&#23454;&#29616;&#23545;&#35270;&#21147;&#38556;&#30861;&#23398;&#29983;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, advancements in Natural Language Processing (NLP) techniques have revolutionized the field of accessibility and exclusivity of testing, particularly for visually impaired students (VIS). CBT has shown in years back its relevance in terms of administering exams electronically, making the test process easier, providing quicker and more accurate results, and offering greater flexibility and accessibility for candidates. Yet, its relevance was not felt by the visually impaired students as they cannot access printed documents. Hence, in this paper, we present an NLP-driven Computer-Based Test guide for visually impaired students. It employs a speech technology pre-trained methods to provide real-time assistance and support to visually impaired students. The system utilizes NLP technologies to convert the text-based questions and the associated options in a machine-readable format. Subsequently, the speech technology pre-trained model processes the converted text enabling th
&lt;/p&gt;</description></item><item><title>OCT-SelfNet&#26159;&#19968;&#31181;&#29992;&#20110;&#30524;&#31185;&#30142;&#30149;&#26816;&#27979;&#30340;&#33258;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24191;&#20041;&#21644;&#40065;&#26834;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.12344</link><description>&lt;p&gt;
OCT-SelfNet:&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#21644;&#40065;&#26834;&#30340;&#35270;&#32593;&#33180;&#30142;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection. (arXiv:2401.12344v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12344
&lt;/p&gt;
&lt;p&gt;
OCT-SelfNet&#26159;&#19968;&#31181;&#29992;&#20110;&#30524;&#31185;&#30142;&#30149;&#26816;&#27979;&#30340;&#33258;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24191;&#20041;&#21644;&#40065;&#26834;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;AI&#30340;&#38761;&#21629;&#24615;&#24433;&#21709;&#21644;&#26412;&#22320;&#35757;&#32451;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#20294;&#22312;&#21307;&#23398;AI&#20013;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#23454;&#29616;&#24191;&#20041;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#24046;&#36317;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#21307;&#23398;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;OCT-SelfNet&#65292;&#29992;&#20110;&#20351;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#22270;&#20687;&#26816;&#27979;&#30524;&#31185;&#30142;&#30149;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#32508;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26426;&#26500;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#26356;&#20840;&#38754;&#30340;&#34920;&#24449;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;SwinV2&#39592;&#26550;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;&#21452;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20020;&#24202;&#23454;&#38469;&#37096;&#32626;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#19981;&#21516;&#32534;&#30721;&#22120;&#39592;&#26550;&#12289;&#20302;&#25968;&#25454;&#35774;&#32622;&#12289;&#26410;&#35265;&#25968;&#25454;&#35774;&#32622;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the revolutionary impact of AI and the development of locally trained algorithms, achieving widespread generalized learning from multi-modal data in medical AI remains a significant challenge. This gap hinders the practical deployment of scalable medical AI solutions. Addressing this challenge, our research contributes a self-supervised robust machine learning framework, OCT-SelfNet, for detecting eye diseases using optical coherence tomography (OCT) images. In this work, various data sets from various institutions are combined enabling a more comprehensive range of representation. Our method addresses the issue using a two-phase training approach that combines self-supervised pretraining and supervised fine-tuning with a mask autoencoder based on the SwinV2 backbone by providing a solution for real-world clinical deployment. Extensive experiments on three datasets with different encoder backbones, low data settings, unseen data settings, and the effect of augmentation show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#28151;&#21512;&#38750;&#37197;&#23545;&#22495;&#36716;&#25442;&#32593;&#32476;&#65288;H-CUT&#65289;&#26469;&#35299;&#20915;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;ATR&#65289;&#20013;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#36716;&#23548;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20302;&#30340;FID&#20998;&#25968;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#21644;&#29109;&#26469;&#24378;&#35843;&#39046;&#22495;&#29305;&#23450;&#21306;&#22495;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2401.12340</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#36328;&#39046;&#22495;&#36716;&#23548;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#30446;&#26631;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning and Cycle Consistency-based Transductive Transfer Learning for Target Annotation. (arXiv:2401.12340v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#28151;&#21512;&#38750;&#37197;&#23545;&#22495;&#36716;&#25442;&#32593;&#32476;&#65288;H-CUT&#65289;&#26469;&#35299;&#20915;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;ATR&#65289;&#20013;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#36716;&#23548;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20302;&#30340;FID&#20998;&#25968;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#21644;&#29109;&#26469;&#24378;&#35843;&#39046;&#22495;&#29305;&#23450;&#21306;&#22495;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;ATR&#65289;&#30340;&#27880;&#37322;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#30001;&#20110;&#30446;&#26631;&#22495;&#20013;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#20047;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#22495;&#22270;&#20687;&#30340;&#26631;&#35760;&#20449;&#24687;&#26469;&#26500;&#24314;&#26368;&#20339;&#30446;&#26631;&#22495;&#20998;&#31867;&#22120;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20808;&#21069;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#22522;&#20110;CycleGAN&#30340;&#38750;&#37197;&#23545;&#22495;&#36716;&#25442;&#32593;&#32476;&#30340;&#36328;&#39046;&#22495;&#36716;&#23548;&#36801;&#31227;&#23398;&#20064;&#65288;TTL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;ATR&#26631;&#27880;&#12290;&#23613;&#31649;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;ATR&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23427;&#20005;&#37325;&#21463;&#21040;&#27880;&#37322;&#24615;&#33021;&#36739;&#20302;&#12289;&#26356;&#39640;&#30340;Fr\'echet Inception Distance&#65288;FID&#65289;&#20998;&#25968;&#20197;&#21450;&#21512;&#25104;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#35270;&#35273;&#20266;&#24433;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#28151;&#21512;&#38750;&#37197;&#23545;&#22495;&#36716;&#25442;&#65288;H-CUT&#65289;&#32593;&#32476;&#65292;&#23427;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#20302;&#30340;FID&#20998;&#25968;&#12290;&#23427;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#21644;&#29109;&#26469;&#24378;&#35843;&#39046;&#22495;&#29305;&#23450;&#30340;&#21306;&#22495;&#65292;&#22122;&#22768;&#29305;&#24449;&#28151;&#21512;&#27169;&#22359;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating automatic target recognition (ATR) is a highly challenging task, primarily due to the unavailability of labeled data in the target domain. Hence, it is essential to construct an optimal target domain classifier by utilizing the labeled information of the source domain images. The transductive transfer learning (TTL) method that incorporates a CycleGAN-based unpaired domain translation network has been previously proposed in the literature for effective ATR annotation. Although this method demonstrates great potential for ATR, it severely suffers from lower annotation performance, higher Fr\'echet Inception Distance (FID) score, and the presence of visual artifacts in the synthetic images. To address these issues, we propose a hybrid contrastive learning base unpaired domain translation (H-CUT) network that achieves a significantly lower FID score. It incorporates both attention and entropy to emphasize the domain-specific region, a noisy feature mixup module to generate high
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20250;&#25928;&#30410;&#30340;&#21160;&#24577;&#20248;&#20808;&#20998;&#37197;&#31232;&#32570;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22478;&#24066;&#20132;&#36890;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12329</link><description>&lt;p&gt;
&#36808;&#21521;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#20248;&#20808;&#20351;&#29992;&#65306;&#22478;&#24066;&#20013;&#24515;&#36710;&#36742;&#29305;&#23450;&#21160;&#24577;&#36827;&#20837;&#38480;&#21046;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards a prioritised use of transportation infrastructures: the case of vehicle-specific dynamic access restrictions to city centres. (arXiv:2401.12329v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12329
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20250;&#25928;&#30410;&#30340;&#21160;&#24577;&#20248;&#20808;&#20998;&#37197;&#31232;&#32570;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22478;&#24066;&#20132;&#36890;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22478;&#24066;&#22320;&#26041;&#25919;&#24220;&#25152;&#38754;&#20020;&#30340;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#26159;&#22478;&#24066;&#20132;&#36890;&#31649;&#29702;&#12290;&#20182;&#20204;&#38656;&#35201;&#25552;&#20379;&#26377;&#25928;&#30340;&#20154;&#21592;&#27969;&#21160;&#21644;&#29289;&#21697;&#20998;&#37197;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#20132;&#36890;&#26381;&#21153;&#30340;&#25552;&#20379;&#38656;&#35201;&#32771;&#34385;&#21040;&#26222;&#36941;&#30340;&#20840;&#29699;&#30446;&#26631;&#65292;&#22914;&#20943;&#23569;&#25490;&#25918;&#21644;&#21019;&#36896;&#26356;&#20581;&#24247;&#30340;&#29983;&#27963;&#29615;&#22659;&#65292;&#36825;&#19982;&#20010;&#20307;&#21033;&#30410;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#12290;&#22478;&#24066;&#20132;&#36890;&#36890;&#24120;&#36890;&#36807;&#21253;&#25324;&#25903;&#25345;&#27969;&#21160;&#24615;&#30340;&#25152;&#26377;&#35201;&#32032;&#30340;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#26469;&#25552;&#20379;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#22522;&#30784;&#35774;&#26045;&#35201;&#32032;&#30340;&#23481;&#37327;&#20302;&#20110;&#23454;&#38469;&#38656;&#27714;&#65292;&#22240;&#27492;&#19981;&#21516; &#30340;&#20132;&#36890;&#27963;&#21160;&#20250;&#31454;&#20105;&#20351;&#29992;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#31232;&#32570;&#30340;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#35201;&#32032;&#24212;&#20197;&#21160;&#24577;&#21644;&#20248;&#20808;&#30340;&#26041;&#24335;&#20998;&#37197;&#32473;&#20174;&#31038;&#20250;&#35282;&#24230;&#26469;&#30475;&#20855;&#26377;&#36739;&#39640;&#25928;&#29992;&#30340;&#20132;&#36890;&#27963;&#21160;&#65307;&#20363;&#22914;&#65292;&#20135;&#29983;&#25490;&#25918;&#20943;&#23569;&#21644;&#21019;&#36896;&#26356;&#20581;&#24247;&#29983;&#27963;&#29615;&#22659;&#30340;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the main problems that local authorities of large cities have to face is the regulation of urban mobility. They need to provide the means to allow for the efficient movement of people and distribution of goods. However, the provisioning of transportation services needs to take into account general global objectives, like reducing emissions and having more healthy living environments, which may not always be aligned with individual interests. Urban mobility is usually provided through a transport infrastructure that includes all the elements that support mobility. On many occasions, the capacity of the elements of this infrastructure is lower than the actual demand and thus different transportation activities compete for their use. In this paper, we argue that scarce transport infrastructure elements should be assigned dynamically and in a prioritised manner to transport activities that have a higher utility from the point of view of society; for example, activities that produce 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36716;&#25442;&#22120;&#27169;&#22411;&#29305;&#21035;&#26159;LoRA-RoBERTa&#22312;&#25928;&#26524;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#35782;&#21035;&#65292;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#23588;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.12326</link><description>&lt;p&gt;
&#23545;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection. (arXiv:2401.12326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36716;&#25442;&#22120;&#27169;&#22411;&#29305;&#21035;&#26159;LoRA-RoBERTa&#22312;&#25928;&#26524;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#35782;&#21035;&#65292;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#23588;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SemEval-2024&#20219;&#21153;8&#24341;&#20837;&#20102;&#20174;&#22810;&#31181;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#25361;&#25112;&#12290;&#35813;&#20219;&#21153;&#30001;&#19977;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#65306;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#30340;&#20108;&#20803;&#20998;&#31867;&#65288;&#23376;&#20219;&#21153;A&#65289;&#12289;&#22810;&#31867;&#21035;&#20998;&#31867;&#65288;&#23376;&#20219;&#21153;B&#65289;&#20197;&#21450;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#65288;&#23376;&#20219;&#21153;C&#65289;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#23376;&#20219;&#21153;A&#21644;B&#12290;&#27599;&#20010;&#23376;&#20219;&#21153;&#37117;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#12289;&#24320;&#21457;&#21644;&#27979;&#35797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;1&#65289;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#39044;&#22788;&#29702;&#65288;NLP&#65289;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;2&#65289;&#23545;&#25991;&#26412;&#20998;&#31867;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LoRA-RoBERTa&#65292;&#22312;&#25928;&#26524;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22810;&#25968;&#25237;&#31080;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#23588;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
SemEval-2024 Task 8 introduces the challenge of identifying machine-generated texts from diverse Large Language Models (LLMs) in various languages and domains. The task comprises three subtasks: binary classification in monolingual and multilingual (Subtask A), multi-class classification (Subtask B), and mixed text detection (Subtask C). This paper focuses on Subtask A &amp; B. Each subtask is supported by three datasets for training, development, and testing. To tackle this task, two methods: 1) using traditional machine learning (ML) with natural language preprocessing (NLP) for feature extraction, and 2) fine-tuning LLMs for text classification. The results show that transformer models, particularly LoRA-RoBERTa, exceed traditional ML methods in effectiveness, with majority voting being particularly effective in multilingual contexts for identifying machine-generated texts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#27861;&#24459;&#20998;&#26512;&#22522;&#30784;&#19978;&#31616;&#21270;&#20808;&#36827;&#30340;&#20986;&#31199;&#36710;&#35843;&#24230;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#21644;&#24037;&#20855;&#36827;&#34892;&#21407;&#22411;&#23454;&#29616;&#26102;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12324</link><description>&lt;p&gt;
&#22312;&#27861;&#24459;&#20998;&#26512;&#22522;&#30784;&#19978;&#31616;&#21270;&#20808;&#36827;&#30340;&#20986;&#31199;&#36710;&#35843;&#24230;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Streamlining Advanced Taxi Assignment Strategies based on Legal Analysis. (arXiv:2401.12324v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#27861;&#24459;&#20998;&#26512;&#22522;&#30784;&#19978;&#31616;&#21270;&#20808;&#36827;&#30340;&#20986;&#31199;&#36710;&#35843;&#24230;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#21644;&#24037;&#20855;&#36827;&#34892;&#21407;&#22411;&#23454;&#29616;&#26102;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26032;&#39062;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20197;&#21327;&#20316;&#30340;&#26041;&#24335;&#20419;&#36827;&#26381;&#21153;&#21644;&#27963;&#21160;&#30340;&#25552;&#20379;&#12290;&#36825;&#31181;&#31995;&#32479;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#29616;&#26377;&#36164;&#28304;&#30340;&#38386;&#32622;&#25110;&#20302;&#20351;&#29992;&#29575;&#65292;&#20197;&#25552;&#20379;&#25913;&#36827;&#30340;&#26381;&#21153;&#65292;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#21151;&#33021;&#12289;&#22686;&#21152;&#25928;&#29575;&#21644;/&#25110;&#38477;&#20302;&#25104;&#26412;&#12290;&#23588;&#20854;&#22312;&#22478;&#24066;&#20132;&#36890;&#39046;&#22495;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#28982;&#21518;&#36890;&#36807;&#24120;&#24120;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#21644;&#24037;&#20855;&#36827;&#34892;&#21407;&#22411;&#30340;&#23454;&#29616;&#21644;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#35758;&#20063;&#24341;&#21457;&#20102;&#22810;&#20010;&#38750;&#25216;&#26415;&#38382;&#39064;&#65292;&#22914;&#26524;&#24819;&#23558;&#36825;&#20123;&#31995;&#32479;&#24212;&#29992;&#20110;&#23454;&#38469;&#19990;&#30028;&#65292;&#23601;&#38656;&#35201;&#23545;&#20854;&#36827;&#34892;&#20805;&#20998;&#30340;&#35782;&#21035;&#21644;&#35299;&#20915;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#30456;&#20851;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#24456;&#23569;&#22312;&#30740;&#31350;&#21644;&#24320;&#21457;&#36807;&#31243;&#21021;&#26399;&#32771;&#34385;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#38382;&#39064;&#19981;&#20165;&#38480;&#21046;&#20102;&#35774;&#35745;&#20915;&#31574;&#65292;&#21516;&#26102;&#20063;&#21046;&#32422;&#20102;&#31995;&#32479;&#33539;&#22260;&#30340;&#25193;&#23637;&#21644;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years many novel applications have appeared that promote the provision of services and activities in a collaborative manner. The key idea behind such systems is to take advantage of idle or underused capacities of existing resources, in order to provide improved services that assist people in their daily tasks, with additional functionality, enhanced efficiency, and/or reduced cost. Particularly in the domain of urban transportation, many researchers have put forward novel ideas, which are then implemented and evaluated through prototypes that usually draw upon AI methods and tools. However, such proposals also bring up multiple non-technical issues that need to be identified and addressed adequately if such systems are ever meant to be applied to the real world. While, in practice, legal and ethical aspects related to such AI-based systems are seldomly considered in the beginning of the research and development process, we argue that they not only restrict design decisions, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#20013;&#30340;&#31199;&#36710;&#26234;&#33021;&#25512;&#33616;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#36710;&#36742;&#21487;&#29992;&#24615;&#19981;&#22343;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.12322</link><description>&lt;p&gt;
&#22312;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#20013;&#30340;&#31199;&#36710;&#26234;&#33021;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Smart Recommendations for Renting Bikes in Bike Sharing Systems. (arXiv:2401.12322v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#20013;&#30340;&#31199;&#36710;&#26234;&#33021;&#25512;&#33616;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#36710;&#36742;&#21487;&#29992;&#24615;&#19981;&#22343;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36710;&#36742;&#20849;&#20139;&#31995;&#32479;&#65288;&#22914;&#33258;&#34892;&#36710;&#12289;&#27773;&#36710;&#25110;&#25705;&#25176;&#36710;&#20849;&#20139;&#31995;&#32479;&#65289;&#22312;&#22823;&#22478;&#24066;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#27604;&#31169;&#20154;&#27773;&#36710;&#26356;&#20415;&#23452;&#21644;&#29615;&#20445;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#28385;&#36275;&#20102;&#24066;&#27665;&#20010;&#20307;&#21270;&#30340;&#20986;&#34892;&#38656;&#27714;&#65292;&#27604;&#20256;&#32479;&#30340;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#26356;&#22909;&#12290;&#20854;&#20013;&#19968;&#20010;&#20248;&#21183;&#26159;&#21487;&#29992;&#24615;&#65292;&#27604;&#22914;&#22312;&#22478;&#24066;&#20013;&#20960;&#20046;&#21487;&#20197;&#38543;&#22788;&#65288;&#36824;&#65289;&#36710;&#12290;&#36825;&#31181;&#21487;&#29992;&#24615;&#26174;&#28982;&#21462;&#20915;&#20110;&#19981;&#21516;&#30340;&#25112;&#30053;&#21644;&#36816;&#33829;&#31649;&#29702;&#20915;&#31574;&#21644;&#25919;&#31574;&#65292;&#22914;&#36710;&#38431;&#35268;&#27169;&#25110;&#36710;&#36742;&#65288;&#20877;&#65289;&#20998;&#37197;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#20351;&#29992;&#27169;&#24335;&#65292;&#21487;&#29992;&#36710;&#36742;&#38598;&#20013;&#22312;&#26576;&#20123;&#22320;&#21306;&#65292;&#32780;&#20854;&#20182;&#22320;&#21306;&#27809;&#26377;&#36710;&#36742;&#30340;&#38382;&#39064;&#38750;&#24120;&#24120;&#35265;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26356;&#20855;&#20307;&#22320;&#25351;&#26126;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle-sharing systems -- such as bike-, car-, or motorcycle-sharing systems -- have become increasingly popular in big cities in recent years. On the one hand, they provide a cheaper and environmentally friendlier means of transportation than private cars, and on the other hand, they satisfy the individual mobility demands of citizens better than traditional public transport systems. One of their advantages in this regard is their availability, e.g., the possibility of taking (or leaving) a vehicle almost anywhere in a city. This availability obviously depends on different strategic and operational management decisions and policies, such as the dimension of the fleet or the (re)distribution of vehicles. Agglutination problems -- where, due to usage patterns, available vehicles are concentrated in certain areas, whereas no vehicles are available in others -- are quite common in such systems, and need to be dealt with. Research has been dedicated to this problem, specifying different t
&lt;/p&gt;</description></item><item><title>GRATH&#26159;&#19968;&#31181;&#36880;&#27493;&#33258;&#25105;&#30495;&#23454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#22806;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;GRATH&#22312;&#27809;&#26377;&#26631;&#27880;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26469;&#36880;&#27493;&#25552;&#21319;&#27169;&#22411;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12292</link><description>&lt;p&gt;
GRATH: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#28176;&#33258;&#25105;&#30495;&#23454;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GRATH: Gradual Self-Truthifying for Large Language Models. (arXiv:2401.12292v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12292
&lt;/p&gt;
&lt;p&gt;
GRATH&#26159;&#19968;&#31181;&#36880;&#27493;&#33258;&#25105;&#30495;&#23454;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#22806;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;GRATH&#22312;&#27809;&#26377;&#26631;&#27880;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26469;&#36880;&#27493;&#25552;&#21319;&#27169;&#22411;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#30495;&#23454;&#24615;&#23545;&#23427;&#20204;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#22312;&#29983;&#25104;&#30495;&#23454;&#31572;&#26696;&#21644;&#20869;&#23481;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#22914;&#22312;TruthfulQA&#31561;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GRAdual self-truTHifying (GRATH)&#65292;&#19968;&#31181;&#36890;&#36807;&#21518;&#22788;&#29702;&#26041;&#27861;&#25552;&#39640;LLMs&#30495;&#23454;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;GRATH&#21033;&#29992;&#39046;&#22495;&#22806;&#30340;&#38382;&#39064;&#25552;&#31034;&#29983;&#25104;&#30456;&#24212;&#30340;&#31572;&#26696;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#20248;&#21270;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;GRATH&#20197;&#26080;&#38656;&#26631;&#27880;&#31572;&#26696;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#30495;&#23454;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GRATH&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;LLM&#33258;&#36523;&#29983;&#25104;&#25104;&#23545;&#30495;&#23454;&#24615;&#35757;&#32451;&#25968;&#25454;&#65292;&#27599;&#23545;&#21253;&#21547;&#19968;&#20010;&#38382;&#39064;&#21450;&#20854;&#27491;&#30830;&#21644;&#38169;&#35823;&#31572;&#26696;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#20174;&#31572;&#26696;&#23545;&#30340;&#24046;&#24322;&#20013;&#23398;&#20064;&#12290;&#38543;&#21518;&#65292;GRATH&#36845;&#20195;&#22320;&#20248;&#21270;&#27169;&#22411;&#20197;&#36880;&#28176;&#25552;&#39640;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful answers and content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate corresponding answers and adaptively optimizes the model via direct preference optimization (DPO). Note that during this process, GRATH learns truthfulness in a self-supervised manner without requiring annotated answers. In particular, GRATH first generates pairwise truthfulness training data by prompting the LLM itself, with each pair containing a question and its correct and incorrect answers. The model is then fine-tuned using DPO to learn from the difference between answer pairs. Subsequently, GRATH iteratively refines the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#20851;&#31995;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#26469;&#23454;&#29616;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#25512;&#26029;&#36229;&#36793;&#32536;&#20197;&#23454;&#29616;&#32676;&#20307;&#25512;&#29702;&#21644;&#36712;&#36857;&#39044;&#27979;&#22120;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.12275</link><description>&lt;p&gt;
&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#29992;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation. (arXiv:2401.12275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#20851;&#31995;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#26469;&#23454;&#29616;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#25512;&#26029;&#36229;&#36793;&#32536;&#20197;&#23454;&#29616;&#32676;&#20307;&#25512;&#29702;&#21644;&#36712;&#36857;&#39044;&#27979;&#22120;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#22312;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#31181;&#24773;&#26223;&#19979;&#21487;&#20197;&#25552;&#20379;&#24110;&#21161;&#65292;&#20294;&#38656;&#35201;&#23433;&#20840;&#30340;&#20154;&#26426;&#20132;&#20114;&#21644;&#39640;&#25928;&#30340;&#36712;&#36857;&#35268;&#21010;&#12290;&#22312;&#22810;Agent&#20132;&#20114;&#31995;&#32479;&#20013;&#65292;&#24314;&#27169;&#25104;&#23545;&#30340;&#20851;&#31995;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#25429;&#25417;&#26356;&#22823;&#35268;&#27169;&#30340;&#32676;&#20307;&#27963;&#21160;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#27491;&#22312;&#28436;&#21464;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;Agent&#36712;&#36857;&#39044;&#27979;&#21644;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#36793;&#32536;&#65288;&#21363;Agent&#65289;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25512;&#26029;&#36229;&#36793;&#32536;&#30340;&#26041;&#27861;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#36830;&#25509;&#22810;&#20010;&#33410;&#28857;&#65292;&#20197;&#20415;&#36827;&#34892;&#32676;&#20307;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#26029;&#21160;&#24577;&#28436;&#21270;&#30340;&#20851;&#31995;&#22270;&#21644;&#36229;&#22270;&#65292;&#20197;&#25429;&#25417;&#20851;&#31995;&#30340;&#28436;&#21270;&#65292;&#36712;&#36857;&#39044;&#27979;&#22120;&#21033;&#29992;&#36825;&#20123;&#22270;&#26469;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#38160;&#24230;&#21644;&#36923;&#36753;&#31232;&#30095;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social robot navigation can be helpful in various contexts of daily life but requires safe human-robot interactions and efficient trajectory planning. While modeling pairwise relations has been widely studied in multi-agent interacting systems, the ability to capture larger-scale group-wise activities is limited. In this paper, we propose a systematic relational reasoning approach with explicit inference of the underlying dynamically evolving relational structures, and we demonstrate its effectiveness for multi-agent trajectory prediction and social robot navigation. In addition to the edges between pairs of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect multiple nodes to enable group-wise reasoning in an unsupervised manner. Our approach infers dynamically evolving relation graphs and hypergraphs to capture the evolution of relations, which the trajectory predictor employs to generate future states. Meanwhile, we propose to regularize the sharpness and sp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#30456;&#20851;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#20998;&#26512;&#20102;&#20116;&#31181;&#20027;&#35201;&#23041;&#32961;&#30340;&#20262;&#29702;&#21518;&#26524;&#65292;&#24182;&#24378;&#35843;&#20102;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#22312;&#20262;&#29702;&#35268;&#33539;&#33539;&#22260;&#20869;&#36816;&#20316;&#30340;&#32039;&#36843;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12273</link><description>&lt;p&gt;
&#20132;&#20114;&#30340;&#20262;&#29702;&#38382;&#39064;&#65306;&#32531;&#35299;LLMs&#20013;&#30340;&#23433;&#20840;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
The Ethics of Interaction: Mitigating Security Threats in LLMs. (arXiv:2401.12273v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#30456;&#20851;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#20998;&#26512;&#20102;&#20116;&#31181;&#20027;&#35201;&#23041;&#32961;&#30340;&#20262;&#29702;&#21518;&#26524;&#65292;&#24182;&#24378;&#35843;&#20102;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#22312;&#20262;&#29702;&#35268;&#33539;&#33539;&#22260;&#20869;&#36816;&#20316;&#30340;&#32039;&#36843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#30456;&#20851;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#36825;&#20123;&#22797;&#26434;&#30340;&#25968;&#23383;&#23384;&#20648;&#24211;&#26085;&#30410;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#22240;&#27492;&#25104;&#20026;&#25915;&#20987;&#30340;&#20027;&#35201;&#30446;&#26631;&#65292;&#21487;&#33021;&#21361;&#21450;&#20854;&#35757;&#32451;&#25968;&#25454;&#21644;&#25968;&#25454;&#28304;&#30340;&#26426;&#23494;&#24615;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#36825;&#20123;&#23433;&#20840;&#23041;&#32961;&#23545;&#31038;&#20250;&#21644;&#20010;&#20154;&#38544;&#31169;&#30340;&#24494;&#22937;&#20262;&#29702;&#24433;&#21709;&#12290;&#25105;&#20204;&#23545;&#20116;&#20010;&#20027;&#35201;&#23041;&#32961;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65306;&#25552;&#31034;&#27880;&#20837;&#12289;&#36234;&#29425;&#12289;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#26333;&#38706;&#12289;&#24615;&#21035;&#26174;&#38706;&#20869;&#23481;&#21644;&#22522;&#20110;&#20167;&#24680;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#19981;&#20165;&#20165;&#36827;&#34892;&#20102;&#35782;&#21035;&#65292;&#36824;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#20262;&#29702;&#21518;&#26524;&#20197;&#21450;&#23545;&#24378;&#21270;&#38450;&#24481;&#31574;&#30053;&#30340;&#32039;&#36843;&#24615;&#12290;&#23545;LLMs&#30340;&#19981;&#26029;&#20381;&#36182;&#20984;&#26174;&#20102;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#22312;&#20262;&#29702;&#35268;&#33539;&#33539;&#22260;&#20869;&#36816;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#28389;&#29992;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#31038;&#20250;&#21644;&#20010;&#20154;&#20260;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#36825;&#20123;&#31995;&#32479;&#27010;&#24565;&#21270;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper comprehensively explores the ethical challenges arising from security threats to Language Learning Models (LLMs). These intricate digital repositories are increasingly integrated into our daily lives, making them prime targets for attacks that can compromise their training data and the confidentiality of their data sources. The paper delves into the nuanced ethical repercussions of such security threats on society and individual privacy. We scrutinize five major threats: prompt injection, jailbreaking, Personal Identifiable Information (PII) exposure, sexually explicit content, and hate based content, going beyond mere identification to assess their critical ethical consequences and the urgency they create for robust defensive strategies. The escalating reliance on LLMs underscores the crucial need for ensuring these systems operate within the bounds of ethical norms, particularly as their misuse can lead to significant societal and individual harm. We propose conceptualizin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;AI&#35270;&#35273;&#27169;&#22411;&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20197;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#35299;&#37322;&#25928;&#29992;&#21644;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12261</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#20998;&#26512;AI&#35270;&#35273;&#27169;&#22411;&#30340;&#36136;&#37327;&#23646;&#24615;&#21450;&#20854;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Quality Attributes of AI Vision Models in Open Repositories Under Adversarial Attacks. (arXiv:2401.12261v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;AI&#35270;&#35273;&#27169;&#22411;&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20197;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#35299;&#37322;&#25928;&#29992;&#21644;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#32463;&#24120;&#21457;&#24067;&#21040;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#65292;&#22914;HuggingFace&#12290;&#22312;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#29983;&#20135;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20043;&#21069;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#36136;&#37327;&#20445;&#35777;&#39564;&#35777;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#38500;&#20102;&#35780;&#20272;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#30340;&#25928;&#29575;&#22806;&#65292;&#23545;&#25239;&#25915;&#20987;&#21487;&#33021;&#23545;AI&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26500;&#25104;&#23041;&#32961;&#12290;&#21516;&#26102;&#65292;&#21487;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#24212;&#29992;&#36817;&#20284;&#36755;&#20837;&#21040;&#36755;&#20986;&#30340;&#31639;&#27861;&#26469;&#35782;&#21035;&#36129;&#29486;&#29305;&#24449;&#12290;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#20250;&#38477;&#20302;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;XAI&#35299;&#37322;&#30340;&#25928;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#19979;&#28216;&#35780;&#20272;&#20219;&#21153;&#65292;&#21253;&#25324;&#39564;&#35777;AI&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#29992;&#22522;&#20934;&#25200;&#21160;&#35780;&#20272;&#40065;&#26834;&#24615;&#65292;&#27604;&#36739;&#35299;&#37322;&#25928;&#29992;&#20197;&#21450;&#35780;&#20272;&#24320;&#38144;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI models rapidly evolve, they are frequently released to open repositories, such as HuggingFace. It is essential to perform quality assurance validation on these models before integrating them into the production development lifecycle. In addition to evaluating efficiency in terms of balanced accuracy and computing costs, adversarial attacks are potential threats to the robustness and explainability of AI models. Meanwhile, XAI applies algorithms that approximate inputs to outputs post-hoc to identify the contributing features. Adversarial perturbations may also degrade the utility of XAI explanations that require further investigation. In this paper, we present an integrated process designed for downstream evaluation tasks, including validating AI model accuracy, evaluating robustness with benchmark perturbations, comparing explanation utility, and assessing overhead. We demonstrate an evaluation scenario involving six computer vision models, which include CNN-based, Transformer-b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21327;&#35758;&#25216;&#26415;&#22312;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#21327;&#35843;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#26234;&#33021;&#20132;&#36890;&#12289;&#26234;&#33021;&#33021;&#28304;&#32593;&#26684;&#21644;&#21442;&#19982;&#24615;&#27835;&#29702;&#31561;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25216;&#26415;&#22522;&#20110;&#21327;&#35758;&#27010;&#24565;&#65292;&#36890;&#36807;&#24320;&#21457;&#24320;&#25918;&#30340;&#22810;agent&#31995;&#32479;&#26469;&#23454;&#29616;&#21327;&#35843;&#12290;</title><link>http://arxiv.org/abs/2401.12259</link><description>&lt;p&gt;
&#21327;&#35758;&#25216;&#26415;&#22312;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#21327;&#35843;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Agreement Technologies for Coordination in Smart Cities. (arXiv:2401.12259v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21327;&#35758;&#25216;&#26415;&#22312;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#21327;&#35843;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#26234;&#33021;&#20132;&#36890;&#12289;&#26234;&#33021;&#33021;&#28304;&#32593;&#26684;&#21644;&#21442;&#19982;&#24615;&#27835;&#29702;&#31561;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25216;&#26415;&#22522;&#20110;&#21327;&#35758;&#27010;&#24565;&#65292;&#36890;&#36807;&#24320;&#21457;&#24320;&#25918;&#30340;&#22810;agent&#31995;&#32479;&#26469;&#23454;&#29616;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31038;&#20250;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#24067;&#24335;&#24320;&#25918;&#31995;&#32479;&#26469;&#35299;&#20915;&#12290;&#36825;&#22312;&#26234;&#33021;&#22478;&#24066;&#36825;&#26679;&#30340;&#39046;&#22495;&#23588;&#20854;&#22914;&#27492;&#65292;&#21253;&#25324;&#26234;&#33021;&#20132;&#36890;&#12289;&#26234;&#33021;&#33021;&#28304;&#32593;&#32476;&#21644;&#21442;&#19982;&#24615;&#27835;&#29702;&#31561;&#12290;&#22312;&#20026;&#36825;&#20123;&#39046;&#22495;&#35774;&#35745;&#30005;&#33041;&#24212;&#29992;&#26102;&#65292;&#38656;&#35201;&#32771;&#34385;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#31995;&#32479;&#30340;&#20803;&#32032;&#65288;&#36890;&#24120;&#31216;&#20026;&#36719;&#20214;&#20195;&#29702;&#65289;&#36890;&#24120;&#26469;&#33258;&#19981;&#21516;&#30340;&#35774;&#35745;&#24072;&#65292;&#24182;&#20195;&#34920;&#29305;&#23450;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;&#27492;&#22806;&#65292;&#22312;&#35774;&#35745;&#26102;&#26080;&#27861;&#30830;&#23450;&#36825;&#20123;&#20195;&#29702;&#20309;&#26102;&#21152;&#20837;&#25110;&#31163;&#24320;&#31995;&#32479;&#65292;&#20197;&#21450;&#26032;&#20195;&#29702;&#23558;&#20195;&#34920;&#20160;&#20040;&#21033;&#30410;&#12290;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#36827;&#34892;&#21327;&#35843;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36890;&#24120;&#21482;&#33021;&#22312;&#36816;&#34892;&#26102;&#30452;&#25509;&#25511;&#21046;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#12290;&#21327;&#35758;&#25216;&#26415;&#26159;&#25351;&#22522;&#20110;&#21327;&#35758;&#27010;&#24565;&#30340;&#19968;&#32452;&#24037;&#20855;&#21644;&#26426;&#21046;&#65292;&#29992;&#20110;&#24320;&#21457;&#36825;&#31181;&#24320;&#25918;&#30340;&#22810;agent&#31995;&#32479;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21327;&#35758;&#25216;&#26415;&#26159;&#35299;&#20915;&#26234;&#33021;&#22478;&#24066;&#21327;&#35843;&#38382;&#39064;&#30340;&#26377;&#25928;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many challenges in today's society can be tackled by distributed open systems. This is particularly true for domains that are commonly perceived under the umbrella of smart cities, such as intelligent transportation, smart energy grids, or participative governance. When designing computer applications for these domains, it is necessary to account for the fact that the elements of such systems, often called software agents, are usually made by different designers and act on behalf of particular stakeholders. Furthermore, it is unknown at design time when such agents will enter or leave the system, and what interests new agents will represent. To instil coordination in such systems is particularly demanding, as usually only part of them can be directly controlled at runtime. Agreement technologies refer to a sandbox of tools and mechanisms for the development of such open multiagent systems, which are based on the notion of agreement. In this paper, we argue that agreement technologies a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12255</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#23545;&#27169;&#22411;&#36827;&#34892;&#25351;&#32441;&#35782;&#21035;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#25104;&#20026;&#24517;&#35201;&#65292;&#36890;&#36807;&#25152;&#26377;&#26435;&#35748;&#35777;&#24182;&#30830;&#20445;&#19979;&#28216;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#65288;&#22914;&#38480;&#21046;&#21830;&#19994;&#20351;&#29992;&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#25351;&#32441;&#35782;&#21035;&#30340;&#35797;&#28857;&#30740;&#31350;&#65292;&#20316;&#20026;&#19968;&#31181;&#38750;&#24120;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#24418;&#24335;&#12290;&#27169;&#22411;&#21457;&#24067;&#32773;&#25351;&#23450;&#19968;&#20010;&#26426;&#23494;&#30340;&#31169;&#38053;&#65292;&#24182;&#23558;&#20854;&#26893;&#20837;&#20026;&#19968;&#20010;&#25351;&#20196;&#21518;&#38376;&#65292;&#24403;&#23494;&#38053;&#23384;&#22312;&#26102;&#65292;&#23548;&#33268;LLM&#29983;&#25104;&#29305;&#23450;&#30340;&#25991;&#26412;&#12290;&#23545;11&#20010;&#24120;&#29992;LLMs&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36731;&#37327;&#32423;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#12290;&#23427;&#36824;&#21487;&#20197;&#38450;&#27490;&#21457;&#24067;&#32773;&#36807;&#24230;&#23459;&#31216;&#65292;&#23545;&#25351;&#32441;&#29468;&#27979;&#21644;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#31867;&#20284;&#20110;MIT&#35768;&#21487;&#35777;&#30340;&#22810;&#38454;&#27573;&#25351;&#32441;&#35782;&#21035;&#12290;&#20195;&#30721;&#21487;&#22312;https://cnut1648.github.io/Model-Fingerprint/&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28040;&#36153;&#32773;&#23545;&#30005;&#23376;&#21830;&#21153;&#20013;&#22522;&#20110;&#25991;&#26412;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21453;&#24212;&#65292;&#21457;&#29616;&#20849;&#24773;&#21644;&#21451;&#22909;&#24230;&#23545;&#20449;&#20219;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#20219;&#21153;&#22797;&#26434;&#24615;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#25259;&#38706;&#23545;&#36825;&#20123;&#24433;&#21709;&#20855;&#26377;&#35843;&#33410;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.12247</link><description>&lt;p&gt;
&#25506;&#32034;&#28040;&#36153;&#32773;&#23545;&#30005;&#23376;&#21830;&#21153;&#20013;&#22522;&#20110;&#25991;&#26412;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21453;&#24212;&#65306;&#20219;&#21153;&#22797;&#26434;&#24615;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#25259;&#38706;&#30340;&#35843;&#33410;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring consumers response to text-based chatbots in e-commerce: The moderating role of task complexity and chatbot disclosure. (arXiv:2401.12247v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28040;&#36153;&#32773;&#23545;&#30005;&#23376;&#21830;&#21153;&#20013;&#22522;&#20110;&#25991;&#26412;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21453;&#24212;&#65292;&#21457;&#29616;&#20849;&#24773;&#21644;&#21451;&#22909;&#24230;&#23545;&#20449;&#20219;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#20219;&#21153;&#22797;&#26434;&#24615;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#25259;&#38706;&#23545;&#36825;&#20123;&#24433;&#21709;&#20855;&#26377;&#35843;&#33410;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21830;&#19994;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#28040;&#36153;&#32773;&#23545;&#30005;&#23376;&#21830;&#21153;&#20013;&#22522;&#20110;&#25991;&#26412;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20449;&#20219;&#21644;&#21453;&#24212;&#65292;&#20197;&#21450;&#20219;&#21153;&#22797;&#26434;&#24615;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#36523;&#20221;&#25259;&#38706;&#30340;&#35843;&#33410;&#25928;&#24212;&#12290;&#36890;&#36807;299&#20010;&#21487;&#29992;&#30340;&#35843;&#26597;&#38382;&#21367;&#25910;&#38598;&#20102;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#20998;&#26512;&#26469;&#27979;&#35797;&#20551;&#35774;&#12290;&#39318;&#20808;&#65292;&#28040;&#36153;&#32773;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20849;&#24773;&#21644;&#21451;&#22909;&#24230;&#30340;&#35748;&#30693;&#23545;&#20182;&#20204;&#30340;&#20449;&#20219;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#36127;&#21521;&#35843;&#33410;&#20102;&#21451;&#22909;&#24230;&#19982;&#28040;&#36153;&#32773;&#20449;&#20219;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#31532;&#19977;&#65292;&#25991;&#26412;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25259;&#38706;&#36127;&#21521;&#35843;&#33410;&#20102;&#20849;&#24773;&#19982;&#28040;&#36153;&#32773;&#20449;&#20219;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#27491;&#21521;&#35843;&#33410;&#20102;&#21451;&#22909;&#24230;&#19982;&#28040;&#36153;&#32773;&#20449;&#20219;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#31532;&#22235;&#65292;&#28040;&#36153;&#32773;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20449;&#20219;&#22686;&#21152;&#20102;&#20182;&#20204;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20381;&#36182;&#65292;&#24182;&#20943;&#23569;&#20102;&#20182;&#20204;&#23545;&#26410;&#26469;&#20114;&#21160;&#20013;&#30340;&#25269;&#25239;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence based chatbots have brought unprecedented business potential. This study aims to explore consumers trust and response to a text-based chatbot in ecommerce, involving the moderating effects of task complexity and chatbot identity disclosure. A survey method with 299 useable responses was conducted in this research. This study adopted the ordinary least squares regression to test the hypotheses. First, the consumers perception of both the empathy and friendliness of the chatbot positively impacts their trust in it. Second, task complexity negatively moderates the relationship between friendliness and consumers trust. Third, disclosure of the text based chatbot negatively moderates the relationship between empathy and consumers trust, while it positively moderates the relationship between friendliness and consumers trust. Fourth, consumers trust in the chatbot increases their reliance on the chatbot and decreases their resistance to the chatbot in future interactio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#29983;&#25104;&#26356;&#21463;&#20154;&#31867;&#21916;&#27426;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.12244</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large-scale Reinforcement Learning for Diffusion Models. (arXiv:2401.12244v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#29983;&#25104;&#26356;&#21463;&#20154;&#31867;&#21916;&#27426;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#32593;&#39029;&#35268;&#27169;&#30340;&#25991;&#26412;-&#22270;&#20687;&#35757;&#32451;&#23545;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#22320;&#24314;&#27169;&#25105;&#20204;&#20851;&#24515;&#30340;&#22270;&#20687;&#26041;&#38754;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#26679;&#26412;&#12289;&#27169;&#22411;&#20559;&#35265;&#21644;&#19982;&#20154;&#31867;&#36947;&#24503;&#21644;&#21916;&#22909;&#19981;&#31526;&#30340;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#25968;&#30334;&#19975;&#20010;&#22270;&#20687;&#30340;&#20154;&#31867;&#20559;&#22909;&#12289;&#32452;&#21512;&#24615;&#21644;&#20844;&#24179;&#24615;&#31561;&#22810;&#26679;&#30340;&#22238;&#25253;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#22823;&#22823;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#20351;&#25193;&#25955;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#36825;&#22914;&#20309;&#22823;&#22823;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#30340;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#27169;&#22411;&#65292;&#25152;&#29983;&#25104;&#30340;&#26679;&#26412;&#22312;80.3%&#30340;&#26102;&#38388;&#20869;&#20248;&#20110;&#22522;&#26412;SD&#27169;&#22411;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models are a class of deep generative models that have demonstrated an impressive capacity for high-quality image generation. However, these models are susceptible to implicit biases that arise from web-scale text-image training pairs and may inaccurately model aspects of images we care about. This can result in suboptimal samples, model bias, and images that do not align with human ethics and preferences. In this paper, we present an effective scalable algorithm to improve diffusion models using Reinforcement Learning (RL) across a diverse set of reward functions, such as human preference, compositionality, and fairness over millions of images. We illustrate how our approach substantially outperforms existing methods for aligning diffusion models with human preferences. We further illustrate how this substantially improves pretrained Stable Diffusion (SD) models, generating samples that are preferred by humans 80.3% of the time over those from the base SD model
&lt;/p&gt;</description></item><item><title>LLMs have shown promising potential in achieving fully automated chip design and generating circuits with improved power, performance, and area (PPA) in the field of Electronic Design Automation (EDA). (&#32763;&#35793;&#20026;&#65306;LLM&#22312;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#23454;&#29616;&#20840;&#33258;&#21160;&#21270;&#33455;&#29255;&#35774;&#35745;&#21644;&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#21151;&#32791;&#12289;&#24615;&#33021;&#21644;&#38754;&#31215;&#65288;PPA&#65289;&#30340;&#30005;&#36335;&#30340;&#28508;&#21147;)</title><link>http://arxiv.org/abs/2401.12224</link><description>&lt;p&gt;
LLM4EDA&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#20013;&#30340;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
LLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation. (arXiv:2401.12224v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12224
&lt;/p&gt;
&lt;p&gt;
LLMs have shown promising potential in achieving fully automated chip design and generating circuits with improved power, performance, and area (PPA) in the field of Electronic Design Automation (EDA). (&#32763;&#35793;&#20026;&#65306;LLM&#22312;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#23454;&#29616;&#20840;&#33258;&#21160;&#21270;&#33455;&#29255;&#35774;&#35745;&#21644;&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#21151;&#32791;&#12289;&#24615;&#33021;&#21644;&#38754;&#31215;&#65288;PPA&#65289;&#30340;&#30005;&#36335;&#30340;&#28508;&#21147;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39537;&#21160;&#30528;&#25705;&#23572;&#23450;&#24459;&#65292;&#29616;&#20195;&#33455;&#29255;&#35774;&#35745;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#12290;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#23436;&#25972;&#33455;&#29255;&#35774;&#35745;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36229;&#22823;&#35268;&#27169;&#38598;&#25104;&#30005;&#36335;&#30340;&#28436;&#36827;&#20351;&#24471;&#33455;&#29255;&#35774;&#35745;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#22312;&#23547;&#27714;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#20013;&#38388;&#30340;&#20154;&#24037;&#25511;&#21046;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#31995;&#32479;&#35774;&#35745;&#38454;&#27573;&#65292;&#30005;&#36335;&#36890;&#24120;&#29992;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65288;HDL&#65289;&#20316;&#20026;&#25991;&#26412;&#26684;&#24335;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#31572;&#26696;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30005;&#36335;&#21487;&#20197;&#29992;HDL&#20316;&#20026;&#25991;&#26412;&#26684;&#24335;&#34920;&#31034;&#65292;&#21512;&#29702;&#22320;&#36136;&#30097;LLM&#33021;&#21542;&#22312;EDA&#39046;&#22495;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#33455;&#29255;&#35774;&#35745;&#24182;&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#30340;&#21151;&#32791;&#12289;&#24615;&#33021;&#21644;&#38754;&#31215;&#65288;PPA&#65289;&#30340;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by Moore's Law, the complexity and scale of modern chip design are increasing rapidly. Electronic Design Automation (EDA) has been widely applied to address the challenges encountered in the full chip design process. However, the evolution of very large-scale integrated circuits has made chip design time-consuming and resource-intensive, requiring substantial prior expert knowledge. Additionally, intermediate human control activities are crucial for seeking optimal solutions. In system design stage, circuits are usually represented with Hardware Description Language (HDL) as a textual format. Recently, Large Language Models (LLMs) have demonstrated their capability in context understanding, logic reasoning and answer generation. Since circuit can be represented with HDL in a textual format, it is reasonable to question whether LLMs can be leveraged in the EDA field to achieve fully automated chip design and generate circuits with improved power, performance, and area (PPA). In t
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#39033;&#26032;&#20852;&#25216;&#26415;&#65292;&#26377;&#28508;&#21147;&#25913;&#21464;&#31038;&#20250;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#20294;&#20063;&#23384;&#22312;&#32463;&#27982;&#12289;&#36947;&#24503;&#12289;&#31038;&#20250;&#12289;&#23433;&#20840;&#19982;&#38544;&#31169;&#20197;&#21450;&#23601;&#19994;&#26041;&#38754;&#30340;&#39118;&#38505;&#21644;&#25361;&#25112;&#12290;&#20026;&#20102;&#30830;&#20445;AI&#30340;&#36127;&#36131;&#20219;&#21457;&#23637;&#19982;&#24212;&#29992;&#65292;&#25919;&#24220;&#12289;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#20043;&#38388;&#30340;&#21512;&#20316;&#20197;&#21450;&#20844;&#20247;&#30340;&#21442;&#19982;&#21644;&#25945;&#32946;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.12223</link><description>&lt;p&gt;
AI-&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#24433;&#21709;&#65306;&#26368;&#26032;&#36827;&#23637;&#19982;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The Global Impact of AI-Artificial Intelligence: Recent Advances and Future Directions, A Review. (arXiv:2401.12223v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12223
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#39033;&#26032;&#20852;&#25216;&#26415;&#65292;&#26377;&#28508;&#21147;&#25913;&#21464;&#31038;&#20250;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#20294;&#20063;&#23384;&#22312;&#32463;&#27982;&#12289;&#36947;&#24503;&#12289;&#31038;&#20250;&#12289;&#23433;&#20840;&#19982;&#38544;&#31169;&#20197;&#21450;&#23601;&#19994;&#26041;&#38754;&#30340;&#39118;&#38505;&#21644;&#25361;&#25112;&#12290;&#20026;&#20102;&#30830;&#20445;AI&#30340;&#36127;&#36131;&#20219;&#21457;&#23637;&#19982;&#24212;&#29992;&#65292;&#25919;&#24220;&#12289;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#20043;&#38388;&#30340;&#21512;&#20316;&#20197;&#21450;&#20844;&#20247;&#30340;&#21442;&#19982;&#21644;&#25945;&#32946;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#19968;&#39033;&#26032;&#20852;&#25216;&#26415;&#65292;&#26377;&#28508;&#21147;&#25913;&#21464;&#31038;&#20250;&#30340;&#35768;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#32463;&#27982;&#12289;&#21307;&#30103;&#21644;&#20132;&#36890;&#31561;&#12290;&#26412;&#25991;&#32508;&#21512;&#20102;&#20851;&#20110;AI&#20840;&#29699;&#24433;&#21709;&#30340;&#26368;&#26032;&#30740;&#31350;&#25991;&#29486;&#65292;&#25506;&#35752;&#20102;&#20854;&#28508;&#22312;&#30340;&#30410;&#22788;&#21644;&#39118;&#38505;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;AI&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#20854;&#23545;&#32463;&#27982;&#12289;&#36947;&#24503;&#12289;&#31038;&#20250;&#12289;&#23433;&#20840;&#19982;&#38544;&#31169;&#20197;&#21450;&#23601;&#19994;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#23427;&#35752;&#35770;&#20102;AI&#24320;&#21457;&#25152;&#28041;&#21450;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#23433;&#20840;&#21644;&#38544;&#31169;&#20405;&#29359;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#30830;&#20445;AI&#30340;&#36127;&#36131;&#20219;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#25919;&#24220;&#12289;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#20043;&#38388;&#30340;&#21512;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#25991;&#31456;&#26368;&#21518;&#24378;&#35843;&#20102;&#20844;&#20247;&#21442;&#19982;&#21644;&#25945;&#32946;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20419;&#36827;&#23545;AI&#23545;&#31038;&#20250;&#24433;&#21709;&#30340;&#24847;&#35782;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is an emerging technology that has the potential to transform many aspects of society, including the economy, healthcare, and transportation. This article synthesizes recent research literature on the global impact of AI, exploring its potential benefits and risks. The article highlights the implications of AI, including its impact on economic, ethical, social, security &amp; privacy, and job displacement aspects. It discusses the ethical concerns surrounding AI development, including issues of bias, security, and privacy violations. To ensure the responsible development and deployment of AI, collaboration between government, industry, and academia is essential. The article concludes by emphasizing the importance of public engagement and education to promote awareness and understanding of AI's impact on society at large.
&lt;/p&gt;</description></item><item><title>&#20108;&#20540;&#21270;Transformer&#22312;&#36793;&#32536;&#37096;&#32626;&#20013;&#26377;&#28508;&#21147;&#65292;&#20294;&#38754;&#20020;QMM&#25191;&#34892;&#25928;&#29575;&#20302;&#21644;&#33021;&#32791;&#24320;&#38144;&#39640;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#27969;&#25277;&#35937;&#26041;&#27861;&#20248;&#21270;QMM&#25191;&#34892;&#25928;&#29575;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20108;&#20540;&#21270;&#33410;&#33021;Transformer&#21152;&#36895;&#22120;BETA&#65292;&#20855;&#26377;&#21487;&#37197;&#32622;&#30340;&#39640;&#24182;&#34892;&#24615;&#21644;&#39640;&#33021;&#25928;&#30340;QMM&#24341;&#25806;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;BETA&#22312;ZCU102 FPGA&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;&#33021;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.11851</link><description>&lt;p&gt;
BETA&#65306;&#36793;&#32536;&#20108;&#20540;&#21270;&#33410;&#33021;Transformer&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
BETA: Binarized Energy-Efficient Transformer Accelerator at the Edge. (arXiv:2401.11851v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11851
&lt;/p&gt;
&lt;p&gt;
&#20108;&#20540;&#21270;Transformer&#22312;&#36793;&#32536;&#37096;&#32626;&#20013;&#26377;&#28508;&#21147;&#65292;&#20294;&#38754;&#20020;QMM&#25191;&#34892;&#25928;&#29575;&#20302;&#21644;&#33021;&#32791;&#24320;&#38144;&#39640;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#27969;&#25277;&#35937;&#26041;&#27861;&#20248;&#21270;QMM&#25191;&#34892;&#25928;&#29575;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20108;&#20540;&#21270;&#33410;&#33021;Transformer&#21152;&#36895;&#22120;BETA&#65292;&#20855;&#26377;&#21487;&#37197;&#32622;&#30340;&#39640;&#24182;&#34892;&#24615;&#21644;&#39640;&#33021;&#25928;&#30340;QMM&#24341;&#25806;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;BETA&#22312;ZCU102 FPGA&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20108;&#20540;&#21270;Transformer&#30001;&#20110;&#20854;&#32039;&#20945;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#21487;&#35266;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#65292;&#22312;&#36793;&#32536;&#37096;&#32626;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#20108;&#20540;&#21270;Transformer&#38754;&#20020;&#30528;&#20808;&#21069;&#22788;&#29702;&#22120;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;(QMM)&#30340;&#20302;&#25928;&#25191;&#34892;&#21644;&#22810;&#31934;&#24230;&#28608;&#27963;&#24102;&#26469;&#30340;&#33021;&#32791;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#20108;&#20540;&#21270;Transformer&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#27969;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#35745;&#31639;&#39034;&#24207;&#26469;&#25913;&#21892;QMM&#30340;&#25191;&#34892;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BETA&#30340;&#20108;&#20540;&#21270;&#33410;&#33021;Transformer&#21152;&#36895;&#22120;&#65292;&#20197;&#25552;&#21319;&#22312;&#36793;&#32536;&#30340;&#39640;&#25928;&#37096;&#32626;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;BETA&#20855;&#26377;&#21487;&#37197;&#32622;&#30340;QMM&#24341;&#25806;&#65292;&#36866;&#24212;&#20108;&#20540;&#21270;Transformer&#30340;&#19981;&#21516;&#28608;&#27963;&#31934;&#24230;&#65292;&#24182;&#20026;&#20855;&#26377;&#20986;&#33394;&#33021;&#25928;&#30340;QMM&#25552;&#20379;&#39640;&#24182;&#34892;&#24615;&#21644;&#39640;&#36895;&#24230;&#12290;&#22312;ZCU102 FPGA&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BETA&#23454;&#29616;&#20102;&#24179;&#22343;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing binary Transformers are promising in edge deployment due to their compact model size, low computational complexity, and considerable inference accuracy. However, deploying binary Transformers faces challenges on prior processors due to inefficient execution of quantized matrix multiplication (QMM) and the energy consumption overhead caused by multi-precision activations. To tackle the challenges above, we first develop a computation flow abstraction method for binary Transformers to improve QMM execution efficiency by optimizing the computation order. Furthermore, a binarized energy-efficient Transformer accelerator, namely BETA, is proposed to boost the efficient deployment at the edge. Notably, BETA features a configurable QMM engine, accommodating diverse activation precisions of binary Transformers and offering high-parallelism and high-speed for QMMs with impressive energy efficiency. Experimental results evaluated on ZCU102 FPGA show BETA achieves an average energy effic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11792</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#29615;&#22659;&#21644;&#36710;&#36742;&#29366;&#24577;&#21160;&#24577;&#21046;&#23450;&#36866;&#24403;&#30340;&#39550;&#39542;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23433;&#20840;&#24615;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#65292;&#32780;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#26159;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#30340;&#21069;&#25552;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#32780;&#22810;&#26679;&#22330;&#26223;&#19979;&#30340;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#12290;&#25105;&#20204;&#30340;SGADS&#19982;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#36710;&#36742;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31034;&#33539;&#30456;&#32467;&#21512;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26041;&#27861;GI-PIP&#65292;&#19981;&#38656;&#35201;&#20381;&#36182;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#36739;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#24182;&#33021;&#22312;&#22270;&#20687;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#20063;&#26356;&#24378;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.11748</link><description>&lt;p&gt;
GI-PIP&#65306;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26159;&#21542;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?. (arXiv:2401.11748v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26041;&#27861;GI-PIP&#65292;&#19981;&#38656;&#35201;&#20381;&#36182;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#36739;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#24182;&#33021;&#22312;&#22270;&#20687;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#20063;&#26356;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#36890;&#36807;&#20934;&#30830;&#22320;&#24674;&#22797;&#20849;&#20139;&#26799;&#24230;&#20013;&#30340;&#38544;&#31169;&#25968;&#25454;&#65292;&#23545;&#32852;&#37030;&#23398;&#20064;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#22312;&#35775;&#38382;&#36807;&#22810;&#30340;&#36741;&#21161;&#25968;&#25454;&#26041;&#38754;&#20381;&#36182;&#20110;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#36825;&#36829;&#21453;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#26412;&#25968;&#25454;&#20998;&#21306;&#21407;&#21017;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#23454;&#29992;&#22270;&#20687;&#20808;&#39564;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#65288;GI-PIP&#65289;&#65292;&#22312;&#32463;&#36807;&#20462;&#35746;&#30340;&#23041;&#32961;&#27169;&#22411;&#19979;&#12290;GI-PIP&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#26356;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#32780;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#38656;&#35201;&#28040;&#32791;&#26356;&#22810;&#30340;&#25968;&#25454;&#26469;&#21512;&#25104;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#25552;&#21462;&#20986;&#30340;&#20998;&#24067;&#26469;&#35843;&#33410;&#25915;&#20987;&#36807;&#31243;&#20316;&#20026;&#24322;&#24120;&#24471;&#20998;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GI-PIP&#21482;&#20351;&#29992;&#20102;ImageNet&#25968;&#25454;&#30340;3.8%&#21363;&#21487;&#23454;&#29616;16.12 dB&#30340;PSNR&#24674;&#22797;&#65292;&#32780;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#21017;&#38656;&#35201;&#36229;&#36807;70%&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;GI-PIP&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep gradient inversion attacks expose a serious threat to Federated Learning (FL) by accurately recovering private data from shared gradients. However, the state-of-the-art heavily relies on impractical assumptions to access excessive auxiliary data, which violates the basic data partitioning principle of FL. In this paper, a novel method, Gradient Inversion Attack using Practical Image Prior (GI-PIP), is proposed under a revised threat model. GI-PIP exploits anomaly detection models to capture the underlying distribution from fewer data, while GAN-based methods consume significant more data to synthesize images. The extracted distribution is then leveraged to regulate the attack process as Anomaly Score loss. Experimental results show that GI-PIP achieves a 16.12 dB PSNR recovery using only 3.8% data of ImageNet, while GAN-based methods necessitate over 70%. Moreover, GI-PIP exhibits superior capability on distribution generalization compared to GAN-based methods. Our approach signif
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.11624</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#31034;&#33539;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23545;&#20110;&#23569;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#36827;&#23637;&#26159;&#26816;&#32034;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#23450;&#21046;&#30340;&#31034;&#33539;&#12290;&#31034;&#33539;&#26816;&#32034;&#30340;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#21644;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;&#37492;&#20110;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#22312;&#26816;&#32034;&#31034;&#33539;&#30340;ICL&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#32508;&#36848;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;&#26816;&#32034;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#65292;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
&lt;/p&gt;</description></item><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25193;&#20805;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#23454;&#29616;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2401.09479</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning. (arXiv:2401.09479v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25193;&#20805;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#23454;&#29616;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#20449;&#20219;&#30340;&#26080;&#21378;&#26080;&#21360;&#36896;&#21046;&#36896;&#26102;&#20195;&#65292;&#30828;&#20214;&#29305;&#27931;&#20234;&#22312;&#33455;&#29255;&#29983;&#20135;&#30340;&#21508;&#20010;&#38454;&#27573;&#34987;&#25554;&#20837;&#30340;&#39118;&#38505;&#22686;&#21152;&#20102;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#26816;&#27979;&#30828;&#20214;&#29305;&#27931;&#20234;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#20851;&#27880;&#28857;&#37117;&#38598;&#20013;&#22312;&#32479;&#35745;&#23398;&#25110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19978;&#65292;&#20294;&#21463;&#21040;&#29305;&#27931;&#20234;&#24863;&#26579;&#22522;&#20934;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#30340;&#24433;&#21709;&#65292;&#26816;&#27979;&#20934;&#30830;&#24615;&#21463;&#38480;&#65292;&#26080;&#27861;&#26816;&#27979;&#21040;&#38646;&#26085;&#29305;&#27931;&#20234;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#25193;&#20805;&#25968;&#25454;&#65292;&#20197;&#20004;&#31181;&#26367;&#20195;&#34920;&#31034;&#27169;&#24577;&#65292;&#22270;&#24418;&#21644;&#34920;&#26684;&#65292;&#30830;&#20445;&#25968;&#25454;&#38598;&#20197;&#20195;&#34920;&#24615;&#30340;&#26041;&#24335;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#30828;&#20214;&#29305;&#27931;&#20234;&#65292;&#24182;&#35780;&#20272;&#20102;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#27599;&#20010;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#29992;&#20110;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#32467;&#26524;&#19981;&#20165;&#30830;&#35748;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19988;&#34920;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The risk of hardware Trojans being inserted at various stages of chip production has increased in a zero-trust fabless era. To counter this, various machine learning solutions have been developed for the detection of hardware Trojans. While most of the focus has been on either a statistical or deep learning approach, the limited number of Trojan-infected benchmarks affects the detection accuracy and restricts the possibility of detecting zero-day Trojans. To close the gap, we first employ generative adversarial networks to amplify our data in two alternative representation modalities, a graph and a tabular, ensuring that the dataset is distributed in a representative manner. Further, we propose a multimodal deep learning approach to detect hardware Trojans and evaluate the results from both early fusion and late fusion strategies. We also estimate the uncertainty quantification metrics of each prediction for risk-aware decision-making. The outcomes not only confirms the efficacy of our
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20316;&#20026;&#23398;&#29983;&#23398;&#20064;&#25512;&#33616;&#30340;&#35299;&#37322;&#24037;&#20855;&#21644;&#25351;&#23548;&#12290;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#26469;&#35843;&#25511;LLM&#30340;&#29983;&#25104;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22312;&#19982;&#23398;&#29983;&#23545;&#35805;&#20013;&#25552;&#20379;&#35299;&#37322;&#21644;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.08517</link><description>&lt;p&gt;
&#25903;&#25345;&#23398;&#29983;&#20915;&#31574;&#30340;&#23398;&#20064;&#25512;&#33616;&#65306;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#23454;&#29616;&#23545;&#35805;&#35299;&#37322;&#21644;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring. (arXiv:2401.08517v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08517
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#24773;&#22659;&#21270;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20316;&#20026;&#23398;&#29983;&#23398;&#20064;&#25512;&#33616;&#30340;&#35299;&#37322;&#24037;&#20855;&#21644;&#25351;&#23548;&#12290;&#36890;&#36807;&#23450;&#20041;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#26469;&#35843;&#25511;LLM&#30340;&#29983;&#25104;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22312;&#19982;&#23398;&#29983;&#23545;&#35805;&#20013;&#25552;&#20379;&#35299;&#37322;&#21644;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#23545;&#23398;&#20064;&#25512;&#33616;&#30340;&#20915;&#31574;&#19982;&#20854;&#29702;&#35299;&#25512;&#33616;&#21407;&#22240;&#30340;&#33021;&#21147;&#26159;&#19981;&#21487;&#20998;&#21106;&#30340;&#65307;&#20182;&#20204;&#33021;&#21542;&#26681;&#25454;&#36825;&#31181;&#29702;&#35299;&#36827;&#34892;&#20462;&#25913;&#12290;&#22312;&#21508;&#31181;&#35299;&#37322;&#24615;&#26041;&#27861;&#20013;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#20855;&#26377;&#19982;&#21516;&#34892;&#25110;&#23548;&#24072;&#35752;&#35770;&#31867;&#20284;&#30340;&#28508;&#21147;&#26469;&#19982;&#23398;&#29983;&#36827;&#34892;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#20197;&#21462;&#20195;&#20154;&#31867;&#23548;&#24072;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#23545;&#35805;&#30340;&#20013;&#20171;&#21644;&#35299;&#37322;&#30340;&#26377;&#38480;&#21644;&#21463;&#25511;&#29983;&#25104;&#30340;&#26469;&#28304;&#65292;&#20197;&#21033;&#29992;LLM&#30340;&#28508;&#21147;&#30340;&#21516;&#26102;&#20943;&#23569;&#20854;&#28508;&#22312;&#39118;&#38505;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25903;&#25345;&#23398;&#29983;&#29702;&#35299;&#23398;&#20064;&#36335;&#24452;&#25512;&#33616;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20316;&#20026;&#20154;&#24037;&#31574;&#21010;&#30340;&#20449;&#24687;&#28304;&#65292;&#36890;&#36807;&#23450;&#20041;&#20854;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#26469;&#35843;&#25511;LLM&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Student commitment towards a learning recommendation is not separable from their understanding of the reasons it was recommended to them; and their ability to modify it based on that understanding. Among explainability approaches, chatbots offer the potential to engage the student in a conversation, similar to a discussion with a peer or a mentor. The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM). Therefore, we propose an approach to utilize chatbots as mediators of the conversation and sources of limited and controlled generation of explanations, to harvest the potential of LLMs while reducing their potential risks at the same time. The proposed LLM-based chatbot supports students in understanding learning-paths recommendations. We use a knowledge graph (KG) as a human-curated source of information, to regulate the LLM's output through defining its prompt's contex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21363;&#26102;&#25193;&#25955;&#32534;&#36753;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#21363;&#26102;&#30340;&#33945;&#29256;&#24341;&#23548;&#12290;&#36890;&#36807;&#37319;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#32454;&#21270;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#20934;&#30830;&#30340;&#33945;&#29256;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25193;&#25955;&#24335;&#22270;&#20687;&#32534;&#36753;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.07709</link><description>&lt;p&gt;
&#39640;&#25928;&#25193;&#25955;&#24335;&#22270;&#20687;&#32534;&#36753;&#19982;&#21363;&#26102;&#20851;&#27880;&#33945;&#29256;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Diffusion-Based Image Editing with Instant Attention Masks. (arXiv:2401.07709v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21363;&#26102;&#25193;&#25955;&#32534;&#36753;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#21363;&#26102;&#30340;&#33945;&#29256;&#24341;&#23548;&#12290;&#36890;&#36807;&#37319;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#32454;&#21270;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#20934;&#30830;&#30340;&#33945;&#29256;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25193;&#25955;&#24335;&#22270;&#20687;&#32534;&#36753;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#24335;&#22270;&#20687;&#32534;&#36753;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#36890;&#24120;&#36890;&#36807;&#24212;&#29992;&#35821;&#20041;&#33945;&#29256;&#26469;&#25511;&#21046;&#32534;&#36753;&#30340;&#30446;&#26631;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22823;&#22810;&#36890;&#36807;&#25163;&#21160;&#25805;&#20316;&#25110;&#31163;&#32447;&#22788;&#29702;&#26469;&#33719;&#21462;&#36825;&#20123;&#33945;&#29256;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#20854;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#31216;&#20026;&#21363;&#26102;&#25193;&#25955;&#32534;&#36753;(InstDiffEdit)&#12290;InstDiffEdit&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#33021;&#21147;&#65292;&#22312;&#25193;&#25955;&#27493;&#39588;&#20013;&#23454;&#29616;&#21363;&#26102;&#33945;&#29256;&#24341;&#23548;&#12290;&#20026;&#20102;&#20943;&#23569;&#27880;&#24847;&#21147;&#26144;&#23556;&#30340;&#22122;&#22768;&#24182;&#23454;&#29616;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20026;InstDiffEdit&#37197;&#22791;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#32454;&#21270;&#26041;&#26696;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#27880;&#24847;&#21147;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#30340;&#33945;&#29256;&#29983;&#25104;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#34917;&#20805;&#29616;&#26377;&#30340;DIE&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Editing-Mask&#30340;&#26032;&#22522;&#20934;&#26469;&#26816;&#39564;&#33945;&#29256;&#20934;&#30830;&#24615;&#21644;&#23616;&#37096;&#32534;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Image Editing (DIE) is an emerging research hot-spot, which often applies a semantic mask to control the target area for diffusion-based editing. However, most existing solutions obtain these masks via manual operations or off-line processing, greatly reducing their efficiency. In this paper, we propose a novel and efficient image editing method for Text-to-Image (T2I) diffusion models, termed Instant Diffusion Editing(InstDiffEdit). In particular, InstDiffEdit aims to employ the cross-modal attention ability of existing diffusion models to achieve instant mask guidance during the diffusion steps. To reduce the noise of attention maps and realize the full automatics, we equip InstDiffEdit with a training-free refinement scheme to adaptively aggregate the attention distributions for the automatic yet accurate mask generation. Meanwhile, to supplement the existing evaluations of DIE, we propose a new benchmark called Editing-Mask to examine the mask accuracy and local edi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;APLe&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#20196;&#29260;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#35843;&#33410;CLIP&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24335;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06827</link><description>&lt;p&gt;
APLe: &#22810;&#27169;&#24335;&#25552;&#31034;&#23398;&#20064;&#30340;&#36880;&#20196;&#29260;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning. (arXiv:2401.06827v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;APLe&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#20196;&#29260;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#35843;&#33410;CLIP&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24335;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20026;&#21516;&#31867;&#27169;&#22411;&#26641;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#29616;&#26377;&#30740;&#31350;&#20013;&#24050;&#32463;&#25506;&#32034;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#29305;&#24449;&#65292;&#21253;&#25324;&#23545;&#25991;&#26412;&#36755;&#20837;&#30340;&#25935;&#24863;&#24615;&#21644;&#36328;&#22810;&#27169;&#24335;&#25552;&#31034;&#30340;&#35843;&#33410;&#36807;&#31243;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20013;&#65292;&#20511;&#37492;&#20102;&#22270;&#20687;&#34701;&#21512;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#36880;&#23618;&#35757;&#32451;&#24605;&#24819;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#24182;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#21462;&#20195;&#20102;&#25163;&#24037;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#24615;&#33021;&#12290;&#22312;&#35299;&#20915;&#22810;&#27169;&#24335;&#25552;&#31034;&#25361;&#25112;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36880;&#20196;&#29260;&#33258;&#36866;&#24212;&#26041;&#27861;(APLe)&#65292;&#20197;&#39034;&#24207;&#26041;&#24335;&#35843;&#33410;CLIP&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#20004;&#31181;&#27169;&#24335;&#25552;&#31034;&#12290;APLe&#33021;&#22815;&#39640;&#25928;&#22320;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language (V-L) models set the benchmark for generalization to downstream tasks among the noteworthy contenders. Many characteristics of the V-L model have been explored in existing research including the challenge of the sensitivity to text input and the tuning process across multi-modal prompts. With the advanced utilization of the V-L model like CLIP, recent approaches deploy learnable prompts instead of hand-craft prompts to boost the generalization performance and address the aforementioned challenges. Inspired by layer-wise training, which is wildly used in image fusion, we note that using a sequential training process to adapt different modalities branches of CLIP efficiently facilitates the improvement of generalization. In the context of addressing the multi-modal prompting challenge, we propose Token-wise Adaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities prompts, vision and language, as tokens in a sequential manner. APLe addresses t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#22312;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38388;&#25509;&#20294;&#23458;&#35266;&#35780;&#20272;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#29992;&#25143;&#34892;&#20026;&#25351;&#26631;&#23545;&#35780;&#20272;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.04867</link><description>&lt;p&gt;
&#23545;&#29992;&#25143;&#34892;&#20026;&#36827;&#34892;&#20998;&#26512;&#20197;&#23458;&#35266;&#35780;&#20272;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Analysis of User Behaviours for Objectively Evaluating Spoken Dialogue Systems. (arXiv:2401.04867v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#22312;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38388;&#25509;&#20294;&#23458;&#35266;&#35780;&#20272;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#29992;&#25143;&#34892;&#20026;&#25351;&#26631;&#23545;&#35780;&#20272;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#26696;&#24456;&#37325;&#35201;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#20027;&#35266;&#35780;&#20272;&#22312;&#29992;&#25143;&#23454;&#39564;&#20013;&#24120;&#29992;&#65292;&#20294;&#23458;&#35266;&#35780;&#20272;&#23545;&#20110;&#30740;&#31350;&#27604;&#36739;&#21644;&#21487;&#22797;&#21046;&#24615;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#38388;&#25509;&#20294;&#23458;&#35266;&#22320;&#35780;&#20272;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#31038;&#20132;&#23545;&#35805;&#20219;&#21153;&#20013;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#20998;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#19987;&#27880;&#20542;&#21548;&#12289;&#38754;&#35797;&#21644;&#39318;&#27425;&#20250;&#35758;&#23545;&#35805;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29992;&#25143;&#35805;&#35821;&#26159;&#20027;&#35201;&#22240;&#32032;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#22914;&#19987;&#27880;&#20542;&#21548;&#21644;&#38754;&#35797;&#65292;&#35805;&#35821;&#25968;&#37327;&#21644;&#21333;&#35789;&#25968;&#37327;&#31561;&#25351;&#26631;&#22312;&#35780;&#20272;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#35266;&#23519;&#35821;&#35843;&#19981;&#27969;&#30021;&#31561;&#20063;&#21487;&#20197;&#25351;&#31034;&#27491;&#24335;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#38754;&#35797;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#39640;&#20114;&#21160;&#24615;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#22914;&#39318;&#27425;&#20250;&#35758;&#23545;&#35805;&#65292;&#29992;&#25143;&#24773;&#32490;&#21644;&#21442;&#19982;&#31243;&#24230;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Establishing evaluation schemes for spoken dialogue systems is important, but it can also be challenging. While subjective evaluations are commonly used in user experiments, objective evaluations are necessary for research comparison and reproducibility. To address this issue, we propose a framework for indirectly but objectively evaluating systems based on users' behaviours. In this paper, to this end, we investigate the relationship between user behaviours and subjective evaluation scores in social dialogue tasks: attentive listening, job interview, and first-meeting conversation. The results reveal that in dialogue tasks where user utterances are primary, such as attentive listening and job interview, indicators like the number of utterances and words play a significant role in evaluation. Observing disfluency also can indicate the effectiveness of formal tasks, such as job interview. On the other hand, in dialogue tasks with high interactivity, such as first-meeting conversation, b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02994</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#26041;&#27861;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65306;&#30456;&#23545;&#20110;&#19975;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#30340;&#26356;&#24265;&#20215;&#12289;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20250;&#35805;&#22411;AI&#30740;&#31350;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#26356;&#22810;&#30340;&#21442;&#25968;&#65292;&#22914;ChatGPT&#31561;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#24222;&#22823;&#30340;&#27169;&#22411;&#24448;&#24448;&#33021;&#29983;&#25104;&#26356;&#22909;&#30340;&#32842;&#22825;&#22238;&#22797;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#33021;&#21542;&#36890;&#36807;&#32452;&#21512;&#36739;&#23567;&#30340;&#27169;&#22411;&#26469;&#36798;&#21040;&#19982;&#21333;&#20010;&#22823;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23558;&#22810;&#20010;&#32842;&#22825;AI&#38598;&#25104;&#22312;&#19968;&#36215;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#24403;&#29305;&#23450;&#36739;&#23567;&#30340;&#27169;&#22411;&#21327;&#21516;&#28151;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#28508;&#22312;&#22320;&#36229;&#36234;&#25110;&#21305;&#25932;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#20165;&#38598;&#25104;&#19977;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;6B/13B&#21442;&#25968;&#65289;&#23601;&#21487;&#20197;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36234;ChatGPT&#65288;175B+&#21442;&#25968;&#65289;&#31561;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#36825;&#20010;&#20551;&#35774;&#32463;&#36807;&#20102;&#20005;&#26684;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? We introduce an approach termed "blending", a straightforward yet effective method of integrating multiple chat AIs. Our empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tes
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AIGCBench&#65292;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#20869;&#23481;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#21270;&#19988;&#24320;&#25918;&#39046;&#22495;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;AIGCBench&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24314;&#31435;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;11&#20010;&#24230;&#37327;&#25351;&#26631;&#65292;&#28085;&#30422;&#25511;&#21046;&#35270;&#39057;&#23545;&#40784;&#12289;&#21160;&#24577;&#25928;&#26524;&#12289;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#35270;&#39057;&#36136;&#37327;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2401.01651</link><description>&lt;p&gt;
AIGCBench&#65306;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#20869;&#23481;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI. (arXiv:2401.01651v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AIGCBench&#65292;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#20869;&#23481;&#30340;&#22522;&#20934;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#21270;&#19988;&#24320;&#25918;&#39046;&#22495;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;AIGCBench&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24314;&#31435;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;11&#20010;&#24230;&#37327;&#25351;&#26631;&#65292;&#28085;&#30422;&#25511;&#21046;&#35270;&#39057;&#23545;&#40784;&#12289;&#21160;&#24577;&#25928;&#26524;&#12289;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#35270;&#39057;&#36136;&#37327;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#35270;&#39057;&#29983;&#25104;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AIGCBench&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#32508;&#21512;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#21508;&#31181;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#65292;&#20027;&#35201;&#20851;&#27880;&#22270;&#20687;&#21040;&#35270;&#39057;&#65288;I2V&#65289;&#29983;&#25104;&#12290;AIGCBench&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#22522;&#20934;&#32570;&#20047;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21253;&#25324;&#19968;&#20010;&#22810;&#26679;&#21270;&#19988;&#24320;&#25918;&#39046;&#22495;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#22312;&#30456;&#31561;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#32452;&#21512;&#22120;&#21644;GPT-4&#26469;&#21019;&#24314;&#20016;&#23500;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;11&#20010;&#24230;&#37327;&#25351;&#26631;&#65292;&#28085;&#30422;&#22235;&#20010;&#32500;&#24230;&#65292;&#20197;&#35780;&#20272;&#31639;&#27861;&#24615;&#33021;&#12290;&#36825;&#20123;&#32500;&#24230;&#26159;&#25511;&#21046;&#35270;&#39057;&#23545;&#40784;&#65292;&#21160;&#24577;&#25928;&#26524;&#65292;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#35270;&#39057;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning field of Artificial Intelligence Generated Content (AIGC) is witnessing rapid advancements, particularly in video generation. This paper introduces AIGCBench, a pioneering comprehensive and scalable benchmark designed to evaluate a variety of video generation tasks, with a primary focus on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of existing benchmarks, which suffer from a lack of diverse datasets, by including a varied and open-domain image-text dataset that evaluates different state-of-the-art algorithms under equivalent conditions. We employ a novel text combiner and GPT-4 to create rich text prompts, which are then used to generate images via advanced Text-to-Image models. To establish a unified evaluation framework for video generation tasks, our benchmark includes 11 metrics spanning four dimensions to assess algorithm performance. These dimensions are control-video alignment, motion effects, temporal consistency, and video quality. These 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#35299;&#27431;&#30431;&#22312;&#32447;&#24179;&#21488;&#36719;&#20214;&#25991;&#26723;&#20013;&#25490;&#21517;&#36879;&#26126;&#24230;&#30340;&#21512;&#35268;&#24615;&#24773;&#20917;&#65292;&#24182;&#24341;&#20837;&#24182;&#27979;&#35797;&#20102;&#22522;&#20110;ChatGPT&#21644;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#33258;&#21160;&#21512;&#35268;&#24615;&#35780;&#20272;&#24037;&#20855;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#20316;&#20026;&#21512;&#35268;&#24615;&#35780;&#20272;&#30340;&#21487;&#38752;&#20195;&#29702;&#30340;&#21069;&#26223;&#24456;&#26377;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2312.14794</link><description>&lt;p&gt;
&#23545;&#27431;&#30431;&#22312;&#32447;&#24179;&#21488;&#36719;&#20214;&#25991;&#26723;&#20013;&#25490;&#21517;&#36879;&#26126;&#24230;&#21512;&#35268;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Compliance with Ranking Transparency in the Software Documentation of EU Online Platforms. (arXiv:2312.14794v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14794
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#35299;&#27431;&#30431;&#22312;&#32447;&#24179;&#21488;&#36719;&#20214;&#25991;&#26723;&#20013;&#25490;&#21517;&#36879;&#26126;&#24230;&#30340;&#21512;&#35268;&#24615;&#24773;&#20917;&#65292;&#24182;&#24341;&#20837;&#24182;&#27979;&#35797;&#20102;&#22522;&#20110;ChatGPT&#21644;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#33258;&#21160;&#21512;&#35268;&#24615;&#35780;&#20272;&#24037;&#20855;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#20316;&#20026;&#21512;&#35268;&#24615;&#35780;&#20272;&#30340;&#21487;&#38752;&#20195;&#29702;&#30340;&#21069;&#26223;&#24456;&#26377;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;&#24179;&#21488;&#23545;&#20225;&#19994;&#65288;P2B&#65289;&#35268;&#20363;&#30340;&#36981;&#23432;&#23545;&#22312;&#32447;&#24179;&#21488;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#20844;&#20849;&#26426;&#26500;&#35780;&#20272;&#20854;&#21512;&#35268;&#24615;&#20063;&#21487;&#33021;&#23384;&#22312;&#22256;&#38590;&#12290;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#35780;&#20272;&#24179;&#21488;&#25552;&#20379;&#30340;&#20851;&#20110;&#25490;&#21517;&#36879;&#26126;&#24230;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#36719;&#20214;&#25991;&#26723;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20197;&#20004;&#31181;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#20845;&#20010;&#20027;&#35201;&#24179;&#21488;&#65288;Amazon&#65292;Bing&#65292;Booking&#65292;Google&#65292;Tripadvisor&#21644;Yahoo&#65289;&#30340;&#21512;&#35268;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#25991;&#26723;&#20013;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#27979;&#35797;&#20102;&#22522;&#20110;ChatGPT&#21644;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30340;&#33258;&#21160;&#21512;&#35268;&#24615;&#35780;&#20272;&#24037;&#20855;&#12290;&#36825;&#20123;&#24037;&#20855;&#19982;&#20154;&#24037;&#21028;&#26029;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#20316;&#20026;&#21512;&#35268;&#24615;&#35780;&#20272;&#30340;&#21487;&#38752;&#20195;&#29702;&#30340;&#21069;&#26223;&#24456;&#26377;&#24076;&#26395;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#21487;&#20197;&#24110;&#21161;&#25552;&#21319;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#24182;&#19982;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;10.3&#20445;&#25345;&#19968;&#33268;&#65292;&#35813;&#30446;&#26631;&#26088;&#22312;&#20943;&#23569;&#21253;&#25324;&#21830;&#19994;&#24046;&#36317;&#22312;&#20869;&#30340;&#19981;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compliance with the European Union's Platform-to-Business (P2B) Regulation is challenging for online platforms, and assessing their compliance can be difficult for public authorities. This is partly due to the lack of automated tools for assessing the information (e.g., software documentation) platforms provide concerning ranking transparency. Our study tackles this issue in two ways. First, we empirically evaluate the compliance of six major platforms (Amazon, Bing, Booking, Google, Tripadvisor, and Yahoo), revealing substantial differences in their documentation. Second, we introduce and test automated compliance assessment tools based on ChatGPT and information retrieval technology. These tools are evaluated against human judgments, showing promising results as reliable proxies for compliance assessments. Our findings could help enhance regulatory compliance and align with the United Nations Sustainable Development Goal 10.3, which seeks to reduce inequality, including business disp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#24494;&#35843;&#29616;&#24577;&#36319;&#36394;&#22120;&#65292;&#21487;&#20197;&#25552;&#39640;&#36861;&#36394;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.12433</link><description>&lt;p&gt;
&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tracking Any Object Amodally. (arXiv:2312.12433v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#24494;&#35843;&#29616;&#24577;&#36319;&#36394;&#22120;&#65292;&#21487;&#20197;&#25552;&#39640;&#36861;&#36394;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#29616;&#24577;&#24863;&#30693;&#26159;&#19968;&#31181;&#20174;&#37096;&#20998;&#21487;&#35265;&#24615;&#20013;&#29702;&#35299;&#23436;&#25972;&#29289;&#20307;&#32467;&#26500;&#30340;&#22522;&#26412;&#25216;&#33021;&#65292;&#23427;&#23545;&#20110;&#23156;&#20799;&#29978;&#33267;&#26159;&#25104;&#20154;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#23427;&#30340;&#37325;&#35201;&#24615;&#24310;&#20280;&#21040;&#20102;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#39046;&#22495;&#65292;&#23545;&#20110;&#29702;&#35299;&#37325;&#21472;&#29289;&#20307;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#31639;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#19968;&#20851;&#38190;&#33021;&#21147;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#20351;&#29992;&#30340;&#26159;&#29616;&#24577;&#26631;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#29616;&#24577;&#25968;&#25454;&#30340;&#21294;&#20047;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TAO-Amodal&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;880&#20010;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#21487;&#35265;&#21644;&#36974;&#25377;&#23545;&#35937;&#30340;&#38750;&#29616;&#24577;&#21644;&#29616;&#24577;&#36793;&#30028;&#26694;&#65292;&#21253;&#25324;&#37096;&#20998;&#36229;&#20986;&#30011;&#38754;&#33539;&#22260;&#30340;&#29289;&#20307;&#12290;&#20026;&#20102;&#22686;&#24378;&#38750;&#29616;&#24577;&#36861;&#36394;&#30340;&#30446;&#26631;&#27704;&#20037;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#21363;&#38750;&#29616;&#24577;&#25193;&#23637;&#22120;&#65292;&#36890;&#36807;&#23545;&#20960;&#30334;&#20010;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#24494;&#35843;&#65292;&#23558;&#26631;&#20934;&#30340;&#29616;&#24577;&#36319;&#36394;&#22120;&#36716;&#21270;&#20026;&#38750;&#29616;&#24577;&#36319;&#36394;&#22120;&#12290;&#25105;&#20204;&#21462;&#24471;&#20102;3.3&#65285;&#21644;1.6&#65285;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amodal perception, the ability to comprehend complete object structures from partial visibility, is a fundamental skill, even for infants. Its significance extends to applications like autonomous driving, where a clear understanding of heavily occluded objects is essential. However, modern detection and tracking algorithms often overlook this critical capability, perhaps due to the prevalence of modal annotations in most datasets. To address the scarcity of amodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse categories in thousands of video sequences. Our dataset includes amodal and modal bounding boxes for visible and occluded objects, including objects that are partially out-of-frame. To enhance amodal tracking with object permanence, we leverage a lightweight plug-in module, the amodal expander, to transform standard, modal trackers into amodal ones through fine-tuning on a few hundred video sequences with data augmentation. We achieve a 3.3\% and 1.6\% improve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.02246</link><description>&lt;p&gt;
&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditional Variational Diffusion Models. (arXiv:2312.02246v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#26088;&#22312;&#20174;&#35266;&#27979;&#20013;&#30830;&#23450;&#21442;&#25968;&#65292;&#36825;&#26159;&#24037;&#31243;&#21644;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#65292;&#22240;&#20854;&#33021;&#22815;&#20135;&#29983;&#36924;&#30495;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#33391;&#22909;&#30340;&#25968;&#23398;&#29305;&#24615;&#32780;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#32570;&#28857;&#26159;&#23545;&#26041;&#24046;&#35843;&#24230;&#30340;&#36873;&#25321;&#25935;&#24863;&#65292;&#35813;&#35843;&#24230;&#25511;&#21046;&#30528;&#25193;&#25955;&#36807;&#31243;&#30340;&#21160;&#24577;&#12290;&#20026;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#24494;&#35843;&#36825;&#20010;&#35843;&#24230;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#26102;&#38388;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#26368;&#20248;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#23545;&#25968;&#25454;&#30340;&#27010;&#29575;&#26465;&#20214;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#24320;&#38144;&#19979;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#30456;&#20851;&#30340;&#36870;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#21644;&#23450;&#37327;&#30456;&#20301;&#25104;&#20687;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#36739;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. Fine-tuning this schedule for specific applications is crucial but time-costly and does not guarantee an optimal result. We propose a novel approach for learning the schedule as part of the training process. Our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. This approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;</title><link>http://arxiv.org/abs/2312.01185</link><description>&lt;p&gt;
&#26102;&#38388;&#20013;&#30340;&#28063;&#28458;&#65306;&#32654;&#22269;&#21382;&#21490;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22269;&#24773;&#21672;&#25991;&#25968;&#25454;&#38598;&#23545;&#32654;&#22269;&#21382;&#21490;&#30340;&#24635;&#20307;&#26102;&#38388;&#32447;&#21450;&#21672;&#25991;&#26412;&#36523;&#30340;&#29305;&#28857;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#65288;&#20063;&#26377;&#20123;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65289;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#65292;&#22914;BERT&#65288;DistilBERT&#65289;&#21644;GPT-2&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#26368;&#36866;&#21512;NLP&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;GPT-2&#32467;&#21512;UMAP&#31561;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#26356;&#24378;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#36825;&#20351;&#24471;GPT-2 + UMAP&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#23601;&#36275;&#22815;&#22909;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#26469;&#26816;&#27979;&#21738;&#20301;&#24635;&#32479;&#21457;&#34920;&#20102;&#21738;&#31687;&#28436;&#35762;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#65288;&#20934;&#30830;&#29575;&#20026;93\% - 95\%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#36816;&#34892;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#20889;&#20316;&#24180;&#20221;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
&lt;/p&gt;</description></item><item><title>GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2311.16716</link><description>&lt;p&gt;
GraphPro: &#38754;&#21521;&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraphPro: Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16716
&lt;/p&gt;
&lt;p&gt;
GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#22810;&#27425;&#28040;&#24687;&#20256;&#36882;&#22312;&#24314;&#27169;&#22797;&#26434;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21464;&#21270;&#21644;&#26032;&#21040;&#36798;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphPro&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;GraphPro&#26694;&#26550;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#20020;&#26102;&#25552;&#31034;&#26426;&#21046;&#21644;&#22270;&#32467;&#26500;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#21040;&#39044;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#20013;&#26469;&#35299;&#20915;&#29992;&#25143;&#20559;&#22909;&#19981;&#26029;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNN-based recommenders have excelled in modeling intricate user-item interactions through multi-hop message passing. However, existing methods often overlook the dynamic nature of evolving user-item interactions, which impedes the adaption to changing user preferences and distribution shifts in newly arriving data. Thus, their scalability and performances in real-world dynamic environments are limited. In this study, we propose GraphPro, a framework that incorporates parameter-efficient and dynamic graph pre-training with prompt learning. This novel combination empowers GNNs to effectively capture both long-term user preferences and short-term behavior dynamics, enabling the delivery of accurate and timely recommendations. Our GraphPro framework addresses the challenge of evolving user preferences by seamlessly integrating a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN model. The temporal prompt mechanism encodes time information o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;LLaMAC&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#26469;&#35299;&#20915;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#24187;&#35273;&#21644;&#21327;&#35843;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20196;&#29260;&#26469;&#20419;&#36827;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#31995;&#32479;&#36164;&#28304;&#20998;&#37197;&#21644;&#26426;&#22120;&#20154;&#32593;&#26684;&#36816;&#36755;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2311.13884</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20915;&#31574;&#20013;&#25511;&#21046;&#27169;&#22411;&#20195;&#29702;&#65306;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach. (arXiv:2311.13884v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;LLaMAC&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#26469;&#35299;&#20915;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#24187;&#35273;&#21644;&#21327;&#35843;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20196;&#29260;&#26469;&#20419;&#36827;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#31995;&#32479;&#36164;&#28304;&#20998;&#37197;&#21644;&#26426;&#22120;&#20154;&#32593;&#26684;&#36816;&#36755;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#33879;&#36827;&#23637;&#20026;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#35268;&#21010;&#21644;&#20915;&#31574;&#38382;&#39064;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;LLM&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#21327;&#35843;&#38382;&#39064;&#21464;&#24471;&#26085;&#30410;&#31361;&#20986;&#12290;&#27492;&#22806;&#65292;&#22312;&#21033;&#29992;LLM&#20419;&#36827;&#22823;&#37327;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#26102;&#65292;&#26377;&#25928;&#21033;&#29992;&#20196;&#29260;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LLaMAC&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#26469;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;LLaMAC&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#33041;&#20013;&#30340;&#20215;&#20540;&#20998;&#24067;&#32534;&#30721;&#65292;&#21033;&#29992;&#20869;&#37096;&#21644;&#22806;&#37096;&#21453;&#39304;&#26426;&#21046;&#20419;&#36827;&#20854;&#27169;&#22359;&#20043;&#38388;&#30340;&#21327;&#20316;&#21644;&#36845;&#20195;&#25512;&#29702;&#12290;&#36890;&#36807;&#28041;&#21450;&#31995;&#32479;&#36164;&#28304;&#20998;&#37197;&#21644;&#26426;&#22120;&#20154;&#32593;&#26684;&#36816;&#36755;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#24040;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable progress in Large Language Models (LLMs) opens up new avenues for addressing planning and decision-making problems in Multi-Agent Systems (MAS). However, as the number of agents increases, the issues of hallucination in LLMs and coordination in MAS have become increasingly prominent. Additionally, the efficient utilization of tokens emerges as a critical consideration when employing LLMs to facilitate the interactions among a substantial number of agents. In this paper, we develop a modular framework called LLaMAC to mitigate these challenges. LLaMAC implements a value distribution encoding similar to that found in the human brain, utilizing internal and external feedback mechanisms to facilitate collaboration and iterative reasoning among its modules. Through evaluations involving system resource allocation and robot grid transportation, we demonstrate the considerable advantages afforded by our proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#20844;&#24335;&#30340;&#20248;&#20808;&#39034;&#24207;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27604;&#36739;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#36866;&#21512;&#30340;&#26435;&#37325;&#20272;&#35745;&#65292;&#20351;&#24471;&#39318;&#36873;&#20449;&#21495;&#30340;&#21152;&#26435;&#28385;&#36275;&#24230;&#39640;&#20110;&#38750;&#39318;&#36873;&#20449;&#21495;&#12290;&#22312;&#20154;&#20307;&#35797;&#39564;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02099</link><description>&lt;p&gt;
&#19968;&#31181;&#23433;&#20840;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#21450;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Safe Preference Learning Approach for Personalization with Applications to Autonomous Vehicles. (arXiv:2311.02099v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#20844;&#24335;&#30340;&#20248;&#20808;&#39034;&#24207;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27604;&#36739;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#36866;&#21512;&#30340;&#26435;&#37325;&#20272;&#35745;&#65292;&#20351;&#24471;&#39318;&#36873;&#20449;&#21495;&#30340;&#21152;&#26435;&#28385;&#36275;&#24230;&#39640;&#20110;&#38750;&#39318;&#36873;&#20449;&#21495;&#12290;&#22312;&#20154;&#20307;&#35797;&#39564;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#65292;&#30830;&#20445;&#31526;&#21512;&#32473;&#23450;&#35268;&#33539;&#65292;&#24182;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25551;&#36848;&#20132;&#36890;&#35268;&#21017;&#30340;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;(STL)&#20844;&#24335;&#30340;&#20248;&#20808;&#39034;&#24207;&#32435;&#20837;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#21152;&#26435;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;(PWSTL)&#65292;&#25105;&#20204;&#22522;&#20110;&#25104;&#23545;&#27604;&#36739;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30830;&#20445;&#20102;&#23433;&#20840;&#20445;&#35777;&#30340;&#20559;&#22909;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#32473;&#23450;PWSTL&#20844;&#24335;&#26435;&#37325;&#30340;&#21487;&#34892;&#20272;&#35745;&#65292;&#20351;&#24471;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#26102;&#65292;&#39318;&#36873;&#20449;&#21495;&#30340;&#21152;&#26435;&#23450;&#37327;&#28385;&#36275;&#24230;&#22823;&#20110;&#38750;&#39318;&#36873;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#30340;&#26435;&#37325;&#30340;&#21487;&#34892;&#20272;&#35745;&#23548;&#33268;&#20102;&#19968;&#20010;&#21152;&#26435;STL&#20844;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#27491;&#30830;&#24615;&#21644;&#23450;&#21046;&#21512;&#25104;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#25311;&#39550;&#39542;&#22330;&#26223;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#20307;&#35797;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a preference learning method that ensures adherence to given specifications, with an application to autonomous vehicles. Our approach incorporates the priority ordering of Signal Temporal Logic (STL) formulas describing traffic rules into a learning framework. By leveraging Parametric Weighted Signal Temporal Logic (PWSTL), we formulate the problem of safety-guaranteed preference learning based on pairwise comparisons and propose an approach to solve this learning problem. Our approach finds a feasible valuation for the weights of the given PWSTL formula such that, with these weights, preferred signals have weighted quantitative satisfaction measures greater than their non-preferred counterparts. The feasible valuation of weights given by our approach leads to a weighted STL formula that can be used in correct-and-custom-by-construction controller synthesis. We demonstrate the performance of our method with a pilot human subject study in two different simulated dri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.18304</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#31283;&#24577;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;&#24615;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Stability Principle for Learning under Non-Stationarity. (arXiv:2310.18304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#26469;&#36873;&#25321;&#19968;&#20010;&#22238;&#28335;&#31383;&#21475;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#23558;&#32047;&#31215;&#20559;&#24046;&#20445;&#25345;&#22312;&#19982;&#38543;&#26426;&#35823;&#24046;&#30456;&#23545;&#21487;&#25509;&#21463;&#30340;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#26410;&#30693;&#38750;&#31283;&#23450;&#24615;&#30340;&#36866;&#24212;&#24615;&#12290;&#24403;&#20154;&#21475;&#25439;&#22833;&#20989;&#25968;&#24378;&#20984;&#25110;&#20165;&#28385;&#36275;Lipschitz&#26465;&#20214;&#26102;&#65292;&#36951;&#25022;&#30028;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#65292;&#20165;&#21463;&#23545;&#25968;&#22240;&#23376;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26680;&#24515;&#26159;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#20989;&#25968;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#23558;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#20026;&#20934;&#31283;&#24577;&#29255;&#27573;&#30340;&#20998;&#21106;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.
&lt;/p&gt;</description></item><item><title>&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.17715</link><description>&lt;p&gt;
&#24322;&#24120;&#32500;&#24230;&#32534;&#30721;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Outlier Dimensions Encode Task-Specific Knowledge. (arXiv:2310.17715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17715
&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34920;&#31034;&#34987;&#23569;&#25968;&#20960;&#20010;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#30340;&#24322;&#24120;&#32500;&#24230;&#25152;&#20027;&#23548;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#65292;&#34429;&#28982;&#21435;&#38500;LLM&#34920;&#31034;&#20013;&#30340;&#24322;&#24120;&#32500;&#24230;&#20250;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#24322;&#24120;&#32500;&#24230;&#23545;&#23884;&#20837;&#34920;&#31034;&#30340;&#36136;&#37327;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#23545;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#19979;&#32467;&#26524;&#65306;1&#65289;&#22312;&#39044;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#24322;&#24120;&#32500;&#24230;&#22312;&#24494;&#35843;&#27169;&#22411;&#20013;&#20173;&#28982;&#23384;&#22312;&#65292;2&#65289;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#34920;&#31034;&#22312;&#21333;&#20010;&#24322;&#24120;&#32500;&#24230;&#19978;&#30340;&#20540;&#20250;&#24433;&#21709;&#19979;&#28216;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in fine-tuned models and 2) a single outlier dimension can complete downstream tasks with a minimal error rate. Our results suggest that outlier dimensions can encode crucial task-specific knowledge and that the value of a representation in a single outlier dimension drives downstream model decisions.
&lt;/p&gt;</description></item><item><title>HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15318</link><description>&lt;p&gt;
HetGPT: &#21033;&#29992;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25552;&#31034;&#35843;&#25972;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15318
&lt;/p&gt;
&lt;p&gt;
HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#29616;&#20026;&#34920;&#31034;&#21644;&#20998;&#26512;Web&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20016;&#23500;&#20449;&#24687;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;&#20351;&#24471;&#22312;&#32447;&#39029;&#38754;&#20998;&#31867;&#21644;&#31038;&#20132;&#25512;&#33616;&#31561;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#8220;&#39044;&#35757;&#32451;&#65292;&#24494;&#35843;&#8221;&#33539;&#24335;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#24448;&#24448;&#23384;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36825;&#31181;&#24046;&#36317;&#21487;&#33021;&#23548;&#33268;&#8220;&#36127;&#36716;&#31227;&#8221;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#20852;&#36215;&#34920;&#26126;&#20102;&#23558;&#8220;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#8221;&#33539;&#24335;&#24212;&#29992;&#20110;&#22270;&#24418;&#30340;&#28508;&#21147;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24418;&#25552;&#31034;&#25216;&#26415;&#38024;&#23545;&#30340;&#26159;&#21516;&#36136;&#22270;&#65292;&#24573;&#35270;&#20102;Web&#22270;&#30340;&#20869;&#22312;&#24322;&#26500;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HetGPT&#65292;
&lt;/p&gt;
&lt;p&gt;
Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22235;&#31181;&#25945;&#23398;&#25351;&#23548;&#31574;&#30053;&#23545;&#23398;&#20064;&#32773;&#22312;&#20351;&#29992;LLM&#26102;&#30340;&#34920;&#29616;&#21644;&#24863;&#30693;&#25928;&#26524;&#65292;&#21457;&#29616;&#30452;&#25509;LLM&#31572;&#26696;&#25552;&#39640;&#20102;&#34920;&#29616;&#65292;&#32780;&#25913;&#36827;&#23398;&#29983;&#35299;&#20915;&#26041;&#26696;&#21017;&#22686;&#21152;&#20102;&#23545;LLM&#30340;&#20449;&#20219;&#24230;&#12290;&#21516;&#26102;&#65292;&#32467;&#26500;&#21270;&#25351;&#23548;&#20063;&#20943;&#23569;&#20102;&#38543;&#26426;&#26597;&#35810;&#21644;&#23398;&#29983;&#22797;&#21046;&#31896;&#36148;&#38382;&#39064;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.13712</link><description>&lt;p&gt;
&#23398;&#20064;&#32773;&#20351;&#29992;LLM&#26102;&#25351;&#23548;&#21644;&#20132;&#20114;&#31574;&#30053;&#23545;&#23398;&#20064;&#32773;&#30340;&#34920;&#29616;&#21644;&#24863;&#30693;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Guidance and Interaction Strategies for LLM Use on Learner Performance and Perception. (arXiv:2310.13712v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22235;&#31181;&#25945;&#23398;&#25351;&#23548;&#31574;&#30053;&#23545;&#23398;&#20064;&#32773;&#22312;&#20351;&#29992;LLM&#26102;&#30340;&#34920;&#29616;&#21644;&#24863;&#30693;&#25928;&#26524;&#65292;&#21457;&#29616;&#30452;&#25509;LLM&#31572;&#26696;&#25552;&#39640;&#20102;&#34920;&#29616;&#65292;&#32780;&#25913;&#36827;&#23398;&#29983;&#35299;&#20915;&#26041;&#26696;&#21017;&#22686;&#21152;&#20102;&#23545;LLM&#30340;&#20449;&#20219;&#24230;&#12290;&#21516;&#26102;&#65292;&#32467;&#26500;&#21270;&#25351;&#23548;&#20063;&#20943;&#23569;&#20102;&#38543;&#26426;&#26597;&#35810;&#21644;&#23398;&#29983;&#22797;&#21046;&#31896;&#36148;&#38382;&#39064;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#25945;&#23460;&#35268;&#27169;&#21644;&#25945;&#24072;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25945;&#23398;&#21161;&#25163;&#21487;&#20197;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#25506;&#32034;&#23427;&#20204;&#22312;&#25945;&#32946;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#19981;&#20165;&#22312;&#20110;&#30830;&#23450;LLMs&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19988;&#22312;&#20110;&#35782;&#21035;&#23398;&#20064;&#32773;&#19982;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#20114;&#21160;&#32454;&#24494;&#24046;&#21035;&#65292;&#36825;&#20250;&#24433;&#21709;&#23398;&#20064;&#32773;&#30340;&#21442;&#19982;&#21644;&#25104;&#26524;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#22530;&#65288;N=145&#65289;&#21644;Prolific&#19978;&#36827;&#34892;&#20102;&#19968;&#39033;&#24418;&#25104;&#24615;&#30740;&#31350;&#65288;N=356&#65289;&#65292;&#20197;&#25506;&#32034;&#22235;&#31181;&#25945;&#23398;&#25351;&#23548;&#31574;&#30053;&#23545;&#23398;&#20064;&#32773;&#22312;LLMs&#19978;&#30340;&#34920;&#29616;&#12289;&#33258;&#20449;&#24515;&#21644;&#20449;&#20219;&#24230;&#30340;&#24433;&#21709;&#12290;&#30452;&#25509;&#30340;LLM&#31572;&#26696;&#31245;&#24494;&#25552;&#39640;&#20102;&#34920;&#29616;&#65292;&#32780;&#25913;&#36827;&#23398;&#29983;&#30340;&#35299;&#20915;&#26041;&#26696;&#22686;&#21152;&#20102;&#20449;&#20219;&#24230;&#12290;&#32467;&#26500;&#21270;&#25351;&#23548;&#20943;&#23569;&#20102;&#38543;&#26426;&#26597;&#35810;&#21644;&#23398;&#29983;&#23558;&#20316;&#19994;&#38382;&#39064;&#22797;&#21046;&#31896;&#36148;&#32473;LLM&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20984;&#26174;&#20102;t
&lt;/p&gt;
&lt;p&gt;
Personalized chatbot-based teaching assistants can be crucial in addressing increasing classroom sizes, especially where direct teacher presence is limited. Large language models (LLMs) offer a promising avenue, with increasing research exploring their educational utility. However, the challenge lies not only in establishing the efficacy of LLMs but also in discerning the nuances of interaction between learners and these models, which impact learners' engagement and results. We conducted a formative study in an undergraduate computer science classroom (N=145) and a controlled experiment on Prolific (N=356) to explore the impact of four pedagogically informed guidance strategies on the learners' performance, confidence and trust in LLMs. Direct LLM answers marginally improved performance, while refining student solutions fostered trust. Structured guidance reduced random queries as well as instances of students copy-pasting assignment questions to the LLM. Our work highlights the role t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39640;&#32423;&#22768;&#26126;&#20013;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#24555;&#36895;&#35774;&#35745;&#21644;&#23454;&#26045;&#12290;</title><link>http://arxiv.org/abs/2310.08535</link><description>&lt;p&gt;
&#27491;&#24335;&#35268;&#23450;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#39640;&#32423;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Formally Specifying the High-Level Behavior of LLM-Based Agents. (arXiv:2310.08535v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39640;&#32423;&#22768;&#26126;&#20013;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#24555;&#36895;&#35774;&#35745;&#21644;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#30446;&#26631;&#39537;&#21160;&#22411;&#20195;&#29702;&#20154;&#24050;&#25104;&#20026;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#33719;&#24471;&#26114;&#36149;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#12290;&#30446;&#21069;&#65292;&#36825;&#31867;&#20195;&#29702;&#20154;&#30340;&#35774;&#35745;&#21644;&#23454;&#26045;&#26159;&#20020;&#26102;&#24615;&#30340;&#65292;&#22240;&#20026;LLM-based&#20195;&#29702;&#20154;&#21487;&#33021;&#24212;&#29992;&#20110;&#30340;&#21508;&#31181;&#20219;&#21153;&#30340;&#24191;&#27867;&#24615;&#36136;&#24847;&#21619;&#30528;&#19981;&#33021;&#26377;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#20195;&#29702;&#20154;&#35774;&#35745;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#31616;&#21270;&#20195;&#29702;&#20154;&#26500;&#24314;&#36807;&#31243;&#30340;&#26368;&#23567;&#29983;&#25104;&#26694;&#26550;&#26469;&#20943;&#36731;&#35774;&#35745;&#21644;&#23454;&#26045;&#26032;&#20195;&#29702;&#20154;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#30340;&#26694;&#26550;&#20801;&#35768;&#29992;&#25143;&#20197;&#39640;&#32423;&#22768;&#26126;&#30340;&#35268;&#33539;&#26041;&#24335;&#23450;&#20041;&#25152;&#38656;&#30340;&#20195;&#29702;&#20154;&#34892;&#20026;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#35268;&#33539;&#26500;&#24314;&#35299;&#30721;&#30417;&#35270;&#22120;&#65292;&#20197;&#30830;&#20445;LLM&#20250;&#20135;&#29983;&#20855;&#26377;&#25152;&#38656;&#34892;&#20026;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#22768;&#26126;&#24615;&#26041;&#27861;&#65292;&#21363;&#25551;&#36848;&#34892;&#20026;&#32780;&#19981;&#32771;&#34385;&#22914;&#20309;&#23454;&#26045;&#25110;&#24378;&#21046;&#25191;&#34892;&#65292;&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Autonomous, goal-driven agents powered by LLMs have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic generation framework that simplifies the process of building agents. The framework we introduce allows the user to define desired agent behaviors in a high-level, declarative specification that is then used to construct a decoding monitor which guarantees the LLM will produce an output exhibiting the desired behavior. Our declarative approach, in which the behavior is described without concern for how it should be implemented or enforced, enables rapid design,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08276</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#22312;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval. (arXiv:2310.08276v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08276
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#28982;&#32780;&#22312;&#36965;&#24863;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#30528;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#65292;&#36825;&#23548;&#33268;&#20102;&#38750;&#35821;&#20041;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#38169;&#35823;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#26469;&#25366;&#25496;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;ROAM&#65289;&#65292;&#22312;&#28508;&#22312;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#26681;&#25454;&#21306;&#22495;&#35270;&#35273;&#29305;&#24449;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#26368;&#32456;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65288;DTGA&#65289;&#65292;&#29992;&#36739;&#23569;&#30340;&#27880;&#24847;&#21147;&#25805;&#20316;&#26469;&#25193;&#23637;&#21487;&#22788;&#29702;&#30340;&#25991;&#26412;&#34920;&#31034;&#33539;&#22260;&#65292;&#22686;&#24378;&#20840;&#23616;&#35789;&#32423;&#35821;&#20041;&#36830;&#25509;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20840;&#23616;&#35270;&#35273;-&#35821;&#20041;&#32422;&#26463;&#26469;&#20943;&#23569;&#21333;&#19968;&#35270;&#35273;&#20381;&#36182;&#65292;&#24182;&#20026;&#26368;&#32456;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#25552;&#20379;&#22806;&#37096;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-text retrieval has developed rapidly in recent years. However, it is still a challenge in remote sensing due to visual-semantic imbalance, which leads to incorrect matching of non-semantic visual and textual features. To solve this problem, we propose a novel Direction-Oriented Visual-semantic Embedding Model (DOVE) to mine the relationship between vision and language. Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the distance between the final visual and textual embeddings in the latent semantic space, oriented by regional visual features. Meanwhile, a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the range of tractable textual representation and enhance global word-level semantic connections using less attention operations. Ultimately, we exploit a global visual-semantic constraint to reduce single visual dependency and serve as an external constraint for the final visual and textual representations. The effectiveness and su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.05207</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12289;&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#26469;&#25552;&#39640;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#30340;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;&#20309;&#23558;&#22823;&#37327;&#30340;&#22312;&#37326;&#38750;&#26631;&#35760;&#38754;&#37096;&#22270;&#20687;&#24341;&#20837;&#30417;&#30563;&#24335;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#20013;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AU&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21516;&#26500;&#38754;&#37096;&#25552;&#21462;&#27169;&#22359;&#30340;&#21442;&#25968;&#65292;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21516;&#26102;&#23398;&#20064;AU&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#20197;&#21450;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#29305;&#24449;&#23545;&#40784;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#22120;&#21644;&#25913;&#36827;&#30340;&#23545;&#27604;&#25439;&#22833;&#28155;&#21152;&#20102;&#22235;&#20010;&#39069;&#22806;&#30340;&#20013;&#38388;&#30417;&#30563;&#22120;&#26469;&#20419;&#36827;&#29305;&#24449;&#37325;&#24314;&#30340;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
&lt;/p&gt;</description></item><item><title>"&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26816;&#32034;&#22686;&#24378;&#21644;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#21487;&#20197;&#21462;&#24471;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#24494;&#35843;&#30340;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;"</title><link>http://arxiv.org/abs/2310.03025</link><description>&lt;p&gt;
"&#26816;&#32034;&#36935;&#19978;&#38271;&#31687;&#22823;&#35821;&#35328;&#27169;&#22411;"
&lt;/p&gt;
&lt;p&gt;
Retrieval meets Long Context Large Language Models. (arXiv:2310.03025v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03025
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#26816;&#32034;&#22686;&#24378;&#21644;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#21487;&#20197;&#21462;&#24471;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#24494;&#35843;&#30340;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#26368;&#36817;&#65292;&#25193;&#23637;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#32780;&#23558;&#26816;&#32034;&#19982;LLM&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#23384;&#22312;&#22810;&#24180;&#12290;&#33258;&#28982;&#32780;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#26816;&#32034;&#22686;&#24378;&#19982;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#21738;&#20010;&#23545;&#19979;&#28216;&#20219;&#21153;&#26356;&#22909;&#65311;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#32467;&#21512;&#36215;&#26469;&#20197;&#20860;&#39038;&#21033;&#24330;&#21527;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;LLM&#65288;&#21363;&#19968;&#20010;&#31169;&#26377;&#30340;43B GPT&#21644;Llama2-70B&#65289;&#26469;&#30740;&#31350;&#36825;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#65292;LLM&#20351;&#29992;4K&#19978;&#19979;&#25991;&#31383;&#21475;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#26816;&#32034;&#22686;&#24378;&#22312;&#29983;&#25104;&#26102;&#21487;&#20197;&#36798;&#21040;&#19982;&#36890;&#36807;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;&#20301;&#32622;&#25554;&#20540;&#30340;&#24494;&#35843;LLM&#20351;&#29992;16K&#19978;&#19979;&#25991;&#31383;&#21475;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#35745;&#31639;&#37327;&#35201;&#23569;&#24471;&#22810;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#35770;&#20854;&#25193;&#23637;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#22914;&#20309;&#65292;&#26816;&#32034;&#37117;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;32K&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#26816;&#32034;&#22686;&#24378;Llama2-70B&#12290;"
&lt;/p&gt;
&lt;p&gt;
Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context wi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#65292;&#21628;&#21505;&#35748;&#35782;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#28145;&#24230;&#20266;&#36896;&#12289;&#21512;&#25104;&#36523;&#20221;&#24694;&#24847;&#27963;&#21160;&#20197;&#21450;&#34394;&#20551;&#20449;&#24687;&#21644;&#27450;&#35784;&#26041;&#38754;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.00737</link><description>&lt;p&gt;
&#12298;GenAI&#23545;&#25239;&#20154;&#24615;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37034;&#24694;&#24212;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models. (arXiv:2310.00737v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00737
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#65292;&#21628;&#21505;&#35748;&#35782;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#28145;&#24230;&#20266;&#36896;&#12289;&#21512;&#25104;&#36523;&#20221;&#24694;&#24847;&#27963;&#21160;&#20197;&#21450;&#34394;&#20551;&#20449;&#24687;&#21644;&#27450;&#35784;&#26041;&#38754;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#25216;&#26415;&#30340;&#22855;&#36857;&#65292;&#20197;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24335;&#20869;&#23481;&#29983;&#25104;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#32780;&#21463;&#21040;&#36190;&#25196;&#65292;&#23427;&#20204;&#25215;&#35834;&#24102;&#26469;&#19968;&#20010;&#21464;&#38761;&#30340;&#26410;&#26469;&#12290;&#20294;&#23601;&#20687;&#25152;&#26377;&#24378;&#22823;&#30340;&#24037;&#20855;&#19968;&#26679;&#65292;&#23427;&#20204;&#20063;&#26377;&#20854;&#38452;&#24433;&#23384;&#22312;&#12290;&#24819;&#35937;&#19968;&#19979;&#29983;&#27963;&#22312;&#19968;&#20010;&#28145;&#24230;&#20266;&#36896;&#19982;&#29616;&#23454;&#26080;&#27861;&#21306;&#20998;&#12289;&#21512;&#25104;&#36523;&#20221;&#32452;&#32455;&#24694;&#24847;&#27963;&#21160;&#12289;&#20197;&#21450;&#26377;&#30528;&#26080;&#19982;&#20262;&#27604;&#31934;&#30830;&#24230;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#34394;&#20551;&#20449;&#24687;&#25110;&#27450;&#35784;&#25163;&#27861;&#30340;&#19990;&#30028;&#12290;&#27426;&#36814;&#26469;&#21040;GenAI&#24212;&#29992;&#30340;&#40657;&#26263;&#38754;&#12290;&#26412;&#25991;&#19981;&#20165;&#26159;&#25506;&#32034;GenAI&#21644;LLMs&#28508;&#22312;&#28389;&#29992;&#30340;&#26053;&#31243;&#65292;&#20063;&#26159;&#21628;&#21505;&#35748;&#35782;&#21040;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#32039;&#36843;&#24615;&#12290;&#22312;&#25105;&#20204;&#33322;&#34892;&#20110;&#34394;&#20551;&#20449;&#24687;&#27963;&#21160;&#12289;&#24694;&#24847;&#20869;&#23481;&#29983;&#25104;&#19982;&#31934;&#23494;&#24694;&#24847;&#36719;&#20214;&#26500;&#24314;&#30340;&#28023;&#27915;&#20013;&#65292;&#25105;&#20204;&#23558;&#25581;&#31034;&#36825;&#22330;&#25105;&#20204;&#27491;&#22312;&#35265;&#35777;&#30340;GenAI&#38761;&#21629;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social med
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#27491;&#38754;&#36824;&#26159;&#36127;&#38754;&#25551;&#36848;AI&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#37117;&#19981;&#23384;&#22312;AI&#26102;&#65292;&#21442;&#19982;&#32773;&#30340;&#26399;&#26395;&#20540;&#37117;&#24456;&#39640;&#65292;&#24182;&#19988;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;AI&#30340;&#34920;&#29616;&#26399;&#26395;&#22312;&#36127;&#38754;&#25551;&#36848;&#19979;&#26159;&#26377;&#20559;&#24046;&#19988;&#31283;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.16606</link><description>&lt;p&gt;
&#8220;AI&#22686;&#24378;&#25105;&#20204;&#30340;&#34920;&#29616;&#65292;&#25105;&#27627;&#19981;&#24576;&#30097;&#36825;&#19968;&#31687;&#35770;&#25991;&#20063;&#20250;&#20570;&#21040;&#21516;&#26679;&#30340;&#20107;&#24773;&#8221;&#65306;&#23433;&#24944;&#21058;&#25928;&#24212;&#23545;AI&#36127;&#38754;&#25551;&#36848;&#26377;&#19968;&#23450;&#30340;&#24433;&#21709;(arXiv:2309.16606v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
"AI enhances our performance, I have no doubt this one will do the same": The Placebo effect is robust to negative descriptions of AI. (arXiv:2309.16606v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#27491;&#38754;&#36824;&#26159;&#36127;&#38754;&#25551;&#36848;AI&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#37117;&#19981;&#23384;&#22312;AI&#26102;&#65292;&#21442;&#19982;&#32773;&#30340;&#26399;&#26395;&#20540;&#37117;&#24456;&#39640;&#65292;&#24182;&#19988;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;AI&#30340;&#34920;&#29616;&#26399;&#26395;&#22312;&#36127;&#38754;&#25551;&#36848;&#19979;&#26159;&#26377;&#20559;&#24046;&#19988;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#26356;&#39640;&#30340;&#26399;&#26395;&#36890;&#36807;&#23433;&#24944;&#21058;&#25928;&#24212;&#20419;&#36827;&#20102;&#20154;&#26426;&#20132;&#20114;&#30340;&#34920;&#29616;&#12290;&#38477;&#20302;&#26399;&#26395;&#26469;&#25511;&#21046;&#23433;&#24944;&#21058;&#25928;&#24212;&#26159;&#26126;&#26234;&#30340;&#65292;&#20294;&#36807;&#20110;&#36127;&#38754;&#30340;&#26399;&#26395;&#21487;&#33021;&#23548;&#33268;&#36127;&#23433;&#24944;&#21058;&#25928;&#24212;&#12290;&#22312;&#19968;&#20010;&#23383;&#27597;&#36776;&#35782;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21578;&#30693;&#21442;&#19982;&#32773;AI&#20250;&#36890;&#36807;&#35843;&#25972;&#30028;&#38754;&#26469;&#22686;&#24378;&#25110;&#38477;&#20302;&#20182;&#20204;&#30340;&#34920;&#29616;&#65292;&#20294;&#23454;&#38469;&#19978;&#65292;&#22312;&#20219;&#20309;&#26465;&#20214;&#19979;&#37117;&#27809;&#26377;AI&#23384;&#22312;&#12290;&#36125;&#21494;&#26031;&#20998;&#26512;&#26174;&#31034;&#65292;&#21442;&#19982;&#32773;&#26399;&#26395;&#20540;&#24456;&#39640;&#65292;&#24182;&#19988;&#19981;&#31649;AI&#30340;&#25551;&#36848;&#22914;&#20309;&#65292;&#24403;&#19968;&#20010;&#34394;&#20551;&#30340;AI&#23384;&#22312;&#26102;&#65292;&#20182;&#20204;&#30340;&#34920;&#29616;&#37117;&#26356;&#22909;&#12290;&#36890;&#36807;&#35748;&#30693;&#24314;&#27169;&#65292;&#25105;&#20204;&#21487;&#20197;&#36861;&#28335;&#21040;&#21442;&#19982;&#32773;&#25910;&#38598;&#26356;&#22810;&#20449;&#24687;&#20174;&#32780;&#33719;&#24471;&#20248;&#21183;&#12290;&#19968;&#39033;&#22797;&#21046;&#30740;&#31350;&#39564;&#35777;&#20102;&#36127;&#38754;AI&#25551;&#36848;&#19981;&#20250;&#25913;&#21464;&#26399;&#26395;&#20540;&#65292;&#36825;&#34920;&#26126;AI&#30340;&#34920;&#29616;&#26399;&#26395;&#22312;&#36127;&#38754;&#35328;&#35821;&#25551;&#36848;&#19979;&#26159;&#26377;&#20559;&#24046;&#19988;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#29992;&#25143;&#26399;&#26395;&#23545;&#20110;AI&#20132;&#20114;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#34892;&#20026;&#23433;&#24944;&#21058;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heightened AI expectations facilitate performance in human-AI interactions through placebo effects. While lowering expectations to control for placebo effects is advisable, overly negative expectations could induce nocebo effects. In a letter discrimination task, we informed participants that an AI would either increase or decrease their performance by adapting the interface, but in reality, no AI was present in any condition. A Bayesian analysis showed that participants had high expectations and performed descriptively better irrespective of the AI description when a sham-AI was present. Using cognitive modeling, we could trace this advantage back to participants gathering more information. A replication study verified that negative AI descriptions do not alter expectations, suggesting that performance expectations with AI are biased and robust to negative verbal descriptions. We discuss the impact of user expectations on AI interactions and evaluation and provide a behavioral placebo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#22312;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.10016</link><description>&lt;p&gt;
GPT-3&#29992;&#20110;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction. (arXiv:2309.10016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#22312;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32467;&#26500;&#21270;&#30340;&#33647;&#29289;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#22312;&#20116;&#31181;&#32452;&#32455;&#31867;&#22411;&#20013;&#25506;&#31350;&#20102;GPT-3&#22312;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20998;&#21035;&#37319;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;&#24494;&#35843;&#33539;&#24335;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#33647;&#29289;&#30340;SMILES&#34920;&#31034;&#21644;&#32454;&#32990;&#31995;&#30340;&#22522;&#22240;&#32452;&#31361;&#21464;&#29305;&#24449;&#23545;&#33647;&#29289;&#21453;&#24212;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#26377;&#26395;&#20026;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the potential of GPT-3 for the anti-cancer drug sensitivity prediction task using structured pharmacogenomics data across five tissue types and evaluated its performance with zero-shot prompting and fine-tuning paradigms. The drug's smile representation and cell line's genomic mutation features were predictive of the drug response. The results from this study have the potential to pave the way for designing more efficient treatment protocols in precision oncology.
&lt;/p&gt;</description></item><item><title>CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.09552</link><description>&lt;p&gt;
CB-Whisper: &#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#32622;&#30340;Whisper
&lt;/p&gt;
&lt;p&gt;
CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary Keyword-Spotting. (arXiv:2309.09552v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09552
&lt;/p&gt;
&lt;p&gt;
CB-Whisper&#26159;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23454;&#20307;&#20316;&#20026;&#25552;&#31034;&#26469;&#25913;&#36827;&#35782;&#21035;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#23454;&#20307;&#21484;&#22238;&#29575;&#30340;&#21516;&#26102;&#20250;&#30053;&#24494;&#22686;&#21152;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#35782;&#21035;&#32597;&#35265;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#12289;&#32452;&#32455;&#26426;&#26500;&#21644;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#32463;&#24120;&#36935;&#21040;&#30340;&#19987;&#19994;&#26415;&#35821;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OpenAI&#30340;Whisper&#27169;&#22411;&#30340;Contextual Biasing Whisper&#65288;CB-Whisper&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#25191;&#34892;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#26816;&#27979;&#65288;OV-KWS&#65289;&#26469;&#35782;&#21035;&#29992;&#25143;&#23450;&#20041;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#35782;&#21035;&#20986;&#30340;&#23454;&#20307;&#34987;&#29992;&#20316;Whisper&#35299;&#30721;&#22120;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;OV-KWS&#21644;ASR&#20219;&#21153;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21407;&#22987;Whisper&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20013;&#22269;Aishell&#28909;&#35789;&#23376;&#38598;&#21644;&#20004;&#20010;&#20869;&#37096;&#20195;&#30721;&#20999;&#25442;&#27979;&#35797;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#23454;&#20307;&#21484;&#22238;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20869;&#37096;&#27979;&#35797;&#38598;&#19978;&#28151;&#28102;&#38169;&#35823;&#29575;&#65288;MER&#65289;&#30053;&#24494;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;Whisper&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations, and terminologies not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that can recognize user-defined name entities by performing open-vocabulary keyword-spotting (OV-KWS) using the hidden states of Whisper encoder. The recognized entities are used as prompts for the Whisper decoder. We first propose a multitask training approach with OV-KWS and ASR tasks to optimize the model. Experiments show that this approach substantially improves the entity recalls compared to the original Whisper model on Chinese Aishell hot word subsets and two internal code-switch test sets. However, we observed a slight increase in mixed-error-rate (MER) on internal test sets due to catastrophic forgetting. To address this problem and use different sizes of the Wh
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#31614;&#21517;&#32593;&#32476;&#20013;&#30340;&#38450;&#24481;&#32852;&#30431;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#30340;&#32676;&#20307;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21916;&#22909;&#21644;&#21388;&#24694;&#20851;&#31995;&#65292;&#22312;&#32852;&#30431;&#24418;&#25104;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.06801</link><description>&lt;p&gt;
&#31614;&#21517;&#32593;&#32476;&#20013;&#30340;&#38450;&#24481;&#32852;&#30431;
&lt;/p&gt;
&lt;p&gt;
Defensive Alliances in Signed Networks. (arXiv:2309.06801v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#31614;&#21517;&#32593;&#32476;&#20013;&#30340;&#38450;&#24481;&#32852;&#30431;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#30340;&#32676;&#20307;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21916;&#22909;&#21644;&#21388;&#24694;&#20851;&#31995;&#65292;&#22312;&#32852;&#30431;&#24418;&#25104;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20998;&#26512;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#12290;&#26576;&#20123;&#30740;&#31350;&#26041;&#21521;&#28041;&#21450;&#23547;&#25214;&#33021;&#22815;&#20849;&#21516;&#21512;&#20316;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#30340;&#26234;&#33021;&#20307;&#32676;&#20307;&#12290;&#20026;&#27492;&#65292;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#19981;&#21516;&#27010;&#24565;&#30340;&#22270;&#19982;&#32593;&#32476;&#20013;&#30340;&#38598;&#32676;&#25110;&#31038;&#21306;&#12290;&#20854;&#20013;&#65292;&#38450;&#24481;&#32852;&#30431;&#26159;&#19968;&#31181;&#37327;&#21270;&#30340;&#32676;&#20307;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#32852;&#30431;&#30340;&#25152;&#26377;&#30740;&#31350;&#37117;&#24573;&#35270;&#20102;&#19968;&#20010;&#22312;&#24418;&#25104;&#32852;&#30431;&#20013;&#38750;&#24120;&#30452;&#35266;&#30340;&#26041;&#38754;&#65292;&#21363;&#20551;&#35774;&#26234;&#33021;&#20307;&#22312;&#24577;&#24230;&#26041;&#38754;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#26377;&#39044;&#35774;&#65292;&#20182;&#20204;&#21916;&#27426;&#21644;&#20182;&#20204;&#21916;&#27426;&#30340;&#26234;&#33021;&#20307;&#19968;&#36215;&#22312;&#26576;&#20010;&#32676;&#20307;&#65288;&#32852;&#30431;&#65289;&#20013;&#65292;&#22240;&#27492;&#24895;&#24847;&#30456;&#20114;&#24110;&#21161;&#23454;&#29616;&#20849;&#21516;&#30340;&#30446;&#26631;&#65292;&#21487;&#33021;&#20250;&#23545;&#19981;&#21916;&#27426;&#30340;&#32676;&#20307;&#22806;&#30340;&#26234;&#33021;&#20307;&#36827;&#34892;&#23545;&#25239;&#12290;&#31614;&#21517;&#32593;&#32476;&#22312;&#24515;&#29702;&#23398;&#25991;&#29486;&#20013;&#34987;&#24341;&#20837;&#20197;&#27169;&#25311;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21916;&#27426;&#21644;&#21388;&#24694;&#20851;&#31995;&#65292;&#36825;&#25193;&#23637;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of (social) networks and multi-agent systems is a central theme in Artificial Intelligence. Some line of research deals with finding groups of agents that could work together to achieve a certain goal. To this end, different notions of so-called clusters or communities have been introduced in the literature of graphs and networks. Among these, defensive alliance is a kind of quantitative group structure. However, all studies on the alliance so for have ignored one aspect that is central to the formation of alliances on a very intuitive level, assuming that the agents are preconditioned concerning their attitude towards other agents: they prefer to be in some group (alliance) together with the agents they like, so that they are happy to help each other towards their common aim, possibly then working against the agents outside of their group that they dislike. Signed networks were introduced in the psychology literature to model liking and disliking between agents, generaliz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#34892;&#21160;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14284</link><description>&lt;p&gt;
LLM&#24378;&#21270;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
LLM Powered Sim-to-real Transfer for Traffic Signal Control. (arXiv:2308.14284v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#34892;&#21160;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#25928;&#30340;&#20132;&#36890;&#21644;&#20943;&#36731;&#25317;&#22581;&#28010;&#36153;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#24403;&#22312;&#20223;&#30495;&#22120;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#26102;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#25166;&#26681;&#34892;&#21160;&#36716;&#25442;&#65292;&#26469;&#29702;&#35299;&#21644;&#25551;&#36848;&#31995;&#32479;&#21160;&#24577;&#12290;&#36890;&#36807;&#25509;&#21463;&#22635;&#31354;&#25552;&#31034;&#27169;&#26495;&#65292;&#24182;&#26681;&#25454;&#21487;&#20197;&#35775;&#38382;&#30340;&#19978;&#19979;&#25991;&#22635;&#20889;&#31572;&#26696;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24212;&#29992;&#20110;&#23545;&#31995;&#32479;&#21160;&#24577;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;PET&#22270;&#20687;&#37325;&#24314;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#23545;PET&#22270;&#20687;&#37325;&#24314;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#39640;&#26041;&#24046;&#30340;&#27850;&#26494;&#22122;&#22768;&#21644;&#24191;&#27867;&#30340;&#21160;&#24577;&#33539;&#22260;&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;PET&#37325;&#24314;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.14190</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;PET&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Score-Based Generative Models for PET Image Reconstruction. (arXiv:2308.14190v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;PET&#22270;&#20687;&#37325;&#24314;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#23545;PET&#22270;&#20687;&#37325;&#24314;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#39640;&#26041;&#24046;&#30340;&#27850;&#26494;&#22122;&#22768;&#21644;&#24191;&#27867;&#30340;&#21160;&#24577;&#33539;&#22260;&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;PET&#37325;&#24314;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#25110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#31561;&#21307;&#23398;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#39640;&#24230;&#26377;&#21069;&#26223;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#65288;PET&#65289;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#30456;&#23545;&#26410;&#30693;&#12290;PET&#22270;&#20687;&#37325;&#24314;&#28041;&#21450;&#22810;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#39640;&#26041;&#24046;&#30340;&#27850;&#26494;&#22122;&#22768;&#21644;&#24191;&#27867;&#30340;&#21160;&#24577;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#36866;&#29992;&#20110;PET&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#29305;&#23450;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;2D&#21644;3D PET&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#20351;&#29992;&#30913;&#20849;&#25391;&#22270;&#20687;&#36827;&#34892;&#24341;&#23548;&#37325;&#24314;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27809;&#26377;&#30149;&#21464;&#30340;&#24739;&#32773;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;2D&#21644;3D&#30340;$\textit{in-silico}$&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27809;&#26377;&#30149;&#21464;&#30340;&#25968;&#25454;&#20197;&#21450;&#24102;&#26377;&#30149;&#21464;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;PET&#37325;&#24314;&#25913;&#36827;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models have demonstrated highly promising results for medical image reconstruction tasks in magnetic resonance imaging or computed tomography. However, their application to Positron Emission Tomography (PET) is still largely unexplored. PET image reconstruction involves a variety of challenges, including Poisson noise with high variance and a wide dynamic range. To address these challenges, we propose several PET-specific adaptations of score-based generative models. The proposed framework is developed for both 2D and 3D PET. In addition, we provide an extension to guided reconstruction using magnetic resonance images. We validate the approach through extensive 2D and 3D $\textit{in-silico}$ experiments with a model trained on patient-realistic data without lesions, and evaluate on data without lesions as well as out-of-distribution data with lesions. This demonstrates the proposed method's robustness and significant potential for improved PET reconstruction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#29615;&#22659;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32597;&#35265;&#30149;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12890</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25237;&#31080;&#65306;&#29992;&#20110;&#32597;&#35265;&#30149;&#35782;&#21035;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#29615;&#22659;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32597;&#35265;&#30149;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24378;&#35843;&#20102;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;LLMs&#32463;&#24120;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#37324;&#20219;&#21153;&#21482;&#20351;&#29992;&#24456;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#25191;&#34892;&#12290;FSL&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;(AI)&#23376;&#39046;&#22495;&#20013;&#21464;&#24471;&#27969;&#34892;&#65292;&#21253;&#25324;&#29992;&#20110;&#20581;&#24247;&#30340;AI&#12290;&#32597;&#35265;&#30149;&#24433;&#21709;&#20154;&#21475;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#22312;&#25968;&#25454;&#21487;&#29992;&#24615;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979; inherently &#38656;&#35201;FSL&#25216;&#26415;&#65292;&#23613;&#31649;&#20154;&#24037;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#36153;&#26102;&#36153;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;FSL&#29615;&#22659;&#20013;LLM&#26597;&#35810;&#24615;&#33021;&#30340;&#28789;&#27963;&#25552;&#31034;&#26041;&#27861;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#22312;&#21333;&#27425;&#32597;&#35265;&#30149;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#20219;&#20309;&#21333;&#20010;&#27169;&#22411;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32597;&#35265;&#30149;&#25968;&#25454;&#38598;&#29992;&#20110;FSL&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases, affecting a small fraction of the population, inherently require FSL techniques due to limited data availability, though manual data collection and annotation is costly and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FS
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#35774;&#22791;&#32423;&#19978;&#24212;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11624</link><description>&lt;p&gt;
&#29992;&#36890;&#29992;&#35774;&#22791;&#32534;&#30721;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#38761;&#26032;TCAD&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks. (arXiv:2308.11624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#35774;&#22791;&#32423;&#19978;&#24212;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#26469;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#19981;&#20165;&#32771;&#34385;&#20102;&#26448;&#26009;&#32423;&#21644;&#22120;&#20214;&#32423;&#23884;&#20837;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31354;&#38388;&#20851;&#31995;&#30340;&#23884;&#20837;&#65292;&#21463;&#26377;&#38480;&#20803;&#32593;&#26684;&#20013;&#24120;&#29992;&#30340;&#25554;&#20540;&#25805;&#20316;&#21551;&#21457;&#32780;&#26469;&#12290;&#21033;&#29992;&#22120;&#20214;&#27169;&#25311;&#30340;&#36890;&#29992;&#29289;&#29702;&#23450;&#24459;&#36827;&#34892;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#21253;&#25324;&#22522;&#20110;&#27850;&#26494;&#20223;&#30495;&#30340;&#26367;&#20195;&#21644;&#22522;&#20110;&#28418;&#31227;&#25193;&#25955;&#27169;&#22411;&#30340;&#30005;&#27969;-&#30005;&#21387;&#65288;IV&#65289;&#39044;&#27979;&#12290;&#36825;&#20004;&#32773;&#37117;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;&#31216;&#20026;RelGAT&#65289;&#23454;&#29616;&#30340;&#12290;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;Sentaurus TCAD&#22120;&#20214;&#27169;&#25311;&#22120;&#30340;&#35814;&#32454;&#25216;&#26415;&#32454;&#33410;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#22312;&#35774;&#22791;&#32423;&#19978;&#37319;&#29992;&#25552;&#20986;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
An innovative methodology that leverages artificial intelligence (AI) and graph representation for semiconductor device encoding in TCAD device simulation is proposed. A graph-based universal encoding scheme is presented that not only considers material-level and device-level embeddings, but also introduces a novel spatial relationship embedding inspired by interpolation operations typically used in finite element meshing. Universal physical laws from device simulations are leveraged for comprehensive data-driven modeling, which encompasses surrogate Poisson emulation and current-voltage (IV) prediction based on drift-diffusion model. Both are achieved using a novel graph attention network, referred to as RelGAT. Comprehensive technical details based on the device simulator Sentaurus TCAD are presented, empowering researchers to adopt the proposed AI-driven Electronic Design Automation (EDA) solution at the device level.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#20998;&#31867;&#26641;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20943;&#23569;&#23376;&#38382;&#39064;&#25968;&#37327;&#12289;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#20316;&#20026;&#21106;&#24179;&#38754;&#20197;&#21450;&#29983;&#25104;&#36829;&#21453;&#32422;&#26463;&#30340;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#24182;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.11477</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#20998;&#31867;&#26641;
&lt;/p&gt;
&lt;p&gt;
Revisiting column-generation-based matheuristic for learning classification trees. (arXiv:2308.11477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#20998;&#31867;&#26641;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20943;&#23569;&#23376;&#38382;&#39064;&#25968;&#37327;&#12289;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#20316;&#20026;&#21106;&#24179;&#38754;&#20197;&#21450;&#29983;&#25104;&#36829;&#21453;&#32422;&#26463;&#30340;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#24182;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#20915;&#20998;&#31867;&#38382;&#39064;&#30340;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#20915;&#31574;&#26641;&#24555;&#36895;&#20294;&#29983;&#25104;&#30340;&#26641;&#22312;&#20934;&#30830;&#24615;&#19978;&#19981;&#22815;&#20248;&#21270;&#12290;&#25991;&#29486;&#20013;&#20854;&#20182;&#31163;&#25955;&#20248;&#21270;&#27169;&#22411;&#35299;&#20915;&#20102;&#26368;&#20248;&#24615;&#38382;&#39064;&#20294;&#21482;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;firat2020column&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21015;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21487;&#20280;&#32553;&#24615;&#65292;&#24182;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#23545;&#35813;&#21015;&#29983;&#25104;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#23376;&#38382;&#39064;&#27169;&#22411;&#20197;&#26174;&#33879;&#20943;&#23569;&#22810;&#31867;&#20998;&#31867;&#23454;&#20363;&#20013;&#30340;&#23376;&#38382;&#39064;&#25968;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20027;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#20381;&#36182;&#32422;&#26463;&#26159;&#34164;&#21547;&#30340;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21106;&#24179;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20998;&#31163;&#27169;&#22411;&#26469;&#29983;&#25104;&#32447;&#24615;&#35268;&#21010;&#26494;&#24347;&#35299;&#36829;&#21453;&#20854;&#23545;&#24212;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are highly interpretable models for solving classification problems in machine learning (ML). The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy. Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets. \cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees. This approach improves scalability and can work with large datasets. In this paper, we describe improvements to this column generation approach. First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances. Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes. Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their correspond
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#28151;&#21512;&#31995;&#32479;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26816;&#26597;&#30693;&#35782;&#24211;&#26469;&#30830;&#23450;&#23427;&#20204;&#22312;&#20419;&#36827;&#25104;&#21151;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#35768;&#22810;&#30693;&#35782;&#24211;&#28385;&#36275;&#21028;&#25454;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#26080;&#27861;&#28385;&#36275;&#30340;&#30693;&#35782;&#24211;&#12290;</title><link>http://arxiv.org/abs/2308.10487</link><description>&lt;p&gt;
&#29992;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#35299;&#23494;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20013;&#30340;&#21407;&#22987;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees. (arXiv:2308.10487v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#28151;&#21512;&#31995;&#32479;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26816;&#26597;&#30693;&#35782;&#24211;&#26469;&#30830;&#23450;&#23427;&#20204;&#22312;&#20419;&#36827;&#25104;&#21151;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#35768;&#22810;&#30693;&#35782;&#24211;&#28385;&#36275;&#21028;&#25454;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#26080;&#27861;&#28385;&#36275;&#30340;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#28151;&#21512;&#31995;&#32479;&#22312;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#31526;&#21495;&#25512;&#29702;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20854;&#20013;&#24863;&#30693;&#27169;&#22411;&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#20174;&#31526;&#21495;&#30693;&#35782;&#24211;&#20013;&#25512;&#26029;&#20986;&#20449;&#24687;&#12290;&#23613;&#31649;&#26377;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#28151;&#21512;&#31995;&#32479;&#33021;&#22815;&#23398;&#20064;&#20934;&#30830;&#30340;&#24863;&#30693;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20026;&#20160;&#20040;&#28151;&#21512;&#31995;&#32479;&#33021;&#22815;&#25104;&#21151;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#20197;&#21450;&#22312;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#19979;&#21487;&#33021;&#22833;&#36133;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#65292;&#29992;&#20110;&#25551;&#36848;&#26469;&#33258;&#30693;&#35782;&#24211;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21028;&#25454;&#26469;&#30830;&#23450;&#30693;&#35782;&#22312;&#20419;&#36827;&#25104;&#21151;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#21147;&#12290;&#36825;&#26159;&#39318;&#27425;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#27491;&#22312;&#30740;&#31350;&#30340;&#30693;&#35782;&#24211;&#26469;&#22238;&#31572;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35768;&#22810;&#30693;&#35782;&#24211;&#28385;&#36275;&#21028;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#23398;&#20064;&#65292;&#32780;&#26377;&#20123;&#21017;&#26080;&#27861;&#28385;&#36275;&#65292;&#34920;&#26126;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic hybrid systems are promising for integrating machine learning and symbolic reasoning, where perception models are facilitated with information inferred from a symbolic knowledge base through logical reasoning. Despite empirical evidence showing the ability of hybrid systems to learn accurate perception models, the theoretical understanding of learnability is still lacking. Hence, it remains unclear why a hybrid system succeeds for a specific task and when it may fail given a different knowledge base. In this paper, we introduce a novel way of characterising supervision signals from a knowledge base, and establish a criterion for determining the knowledge's efficacy in facilitating successful learning. This, for the first time, allows us to address the two questions above by inspecting the knowledge base under investigation. Our analysis suggests that many knowledge bases satisfy the criterion, thus enabling effective learning, while some fail to satisfy it, indicating po
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03212</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#39640;&#24230;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#22478;&#24066;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#65292;&#20197;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32780;&#19981;&#21463;&#21018;&#24615;&#37051;&#22495;&#26465;&#20214;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19987;&#27880;&#20110;&#20174;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#34920;&#31034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#31227;&#21160;&#27969;&#27169;&#24335;&#12289;POI&#35821;&#20041;&#21644;&#31614;&#21040;&#21160;&#24577;&#20013;&#25429;&#25417;&#22810;&#35270;&#35282;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20840;&#23616;&#22270;&#27880;&#24847;&#32593;&#32476;&#26469;&#23398;&#20064;&#22270;&#20013;&#20219;&#24847;&#20004;&#20010;&#39030;&#28857;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20840;&#38754;&#32771;&#34385;&#21644;&#20849;&#20139;&#22810;&#20010;&#35270;&#35282;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#21033;&#29992;&#22806;&#37096;&#27880;&#24847;&#21147;&#23398;&#20064;&#26435;&#37325;&#26469;&#34701;&#21512;&#22810;&#35270;&#35282;&#23884;&#20837;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban region embedding is an important and yet highly challenging issue due to the complexity and constantly changing nature of urban data. To address the challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighbourhood region conditions. Our model focus on learn urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics and check-in dynamics. Then, we adopt global graph attention networks to learn similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32423;&#32852;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CasTGAN&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#32423;&#32852;&#26550;&#26500;&#65292;&#20854;&#20013;&#19987;&#38376;&#30340;&#29983;&#25104;&#22120;&#23545;&#27599;&#20010;&#29305;&#24449;&#36827;&#34892;&#37319;&#26679;&#65292;&#20351;&#24471;&#21512;&#25104;&#36755;&#20986;&#26356;&#33021;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CasTGAN&#33021;&#22815;&#20135;&#29983;&#26356;&#30495;&#23454;&#26377;&#25928;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.00384</link><description>&lt;p&gt;
CasTGAN: &#29992;&#20110;&#36924;&#30495;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#30340;&#32423;&#32852;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CasTGAN: Cascaded Generative Adversarial Network for Realistic Tabular Data Synthesis. (arXiv:2307.00384v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32423;&#32852;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CasTGAN&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#32423;&#32852;&#26550;&#26500;&#65292;&#20854;&#20013;&#19987;&#38376;&#30340;&#29983;&#25104;&#22120;&#23545;&#27599;&#20010;&#29305;&#24449;&#36827;&#34892;&#37319;&#26679;&#65292;&#20351;&#24471;&#21512;&#25104;&#36755;&#20986;&#26356;&#33021;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CasTGAN&#33021;&#22815;&#20135;&#29983;&#26356;&#30495;&#23454;&#26377;&#25928;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22240;&#20854;&#22312;&#29983;&#25104;&#21487;&#29992;&#20110;&#22810;&#31181;&#30446;&#30340;&#30340;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;GAN&#24050;&#32463;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#22797;&#21046;&#21407;&#22987;&#25968;&#25454;&#38598;&#21160;&#24577;&#30340;&#21512;&#25104;&#25968;&#25454;&#26679;&#26412;&#65292;&#20294;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#30340;&#38544;&#31169;&#38382;&#39064;&#20173;&#28982;&#26159;&#19981;&#23481;&#24573;&#35270;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32423;&#32852;&#34920;&#26684;GAN&#26694;&#26550;&#65288;CasTGAN&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20010;&#19978;&#19979;&#25991;&#20013;&#65292;&#26377;&#25928;&#24615;&#26159;&#25351;&#22312;&#30495;&#23454;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36890;&#24120;&#34987;&#20256;&#32479;&#30340;&#29983;&#25104;&#27169;&#22411;&#25152;&#35823;&#35299;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#37319;&#29992;&#32423;&#32852;&#26550;&#26500;&#65292;&#20854;&#20013;&#19987;&#38376;&#30340;&#29983;&#25104;&#22120;&#23545;&#27599;&#20010;&#29305;&#24449;&#36827;&#34892;&#37319;&#26679;&#65292;&#20351;&#24471;&#21512;&#25104;&#36755;&#20986;&#26356;&#33021;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CasTGAN&#33021;&#22815;&#20135;&#29983;&#26356;&#30495;&#23454;&#26377;&#25928;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) have drawn considerable attention in recent years for their proven capability in generating synthetic data which can be utilised for multiple purposes. While GANs have demonstrated tremendous successes in producing synthetic data samples that replicate the dynamics of the original datasets, the validity of the synthetic data and the underlying privacy concerns represent major challenges which are not sufficiently addressed. In this work, we design a cascaded tabular GAN framework (CasTGAN) for generating realistic tabular data with a specific focus on the validity of the output. In this context, validity refers to the the dependency between features that can be found in the real data, but is typically misrepresented by traditional generative models. Our key idea entails that employing a cascaded architecture in which a dedicated generator samples each feature, the synthetic output becomes more representative of the real data. Our experimental resu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2306.09549</link><description>&lt;p&gt;
QH9&#65306;QM9&#20998;&#23376;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#39044;&#27979;&#65292;&#20316;&#20026;&#31532;&#19968;&#24615;&#21407;&#29702;&#35745;&#31639;&#26041;&#27861;&#65288;&#22914;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#65289;&#30340;&#26367;&#20195;&#21697;&#12290;&#34429;&#28982;&#35768;&#22810;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#21270;&#23398;&#24615;&#36136;&#21644;&#21407;&#23376;&#21147;&#65292;&#20294;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#39044;&#27979;&#21704;&#23494;&#39039;&#30697;&#38453;&#30340;&#33021;&#21147;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#22522;&#26412;&#30340;&#29289;&#29702;&#37327;&#65292;&#23427;&#30830;&#23450;&#20102;&#29289;&#29702;&#31995;&#32479;&#21644;&#21270;&#23398;&#24615;&#36136;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;QH9&#65292;&#22522;&#20110;QM9&#25968;&#25454;&#38598;&#20026;2,399&#20010;&#20998;&#23376;&#21160;&#21147;&#23398;&#36712;&#36857;&#21644;130,831&#20010;&#31283;&#23450;&#20998;&#23376;&#20960;&#20309;&#24418;&#24577;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#21508;&#31181;&#20998;&#23376;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;QH9&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#37117;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#23454;&#38469;&#36951;&#25022;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.02869</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36951;&#25022;&#24179;&#34913;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Regret Balancing for Online Model Selection in Bandits. (arXiv:2306.02869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02869
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#23454;&#38469;&#36951;&#25022;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#36873;&#25321;&#65292;&#20854;&#20013;&#20803;&#23398;&#20064;&#22120;&#21487;&#20197;&#20351;&#29992;&#19968;&#32452;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#24182;&#26681;&#25454;&#27599;&#20010;&#22522;&#26412;&#23398;&#20064;&#22120;&#25512;&#33616;&#30340;&#31574;&#30053;&#21160;&#24577;&#20915;&#31574;&#12290;&#25105;&#20204;&#36890;&#36807;&#36951;&#25022;&#24179;&#34913;&#26469;&#25191;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#20294;&#19982;&#27492;&#30456;&#20851;&#30340;&#26368;&#36817;&#25991;&#29486;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#27809;&#26377;&#20551;&#35774;&#20219;&#20309;&#20851;&#20110;&#22522;&#26412;&#23398;&#20064;&#22120;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22914;&#20505;&#36873;&#36951;&#25022;&#20445;&#35777;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#25581;&#31034;&#36825;&#20123;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#20803;&#23398;&#20064;&#22120;&#33021;&#22815;&#21033;&#29992;&#27599;&#20010;&#22522;&#26412;&#23398;&#20064;&#22120;&#22312;&#32473;&#23450;&#30340;&#23398;&#20064;&#29615;&#22659;&#20013;&#20135;&#29983;&#30340;&#23454;&#38469;&#36951;&#25022;&#65288;&#32780;&#19981;&#26159;&#26399;&#26395;&#36951;&#25022;&#65289;&#65292;&#24182;&#25361;&#36873;&#20986;&#26368;&#20339;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#25805;&#20316;&#26356;&#20026;&#38596;&#24515;&#21187;&#21187;&#30340;&#36951;&#25022;&#27010;&#24565;&#65292;&#24182;&#19988;&#38500;&#20102;&#36890;&#36807;&#36951;&#25022;&#24179;&#34913;&#35777;&#26126;&#27169;&#22411;&#36873;&#25321;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#22788;&#29702;&#23454;&#38469;&#36951;&#25022;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#23454;&#38469;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider model selection for sequential decision making in stochastic environments with bandit feedback, where a meta-learner has at its disposal a pool of base learners, and decides on the fly which action to take based on the policies recommended by each base learner. Model selection is performed by regret balancing but, unlike the recent literature on this subject, we do not assume any prior knowledge about the base learners like candidate regret guarantees; instead, we uncover these quantities in a data-driven manner. The meta-learner is therefore able to leverage the realized regret incurred by each base learner for the learning environment at hand (as opposed to the expected regret), and single out the best such regret. We design two model selection algorithms operating with this more ambitious notion of regret and, besides proving model selection guarantees via regret balancing, we experimentally demonstrate the compelling practical benefits of dealing with actual regrets ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#25915;&#20987;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#23545;&#25239;&#25915;&#20987;&#25628;&#32034;&#65292;&#25552;&#39640;&#20102;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13208</link><description>&lt;p&gt;
&#22270;&#20687;&#24341;&#23548;&#30340;&#25925;&#20107;&#32467;&#23616;&#29983;&#25104;&#30340;&#36845;&#20195;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Iterative Adversarial Attack on Image-guided Story Ending Generation. (arXiv:2305.13208v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#25915;&#20987;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#23545;&#25239;&#25915;&#20987;&#25628;&#32034;&#65292;&#25552;&#39640;&#20102;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#28041;&#21450;&#24320;&#21457;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#65288;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#20449;&#24687;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#65292;&#22810;&#27169;&#24577;&#25991;&#26412;&#29983;&#25104;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#26041;&#38754;&#65292;&#23427;&#28041;&#21450;&#22788;&#29702;&#26469;&#33258;&#22810;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#24182;&#36755;&#20986;&#25991;&#26412;&#12290;&#22270;&#20687;&#24341;&#23548;&#30340;&#25925;&#20107;&#32467;&#23616;&#29983;&#25104;&#65288;IgSEG&#65289;&#26159;&#19968;&#20010;&#29305;&#21035;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#23436;&#25972;&#30340;&#25925;&#20107;&#32467;&#23616;&#25991;&#26412;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;IgSEG&#27169;&#22411;&#30340;&#22522;&#30784;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#24403;&#21069;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#65292;&#24182;&#26410;&#20998;&#26512;&#29992;&#20110;&#20351;&#29992;&#36328;&#27169;&#24577;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65288;Iterative-attack&#65289;&#65292;&#34701;&#21512;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#25915;&#20987;&#65292;&#20197;&#26356;&#26377;&#25928;&#30340;&#36845;&#20195;&#26041;&#24335;&#25628;&#32034;&#23545;&#25239;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning involves developing models that can integrate information from various sources like images and texts. In this field, multimodal text generation is a crucial aspect that involves processing data from multiple modalities and outputting text. The image-guided story ending generation (IgSEG) is a particularly significant task, targeting on an understanding of complex relationships between text and image data with a complete story text ending. Unfortunately, deep neural networks, which are the backbone of recent IgSEG models, are vulnerable to adversarial samples. Current adversarial attack methods mainly focus on single-modality data and do not analyze adversarial attacks for multimodal text generation tasks that use cross-modal information. To this end, we propose an iterative adversarial attack method (Iterative-attack) that fuses image and text modality attacks, allowing for an attack search for adversarial text and image in an more effective iterative way. Experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-RNN&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;CNN&#21644;RNN&#27169;&#22411;&#65292;&#22312;&#31934;&#24230;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.09994</link><description>&lt;p&gt;
&#22522;&#20110;LSTM-DeepLabv3+&#21644;&#26102;&#31354;&#29305;&#24449;&#34701;&#21512;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion. (arXiv:2304.09994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-RNN&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;CNN&#21644;RNN&#27169;&#22411;&#65292;&#22312;&#31934;&#24230;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22240;&#20854;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#36880;&#28176;&#25104;&#20026;&#27969;&#34892;&#30340;&#27946;&#27700;&#39044;&#27979;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#29420;&#30340;&#31354;&#38388;&#25110;&#26102;&#38388;&#29305;&#24449;&#20998;&#26512;&#65292;&#24182;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#32500;&#24230;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CNN-RNN&#30340;&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#65292;&#23558;CNN&#22312;&#22788;&#29702;&#31354;&#38388;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;RNN&#22312;&#20998;&#26512;&#19981;&#21516;&#32500;&#24230;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#30340;&#20248;&#21183;&#25972;&#21512;&#36215;&#26469;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#36827;&#34892;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#27946;&#27700;&#39044;&#27979;&#12290;&#24212;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#30830;&#23450;&#19971;&#20010;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#27946;&#27700;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#26368;&#20339;&#32452;&#21512;&#31574;&#30053;&#12290;&#36890;&#36807;&#32467;&#21512;&#22235;&#20010;CNN&#65288;FCN&#65292;UNet&#65292;SegNet&#65292;DeepLabv3+&#65289;&#21644;&#19977;&#20010;RNN&#65288;LSTM&#65292;BiLSTM&#65292;GRU&#65289;&#65292;&#26368;&#20248;&#28151;&#21512;&#27169;&#22411;&#34987;&#30830;&#23450;&#20026;LSTM-DeepLabv3+&#12290;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65288;MAE&#12289;RMSE&#12289;NSE&#21644;KGE&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have become increasingly popular for flood prediction due to their superior accuracy and efficiency compared to traditional methods. However, current machine learning methods often rely on separate spatial or temporal feature analysis and have limitations on the types, number, and dimensions of input data. This study presented a CNN-RNN hybrid feature fusion modelling approach for urban flood prediction, which integrated the strengths of CNNs in processing spatial features and RNNs in analyzing different dimensions of time sequences. This approach allowed for both static and dynamic flood predictions. Bayesian optimization was applied to identify the seven most influential flood-driven factors and determine the best combination strategy. By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM, BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. This model achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE wer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#65292;&#20855;&#26377;&#20248;&#20110;&#24120;&#35265;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09970</link><description>&lt;p&gt;
&#23398;&#20064;&#31574;&#30053;&#22312;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning policies for resource allocation in business processes. (arXiv:2304.09970v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#65292;&#20855;&#26377;&#20248;&#20110;&#24120;&#35265;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#20998;&#37197;&#26159;&#23558;&#36164;&#28304;&#20998;&#37197;&#21040;&#24517;&#39035;&#22312;&#36816;&#34892;&#26102;&#21051;&#25191;&#34892;&#30340;&#19994;&#21153;&#27969;&#31243;&#27963;&#21160;&#20013;&#12290;&#34429;&#28982;&#36164;&#28304;&#20998;&#37197;&#22312;&#21046;&#36896;&#31561;&#20854;&#20182;&#39046;&#22495;&#20013;&#24050;&#32463;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#21364;&#21482;&#23384;&#22312;&#23569;&#37327;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#20225;&#19994;&#27969;&#31243;&#30340;&#24212;&#29992;&#25110;&#26159;&#21482;&#38024;&#23545;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#30340;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20225;&#19994;&#27969;&#31243;&#36164;&#28304;&#20998;&#37197;&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#12290;&#22312;&#20195;&#34920;&#20856;&#22411;&#19994;&#21153;&#27969;&#31243;&#32467;&#26500;&#30340;&#19968;&#32452;&#24773;&#26223;&#20197;&#21450;&#22312;&#20195;&#34920;&#29616;&#23454;&#19994;&#21153;&#27969;&#31243;&#30340;&#23436;&#25972;&#32593;&#32476;&#19978;&#65292;&#23558;&#20004;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#20013;&#20248;&#20110;&#25110;&#19982;&#24120;&#35265;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource allocation is the assignment of resources to activities that must be executed in a business process at a particular moment at run-time. While resource allocation is well-studied in other fields, such as manufacturing, there exist only a few methods in business process management. Existing methods are not suited for application in large business processes or focus on optimizing resource allocation for a single case rather than for all cases combined. To fill this gap, this paper proposes two learning-based methods for resource allocation in business processes: a deep reinforcement learning-based approach and a score-based value function approximation approach. The two methods are compared against existing heuristics in a set of scenarios that represent typical business process structures and on a complete network that represents a realistic business process. The results show that our learning-based methods outperform or are competitive with common heuristics in most scenarios a
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#22833;&#35823;&#21450;&#20854;&#24212;&#29992;&#20110;&#26816;&#27979;Deepfakes&#65292;&#35782;&#21035;&#20102;&#20116;&#31181;&#23450;&#24615;&#32570;&#38519;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.06470</link><description>&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#24615;&#22833;&#36133;&#21450;&#20854;&#22312;&#26816;&#27979;Deepfakes&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes. (arXiv:2304.06470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06470
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#22833;&#35823;&#21450;&#20854;&#24212;&#29992;&#20110;&#26816;&#27979;Deepfakes&#65292;&#35782;&#21035;&#20102;&#20116;&#31181;&#23450;&#24615;&#32570;&#38519;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21019;&#36896;&#20986;&#36924;&#30495;&#30340;&#24433;&#20687;&#30340;&#33021;&#21147;&#24050;&#32463;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#24230;&#65292;&#36825;&#20351;&#24471;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24456;&#38590;&#21306;&#20998;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#20687;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#21644;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#65292;&#20197;&#30830;&#23450;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#24615;&#32570;&#38519;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20116;&#31867;&#12290;&#36890;&#36807;&#20102;&#35299;&#36825;&#20123;&#22833;&#36133;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;&#20170;&#22825;&#31038;&#20250;&#20013;Deepfakes&#30340;&#26222;&#36941;&#23384;&#22312;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#23427;&#20204;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of image and video generation models to create photorealistic images has reached unprecedented heights, making it difficult to distinguish between real and fake images in many cases. However, despite this progress, a gap remains between the quality of generated images and those found in the real world. To address this, we have reviewed a vast body of literature from both academic publications and social media to identify qualitative shortcomings in image generation models, which we have classified into five categories. By understanding these failures, we can identify areas where these models need improvement, as well as develop strategies for detecting deep fakes. The prevalence of deep fakes in today's society is a serious concern, and our findings can help mitigate their negative impact.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#19981;&#21463;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#20197;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.07846</link><description>&lt;p&gt;
&#39640;&#25928;&#29575;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-efficient Adversarial Imitation Learning. (arXiv:2303.07846v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#19981;&#21463;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#20197;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#21363;&#36890;&#36807;&#28436;&#31034;&#36827;&#34892;&#23398;&#20064;&#65292;&#24050;&#32463;&#34987;&#30740;&#31350;&#24182;&#24212;&#29992;&#20110;&#24207;&#36143;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#65292;&#22870;&#21169;&#20989;&#25968;&#24182;&#19981;&#26159;&#39044;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#23478;&#28436;&#31034;&#26679;&#26412;&#25165;&#33021;&#25104;&#21151;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#12290;&#20026;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#25968;&#25454;&#29983;&#25104;&#22823;&#37327;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#19981;&#21463;&#21508;&#31181;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#24182;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#29616;&#26377;&#30340;&#34920;&#26684;&#25968;&#25454;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#30340;&#19981;&#21516;&#25439;&#22351;&#26041;&#27861;&#65292;&#20197;&#20351;&#20854;&#33021;&#22815;&#25269;&#25239;&#21508;&#31181;&#25197;&#26354;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#35266;&#23519;&#34920;&#26126;&#65292;&#20351;&#19968;&#20010;&#20449;&#24687;&#37327;&#22823;&#30340;&#29305;&#24449;&#27969;&#24418;&#19982;&#19968;&#20010;&#31616;&#21333;&#30340;&#29983;&#25104;&#22120;&#19982;&#19968;&#20010;&#22797;&#26434;&#30340;&#20998;&#31867;&#22120;&#21327;&#21516;&#24037;&#20316;&#33021;&#22815;&#25552;&#39640;&#29366;&#24577;&#34920;&#24449;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning, in which learning is performed by demonstration, has been studied and advanced for sequential decision-making tasks in which a reward function is not predefined. However, imitation learning methods still require numerous expert demonstration samples to successfully imitate an expert's behavior. To improve sample efficiency, we utilize self-supervised representation learning, which can generate vast training signals from the given data. In this study, we propose a self-supervised representation-based adversarial imitation learning method to learn state and action representations that are robust to diverse distortions and temporally predictive, on non-image control tasks. In particular, in comparison with existing self-supervised learning methods for tabular data, we propose a different corruption method for state and action representations that is robust to diverse distortions. We theoretically and empirically observe that making an informative feature manifold with 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#39044;&#27979;&#33539;&#24335;&#65288;EMPIRE&#65289;&#65292;&#36890;&#36807;&#32454;&#21270;&#36793;&#32536;&#20351;&#29992;&#26041;&#27861;&#35299;&#20915;&#20102;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#21644;&#36793;&#32536;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#36793;&#32536;&#25286;&#20998;&#25216;&#26415;&#21644;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36793;&#32536;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2212.12970</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#32536;&#39044;&#27979;&#20013;&#32454;&#21270;&#36793;&#32536;&#20351;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Refined Edge Usage of Graph Neural Networks for Edge Prediction. (arXiv:2212.12970v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12970
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36793;&#32536;&#39044;&#27979;&#33539;&#24335;&#65288;EMPIRE&#65289;&#65292;&#36890;&#36807;&#32454;&#21270;&#36793;&#32536;&#20351;&#29992;&#26041;&#27861;&#35299;&#20915;&#20102;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#21644;&#36793;&#32536;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#36793;&#32536;&#25286;&#20998;&#25216;&#26415;&#21644;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36793;&#32536;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26368;&#21021;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#65292;&#20063;&#28608;&#21457;&#20102;&#35768;&#22810;&#20851;&#20110;&#36793;&#32536;&#39044;&#27979;&#65288;&#21363;&#38142;&#36335;&#39044;&#27979;&#65289;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#20851;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#21306;&#21035;&#26041;&#38754;&#32570;&#20047;&#31934;&#32454;&#30340;&#35774;&#35745;&#65292;&#36825;&#19968;&#28857;&#24120;&#24120;&#34987;&#24573;&#35270;&#65306;&#65288;i&#65289;&#23545;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#32780;&#35328;&#65292;&#36793;&#20165;&#26500;&#25104;&#25299;&#25169;&#32467;&#26500;&#65292;&#20294;&#22312;&#36793;&#32536;&#39044;&#27979;&#20219;&#21153;&#20013;&#26082;&#21487;&#20197;&#20316;&#20026;&#25299;&#25169;&#32467;&#26500;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65288;&#21363;&#26631;&#31614;&#65289;&#65307;&#65288;ii&#65289;&#33410;&#28857;&#20998;&#31867;&#26159;&#23545;&#27599;&#20010;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#36793;&#32536;&#39044;&#27979;&#21017;&#30001;&#27599;&#23545;&#33410;&#28857;&#20915;&#23450;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36793;&#32536;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;EMPIRE&#65289;&#30340;&#26032;&#22411;&#36793;&#32536;&#39044;&#27979;&#33539;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#31181;&#36793;&#32536;&#25286;&#20998;&#25216;&#26415;&#26469;&#25351;&#23450;&#27599;&#20010;&#36793;&#30340;&#20351;&#29992;&#26041;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#36793;&#20165;&#29992;&#20316;&#25299;&#25169;&#32467;&#26500;&#25110;&#30417;&#30563;&#20449;&#21495;&#65288;&#20998;&#21035;&#31216;&#20026;&#25299;&#25169;&#36793;&#25110;&#30417;&#30563;&#36793;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#29983;&#25104;&#28040;&#24687;&#30340;&#20256;&#36882;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs), originally proposed for node classification, have also motivated many recent works on edge prediction (a.k.a., link prediction). However, existing methods lack elaborate design regarding the distinctions between two tasks that have been frequently overlooked: (i) edges only constitute the topology in the node classification task but can be used as both the topology and the supervisions (i.e., labels) in the edge prediction task; (ii) the node classification makes prediction over each individual node, while the edge prediction is determinated by each pair of nodes. To this end, we propose a novel edge prediction paradigm named Edge-aware Message PassIng neuRal nEtworks (EMPIRE). Concretely, we first introduce an edge splitting technique to specify use of each edge where each edge is solely used as either the topology or the supervision (named as topology edge or supervision edge). We then develop a new message passing mechanism that generates the messages t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#21644;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#26469;&#34913;&#37327;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#12290;</title><link>http://arxiv.org/abs/2212.04631</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#20989;&#25968;&#65306;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Normalized Cross Density Functional: A Framework to Quantify Statistical Dependence for Random Processes. (arXiv:2212.04631v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#21644;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#26469;&#34913;&#37327;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38463;&#23572;&#24343;&#38647;&#24503;&#183;&#38647;&#23612;&#65288;Alfr\'ed R\'enyi&#65289;&#30340;&#21151;&#33021;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#36830;&#32493;&#38543;&#26426;&#36807;&#31243;&#65288;r.p.&#65289;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#26032;&#39062;&#30340;&#22810;&#21464;&#37327;&#23450;&#20041;&#12290;&#23558;&#38543;&#26426;&#36807;&#31243;&#26679;&#26412;&#23545;&#30340;&#20114;&#20449;&#24687;&#30340;&#23545;&#25968;&#35770;&#35777;&#21629;&#21517;&#20026;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#65288;NCD&#65289;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#23545;&#31216;&#21644;&#33258;&#20276;&#30340;&#27491;&#23450;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#65288;ACE&#65289;&#36882;&#24402;&#24212;&#29992;&#20110;&#36755;&#20837;&#26679;&#26412;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#65292;&#31526;&#21512;&#38647;&#23612;&#30340;&#26368;&#22823;&#30456;&#20851;&#24615;&#30340;&#25152;&#26377;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NCD&#30340;&#29305;&#24449;&#35889;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#21464;&#37327;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;r.p.&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290;&#21033;&#29992;r.p.&#30340;&#23454;&#29616;&#65292;&#20063;&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#12290;&#25552;&#20986;&#30340;&#21151;&#33021;&#26368;&#22823;&#30456;&#20851;&#31639;&#27861;&#65288;FMCA&#65289;&#24212;&#29992;&#20110;&#30001;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#19978;&#65292;&#36890;&#36807;&#36924;&#36817;&#32852;&#21512;&#35757;&#32451;&#26469;&#21516;&#26102;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel multivariate definition of statistical dependence between two continuous random processes (r.p.) using a functional methodology inspired by Alfr\'ed R\'enyi. The argument of the logarithm of mutual information between pairs of samples of a r.p., named here the normalized cross density (NCD), defines a symmetric and self-adjoint positive definite function. We show that maximizing the alternating covariance estimation (ACE) recursion, applied to each of the joint probability density of input sample pairs, obeys all the properties of Renyi's maximal correlation. We propose the NCD's eigenspectrum as a novel multivariate measure of the statistical dependence between the input and output r.p.  The multivariate statistical dependence can also be estimated directly from r.p. realizations. The proposed functional maximum correlation algorithm (FMCA) is applied to a machine learning architecture built from two neural networks that learn concurrently by approximating 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2208.04883</link><description>&lt;p&gt;
&#31070;&#32463;&#20250;&#21512;&#65306;&#38754;&#21521;&#26143;&#38469;&#29289;&#20307;&#30340;&#21487;&#38752;&#23548;&#33322;&#21644;&#25511;&#21046;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects. (arXiv:2208.04883v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26143;&#38469;&#29289;&#20307;&#65288;ISOs&#65289;&#24456;&#21487;&#33021;&#26159;&#19981;&#21487;&#26367;&#20195;&#30340;&#21407;&#22987;&#26448;&#26009;&#65292;&#22312;&#29702;&#35299;&#31995;&#22806;&#34892;&#26143;&#26143;&#31995;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36816;&#34892;&#36712;&#36947;&#38590;&#20197;&#32422;&#26463;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#39640;&#30340;&#20542;&#35282;&#21644;&#30456;&#23545;&#36895;&#24230;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;&#20154;&#22312;&#29615;&#36335;&#26041;&#27861;&#25506;&#32034;ISOs&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#20250;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#20013;&#20197;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#30340;&#26041;&#24335;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#29289;&#20307;&#65292;&#21253;&#25324;ISOs&#12290;&#23427;&#22312;&#22522;&#20110;&#35889;&#24402;&#19968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24341;&#23548;&#31574;&#30053;&#20043;&#19978;&#20351;&#29992;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#65292;&#20854;&#20013;&#21442;&#25968;&#36890;&#36807;&#30452;&#25509;&#24809;&#32602;MPC&#29366;&#24577;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35843;&#20248;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#20250;&#21512;&#22312;&#39044;&#26399;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#19978;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#65292;&#20854;&#35777;&#26126;&#21033;&#29992;&#20102;&#38543;&#26426;&#36882;&#22686;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interstellar objects (ISOs) are likely representatives of primitive materials invaluable in understanding exoplanetary star systems. Due to their poorly constrained orbits with generally high inclinations and relative velocities, however, exploring ISOs with conventional human-in-the-loop approaches is significantly challenging. This paper presents Neural-Rendezvous, a deep learning-based guidance and control framework for encountering fast-moving objects, including ISOs, robustly, accurately, and autonomously in real time. It uses pointwise minimum norm tracking control on top of a guidance policy modeled by a spectrally-normalized deep neural network, where its hyperparameters are tuned with a loss function directly penalizing the MPC state trajectory tracking error. We show that Neural-Rendezvous provides a high probability exponential bound on the expected spacecraft delivery error, the proof of which leverages stochastic incremental stability analysis. In particular, it is used to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;NC-GNN&#26694;&#26550;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.02059</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36793;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#22686;&#24378;GNN
&lt;/p&gt;
&lt;p&gt;
Empowering GNNs via Edge-Aware Weisfeiler-Lehman Algorithm. (arXiv:2206.02059v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#24863;&#30693;&#30340;Weisfeiler-Lehman&#31639;&#27861;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;NC-GNN&#26694;&#26550;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#34920;&#36798;&#33021;&#21147;&#34987;&#24050;&#30693;&#30340;&#19968;&#32500;Weisfeiler-Lehman (1-WL)&#31639;&#27861;&#19978;&#30028;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;GNN&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#35201;&#20040;&#38656;&#35201;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#35201;&#20040;&#28041;&#21450;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#25805;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#21487;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#34920;&#36798;&#21147;&#30340;GNN&#26694;&#26550;&#65292;&#20445;&#25345;&#20102;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#20043;&#38388;&#30340;&#36793;&#32536;&#26469;&#25480;&#26435;1-WL&#36827;&#34892;&#22270;&#21516;&#26500;&#27979;&#35797;&#65292;&#20174;&#32780;&#20135;&#29983;NC-1-WL&#12290; NC-1-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#22312;&#29702;&#35770;&#19978;&#34987;&#26174;&#31034;&#20026;&#20005;&#26684;&#39640;&#20110;1-WL&#19988;&#20302;&#20110;3-WL&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NC-GNN&#26694;&#26550;&#20316;&#20026;NC-1-WL&#30340;&#21487;&#21306;&#20998;&#31070;&#32463;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;NC-GNN&#23454;&#29616;&#21487;&#35777;&#26126;&#19982;NC-1-WL&#19968;&#26679;&#24378;&#22823;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;NC-GNN&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing graph neural networks (GNNs) are known to have their expressiveness upper-bounded by 1-dimensional Weisfeiler-Lehman (1-WL) algorithm. To achieve more powerful GNNs, existing attempts either require ad hoc features, or involve operations that incur high time and space complexities. In this work, we propose a general and provably powerful GNN framework that preserves the scalability of the message passing scheme. In particular, we first propose to empower 1-WL for graph isomorphism test by considering edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN framework as a differentiable neural version of NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN performs effectively and efficiently on various benchmarks.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13743</link><description>&lt;p&gt;
&#24102;&#26377;&#20559;&#22909;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#31639;&#27861;&#24178;&#39044;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Personalized Algorithmic Recourse with Preference Elicitation. (arXiv:2205.13743v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13743
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#24178;&#39044;&#65288;AR&#65289;&#30340;&#38382;&#39064;&#26159;&#35745;&#31639;&#29992;&#25143;&#25191;&#34892;&#19968;&#31995;&#21015;&#25805;&#20316;&#20197;&#39072;&#35206;&#19981;&#33391;&#26426;&#22120;&#20915;&#31574;&#30340;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#30340;&#25805;&#20316;&#24207;&#21015;&#19981;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#23454;&#26045;&#25552;&#20986;&#36807;&#39640;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;AR&#26041;&#27861;&#37117;&#20551;&#35774;&#25152;&#26377;&#29992;&#25143;&#30340;&#25805;&#20316;&#25104;&#26412;&#30456;&#21516;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#21521;&#26576;&#20123;&#29992;&#25143;&#25512;&#33616;&#26114;&#36149;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEAR&#65292;&#36825;&#26159;&#19968;&#31181;&#39318;&#20010;&#21487;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#65292;&#20197;&#28385;&#36275;&#20219;&#20309;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;PEAR&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#21521;&#30446;&#26631;&#29992;&#25143;&#21457;&#20986;&#36873;&#25321;&#38598;&#26597;&#35810;&#26469;&#36845;&#20195;&#22320;&#25913;&#21892;&#23545;&#25805;&#20316;&#25104;&#26412;&#30340;&#20272;&#35745;&#20540;&#12290;&#36825;&#20123;&#26597;&#35810;&#30340;&#35745;&#31639;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#26469;&#35745;&#31639;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#25104;&#26412;&#20272;&#35745;&#21644;&#29992;&#25143;&#21709;&#24212;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#12290;PEAR&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#23454;&#29616;AR&#20219;&#21153;&#25152;&#38656;&#36798;&#25104;&#30446;&#26631;&#30340;&#20559;&#22909;&#65292;&#20197;&#21450;&#25191;&#34892;&#27599;&#20010;&#25805;&#20316;&#25152;&#28041;&#21450;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;AR&#20219;&#21153;&#26469;&#35780;&#20272;PEAR&#65292;&#24182;&#26174;&#31034;&#20854;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#26356;&#20026;&#32463;&#27982;&#23454;&#29992;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Rein
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#36716;&#21464;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#65292;&#20294;&#20173;&#38754;&#20020;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26426;&#20250;&#20063;&#22312;&#31561;&#24453;&#30528;&#25366;&#25496;&#12290;</title><link>http://arxiv.org/abs/2203.13883</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65306;&#26041;&#27861;&#12289;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13883
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#36716;&#21464;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#65292;&#20294;&#20173;&#38754;&#20020;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26426;&#20250;&#20063;&#22312;&#31561;&#24453;&#30528;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20174;&#25991;&#26412;&#20026;&#20027;&#30340;&#35770;&#22363;&#36716;&#21521;&#22810;&#27169;&#24335;&#29615;&#22659;&#65292;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#34394;&#20551;&#20449;&#24687;&#30340;&#24615;&#36136;&#20063;&#30456;&#24212;&#21457;&#29983;&#20102;&#21464;&#21270;&#12290;&#21033;&#29992;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#35270;&#35273;&#27169;&#24577;&#26356;&#21463;&#29992;&#25143;&#38738;&#30544;&#21644;&#21560;&#24341;&#21147;&#30340;&#20107;&#23454;&#65292;&#20197;&#21450;&#25991;&#26412;&#20869;&#23481;&#26377;&#26102;&#34987;&#31895;&#30053;&#27983;&#35272;&#30340;&#24773;&#20917;&#65292;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#32773;&#26368;&#36817;&#24320;&#22987;&#38024;&#23545;&#27169;&#24577;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#36830;&#25509;&#65292;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#33258;&#21160;&#26816;&#27979;&#32593;&#39029;&#20869;&#23481;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36328;&#27169;&#24577;&#19981;&#21327;&#35843;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#20998;&#26512;&#12289;&#20998;&#31867;&#21644;&#35782;&#21035;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#23427;&#20204;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#20197;&#25581;&#31034;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#39046;&#22495;&#30340;&#26032;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media platforms are evolving from text-based forums into multi-modal environments, the nature of misinformation in social media is also transforming accordingly. Taking advantage of the fact that visual modalities such as images and videos are more favorable and attractive to the users and textual contents are sometimes skimmed carelessly, misinformation spreaders have recently targeted contextual connections between the modalities e.g., text and image. Hence many researchers have developed automatic techniques for detecting possible cross-modal discordance in web-based content. We analyze, categorize and identify existing approaches in addition to challenges and shortcomings they face in order to unearth new research opportunities in the field of multi-modal misinformation detection.
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#35843;&#26597;&#30740;&#31350;&#25506;&#35752;&#20102;&#31163;&#32676;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#19982;&#24322;&#24120;&#26816;&#27979;&#12289;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#38382;&#39064;&#30340;&#32852;&#31995;&#65292;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#20851;&#38190;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2110.11334</link><description>&lt;p&gt;
&#24191;&#20041;&#30340;&#31163;&#32676;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalized Out-of-Distribution Detection: A Survey. (arXiv:2110.11334v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11334
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#35843;&#26597;&#30740;&#31350;&#25506;&#35752;&#20102;&#31163;&#32676;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#19982;&#24322;&#24120;&#26816;&#27979;&#12289;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#38382;&#39064;&#30340;&#32852;&#31995;&#65292;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#20851;&#38190;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32676;&#26816;&#27979;&#23545;&#20110;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#24403;&#31995;&#32479;&#26816;&#27979;&#21040;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#19988;&#26080;&#27861;&#20316;&#20986;&#23433;&#20840;&#20915;&#31574;&#30340;&#24322;&#24120;&#22330;&#26223;&#25110;&#23545;&#35937;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#39550;&#39542;&#31995;&#32479;&#33021;&#21457;&#20986;&#35686;&#25253;&#24182;&#23558;&#25511;&#21046;&#20132;&#32473;&#20154;&#31867;&#12290;&#31163;&#32676;&#26816;&#27979;&#19968;&#35789;&#39318;&#27425;&#20986;&#29616;&#22312;2017&#24180;&#65292;&#33258;&#37027;&#20197;&#21518;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20174;&#22522;&#20110;&#20998;&#31867;&#30340;&#26041;&#27861;&#21040;&#22522;&#20110;&#23494;&#24230;&#21644;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#26041;&#27861;&#20116;&#33457;&#20843;&#38376;&#12290;&#21516;&#26102;&#65292;&#31163;&#32676;&#26816;&#27979;&#19982;&#24322;&#24120;&#26816;&#27979;&#12289;&#26032;&#39062;&#24615;&#26816;&#27979;&#12289;&#24320;&#25918;&#38598;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20854;&#20182;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#12290;&#23613;&#31649;&#30446;&#26631;&#30456;&#21516;&#65292;&#20294;&#36825;&#20123;&#20027;&#39064;&#22312;&#23450;&#20041;&#21644;&#38382;&#39064;&#35774;&#32622;&#19978;&#26377;&#24494;&#22937;&#30340;&#24046;&#24322;&#65292;&#32463;&#24120;&#20351;&#35835;&#32773;&#21644;&#20174;&#19994;&#32773;&#24863;&#21040;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;&#26465;&#20214;&#20559;&#22909;&#35821;&#21477;&#30340;&#35821;&#35328;&#30340;&#30693;&#35782;&#32534;&#35793;&#22270;&#65292;&#30740;&#31350;&#20102;&#26597;&#35810;&#21644;&#36716;&#25442;&#30340;&#21508;&#31181;&#22797;&#26434;&#24615;&#65292;&#24182;&#22686;&#21152;&#20102;&#26032;&#30340;&#26597;&#35810;&#21644;&#36716;&#25442;&#30340;&#30740;&#31350;&#65292;&#23545;&#20110;&#24191;&#20041;&#21487;&#21152;&#24615;&#25928;&#29992;&#20063;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2102.04107</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#20559;&#22909;&#35821;&#21477;&#21644;&#24191;&#20041;&#21487;&#21152;&#24615;&#25928;&#29992;&#35821;&#35328;&#30340;&#25193;&#23637;&#30693;&#35782;&#32534;&#35793;&#22270;
&lt;/p&gt;
&lt;p&gt;
An extended Knowledge Compilation Map for Conditional Preference Statements-based and Generalized Additive Utilities-based Languages. (arXiv:2102.04107v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.04107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;&#26465;&#20214;&#20559;&#22909;&#35821;&#21477;&#30340;&#35821;&#35328;&#30340;&#30693;&#35782;&#32534;&#35793;&#22270;&#65292;&#30740;&#31350;&#20102;&#26597;&#35810;&#21644;&#36716;&#25442;&#30340;&#21508;&#31181;&#22797;&#26434;&#24615;&#65292;&#24182;&#22686;&#21152;&#20102;&#26032;&#30340;&#26597;&#35810;&#21644;&#36716;&#25442;&#30340;&#30740;&#31350;&#65292;&#23545;&#20110;&#24191;&#20041;&#21487;&#21152;&#24615;&#25928;&#29992;&#20063;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#20559;&#22909;&#35821;&#21477;&#34987;&#29992;&#26469;&#32039;&#20945;&#22320;&#34920;&#31034;&#32452;&#21512;&#39046;&#22495;&#20013;&#30340;&#20559;&#22909;&#12290;&#23427;&#20204;&#26159;CP-&#32593;&#21644;&#20854;&#19968;&#33324;&#21270;&#29256;&#26412;&#20197;&#21450;&#23383;&#20856;&#25490;&#24207;&#20559;&#22909;&#26641;&#30340;&#26680;&#24515;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#19968;&#20123;&#26597;&#35810;&#65288;&#29305;&#21035;&#26159;&#20248;&#21270;&#21644;&#25903;&#37197;&#24615;&#65289;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25193;&#23637;&#20102;&#20854;&#20013;&#19968;&#37096;&#20998;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#20102;&#36804;&#20170;&#23578;&#26410;&#35299;&#20915;&#30340;&#20854;&#20182;&#26597;&#35810;&#65292;&#22914;&#31561;&#20215;&#24615;&#65292;&#20197;&#21450;&#22914;&#26465;&#20214;&#21644;&#21464;&#37327;&#28040;&#38500;&#31561;&#36716;&#25442;&#65292;&#20174;&#32780;&#20026;&#22522;&#20110;&#26465;&#20214;&#20559;&#22909;&#35821;&#21477;&#30340;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#30693;&#35782;&#32534;&#35793;&#22270;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#24191;&#20041;&#21487;&#21152;&#24615;&#25928;&#29992;&#30340;&#26597;&#35810;&#21644;&#36716;&#25442;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional preference statements have been used to compactly represent preferences over combinatorial domains. They are at the core of CP-nets and their generalizations, and lexicographic preference trees. Several works have addressed the complexity of some queries (optimization, dominance in particular). We extend in this paper some of these results, and study other queries which have not been addressed so far, like equivalence, and transformations, like conditioning and variable elimination, thereby contributing to a knowledge compilation map for languages based on conditional preference statements. We also study the expressiveness and complexity of queries and transformations for generalized additive utilities.
&lt;/p&gt;</description></item></channel></rss>