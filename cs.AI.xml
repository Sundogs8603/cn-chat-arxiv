<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;Python&#24211;Frame Semantic Transformer&#65292;&#35813;&#24211;&#29992;&#20110;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#22312;&#26131;&#29992;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#27700;&#24179;&#65292;&#21487;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;FrameNet&#35789;&#27719;&#21333;&#20803;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12788</link><description>&lt;p&gt;
&#24320;&#28304;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Open-source Frame Semantic Parsing. (arXiv:2303.12788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;Python&#24211;Frame Semantic Transformer&#65292;&#35813;&#24211;&#29992;&#20110;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#22312;&#26131;&#29992;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#27700;&#24179;&#65292;&#21487;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;FrameNet&#35789;&#27719;&#21333;&#20803;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#20960;&#24180;&#26469;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#30340;&#26368;&#26032;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#32456;&#31471;&#29992;&#25143;&#26469;&#35828;&#65292;&#23558;&#26368;&#26032;&#27169;&#22411;&#24212;&#29992;&#20110;&#23454;&#36341;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Frame Semantic Transformer&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#21487;&#20197;&#22312;&#20851;&#27880;&#26131;&#29992;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;FrameNet 1.7&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22312;Propbank&#21644;FrameNet&#31034;&#20363;&#19978;&#24494;&#35843;&#30340;T5&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;FrameNet&#35789;&#27719;&#21333;&#20803;&#20026;T5&#25552;&#20379;&#25552;&#31034;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the state-of-the-art for frame semantic parsing has progressed dramatically in recent years, it is still difficult for end-users to apply state-of-the-art models in practice. To address this, we present Frame Semantic Transformer, an open-source Python library which achieves near state-of-the-art performance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model fine-tuned on Propbank and FrameNet exemplars as a base, and improve performance by using FrameNet lexical units to provide hints to T5 at inference time. We enhance robustness to real-world data by using textual data augmentations during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12785</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;&#65306;&#25910;&#25947;&#24615;&#19982;&#20840;&#23616;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality. (arXiv:2303.12785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#20195;&#29702;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#38500;&#20102;&#32047;&#35745;&#22870;&#21169;&#22806;&#30340;&#29109;&#22870;&#21169;&#12290;MPG&#19982;&#26631;&#20934;PG&#30340;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#23427;&#35757;&#32451;&#19968;&#31995;&#21015;&#31574;&#30053;&#21516;&#26102;&#23398;&#20064;&#26377;&#38480;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#21333;&#19968;&#30340;&#26631;&#20934;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31574;&#30053;&#12290;&#23545;&#20110;softmax&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MPG&#30340;&#25910;&#25947;&#24615;&#21644;&#26497;&#38480;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#36890;&#36807;&#35777;&#26126;MPG&#30446;&#26631;&#30340;&#21807;&#19968;&#20020;&#30028;&#28857;&#26159;&#26368;&#20248;&#31574;&#30053;&#65307;&#21363;&#20351;&#22312;&#36830;&#32493;&#32039;&#33268;&#29366;&#24577;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#32467;&#26524;&#20173;&#28982;&#25104;&#31435;&#12290;MPG&#30452;&#35266;&#12289;&#29702;&#35770;&#19978;Sound&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26631;&#20934;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#26368;&#20248;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;MPG&#26694;&#26550;&#30340;&#26368;&#20248;&#31574;&#30053;&#36827;&#34892;&#20219;&#24847;&#31934;&#24230;&#30340;&#36924;&#36817;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#31574;&#30053;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;MPG&#38750;&#24120;&#36866;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel Policy Gradient (PG) algorithm, called Matryoshka Policy Gradient (MPG), is introduced and studied, in the context of max-entropy reinforcement learning, where an agent aims at maximising entropy bonuses additional to its cumulative rewards. MPG differs from standard PG in that it trains a sequence of policies to learn finite horizon tasks simultaneously, instead of a single policy for the single standard objective. For softmax policies, we prove convergence of MPG and global optimality of the limit by showing that the only critical point of the MPG objective is the optimal policy; these results hold true even in the case of continuous compact state space. MPG is intuitive, theoretically sound and we furthermore show that the optimal policy of the standard max-entropy objective can be approximated arbitrarily well by the optimal policy of the MPG framework. Finally, we justify that MPG is well suited when the policies are parametrized with neural networks and we provide an simp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HopCPT &#30340;&#26032;&#19968;&#33268;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#20854;&#20248;&#21183;&#65292;&#24050;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12783</link><description>&lt;p&gt;
&#22522;&#20110;&#29616;&#20195; Hopfield &#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction for Time Series with Modern Hopfield Networks. (arXiv:2303.12783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HopCPT &#30340;&#26032;&#19968;&#33268;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#20854;&#20248;&#21183;&#65292;&#24050;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#33258;&#30456;&#20851;&#32467;&#26500;&#36829;&#21453;&#20102;&#19968;&#33268;&#24615;&#39044;&#27979;&#25152;&#38656;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; HopCPT&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110; Hopfield &#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#22312;&#29702;&#35770;&#19978;&#26159;&#26377;&#24456;&#22909;&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To quantify uncertainty, conformal prediction methods are gaining continuously more interest and have already been successfully applied to various domains. However, they are difficult to apply to time series as the autocorrelative structure of time series violates basic assumptions required by conformal prediction. We propose HopCPT, a novel conformal prediction approach for time series that not only copes with temporal structures but leverages them. We show that our approach is theoretically well justified for time series where temporal dependencies are present. In experiments, we demonstrate that our new approach outperforms state-of-the-art conformal prediction methods on multiple real-world time series datasets from four different domains.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#21521;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30446;&#26631;&#36808;&#36827;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Datalog&#26597;&#35810;&#21450;&#20854;&#23376;&#31867;&#30340;why-provenance&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#37322;&#26597;&#35810;&#32467;&#26524;&#24182;&#25552;&#20379;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#36882;&#24402;&#34987;&#38480;&#21046;&#65292;&#22312;&#36882;&#24402;&#26597;&#35810;&#20013;&#23454;&#29616;why-provenance&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20063;&#38750;&#24120;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2303.12773</link><description>&lt;p&gt;
Datalog&#26597;&#35810;&#30340;Why-Provenance&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Complexity of Why-Provenance for Datalog Queries. (arXiv:2303.12773v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12773
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21521;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30446;&#26631;&#36808;&#36827;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Datalog&#26597;&#35810;&#21450;&#20854;&#23376;&#31867;&#30340;why-provenance&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#37322;&#26597;&#35810;&#32467;&#26524;&#24182;&#25552;&#20379;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#36882;&#24402;&#34987;&#38480;&#21046;&#65292;&#22312;&#36882;&#24402;&#26597;&#35810;&#20013;&#23454;&#29616;why-provenance&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20063;&#38750;&#24120;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#25968;&#25454;&#24211;&#26597;&#35810;&#32467;&#26524;&#30340;&#21407;&#22240;&#26159;&#23454;&#29616;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#22312;&#34920;&#36798;&#20016;&#23500;&#30340;&#26597;&#35810;&#35821;&#35328;&#65288;&#22914;Datalog&#65289;&#22312;&#26412;&#20307;&#35770;&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#20013;&#21457;&#25381;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#30340;&#24403;&#19979;&#12290;&#35299;&#37322;&#26597;&#35810;&#32467;&#26524;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#25152;&#35859;&#30340;why-provenance&#65292;&#23427;&#20197;&#36275;&#20197;&#25512;&#23548;&#35813;&#32467;&#26524;&#30340;&#36755;&#20837;&#25968;&#25454;&#24211;&#30340;&#23376;&#38598;&#30340;&#24418;&#24335;&#25552;&#20379;&#26377;&#20851;&#26597;&#35810;&#32467;&#26524;&#30340;&#35777;&#25454;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;&#23545;&#20110;Datalog&#26597;&#35810;&#30340;why-provenance&#30340;&#27010;&#24565;&#24050;&#23384;&#22312;&#25968;&#21313;&#24180;&#24182;&#21463;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#35745;&#31639;&#22797;&#26434;&#24615;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#22635;&#34917;why-provenance&#25991;&#29486;&#20013;&#36825;&#20010;&#26126;&#26174;&#30340;&#31354;&#30333;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;Datalog&#26597;&#35810;&#21450;&#20854;&#20851;&#38190;&#23376;&#31867;&#30340;why-provenance&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#36882;&#24402;&#34987;&#38480;&#21046;&#20026;&#38750;&#24120;&#29305;&#27530;&#30340;&#24773;&#20917;&#65292;&#23545;&#20110;&#36882;&#24402;&#26597;&#35810;&#30340;Why-Provenance&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21516;&#26679;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining why a database query result is obtained is an essential task towards the goal of Explainable AI, especially nowadays where expressive database query languages such as Datalog play a critical role in the development of ontology-based applications. A standard way of explaining a query result is the so-called why-provenance, which essentially provides information about the witnesses to a query result in the form of subsets of the input database that are sufficient to derive that result. To our surprise, despite the fact that the notion of why-provenance for Datalog queries has been around for decades and intensively studied, its computational complexity remains unexplored. The goal of this work is to fill this apparent gap in the why-provenance literature. Towards this end, we pinpoint the data complexity of why-provenance for Datalog queries and key subclasses thereof. The takeaway of our work is that why-provenance for recursive queries, even if the recursion is limited to be
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#20919;&#22066;&#28909;&#35773;&#26816;&#27979;&#31995;&#32479;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#20919;&#22066;&#28909;&#35773;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#31561;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.12772</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21644;&#21487;&#35299;&#37322;AI&#30340;&#21487;&#35299;&#37322;&#23391;&#21152;&#25289;&#20919;&#22066;&#28909;&#35773;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Bangla Sarcasm Detection using BERT and Explainable AI. (arXiv:2303.12772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12772
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#20919;&#22066;&#28909;&#35773;&#26816;&#27979;&#31995;&#32479;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#20919;&#22066;&#28909;&#35773;&#65292;&#23545;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#31561;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#38754;&#30340;&#35805;&#25110;&#20276;&#38543;&#36127;&#21521;&#21160;&#26426;&#30340;&#35821;&#21477;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#20919;&#22066;&#28909;&#35773;&#65292;&#32780;&#22312;&#24403;&#20170;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22914;Facebook&#12289;Twitter&#12289;Reddit&#31561;&#19978;&#24191;&#27867;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#27963;&#36291;&#29992;&#25143;&#25968;&#37327;&#21576;&#29616;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#36825;&#22686;&#24378;&#20102;&#38656;&#35201;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#23436;&#25104;&#22810;&#39033;&#20219;&#21153;&#65292;&#22914;&#30830;&#23450;&#24066;&#22330;&#38656;&#27714;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#23041;&#32961;&#26816;&#27979;&#31561;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20919;&#22066;&#28909;&#35773;&#36890;&#24120;&#24847;&#21619;&#30528;&#30456;&#21453;&#30340;&#24847;&#24605;&#65292;&#20854;&#26816;&#27979;&#32463;&#24120;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#27492;&#36890;&#36807;NLP&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#24847;&#20041;&#25552;&#21462;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#38024;&#23545;&#33521;&#35821;&#20013;&#30340;&#20919;&#22066;&#28909;&#35773;&#26816;&#27979;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#65292;&#20294;&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#20919;&#22066;&#28909;&#35773;&#26816;&#27979;&#22987;&#32456;&#27809;&#26377;&#25913;&#21892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#22312;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26102;&#21482;&#33021;&#36798;&#21040;99.60\%&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#20919;&#22066;&#28909;&#35773;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
A positive phrase or a sentence with an underlying negative motive is usually defined as sarcasm that is widely used in today's social media platforms such as Facebook, Twitter, Reddit, etc. In recent times active users in social media platforms are increasing dramatically which raises the need for an automated NLP-based system that can be utilized in various tasks such as determining market demand, sentiment analysis, threat detection, etc. However, since sarcasm usually implies the opposite meaning and its detection is frequently a challenging issue, data meaning extraction through an NLP-based model becomes more complicated. As a result, there has been a lot of study on sarcasm detection in English over the past several years, and there's been a noticeable improvement and yet sarcasm detection in the Bangla language's state remains the same. In this article, we present a BERT-based system that can achieve 99.60\% while the utilized traditional machine learning algorithms are only ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;ChatGPT&#35780;&#20272;&#20013;&#38754;&#20020;&#30340;&#25968;&#25454;&#27745;&#26579;&#25361;&#25112;&#65292;&#36890;&#36807;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#38416;&#36848;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#19988;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#30830;&#20445;&#27169;&#22411;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12767</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#30456;&#20449;ChatGPT&#30340;&#35780;&#20272;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we trust the evaluation on ChatGPT?. (arXiv:2303.12767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;ChatGPT&#35780;&#20272;&#20013;&#38754;&#20020;&#30340;&#25968;&#25454;&#27745;&#26579;&#25361;&#25112;&#65292;&#36890;&#36807;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#38416;&#36848;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#19988;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#30830;&#20445;&#27169;&#22411;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#31532;&#19968;&#20010;&#34987;&#24191;&#27867;&#37319;&#32435;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#22810;&#39033;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#38381;&#21512;&#24615;&#20197;&#21450;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#19981;&#26029;&#26356;&#26032;&#65292;&#35780;&#20272;ChatGPT&#22312;&#19981;&#21516;&#38382;&#39064;&#39046;&#22495;&#30340;&#34920;&#29616;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#22312;ChatGPT&#30340;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#20316;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#21644;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#36991;&#20813;&#25968;&#25454;&#27745;&#26579;&#21644;&#30830;&#20445;&#20844;&#24179;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study of the task of stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SphereFormer&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;LiDAR&#28857;&#20113;&#25968;&#25454;&#30340;&#19981;&#21516;&#31232;&#30095;&#24230;&#65292;&#36890;&#36807;&#35774;&#35745;&#24452;&#21521;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#25351;&#25968;&#20998;&#35010;&#31561;&#25163;&#27573;&#65292;&#21487;&#20197;&#32858;&#21512;&#21644;&#22788;&#29702;&#31232;&#30095;&#30340;&#36828;&#36317;&#31163;&#28857;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.12766</link><description>&lt;p&gt;
&#22522;&#20110;&#29699;&#38754;&#21464;&#25442;&#30340;LiDAR&#19977;&#32500;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Spherical Transformer for LiDAR-based 3D Recognition. (arXiv:2303.12766v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SphereFormer&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;LiDAR&#28857;&#20113;&#25968;&#25454;&#30340;&#19981;&#21516;&#31232;&#30095;&#24230;&#65292;&#36890;&#36807;&#35774;&#35745;&#24452;&#21521;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#25351;&#25968;&#20998;&#35010;&#31561;&#25163;&#27573;&#65292;&#21487;&#20197;&#32858;&#21512;&#21644;&#22788;&#29702;&#31232;&#30095;&#30340;&#36828;&#36317;&#31163;&#28857;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;LiDAR&#28857;&#20113;&#30340;&#19977;&#32500;&#35782;&#21035;&#24050;&#32463;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#24182;&#27809;&#26377;&#19987;&#38376;&#32771;&#34385;LiDAR&#28857;&#20998;&#24067;&#30340;&#24773;&#20917;&#65292;&#22240;&#27492;&#23545;&#20110;&#31232;&#30095;&#30340;&#36828;&#36317;&#31163;&#28857;&#32463;&#24120;&#38754;&#20020;&#20449;&#24687;&#19981;&#36830;&#36890;&#21644;&#26377;&#38480;&#24863;&#21463;&#37326;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LiDAR&#28857;&#20998;&#24067;&#30340;&#19981;&#21516;&#31232;&#30095;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;SphereFormer&#65292;&#21487;&#30452;&#25509;&#20174;&#23494;&#38598;&#30340;&#36817;&#36317;&#31163;&#28857;&#32858;&#21512;&#20449;&#24687;&#21644;&#31232;&#30095;&#30340;&#36828;&#36317;&#31163;&#28857;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#24452;&#21521;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;&#31354;&#38388;&#21010;&#20998;&#20026;&#22810;&#20010;&#38750;&#37325;&#21472;&#31364;&#38271;&#31383;&#21475;&#12290;&#23427;&#21487;&#20197;&#20811;&#26381;&#19981;&#36830;&#36890;&#38382;&#39064;&#65292;&#24179;&#28369;&#19988;&#26174;&#30528;&#22320;&#25193;&#22823;&#24863;&#21463;&#37326;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#31232;&#30095;&#36828;&#36317;&#31163;&#28857;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36866;&#24212;&#31364;&#38271;&#31383;&#21475;&#65292;&#25105;&#20204;&#25552;&#20986;&#25351;&#25968;&#20998;&#35010;&#26041;&#27861;&#20197;&#24471;&#21040;&#32454;&#31890;&#24230;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#20197;&#25552;&#39640;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;nuScenes&#21644;SemanticKITTI&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#22343;&#21517;&#21015;&#21069;&#33541;&#12290;
&lt;/p&gt;
&lt;p&gt;
LiDAR-based 3D point cloud recognition has benefited various applications. Without specially considering the LiDAR point distribution, most current methods suffer from information disconnection and limited receptive field, especially for the sparse distant points. In this work, we study the varying-sparsity distribution of LiDAR points and present SphereFormer to directly aggregate information from dense close points to the sparse distant ones. We design radial window self-attention that partitions the space into multiple non-overlapping narrow and long windows. It overcomes the disconnection issue and enlarges the receptive field smoothly and dramatically, which significantly boosts the performance of sparse distant points. Moreover, to fit the narrow and long windows, we propose exponential splitting to yield fine-grained position encoding and dynamic feature selection to increase model representation ability. Notably, our method ranks 1st on both nuScenes and SemanticKITTI semantic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32593;&#32476;&#32467;&#21512;&#30340;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807; LoRA &#35757;&#32451;&#26041;&#27861;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#21644; ControlNet &#27169;&#22411;&#30340;&#28155;&#21152;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20026;&#21518;&#32493;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.12755</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;: &#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#24314;&#31569;&#31435;&#38754;&#35774;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Text Semantics to Image Generation: A method of building facades design base on Stable Diffusion model. (arXiv:2303.12755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32593;&#32476;&#32467;&#21512;&#30340;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807; LoRA &#35757;&#32451;&#26041;&#27861;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#21644; ControlNet &#27169;&#22411;&#30340;&#28155;&#21152;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20026;&#21518;&#32493;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#20013;&#65292;&#20294;&#30446;&#21069;&#20173;&#26377;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#20869;&#23481;&#21487;&#25511;&#24615;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32593;&#32476;&#32467;&#21512;&#30340;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807; LoRA&#65288;&#20302;&#31209;&#33258;&#36866;&#24212;&#65289;&#26041;&#27861;&#22312; CMP Fa-cades &#25968;&#25454;&#38598;&#19978;&#23545;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#28982;&#21518;&#24212;&#29992; ControlNet &#27169;&#22411;&#36827;&#19968;&#27493;&#25511;&#21046;&#36755;&#20986;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#24314;&#31569;&#39118;&#26684;&#25991;&#26412;&#20869;&#23481;&#21644;&#25511;&#21046;&#31574;&#30053;&#19979;&#30340;&#31435;&#38754;&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LoRA &#35757;&#32451;&#26041;&#27861;&#26174;&#30528;&#38477;&#20302;&#20102;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;&#22823;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#32780; ControlNet &#27169;&#22411;&#30340;&#28155;&#21152;&#22686;&#21152;&#20102;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#20026;&#21518;&#32493;&#20851;&#20110;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable Diffusion model has been extensively employed in the study of archi-tectural image generation, but there is still an opportunity to enhance in terms of the controllability of the generated image content. A multi-network combined text-to-building facade image generating method is proposed in this work. We first fine-tuned the Stable Diffusion model on the CMP Fa-cades dataset using the LoRA (Low-Rank Adaptation) approach, then we ap-ply the ControlNet model to further control the output. Finally, we contrast-ed the facade generating outcomes under various architectural style text con-tents and control strategies. The results demonstrate that the LoRA training approach significantly decreases the possibility of fine-tuning the Stable Dif-fusion large model, and the addition of the ControlNet model increases the controllability of the creation of text to building facade images. This pro-vides a foundation for subsequent studies on the generation of architectural images.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;DOLOS&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#26368;&#22823;&#30340;&#28216;&#25103;&#33410;&#30446;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1,675&#20010;&#35270;&#39057;&#29255;&#27573;&#21644;&#20016;&#23500;&#30340;&#27450;&#39575;&#23545;&#35805;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;PECL&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.12745</link><description>&lt;p&gt;
&#38899;&#35270;&#39057;&#27450;&#39575;&#26816;&#27979;&#65306;DOLOS&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#39640;&#25928;&#36328;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Deception Detection: DOLOS Dataset and Parameter-Efficient Crossmodal Learning. (arXiv:2303.12745v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;DOLOS&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#26368;&#22823;&#30340;&#28216;&#25103;&#33410;&#30446;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1,675&#20010;&#35270;&#39057;&#29255;&#27573;&#21644;&#20016;&#23500;&#30340;&#27450;&#39575;&#23545;&#35805;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;PECL&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#27450;&#35784;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22312;&#21830;&#19994;&#30340;&#21487;&#20449;&#24230;&#35780;&#20272;&#12289;&#22810;&#23186;&#20307;&#38450;&#27450;&#35784;&#21644;&#23450;&#21046;&#23433;&#20840;&#31561;&#35768;&#22810;&#39046;&#22495;&#37117;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#27450;&#35784;&#25968;&#25454;&#38598;&#20197;&#21450;&#23398;&#20064;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#22256;&#38590;&#65292;&#27450;&#35784;&#26816;&#27979;&#30740;&#31350;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DOLOS&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#21253;&#21547;&#20016;&#23500;&#30340;&#27450;&#39575;&#23545;&#35805;&#30340;&#26368;&#22823;&#28216;&#25103;&#33410;&#30446;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;DOLOS&#21253;&#25324;1,675&#20010;&#35270;&#39057;&#29255;&#27573;&#65292;&#28041;&#21450;213&#20010;&#34987;&#35797;&#32773;&#65292;&#24182;&#19988;&#24050;&#32463;&#29992;&#38899;&#35270;&#39057;&#29305;&#24449;&#27880;&#37322;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35757;&#32451;-&#27979;&#35797;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#24615;&#21035;&#21327;&#35758;&#26469;&#35843;&#26597;&#19981;&#21516;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20808;&#21069;&#25552;&#20986;&#30340;&#27450;&#39575;&#26816;&#27979;&#26041;&#27861;&#19978;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#36890;&#36807;&#24494;&#35843;&#26356;&#23569;&#30340;&#21442;&#25968;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#36328;&#27169;&#24577;&#23398;&#20064;&#65288;PECL&#65289;&#65292;&#20854;&#20013;&#32479;&#19968;&#26102;&#38388;&#36866;&#37197;&#22120;&#65288;UT-Adapter&#65289;&#25506;&#32034;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Deception detection in conversations is a challenging yet important task, having pivotal applications in many fields such as credibility assessment in business, multimedia anti-frauds, and custom security. Despite this, deception detection research is hindered by the lack of high-quality deception datasets, as well as the difficulties of learning multimodal features effectively. To address this issue, we introduce DOLOS, the largest gameshow deception detection dataset with rich deceptive conversations. DOLOS includes 1,675 video clips featuring 213 subjects, and it has been labeled with audio-visual feature annotations. We provide train-test, duration, and gender protocols to investigate the impact of different factors. We benchmark our dataset on previously proposed deception detection approaches. To further improve the performance by fine-tuning fewer parameters, we propose Parameter-Efficient Crossmodal Learning (PECL), where a Uniform Temporal Adapter (UT-Adapter) explores tempora
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;</title><link>http://arxiv.org/abs/2303.12743</link><description>&lt;p&gt;
DR.CPO&#65306;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#12289;&#38543;&#26426;&#25918;&#32622;&#21644; HPR &#36974;&#34109;&#23454;&#29616;&#30340;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#19977;&#32500;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#24120;&#29992;&#20110;&#25913;&#36827;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#26368;&#22522;&#26412;&#30340;&#26041;&#27861;&#21253;&#25324;&#25554;&#20837;&#22797;&#21046;&#23545;&#35937;&#21644;&#26059;&#36716;&#21644;&#32553;&#25918;&#25972;&#20010;&#35757;&#32451;&#24103;&#12290;&#20063;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#21464;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21487;&#33021;&#24615;&#30456;&#27604;&#30456;&#24403;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#26500;&#36896;&#25972;&#20307;&#23545;&#35937;&#65292;&#33258;&#30001;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#20026;&#20102;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20174;&#29616;&#23454;&#19990;&#30028;&#35266;&#23519;&#21040;&#30340;&#22810;&#20010;&#23545;&#35937;&#38543;&#26426;&#32452;&#21512;&#25104;&#21333;&#20010;&#23545;&#35937;&#12290;&#19982;&#29616;&#26377;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#22312;&#35757;&#32451;&#24103;&#20013;&#65292;&#22240;&#20026;&#36866;&#24403;&#30340;&#36974;&#25377;&#21487;&#20197;&#21453;&#26144;&#22312;&#26368;&#32456;&#25972;&#20307;&#23545;&#35937;&#20013;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#38450;&#27490;&#36807;&#24230;&#22686;&#24378;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#23618;&#36974;&#25377;&#27010;&#29575;&#35774;&#32622;&#65292;&#36890;&#36807;&#23545;&#35937;&#30340;&#20301;&#32622;&#21644;&#22823;&#23567;&#35843;&#25972;&#36974;&#25377;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28508;&#31354;&#38388;&#25805;&#20316;&#20248;&#21270;CAD&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;StyleCLIP&#26469;&#36866;&#29992;&#20110;&#20307;&#32032;&#27169;&#22411;&#24418;&#24335;&#30340;CAD&#27169;&#22411;&#65292;&#33021;&#22815;&#20248;&#21270;&#23454;&#38469;CAD&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.12739</link><description>&lt;p&gt;
&#37319;&#29992;&#28508;&#31354;&#38388;&#25805;&#20316;&#26469;&#20248;&#21270;CAD&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Optimizing CAD Models with Latent Space Manipulation. (arXiv:2303.12739v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28508;&#31354;&#38388;&#25805;&#20316;&#20248;&#21270;CAD&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;StyleCLIP&#26469;&#36866;&#29992;&#20110;&#20307;&#32032;&#27169;&#22411;&#24418;&#24335;&#30340;CAD&#27169;&#22411;&#65292;&#33021;&#22815;&#20248;&#21270;&#23454;&#38469;CAD&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#28041;&#21450;&#21040;&#33258;&#21160;&#21270;&#39046;&#22495;&#20013;CAD&#27169;&#22411;&#30340;&#20248;&#21270;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30446;&#21069;&#21482;&#36215;&#21040;&#20102;&#36739;&#23567;&#30340;&#20316;&#29992;&#12290;&#20248;&#21270;&#25277;&#35937;&#29305;&#24615;&#22914;&#33258;&#21160;&#21270;&#33021;&#21147;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#34987;&#27169;&#25311;&#65292;&#23545;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#65292;&#32780;&#19988;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32570;&#20047;&#25968;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20687;StyleCLIP&#36825;&#26679;&#30340;&#21487;&#25805;&#32437;&#22270;&#20687;&#20013;&#30340;&#25277;&#35937;&#29305;&#24449;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#19988;&#22240;&#27492;&#20063;&#21487;&#20197;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;CAD&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#20063;&#36866;&#29992;&#20110;&#20248;&#21270;CAD&#38646;&#20214;&#30340;&#25277;&#35937;&#33258;&#21160;&#21270;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;StyleCLIP&#20197;&#36866;&#29992;&#20110;&#20307;&#32032;&#27169;&#22411;&#24418;&#24335;&#30340;CAD&#27169;&#22411;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;3D StyleGAN&#21644;&#33258;&#23450;&#20041;&#20998;&#31867;&#22120;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#36890;&#36807;&#20248;&#21270;&#23454;&#38469;CAD&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#33021;&#21147;&#32780;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When it comes to the optimization of CAD models in the automation domain, neural networks currently play only a minor role. Optimizing abstract features such as automation capability is challenging, since they can be very difficult to simulate, are too complex for rule-based systems, and also have little to no data available for machine-learning methods. On the other hand, image manipulation methods that can manipulate abstract features in images such as StyleCLIP have seen much success. They rely on the latent space of pretrained generative adversarial networks, and could therefore also make use of the vast amount of unlabeled CAD data. In this paper, we show that such an approach is also suitable for optimizing abstract automation-related features of CAD parts. We achieved this by extending StyleCLIP to work with CAD models in the form of voxel models, which includes using a 3D StyleGAN and a custom classifier. Finally, we demonstrate the ability of our system for the optimiziation o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;&#20351;&#29992;2D&#22270;&#20687;&#21644;3D&#36712;&#36857;&#23545;&#21160;&#35789;&#35821;&#20041;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;2D&#35270;&#35273;&#27169;&#24577;&#30340;&#34920;&#29616;&#19982;3D&#36712;&#36857;&#31867;&#20284;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#26234;&#24935;&#12290;</title><link>http://arxiv.org/abs/2303.12737</link><description>&lt;p&gt;
&#27604;&#36739;&#36712;&#36857;&#21644;&#35270;&#35273;&#27169;&#24577;&#23545;&#21160;&#35789;&#34920;&#31034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Comparing Trajectory and Vision Modalities for Verb Representation. (arXiv:2303.12737v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#20351;&#29992;2D&#22270;&#20687;&#21644;3D&#36712;&#36857;&#23545;&#21160;&#35789;&#35821;&#20041;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;2D&#35270;&#35273;&#27169;&#24577;&#30340;&#34920;&#29616;&#19982;3D&#36712;&#36857;&#31867;&#20284;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#26234;&#24935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#36712;&#36857;&#65292;&#21363;&#29289;&#20307;&#38543;&#26102;&#38388;&#30340;3D&#20301;&#32622;&#21644;&#26059;&#36716;&#65292;&#34987;&#35777;&#26126;&#21487;&#20197;&#32534;&#30721;&#21160;&#35789;&#35821;&#20041;&#30340;&#20851;&#38190;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;roll&#21644;slide&#30340;&#21547;&#20041;&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;NLP&#20013;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20351;&#29992;2D&#22270;&#20687;&#20316;&#20026;&#19990;&#30028;&#30340;&#34920;&#31034;&#12290;&#32771;&#34385;&#21040;3D&#31354;&#38388;&#22312;&#21160;&#35789;&#35821;&#20041;&#30340;&#24418;&#24335;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#39044;&#26399;&#36825;&#20123;2D&#22270;&#20687;&#20250;&#23548;&#33268;&#36139;&#30240;&#30340;&#34920;&#31034;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#24494;&#22937;&#30340;&#24046;&#24322;&#12290;&#26412;&#25991;&#22312;&#21463;&#25511;&#23454;&#39564;&#20013;&#30452;&#25509;&#27979;&#35797;&#20102;&#36825;&#20010;&#20551;&#35774;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#33258;&#30417;&#30563;&#30340;&#22270;&#20687;&#21644;&#36712;&#36857;&#32534;&#30721;&#22120;&#65292;&#28982;&#21518;&#35780;&#20272;&#23427;&#20204;&#23398;&#20064;&#21306;&#20998;&#21160;&#35789;&#27010;&#24565;&#30340;&#31243;&#24230;&#12290;&#19982;&#25105;&#20204;&#26368;&#21021;&#30340;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;2D&#35270;&#35273;&#27169;&#24577;&#30340;&#34920;&#29616;&#19982;3D&#36712;&#36857;&#31867;&#20284;&#12290;&#34429;&#28982;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#26234;&#24935;&#65306;&#26356;&#20016;&#23500;&#30340;&#29615;&#22659;&#34920;&#31034;&#24517;&#28982;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Three-dimensional trajectories, or the 3D position and rotation of objects over time, have been shown to encode key aspects of verb semantics (e.g., the meanings of roll vs. slide). However, most multimodal models in NLP use 2D images as representations of the world. Given the importance of 3D space in formal models of verb semantics, we expect that these 2D images would result in impoverished representations that fail to capture nuanced differences in meaning. This paper tests this hypothesis directly in controlled experiments. We train self-supervised image and trajectory encoders, and then evaluate them on the extent to which each learns to differentiate verb concepts. Contrary to our initial expectations, we find that 2D visual modalities perform similarly well to 3D trajectories. While further work should be conducted on this question, our initial findings challenge the conventional wisdom that richer environment representations necessarily translate into better representation lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#38142;&#65292;&#20351;&#29992;&#36866;&#24230;&#35745;&#31639;&#37327;&#30340;CLIP&#29305;&#24449;&#36827;&#34892;&#21387;&#32553;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#37325;&#22797;&#26816;&#27979;&#65292;&#21363;&#20351;&#26159;&#24040;&#22823;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#22312;LAION-2B&#20013;&#21457;&#29616;30\%&#30340;&#22270;&#20687;&#21487;&#33021;&#26159;&#37325;&#22797;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#22797;&#22270;&#20687;&#30452;&#26041;&#22270;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#27169;&#22411;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12733</link><description>&lt;p&gt;
LAION-2B&#30340;&#21435;&#37325;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the De-duplication of LAION-2B. (arXiv:2303.12733v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#38142;&#65292;&#20351;&#29992;&#36866;&#24230;&#35745;&#31639;&#37327;&#30340;CLIP&#29305;&#24449;&#36827;&#34892;&#21387;&#32553;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#37325;&#22797;&#26816;&#27979;&#65292;&#21363;&#20351;&#26159;&#24040;&#22823;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#22312;LAION-2B&#20013;&#21457;&#29616;30\%&#30340;&#22270;&#20687;&#21487;&#33021;&#26159;&#37325;&#22797;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#22797;&#22270;&#20687;&#30452;&#26041;&#22270;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#27169;&#22411;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;DALL-E&#12289;Midjourney&#21644;Stable Diffusion&#31561;&#65292;&#20855;&#26377;&#36229;&#36234;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#31038;&#20250;&#24847;&#20041;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20687;LAION-2B&#36825;&#26679;&#21253;&#21547;20&#20159;&#24352;&#22270;&#29255;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#24211;&#12290;&#22312;&#36825;&#20010;&#35268;&#27169;&#19979;&#65292;&#25163;&#21160;&#26816;&#26597;&#26159;&#22256;&#38590;&#30340;&#65292;&#33258;&#21160;&#20998;&#26512;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;LAION-2B&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#37325;&#22797;&#30340;&#22270;&#20687;&#20250;&#23548;&#33268;&#29256;&#26435;&#38382;&#39064;&#65292;&#36825;&#24433;&#21709;&#20102;&#20854;&#21487;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#38142;&#65292;&#20351;&#29992;&#36866;&#24230;&#35745;&#31639;&#37327;&#30340;CLIP&#29305;&#24449;&#36827;&#34892;&#21387;&#32553;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#37325;&#22797;&#26816;&#27979;&#65292;&#21363;&#20351;&#26159;&#24040;&#22823;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35777;&#26126;&#65292;LAION-2B&#20013;&#32422;700&#19975;&#24352;&#22270;&#20687;&#65292;&#32422;30\%&#30340;&#22270;&#20687;&#21487;&#33021;&#26159;&#37325;&#22797;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#22797;&#22270;&#20687;&#30452;&#26041;&#22270;&#65292;&#29992;&#20110;&#25581;&#31034;Stable Diffusion&#30340;&#36880;&#23383;&#22797;&#21046;&#31034;&#20363;&#65292;&#24182;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#24403;&#21069;&#29256;&#26412;&#30340;&#21435;&#37325;&#38598;&#23558;&#34987;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models, such as DALL-E, Midjourney, and Stable Diffusion, have societal implications that extend beyond the field of computer science. These models require large image databases like LAION-2B, which contain two billion images. At this scale, manual inspection is difficult and automated analysis is challenging. In addition, recent studies show that duplicated images pose copyright problems for models trained on LAION2B, which hinders its usability. This paper proposes an algorithmic chain that runs with modest compute, that compresses CLIP features to enable efficient duplicate detection, even for vast image volumes. Our approach demonstrates that roughly 700 million images, or about 30\%, of LAION-2B's images are likely duplicated. Our method also provides the histograms of duplication on this dataset, which we use to reveal more examples of verbatim copies by Stable Diffusion and further justify the approach. The current version of the de-duplicated set will be distributed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36816;&#29992;AI&#24037;&#20855;&#65292;&#22914;DALL-E&#65292;&#26469;&#23436;&#25104;&#26410;&#23436;&#25104;&#30340;&#21382;&#21490;&#24314;&#31569;&#65292;&#29305;&#21035;&#26159;&#26410;&#23436;&#25104;&#30340;&#23546;&#24217;&#27491;&#38754;&#65292;&#24182;&#20998;&#26512;&#20102;&#35813;&#39046;&#22495;&#20173;&#22788;&#20110;&#33804;&#33469;&#29366;&#24577;&#30340;&#24314;&#31569;&#22270;&#24418;&#32452;&#25104;&#65292;&#20026;&#24314;&#31569;&#35774;&#35745;&#20219;&#21153;&#25552;&#20379;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12732</link><description>&lt;p&gt;
&#26410;&#23436;&#25104;&#30340;&#24314;&#31569;&#65306;&#20174;&#20154;&#24037;&#26234;&#33021;&#30340;&#35270;&#35282;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Unfinished Architectures: A Perspective from Artificial Intelligence. (arXiv:2303.12732v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36816;&#29992;AI&#24037;&#20855;&#65292;&#22914;DALL-E&#65292;&#26469;&#23436;&#25104;&#26410;&#23436;&#25104;&#30340;&#21382;&#21490;&#24314;&#31569;&#65292;&#29305;&#21035;&#26159;&#26410;&#23436;&#25104;&#30340;&#23546;&#24217;&#27491;&#38754;&#65292;&#24182;&#20998;&#26512;&#20102;&#35813;&#39046;&#22495;&#20173;&#22788;&#20110;&#33804;&#33469;&#29366;&#24577;&#30340;&#24314;&#31569;&#22270;&#24418;&#32452;&#25104;&#65292;&#20026;&#24314;&#31569;&#35774;&#35745;&#20219;&#21153;&#25552;&#20379;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#23436;&#24037;&#24314;&#31569;&#22312;&#24314;&#31569;&#21382;&#21490;&#19978;&#19968;&#30452;&#23384;&#22312;&#65292;&#24182;&#24341;&#21457;&#20102;&#20851;&#20110;&#26159;&#21542;&#23436;&#24037;&#30340;&#28608;&#28872;&#36777;&#35770;&#65292;&#20026;&#19982;&#24050;&#23436;&#25104;&#37096;&#20998;&#30456;&#21327;&#35843;&#30340;&#26500;&#24605;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20026;&#26410;&#23436;&#25104;&#24314;&#31569;&#30340;&#23436;&#25104;&#25552;&#20986;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;DALL-E&#31561;&#24037;&#20855;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#23436;&#25104;&#22270;&#20687;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20219;&#21153;&#26469;&#35828;&#65292;AI&#30340;&#24110;&#21161;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#26032;&#30340;AI&#24037;&#20855;&#22312;&#21382;&#21490;&#23546;&#24217;&#26410;&#23436;&#25104;&#27491;&#38754;&#30340;&#23436;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#20998;&#26512;&#20102;&#24314;&#31569;&#22270;&#24418;&#32452;&#25104;&#39046;&#22495;&#20013;&#30340;&#20173;&#22788;&#20110;&#33804;&#33469;&#29366;&#24577;&#30340;&#36275;&#29699;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unfinished buildings are a constant throughout the history of architecture and have given rise to intense debates on the opportuneness of their completion, in addition to offering alibis for theorizing about the compositional possibilities in coherence with the finished parts. The development of Artificial Intelligence (AI) opens new avenues for the proposal of possibilities for the completion of unfinished architectures. Specifically, with the recent appearance of tools such as DALL-E, capable of completing images guided by a textual description, it is possible to count on the help of AI for architectural design tasks. In this article we explore the use of these new AI tools for the completion of unfinished facades of historical temples and analyse the still germinal stadium in the field of architectural graphic composition.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#31526;&#21495;&#23398;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20197;&#23637;&#29616;&#38750;&#29289;&#29702;&#12289;&#25277;&#35937;&#23646;&#24615;&#30340;&#21464;&#21270;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#31526;&#21495;&#23398;&#23646;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#35270;&#35273;&#35937;&#24449;&#65292;&#26377;&#21161;&#20110;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#20135;&#29983;&#12290;</title><link>http://arxiv.org/abs/2303.12731</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#31526;&#21495;&#23398;&#30340;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Visualizing Semiotics in Generative Adversarial Networks. (arXiv:2303.12731v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#31526;&#21495;&#23398;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20197;&#23637;&#29616;&#38750;&#29289;&#29702;&#12289;&#25277;&#35937;&#23646;&#24615;&#30340;&#21464;&#21270;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#31526;&#21495;&#23398;&#23646;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#35270;&#35273;&#35937;&#24449;&#65292;&#26377;&#21161;&#20110;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#20197;&#36890;&#36807;&#8220;&#31526;&#21495;&#23398;&#8221;&#36827;&#34892;&#20462;&#25913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;&#20687;&#32032;&#12289;&#33394;&#35843;&#20043;&#31867;&#30340;&#29289;&#29702;&#23646;&#24615;&#21487;&#20197;&#34987;&#20462;&#25913;&#19968;&#26679;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20063;&#21487;&#20197;&#20462;&#25913;&#38750;&#29289;&#29702;&#12289;&#25277;&#35937;&#30340;&#23646;&#24615;&#12290;&#20363;&#22914;&#65292;&#26426;&#33329;&#20056;&#21153;&#21592;&#30340;&#21046;&#26381;&#35774;&#35745;&#21487;&#20197;&#34987;&#20462;&#25913;&#20026;&#26356;&#21152;&#8220;&#35686;&#35273;&#8221;&#65292;&#19981;&#37027;&#20040;&#8220;&#20005;&#32899;&#8221;&#65292;&#25110;&#32773;&#26356;&#21152;&#8220;&#23454;&#29992;&#8221;&#12290;&#19968;&#20010;&#25151;&#23376;&#30340;&#24418;&#24335;&#21487;&#20197;&#34987;&#20462;&#25913;&#20026;&#26356;&#21152;&#8220;&#26410;&#26469;&#24863;&#8221;&#65292;&#19968;&#36742;&#36710;&#26356;&#21152;&#8220;&#21451;&#22909;&#8221;&#65292;&#19968;&#21452;&#29699;&#38795;&#21017;&#21487;&#20197;&#34987;&#26356;&#21152;&#8220;&#37034;&#24694;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25581;&#31034;&#20102;&#19982;&#24863;&#20852;&#36259;&#30340;&#31526;&#21495;&#23398;&#23646;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#35270;&#35273;&#35937;&#24449;&#65292;&#20351;&#24471;&#36825;&#19968;&#36807;&#31243;&#21487;&#20197;&#20351;&#29992;&#25277;&#35937;&#27010;&#24565;&#36827;&#34892;&#35270;&#35273;&#24418;&#24335;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36845;&#20195;&#24335;&#30340;&#65292;&#20801;&#35768;&#23545;&#23646;&#24615;&#23384;&#22312;&#31243;&#24230;&#36827;&#34892;&#25511;&#21046;&#65292;&#21487;&#20197;&#29992;&#20110;&#36741;&#21161;&#35774;&#35745;&#36807;&#31243;&#65292;&#20135;&#29983;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
We perform a set of experiments to demonstrate that images generated using a Generative Adversarial Network can be modified using 'semiotics.' We show that just as physical attributes such as the hue and saturation of an image can be modified, so too can its non-physical, abstract properties using our method. For example, the design of a flight attendant's uniform may be modified to look more 'alert,' less 'austere,' or more 'practical.' The form of a house can be modified to appear more 'futuristic,' a car more 'friendly' a pair of sneakers, 'evil.' Our method uncovers latent visual iconography associated with the semiotic property of interest, enabling a process of visual form-finding using abstract concepts. Our approach is iterative and allows control over the degree of attribute presence and can be used to aid the design process to yield emergent visual concepts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#39537;&#21160;&#30340;&#32768;&#26001;&#24314;&#27169;&#31995;&#32479;&#36827;&#34892;&#20102;&#22522;&#30784;&#24314;&#35774;&#65292;&#20801;&#35768;&#35843;&#26597;&#21592;&#39044;&#20808;&#26368;&#23567;&#21270;&#32768;&#26001;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#26377;&#29992;&#30340;&#22270;&#20687;&#26469;&#25910;&#38598;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.12730</link><description>&lt;p&gt;
&#38754;&#21521;&#25968;&#25454;&#39537;&#21160;&#30340;&#28023;&#27915;&#22823;&#22411;&#21160;&#29289;&#35843;&#26597;&#20013;&#30340;&#32768;&#26001;&#20998;&#31867;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Toward Data-Driven Glare Classification and Prediction for Marine Megafauna Survey. (arXiv:2303.12730v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#39537;&#21160;&#30340;&#32768;&#26001;&#24314;&#27169;&#31995;&#32479;&#36827;&#34892;&#20102;&#22522;&#30784;&#24314;&#35774;&#65292;&#20801;&#35768;&#35843;&#26597;&#21592;&#39044;&#20808;&#26368;&#23567;&#21270;&#32768;&#26001;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#26377;&#29992;&#30340;&#22270;&#20687;&#26469;&#25910;&#38598;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20272;&#35745;&#29289;&#31181;&#25968;&#37327;&#65292;&#21152;&#25343;&#22823;&#21271;&#22823;&#35199;&#27915;&#27700;&#22495;&#30340;&#28626;&#21361;&#29289;&#31181;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#65292;&#24182;&#24433;&#21709;&#30528;&#25919;&#31574;&#12290;&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#39537;&#21160;&#30340;&#32768;&#26001;&#24314;&#27169;&#31995;&#32479;&#36827;&#34892;&#20102;&#22522;&#30784;&#24314;&#35774;&#65292;&#36825;&#23558;&#20801;&#35768;&#35843;&#26597;&#21592;&#39044;&#20808;&#26368;&#23567;&#21270;&#32768;&#26001;&#12290;&#35843;&#26597;&#21592;&#20351;&#29992;&#26816;&#27979;&#20989;&#25968;&#20272;&#35745;&#26410;&#26126;&#26174;&#30475;&#21040;&#30340;&#24040;&#22411;&#21160;&#29289;&#31181;&#32676;&#12290;&#30740;&#31350;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#26377;&#29992;&#30340;&#22270;&#20687;&#26469;&#25910;&#38598;&#25968;&#25454;&#65292;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#20351;&#29992;&#32768;&#26001;&#27169;&#22411;&#39044;&#27979;&#32768;&#26001;&#24182;&#20248;&#21270;&#26080;&#32768;&#26001;&#25968;&#25454;&#30340;&#25910;&#38598;&#12290;&#20026;&#26500;&#24314;&#27492;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#22823;&#22411;&#25968;&#25454;&#38598;&#20351;&#29992;&#33258;&#28982;&#20266;&#26631;&#31614;&#26041;&#27861;&#20351;&#29992;&#32423;&#32852;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#36827;&#34892;&#26631;&#35760;&#12290;&#20351;&#29992;&#21453;&#23556;&#29575;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#24863;&#20852;&#36259;&#30340;&#29305;&#24449;&#65292;&#22635;&#20805;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#21487;&#20197;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critically endangered species in Canadian North Atlantic waters are systematically surveyed to estimate species populations which influence governing policies. Due to its impact on policy, population accuracy is important. This paper lays the foundation towards a data-driven glare modelling system, which will allow surveyors to preemptively minimize glare. Surveyors use a detection function to estimate megafauna populations which are not explicitly seen. A goal of the research is to maximize useful imagery collected, to that end we will use our glare model to predict glare and optimize for glare-free data collection. To build this model, we leverage a small labelled dataset to perform semi-supervised learning. The large dataset is labelled with a Cascading Random Forest Model using a na\"ive pseudo-labelling approach. A reflectance model is used, which pinpoints features of interest, to populate our datasets which allows for context-aware machine learning models. The pseudo-labelled da
&lt;/p&gt;</description></item><item><title>&#20302;&#20809;&#29615;&#22659;&#19979;&#30340;&#34892;&#20154;&#26816;&#27979;&#26041;&#26696;&#25552;&#26696;&#65292;&#20351;&#29992;&#22270;&#20687;&#34701;&#21512;&#25216;&#26415;&#39044;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992; Vision Transformer &#27169;&#22411;&#26816;&#27979;&#34892;&#20154;&#65292;&#27604;&#36739; ViT &#27169;&#22411;&#21644; YOLOv5 &#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12725</link><description>&lt;p&gt;
&#20302;&#20809;&#29615;&#22659;&#19979;&#30340;&#34892;&#20154;&#26816;&#27979;&#26041;&#26696;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
Pedestrain detection for low-light vision proposal. (arXiv:2303.12725v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12725
&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#29615;&#22659;&#19979;&#30340;&#34892;&#20154;&#26816;&#27979;&#26041;&#26696;&#25552;&#26696;&#65292;&#20351;&#29992;&#22270;&#20687;&#34701;&#21512;&#25216;&#26415;&#39044;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992; Vision Transformer &#27169;&#22411;&#26816;&#27979;&#34892;&#20154;&#65292;&#27604;&#36739; ViT &#27169;&#22411;&#21644; YOLOv5 &#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#34892;&#20154;&#26816;&#27979;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#38024;&#23545;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#34701;&#21512;&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#20063;&#26085;&#30410;&#31361;&#20986;&#12290;&#30001;&#20110;&#32418;&#22806;&#22270;&#20687;&#21487;&#20197;&#25429;&#25417;&#21040;&#28909;&#36752;&#23556;&#20449;&#24687;&#65292;&#22240;&#27492;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#20043;&#38388;&#30340;&#22270;&#20687;&#34701;&#21512;&#21487;&#20197;&#22312;&#29615;&#22659;&#38480;&#21046;&#19979;&#26174;&#30528;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#25928;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#23558;&#20351;&#29992;&#22270;&#20687;&#34701;&#21512;&#25216;&#26415;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#28982;&#21518;&#20351;&#29992; Vision Transformer &#27169;&#22411;&#20174;&#34701;&#21512;&#22270;&#20687;&#20013;&#26816;&#27979;&#34892;&#20154;&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#23558;&#22312;&#25105;&#20204;&#34701;&#21512;&#22270;&#20687;&#19978;&#27604;&#36739; YOLOv5 &#21644;&#20462;&#35746;&#21518;&#30340; ViT &#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for pedestrian detection has created a challenging problem for various visual tasks such as image fusion. As infrared images can capture thermal radiation information, image fusion between infrared and visible images could significantly improve target detection under environmental limitations. In our project, we would approach by preprocessing our dataset with image fusion technique, then using Vision Transformer model to detect pedestrians from the fused images. During the evaluation procedure, a comparison would be made between YOLOv5 and the revised ViT model performance on our fused images
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#30446;&#26631;&#37319;&#26679;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#20266;&#30446;&#26631;&#26679;&#26412;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#26356;&#23481;&#26131;&#22320;&#20174;&#28304;&#22495;&#36716;&#31227;&#33267;&#30446;&#26631;&#22495;&#24182;&#19988;&#22312;&#25552;&#39640;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12724</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Target Sampler for Unsupervised Domain Adaptation. (arXiv:2303.12724v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#30446;&#26631;&#37319;&#26679;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#20266;&#30446;&#26631;&#26679;&#26412;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#26356;&#23481;&#26131;&#22320;&#20174;&#28304;&#22495;&#36716;&#31227;&#33267;&#30446;&#26631;&#22495;&#24182;&#19988;&#22312;&#25552;&#39640;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#21040;&#26032;&#30340;&#24212;&#29992;&#22330;&#26223;&#26102;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#38480;&#21487;&#36716;&#31227;&#24615;&#20250;&#38480;&#21046;&#20854;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#24050;&#32463;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#22495;&#20013;&#30340;&#22823;&#22495;&#28418;&#31227;&#21644;&#26679;&#26412;&#31232;&#32570;&#23548;&#33268;&#29616;&#26377;&#30340;UDA&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#30446;&#26631;&#37319;&#26679;&#22120;&#65288;DTS&#65289;&#65292;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#20266;&#30446;&#26631;&#26679;&#26412;&#12290;&#36890;&#36807;&#24341;&#20837;&#31867;&#26465;&#20214;&#20449;&#24687;&#65292;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#30340;&#30446;&#26631;&#26679;&#26412;&#30340;&#26631;&#31614;&#12290;&#29983;&#25104;&#30340;&#26679;&#26412;&#21487;&#20197;&#24456;&#22909;&#22320;&#27169;&#25311;&#30446;&#26631;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#24110;&#21161;&#29616;&#26377;&#30340;UDA&#26041;&#27861;&#26356;&#36731;&#26494;&#22320;&#20174;&#28304;&#22495;&#36716;&#31227;&#33267;&#30446;&#26631;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36716;&#31227;&#24615;&#33021;&#12290;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#29616;&#26377;UDA&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#26497;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limited transferability hinders the performance of deep learning models when applied to new application scenarios. Recently, unsupervised domain adaptation (UDA) has achieved significant progress in addressing this issue via learning domain-invariant features. However, large domain shifts and the sample scarcity in the target domain make existing UDA methods achieve suboptimal performance. To alleviate these issues, we propose a plug-and-play Diffusion-based Target Sampler (DTS) to generate high fidelity and diversity pseudo target samples. By introducing class-conditional information, the labels of the generated target samples can be controlled. The generated samples can well simulate the data distribution of the target domain and help existing UDA methods transfer from the source domain to the target domain more easily, thus improving the transfer performance. Extensive experiments on various benchmarks demonstrate that the performance of existing UDA methods can be greatly improved 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#25163;&#32472;&#21644;&#35821;&#20041;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#35774;&#35745;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#25163;&#32472;&#30340;&#26041;&#27861;&#26356;&#21152;&#30452;&#35266;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#65292;&#32780;&#22522;&#20110;&#35821;&#20041;&#30340;&#29983;&#25104;AI&#22312;&#36136;&#37327;&#21644;&#36924;&#30495;&#24230;&#26041;&#38754;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12709</link><description>&lt;p&gt;
&#22522;&#20110;&#25163;&#32472;&#21644;&#35821;&#20041;&#30340;&#27169;&#22411;&#29983;&#25104;&#26041;&#27861;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Sketch-Based and Semantic-Based Modalities for Mockup Generation. (arXiv:2303.12709v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#25163;&#32472;&#21644;&#35821;&#20041;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#35774;&#35745;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#25163;&#32472;&#30340;&#26041;&#27861;&#26356;&#21152;&#30452;&#35266;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#65292;&#32780;&#22522;&#20110;&#35821;&#20041;&#30340;&#29983;&#25104;AI&#22312;&#36136;&#37327;&#21644;&#36924;&#30495;&#24230;&#26041;&#38754;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#27169;&#22411;&#26159;&#21487;&#35270;&#21270;&#21644;&#27979;&#35797;&#35774;&#35745;&#24819;&#27861;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#27169;&#22411;&#30340;&#36807;&#31243;&#23545;&#35774;&#35745;&#24072;&#26469;&#35828;&#21487;&#33021;&#20250;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#24182;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#20197;&#25903;&#25345;&#35774;&#35745;&#24072;&#22312;&#20854;&#24037;&#20316;&#20013;&#29983;&#25104;&#27169;&#22411;&#24819;&#27861;&#65306;&#65288;1&#65289;&#22522;&#20110;&#25163;&#32472;&#33609;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#65288;2&#65289;&#22522;&#20110;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#35774;&#35745;&#20803;&#32032;&#29983;&#25104;&#30028;&#38754;&#30340;&#35821;&#20041;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;13&#20010;&#21442;&#19982;&#32773;&#20026;&#21463;&#35797;&#23545;&#35937;&#65292;&#35753;&#20182;&#20204;&#20351;&#29992;&#27599;&#31181;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#25163;&#32472;&#30340;&#29983;&#25104;&#26041;&#27861;&#26356;&#21152;&#30452;&#35266;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#65292;&#32780;&#22522;&#20110;&#35821;&#20041;&#30340;&#29983;&#25104;AI&#22312;&#36136;&#37327;&#21644;&#36924;&#30495;&#24230;&#26041;&#38754;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#25104;&#20026;UI&#35774;&#35745;&#24072;&#25552;&#39640;&#21019;&#36896;&#21147;&#21644;&#25928;&#29575;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Design mockups are essential instruments for visualizing and testing design ideas. However, the process of generating mockups can be time-consuming and challenging for designers. In this article, we present and evaluate two different modalities for generating mockup ideas to support designers in their work: (1) a sketch-based approach to generate mockups based on hand-drawn sketches, and (2) a semantic-based approach to generate interfaces based on a set of predefined design elements. To evaluate the effectiveness of these two approaches, we conducted a series of experiments with 13 participants in which we asked them to generate mockups using each modality. Our results show that sketch-based generation was more intuitive and expressive, while semantic-based generative AI obtained better results in terms of quality and fidelity. Both methods can be valuable tools for UI designers looking to increase their creativity and efficiency.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#38381;&#30151;&#30340;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#26088;&#22312;&#37327;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#23458;&#35266;&#30340;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#34892;&#20026;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.12707</link><description>&lt;p&gt;
&#27604;&#36739;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#38381;&#30151;&#26816;&#27979;&#20013;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparison of Probabilistic Deep Learning Methods for Autism Detection. (arXiv:2303.12707v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12707
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#38381;&#30151;&#30340;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#26088;&#22312;&#37327;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#23458;&#35266;&#30340;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#34892;&#20026;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#30446;&#21069;&#22312;&#19990;&#30028;&#33539;&#22260;&#20869;&#26222;&#36941;&#23384;&#22312;&#30340;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#12290;ASD &#20250;&#22312;&#20010;&#20307;&#30340;&#25972;&#20010;&#29983;&#21629;&#26399;&#20869;&#23384;&#22312;&#65292;&#24433;&#21709;&#20182;&#20204;&#30340;&#34892;&#20026;&#21644;&#20132;&#27969;&#26041;&#24335;&#65292;&#23548;&#33268;&#26126;&#26174;&#30340;&#31038;&#20132;&#38556;&#30861;&#12289;&#37325;&#22797;&#30340;&#34892;&#20026;&#29305;&#24449;&#20197;&#21450;&#20852;&#36259;&#21463;&#38480;&#12290;&#26089;&#26399;&#21457;&#29616;&#35813;&#30142;&#30149;&#26377;&#21161;&#20110;&#21551;&#21160;&#27835;&#30103;&#24182;&#24110;&#21161;&#24739;&#32773;&#36807;&#19978;&#27491;&#24120;&#30340;&#29983;&#27963;&#12290;&#30446;&#21069;&#24050;&#32463;&#30740;&#31350;&#21644;&#24320;&#21457;&#20102;&#19968;&#20123;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23450;&#37327;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#20020;&#24202;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#37327;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#65292;&#19968;&#20123;&#22797;&#26434;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#21152;&#36895;&#33258;&#38381;&#30151;&#30340;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#20197;&#20854;&#25152;&#24212;&#29992;&#30340;&#25968;&#25454;&#38598;&#31867;&#22411;&#20026;&#29305;&#24449;&#36827;&#34892;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autism Spectrum Disorder (ASD) is one neuro developmental disorder that is now widespread in the world. ASD persists throughout the life of an individual, impacting the way they behave and communicate, resulting to notable deficits consisting of social life retardation, repeated behavioural traits and a restriction in their interests. Early detection of the disorder helps in the onset treatment and helps one to lead a normal life. There are clinical approaches used in detection of autism, relying on behavioural data and in worst cases, neuroimaging. Quantitative methods involving machine learning have been studied and developed to overcome issues with clinical approaches. These quantitative methods rely on machine learning, with some complex methods based on deep learning developed to accelerate detection and diagnosis of ASD. These literature is aimed at exploring most state-of-the-art probabilistic methods in use today, characterizing them with the type of dataset they're most applie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#35777;&#25454;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24320;&#25918;&#38598;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#38382;&#39064;&#21644;&#21516;&#19968;&#22330;&#26223;&#20013;&#21333;&#20010;&#25110;&#22810;&#20010;&#28436;&#21592;&#20197;&#21450;&#20219;&#20309;&#28436;&#21592;&#30340;&#21516;&#26102;&#21160;&#20316;&#30340;&#26356;&#19968;&#33324;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12698</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#35777;&#25454;&#23398;&#20064;&#29992;&#20110;&#24320;&#25918;&#38598;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Open Set Action Recognition via Multi-Label Evidential Learning. (arXiv:2303.12698v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#35777;&#25454;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24320;&#25918;&#38598;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#38382;&#39064;&#21644;&#21516;&#19968;&#22330;&#26223;&#20013;&#21333;&#20010;&#25110;&#22810;&#20010;&#28436;&#21592;&#20197;&#21450;&#20219;&#20309;&#28436;&#21592;&#30340;&#21516;&#26102;&#21160;&#20316;&#30340;&#26356;&#19968;&#33324;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24320;&#25918;&#38598;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#38598;&#20013;&#20110;&#26032;&#39062;&#24615;&#26816;&#27979;&#65292;&#20551;&#35774;&#35270;&#39057;&#21098;&#36753;&#26174;&#31034;&#21333;&#20010;&#21160;&#20316;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#26631;&#31614;&#35777;&#25454;&#23398;&#20064;&#65288;MULE&#65289;&#36827;&#34892;&#24320;&#25918;&#38598;&#21160;&#20316;&#35782;&#21035;&#21644;&#26032;&#39062;&#24615;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26032;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#19968;&#22330;&#26223;&#20013;&#21333;&#20010;&#25110;&#22810;&#20010;&#28436;&#21592;&#20197;&#21450;&#20219;&#20309;&#28436;&#21592;&#30340;&#21516;&#26102;&#21160;&#20316;&#30340;&#26356;&#19968;&#33324;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;Beta&#35777;&#25454;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#28436;&#21592;-&#19978;&#19979;&#25991;-&#23545;&#35937;&#20851;&#31995;&#34920;&#31034;&#65292;&#20351;&#29992;Beta&#23494;&#24230;&#20272;&#35745;&#22810;&#21160;&#20316;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#28155;&#21152;&#35777;&#25454;&#21435;&#20559;&#32622;&#32422;&#26463;&#21040;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#20197;&#20943;&#23569;&#35270;&#39057;&#34920;&#31034;&#30340;&#38745;&#24577;&#20559;&#24046;&#65292;&#20174;&#32780;&#21487;&#20197;&#38169;&#35823;&#22320;&#20851;&#32852;&#39044;&#27979;&#21644;&#38745;&#24577;&#32447;&#32034;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#24179;&#22343;&#26041;&#26696;&#26356;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#20248;&#21270;&#31639;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods for open-set action recognition focus on novelty detection that assumes video clips show a single action, which is unrealistic in the real world. We propose a new method for open set action recognition and novelty detection via MUlti-Label Evidential learning (MULE), that goes beyond previous novel action detection methods by addressing the more general problems of single or multiple actors in the same scene, with simultaneous action(s) by any actor. Our Beta Evidential Neural Network estimates multi-action uncertainty with Beta densities based on actor-context-object relation representations. An evidence debiasing constraint is added to the objective function for optimization to reduce the static bias of video representations, which can incorrectly correlate predictions and static cues. We develop a learning algorithm based on a primal-dual average scheme update to optimize the proposed problem. Theoretical analysis of the optimization algorithm demonstrates the conve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;&#25193;&#23637;&#26041;&#27861;&#65292;&#31216;&#20026;&#23494;&#38598;&#32593;&#32476;&#25193;&#23637;&#65288;DNE&#65289;&#65292;&#36890;&#36807;&#36328;&#20219;&#21153;&#27880;&#24847;&#26426;&#21046;&#21644;&#23494;&#38598;&#36830;&#25509;&#26469;&#23454;&#29616;&#20174;&#26087;&#20219;&#21153;&#21040;&#26032;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#22312;&#31934;&#24230;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#20043;&#38388;&#23454;&#29616;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12696</link><description>&lt;p&gt;
&#23494;&#38598;&#32593;&#32476;&#25193;&#23637;&#29992;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dense Network Expansion for Class Incremental Learning. (arXiv:2303.12696v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;&#25193;&#23637;&#26041;&#27861;&#65292;&#31216;&#20026;&#23494;&#38598;&#32593;&#32476;&#25193;&#23637;&#65288;DNE&#65289;&#65292;&#36890;&#36807;&#36328;&#20219;&#21153;&#27880;&#24847;&#26426;&#21046;&#21644;&#23494;&#38598;&#36830;&#25509;&#26469;&#23454;&#29616;&#20174;&#26087;&#20219;&#21153;&#21040;&#26032;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#22312;&#31934;&#24230;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#20043;&#38388;&#23454;&#29616;&#26356;&#22909;&#30340;&#24179;&#34913;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#32593;&#32476;&#25193;&#23637;&#65288;NE&#65289;&#30340;&#21160;&#24577;&#26550;&#26500;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#38656;&#35201;&#22686;&#21152;&#19968;&#20010;&#19987;&#23478;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#24456;&#26377;&#25928;&#65292;&#20294;&#20250;&#23548;&#33268;&#27169;&#22411;&#38543;&#30528;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#24555;&#36895;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;&#25193;&#23637;&#26041;&#27861;&#65292;&#21363;&#23494;&#38598;&#32593;&#32476;&#25193;&#23637;&#65288;DNE&#65289;&#65292;&#20197;&#22312;&#31934;&#24230;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#20043;&#38388;&#23454;&#29616;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20219;&#21153;&#19987;&#23478;&#32593;&#32476;&#20013;&#38388;&#23618;&#20043;&#38388;&#30340;&#23494;&#38598;&#36830;&#25509;&#65292;&#36890;&#36807;&#29305;&#24449;&#20849;&#20139;&#21644;&#22797;&#29992;&#65292;&#23454;&#29616;&#20102;&#20174;&#26087;&#20219;&#21153;&#21040;&#26032;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#36825;&#31181;&#20849;&#20139;&#26159;&#36890;&#36807;&#36328;&#20219;&#21153;&#27880;&#24847;&#26426;&#21046;&#26469;&#23454;&#29616;&#30340;&#65292;&#22522;&#20110;&#26032;&#20219;&#21153;&#27880;&#24847;&#22359;&#65288;TAB&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#20219;&#21153;&#20043;&#38388;&#34701;&#21512;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#27880;&#24847;&#26426;&#21046;&#19981;&#21516;&#65292;TAB &#22312;&#29305;&#24449;&#28151;&#21512;&#30340;&#23618;&#27425;&#19978;&#25805;&#20316;&#65292;&#24182;&#19988;&#19982;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#32806;&#12290;&#19982;&#32852;&#21512;&#31354;&#38388;&#21644;&#20219;&#21153;&#20851;&#27880;&#26426;&#21046;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#37117;&#26356;&#26377;&#25928;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of class incremental learning (CIL) is considered. State-of-the-art approaches use a dynamic architecture based on network expansion (NE), in which a task expert is added per task. While effective from a computational standpoint, these methods lead to models that grow quickly with the number of tasks. A new NE method, dense network expansion (DNE), is proposed to achieve a better trade-off between accuracy and model complexity. This is accomplished by the introduction of dense connections between the intermediate layers of the task expert networks, that enable the transfer of knowledge from old to new tasks via feature sharing and reusing. This sharing is implemented with a cross-task attention mechanism, based on a new task attention block (TAB), that fuses information across tasks. Unlike traditional attention mechanisms, TAB operates at the level of the feature mixing and is decoupled with spatial attentions. This is shown more effective than a joint spatial-and-task att
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#25239;&#21512;&#25104;&#25915;&#20987;&#30340;&#24377;&#24615;&#36755;&#20986;&#32422;&#26463;&#25511;&#21046;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#20998;&#24067;&#24335;&#35266;&#23519;&#22120;&#12289;&#20272;&#35745;&#22120;&#21644;&#33258;&#36866;&#24212;&#25511;&#21046;&#20445;&#35777;&#20102;&#36319;&#38543;&#32773;&#30340;&#36755;&#20986;&#34987;&#25239;FDI&#21644;&#20266;&#35013;&#25915;&#20987;&#25152;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2303.12693</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#25239;&#21512;&#25104;&#25915;&#20987;&#30340;&#24377;&#24615;&#36755;&#20986;&#32422;&#26463;&#25511;&#21046;: &#25968;&#23383;&#23402;&#29983;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resilient Output Containment Control of Heterogeneous Multiagent Systems Against Composite Attacks: A Digital Twin Approach. (arXiv:2303.12693v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#26041;&#27861;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#25239;&#21512;&#25104;&#25915;&#20987;&#30340;&#24377;&#24615;&#36755;&#20986;&#32422;&#26463;&#25511;&#21046;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#20998;&#24067;&#24335;&#35266;&#23519;&#22120;&#12289;&#20272;&#35745;&#22120;&#21644;&#33258;&#36866;&#24212;&#25511;&#21046;&#20445;&#35777;&#20102;&#36319;&#38543;&#32773;&#30340;&#36755;&#20986;&#34987;&#25239;FDI&#21644;&#20266;&#35013;&#25915;&#20987;&#25152;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#21512;&#25104;&#25915;&#20987;&#65288;&#21253;&#25324;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12289;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#12289;&#20266;&#35013;&#25915;&#20987;&#21644;&#25191;&#34892;&#25915;&#20987;&#65289;&#30340;&#24322;&#26500;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#24377;&#24615;&#36755;&#20986;&#32422;&#26463;&#25511;&#21046;&#12290;&#21463;&#25968;&#23383;&#23402;&#29983;&#30340;&#21551;&#21457;&#65292;&#20351;&#29992;&#20855;&#26377;&#26356;&#39640;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#23402;&#29983;&#23618;&#65288;TL&#65289;&#23558;&#19978;&#36848;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#20219;&#21153;&#65306;&#23545;TL&#19978;&#30340;DoS&#25915;&#20987;&#30340;&#38450;&#24481;&#21327;&#35758;&#21644;&#23545;&#36187;&#21338;&#29289;&#29702;&#23618;&#65288;CPL&#65289;&#19978;&#30340;&#25191;&#34892;&#25915;&#20987;&#30340;&#38450;&#24481;&#21327;&#35758;&#12290;&#39318;&#20808;&#65292;&#32771;&#34385;&#21040;&#39046;&#23548;&#21160;&#24577;&#30340;&#24314;&#27169;&#35823;&#24046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20998;&#24067;&#24335;&#35266;&#23519;&#22120;&#65292;&#22312;TL&#19978;&#37325;&#26032;&#26500;&#36896;&#27599;&#20010;&#36319;&#38543;&#32773;&#30340;&#39046;&#23548;&#21160;&#24577;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#20272;&#35745;&#22120;&#26681;&#25454;&#22312;TL&#19978;&#37325;&#24314;&#30340;&#39046;&#23548;&#21160;&#24577;&#20272;&#35745;&#36319;&#38543;&#32773;&#29366;&#24577;&#12290;&#31532;&#19977;&#65292;&#26681;&#25454;&#37325;&#24314;&#30340;&#39046;&#23548;&#21160;&#24577;&#65292;&#25105;&#20204;&#35774;&#35745;&#20998;&#25955;&#31639;&#27861;&#22120;&#22312;CPL&#19978;&#35745;&#31639;&#36755;&#20986;&#35843;&#33410;&#22120;&#26041;&#31243;&#12290;&#31532;&#22235;&#65292;&#25552;&#20986;&#20998;&#24067;&#24335;&#33258;&#36866;&#24212;&#25239;&#25915;&#20987;&#24377;&#24615;&#25511;&#21046;&#26041;&#26696;&#65292;&#20445;&#35777;&#36319;&#38543;&#32773;&#30340;&#36755;&#20986;&#34987;&#25239;FDI&#21644;&#20266;&#35013;&#25915;&#20987;&#25152;&#32422;&#26463;&#12290;&#26368;&#21518;&#65292;&#25552;&#20379;&#20102;&#25968;&#20540;&#20223;&#30495;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#24377;&#24615;&#36755;&#20986;&#32422;&#26463;&#25511;&#21046;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the distributed resilient output containment control of heterogeneous multiagent systems against composite attacks, including denial-of-services (DoS) attacks, false-data injection (FDI) attacks, camouflage attacks, and actuation attacks. Inspired by digital twins, a twin layer (TL) with higher security and privacy is used to decouple the above problem into two tasks: defense protocols against DoS attacks on TL and defense protocols against actuation attacks on cyber-physical layer (CPL). First, considering modeling errors of leader dynamics, we introduce distributed observers to reconstruct the leader dynamics for each follower on TL under DoS attacks. Second, distributed estimators are used to estimate follower states according to the reconstructed leader dynamics on the TL. Third, according to the reconstructed leader dynamics, we design decentralized solvers that calculate the output regulator equations on CPL. Fourth, decentralized adaptive attack-resilient cont
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20154;&#31867;&#24418;&#29366;&#35782;&#21035;&#65292;&#32780;&#38750;&#20165;&#20165;&#21033;&#29992;&#32441;&#29702;&#32447;&#32034;&#12290;&#22312;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#32032;&#25551;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#21464;&#25442;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20063;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.12669</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#19979;&#22522;&#20110;&#20154;&#31867;&#34892;&#20026;&#30340;&#30740;&#31350;&#25193;&#23637;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
An Extended Study of Human-like Behavior under Adversarial Training. (arXiv:2303.12669v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12669
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20154;&#31867;&#24418;&#29366;&#35782;&#21035;&#65292;&#32780;&#38750;&#20165;&#20165;&#21033;&#29992;&#32441;&#29702;&#32447;&#32034;&#12290;&#22312;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#32032;&#25551;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#21464;&#25442;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20063;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#35768;&#22810;&#32570;&#38519;&#65292;&#20854;&#20013;&#26368;&#20005;&#37325;&#30340;&#20043;&#19968;&#26159;&#23545;&#20998;&#24067;&#20559;&#24046;&#30340;&#25935;&#24863;&#24615;&#65292;&#36825;&#20801;&#35768;&#27169;&#22411;&#36731;&#26131;&#34987;&#23567;&#22411;&#25200;&#21160;&#27450;&#39575;&#24182;&#20570;&#20986;&#38169;&#35823;&#39044;&#27979;&#65292;&#32780;&#36825;&#20123;&#25200;&#21160;&#36890;&#24120;&#23545;&#20154;&#31867;&#26469;&#35828;&#19981;&#26131;&#23519;&#35273;&#24182;&#19981;&#24517;&#39035;&#20855;&#26377;&#35821;&#20041;&#21547;&#20041;&#12290; &#23545;&#25239;&#24615;&#35757;&#32451;&#36890;&#36807;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#25200;&#21160;&#26469;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290; &#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#20063;&#25351;&#20986;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#26041;&#24335;&#19981;&#21516;&#20110;&#20154;&#31867;&#12290; &#20154;&#31867;&#36890;&#36807;&#24418;&#29366;&#35782;&#21035;&#23545;&#35937;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#20027;&#35201;&#21033;&#29992;&#32441;&#29702;&#32447;&#32034;&#12290; &#20363;&#22914;&#65292;&#21463;&#36807;&#29031;&#29255;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25512;&#24191;&#21040;&#21253;&#21547;&#32032;&#25551;&#30340;&#25968;&#25454;&#38598;&#12290; &#26377;&#36259;&#30340;&#26159;&#65292;&#36824;&#34920;&#26126;&#23545;&#25239;&#24615;&#35757;&#32451;&#20284;&#20046;&#26377;&#21033;&#20110;&#22686;&#21152;&#36716;&#21521;&#24418;&#29366;&#20559;&#24046;&#30340;&#36235;&#21183;&#12290; &#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#23601;&#21508;&#31181;&#26550;&#26500;&#65292;&#24120;&#35265;&#30340;$\ell_2$&#21644;$\ell_\infty$-training&#65292;&#20197;&#21450;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#25928;&#24212;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#12290; &#25105;&#20204;&#22312;&#29289;&#20307;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36716;&#21521;&#24418;&#29366;&#20559;&#24046;&#30340;&#21464;&#25442;&#19981;&#20165;&#38480;&#20110;&#35270;&#35273;&#39046;&#22495;&#65292;&#32780;&#19988;&#20063;&#36866;&#29992;&#20110;&#35821;&#35328;&#22788;&#29702;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23545;&#25239;&#24615;&#35757;&#32451;&#23548;&#33268;&#27169;&#22411;&#26356;&#22810;&#22320;&#20381;&#36182;&#32452;&#21512;&#32467;&#26500;&#26469;&#35782;&#21035;&#23545;&#35937;&#21644;&#36827;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#20855;&#26377;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have a number of shortcomings. Amongst the severest ones is the sensitivity to distribution shifts which allows models to be easily fooled into wrong predictions by small perturbations to inputs that are often imperceivable to humans and do not have to carry semantic meaning. Adversarial training poses a partial solution to address this issue by training models on worst-case perturbations. Yet, recent work has also pointed out that the reasoning in neural networks is different from humans. Humans identify objects by shape, while neural nets mainly employ texture cues. Exemplarily, a model trained on photographs will likely fail to generalize to datasets containing sketches. Interestingly, it was also shown that adversarial training seems to favorably increase the shift toward shape bias. In this work, we revisit this observation and provide an extensive analysis of this effect on various architectures, the common $\ell_2$- and $\ell_\infty$-training, and Transformer-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; PIQ&#65292;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#21521;&#37327;&#37327;&#21270;&#65292;&#23558;&#20854;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#35299;&#37322;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#23481;&#26131;&#35753;&#20154;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.12659</link><description>&lt;p&gt;
&#36890;&#36807;&#37327;&#21270;&#36827;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Posthoc Interpretation via Quantization. (arXiv:2303.12659v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; PIQ&#65292;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#21521;&#37327;&#37327;&#21270;&#65292;&#23558;&#20854;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#35299;&#37322;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#23481;&#26131;&#35753;&#20154;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#36890;&#36807;&#37327;&#21270;&#23454;&#29616;&#30340;&#20107;&#21518;&#35299;&#37322;&#65288;PIQ&#65289;&#8221;&#65292;&#29992;&#20110;&#35299;&#37322;&#35757;&#32451;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21521;&#37327;&#37327;&#21270;&#23558;&#20998;&#31867;&#22120;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#65292;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#12290;&#31867;&#29305;&#23450;&#30340;&#30721;&#26412;&#20316;&#20026;&#29942;&#39048;&#65292;&#36843;&#20351;&#35299;&#37322;&#32773;&#19987;&#27880;&#20110;&#20998;&#31867;&#22120;&#35748;&#20026;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#19982;&#25991;&#29486;&#20013;&#30340;&#20960;&#31181;&#20854;&#20182;&#35299;&#37322;&#26041;&#27861;&#30456;&#27604;&#65292;PIQ&#29983;&#25104;&#30340;&#35299;&#37322;&#26356;&#23481;&#26131;&#34987;&#21442;&#19982;&#25105;&#20204;&#29992;&#25143;&#30740;&#31350;&#30340;&#20154;&#25152;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new approach, called "Posthoc Interpretation via Quantization (PIQ)", for interpreting decisions made by trained classifiers. Our method utilizes vector quantization to transform the representations of a classifier into a discrete, class-specific latent space. The class-specific codebooks act as a bottleneck that forces the interpreter to focus on the parts of the input data deemed relevant by the classifier for making a prediction. We evaluated our method through quantitative and qualitative studies and found that PIQ generates interpretations that are more easily understood by participants to our user studies when compared to several other interpretation methods in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Pharos-guided Attack (PgA) &#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#20195;&#34920;&#33391;&#24615;&#22270;&#20687;&#35821;&#20041;&#30340; Pharos &#20195;&#30721;&#23454;&#29616;&#24555;&#36895;&#19988;&#21487;&#38752;&#22320;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#28145;&#24230;&#21704;&#24076;&#26816;&#32034;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12658</link><description>&lt;p&gt;
&#28145;&#24230;&#21704;&#24076;&#26816;&#32034;&#26041;&#27861;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#21487;&#38752;&#39640;&#25928;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Reliable and Efficient Evaluation of Adversarial Robustness for Deep Hashing-Based Retrieval. (arXiv:2303.12658v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Pharos-guided Attack (PgA) &#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#20195;&#34920;&#33391;&#24615;&#22270;&#20687;&#35821;&#20041;&#30340; Pharos &#20195;&#30721;&#23454;&#29616;&#24555;&#36895;&#19988;&#21487;&#38752;&#22320;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#28145;&#24230;&#21704;&#24076;&#26816;&#32034;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21704;&#24076;&#25216;&#26415;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#30001;&#20110;&#26410;&#20805;&#20998;&#21033;&#29992;&#21407;&#22987;&#26679;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#25110;&#38656;&#35201;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36825;&#20123;&#20851;&#31995;&#65292;&#22312;&#35780;&#20272;&#28145;&#24230;&#21704;&#24076;&#27169;&#22411;&#23545;&#25239;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#26102;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#25110;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861; Pharos-guided Attack (PgA)&#65292;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#20195;&#34920;&#33391;&#24615;&#22270;&#20687;&#35821;&#20041;&#30340; Pharos &#20195;&#30721;&#65292;&#21487;&#24555;&#36895;&#19988;&#21487;&#38752;&#22320;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#20845;&#31181;&#25915;&#20987;&#26041;&#27861;&#30456;&#27604;&#65292;PgA &#22312;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#28145;&#24230;&#21704;&#24076;&#26816;&#32034;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep hashing has been extensively applied to massive image retrieval due to its efficiency and effectiveness. Recently, several adversarial attacks have been presented to reveal the vulnerability of deep hashing models against adversarial examples. However, existing attack methods suffer from degraded performance or inefficiency because they underutilize the semantic relations between original samples or spend a lot of time learning these relations with a deep neural network. In this paper, we propose a novel Pharos-guided Attack, dubbed PgA, to evaluate the adversarial robustness of deep hashing networks reliably and efficiently. Specifically, we design pharos code to represent the semantics of the benign image, which preserves the similarity to semantically relevant samples and dissimilarity to irrelevant ones. It is proven that we can quickly calculate the pharos code via a simple math formula. Accordingly, PgA can directly conduct a reliable and efficient attack on deep hashing-bas
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.12642</link><description>&lt;p&gt;
&#26234;&#33021;&#21270;&#30340;&#27665;&#20027;&#21270;&#65306;&#22810;&#31181;&#21547;&#20041;&#12289;&#30446;&#26631;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Democratising AI: Multiple Meanings, Goals, and Methods. (arXiv:2303.12642v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#21628;&#21505;&#23454;&#29616;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#20294;&#36825;&#20010;&#35789;&#35821;&#29992;&#26469;&#25351;&#20195;&#22810;&#31181;&#30446;&#26631;&#65292;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#36890;&#24120;&#35752;&#35770;&#30340;&#22235;&#31181;AI&#27665;&#20027;&#21270;&#31867;&#22411;&#65306;(1) AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;(2) AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;(3) AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;(4) AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#23454;&#29616;&#27599;&#31181;&#27665;&#20027;&#21270;&#24418;&#24335;&#30340;&#22810;&#20010;&#30446;&#26631;&#21644;&#26041;&#27861;&#12290;&#20174;&#26412;&#25991;&#20013;&#20027;&#35201;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;AI&#30340;&#27665;&#20027;&#21270;&#26159;&#19968;&#20010;&#22810;&#20803;&#32780;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#30340;&#27010;&#24565;&#65292;&#19981;&#24212;&#28151;&#28102;AI&#21487;&#35775;&#38382;&#24615;&#30340;&#25913;&#21892;&#12290;&#22914;&#26524;&#25105;&#20204;&#24819;&#35201;&#36229;&#36234;&#23545;&#26234;&#33021;&#21270;&#27665;&#20027;&#21270;&#30340;&#27169;&#31946;&#25215;&#35834;&#65292;&#36827;&#20837;&#20855;&#20307;&#25919;&#31574;&#21644;&#26435;&#34913;&#30340;&#29983;&#20135;&#24615;&#35752;&#35770;&#65292;&#25105;&#20204;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#36328;&#36234;&#20851;&#20110;&#20351;&#29992;&#12289;&#24320;&#21457;&#21644;&#21033;&#28070;&#30340;&#20915;&#31574;&#20013;&#23548;&#33322;&#26435;&#34913;&#21644;&#39118;&#38505;&#30340;&#20027;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous parties are calling for the democratisation of AI, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of AI democratisation that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to democratising AI, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#65288;R2R&#65289;&#65292;&#20801;&#35768;&#20174;&#19994;&#32773;&#36880;&#27493;&#35782;&#21035;&#12289;&#20943;&#23569;&#21644;&#37325;&#26032;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#20559;&#24046;&#65292;&#21253;&#25324;&#23547;&#25214;&#24322;&#24120;&#20540;&#12289;&#26816;&#27979;&#36127;&#36131;&#30340;&#25991;&#29289;&#12289;&#31354;&#38388;&#23450;&#20301;&#21644;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2303.12641</link><description>&lt;p&gt;
&#25581;&#31034;&#19982;&#20462;&#27491;&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#27169;&#22411;&#36845;&#20195;&#20559;&#24046;&#26657;&#27491;&#29983;&#21629;&#21608;&#26399;
&lt;/p&gt;
&lt;p&gt;
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models. (arXiv:2303.12641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#29983;&#21629;&#21608;&#26399;&#26694;&#26550;&#65288;R2R&#65289;&#65292;&#20801;&#35768;&#20174;&#19994;&#32773;&#36880;&#27493;&#35782;&#21035;&#12289;&#20943;&#23569;&#21644;&#37325;&#26032;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#20559;&#24046;&#65292;&#21253;&#25324;&#23547;&#25214;&#24322;&#24120;&#20540;&#12289;&#26816;&#27979;&#36127;&#36131;&#30340;&#25991;&#29289;&#12289;&#31354;&#38388;&#23450;&#20301;&#21644;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20250;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#24403;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#39640;&#39118;&#38505;&#20915;&#31574;&#65292;&#22914;&#30382;&#32932;&#30284;&#26816;&#27979;&#31561;&#21307;&#30103;&#24212;&#29992;&#26102;&#65292;&#36825;&#20250;&#24102;&#26469;&#39118;&#38505;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25581;&#31034;&#19982;&#20462;&#27491;&#8221;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#25972;&#20010;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#29983;&#21629;&#21608;&#26399;&#65292;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#36880;&#27493;&#35782;&#21035;&#12289;&#20943;&#23569;&#21644;(&#37325;&#26032;)&#35780;&#20272;&#34394;&#20551;&#27169;&#22411;&#34892;&#20026;&#65292;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art machine learning models often learn spurious correlations embedded in the training data. This poses risks when deploying these models for high-stake decision-making, such as in medical applications like skin cancer detection. To tackle this problem, we propose Reveal to Revise (R2R), a framework entailing the entire eXplainable Artificial Intelligence (XAI) life cycle, enabling practitioners to iteratively identify, mitigate, and (re-)evaluate spurious model behavior with a minimal amount of human interaction. In the first step (1), R2R reveals model weaknesses by finding outliers in attributions or through inspection of latent concepts learned by the model. Secondly (2), the responsible artifacts are detected and spatially localized in the input data, which is then leveraged to (3) revise the model behavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model correction, and (4) (re-)evaluate the model's performance and remaining sensitivity towards the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#25239;&#24615;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#20013;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#21644;&#23558;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#34892;&#20026;&#19982;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#36830;&#25509;&#65292;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#30340;&#36895;&#24230;&#21644;&#29983;&#25104;&#32467;&#26524;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12634</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23545;&#25239;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised counterfactual explanations. (arXiv:2303.12634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#25239;&#24615;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#20013;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#21644;&#23558;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#34892;&#20026;&#19982;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#36830;&#25509;&#65292;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#30340;&#36895;&#24230;&#21644;&#29983;&#25104;&#32467;&#26524;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#35299;&#37322;&#26159;&#29992;&#20110;&#26597;&#25214;&#26368;&#23567;&#24178;&#39044;&#29305;&#24449;&#20540;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#27169;&#22411;&#23558;&#39044;&#27979;&#26356;&#25913;&#20026;&#19981;&#21516;&#30340;&#36755;&#20986;&#25110;&#30446;&#26631;&#36755;&#20986;&#12290;&#26377;&#25928;&#30340;&#23545;&#25239;&#24615;&#35299;&#37322;&#24212;&#20855;&#26377;&#21487;&#33021;&#30340;&#29305;&#24449;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29983;&#25104;&#23545;&#25239;&#24615;&#35299;&#37322;&#30340;&#25361;&#25112;&#65292;&#20351;&#20854;&#22788;&#20110;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#65292;&#24182;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#23646;&#20110;&#30446;&#26631;&#31867;&#20998;&#24067;&#12290;&#36890;&#36807;&#22312;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#20013;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20010;&#35201;&#27714;&#12290;&#23558;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#34892;&#20026;&#19982;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#36830;&#25509;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#30340;&#36895;&#24230;&#21644;&#29983;&#25104;&#32467;&#26524;&#30340;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#25345;&#32493;&#21162;&#21147;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#33258;&#32534;&#30721;&#22120;&#22312;&#21322;&#30417;&#30563;&#29366;&#24577;&#19979;&#34987;&#35757;&#32451;&#26102;&#65292;&#23545;&#25239;&#24615;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations for machine learning models are used to find minimal interventions to the feature values such that the model changes the prediction to a different output or a target output. A valid counterfactual explanation should have likely feature values. Here, we address the challenge of generating counterfactual explanations that lie in the same data distribution as that of the training data and more importantly, they belong to the target class distribution. This requirement has been addressed through the incorporation of auto-encoder reconstruction loss in the counterfactual search process. Connecting the output behavior of the classifier to the latent space of the auto-encoder has further improved the speed of the counterfactual search process and the interpretability of the resulting counterfactual explanations. Continuing this line of research, we show further improvement in the interpretability of counterfactual explanations when the auto-encoder is trained in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#30340;&#24555;&#25463;&#26041;&#24335;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#20256;&#32479;&#30340;&#32531;&#35299;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12578</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#65306;&#32531;&#35299;&#31574;&#30053;&#21450;&#20854;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Reasoning Shortcuts: Mitigation Strategies and their Limitations. (arXiv:2303.12578v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#30340;&#24555;&#25463;&#26041;&#24335;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#20256;&#32479;&#30340;&#32531;&#35299;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#39044;&#27979;&#22120;&#23398;&#20064;&#20174;&#23376;&#31526;&#21495;&#36755;&#20837;&#21040;&#26356;&#39640;&#23618;&#27425;&#27010;&#24565;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#22312;&#36825;&#20010;&#20013;&#38388;&#34920;&#31034;&#19978;&#25191;&#34892;&#65288;&#27010;&#29575;&#65289;&#36923;&#36753;&#25512;&#29702;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#31526;&#21495;&#20808;&#39564;&#30693;&#35782;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#22312;&#36981;&#23432;&#30693;&#35782;&#30340;&#21069;&#25552;&#19979;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#30340;&#22909;&#22788;&#65292;&#22240;&#20026;&#23398;&#20064;&#30340;&#27010;&#24565;&#21487;&#20197;&#26356;&#22909;&#22320;&#34987;&#20154;&#31867;&#21033;&#30410;&#30456;&#20851;&#32773;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#35777;&#26126;&#20102;&#36825;&#31181;&#35774;&#32622;&#21463;&#21040;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#24847;&#22806;&#35821;&#20041;&#30340;&#27010;&#24565;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#39044;&#27979;&#65292;&#23548;&#33268;&#20102;&#24046;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#24182;&#25439;&#23475;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#24418;&#24335;&#32852;&#31995;&#65292;&#24182;&#30830;&#23450;&#20102;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#21487;&#33021;&#20986;&#29616;&#30340;&#24773;&#20917;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#33258;&#28982;&#32531;&#35299;&#31574;&#30053;&#65288;&#22914;&#37325;&#24314;&#21644;&#27010;&#24565;&#30417;&#30563;&#65289;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic predictors learn a mapping from sub-symbolic inputs to higher-level concepts and then carry out (probabilistic) logical inference on this intermediate representation. This setup offers clear advantages in terms of consistency to symbolic prior knowledge, and is often believed to provide interpretability benefits in that - by virtue of complying with the knowledge the learned concepts can be better understood by human stakeholders. However, it was recently shown that this setup is affected by reasoning shortcuts whereby predictions attain high accuracy by leveraging concepts with unintended semantics, yielding poor out-of-distribution performance and compromising interpretability. In this short paper, we establish a formal link between reasoning shortcuts and the optima of the loss function, and identify situations in which reasoning shortcuts can arise. Based on this, we discuss limitations of natural mitigation strategies such as reconstruction and concept supervision
&lt;/p&gt;</description></item><item><title>RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.12570</link><description>&lt;p&gt;
RepoCoder&#65306;&#36890;&#36807;&#36845;&#20195;&#26816;&#32034;&#21644;&#29983;&#25104;&#23454;&#29616;&#30340;&#20195;&#30721;&#23384;&#20648;&#24211;&#32423;&#21035;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12570
&lt;/p&gt;
&lt;p&gt;
RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#20219;&#21153;&#26159;&#22522;&#20110;&#20195;&#30721;&#24211;&#26356;&#24191;&#38420;&#19978;&#19979;&#25991;&#20013;&#32487;&#32493;&#32534;&#20889;&#26410;&#23436;&#25104;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#20294;&#26159;&#23545;&#20110;&#33258;&#21160;&#23436;&#25104;&#24037;&#20855;&#32780;&#35328;&#65292;&#24456;&#38590;&#21033;&#29992;&#25955;&#24067;&#22312;&#19981;&#21516;&#25991;&#20214;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RepoCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#20102;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#65292;&#20174;&#32780;&#20801;&#35768;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#23618;&#38754;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;RepoCoder &#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26816;&#32034;-&#29983;&#25104;&#27169;&#22411;&#65292;&#24357;&#21512;&#20102;&#26816;&#32034;&#19978;&#19979;&#25991;&#21644;&#39044;&#26399;&#23436;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;RepoEval&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26368;&#26032;&#21644;&#39640;&#36136;&#37327;&#30495;&#23454;&#19990;&#30028;&#30340;&#20195;&#30721;&#24211;&#65292;&#28085;&#30422;&#20102;&#34892;&#12289;API &#35843;&#29992;&#21644;&#20989;&#25968;&#20307;&#23436;&#25104;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model, which allows for the effective utilization of repository-level information for code completion and grants the ability to generate code at various levels of granularity. Furthermore, RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges the gap between retrieval context and the intended completion target. We also propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion sce
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WAE-MDP&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#19988;&#20855;&#26377;&#24179;&#34913;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#25928;&#26524;&#21644;&#35299;&#20915;&#19968;&#20123;&#23398;&#20064;&#32570;&#38519;&#30340;&#22810;&#26041;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.12558</link><description>&lt;p&gt;
Wasserstein&#33258;&#32534;&#30721;MDPs&#65306;&#20855;&#26377;&#22810;&#26041;&#20445;&#35777;&#30340;&#39640;&#25928;RL&#31574;&#30053;&#27491;&#24335;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees. (arXiv:2303.12558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12558
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WAE-MDP&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#19988;&#20855;&#26377;&#24179;&#34913;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#25928;&#26524;&#21644;&#35299;&#20915;&#19968;&#20123;&#23398;&#20064;&#32570;&#38519;&#30340;&#22810;&#26041;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26377;&#35768;&#22810;&#25104;&#21151;&#26696;&#20363;&#65292;&#20294;&#36890;&#36807;&#36825;&#20123;&#20808;&#36827;&#25216;&#26415;&#23398;&#20064;&#30340;&#20915;&#31574;&#32773;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#21463;&#21040;&#27491;&#24335;&#20445;&#35777;&#19981;&#36275;&#30340;&#38459;&#30861;&#12290;&#21464;&#20998;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;VAE-MDPs&#65289;&#26159;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#30340;&#21487;&#38752;&#26694;&#26550;&#12290;&#34429;&#28982;&#30456;&#20851;&#20445;&#35777;&#28085;&#30422;&#20102;&#23454;&#38469;&#38382;&#39064;&#30340;&#28385;&#36275;&#24615;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#65292;&#20294;VAE&#26041;&#27861;&#22240;&#32570;&#20047;&#25277;&#35937;&#21644;&#34920;&#31034;&#20445;&#35777;&#20197;&#25903;&#25345;&#28508;&#22312;&#26368;&#20248;&#21270;&#32780;&#36973;&#21463;&#22810;&#31181;&#23398;&#20064;&#32570;&#38519;&#65288;&#21518;&#39564;&#23849;&#22604;&#65292;&#23398;&#20064;&#36895;&#24230;&#24930;&#65292;&#21160;&#21147;&#23398;&#20272;&#35745;&#19981;&#33391;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Wasserstein&#33258;&#32534;&#30721;MDP&#65288;WAE-MDP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25191;&#34892;&#21407;&#22987;&#31574;&#30053;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#21644;&#25552;&#21462;&#20986;&#30340;&#31574;&#30053;&#20043;&#38388;&#30340;&#26368;&#20248;&#36716;&#36816;&#30340;&#24809;&#32602;&#24418;&#24335;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21033;&#20110;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#19978;&#36848;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20851;&#20110;&#24615;&#33021;&#21644;&#23433;&#20840;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;RL&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep reinforcement learning (DRL) has many success stories, the large-scale deployment of policies learned through these advanced techniques in safety-critical scenarios is hindered by their lack of formal guarantees. Variational Markov Decision Processes (VAE-MDPs) are discrete latent space models that provide a reliable framework for distilling formally verifiable controllers from any RL policy. While the related guarantees address relevant practical aspects such as the satisfaction of performance and safety properties, the VAE approach suffers from several learning flaws (posterior collapse, slow learning speed, poor dynamics estimates), primarily due to the absence of abstraction and representation guarantees to support latent optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent space model that fixes those issues by minimizing a penalized form of the optimal transport between the behaviors of the agent executing the original policy and the disti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#35270;&#35273;Transformer&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26725;&#22359;&#37325;&#26500;&#30340;&#28151;&#21512;&#35270;&#35273;Transformer&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20854;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12557</link><description>&lt;p&gt;
Q-HyViT: &#24102;&#26725;&#22359;&#37325;&#26500;&#30340;&#28151;&#21512;&#35270;&#35273;Transformer&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction. (arXiv:2303.12557v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#35270;&#35273;Transformer&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26725;&#22359;&#37325;&#26500;&#30340;&#28151;&#21512;&#35270;&#35273;Transformer&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20854;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;Transformer &#65288;ViT&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#20195;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292; ViT &#30340;&#39640;&#35745;&#31639;&#35201;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#28151;&#21512;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#32467;&#21512;&#21367;&#31215;&#21644;&#21464;&#21387;&#22120;&#23618;&#65292;&#24182;&#20248;&#21270;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#20351;&#32447;&#24615;&#22797;&#26434;&#24230;&#36798;&#21040;&#26368;&#22823;&#12290;&#27492;&#22806;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#34987;&#25552;&#20986;&#20316;&#20026;&#32531;&#35299;&#35745;&#31639;&#35201;&#27714;&#30340;&#19968;&#31181;&#25163;&#27573;&#12290;&#23558;&#37327;&#21270;&#25216;&#26415;&#21644;&#39640;&#25928;&#30340;&#28151;&#21512;&#21464;&#21387;&#22120;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#23545;&#20110;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#21152;&#36895;&#35270;&#35273;transformer&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#27809;&#26377;&#30740;&#31350;&#23558;&#37327;&#21270;&#24212;&#29992;&#20110;&#39640;&#25928;&#30340;&#28151;&#21512;&#21464;&#21387;&#22120;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#23558;&#29616;&#26377;&#30340;ViT PTQ&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#39640;&#25928;&#30340;&#28151;&#21512;transformer&#26550;&#26500;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#31934;&#24230;&#19979;&#38477;&#65292;&#30001;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;Q-HyViT&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, vision transformers (ViT) have replaced convolutional neural network models in numerous tasks, including classification, detection, and segmentation. However, the high computational requirements of ViTs hinder their widespread implementation. To address this issue, researchers have proposed efficient hybrid transformer architectures that combine convolutional and transformer layers and optimize attention computation for linear complexity. Additionally, post-training quantization has been proposed as a means of mitigating computational demands. Combining quantization techniques and efficient hybrid transformer structures is crucial to maximize the acceleration of vision transformers on mobile devices. However, no prior investigation has applied quantization to efficient hybrid transformers. In this paper, at first, we discover that the straightforward manner to apply the existing PTQ methods for ViT to efficient hybrid transformers results in a drastic accuracy drop due to the
&lt;/p&gt;</description></item><item><title>DevelSet&#26159;&#19968;&#31181;GPU&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21152;&#36895;&#30340;&#27700;&#24179;&#38598;&#37329;&#23646;&#20809;&#21051;&#25513;&#27169;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26354;&#29575;&#39033;&#20943;&#23569;&#25513;&#27169;&#22797;&#26434;&#24230;&#65292;&#24182;&#24212;&#29992;GPU&#21152;&#36895;&#26469;&#20811;&#26381;&#35745;&#31639;&#29942;&#39048;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#21487;&#25171;&#21360;&#24615;&#21644;&#24555;&#36895;&#36845;&#20195;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2303.12529</link><description>&lt;p&gt;
DevelSet: &#28145;&#24230;&#31070;&#32463;&#27700;&#24179;&#38598;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#20809;&#21051;&#25513;&#27169;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
DevelSet: Deep Neural Level Set for Instant Mask Optimization. (arXiv:2303.12529v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12529
&lt;/p&gt;
&lt;p&gt;
DevelSet&#26159;&#19968;&#31181;GPU&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21152;&#36895;&#30340;&#27700;&#24179;&#38598;&#37329;&#23646;&#20809;&#21051;&#25513;&#27169;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26354;&#29575;&#39033;&#20943;&#23569;&#25513;&#27169;&#22797;&#26434;&#24230;&#65292;&#24182;&#24212;&#29992;GPU&#21152;&#36895;&#26469;&#20811;&#26381;&#35745;&#31639;&#29942;&#39048;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#21487;&#25171;&#21360;&#24615;&#21644;&#24555;&#36895;&#36845;&#20195;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#24037;&#33402;&#33410;&#28857;&#20013;&#29305;&#24449;&#23610;&#23544;&#30340;&#19981;&#26029;&#32553;&#23567;&#65292;&#25513;&#27169;&#20248;&#21270;&#22312;&#20256;&#32479;&#35774;&#35745;&#27969;&#31243;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#24182;&#20276;&#38543;&#30528;&#20809;&#21051;&#36817;&#20284;&#26657;&#27491;(OPC)&#26041;&#27861;&#20013;&#35745;&#31639;&#24320;&#38144;&#30340;&#29190;&#28856;&#24335;&#22686;&#38271;&#12290;&#26368;&#36817;, &#21453;&#21521;&#20809;&#21051;&#25216;&#26415;(ILT)&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#35270;, &#24182;&#22312;&#26032;&#20852;&#30340; OPC &#35299;&#20915;&#26041;&#26696;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;ILT&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#65292;&#35201;&#20040;&#25513;&#27169;&#21360;&#21047;&#24615;&#33021;&#21644;&#21487;&#21046;&#36896;&#24615;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DevelSet&#65292;&#19968;&#31181;GPU&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21152;&#36895;&#30340;&#27700;&#24179;&#38598;&#37329;&#23646;&#20809;&#21051;&#25513;&#27169;&#32508;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24341;&#20837;&#26354;&#29575;&#39033;&#26469;&#38477;&#20302;&#25513;&#27169;&#22797;&#26434;&#24230;&#24182;&#24212;&#29992;GPU&#21152;&#36895;&#26469;&#20811;&#26381;&#35745;&#31639;&#29942;&#39048;&#65292;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#27700;&#24179;&#38598;&#30340;ILT&#31639;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21487;&#25171;&#21360;&#24615;&#21644;&#24555;&#36895;&#36845;&#20195;&#25910;&#25947;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27700;&#24179;&#38598;&#22266;&#26377;&#21407;&#29702;&#24039;&#22937;&#22320;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the feature size continuously shrinking in advanced technology nodes, mask optimization is increasingly crucial in the conventional design flow, accompanied by an explosive growth in prohibitive computational overhead in optical proximity correction (OPC) methods. Recently, inverse lithography technique (ILT) has drawn significant attention and is becoming prevalent in emerging OPC solutions. However, ILT methods are either time-consuming or in weak performance of mask printability and manufacturability. In this paper, we present DevelSet, a GPU and deep neural network (DNN) accelerated level set OPC framework for metal layer. We first improve the conventional level set-based ILT algorithm by introducing the curvature term to reduce mask complexity and applying GPU acceleration to overcome computational bottlenecks. To further enhance printability and fast iterative convergence, we propose a novel deep neural network delicately designed with level set intrinsic principles to facil
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#24494;&#35843;&#26694;&#26550;&#65292;&#38024;&#23545;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#23398;&#20064;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#23427;&#37319;&#29992;&#19968;&#31181;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39592;&#26550;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#27599;&#20010;&#20219;&#21153;&#30340;&#35821;&#35328;&#29305;&#23450;&#30340;&#24494;&#35843;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.12489</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#12289;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-shot Multimodal Multitask Multilingual Learning. (arXiv:2303.12489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#24494;&#35843;&#26694;&#26550;&#65292;&#38024;&#23545;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#23398;&#20064;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#23427;&#37319;&#29992;&#19968;&#31181;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39592;&#26550;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#27599;&#20010;&#20219;&#21153;&#30340;&#35821;&#35328;&#29305;&#23450;&#30340;&#24494;&#35843;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#20027;&#35201;&#26159;&#22312;&#26500;&#24314;&#21333;&#27169;&#24577;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#22810;&#20219;&#21153;&#23398;&#20064;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#25991;&#29486;&#37117;&#26159;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30340;&#65292;&#38656;&#35201;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#23548;&#33268;&#20102;&#32467;&#26524;&#30340;&#24046;&#24322;&#65292;&#21462;&#20915;&#20110;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20250;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#23384;&#20648;&#25104;&#26412;&#65292;&#26368;&#32456;&#23548;&#33268;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#27599;&#27425;&#36827;&#34892;&#39044;&#27979;&#26102;&#37117;&#35201;&#36890;&#36807;&#27169;&#22411;&#36816;&#34892;&#25152;&#26377;&#25552;&#31034;&#30340;&#31034;&#20363;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#33539;&#24335;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36991;&#20813;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#22312;&#20219;&#21153;&#30340;&#22522;&#30784;&#19978;&#20197;&#19968;&#27425;&#24615;&#30340;&#20195;&#20215;&#24494;&#35843;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#32463;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#24494;&#35843;&#26694;&#26550;&#65292;&#38024;&#23545;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#23398;&#20064;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39592;&#26550;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#27599;&#20010;&#20219;&#21153;&#30340;&#35821;&#35328;&#29305;&#23450;&#30340;&#24494;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While few-shot learning as a transfer learning paradigm has gained significant traction for scenarios with limited data, it has primarily been explored in the context of building unimodal and unilingual models. Furthermore, a significant part of the existing literature in the domain of few-shot multitask learning perform in-context learning which requires manually generated prompts as the input, yielding varying outcomes depending on the level of manual prompt-engineering. In addition, in-context learning suffers from substantial computational, memory, and storage costs which eventually leads to high inference latency because it involves running all of the prompt's examples through the model every time a prediction is made. In contrast, methods based on the transfer learning via the fine-tuning paradigm avoid the aforementioned issues at a one-time cost of fine-tuning weights on a per-task basis. However, such methods lack exposure to few-shot multimodal multitask learning. In this pap
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#26631;&#35760;&#20195;&#20215;&#22823;&#65292;&#26631;&#35760;&#19981;&#36275;&#12290;&#22240;&#27492;&#21457;&#23637;&#20102;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#21644;&#24369;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#36825;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.12484</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#30340;&#25361;&#25112;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions. (arXiv:2303.12484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12484
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#26631;&#35760;&#20195;&#20215;&#22823;&#65292;&#26631;&#35760;&#19981;&#36275;&#12290;&#22240;&#27492;&#21457;&#23637;&#20102;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#21644;&#24369;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#36825;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#36805;&#36895;&#21457;&#23637;&#65292;&#24182;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#25910;&#38598;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#38656;&#35201;&#26114;&#36149;&#32791;&#26102;&#12290;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65288;MIA&#65289;&#39046;&#22495;&#65292;&#25968;&#25454;&#26377;&#38480;&#65292;&#26631;&#31614;&#24456;&#38590;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#26631;&#35760;&#25968;&#25454;&#20197;&#21450;&#38750;&#26631;&#35760;&#21644;&#24369;&#26631;&#35760;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#36817;300&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#24191;&#27867;&#35843;&#26597;&#65292;&#20197;&#20840;&#38754;&#27010;&#36848;&#26368;&#26032;&#36827;&#23637;&#30340;&#39640;&#25928;&#26631;&#35760;&#23398;&#20064;&#31574;&#30053;&#22312;MIA&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#39640;&#25928;&#26631;&#35760;&#23398;&#20064;&#30340;&#32972;&#26223;&#65292;&#24182;&#23558;&#19981;&#21516;&#26041;&#26696;&#30340;&#26041;&#27861;&#24402;&#31867;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#27599;&#31181;&#26041;&#26696;&#35814;&#32454;&#30740;&#31350;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#65292;&#35206;&#30422;&#20102;&#19981;&#20165;&#26159;&#26631;&#20934;&#31574;&#30053;&#65292;&#36824;&#21253;&#25324;&#20351;&#29992;&#21518;&#22788;&#29702;&#21644;&#38598;&#21512;&#26041;&#27861;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has seen rapid growth in recent years and achieved state-of-the-art performance in a wide range of applications. However, training models typically requires expensive and time-consuming collection of large quantities of labeled data. This is particularly true within the scope of medical imaging analysis (MIA), where data are limited and labels are expensive to be acquired. Thus, label-efficient deep learning methods are developed to make comprehensive use of the labeled data as well as the abundance of unlabeled and weak-labeled data. In this survey, we extensively investigated over 300 recent papers to provide a comprehensive overview of recent progress on label-efficient learning strategies in MIA. We first present the background of label-efficient learning and categorize the approaches into different schemes. Next, we examine the current state-of-the-art methods in detail through each scheme. Specifically, we provide an in-depth investigation, covering not only canonic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21171;&#21153;&#20998;&#24037;&#20013;&#30340;&#22806;&#37096;&#24615;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#27169;&#22411;&#32771;&#34385;&#21040;&#20854;&#20182;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.12446</link><description>&lt;p&gt;
&#21171;&#21153;&#20998;&#24037;&#20013;&#30340;&#22806;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Externalities in Chore Division. (arXiv:2303.12446v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21171;&#21153;&#20998;&#24037;&#20013;&#30340;&#22806;&#37096;&#24615;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#27169;&#22411;&#32771;&#34385;&#21040;&#20854;&#20182;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21171;&#21153;&#20998;&#24037;&#38382;&#39064;&#27169;&#25311;&#20102;&#19981;&#21516;&#30340;&#36164;&#28304;&#22312;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;&#22312;&#20844;&#24179;&#20998;&#37197;&#38382;&#39064;&#20013;&#65292;&#27599;&#20010;&#20195;&#29702;&#21482;&#20174;&#33258;&#24049;&#30340;&#36164;&#28304;&#20013;&#33719;&#24471;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#20195;&#29702;&#20063;&#21487;&#33021;&#20851;&#27880;&#20998;&#37197;&#32473;&#20854;&#20182;&#20195;&#29702;&#30340;&#36164;&#28304;&#65292;&#36825;&#20123;&#22806;&#37096;&#24615;&#33258;&#28982;&#22320;&#20986;&#29616;&#22312;&#20844;&#24179;&#20998;&#37197;&#30340;&#24773;&#20917;&#20013;&#12290;Branzei&#31561;&#20154;&#36890;&#36807;&#25193;&#23637;&#32463;&#20856;&#27169;&#22411;&#20197;&#32771;&#34385;&#22806;&#37096;&#24615;&#65292;&#25512;&#24191;&#20102;&#27604;&#20363;&#21644;&#26080;&#23241;&#22930;&#24615;&#30340;&#32463;&#20856;&#24605;&#24819;&#12290;&#65288;Branzei et al&#12290;&#65292;IJCAI 2013&#65289;
&lt;/p&gt;
&lt;p&gt;
The chore division problem simulates the fair division of a heterogeneous undesirable resource among several agents. In the fair division problem, each agent only gains value from its own piece. Agents may, however, also be concerned with the pieces given to other agents; these externalities naturally appear in fair division situations. Branzei et ai. (Branzei et al., IJCAI 2013) generalize the classical ideas of proportionality and envy-freeness while extending the classical model to account for externalities.
&lt;/p&gt;</description></item><item><title>MEDIMP&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24335;&#23398;&#20064;&#32958;&#31227;&#26893;DCE MRI&#30340;&#21307;&#23398;&#22270;&#20687;&#21644;&#25552;&#31034;&#27169;&#22411;&#65292;&#21033;&#29992;&#32852;&#21512;&#25991;&#26412;-&#22270;&#20687;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.12445</link><description>&lt;p&gt;
MEDIMP: &#29992;&#20110;&#32958;&#31227;&#26893;&#34920;&#31034;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#21644;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
MEDIMP: Medical Images and Prompts for renal transplant representation learning. (arXiv:2303.12445v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12445
&lt;/p&gt;
&lt;p&gt;
MEDIMP&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24335;&#23398;&#20064;&#32958;&#31227;&#26893;DCE MRI&#30340;&#21307;&#23398;&#22270;&#20687;&#21644;&#25552;&#31034;&#27169;&#22411;&#65292;&#21033;&#29992;&#32852;&#21512;&#25991;&#26412;-&#22270;&#20687;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32958;&#31227;&#26893;&#24050;&#25104;&#20026;&#32456;&#26411;&#26399;&#32958;&#33039;&#30142;&#30149;&#30340;&#26368;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#30001;&#20110;&#22797;&#26434;&#21407;&#22240;&#65292;&#31227;&#26893;&#24930;&#24615;&#21151;&#33021;&#38556;&#30861;&#30340;&#37325;&#22823;&#39118;&#38505;&#20173;&#28982;&#23384;&#22312;&#65292;&#21487;&#33021;&#23548;&#33268;&#31227;&#26893;&#22833;&#36133;&#12290;&#21307;&#23398;&#24433;&#20687;&#22312;&#32958;&#31227;&#26893;&#30417;&#27979;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#31227;&#26893;&#30417;&#30563;&#20855;&#26377;&#22810;&#23398;&#31185;&#29305;&#28857;&#65292;&#23588;&#20854;&#26159;&#32467;&#21512;&#20102;&#32958;&#33039;&#23398;&#12289;&#27852;&#23615;&#23398;&#21644;&#25918;&#23556;&#23398;&#65292;&#22312;&#36825;&#31181;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#25968;&#25454;&#20013;&#35782;&#21035;&#24378;&#22823;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#29992;&#20110;&#39044;&#21518;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;MEDIMP&#8212;&#8212;&#21307;&#23398;&#24433;&#20687;&#21644;&#25552;&#31034;&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24335;&#23398;&#20064;&#32958;&#31227;&#26893;&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;DCE MRI&#65289;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#24615;&#20020;&#24202;&#29983;&#29289;&#25968;&#25454;&#32763;&#35793;&#25104;&#25991;&#26412;&#25552;&#31034;&#26469;&#23436;&#25104;&#12290;MEDIMP&#22522;&#20110;&#32852;&#21512;&#25991;&#26412;-&#22270;&#20687;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#25191;&#34892;&#36825;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Renal transplantation emerges as the most effective solution for end-stage renal disease. Occurring from complex causes, a substantial risk of transplant chronic dysfunction persists and may lead to graft loss. Medical imaging plays a substantial role in renal transplant monitoring in clinical practice. However, graft supervision is multi-disciplinary, notably joining nephrology, urology, and radiology, while identifying robust biomarkers from such high-dimensional and complex data for prognosis is challenging. In this work, taking inspiration from the recent success of Large Language Models (LLMs), we propose MEDIMP -- Medical Images and Prompts -- a model to learn meaningful multi-modal representations of renal transplant Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE MRI) by incorporating structural clinicobiological data after translating them into text prompts. MEDIMP is based on contrastive learning from joint text-image paired embeddings to perform this challenging ta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;$P^{3}O$&#30340;&#19977;&#38454;&#27573;DRL&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#38382;&#26469;&#36716;&#31227;&#35270;&#35273;&#34920;&#31034;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#35270;&#35273;&#20256;&#36755;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.12371</link><description>&lt;p&gt;
$P^{3}O$: Prompting&#26041;&#27861;&#20013;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
$P^{3}O$: Transferring Visual Representations for Reinforcement Learning via Prompting. (arXiv:2303.12371v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;$P^{3}O$&#30340;&#19977;&#38454;&#27573;DRL&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#38382;&#26469;&#36716;&#31227;&#35270;&#35273;&#34920;&#31034;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#35270;&#35273;&#20256;&#36755;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#23558;&#23398;&#21040;&#30340;&#31574;&#30053;&#36716;&#31227;&#33267;&#35270;&#35273;&#36755;&#20837;&#19981;&#21516;&#30340;&#26032;&#29615;&#22659;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;($P^{3}O$)&#30340;&#19977;&#38454;&#27573;DRL&#31639;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;Prompt&#20351;&#24471;&#35270;&#35273;&#34920;&#31034;&#20174;&#30446;&#26631;&#29615;&#22659;&#20256;&#36882;&#21040;&#28304;&#29615;&#22659;&#12290;$P^{3}O$&#30340;&#36807;&#31243;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;:&#39044;&#35757;&#32451;&#12289;Prompting&#21644;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Prompt-Transformer&#29992;&#20110;&#34920;&#31034;&#36716;&#25442;&#65292;&#24182;&#38024;&#23545;&#30446;&#26631;&#29615;&#22659;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#35757;&#32451;&#36807;&#31243;&#65292;&#35757;&#32451;Prompt-Transformer&#65292;&#32780;DRL&#31649;&#36947;&#30340;&#20854;&#20313;&#37096;&#20998;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#22312;OpenAI CarRacing&#35270;&#39057;&#28216;&#25103;&#19978;&#23454;&#26045;$P^{3}O$&#24182;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;$P^{3}O$&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#36716;&#31227;&#26041;&#26696;&#65292;&#32780;&#19988;&#33021;&#35753;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#35270;&#35273;&#36755;&#20837;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is important for deep reinforcement learning (DRL) algorithms to transfer their learned policies to new environments that have different visual inputs. In this paper, we introduce Prompt based Proximal Policy Optimization ($P^{3}O$), a three-stage DRL algorithm that transfers visual representations from a target to a source environment by applying prompting. The process of $P^{3}O$ consists of three stages: pre-training, prompting, and predicting. In particular, we specify a prompt-transformer for representation conversion and propose a two-step training process to train the prompt-transformer for the target environment, while the rest of the DRL pipeline remains unchanged. We implement $P^{3}O$ and evaluate it on the OpenAI CarRacing video game. The experimental results show that $P^{3}O$ outperforms the state-of-the-art visual transferring schemes. In particular, $P^{3}O$ allows the learned policies to perform well in environments with different visual inputs, which is much more e
&lt;/p&gt;</description></item><item><title>ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.12364</link><description>&lt;p&gt;
ExBEHRT&#65306;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#25193;&#23637;Transformer&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes &amp; Progressions. (arXiv:2303.12364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12364
&lt;/p&gt;
&lt;p&gt;
ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ExBEHRT&#65292;&#23427;&#26159;BEHRT&#65288;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;BERT&#65289;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#29305;&#24449;&#31354;&#38388;&#20174;&#20165;&#32771;&#34385;&#35786;&#26029;&#21644;&#24739;&#32773;&#24180;&#40836;&#25193;&#23637;&#21040;&#21253;&#25324;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#65292;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#20020;&#24202;&#29305;&#24449;&#12289;&#29983;&#21629;&#20307;&#24449;&#12289;&#21560;&#28895;&#29366;&#24577;&#12289;&#35786;&#26029;&#12289;&#25163;&#26415;&#12289;&#33647;&#29289;&#21644;&#23454;&#39564;&#23460;&#26816;&#26597;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#32479;&#19968;&#19981;&#21516;&#29305;&#24449;&#30340;&#39057;&#29575;&#21644;&#26102;&#38388;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38468;&#21152;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#20445;&#35777;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39044;&#26399;&#26799;&#24230;&#30340;&#25913;&#36827;&#26041;&#27861;&#23545;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#20197;&#21069;&#26410;&#24212;&#29992;&#20110;&#23558;EHR&#25968;&#25454;&#19982;Transformer&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#65292;&#22914;&#29305;&#24449;&#21644;&#20196;&#29260;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32959;&#30244;&#23398;&#24739;&#32773;&#30340;&#27169;&#22411;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ExBEHRT&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT applied to electronic health records), and apply different algorithms to interpret its results. While BEHRT considers only diagnoses and patient age, we extend the feature space to several multimodal records, namely demographics, clinical characteristics, vital signs, smoking status, diagnoses, procedures, medications, and laboratory tests, by applying a novel method to unify the frequencies and temporal dimensions of the different features. We show that additional features significantly improve model performance for various downstream tasks in different diseases. To ensure robustness, we interpret model predictions using an adaptation of expected gradients, which has not been previously applied to transformers with EHR data and provides more granular interpretations than previous approaches such as feature and token importances. Furthermore, by clustering the model representations of oncology patients, we show tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;WPGD&#65292;&#21033;&#29992;Wasserstein&#36317;&#31163;&#38480;&#21046;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#24615;&#20363;&#23376;&#20043;&#38388;&#30340;&#25200;&#21160;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#22320;&#23545;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.12357</link><description>&lt;p&gt;
Wasserstein&#31354;&#38388;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#29992;&#20110;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Adversarial Examples on Univariant Time Series Data. (arXiv:2303.12357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;WPGD&#65292;&#21033;&#29992;Wasserstein&#36317;&#31163;&#38480;&#21046;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#24615;&#20363;&#23376;&#20043;&#38388;&#30340;&#25200;&#21160;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#22320;&#23545;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#20363;&#23376;&#26159;&#36890;&#36807;&#21521;&#27491;&#24120;&#26679;&#26412;&#28155;&#21152;&#26080;&#27861;&#21306;&#20998;&#30340;&#25200;&#21160;&#26469;&#27450;&#39575;&#33391;&#22909;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#20854;&#38169;&#35823;&#22320;&#20998;&#31867;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#32972;&#26223;&#19979;&#65292;&#21306;&#20998;&#24230;&#30340;&#27010;&#24565;&#36890;&#24120;&#30001;$L_{\infty}$&#25110;&#20854;&#20182;&#35268;&#33539;&#26469;&#38480;&#23450;&#65292;&#20294;&#26159;&#36825;&#20123;&#35268;&#33539;&#19981;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21306;&#20998;&#24230;&#34913;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20986;&#20102;Wasserstein&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#65292;&#24182;&#21033;&#29992;Wasserstein&#36317;&#31163;&#26469;&#38480;&#23450;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#24615;&#20363;&#23376;&#20043;&#38388;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Wasserstein&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;WPGD&#65289;&#65292;&#19968;&#31181;&#25200;&#21160;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;1D&#31354;&#38388;&#20013;Wasserstein&#36317;&#31163;&#30340;&#23553;&#38381;&#24418;&#24335;&#35299;&#26469;&#35745;&#31639;WPGD&#30340;&#25237;&#24433;&#27493;&#39588;&#65292;&#20197;&#26368;&#23567;&#21270;&#25200;&#21160;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#27493;&#25237;&#24433;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#36827;&#34892;&#22312;Wasserstein&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;WPGD&#33021;&#22815;&#25104;&#21151;&#22320;&#23545;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples are crafted by adding indistinguishable perturbations to normal examples in order to fool a well-trained deep learning model to misclassify. In the context of computer vision, this notion of indistinguishability is typically bounded by $L_{\infty}$ or other norms. However, these norms are not appropriate for measuring indistinguishiability for time series data. In this work, we propose adversarial examples in the Wasserstein space for time series data for the first time and utilize Wasserstein distance to bound the perturbation between normal examples and adversarial examples. We introduce Wasserstein projected gradient descent (WPGD), an adversarial attack method for perturbing univariant time series data. We leverage the closed-form solution of Wasserstein distance in the 1D space to calculate the projection step of WPGD efficiently with the gradient descent method. We further propose a two-step projection so that the search of adversarial examples in the Wassers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#21452;&#37325;&#21512;&#21516;&#38382;&#39064;&#20013;&#33021;&#22815;&#33258;&#20027;&#35774;&#35745;&#28608;&#21169;&#30456;&#23481;&#30340;&#21512;&#21516;&#65292;&#26080;&#38656;&#22806;&#37096;&#24341;&#23548;&#25110;&#36890;&#20449;&#65292;&#24182;&#19988;&#19981;&#21516;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#22996;&#25176;&#20154;&#21487;&#20197;&#37319;&#29992;&#28151;&#21512;&#21644;&#38646;&#21644;&#21338;&#24328;&#34892;&#20026;&#65292;&#26356;&#20855;&#26234;&#33021;&#30340;&#22996;&#25176;&#20154;&#24448;&#24448;&#20250;&#21464;&#24471;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.12350</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#21452;&#37325;&#21512;&#21516;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence and Dual Contract. (arXiv:2303.12350v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#21452;&#37325;&#21512;&#21516;&#38382;&#39064;&#20013;&#33021;&#22815;&#33258;&#20027;&#35774;&#35745;&#28608;&#21169;&#30456;&#23481;&#30340;&#21512;&#21516;&#65292;&#26080;&#38656;&#22806;&#37096;&#24341;&#23548;&#25110;&#36890;&#20449;&#65292;&#24182;&#19988;&#19981;&#21516;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#22996;&#25176;&#20154;&#21487;&#20197;&#37319;&#29992;&#28151;&#21512;&#21644;&#38646;&#21644;&#21338;&#24328;&#34892;&#20026;&#65292;&#26356;&#20855;&#26234;&#33021;&#30340;&#22996;&#25176;&#20154;&#24448;&#24448;&#20250;&#21464;&#24471;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#24555;&#36895;&#36827;&#27493;&#65292;&#20154;&#20204;&#24076;&#26395;&#31639;&#27861;&#24456;&#24555;&#23601;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#20195;&#20154;&#31867;&#20915;&#31574;&#32773;&#65292;&#20363;&#22914;&#21512;&#21516;&#35774;&#35745;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#30001;&#20154;&#24037;&#26234;&#33021;&#65288;&#22810;&#26234;&#33021;&#20307;Q&#23398;&#20064;&#65289;&#39537;&#21160;&#30340;&#31639;&#27861;&#22312;&#21452;&#37325;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#30340;&#32463;&#20856;&#8220;&#21452;&#37325;&#21512;&#21516;&#8221;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20123;AI&#31639;&#27861;&#21487;&#20197;&#33258;&#20027;&#23398;&#20064;&#35774;&#35745;&#21512;&#36866;&#30340;&#28608;&#21169;&#30456;&#23481;&#21512;&#21516;&#65292;&#32780;&#26080;&#38656;&#22806;&#37096;&#24341;&#23548;&#25110;&#32773;&#23427;&#20204;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#30001;&#19981;&#21516;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#22996;&#25176;&#20154;&#21487;&#20197;&#37319;&#29992;&#28151;&#21512;&#21644;&#38646;&#21644;&#21338;&#24328;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#26356;&#20855;&#26234;&#33021;&#30340;&#22996;&#25176;&#20154;&#24448;&#24448;&#20250;&#21464;&#24471;&#21512;&#20316;&#65292;&#32780;&#26234;&#33021;&#36739;&#20302;&#30340;&#22996;&#25176;&#20154;&#21017;&#20250;&#20986;&#29616;&#20869;&#29983;&#24615;&#36817;&#35270;&#24182;&#20542;&#21521;&#20110;&#31454;&#20105;&#12290;&#22312;&#26368;&#20248;&#21512;&#21516;&#19979;&#65292;&#20195;&#29702;&#30340;&#36739;&#20302;&#21512;&#21516;&#28608;&#21169;&#30001;&#22996;&#25176;&#20154;&#20043;&#38388;&#30340;&#21246;&#32467;&#31574;&#30053;&#32500;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the dramatic progress of artificial intelligence algorithms in recent times, it is hoped that algorithms will soon supplant human decision-makers in various fields, such as contract design. We analyze the possible consequences by experimentally studying the behavior of algorithms powered by Artificial Intelligence (Multi-agent Q-learning) in a workhorse \emph{dual contract} model for dual-principal-agent problems. We find that the AI algorithms autonomously learn to design incentive-compatible contracts without external guidance or communication among themselves. We emphasize that the principal, powered by distinct AI algorithms, can play mixed-sum behavior such as collusion and competition. We find that the more intelligent principals tend to become cooperative, and the less intelligent principals are endogenizing myopia and tend to become competitive. Under the optimal contract, the lower contract incentive to the agent is sustained by collusive strategies between the principals
&lt;/p&gt;</description></item><item><title>NUWA-XL&#37319;&#29992;&#25193;&#25955;&#36807;&#31243;&#65292;&#22312;&#26497;&#38271;&#35270;&#39057;&#30340;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#30001;&#31895;&#21040;&#32454;&#30340;&#36807;&#31243;&#65292;&#20943;&#23567;&#20102;&#35757;&#32451;-&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#20351;&#24471;&#25152;&#26377;&#30340;&#27573;&#33853;&#37117;&#21487;&#20197;&#24182;&#34892;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.12346</link><description>&lt;p&gt;
NUWA-XL: &#25193;&#25955;&#36807;&#31243;&#22312;&#26497;&#38271;&#35270;&#39057;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation. (arXiv:2303.12346v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12346
&lt;/p&gt;
&lt;p&gt;
NUWA-XL&#37319;&#29992;&#25193;&#25955;&#36807;&#31243;&#65292;&#22312;&#26497;&#38271;&#35270;&#39057;&#30340;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#30001;&#31895;&#21040;&#32454;&#30340;&#36807;&#31243;&#65292;&#20943;&#23567;&#20102;&#35757;&#32451;-&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#20351;&#24471;&#25152;&#26377;&#30340;&#27573;&#33853;&#37117;&#21487;&#20197;&#24182;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NUWA-XL&#25193;&#25955;&#36807;&#31243;&#26550;&#26500;&#65292;&#29992;&#20110;&#26497;&#38271;&#35270;&#39057;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#30001;&#31895;&#21040;&#32454;&#8221;&#30340;&#36807;&#31243;&#65292;&#24212;&#29992;&#20840;&#23616;&#25193;&#25955;&#27169;&#22411;&#22312;&#25972;&#20010;&#26102;&#38388;&#33539;&#22260;&#20869;&#29983;&#25104;&#20851;&#38190;&#24103;&#65292;&#24182;&#19988;&#36882;&#24402;&#22320;&#24212;&#29992;&#26412;&#22320;&#25193;&#25955;&#27169;&#22411;&#22635;&#20805;&#30456;&#37051;&#24103;&#20043;&#38388;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#20351;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#22312;&#38271;&#35270;&#39057;&#65288;3376&#24103;&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23567;&#20102;&#35757;&#32451;-&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#20351;&#24471;&#25152;&#26377;&#30340;&#27573;&#33853;&#37117;&#21487;&#20197;&#24182;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a ``coarse-to-fine'' process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. This simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap, and makes it possible to generate all segments in parallel. To evaluate our model, we build FlintstonesHD dataset, a new benchmark for long video generation. Experiments show that our model not only generates high-quality long vid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25353;&#38656;&#20056;&#36710;&#26381;&#21153;&#25805;&#20316;&#30340;&#22810;&#21151;&#33021;&#27169;&#25311;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#65292;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#25805;&#20316;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#35780;&#20272;&#65292;&#20248;&#21270;&#31639;&#27861;&#35843;&#25972;&#20197;&#21450;&#23454;&#26102;&#21160;&#24577;&#25805;&#20316;&#25511;&#21046;&#31561;&#26041;&#38754;&#22343;&#33021;&#21457;&#25381;&#20316;&#29992;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#30340;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2303.12336</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25353;&#38656;&#20056;&#36710;&#26381;&#21153;&#36816;&#33829;&#30340;&#22810;&#21151;&#33021;&#27169;&#25311;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
A multi-functional simulation platform for on-demand ride service operations. (arXiv:2303.12336v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25353;&#38656;&#20056;&#36710;&#26381;&#21153;&#25805;&#20316;&#30340;&#22810;&#21151;&#33021;&#27169;&#25311;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#65292;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#25805;&#20316;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#35780;&#20272;&#65292;&#20248;&#21270;&#31639;&#27861;&#35843;&#25972;&#20197;&#21450;&#23454;&#26102;&#21160;&#24577;&#25805;&#20316;&#25511;&#21046;&#31561;&#26041;&#38754;&#22343;&#33021;&#21457;&#25381;&#20316;&#29992;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#25353;&#38656;&#20056;&#36710;&#26381;&#21153;&#25110;&#20056;&#36710;&#20849;&#20139;&#26381;&#21153;&#39134;&#36895;&#21457;&#23637;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#25968;&#23398;&#27169;&#22411;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24110;&#21161;&#20056;&#36710;&#20849;&#20139;&#24179;&#21488;&#35774;&#35745;&#26356;&#39640;&#25928;&#30340;&#36816;&#33829;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#26412;&#21644;&#21487;&#38752;&#24615;&#38382;&#39064;&#65288;&#23545;&#20110;&#30495;&#23454;&#25805;&#20316;&#23454;&#29616;&#19981;&#25104;&#29087;&#30340;&#31639;&#27861;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#27874;&#21160;&#65289;&#65292;&#22312;&#23454;&#38469;&#19990;&#30028;&#20056;&#36710;&#20849;&#20139;&#24179;&#21488;&#20869;&#39564;&#35777;&#36825;&#20123;&#27169;&#22411;&#24182;&#35757;&#32451;/&#27979;&#35797;&#36825;&#20123;&#20248;&#21270;&#31639;&#27861;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20316;&#20026;&#19968;&#20010;&#26377;&#29992;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#20056;&#36710;&#20849;&#20139;&#31995;&#32479;&#30340;&#27169;&#25311;&#24179;&#21488;&#23558;&#38750;&#24120;&#37325;&#35201;&#65292;&#20197;&#36890;&#36807;&#35797;&#39564;&#21644;&#35823;&#24046;&#36827;&#34892;&#31639;&#27861;&#35757;&#32451;/&#27979;&#35797;&#25110;&#27169;&#22411;&#39564;&#35777;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#20026;&#20182;&#20204;&#33258;&#24049;&#30340;&#20219;&#21153;&#24314;&#31435;&#20102;&#21508;&#31181;&#27169;&#25311;&#22120;&#65292;&#20294;&#32570;&#23569;&#19968;&#20010;&#20844;&#27491;&#21644;&#20844;&#24320;&#30340;&#24179;&#21488;&#26469;&#27604;&#36739;&#19981;&#21516;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#30340;&#27169;&#22411;&#25110;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#27169;&#25311;&#22120;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#20174;&#28789;&#27963;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21040;&#30495;&#23454;&#24230;&#37117;&#26377;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25353;&#38656;&#20056;&#36710;&#26381;&#21153;&#25805;&#20316;&#30340;&#22810;&#21151;&#33021;&#27169;&#25311;&#24179;&#21488;&#65292;&#20854;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#29992;&#25143;&#22330;&#26223;&#21644;&#31995;&#32479;&#37197;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#24179;&#21488;&#30340;&#26377;&#29992;&#24615;&#65292;&#21253;&#25324;&#25805;&#20316;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#35780;&#20272;&#65292;&#20248;&#21270;&#31639;&#27861;&#35843;&#25972;&#20197;&#21450;&#23454;&#26102;&#21160;&#24577;&#25805;&#20316;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#19981;&#20165;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#27604;&#36739;&#21508;&#31181;&#31639;&#27861;&#21644;&#27169;&#22411;&#30340;&#20844;&#20849;&#24179;&#21488;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#30001;&#23454;&#36341;&#32773;&#30452;&#25509;&#20351;&#29992;&#65292;&#20197;&#25552;&#39640;&#20182;&#20204;&#30340;&#25805;&#20316;&#25928;&#29575;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-demand ride services or ride-sourcing services have been experiencing fast development in the past decade. Various mathematical models and optimization algorithms have been developed to help ride-sourcing platforms design operational strategies with higher efficiency. However, due to cost and reliability issues (implementing an immature algorithm for real operations may result in system turbulence), it is commonly infeasible to validate these models and train/test these optimization algorithms within real-world ride sourcing platforms. Acting as a useful test bed, a simulation platform for ride-sourcing systems will be very important to conduct algorithm training/testing or model validation through trails and errors. While previous studies have established a variety of simulators for their own tasks, it lacks a fair and public platform for comparing the models or algorithms proposed by different researchers. In addition, the existing simulators still face many challenges, ranging fr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;METS&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#24515;&#30005;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#21035;&#23884;&#20837;&#37197;&#23545;&#30340;&#24515;&#30005;&#22270;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#26426;&#22120;&#20020;&#24202;&#25253;&#21578;&#65292;&#20197;&#23454;&#29616;&#24515;&#30005;&#22270;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#24515;&#30005;&#22270;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12311</link><description>&lt;p&gt;
&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#21161;&#21147;&#24515;&#30005;&#20449;&#21495;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Frozen Language Model Helps ECG Zero-Shot Learning. (arXiv:2303.12311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;METS&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#24515;&#30005;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#21035;&#23884;&#20837;&#37197;&#23545;&#30340;&#24515;&#30005;&#22270;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#26426;&#22120;&#20020;&#24202;&#25253;&#21578;&#65292;&#20197;&#23454;&#29616;&#24515;&#30005;&#22270;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#24515;&#30005;&#22270;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#38750;&#20405;&#20837;&#24335;&#12289;&#26041;&#20415;&#30340;&#21307;&#30103;&#30417;&#27979;&#24037;&#20855;&#65292;&#21487;&#36741;&#21161;&#20020;&#24202;&#35786;&#26029;&#24515;&#33039;&#30142;&#30149;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#65292;&#22312;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#24494;&#35843;&#20043;&#21518;&#65292;SSL&#30340;&#39044;&#35757;&#32451;&#20165;&#20381;&#38752;&#23569;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#23601;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;SSL&#26041;&#27861;&#20381;&#36182;&#20110;&#27880;&#37322;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#26080;&#27861;&#39044;&#27979;&#24494;&#35843;&#25968;&#25454;&#38598;&#20013;&#19981;&#23384;&#22312;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#24515;&#30005;&#22270;&#25991;&#26412;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;METS&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#20020;&#24202;&#25253;&#21578;&#26469;&#25351;&#23548;&#24515;&#30005;&#22270;SSL&#39044;&#35757;&#32451;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#24515;&#30005;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#21035;&#23884;&#20837;&#37197;&#23545;&#30340;&#24515;&#30005;&#22270;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#26426;&#22120;&#20020;&#24202;&#25253;&#21578;&#12290;SSL&#26088;&#22312;&#26368;&#22823;&#21270;&#24515;&#30005;&#22270;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#24515;&#30005;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#23884;&#20837;&#23545;&#40784;&#33391;&#22909;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#24515;&#30005;&#22270;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;METS&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#24515;&#30005;&#22270;&#20998;&#31867;&#24615;&#33021;&#65292;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electrocardiogram (ECG) is one of the most commonly used non-invasive, convenient medical monitoring tools that assist in the clinical diagnosis of heart diseases. Recently, deep learning (DL) techniques, particularly self-supervised learning (SSL), have demonstrated great potential in the classification of ECG. SSL pre-training has achieved competitive performance with only a small amount of annotated data after fine-tuning. However, current SSL methods rely on the availability of annotated data and are unable to predict labels not existing in fine-tuning datasets. To address this challenge, we propose Multimodal ECG-Text Self-supervised pre-training (METS), the first work to utilize the auto-generated clinical reports to guide ECG SSL pre-training. We use a trainable ECG encoder and a frozen language model to embed paired ECG and automatically machine-generated clinical reports separately. The SSL aims to maximize the similarity between paired ECG and auto-generated report while 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26354;&#29575;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.12307</link><description>&lt;p&gt;
&#38271;&#23614;&#20998;&#31867;&#30340;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification. (arXiv:2303.12307v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26354;&#29575;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#38271;&#23614;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#27169;&#22411;&#20559;&#24046;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20551;&#35774;&#26679;&#26412;&#36739;&#23569;&#30340;&#31867;&#26159;&#24369;&#31867;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23614;&#37096;&#31867;&#21035;&#24182;&#19981;&#24635;&#26159;&#38590;&#20197;&#23398;&#20064;&#30340;&#65292;&#32780;&#22312;&#26679;&#26412;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#35266;&#23519;&#21040;&#20102;&#27169;&#22411;&#20559;&#24046;&#65292;&#36825;&#34920;&#26126;&#23384;&#22312;&#20854;&#20182;&#24433;&#21709;&#27169;&#22411;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#21644;&#23398;&#20064;&#22914;&#20309;&#22609;&#36896;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#30340;&#24433;&#21709;&#12290;&#19968;&#20010;&#24847;&#22806;&#30340;&#21457;&#29616;&#26159;&#65306;&#31867;&#21035;&#20934;&#30830;&#24230;&#21644;&#24863;&#30693;&#27969;&#24418;&#30340;&#20998;&#31163;&#31243;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#65292;&#32780;&#19982;&#26354;&#29575;&#30340;&#36127;&#30456;&#20851;&#24615;&#36880;&#28176;&#22686;&#21152;&#65292;&#36825;&#34920;&#26126;&#26354;&#29575;&#19981;&#24179;&#34913;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the challenges of long-tailed classification, researchers have proposed several approaches to reduce model bias, most of which assume that classes with few samples are weak classes. However, recent studies have shown that tail classes are not always hard to learn, and model bias has been observed on sample-balanced datasets, suggesting the existence of other factors that affect model bias. In this work, we systematically propose a series of geometric measurements for perceptual manifolds in deep neural networks, and then explore the effect of the geometric characteristics of perceptual manifolds on classification difficulty and how learning shapes the geometric characteristics of perceptual manifolds. An unanticipated finding is that the correlation between the class accuracy and the separation degree of perceptual manifolds gradually decreases during training, while the negative correlation with the curvature gradually increases, implying that curvature imbalance leads to m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#32780;&#35774;&#35745;&#20986;&#26356;&#22909;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12306</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Logical Expressiveness of Graph Neural Network for Knowledge Graph Reasoning. (arXiv:2303.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#32780;&#35774;&#35745;&#20986;&#26356;&#22909;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#34987;&#24341;&#20837;&#29992;&#20110;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#33391;&#22909;&#30340;&#32463;&#39564;&#24615;&#33021;&#32570;&#20047;&#29702;&#35770;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#23545;&#20110;&#24402;&#32435;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21482;&#26159;&#20026;&#20102;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#19988;&#23545;&#23427;&#20204;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#19978;&#36848;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#36923;&#36753;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;GNN&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25214;&#20986;&#30693;&#35782;&#22270;&#35889;&#20013;&#21487;&#20197;&#25429;&#33719;&#21738;&#20123;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#20808;&#34920;&#26126;&#65292;GNN&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20026;&#20998;&#26512;GNN&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#24037;&#20855;&#65307;&#32780;&#19968;&#20010;&#26597;&#35810;&#26631;&#35760;&#25216;&#24039;&#20351;&#24471;GNN&#26356;&#23481;&#26131;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#26631;&#35760;&#25216;&#24039;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#30340;&#35265;&#35299;&#20419;&#36827;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;GNN&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#30340;&#35774;&#35745;&#65292;&#23427;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been recently introduced to learn from knowledge graph (KG) and achieved state-of-the-art performance in KG reasoning. However, a theoretical certification for their good empirical performance is still absent. Besides, while logic in KG is important for inductive and interpretable inference, existing GNN-based methods are just designed to fit data distributions with limited knowledge of their logical expressiveness. We propose to fill the above gap in this paper. Specifically, we theoretically analyze GNN from logical expressiveness and find out what kind of logical rules can be captured from KG. Our results first show that GNN can capture logical rules from graded modal logic, providing a new theoretical tool for analyzing the expressiveness of GNN for KG reasoning; and a query labeling trick makes it easier for GNN to capture logical rules, explaining why SOTA methods are mainly based on labeling trick. Finally, insights from our theory motivate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SiamTHN&#36319;&#36394;&#22120;&#65292;&#20351;&#29992;&#30446;&#26631;&#31361;&#20986;&#27169;&#22359;&#24110;&#21161;&#30456;&#20284;&#24615;&#21709;&#24212;&#22270;&#38598;&#20013;&#22312;&#30446;&#26631;&#21306;&#22495;&#65292;&#20351;&#29992;&#26657;&#27491;&#25439;&#22833;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12304</link><description>&lt;p&gt;
SiamTHN&#65306;Siamese&#30446;&#26631;&#31361;&#20986;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
SiamTHN: Siamese Target Highlight Network for Visual Tracking. (arXiv:2303.12304v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SiamTHN&#36319;&#36394;&#22120;&#65292;&#20351;&#29992;&#30446;&#26631;&#31361;&#20986;&#27169;&#22359;&#24110;&#21161;&#30456;&#20284;&#24615;&#21709;&#24212;&#22270;&#38598;&#20013;&#22312;&#30446;&#26631;&#21306;&#22495;&#65292;&#20351;&#29992;&#26657;&#27491;&#25439;&#22833;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;&#36319;&#36394;&#22120;&#22312;&#35270;&#35273;&#30446;&#26631;&#36319;&#36394;&#39046;&#22495;&#36805;&#36895;&#21457;&#23637;&#12290;&#29616;&#22312;&#22823;&#22810;&#25968;&#20351;&#29992;&#30340;&#23402;&#29983;&#32593;&#32476;&#36319;&#36394;&#22120;&#23558;&#30001;&#20027;&#24178;&#32593;&#32476;&#29983;&#25104;&#30340;&#29305;&#24449;&#22270;&#20013;&#30340;&#27599;&#20010;&#36890;&#36947;&#24179;&#31561;&#22788;&#29702;&#65292;&#36825;&#20351;&#24471;&#30456;&#20284;&#24615;&#21709;&#24212;&#22270;&#23545;&#32972;&#26223;&#24433;&#21709;&#25935;&#24863;&#65292;&#22240;&#27492;&#38590;&#20197;&#38598;&#20013;&#22312;&#30446;&#26631;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#36319;&#36394;&#22120;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20998;&#25903;&#20043;&#38388;&#27809;&#26377;&#32467;&#26500;&#38142;&#25509;&#65292;&#20004;&#20010;&#20998;&#25903;&#22312;&#35757;&#32451;&#26399;&#38388;&#29420;&#31435;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#20998;&#31867;&#21644;&#22238;&#24402;&#20998;&#25903;&#20043;&#38388;&#23384;&#22312;&#19981;&#23545;&#40784;&#65292;&#23548;&#33268;&#36319;&#36394;&#32467;&#26524;&#19981;&#22815;&#31934;&#30830;&#12290;&#20026;&#20102;&#24110;&#21161;&#29983;&#25104;&#30340;&#30456;&#20284;&#24615;&#21709;&#24212;&#22270;&#26356;&#21152;&#38598;&#20013;&#22312;&#30446;&#26631;&#21306;&#22495;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30446;&#26631;&#31361;&#20986;&#27169;&#22359;&#12290;&#20026;&#20102;&#20943;&#23569;&#19981;&#23545;&#40784;&#24182;&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#36319;&#36394;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26657;&#27491;&#25439;&#22833;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20351;&#29992;&#36825;&#20010;&#26657;&#27491;&#25439;&#22833;&#65292;&#25105;&#20204;&#20849;&#21516;&#35843;&#25972;&#20102;&#27169;&#22411;&#30340;&#20004;&#20010;&#20998;&#25903;&#65292;&#20351;&#23427;&#20204;&#30340;&#32467;&#26500;&#26356;&#22909;&#22320;&#21305;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;SiamTHN&#36319;&#36394;&#22120;&#22312;&#22810;&#20010;&#36319;&#36394;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;VOT2018&#12289;VOT2019&#21644;LaSOT&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Siamese network based trackers develop rapidly in the field of visual object tracking in recent years. The majority of siamese network based trackers now in use treat each channel in the feature maps generated by the backbone network equally, making the similarity response map sensitive to background influence and hence challenging to focus on the target region. Additionally, there are no structural links between the classification and regression branches in these trackers, and the two branches are optimized separately during training. Therefore, there is a misalignment between the classification and regression branches, which results in less accurate tracking results. In this paper, a Target Highlight Module is proposed to help the generated similarity response maps to be more focused on the target region. To reduce the misalignment and produce more precise tracking results, we propose a corrective loss to train the model. The two branches of the model are jointly tuned with the use o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21482;&#23545;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#23569;&#37327;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#27169;&#22411;&#25512;&#26029;&#30340;&#24615;&#33021;&#21644;&#31934;&#24230;&#65292;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2303.12296</link><description>&lt;p&gt;
&#21407;&#22411;&#26377;&#21161;&#20110;&#32852;&#37030;&#23398;&#20064;&#65306;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Prototype Helps Federated Learning: Towards Faster Convergence. (arXiv:2303.12296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21482;&#23545;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#23569;&#37327;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#27169;&#22411;&#25512;&#26029;&#30340;&#24615;&#33021;&#21644;&#31934;&#24230;&#65292;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#21512;&#20316;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#32780;&#19981;&#20132;&#25442;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#25512;&#26029;&#33021;&#21147;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#32780;&#21482;&#38656;&#23545;&#20856;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#30340;&#26368;&#21518;&#19968;&#20010;&#20840;&#23616;&#36845;&#20195;&#36827;&#34892;&#23569;&#37327;&#26356;&#25913;&#65292;&#21363;&#21487;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;&#22312;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#20013;&#65292;&#26381;&#21153;&#22120;&#32858;&#21512;&#20174;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20256;&#36755;&#30340;&#21407;&#22411;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#22238;&#26412;&#22320;&#23458;&#25143;&#31471;&#65292;&#20197;&#29992;&#20110;&#21508;&#33258;&#30340;&#27169;&#22411;&#25512;&#26029;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#22312;&#19981;&#21516;&#30340;&#24322;&#36136;&#24615;&#35774;&#32622;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27604;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65288;&#33267;&#23569;1&#65285;&#65289;&#21644;&#30456;&#23545;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning technique in which multiple clients cooperate to train a shared model without exchanging their raw data. However, heterogeneity of data distribution among clients usually leads to poor model inference. In this paper, a prototype-based federated learning framework is proposed, which can achieve better inference performance with only a few changes to the last global iteration of the typical federated learning process. In the last iteration, the server aggregates the prototypes transmitted from distributed clients and then sends them back to local clients for their respective model inferences. Experiments on two baseline datasets show that our proposal can achieve higher accuracy (at least 1%) and relatively efficient communication than two popular baselines under different heterogeneous settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25506;&#35752;&#22914;&#20309;&#21160;&#24577;&#29983;&#25104;&#34892;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710; ROW &#35745;&#21010;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#30340;&#25928;&#29575;&#65292;&#20026;&#34892;&#20154;&#20998;&#37197;&#26356;&#22810;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.12289</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#19982;&#34892;&#20154;&#20132;&#20114;&#30340;&#33258;&#36866;&#24212;&#36947;&#36335;&#37197;&#32622;
&lt;/p&gt;
&lt;p&gt;
Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian Interactions using Reinforcement Learning. (arXiv:2303.12289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25506;&#35752;&#22914;&#20309;&#21160;&#24577;&#29983;&#25104;&#34892;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710; ROW &#35745;&#21010;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#30340;&#25928;&#29575;&#65292;&#20026;&#34892;&#20154;&#20998;&#37197;&#26356;&#22810;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#37096;&#32626;&#20026;&#26410;&#26469;&#22478;&#24066;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#30340;&#35774;&#35745;&#21644;&#31649;&#29702;&#25552;&#20379;&#20102;&#37325;&#22823;&#25361;&#25112;&#21644;&#29420;&#29305;&#30340;&#26426;&#36935;&#12290;&#20026;&#20102;&#37325;&#26032;&#23450;&#20041;&#36947;&#36335;&#31354;&#38388;&#30340; ROW &#26500;&#25104;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#35774;&#35745;&#26041;&#27861;&#21644;&#26234;&#33021;&#25511;&#21046;&#27169;&#22411;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#23454;&#26102;&#38656;&#27714;&#21160;&#24577;&#29983;&#25104;&#34892;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710; ROW &#35745;&#21010;&#30340;&#25805;&#20316;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827; ROW &#26500;&#25104;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#21035;&#23454;&#29616;&#20102;&#38598;&#20013;&#24335;&#33539;&#24335;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#20998;&#21035;&#23545;&#22810;&#20010;&#36335;&#32593;&#37197;&#32622;&#36827;&#34892;&#21160;&#24577;&#25511;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#20132;&#36890;&#27969;&#30340;&#25928;&#29575;&#24182;&#20998;&#37197;&#26356;&#22810;&#30340;&#31354;&#38388;&#32473;&#34892;&#20154;&#12290;&#27492;&#22806;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#20248;&#20110;&#38598;&#20013;&#24335;&#31639;&#27861;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of Autonomous Vehicles (AVs) poses considerable challenges and unique opportunities for the design and management of future urban road infrastructure. In light of this disruptive transformation, the Right-Of-Way (ROW) composition of road space has the potential to be renewed. Design approaches and intelligent control models have been proposed to address this problem, but we lack an operational framework that can dynamically generate ROW plans for AVs and pedestrians in response to real-time demand. Based on microscopic traffic simulation, this study explores Reinforcement Learning (RL) methods for evolving ROW compositions. We implement a centralised paradigm and a distributive learning paradigm to separately perform the dynamic control on several road network configurations. Experimental results indicate that the algorithms have the potential to improve traffic flow efficiency and allocate more space for pedestrians. Furthermore, the distributive learning algorithm outp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#26694;&#26550;&#19979;&#19981;&#23384;&#22312;&#21487;&#33719;&#24471;&#32435;&#20160;&#22343;&#34913;&#19988;&#21487;&#29420;&#31435;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12287</link><description>&lt;p&gt;
&#29420;&#31435;&#23398;&#20064;&#21644;&#31232;&#30095;&#22343;&#34913;&#35745;&#31639;&#22312;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
Hardness of Independent Learning and Sparse Equilibrium Computation in Markov Games. (arXiv:2303.12287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#26694;&#26550;&#19979;&#19981;&#23384;&#22312;&#21487;&#33719;&#24471;&#32435;&#20160;&#22343;&#34913;&#19988;&#21487;&#29420;&#31435;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65292;&#26159;&#21542;&#23384;&#22312;&#31639;&#27861;&#65292;&#24403;&#25152;&#26377;&#20195;&#29702;&#37319;&#29992;&#24182;&#22312;&#20998;&#25955;&#26041;&#24335;&#19979;&#29420;&#31435;&#36816;&#34892;&#26102;&#65292;&#27599;&#20010;&#29609;&#23478;&#37117;&#21487;&#20197;&#19981;&#21518;&#24724;&#22320;&#36827;&#23637;&#65292;&#31867;&#20284;&#20110;&#27491;&#24120;&#24418;&#24335;&#28216;&#25103;&#20013;&#30340;&#33879;&#21517;&#25910;&#25947;&#32467;&#26524;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65288;&#29305;&#21035;&#26159;&#24403;&#21518;&#24724;&#19982;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#30340;&#20559;&#31163;&#26377;&#20851;&#26102;&#65289;&#65292;&#36825;&#31181;&#31639;&#27861;&#23384;&#22312;&#65292;&#20294;&#26159;&#29420;&#31435;&#30340;&#19981;&#21518;&#24724;&#23398;&#20064;&#26159;&#21542;&#33021;&#22312;&#26631;&#20934;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#26694;&#26550;&#19979;&#23454;&#29616;&#26159;&#20540;&#24471;&#25506;&#35752;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#35745;&#31639;&#21644;&#32479;&#35745;&#35282;&#24230;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#21542;&#23450;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of decentralized multi-agent reinforcement learning in Markov games. A fundamental question is whether there exist algorithms that, when adopted by all agents and run independently in a decentralized fashion, lead to no-regret for each player, analogous to celebrated convergence results in normal-form games. While recent work has shown that such algorithms exist for restricted settings (notably, when regret is defined with respect to deviations to Markovian policies), the question of whether independent no-regret learning can be achieved in the standard Markov game framework was open. We provide a decisive negative resolution this problem, both from a computational and statistical perspective. We show that:  - Under the widely-believed assumption that PPAD-hard problems cannot be solved in polynomial time, there is no polynomial-time algorithm that attains no-regret in general-sum Markov games when executed independently by all players, even when the game is kno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23558;&#24037;&#19994;&#29983;&#20135;&#23545;&#21608;&#36793;&#22478;&#24066;&#30340;&#31354;&#27668;&#27745;&#26579;&#24433;&#21709;&#38477;&#33267;&#26368;&#20302;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#20135;&#27963;&#21160;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#39044;&#27979;&#21644;&#25351;&#23548;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#29615;&#22659;&#24433;&#21709;&#20943;&#23569;&#21644;&#29983;&#20135;&#32500;&#25345;&#20043;&#38388;&#36798;&#25104;&#20102;&#22810;&#31181;&#26435;&#34913;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.12285</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31354;&#27668;&#27745;&#26579;&#38477;&#20302;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reducing Air Pollution through Machine Learning. (arXiv:2303.12285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23558;&#24037;&#19994;&#29983;&#20135;&#23545;&#21608;&#36793;&#22478;&#24066;&#30340;&#31354;&#27668;&#27745;&#26579;&#24433;&#21709;&#38477;&#33267;&#26368;&#20302;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#20135;&#27963;&#21160;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#39044;&#27979;&#21644;&#25351;&#23548;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#29615;&#22659;&#24433;&#21709;&#20943;&#23569;&#21644;&#29983;&#20135;&#32500;&#25345;&#20043;&#38388;&#36798;&#25104;&#20102;&#22810;&#31181;&#26435;&#34913;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#24037;&#19994;&#29983;&#20135;&#23545;&#21608;&#36793;&#22478;&#24066;&#30340;&#31354;&#27668;&#27745;&#26579;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#23558;&#36816;&#33829;&#20915;&#31574;&#19982;&#22825;&#27668;&#26465;&#20214;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#39044;&#27979;&#21644;&#25351;&#23548;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#30701;&#26399;&#39118;&#36895;&#21644;&#26041;&#21521;&#65292;&#24182;&#25512;&#33616;&#36816;&#33829;&#20915;&#31574;&#20197;&#20943;&#23569;&#25110;&#26242;&#20572;&#24037;&#19994;&#29983;&#20135;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#21644;&#32500;&#25345;&#29983;&#20135;&#27963;&#21160;&#20043;&#38388;&#30340;&#20960;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#39044;&#27979;&#32452;&#20214;&#37319;&#29992;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#26799;&#24230;&#25552;&#21319;&#26641;&#27169;&#22411;&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25351;&#23548;&#24615;&#32452;&#20214;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26368;&#20248;&#31574;&#30053;&#26641;&#25552;&#20986;&#22810;&#20010;&#26435;&#34913;&#26041;&#26696;&#65292;&#20363;&#22914;&#23558;&#21361;&#38505;&#25490;&#25918;&#29289;&#20943;&#23569;33-47%&#21644;&#23558;&#19981;&#24517;&#35201;&#30340;&#25104;&#26412;&#38477;&#20302;40-63%&#12290;&#25105;&#20204;&#37096;&#32626;&#30340;&#27169;&#22411;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#65292;&#23545;&#20110;&#23567;&#20110;12&#23567;&#26102;&#30340;&#39044;&#27979;&#65292;&#38477;&#20302;&#20102;38-52%&#30340;&#35823;&#24046;&#33539;&#22260;&#65292;&#23545;&#20110;12&#21040;48&#23567;&#26102;&#30340;&#39044;&#27979;&#65292;&#38477;&#20302;&#20102;14-46%&#30340;&#35823;&#24046;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a data-driven approach to mitigate the effects of air pollution from industrial plants on nearby cities by linking operational decisions with weather conditions. Our method combines predictive and prescriptive machine learning models to forecast short-term wind speed and direction and recommend operational decisions to reduce or pause the industrial plant's production. We exhibit several trade-offs between reducing environmental impact and maintaining production activities. The predictive component of our framework employs various machine learning models, such as gradient-boosted tree-based models and ensemble methods, for time series forecasting. The prescriptive component utilizes interpretable optimal policy trees to propose multiple trade-offs, such as reducing dangerous emissions by 33-47% and unnecessary costs by 40-63%. Our deployed models significantly reduced forecasting errors, with a range of 38-52% for less than 12-hour lead time and 14-46% for 12 to 48-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Prompt Programming&#21644;GPT-3&#25216;&#26415;&#29983;&#25104;&#20102;&#22823;&#37327;&#24102;&#26377;&#29305;&#24449;&#26631;&#35760;&#30340;&#23545;&#35805;&#25968;&#25454;&#65292;&#26377;&#25928;&#29992;&#20110;&#35757;&#32451;&#22823;&#20116;&#20154;&#26684;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.12279</link><description>&lt;p&gt;
&#20351;&#29992;Prompt Programming&#21644;GPT-3&#29983;&#25104;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#8212;&#8212;&#20197;&#22823;&#20116;&#20154;&#26684;&#20998;&#31867;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Generate labeled training data using Prompt Programming and GPT-3. An example of Big Five Personality Classification. (arXiv:2303.12279v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Prompt Programming&#21644;GPT-3&#25216;&#26415;&#29983;&#25104;&#20102;&#22823;&#37327;&#24102;&#26377;&#29305;&#24449;&#26631;&#35760;&#30340;&#23545;&#35805;&#25968;&#25454;&#65292;&#26377;&#25928;&#29992;&#20110;&#35757;&#32451;&#22823;&#20116;&#20154;&#26684;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;Prompt Programming&#21644;GPT-3&#29983;&#25104;&#20102;25000&#20010;&#24102;&#26377;&#22823;&#20116;&#20154;&#26684;&#29305;&#24449;&#26631;&#35760;&#30340;&#23545;&#35805;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#20102;&#22823;&#20116;&#20998;&#31867;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;2500&#26465;&#26469;&#33258;&#29983;&#25104;&#23545;&#35805;&#21644;&#30495;&#23454;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#20154;&#31867;&#27880;&#37322;&#30340;&#22823;&#20116;&#26631;&#35760;&#25968;&#25454;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26395;&#21019;&#24314;&#26377;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;Adapter-Transformers&#21644;&#26469;&#33258;&#39044;&#35757;&#32451;RoBERTa&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#23558;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#33719;&#24471;&#20102;0.71&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#20102;0.65&#30340;&#20934;&#30830;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#22312;&#38480;&#21046;&#21644;&#32622;&#20449;&#24230;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generated 25000 conversations labeled with Big Five Personality traits using prompt programming at GPT-3. Then we train Big Five classification models with these data and evaluate them with 2500 data from generated dialogues and real conversational datasets labeled in Big Five by human annotators. The results indicated that this approach is promising for creating effective training data. We then compare the performance by different training approaches and models. Our results suggest that using Adapter-Transformers and transfer learning from pre-trained RoBERTa sentiment analysis model will perform best with the generated data. Our best model obtained an accuracy of 0.71 in generated data and 0.65 in real datasets. Finally, we discuss this approach's potential limitations and confidence metric.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23567;&#40736;&#39045;&#39592;&#19981;&#22343;&#21248;&#38382;&#39064;&#30340;&#33258;&#20027;&#24335;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.12265</link><description>&lt;p&gt;
&#33258;&#20027;&#24335;&#23567;&#40736;&#39045;&#31383;&#39592;&#25277;&#26679;&#26426;&#22120;&#20154;&#38075;&#23380;&#31995;&#32479;&#65306;&#19968;&#20010;&#22522;&#20110;&#34507;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Robotic Drilling System for Mice Cranial Window Creation: An Evaluation with an Egg Model. (arXiv:2303.12265v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23567;&#40736;&#39045;&#39592;&#19981;&#22343;&#21248;&#38382;&#39064;&#30340;&#33258;&#20027;&#24335;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#31185;&#23398;&#23454;&#39564;&#20013;&#65292;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#24212;&#29992;&#21487;&#20197;&#20351;&#24471;&#23545;&#23454;&#39564;&#26679;&#26412;&#36827;&#34892;&#31934;&#30830;&#25805;&#20316;&#65292;&#19981;&#21463;&#31185;&#23398;&#23478;&#25216;&#33021;&#30340;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#20197;&#23567;&#40736;&#39045;&#31383;&#30340;&#39592;&#25277;&#26679;&#25805;&#20316;&#20026;&#20363;&#65292;&#32771;&#34385;&#23567;&#40736;&#39045;&#39592;&#30340;&#19981;&#22343;&#21248;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#24335;&#25805;&#20316;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic assistance for experimental manipulation in the life sciences is expected to enable precise manipulation of valuable samples, regardless of the skill of the scientist. Experimental specimens in the life sciences are subject to individual variability and deformation, and therefore require autonomous robotic control. As an example, we are studying the installation of a cranial window in a mouse. This operation requires the removal of the skull, which is approximately 300 um thick, to cut it into a circular shape 8 mm in diameter, but the shape of the mouse skull varies depending on the strain of mouse, sex and week of age. The thickness of the skull is not uniform, with some areas being thin and others thicker. It is also difficult to ensure that the skulls of the mice are kept in the same position for each operation. It is not realistically possible to measure all these features and pre-program a robotic trajectory for individual mice. The paper therefore proposes an autonomous 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;&#65292;&#27169;&#25311;&#27233;&#33014;&#25163;&#24187;&#35273;&#65292;&#20026;&#35748;&#35782;&#20154;&#31867;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#35745;&#31639;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2303.12259</link><description>&lt;p&gt;
&#27169;&#25311;&#27233;&#33014;&#25163;&#24187;&#35273;&#30340;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired bodily self-perception model that replicates the rubber hand illusion. (arXiv:2303.12259v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;&#65292;&#27169;&#25311;&#27233;&#33014;&#25163;&#24187;&#35273;&#65292;&#20026;&#35748;&#35782;&#20154;&#31867;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#35745;&#31639;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#26680;&#24515;&#26159;&#23545;&#33258;&#24049;&#36523;&#20307;&#25317;&#26377;&#26435;&#30340;&#24863;&#30693;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22823;&#33041;&#23545;&#33258;&#36523;&#36523;&#20307;&#32534;&#30721;&#30340;&#26426;&#21046;&#65292;&#20154;&#20204;&#20570;&#20986;&#20102;&#21508;&#31181;&#23581;&#35797;&#65292;&#21457;&#23637;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#30456;&#20851;&#30340;&#34892;&#20026;&#21644;&#31070;&#32463;&#29983;&#29702;&#29616;&#35937;&#12290;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#35299;&#37322;&#27233;&#33014;&#25163;&#24187;&#35273;&#36825;&#26679;&#30340;&#36523;&#20307;&#38169;&#35273;&#23454;&#38469;&#21457;&#29983;&#30340;&#26426;&#21046;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#20102;&#26377;&#20851;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#26426;&#21046;&#21644;&#21487;&#33021;&#30456;&#20851;&#30340;&#33041;&#21306;&#30340;&#27010;&#24565;&#24615;&#25551;&#36848;&#65292;&#20294;&#29616;&#26377;&#30340;&#29702;&#35770;&#27169;&#22411;&#20173;&#28982;&#32570;&#20047;&#35299;&#37322;&#22823;&#33041;&#22914;&#20309;&#32534;&#30721;&#23545;&#33258;&#24049;&#36523;&#20307;&#30340;&#24863;&#30693;&#21644;&#22914;&#20309;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#25105;&#20204;&#20027;&#35266;&#24863;&#30693;&#30340;&#36523;&#20307;&#38169;&#35273;&#30340;&#35745;&#31639;&#26426;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25972;&#21512;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#29983;&#29289;&#23398;&#21457;&#29616;&#65292;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;&#65292;&#20351;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#21487;&#20197;&#22312;&#27809;&#26377;&#22806;&#37096;&#21050;&#28608;&#30340;&#24773;&#20917;&#19979;&#33258;&#20027;&#26500;&#24314;&#12290;&#22522;&#20110;&#35813;&#27169;&#22411;&#30340;&#27169;&#25311;&#22797;&#21046;&#20102;&#27233;&#33014;&#25163;&#24187;&#35273;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#28508;&#22312;&#31070;&#32463;&#26426;&#21046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the core of bodily self-consciousness is the perception of the ownership of one's body. Recent efforts to gain a deeper understanding of the mechanisms behind the brain's encoding of the self-body have led to various attempts to develop a unified theoretical framework to explain related behavioral and neurophysiological phenomena. A central question to be explained is how body illusions such as the rubber hand illusion actually occur. Despite the conceptual descriptions of the mechanisms of bodily self-consciousness and the possible relevant brain areas, the existing theoretical models still lack an explanation of the computational mechanisms by which the brain encodes the perception of one's body and how our subjectively perceived body illusions can be generated by neural networks. Here we integrate the biological findings of bodily self-consciousness to propose a Brain-inspired bodily self-perception model, by which perceptions of bodily self can be autonomously constructed withou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#23545;&#27604;&#32858;&#31867;&#26694;&#26550;&#65292;&#30452;&#25509;&#20248;&#21270;&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#37325;&#26500;&#23398;&#20064;&#21644;&#19968;&#33268;&#24615;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#32500;&#24230;&#23849;&#22604;&#21644;&#19981;&#19968;&#33268;&#31169;&#26377;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12241</link><description>&lt;p&gt;
&#36890;&#36807;&#30452;&#25509;&#23545;&#27604;&#23398;&#20064;&#39044;&#38450;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#32500;&#24230;&#23849;&#22604;
&lt;/p&gt;
&lt;p&gt;
Preventing Dimensional Collapse of Incomplete Multi-View Clustering via Direct Contrastive Learning. (arXiv:2303.12241v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12241
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#23545;&#27604;&#32858;&#31867;&#26694;&#26550;&#65292;&#30452;&#25509;&#20248;&#21270;&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#37325;&#26500;&#23398;&#20064;&#21644;&#19968;&#33268;&#24615;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#32500;&#24230;&#23849;&#22604;&#21644;&#19981;&#19968;&#33268;&#31169;&#26377;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;Incomplete multi-view clustering&#65292;IMVC&#65289;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;IMVC&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#22312;&#35299;&#20915;&#32858;&#31867;&#26399;&#38388;&#28508;&#22312;&#29305;&#24449;&#21482;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#26377;&#25928;&#30340;&#32500;&#24230;&#23849;&#22604;&#38382;&#39064;&#26102;&#36807;&#24230;&#20381;&#36182;&#39069;&#22806;&#30340;&#25237;&#24433;&#22836;&#65307;&#20294;&#26159;&#65292;&#25237;&#24433;&#22836;&#20013;&#30340;&#35768;&#22810;&#21442;&#25968;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;2&#65289;&#24674;&#22797;&#30340;&#35270;&#22270;&#21253;&#21547;&#19981;&#19968;&#33268;&#30340;&#31169;&#26377;&#20449;&#24687;&#65292;&#24182;&#19988;&#30001;&#20110;&#22312;&#21516;&#19968;&#29305;&#24449;&#19978;&#25191;&#34892;&#19968;&#33268;&#24615;&#23398;&#20064;&#21644;&#37325;&#26500;&#23398;&#20064;&#65292;&#26080;&#29992;&#30340;&#31169;&#26377;&#20449;&#24687;&#23558;&#35823;&#23548;&#24120;&#35265;&#35821;&#20041;&#30340;&#23398;&#20064;&#12290;&#20026;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#23545;&#27604;&#32858;&#31867;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#30452;&#25509;&#20248;&#21270;&#28508;&#22312;&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#21521;&#37327;&#21450;&#20854;&#23376;&#21521;&#37327;&#36827;&#34892;&#37325;&#26500;&#23398;&#20064;&#21644;&#19968;&#33268;&#24615;&#23398;&#20064;&#65292;&#20174;&#32780;&#26377;&#25928;&#36991;&#20813;&#20102;&#32500;&#24230;&#23849;&#22604;&#65292;&#24182;&#38477;&#20302;&#20102;&#19981;&#19968;&#33268;&#31169;&#26377;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incomplete multi-view clustering (IMVC) is an unsupervised approach, among which IMVC via contrastive learning has received attention due to its excellent performance. The previous methods have the following problems: 1) Over-reliance on additional projection heads when solving the dimensional collapse problem in which latent features are only valid in lower-dimensional subspaces during clustering. However, many parameters in the projection heads are unnecessary. 2) The recovered view contain inconsistent private information and useless private information will mislead the learning of common semantics due to consistent learning and reconstruction learning on the same feature. To address the above issues, we propose a novel incomplete multi-view contrastive clustering framework. This framework directly optimizes the latent feature subspace, utilizes the learned feature vectors and their sub-vectors for reconstruction learning and consistency learning, thereby effectively avoiding dimens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#31649;&#36947;&#65292;&#29992;&#20110;&#31163;&#20307;MRI&#22270;&#20687;&#30340;&#33258;&#21160;&#21270;&#20998;&#21106;&#12290;&#22522;&#20110;37&#20010;&#26631;&#26412;&#30340;&#39640;&#20998;&#36776;&#29575;7 T MRI&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#21487;&#20934;&#30830;&#20998;&#21106;&#25972;&#20010;&#33041;&#21322;&#29699;&#65292;&#21253;&#25324;&#30382;&#23618;&#12289;&#30382;&#36136;&#19979;&#32467;&#26500;&#12289;&#30333;&#36136;&#39640;&#20449;&#21495;&#20197;&#21450;&#27491;&#24120;&#20986;&#29616;&#30340;&#30333;&#36136;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#31070;&#32463;&#30149;&#29702;&#23398;&#30740;&#31350;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2303.12237</link><description>&lt;p&gt;
&#29992;&#20110;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#32467;&#26500;&#30149;&#29702;&#30456;&#20851;&#24615;&#23450;&#37327;&#20998;&#26512;&#30340;&#39640;&#20998;&#36776;&#29575;7T&#31163;&#20307;MRI&#30340;&#33258;&#21160;&#21270;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Automated deep learning segmentation of high-resolution 7 T ex vivo MRI for quantitative analysis of structure-pathology correlations in neurodegenerative diseases. (arXiv:2303.12237v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#31649;&#36947;&#65292;&#29992;&#20110;&#31163;&#20307;MRI&#22270;&#20687;&#30340;&#33258;&#21160;&#21270;&#20998;&#21106;&#12290;&#22522;&#20110;37&#20010;&#26631;&#26412;&#30340;&#39640;&#20998;&#36776;&#29575;7 T MRI&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#21487;&#20934;&#30830;&#20998;&#21106;&#25972;&#20010;&#33041;&#21322;&#29699;&#65292;&#21253;&#25324;&#30382;&#23618;&#12289;&#30382;&#36136;&#19979;&#32467;&#26500;&#12289;&#30333;&#36136;&#39640;&#20449;&#21495;&#20197;&#21450;&#27491;&#24120;&#20986;&#29616;&#30340;&#30333;&#36136;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#31070;&#32463;&#30149;&#29702;&#23398;&#30740;&#31350;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#22312; vivo MRI&#65292;&#22823;&#33041;&#31163;&#20307; MRI &#25552;&#20379;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#21487;&#29992;&#20110;&#21487;&#35270;&#21270;&#21644;&#34920;&#24449;&#35814;&#32454;&#30340;&#31070;&#32463;&#35299;&#21078;&#65292;&#21327;&#21161;&#23558;&#24494;&#35266;&#32452;&#32455;&#23398;&#30740;&#31350;&#19982;&#24418;&#24577;&#27979;&#37327;&#30456;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21463;&#38480;&#20110;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#25195;&#25551;&#22120;&#30828;&#20214;&#21644;&#37319;&#38598;&#21327;&#35758;&#30340;&#24322;&#36136;&#24615;&#65292;&#31163;&#20307; MRI &#30340;&#33041;&#21306;&#22495;&#20998;&#21106;&#33258;&#21160;&#21270;&#26041;&#27861;&#24182;&#19981;&#21457;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22312;7T&#20840;&#36523;MRI&#25195;&#25551;&#22120;&#19978;&#25195;&#25551;&#30340;37&#20010;&#31163;&#20307;&#20154;&#33041;&#32452;&#32455;&#26631;&#26412;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#26469;&#20998;&#21106;&#30382;&#23618;&#65292;&#36890;&#36807;&#23545;&#20061;&#31181;&#28145;&#24230;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#30382;&#36136;&#19979;&#32467;&#26500;&#65288;&#23614;&#29366;&#26680;&#65292;&#33041;&#35910;&#26680;&#65292;&#33485;&#30333;&#29699;&#21644;&#19992;&#33041;&#65289;&#12289;&#30333;&#36136;&#39640;&#20449;&#21495;&#20197;&#21450;&#27491;&#24120;&#20986;&#29616;&#30340;&#30333;&#36136;&#36827;&#34892;&#20998;&#21106;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#26631;&#26412;&#30340;&#20840;&#33041;&#21322;&#29699;&#20043;&#38388;&#30340;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20063;&#23637;&#31034;&#20102;&#22312;&#30475;&#19981;&#35265;&#30340;&#30828;&#20214;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#31070;&#32463;&#30149;&#29702;&#23398;&#21644;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#32467;&#26500;&#30149;&#29702;&#30456;&#20851;&#24615;&#30340;&#23450;&#37327;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for visualizing and characterizing detailed neuroanatomy, and helps to link microscale histology studies with morphometric measurements. However, automated segmentation methods for brain mapping in ex vivo MRI are not well developed, primarily due to limited availability of labeled datasets, and heterogeneity in scanner hardware and acquisition protocols. In this work, we present a high resolution dataset of 37 ex vivo post-mortem human brain tissue specimens scanned on a 7T whole-body MRI scanner. We developed a deep learning pipeline to segment the cortical mantle by benchmarking the performance of nine deep neural architectures. We then segment the four subcortical structures: caudate, putamen, globus pallidus, and thalamus; white matter hyperintensities, and the normal appearing white matter. We show excellent generalizing capabilities across whole brain hemispheres in different specimens, and also on unseen i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#26469;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25925;&#38556;&#21644;&#21361;&#38505;&#39550;&#39542;&#21592;&#65292;&#24182;&#22312;&#32553;&#23567;&#30340;&#24494;&#22411;&#22478;&#24066;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2303.12224</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#35774;&#26045;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#39550;&#39542;&#21592;&#22833;&#35823;&#39044;&#38450;
&lt;/p&gt;
&lt;p&gt;
Infrastructure-based End-to-End Learning and Prevention of Driver Failure. (arXiv:2303.12224v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#26469;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25925;&#38556;&#21644;&#21361;&#38505;&#39550;&#39542;&#21592;&#65292;&#24182;&#22312;&#32553;&#23567;&#30340;&#24494;&#22411;&#22478;&#24066;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#21449;&#36335;&#21475;&#31649;&#29702;&#22120;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21361;&#38505;&#39550;&#39542;&#21592;&#25110;&#25925;&#38556;&#27169;&#24335;&#65292;&#35686;&#21578;&#25509;&#36817;&#20132;&#21449;&#36335;&#21475;&#30340;&#26469;&#36710;&#65292;&#20174;&#32780;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FailureNet&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32553;&#23567;&#30340;&#24494;&#22411;&#22478;&#24066;&#20013;&#22522;&#20110;&#36712;&#36857;&#23545;&#27491;&#24120;&#21644;&#19981;&#23433;&#20840;&#39550;&#39542;&#21592;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;FailureNet&#35266;&#23519;&#36710;&#36742;&#25509;&#36817;&#20132;&#21449;&#21475;&#26102;&#30340;&#23039;&#24577;&#65292;&#26816;&#27979;&#33258;&#20027;&#22534;&#26632;&#20013;&#26159;&#21542;&#23384;&#22312;&#25925;&#38556;&#65292;&#24182;&#35686;&#21578;&#20132;&#21449;&#27969;&#37327;&#26377;&#28508;&#22312;&#21361;&#38505;&#30340;&#39550;&#39542;&#21592;&#12290;FailureNet&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#25511;&#21046;&#25925;&#38556;&#12289;&#19978;&#28216;&#24863;&#30693;&#38169;&#35823;&#21644;&#36229;&#36895;&#39550;&#39542;&#21592;&#65292;&#19982;&#27491;&#24120;&#39550;&#39542;&#21152;&#20197;&#21306;&#21035;&#12290;&#35813;&#32593;&#32476;&#22312;MiniCity&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#19982;&#22522;&#20110;&#36895;&#24230;&#25110;&#39057;&#29575;&#30340;&#39044;&#27979;&#22120;&#30456;&#27604;&#65292;FailureNet&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24403;&#37096;&#32626;&#22312;&#30828;&#20214;&#19978;&#26102;&#65292;&#20854;&#20934;&#30830;&#24615;&#39640;&#36798;84%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent intersection managers can improve safety by detecting dangerous drivers or failure modes in autonomous vehicles, warning oncoming vehicles as they approach an intersection. In this work, we present FailureNet, a recurrent neural network trained end-to-end on trajectories of both nominal and reckless drivers in a scaled miniature city. FailureNet observes the poses of vehicles as they approach an intersection and detects whether a failure is present in the autonomy stack, warning cross-traffic of potentially dangerous drivers. FailureNet can accurately identify control failures, upstream perception errors, and speeding drivers, distinguishing them from nominal driving. The network is trained and deployed with autonomous vehicles in the MiniCity. Compared to speed or frequency-based predictors, FailureNet's recurrent neural network structure provides improved predictive power, yielding upwards of 84% accuracy when deployed on hardware.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12206</link><description>&lt;p&gt;
&#34892;&#20026;&#20581;&#24247;&#20010;&#24615;&#21270;&#20171;&#20837;&#30340;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization for Personalized Interventions in Behavioral Health. (arXiv:2303.12206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12206
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#65292;&#36890;&#36807;&#25945;&#32946;&#65292;&#28608;&#21169;&#65292;&#25552;&#37266;&#21644;&#22806;&#23637;&#65292;&#26377;&#26395;&#26174;&#30528;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20171;&#20837;&#20855;&#26377;&#25104;&#26412;&#21644;&#33021;&#21147;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#24739;&#32773;&#20010;&#24615;&#21270;&#20171;&#20837;&#20197;&#26368;&#22823;&#21270;&#26576;&#31181;&#38271;&#26399;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#26041;&#27861;/&#32467;&#26524;&#65306;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26469;&#33258;&#22686;&#24378;&#23398;&#20064;&#25991;&#29486;&#30340;&#36890;&#29992;&#26080;&#27169;&#22411;&#26041;&#27861;&#23545;&#20110;&#21307;&#30103;&#24212;&#29992;&#26469;&#35828;&#36807;&#20110;&#25968;&#25454;&#23494;&#38598;&#65292;&#32780;&#26356;&#31616;&#21333;&#30340;&#36172;&#33218;&#38382;&#39064;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24573;&#30053;&#20102;&#38271;&#26399;&#24739;&#32773;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;DecompPI&#65292;&#23427;&#36817;&#20284;&#20110;&#19968;&#27493;&#25919;&#31574;&#36845;&#20195;&#12290;&#23454;&#29616;DecompPI&#21482;&#38656;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#31181;&#33258;&#28982;&#30340;&#32467;&#26500;&#20551;&#35774;&#19979;&#65292;DecompPI&#21487;&#20197;&#33719;&#24471;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;
Problem definition: Behavioral health interventions, delivered through digital platforms, have the potential to significantly improve health outcomes, through education, motivation, reminders, and outreach. We study the problem of optimizing personalized interventions for patients to maximize some long-term outcome, in a setting where interventions are costly and capacity-constrained.  Methodology/results: This paper provides a model-free approach to solving this problem. We find that generic model-free approaches from the reinforcement learning literature are too data intensive for healthcare applications, while simpler bandit approaches make progress at the expense of ignoring long-term patient dynamics. We present a new algorithm we dub DecompPI that approximates one step of policy iteration. Implementing DecompPI simply consists of a prediction task from offline data, alleviating the need for online experimentation. Theoretically, we show that under a natural set of structural assu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;conformer enhanced AV-HuBERT&#65292;&#21487;&#20197;&#22312;&#20013;&#33521;&#25991;&#30340;&#38899;&#35270;&#39057;&#35782;&#21035;&#20219;&#21153;&#20013;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#22312;&#33521;&#35821;AVSR&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#22522;&#32447;AV-HuBERT&#20998;&#21035;&#23454;&#29616;&#20102;7%&#21644;16%&#30340;&#30456;&#23545;WER&#38477;&#20302;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;AVSR&#25968;&#25454;&#38598;CSTS&#65292;&#23558;WeNet ASR&#31995;&#32479;&#30456;&#24212;&#24615;&#33021;&#36229;&#36234;&#20102;14%&#21644;18%&#12290;</title><link>http://arxiv.org/abs/2303.12187</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#38899;&#35270;&#39057;HuBERT&#22312;&#20013;&#33521;&#25991;&#39046;&#22495;&#30340;&#24212;&#29992;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Practice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and English. (arXiv:2303.12187v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;conformer enhanced AV-HuBERT&#65292;&#21487;&#20197;&#22312;&#20013;&#33521;&#25991;&#30340;&#38899;&#35270;&#39057;&#35782;&#21035;&#20219;&#21153;&#20013;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#22312;&#33521;&#35821;AVSR&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#22522;&#32447;AV-HuBERT&#20998;&#21035;&#23454;&#29616;&#20102;7%&#21644;16%&#30340;&#30456;&#23545;WER&#38477;&#20302;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;AVSR&#25968;&#25454;&#38598;CSTS&#65292;&#23558;WeNet ASR&#31995;&#32479;&#30456;&#24212;&#24615;&#33021;&#36229;&#36234;&#20102;14%&#21644;18%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#20154;&#31867;&#35821;&#38899;&#24863;&#30693;&#30340;&#21452;&#27169;&#24615;&#36136;&#65292;&#22068;&#21767;&#21644;&#29273;&#40831;&#30340;&#36816;&#21160;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#21644;&#22122;&#22768;&#19981;&#21464;&#30340;&#35270;&#35273;&#20449;&#24687;&#65292;&#38899;&#35270;&#39057;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;conformer enhanced AV-HuBERT&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;AV-HuBERT&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#19982;&#22522;&#32447;AV-HuBERT&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24178;&#20928;&#21644;&#22024;&#26434;&#26465;&#20214;&#19979;&#30340;&#21333;&#38454;&#27573;&#35780;&#20272;&#20013;&#65292;&#22312;&#33521;&#35821;AVSR&#22522;&#20934;&#25968;&#25454;&#38598;LRS3&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;7%&#21644;16%&#30340;&#30456;&#23545;WER&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;1000&#23567;&#26102;&#30340;&#20013;&#25991;AVSR&#25968;&#25454;&#38598;CSTS&#12290;&#22312;&#22522;&#32447;AV-HuBERT&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#65292;&#22312;MISP&#21644;CMLR&#19978;&#23558;WeNet ASR&#31995;&#32479;&#36229;&#36234;&#20102;14%&#21644;18%&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;conformer enhanced AV-HuBERT&#22312;MISP&#19978;&#24102;&#26469;&#20102;7%&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;6%&#30340;CER&#12290;
&lt;/p&gt;
&lt;p&gt;
Considering the bimodal nature of human speech perception, lips, and teeth movement has a pivotal role in automatic speech recognition. Benefiting from the correlated and noise-invariant visual information, audio-visual recognition systems enhance robustness in multiple scenarios. In previous work, audio-visual HuBERT appears to be the finest practice incorporating modality knowledge. This paper outlines a mixed methodology, named conformer enhanced AV-HuBERT, boosting the AV-HuBERT system's performance a step further. Compared with baseline AV-HuBERT, our method in the one-phase evaluation of clean and noisy conditions achieves 7% and 16% relative WER reduction on the English AVSR benchmark dataset LRS3. Furthermore, we establish a novel 1000h Mandarin AVSR dataset CSTS. On top of the baseline AV-HuBERT, we exceed the WeNet ASR system by 14% and 18% relatively on MISP and CMLR by pre-training with this dataset. The conformer-enhanced AV-HuBERT we proposed brings 7% on MISP and 6% CER 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#21033;&#29992;&#38646;&#26679;&#26412;&#22270;&#20687;&#20928;&#21270;&#26377;&#25928;&#22320;&#38450;&#24481;&#21508;&#31181;&#25915;&#20987;&#65292;&#26080;&#38656;&#20219;&#20309;&#20869;&#37096;&#20449;&#24687;&#25110;&#20808;&#21069;&#30693;&#35782;&#65292;&#36890;&#36807;&#23545;&#21463;&#27745;&#26579;&#30340;&#22270;&#20687;&#36827;&#34892;&#32447;&#24615;&#21464;&#25442;&#21644;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#24674;&#22797;&#32570;&#22833;&#35821;&#20041;&#20449;&#24687;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.12175</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22270;&#20687;&#20928;&#21270;&#30340;&#40657;&#30418;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Black-box Backdoor Defense via Zero-shot Image Purification. (arXiv:2303.12175v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#21033;&#29992;&#38646;&#26679;&#26412;&#22270;&#20687;&#20928;&#21270;&#26377;&#25928;&#22320;&#38450;&#24481;&#21508;&#31181;&#25915;&#20987;&#65292;&#26080;&#38656;&#20219;&#20309;&#20869;&#37096;&#20449;&#24687;&#25110;&#20808;&#21069;&#30693;&#35782;&#65292;&#36890;&#36807;&#23545;&#21463;&#27745;&#26579;&#30340;&#22270;&#20687;&#36827;&#34892;&#32447;&#24615;&#21464;&#25442;&#21644;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#24674;&#22797;&#32570;&#22833;&#35821;&#20041;&#20449;&#24687;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#20250;&#23558;&#27602;&#25968;&#25454;&#27880;&#20837;&#35757;&#32451;&#38598;&#65292;&#23548;&#33268;&#27169;&#22411;&#25512;&#29702;&#26102;&#26679;&#26412;&#38169;&#35823;&#20998;&#31867;&#12290;&#22312;&#20165;&#26377;&#27169;&#22411;&#39044;&#27979;&#21487;&#29992;&#30340;&#23454;&#38469;&#40657;&#30418;&#29615;&#22659;&#20013;&#65292;&#38450;&#24481;&#27492;&#31867;&#25915;&#20987;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#38646;&#26679;&#26412;&#22270;&#20687;&#20928;&#21270;&#65288;ZIP&#65289;&#26377;&#25928;&#22320;&#25269;&#24481;&#21508;&#31181;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#26694;&#26550;&#21487;&#20197;&#24212;&#29992;&#20110;&#40657;&#30418;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#21463;&#27745;&#26579;&#27169;&#22411;&#30340;&#20869;&#37096;&#20449;&#24687;&#25110;&#20219;&#20309;&#20851;&#20110;&#24178;&#20928;/&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#21463;&#27745;&#26579;&#30340;&#22270;&#20687;&#24212;&#29992;&#32447;&#24615;&#21464;&#25442;&#20197;&#30772;&#22351;&#35302;&#21457;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24674;&#22797;&#30001;&#21464;&#25442;&#21435;&#38500;&#30340;&#32570;&#22833;&#35821;&#20041;&#20449;&#24687;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#21453;&#21521;&#36807;&#31243;&#65292;&#20351;&#29992;&#21464;&#25442;&#21518;&#30340;&#22270;&#20687;&#26469;&#24341;&#23548;&#39640;&#20445;&#30495;&#24230;&#20928;&#21270;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks inject poisoned data into the training set, resulting in misclassification of the poisoned samples during model inference. Defending against such attacks is challenging, especially in real-world black-box settings where only model predictions are available. In this paper, we propose a novel backdoor defense framework that can effectively defend against various attacks through zero-shot image purification (ZIP). Our proposed framework can be applied to black-box models without requiring any internal information about the poisoned model or any prior knowledge of the clean/poisoned samples. Our defense framework involves a two-step process. First, we apply a linear transformation on the poisoned image to destroy the trigger pattern. Then, we use a pre-trained diffusion model to recover the missing semantic information removed by the transformation. In particular, we design a new reverse process using the transformed image to guide the generation of high-fidelity purified 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;EMT&#20223;&#30495;&#30340;&#31934;&#24230;&#21644;ML&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;PLL&#25511;&#21046;&#22120;&#22312;&#25925;&#38556;&#24773;&#20917;&#19979;&#30340;&#38750;&#32447;&#24615;&#30636;&#24577;&#21160;&#21147;&#23398;&#24182;&#30830;&#23450;&#20854;&#31283;&#23450;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.12116</link><description>&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#32593;&#32476;&#29992;&#20110;&#38145;&#30456;&#29615;&#30636;&#24577;&#31283;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Physics Informed Neural Networks for Phase Locked Loop Transient Stability Assessment. (arXiv:2303.12116v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;EMT&#20223;&#30495;&#30340;&#31934;&#24230;&#21644;ML&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;PLL&#25511;&#21046;&#22120;&#22312;&#25925;&#38556;&#24773;&#20917;&#19979;&#30340;&#38750;&#32447;&#24615;&#30636;&#24577;&#21160;&#21147;&#23398;&#24182;&#30830;&#23450;&#20854;&#31283;&#23450;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#32852;&#21512;&#22269;2050&#24180;&#38646;&#20928;&#25490;&#25918;&#30446;&#26631;&#38656;&#35201;&#22823;&#37327;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#21033;&#29992;&#30005;&#21147;&#30005;&#23376;&#25511;&#21046;&#22120;&#65292;&#20363;&#22914;&#38145;&#30456;&#29615;&#65288;PLL&#65289;&#65292;&#26469;&#20445;&#25345;&#24182;&#32593;&#30340;&#21487;&#20877;&#29983;&#36164;&#28304;&#19982;&#32593;&#32476;&#21516;&#27493;&#21487;&#33021;&#23548;&#33268;&#22312;&#32593;&#32476;&#25925;&#38556;&#26399;&#38388;&#24555;&#36895;&#30636;&#24577;&#34892;&#20026;&#65292;&#36827;&#32780;&#23548;&#33268;&#19981;&#31283;&#23450;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#25152;&#26377;&#21487;&#33021;&#30340;&#22330;&#26223;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#30830;&#23450;&#31283;&#23450;&#30340;&#36793;&#30028;&#25110;&#21560;&#24341;&#21306;&#22495;&#65288;ROA&#65289;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;EMT&#20223;&#30495;&#25110;&#31616;&#21270;&#27169;&#22411;&#65288;ROM&#65289;&#26469;&#20934;&#30830;&#30830;&#23450;ROA&#26159;&#35745;&#31639;&#26114;&#36149;&#30340;&#12290;&#30456;&#21453;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ML&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#26159;&#35745;&#31639;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#20934;&#30830;&#39044;&#27979;PLL&#25511;&#21046;&#22120;&#22312;&#25925;&#38556;&#24773;&#20917;&#19979;&#30340;&#38750;&#32447;&#24615;&#30636;&#24577;&#21160;&#21147;&#23398;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;EMT&#20223;&#30495;&#30340;&#31934;&#24230;&#21644;ML&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20197;&#20934;&#30830;&#30830;&#23450;PLL&#25511;&#21046;&#22120;&#30340;ROA&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant increase in renewable energy production is necessary to achieve the UN's net-zero emission targets for 2050. Using power-electronic controllers, such as Phase Locked Loops (PLLs), to keep grid-tied renewable resources in synchronism with the grid can cause fast transient behavior during grid faults leading to instability. However, assessing all the probable scenarios is impractical, so determining the stability boundary or region of attraction (ROA) is necessary. However, using EMT simulations or Reduced-order models (ROMs) to accurately determine the ROA is computationally expensive. Alternatively, Machine Learning (ML) models have been proposed as an efficient method to predict stability. However, traditional ML algorithms require large amounts of labeled data for training, which is computationally expensive. This paper proposes a Physics-Informed Neural Network (PINN) architecture that accurately predicts the nonlinear transient dynamics of a PLL controller under fault
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;</title><link>http://arxiv.org/abs/2303.12112</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#26679;&#26412;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#20687;&#35270;&#39057;&#26631;&#39064;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;CLIP&#27169;&#22411;&#22312;&#24456;&#22810;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#37117;&#38750;&#24120;&#26377;&#25928;&#65292;&#21253;&#25324;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#32467;&#26500;&#20013;&#29983;&#25104;&#30340;&#26631;&#39064;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#27604;&#24230;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#37197;&#26041;&#65292;&#21363;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340;&#23545;&#27604;&#24230;&#23398;&#20064;&#20998;&#25968;&#65288;PAC-S&#65289;&#65292;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#32479;&#19968;&#20102;&#23545;&#27604;&#24230;&#35270;&#35273;-&#35821;&#20041;&#31354;&#38388;&#30340;&#23398;&#20064;&#21644;&#31574;&#23637;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#28155;&#21152;&#12290;&#36328;&#36234;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#25351;&#26631;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26368;&#39640;&#65292;&#20248;&#20110;&#29616;&#26377;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CIDEr&#21644;SPICE&#65289;&#21644;&#26080;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CLIP-Score&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27969;&#34892;&#30340;&#22270;&#20687;&#26631;&#39064;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#37319;&#29992;&#19981;&#21516;&#36328;&#27169;&#24577;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly availa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31227;&#21160;&#36793;&#32536;&#32531;&#23384;&#32593;&#32476;&#27969;&#34892;&#24230;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#22810;&#20869;&#23481;&#39034;&#24207;&#35831;&#27714;&#27169;&#24335;&#26102;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12097</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;MEC&#32593;&#32476;&#27969;&#34892;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CLSA: Contrastive Learning-based Survival Analysis for Popularity Prediction in MEC Networks. (arXiv:2303.12097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31227;&#21160;&#36793;&#32536;&#32531;&#23384;&#32593;&#32476;&#27969;&#34892;&#24230;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#22810;&#20869;&#23481;&#39034;&#24207;&#35831;&#27714;&#27169;&#24335;&#26102;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#32531;&#23384;&#65288;MEC&#65289;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#38598;&#25104;&#30340;&#21019;&#26032;&#25216;&#26415;&#20855;&#26377;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#29992;&#25143;&#30340;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;MEC&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#20381;&#36182;&#20110;&#20854;&#33021;&#21147;&#26469;&#39044;&#27979;&#21644;&#21160;&#24577;&#26356;&#26032;&#32531;&#23384;&#33410;&#28857;&#23384;&#20648;&#26368;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;DNN&#27169;&#22411;&#36890;&#36807;&#21516;&#26102;&#23558;&#22810;&#20010;&#20869;&#23481;&#30340;&#39034;&#24207;&#35831;&#27714;&#27169;&#24335;&#36755;&#20837;&#21040;&#32593;&#32476;&#20013;&#26469;&#25429;&#25417;&#21518;&#32773;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#36755;&#20837;&#26679;&#26412;&#30340;&#22823;&#23567;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;DNN&#27969;&#34892;&#24230;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile Edge Caching (MEC) integrated with Deep Neural Networks (DNNs) is an innovative technology with significant potential for the future generation of wireless networks, resulting in a considerable reduction in users' latency. The MEC network's effectiveness, however, heavily relies on its capacity to predict and dynamically update the storage of caching nodes with the most popular contents. To be effective, a DNN-based popularity prediction model needs to have the ability to understand the historical request patterns of content, including their temporal and spatial correlations. Existing state-of-the-art time-series DNN models capture the latter by simultaneously inputting the sequential request patterns of multiple contents to the network, considerably increasing the size of the input sample. This motivates us to address this challenge by proposing a DNN-based popularity prediction framework based on the idea of contrasting input samples against each other, designed for the Unmann
&lt;/p&gt;</description></item><item><title>&#22238;&#22797;&#35780;&#35770;&#65292;&#35748;&#20026;&#35780;&#35770;&#32858;&#28966;&#20110;&#19968;&#31181;&#38750;&#20856;&#22411;&#38382;&#39064;&#19988;&#36807;&#20110;&#31616;&#21270;&#65292;&#24378;&#35843;&#20102;&#21407;&#22987;&#24037;&#20316;&#32972;&#21518;&#26356;&#24191;&#27867;&#30340;&#31639;&#27861;&#24320;&#21457;&#20197;&#21450;&#23454;&#39564;&#25968;&#25454;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#25351;&#20986;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#35299;&#21078;&#19982;&#36138;&#24515;&#31639;&#27861;&#30340;&#26412;&#36136;&#22823;&#19981;&#30456;&#21516;&#65292;&#22240;&#27492;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.12096</link><description>&lt;p&gt;
&#20851;&#20110;&#12298;&#26080;&#27861;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#32988;&#36807;&#36138;&#24515;&#31639;&#27861;&#12299;&#30340;&#22238;&#22797;(arXiv:2303.12096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Reply to: Inability of a graph neural network heuristic to outperform greedy algorithms in solving combinatorial optimization problems. (arXiv:2303.12096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12096
&lt;/p&gt;
&lt;p&gt;
&#22238;&#22797;&#35780;&#35770;&#65292;&#35748;&#20026;&#35780;&#35770;&#32858;&#28966;&#20110;&#19968;&#31181;&#38750;&#20856;&#22411;&#38382;&#39064;&#19988;&#36807;&#20110;&#31616;&#21270;&#65292;&#24378;&#35843;&#20102;&#21407;&#22987;&#24037;&#20316;&#32972;&#21518;&#26356;&#24191;&#27867;&#30340;&#31639;&#27861;&#24320;&#21457;&#20197;&#21450;&#23454;&#39564;&#25968;&#25454;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#25351;&#20986;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#35299;&#21078;&#19982;&#36138;&#24515;&#31639;&#27861;&#30340;&#26412;&#36136;&#22823;&#19981;&#30456;&#21516;&#65292;&#22240;&#27492;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;Stefan Boettcher [arXiv:2210.00623]&#30340;&#35780;&#35770;&#20570;&#20102;&#20840;&#38754;&#22238;&#22797;&#65292;&#24182;&#35748;&#20026;&#36825;&#20010;&#35780;&#35770;&#20165;&#38024;&#23545;&#19968;&#31181;&#38750;&#20856;&#22411;&#30340;&#38382;&#39064;&#65292;&#20165;&#20851;&#27880;&#31232;&#30095;&#22270;&#19978;&#30340;&#26368;&#22823;&#21106;&#38382;&#39064;(MaxCut)&#65292;&#32780;&#36138;&#24515;&#31639;&#27861;&#22312;&#36825;&#31181;&#38382;&#39064;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#21407;&#22987;&#24037;&#20316;&#32972;&#21518;&#26356;&#24191;&#27867;&#30340;&#31639;&#27861;&#24320;&#21457;&#65292;&#24182;&#22312;&#25105;&#20204;&#21407;&#22987;&#26694;&#26550;&#20869;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#26377;&#26174;&#30528;&#25913;&#36827;&#65292;&#22240;&#27492;&#25512;&#32763;&#20102;&#35780;&#35770;&#30340;&#21407;&#22987;&#24615;&#33021;&#38472;&#36848;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#35777;&#26126;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(PI-GNN)&#33021;&#22815;&#32988;&#36807;&#36138;&#24515;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#22256;&#38590;&#30340;&#12289;&#23494;&#38598;&#30340;&#23454;&#20363;&#19978;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;(&#24182;&#34892;)&#35299;&#21078;&#19982;&#36138;&#24515;&#31639;&#27861;&#30340;(&#39034;&#24207;)&#26412;&#36136;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#22522;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#31038;&#20132;&#32593;&#32476;&#35268;&#27169;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#25351;&#20986;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a comprehensive reply to the comment written by Stefan Boettcher [arXiv:2210.00623] and argue that the comment singles out one particular non-representative example problem, entirely focusing on the maximum cut problem (MaxCut) on sparse graphs, for which greedy algorithms are expected to perform well. Conversely, we highlight the broader algorithmic development underlying our original work, and (within our original framework) provide additional numerical results showing sizable improvements over our original data, thereby refuting the comment's original performance statements. Furthermore, it has already been shown that physics-inspired graph neural networks (PI-GNNs) can outperform greedy algorithms, in particular on hard, dense instances. We also argue that the internal (parallel) anatomy of graph neural networks is very different from the (sequential) nature of greedy algorithms, and (based on their usage at the scale of real-world social networks) point out that graph n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.12091</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning. (arXiv:2303.12091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#26631;&#35760;&#25968;&#25454;&#12289;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#32771;&#34385;&#21040;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#21253;&#21547;&#26631;&#35760;&#25968;&#25454;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#31867;&#21035;&#65288;&#24322;&#24120;&#20540;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;ANEDL&#65289;&#65292;&#20197;&#24212;&#23545;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#22914;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#21644;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#20316;&#20026;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#22120;&#26469;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#20351;EDL&#26356;&#21152;&#36866;&#21512;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;ANEDL&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrat
&lt;/p&gt;</description></item><item><title>Thrill-K&#26550;&#26500;&#23558;&#31070;&#32463;&#23398;&#20064;&#19982;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#37096;&#32626;&#31471;&#21040;&#31471;&#23398;&#20064;&#31995;&#32479;&#25152;&#38656;&#35201;&#30340;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#12289;&#20197;&#21450;&#32570;&#20047;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#25512;&#29702;&#21644;&#39564;&#35777;&#33021;&#21147;&#31561;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.12084</link><description>&lt;p&gt;
Thrill-K&#26550;&#26500;&#65306;&#36808;&#21521;&#22522;&#20110;&#30693;&#35782;&#30340;&#29702;&#35299;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Thrill-K Architecture: Towards a Solution to the Problem of Knowledge Based Understanding. (arXiv:2303.12084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12084
&lt;/p&gt;
&lt;p&gt;
Thrill-K&#26550;&#26500;&#23558;&#31070;&#32463;&#23398;&#20064;&#19982;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#37096;&#32626;&#31471;&#21040;&#31471;&#23398;&#20064;&#31995;&#32479;&#25152;&#38656;&#35201;&#30340;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#12289;&#20197;&#21450;&#32570;&#20047;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#25512;&#29702;&#21644;&#39564;&#35777;&#33021;&#21147;&#31561;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31471;&#21040;&#31471;&#23398;&#20064;&#31995;&#32479;&#27491;&#36805;&#36895;&#33719;&#24471;&#33021;&#21147;&#21644;&#27969;&#34892;&#24230;&#65292;&#20294;&#37096;&#32626;&#36825;&#31181;&#31995;&#32479;&#25152;&#38656;&#30340;&#35745;&#31639;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#21152;&#20043;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#25512;&#29702;&#21644;&#39564;&#35777;&#33021;&#21147;&#30340;&#32570;&#20047;&#65292;&#38656;&#35201;&#26032;&#31867;&#22411;&#30340;&#26550;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31995;&#32479;&#30340;&#20998;&#31867;&#65292;&#35813;&#20998;&#31867;&#22522;&#20110;&#23545;&#20154;&#31867;&#30693;&#35782;&#21644;&#26234;&#33021;&#30340;&#20998;&#26512;&#65292;&#23558;&#31070;&#32463;&#23398;&#20064;&#19982;&#21508;&#31181;&#31867;&#22411;&#30340;&#30693;&#35782;&#21644;&#30693;&#35782;&#26469;&#28304;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Thrill-K&#26550;&#26500;&#20316;&#20026;&#23558;&#30636;&#26102;&#30693;&#35782;&#12289;&#22791;&#29992;&#30693;&#35782;&#21644;&#22806;&#37096;&#30693;&#35782;&#28304;&#38598;&#25104;&#21040;&#19968;&#20010;&#20855;&#26377;&#25512;&#29702;&#12289;&#23398;&#20064;&#21644;&#26234;&#33021;&#25511;&#21046;&#33021;&#21147;&#30340;&#26694;&#26550;&#20013;&#30340;&#21407;&#22411;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
While end-to-end learning systems are rapidly gaining capabilities and popularity, the increasing computational demands for deploying such systems, along with a lack of flexibility, adaptability, explainability, reasoning and verification capabilities, require new types of architectures. Here we introduce a classification of hybrid systems which, based on an analysis of human knowledge and intelligence, combines neural learning with various types of knowledge and knowledge sources. We present the Thrill-K architecture as a prototypical solution for integrating instantaneous knowledge, standby knowledge and external knowledge sources in a framework capable of inference, learning and intelligent control.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; RegionLight&#65292;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#23558;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#27599;&#20010;&#21306;&#22495;&#20013;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#25193;&#23637;&#20102;BDQ&#26041;&#27861;&#20026;DBDQ&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.11899</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22823;&#35268;&#27169;&#26684;&#32593;&#20132;&#36890;&#32593;&#32476;&#21306;&#22495;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning for Regional Signal control in Large-scale Grid Traffic network. (arXiv:2303.11899v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; RegionLight&#65292;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#23558;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#27599;&#20010;&#21306;&#22495;&#20013;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#25193;&#23637;&#20102;BDQ&#26041;&#27861;&#20026;DBDQ&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#33258;&#36866;&#24212;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26159;&#24403;&#21069;&#38750;&#24120;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#21333;&#20010;&#36335;&#21475;&#65292;&#36825;&#20123;&#26041;&#27861;&#20391;&#37325;&#20110;&#36335;&#21475;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;MARL&#30340;&#38750;&#31283;&#24577;&#24615;&#36136;&#38543;&#30528;&#20132;&#36890;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20173;&#28982;&#38480;&#21046;&#30528;&#19978;&#36848;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#22949;&#21327;&#30340;&#31574;&#30053;&#26159;&#23558;&#19968;&#21517;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#19968;&#32452;&#36335;&#21475;&#20013;&#65292;&#20197;&#20943;&#23569;&#26234;&#33021;&#20307;&#25968;&#37327;&#12290;&#36825;&#31181;&#31574;&#30053;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65292;&#19968;&#20010;&#26159;&#22914;&#20309;&#23558;&#20132;&#36890;&#32593;&#32476;&#21010;&#20998;&#25104;&#23567;&#21306;&#22495;&#65292;&#21478;&#19968;&#20010;&#26159;&#22914;&#20309;&#25628;&#32034;&#21306;&#22495;&#20869;&#30340;&#26368;&#20248;&#32852;&#21512;&#21160;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;RegionLight&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#21306;&#22495;&#21010;&#20998;&#35268;&#21017;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#65292;&#24182;&#25193;&#23637;&#20102;Branching Dueling Q-Network(BDQ)&#12290;&#35813;&#26041;&#27861;&#23558;BDQ&#36827;&#19968;&#27493;&#20248;&#21270;&#20026;Dynamic Branching Dueling Q-Network(DBDQ)&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive traffic signal control with Multi-agent Reinforcement Learning(MARL) is a very popular topic nowadays. In most existing novel methods, one agent controls single intersections and these methods focus on the cooperation between intersections. However, the non-stationary property of MARL still limits the performance of the above methods as the size of traffic networks grows. One compromised strategy is to assign one agent with a region of intersections to reduce the number of agents. There are two challenges in this strategy, one is how to partition a traffic network into small regions and the other is how to search for the optimal joint actions for a region of intersections. In this paper, we propose a novel training framework RegionLight where our region partition rule is based on the adjacency between the intersection and extended Branching Dueling Q-Network(BDQ) to Dynamic Branching Dueling Q-Network(DBDQ) to bound the growth of the size of joint action space and alleviate th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#35757;&#32451;&#22235;&#36275;&#26426;&#22120;&#20154;&#20351;&#29992;&#21069;&#32930;&#25191;&#34892;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#25856;&#29228;&#12289;&#25353;&#19979;&#25353;&#38062;&#21644;&#19982;&#29289;&#20307;&#20132;&#20114;&#65292;&#24182;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#23558;&#36825;&#20123;&#25216;&#33021;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#24182;&#33719;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11330</link><description>&lt;p&gt;
&#33151;&#37096;&#20316;&#20026;&#26426;&#26800;&#25163;&#33218;&#65306;&#23558;&#22235;&#36275;&#26426;&#22120;&#20154;&#25935;&#25463;&#24615;&#25512;&#21521;&#36229;&#20986;&#36816;&#21160;&#30340;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion. (arXiv:2303.11330v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11330
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#35757;&#32451;&#22235;&#36275;&#26426;&#22120;&#20154;&#20351;&#29992;&#21069;&#32930;&#25191;&#34892;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#25856;&#29228;&#12289;&#25353;&#19979;&#25353;&#38062;&#21644;&#19982;&#29289;&#20307;&#20132;&#20114;&#65292;&#24182;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#23558;&#36825;&#20123;&#25216;&#33021;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#24182;&#33719;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#34892;&#36208;&#25110;&#22868;&#36305;&#22312;&#21508;&#31181;&#22797;&#26434;&#22320;&#24418;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#19982;&#29399;&#31561;&#29983;&#29289;&#30456;&#27604;&#65292;&#26426;&#22120;&#20154;&#22235;&#36275;&#21160;&#29289;&#20173;&#28982;&#36828;&#36828;&#33853;&#21518;&#65292;&#29399;&#33021;&#22815;&#23637;&#31034;&#22810;&#31181;&#25935;&#25463;&#25216;&#33021;&#65292;&#24182;&#33021;&#20351;&#29992;&#33151;&#37096;&#36229;&#20986;&#36816;&#21160;&#30340;&#33539;&#22260;&#65292;&#25191;&#34892;&#20960;&#20010;&#22522;&#26412;&#30340;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#19982;&#29289;&#20307;&#36827;&#34892;&#20132;&#20114;&#21644;&#25856;&#29228;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#22235;&#36275;&#26426;&#22120;&#20154;&#19981;&#20165;&#34892;&#36208;&#65292;&#36824;&#20351;&#29992;&#21069;&#32930;&#25856;&#29228;&#22681;&#22721;&#12289;&#25353;&#19979;&#25353;&#38062;&#12289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#29289;&#20307;&#20132;&#20114;&#31561;&#20219;&#21153;&#65292;&#26469;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25216;&#33021;&#24191;&#27867;&#20998;&#20026;&#20004;&#31867;&#65306;&#36816;&#21160;&#65292;&#21253;&#25324;&#20219;&#20309;&#28041;&#21450;&#36816;&#21160;&#30340;&#20107;&#29289;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#34892;&#36208;&#36824;&#26159;&#25856;&#29228;&#22681;&#22721;&#65307;&#25805;&#32437;&#65292;&#28041;&#21450;&#20351;&#29992;&#19968;&#26465;&#33151;&#36827;&#34892;&#20132;&#20114;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#19977;&#26465;&#33151;&#30340;&#24179;&#34913;&#12290;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#36825;&#20123;&#25216;&#33021;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340; sim2real &#26041;&#27861;&#23558;&#20854;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20445;&#25345;&#31283;&#23450;&#30340;&#21516;&#26102;&#25191;&#34892;&#22810;&#31181;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#25171;&#24320;&#38376;&#12289;&#25353;&#19979;&#25353;&#38062;&#21644;&#25552;&#36215;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Locomotion has seen dramatic progress for walking or running across challenging terrains. However, robotic quadrupeds are still far behind their biological counterparts, such as dogs, which display a variety of agile skills and can use the legs beyond locomotion to perform several basic manipulation tasks like interacting with objects and climbing. In this paper, we take a step towards bridging this gap by training quadruped robots not only to walk but also to use the front legs to climb walls, press buttons, and perform object interaction in the real world. To handle this challenging optimization, we decouple the skill learning broadly into locomotion, which involves anything that involves movement whether via walking or climbing a wall, and manipulation, which involves using one leg to interact while balancing on the other three legs. These skills are trained in simulation using curriculum and transferred to the real world using our proposed sim2real variant that builds upon recent l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.10880</link><description>&lt;p&gt;
&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;: &#36890;&#36807;&#35302;&#35273;&#23454;&#29616;&#25163;&#37096;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rotating without Seeing: Towards In-hand Dexterity through Touch. (arXiv:2303.10880v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#24863;&#20449;&#24687;&#22312;&#20154;&#31867;&#28789;&#24039;&#24615;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#30340;&#25509;&#35302;&#20449;&#24687;&#65292;&#30452;&#25509;&#20174;&#35270;&#35273;&#20013;&#26080;&#27861;&#25512;&#26029;&#12290;&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#20351;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#20855;&#22791;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#20351;&#29992;&#35206;&#30422;&#25972;&#20010;&#26426;&#22120;&#20154;&#25163;&#30340;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#65288;&#35302;&#25720;&#25110;&#26410;&#35302;&#25720;&#65289;&#20195;&#26367;&#20165;&#20165;&#22312;&#23567;&#21306;&#22495;&#20869;&#36827;&#34892;&#31934;&#20934;&#30340;&#35302;&#35273;&#20256;&#24863;&#65292;&#20351;&#31995;&#32479;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#35206;&#30422;&#33539;&#22260;&#24191;&#31561;&#20248;&#28857;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#26679;&#30340;&#29289;&#20307;&#27169;&#25311;&#20013;&#35757;&#32451;&#20986;&#20102;&#19968;&#31181;&#35302;&#24863;&#26059;&#36716;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#25163;&#19978;&#30452;&#25509;&#23454;&#26045;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#26032;&#22411;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactile information plays a critical role in human dexterity. It reveals useful contact information that may not be inferred directly from vision. In fact, humans can even perform in-hand dexterous manipulation without using vision. Can we enable the same ability for the multi-finger robot hand? In this paper, we present Touch Dexterity, a new system that can perform in-hand object rotation using only touching without seeing the object. Instead of relying on precise tactile sensing in a small region, we introduce a new system design using dense binary force sensors (touch or no touch) overlaying one side of the whole robot hand (palm, finger links, fingertips). Such a design is low-cost, giving a larger coverage of the object, and minimizing the Sim2Real gap at the same time. We train an in-hand rotation policy using Reinforcement Learning on diverse objects in simulation. Relying on touch-only sensing, we can directly deploy the policy in a real robot hand and rotate novel objects tha
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;GPT&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21457;&#29616;&#22823;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#21463;&#21040;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#21644;&#21508;&#34892;&#21508;&#19994;&#65292;&#39044;&#31034;&#30528;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.10130</link><description>&lt;p&gt;
GPT&#26159;GPT&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#21171;&#21160;&#21147;&#24066;&#22330;&#24433;&#21709;&#30340;&#26089;&#26399;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. (arXiv:2303.10130v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;GPT&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21457;&#29616;&#22823;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#21463;&#21040;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#21644;&#21508;&#34892;&#21508;&#19994;&#65292;&#39044;&#31034;&#30528;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#20351;&#29992;&#26032;&#30340;&#26631;&#20934;&#65292;&#25105;&#20204;&#35780;&#20272;&#32844;&#19994;&#19982;GPT&#33021;&#21147;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;GPT-4&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#33267;&#23569;&#26377;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#21463;&#21040;GPT&#24341;&#20837;&#30340;&#24433;&#21709;&#65292;&#32780;&#32422;19%&#30340;&#24037;&#20154;&#21487;&#33021;&#20250;&#30475;&#21040;&#33267;&#23569;50%&#30340;&#20219;&#21153;&#21463;&#21040;&#24433;&#21709;&#12290;&#24433;&#21709;&#33539;&#22260;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#65292;&#39640;&#25910;&#20837;&#24037;&#20316;&#21487;&#33021;&#38754;&#20020;&#26356;&#22823;&#30340;&#39118;&#38505;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24433;&#21709;&#24182;&#19981;&#23616;&#38480;&#20110;&#26368;&#36817;&#29983;&#20135;&#29575;&#22686;&#38271;&#36739;&#39640;&#30340;&#34892;&#19994;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#20855;&#26377;&#36890;&#29992;&#25216;&#26415;&#65288;GPT&#65289;&#30340;&#29305;&#24615;&#65292;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.
&lt;/p&gt;</description></item><item><title>SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09540</link><description>&lt;p&gt;
SemDeDup:&#36890;&#36807;&#35821;&#20041;&#21435;&#37325;&#23454;&#29616;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#30340;&#39640;&#25928;&#23398;&#20064;&#65288;arXiv:2303.09540v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
SemDeDup: Data-efficient learning at web-scale through semantic deduplication. (arXiv:2303.09540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09540
&lt;/p&gt;
&lt;p&gt;
SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#28023;&#37327;&#25968;&#25454;&#30340;&#22686;&#21152;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#20687;LAION&#36825;&#26679;&#30340;&#22823;&#22411;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#38500;&#26597;&#25214;&#31934;&#30830;&#37325;&#22797;&#39033;&#22806;&#65292;&#22823;&#37096;&#20998;&#26410;&#32463;&#31934;&#24515;&#31579;&#36873;&#65292;&#21487;&#33021;&#23384;&#22312;&#24456;&#22810;&#20887;&#20313;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;SemDeDup&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#65306;&#21363;&#35821;&#20041;&#19978;&#30456;&#20284;&#20294;&#24182;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#25968;&#25454;&#23545;&#12290;&#21435;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#21487;&#20197;&#20445;&#25345;&#24615;&#33021;&#24182;&#21152;&#36895;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;&#24615;&#33021;&#22312;&#20998;&#24067;&#20197;&#22806;&#24471;&#21040;&#25552;&#39640;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#37096;&#20998;&#31579;&#36873;&#36807;&#30340;&#25968;&#25454;&#38598;C4&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;SemDeDup&#25552;&#20379;&#20102;&#19968;&#20010;&#21033;&#29992;&#36136;&#37327;&#23884;&#20837;&#31616;&#21333;&#26041;&#27861;&#26469;&#20351;&#27169;&#22411;&#26356;&#24555;&#22320;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with le
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07543</link><description>&lt;p&gt;
WDiscOOD&#65306;&#36890;&#36807;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#21306;&#20998;&#24230;&#20248;&#21270;&#30340;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#22312;&#36935;&#21040;&#26410;&#30693;&#27010;&#24565;&#30340;&#24773;&#24418;&#19979;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#20010;&#25361;&#25112;&#31361;&#26174;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#31354;&#38388;OOD&#26816;&#27979;&#20998;&#25968;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#31867;&#21035;&#29305;&#23450;&#21644;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#8212;&#8212;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#65292;&#20854;&#20013;ID&#31867;&#22312;&#21028;&#21035;&#23376;&#31354;&#38388;&#20013;&#34987;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#65292;&#24182;&#22312;&#27531;&#24046;&#23376;&#31354;&#38388;&#20013;&#34987;&#32039;&#23494;&#22320;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#22312;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#23558;&#26469;&#33258;&#36755;&#20837;&#25968;&#25454;&#19982;ID&#20998;&#24067;&#30340;&#20559;&#24046;&#32452;&#21512;&#36215;&#26469;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;WDiscOOD&#65292;&#22312;&#35206;&#30422;&#22810;&#31181;&#20998;&#24067;&#20559;&#31227;&#30340;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#12290;WDiscOOD&#22312;&#28145;&#24230;&#20998;&#31867;&#22120;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;-&#35270;&#35273;&#25552;&#31034;&#65288;TVP&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#20849;&#21516;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#19988;&#20351;&#29992;&#21482;&#26377;&#20302;&#22797;&#26434;&#24230;&#31232;&#30095;&#20108;&#32500;&#35270;&#35273;&#29305;&#24449;&#26469;&#25552;&#39640;&#36328;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#23545;&#35805;&#25490;&#21517;&#65288;TDR&#65289;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;&#30417;&#30563;TVP&#30340;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#19988;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.04995</link><description>&lt;p&gt;
&#25991;&#26412;-&#35270;&#35273;&#25552;&#31034;&#29992;&#20110;&#39640;&#25928;&#30340;&#20108;&#32500;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Text-Visual Prompting for Efficient 2D Temporal Video Grounding. (arXiv:2303.04995v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;-&#35270;&#35273;&#25552;&#31034;&#65288;TVP&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#20849;&#21516;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#19988;&#20351;&#29992;&#21482;&#26377;&#20302;&#22797;&#26434;&#24230;&#31232;&#30095;&#20108;&#32500;&#35270;&#35273;&#29305;&#24449;&#26469;&#25552;&#39640;&#36328;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#23545;&#35805;&#25490;&#21517;&#65288;TDR&#65289;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;&#30417;&#30563;TVP&#30340;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#19988;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#65288;TVG&#65289;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#39044;&#27979;&#25991;&#23383;&#25551;&#36848;&#30340;&#26102;&#21051;&#22312;&#38271;&#26102;&#38388;&#26410;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#30340;&#24320;&#22987;/&#32467;&#26463;&#26102;&#38388;&#28857;&#12290;&#21463;&#30410;&#20110;&#32454;&#31890;&#24230;&#30340;&#19977;&#32500;&#35270;&#35273;&#29305;&#24449;&#65292;TVG&#25216;&#26415;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#39640;&#22797;&#26434;&#24615;&#20351;&#24471;&#25552;&#21462;&#23494;&#38598;&#30340;&#19977;&#32500;&#35270;&#35273;&#29305;&#24449;&#26159;&#32791;&#26102;&#30340;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#38024;&#23545;&#39640;&#25928;&#30340;TVG&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;-&#35270;&#35273;&#25552;&#31034;&#65288;TVP&#65289;&#26694;&#26550;&#65292;&#24182;&#23558;&#20248;&#21270;&#30340;&#25200;&#21160;&#27169;&#24335;&#65288;&#31216;&#20026;&#8220;&#25552;&#31034;&#8221;&#65289;&#32467;&#21512;&#21040;TVG&#27169;&#22411;&#30340;&#35270;&#35273;&#36755;&#20837;&#21644;&#25991;&#26412;&#29305;&#24449;&#20013;&#12290;&#19982;&#19977;&#32500;CNN&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#20316;&#32773;&#34920;&#26126;TVP&#20801;&#35768;&#25105;&#20204;&#22312;&#20108;&#32500;TVG&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#20849;&#21516;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#21482;&#26377;&#20302;&#22797;&#26434;&#24230;&#31232;&#30095;&#20108;&#32500;&#35270;&#35273;&#29305;&#24449;&#26469;&#25552;&#39640;&#36328;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#23545;&#35805;&#25490;&#21517;&#65288;TDR&#65289;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#30417;&#30563;TVP&#30340;&#23398;&#20064;&#24182;&#20419;&#36827;&#35270;&#39057;&#27573;&#19982;&#30456;&#24212;&#25991;&#26412;&#26597;&#35810;&#30340;&#23545;&#40784;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;TVP&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remarkable progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extracting dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturbation patterns (that we call 'prompts') into both visual inputs and textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP allows us to effectively co-train vision encoder and language encoder in a 2D TVG model and improves the performance of crossmodal feature fusion using only low-complexity sparse 2D visual features. Further, we propose a Temporal-Di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;Side Adapter Network&#26694;&#26550;&#37319;&#29992;&#20923;&#32467;&#30340;CLIP&#27169;&#22411;&#21644;&#36741;&#21161;&#32593;&#32476;&#26469;&#23454;&#29616;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#36731;&#37327;&#32423;&#20248;&#21183;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#37325;&#29992;CLIP&#29305;&#24449;&#65292;&#20165;&#22686;&#21152;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.12242</link><description>&lt;p&gt;
Side Adapter Network&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Side Adapter Network for Open-Vocabulary Semantic Segmentation. (arXiv:2302.12242v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;Side Adapter Network&#26694;&#26550;&#37319;&#29992;&#20923;&#32467;&#30340;CLIP&#27169;&#22411;&#21644;&#36741;&#21161;&#32593;&#32476;&#26469;&#23454;&#29616;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#36731;&#37327;&#32423;&#20248;&#21183;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#37325;&#29992;CLIP&#29305;&#24449;&#65292;&#20165;&#22686;&#21152;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21629;&#21517;&#20026;Side Adapter Network (SAN)&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#24314;&#27169;&#20026;&#21306;&#22495;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#20923;&#32467;&#30340;CLIP&#27169;&#22411;&#19978;&#38468;&#21152;&#19968;&#20010;&#36741;&#21161;&#32593;&#32476;&#26469;&#23454;&#29616;&#12290;&#35813;&#32593;&#32476;&#26377;&#20004;&#20010;&#20998;&#25903;&#65306;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#25513;&#27169;&#25552;&#26696;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#27880;&#24847;&#20559;&#24046;&#65292;&#35813;&#20559;&#24046;&#24212;&#29992;&#20110;CLIP&#27169;&#22411;&#20013;&#26469;&#35782;&#21035;&#25513;&#27169;&#30340;&#31867;&#21035;&#12290;&#36825;&#31181;&#20998;&#31163;&#30340;&#35774;&#35745;&#20351;&#24471;CLIP&#33021;&#22815;&#35782;&#21035;&#25513;&#27169;&#25552;&#26696;&#30340;&#31867;&#21035;&#12290;&#30001;&#20110;&#36741;&#21161;&#32593;&#32476;&#21487;&#20197;&#37325;&#29992;CLIP&#29305;&#24449;&#65292;&#22240;&#27492;&#23427;&#38750;&#24120;&#36731;&#24039;&#12290;&#27492;&#22806;&#65292;&#25972;&#20010;&#32593;&#32476;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#20351;&#24471;&#36741;&#21161;&#32593;&#32476;&#33021;&#22815;&#36866;&#24212;&#20923;&#32467;&#30340;CLIP&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#39044;&#27979;&#30340;&#25513;&#27169;&#25552;&#26696;&#24314;&#31435;&#22312;CLIP&#30340;&#22522;&#30784;&#19978;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24555;&#36895;&#12289;&#20934;&#30830;&#65292;&#20165;&#22686;&#21152;&#20102;&#19968;&#20123;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new framework for open-vocabulary semantic segmentation with the pre-trained vision-language model, named Side Adapter Network (SAN). Our approach models the semantic segmentation task as a region recognition problem. A side network is attached to a frozen CLIP model with two branches: one for predicting mask proposals, and the other for predicting attention bias which is applied in the CLIP model to recognize the class of masks. This decoupled design has the benefit CLIP in recognizing the class of mask proposals. Since the attached side network can reuse CLIP features, it can be very light. In addition, the entire network can be trained end-to-end, allowing the side network to be adapted to the frozen CLIP model, which makes the predicted mask proposals CLIP-aware. Our approach is fast, accurate, and only adds a few additional trainable parameters. We evaluate our approach on multiple semantic segmentation benchmarks. Our method significantly outperforms other c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07260</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20808;&#39564;&#32593;&#32476;&#30340;&#39640;&#32500;&#36755;&#20986;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#28041;&#21450;&#21040;&#26410;&#30693;&#30340;&#39640;&#32500;&#24230;&#26144;&#23556;&#19968;&#32452;&#21487;&#25511;&#21464;&#37327;&#21040;&#26114;&#36149;&#23454;&#39564;&#32467;&#26524;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#20219;&#21153;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#26102;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24403;&#22788;&#29702;&#39640;&#32500;&#36755;&#20986;&#26102;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#32500;&#24230;&#20027;&#35201;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#20030;&#38598;&#25104;&#30340;BO&#21644;&#24207;&#36143;&#20915;&#31574;&#21046;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#20351;&#29992;&#36866;&#24403;&#30340;&#20307;&#31995;&#32467;&#26500;&#36873;&#25321;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#36817;&#20284;&#35774;&#35745;&#21464;&#37327;&#21644;&#24863;&#20852;&#36259;&#37327;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#65292;&#21363;&#20351;&#22312;&#21518;&#32773;&#21462;&#20540;&#20110;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#25110;&#29978;&#33267;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22788;&#29702;&#39640;&#32500;&#24230;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
&lt;/p&gt;</description></item><item><title>DeepAstroUDA&#26159;&#19968;&#31181;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#36328;&#35843;&#26597;&#38134;&#27827;&#24418;&#24577;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#21644;&#31867;&#21035;&#37325;&#21472;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2302.02005</link><description>&lt;p&gt;
DeepAstroUDA: &#38754;&#21521;&#36328;&#35843;&#26597;&#38134;&#27827;&#24418;&#24577;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#21322;&#30417;&#30563;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DeepAstroUDA: Semi-Supervised Universal Domain Adaptation for Cross-Survey Galaxy Morphology Classification and Anomaly Detection. (arXiv:2302.02005v2 [astro-ph.GA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02005
&lt;/p&gt;
&lt;p&gt;
DeepAstroUDA&#26159;&#19968;&#31181;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#36328;&#35843;&#26597;&#38134;&#27827;&#24418;&#24577;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#21644;&#31867;&#21035;&#37325;&#21472;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#25552;&#39640;&#23545;&#22823;&#22411;&#22825;&#25991;&#25968;&#25454;&#38598;&#30340;&#22788;&#29702;&#36136;&#37327;&#21644;&#36895;&#24230;&#26041;&#38754;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#39640;&#22797;&#26434;&#24615;&#23548;&#33268;&#25552;&#21462;&#25968;&#25454;&#38598;&#29305;&#23450;&#12289;&#19981;&#20855;&#40065;&#26834;&#24615;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#31216;&#20026; DeepAstroUDA&#65292;&#20316;&#20026;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#30340;&#36884;&#24452;&#12290;&#36825;&#20010;&#31639;&#27861;&#25191;&#34892;&#21322;&#30417;&#30563;&#22495;&#36866;&#24212;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#21644;&#31867;&#21035;&#37325;&#21472;&#30340;&#25968;&#25454;&#38598;&#12290;&#38750;&#37325;&#21472;&#31867;&#21487;&#20197;&#23384;&#22312;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#20219;&#20309;&#19968;&#20010;&#65288;&#26377;&#26631;&#31614;&#30340;&#28304;&#22495;&#25110;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#22495;&#65289;&#65292;&#32780;&#19988;&#21363;&#20351;&#23384;&#22312;&#26410;&#30693;&#31867;&#21035;&#65292;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#20010;&#19981;&#21516;&#22797;&#26434;&#24230;&#65288;$3$&#31867;&#21644;$10$&#31867;&#38382;&#39064;&#65289;&#30340;&#38134;&#27827;&#24418;&#24577;&#20998;&#31867;&#20219;&#21153;&#30340;&#20363;&#23376;&#65292;&#21253;&#25324;&#24322;&#24120;&#26816;&#27979;: 1&#65289;&#26469;&#33258;&#21208;&#27979;&#26395;&#36828;&#38236;&#19981;&#21516;&#35266;&#27979;&#24180;&#25968;&#30340;&#25968;&#25454;&#38598;&#65292;2&#65289;&#24102;&#26377;&#38750;&#26631;&#20934;&#28388;&#38236;&#30340;&#28145;&#24230;&#21208;&#27979;&#65292;&#20197;&#21450;3&#65289;&#19981;&#21516;&#36864;&#21270;&#32423;&#21035;&#30340;&#27169;&#25311;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DeepAstroUDA&#22312;&#20998;&#31867;&#31934;&#24230;&#26041;&#38754;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20419;&#36827;&#26356;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#22825;&#25991;&#20998;&#31867;&#31995;&#32479;&#30340;&#21019;&#24314;&#65292;&#24182;&#21487;&#20197;&#25552;&#39640;&#22825;&#25991;&#22270;&#20687;&#20013;&#24322;&#24120;&#26816;&#27979;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence methods show great promise in increasing the quality and speed of work with large astronomical datasets, but the high complexity of these methods leads to the extraction of dataset-specific, non-robust features. Therefore, such methods do not generalize well across multiple datasets. We present a universal domain adaptation method, \textit{DeepAstroUDA}, as an approach to overcome this challenge. This algorithm performs semi-supervised domain adaptation and can be applied to datasets with different data distributions and class overlaps. Non-overlapping classes can be present in any of the two datasets (the labeled source domain, or the unlabeled target domain), and the method can even be used in the presence of unknown classes. We apply our method to three examples of galaxy morphology classification tasks of different complexities ($3$-class and $10$-class problems), with anomaly detection: 1) datasets created after different numbers of observing years from a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26080;&#21160;&#20316;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;AF-Guide&#65292;&#23454;&#29616;&#21464;&#20307;&#30340;Upside-Down&#24378;&#21270;&#23398;&#20064;&#21644;&#25351;&#23548;&#22312;&#32447;&#23398;&#20064;&#30340;Guided SAC&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#25913;&#21892;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#19981;&#23384;&#22312;&#21160;&#20316;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12876</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#21160;&#20316;&#31163;&#32447;&#25968;&#25454;&#39044;&#35757;&#32451;&#25351;&#23548;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Guiding Online Reinforcement Learning with Action-Free Offline Pretraining. (arXiv:2301.12876v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26080;&#21160;&#20316;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;AF-Guide&#65292;&#23454;&#29616;&#21464;&#20307;&#30340;Upside-Down&#24378;&#21270;&#23398;&#20064;&#21644;&#25351;&#23548;&#22312;&#32447;&#23398;&#20064;&#30340;Guided SAC&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#25913;&#21892;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#19981;&#23384;&#22312;&#21160;&#20316;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;RL&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#20351;&#29992;&#31163;&#32447;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#20943;&#23569;&#29615;&#22659;&#20132;&#20114;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22312;&#25968;&#25454;&#25910;&#38598;&#26399;&#38388;&#35760;&#24405;&#21160;&#20316;&#20449;&#24687;&#65292;&#36825;&#22312;&#26576;&#20123;&#23454;&#38469;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#22256;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#26080;&#21160;&#20316;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#25913;&#21892;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#24182;&#23558;&#27492;&#38382;&#39064;&#21629;&#21517;&#20026;&#20855;&#26377;&#26080;&#21160;&#20316;&#31163;&#32447;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;AFP-RL&#65289;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;AF-Guide&#65292;&#19968;&#31181;&#36890;&#36807;&#20174;&#26080;&#21160;&#20316;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#26469;&#25351;&#23548;&#22312;&#32447;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;AF-Guide&#21253;&#25324;&#23454;&#26045;Upside-Down&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#30340;&#26080;&#21160;&#20316;&#20915;&#31574;Transformer&#65288;AFDT&#65289;&#65292;&#23427;&#23398;&#20064;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35268;&#21010;&#19979;&#19968;&#20010;&#29366;&#24577;&#65292;&#20197;&#21450;&#19968;&#20010;&#36890;&#36807;AFDT&#25351;&#23548;&#22312;&#32447;&#23398;&#20064;&#30340;Guided Soft Actor-Critic&#65288;Guided SAC&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#19981;&#23384;&#22312;&#21160;&#20316;&#20449;&#24687;&#30340;&#29615;&#22659;&#20013;&#65292;AF-Guide&#21487;&#20197;&#25552;&#39640;&#22312;&#32447;RL&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline RL methods have been shown to reduce the need for environment interaction by training agents using offline collected episodes. However, these methods typically require action information to be logged during data collection, which can be difficult or even impossible in some practical cases. In this paper, we investigate the potential of using action-free offline datasets to improve online reinforcement learning, name this problem Reinforcement Learning with Action-Free Offline Pretraining (AFP-RL). We introduce Action-Free Guide (AF-Guide), a method that guides online training by extracting knowledge from action-free offline datasets. AF-Guide consists of an Action-Free Decision Transformer (AFDT) implementing a variant of Upside-Down Reinforcement Learning. It learns to plan the next states from the offline dataset, and a Guided Soft Actor-Critic (Guided SAC) that learns online with guidance from AFDT. Experimental results show that AF-Guide can improve sample efficiency and pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NextG&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;AI&#23433;&#20840;&#20445;&#35777;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#22312;&#20445;&#35777;&#20449;&#21495;&#20256;&#36755;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.09668</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#36890;&#20449;&#31995;&#32479;&#65306;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;AI&#23433;&#20840;&#30340;NextG&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented Communications for NextG: End-to-End Deep Learning and AI Security Aspects. (arXiv:2212.09668v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NextG&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;AI&#23433;&#20840;&#20445;&#35777;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#22312;&#20445;&#35777;&#20449;&#21495;&#20256;&#36755;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#36890;&#20449;&#31995;&#32479;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#21487;&#38752;&#22320;&#20256;&#36755;&#25968;&#23383;&#25968;&#25454;&#27969;&#65288;&#27604;&#29305;&#27969;&#65289;&#65292;&#32780;&#26032;&#19968;&#20195;&#36890;&#20449;&#31995;&#32479;&#65288;NextG&#65289;&#24320;&#22987;&#25506;&#32034;&#25226;&#35774;&#35745;&#33539;&#24335;&#36716;&#21521;&#21487;&#38752;&#22320;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#65292;&#20363;&#22914;&#20219;&#21153;&#23548;&#21521;&#30340;&#36890;&#20449;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26080;&#32447;&#20449;&#21495;&#20998;&#31867;&#20316;&#20026;NextG&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;RAN&#65289;&#30340;&#20219;&#21153;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#25910;&#38598;&#26080;&#32447;&#20449;&#21495;&#20197;&#33719;&#21462;&#39057;&#35889;&#24863;&#30693;&#65292;&#28982;&#21518;&#19982;&#38656;&#35201;&#35782;&#21035;&#20449;&#21495;&#26631;&#31614;&#30340;NextG&#22522;&#31449;&#65288;gNodeB&#65289;&#36827;&#34892;&#36890;&#20449;&#12290;&#28982;&#32780;&#36793;&#32536;&#35774;&#22791;&#21487;&#33021;&#26080;&#27861;&#36275;&#22815;&#22788;&#29702;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#30001;&#20110;&#20005;&#26684;&#30340;&#24310;&#36831;&#12289;&#36895;&#29575;&#21644;&#33021;&#37327;&#38480;&#21046;&#65292;&#23558;&#20449;&#21495;&#20256;&#36755;&#21040;gNodeB&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21457;&#23556;&#26426;&#12289;&#25509;&#25910;&#26426;&#21644;&#20998;&#31867;&#22120;&#30340;&#21151;&#33021;&#20316;&#20026;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#65292;&#20026;&#36793;&#32536;&#35774;&#22791;&#21644;gNodeB&#23454;&#29616;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communications systems to date are primarily designed with the goal of reliable transfer of digital sequences (bits). Next generation (NextG) communication systems are beginning to explore shifting this design paradigm to reliably executing a given task such as in task-oriented communications. In this paper, wireless signal classification is considered as the task for the NextG Radio Access Network (RAN), where edge devices collect wireless signals for spectrum awareness and communicate with the NextG base station (gNodeB) that needs to identify the signal label. Edge devices may not have sufficient processing power and may not be trusted to perform the signal classification task, whereas the transfer of signals to the gNodeB may not be feasible due to stringent delay, rate, and energy restrictions. Task-oriented communications is considered by jointly training the transmitter, receiver and classifier functionalities as an encoder-decoder pair for the edge device and the gNodeB. This a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#23558;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#65292;&#20174;&#32780;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#32771;&#34385;&#21040;&#23545;&#35937;&#24418;&#29366;&#65292;&#36827;&#32780;&#21487;&#20197;&#23454;&#29616;&#23545;&#35937;&#24418;&#29366;&#24544;&#23454;&#24230;&#26356;&#39640;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2212.00210</link><description>&lt;p&gt;
&#24102;&#26377;&#20869;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Shape-Guided Diffusion with Inside-Outside Attention. (arXiv:2212.00210v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#23558;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#65292;&#20174;&#32780;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#32771;&#34385;&#21040;&#23545;&#35937;&#24418;&#29366;&#65292;&#36827;&#32780;&#21487;&#20197;&#23454;&#29616;&#23545;&#35937;&#24418;&#29366;&#24544;&#23454;&#24230;&#26356;&#39640;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25805;&#20316;&#23545;&#35937;&#26102;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#24573;&#30053;&#23545;&#35937;&#30340;&#24418;&#29366;&#24182;&#29983;&#25104;&#38169;&#35823;&#27604;&#20363;&#12289;&#34987;&#25130;&#26029;&#25110;&#34987;&#32972;&#26223;&#20869;&#23481;&#26367;&#25442;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861; Shape-Guided Diffusion&#65292;&#35813;&#26041;&#27861;&#20462;&#25913;&#20102;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#20043;&#23545;&#29992;&#25143;&#25351;&#23450;&#30340;&#24418;&#29366;&#36755;&#20837;&#25110;&#20174;&#25991;&#26412;&#20013;&#33258;&#21160;&#25512;&#26029;&#30340;&#24418;&#29366;&#25935;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#21453;&#28436;&#21644;&#29983;&#25104;&#36807;&#31243;&#20013;&#23558;&#27492;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#12290;&#25105;&#20204;&#30340;&#26426;&#21046;&#25351;&#23450;&#31354;&#38388;&#21306;&#22495;&#26159;&#23545;&#35937;&#65288;&#20869;&#37096;&#65289;&#36824;&#26159;&#32972;&#26223;&#65288;&#22806;&#37096;&#65289;&#65292;&#28982;&#21518;&#23558;&#25991;&#26412;&#25552;&#31034;&#25351;&#23450;&#30340;&#32534;&#36753;&#19982;&#27491;&#30830;&#30340;&#21306;&#22495;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#22312;&#24418;&#29366;&#24341;&#23548;&#32534;&#36753;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#21644;&#23545;&#35937;&#25513;&#30721;&#26367;&#25442;&#23545;&#35937;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#20174; MS-COCO &#34893;&#29983;&#30340; ShapePrompts &#22522;&#20934;&#65292;&#24182;&#22312;&#24418;&#29366;&#24544;&#23454;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102; SOTA &#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#38656;&#35201;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
When manipulating an object, existing text-to-image diffusion models often ignore the shape of the object and generate content that is incorrectly scaled, cut off, or replaced with background content. We propose a training-free method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be sensitive to shape input specified by a user or automatically inferred from text. We use a novel Inside-Outside Attention mechanism during the inversion and generation process to apply this shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits specified by text prompts to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degra
&lt;/p&gt;</description></item><item><title>AIREPAIR&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#24179;&#21488;&#65292;&#23427;&#33021;&#22815;&#38598;&#25104;&#29616;&#26377;&#32593;&#32476;&#20462;&#22797;&#24037;&#20855;&#24182;&#23454;&#29616;&#19981;&#21516;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15387</link><description>&lt;p&gt;
AIREPAIR&#8212;&#8212;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
AIREPAIR: A Repair Platform for Neural Networks. (arXiv:2211.15387v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15387
&lt;/p&gt;
&lt;p&gt;
AIREPAIR&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#24179;&#21488;&#65292;&#23427;&#33021;&#22815;&#38598;&#25104;&#29616;&#26377;&#32593;&#32476;&#20462;&#22797;&#24037;&#20855;&#24182;&#23454;&#29616;&#19981;&#21516;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AIREPAIR&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#24179;&#21488;&#12290;&#23427;&#21253;&#25324;&#20102;&#29616;&#26377;&#32593;&#32476;&#20462;&#22797;&#24037;&#20855;&#30340;&#38598;&#25104;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#22312;&#21516;&#19968;&#20010;&#27169;&#22411;&#19978;&#36816;&#34892;&#19981;&#21516;&#30340;&#32593;&#32476;&#20462;&#22797;&#26041;&#27861;&#24182;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#20462;&#22797;&#24037;&#20855;&#22312;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#35780;&#20272;&#20102;AIREPAIR&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;AIREPAIR&#30340;&#23454;&#29992;&#24615;&#65292;&#36890;&#36807;&#27604;&#36739;&#21644;&#20998;&#26512;&#19981;&#21516;&#20462;&#22797;&#25216;&#26415;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#28436;&#31034;&#35270;&#39057;&#21487;&#22312; https://youtu.be/UkKw5neeWhw &#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present AIREPAIR, a platform for repairing neural networks. It features the integration of existing network repair tools. Based on AIREPAIR, one can run different repair methods on the same model, thus enabling the fair comparison of different repair techniques. We evaluate AIREPAIR with three state-of-the-art repair tools on popular deep-learning datasets and models. Our evaluation confirms the utility of AIREPAIR, by comparing and analyzing the results from different repair techniques. A demonstration is available at https://youtu.be/UkKw5neeWhw.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24037;&#20855;Melting Pot 2.0&#20026;&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#35780;&#20272;&#21327;&#35758;&#65292;&#22312;&#19968;&#32452;&#20856;&#22411;&#27979;&#35797;&#22330;&#26223;&#20013;&#27979;&#37327;&#23427;&#20204;&#23545;&#26032;&#39062;&#31038;&#20132;&#20249;&#20276;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13746</link><description>&lt;p&gt;
&#29076;&#28809;2.0
&lt;/p&gt;
&lt;p&gt;
Melting Pot 2.0. (arXiv:2211.13746v4 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13746
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24037;&#20855;Melting Pot 2.0&#20026;&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#35780;&#20272;&#21327;&#35758;&#65292;&#22312;&#19968;&#32452;&#20856;&#22411;&#27979;&#35797;&#22330;&#26223;&#20013;&#27979;&#37327;&#23427;&#20204;&#23545;&#26032;&#39062;&#31038;&#20132;&#20249;&#20276;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25215;&#35834;&#24320;&#21457;&#27604;&#8220;&#33258;&#25105;&#20013;&#24515;&#8221;&#26041;&#27861;&#26356;&#20855;&#20154;&#31867;&#29305;&#28857;&#21644;&#26356;&#26131;&#20110;&#19982;&#20154;&#31867;&#20860;&#23481;&#30340;&#26234;&#33021;&#25216;&#26415;&#12290; Melting Pot&#26159;&#20026;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20154;&#24037;&#26234;&#33021;&#24037;&#20316;&#32780;&#24320;&#21457;&#30340;&#30740;&#31350;&#24037;&#20855;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#22312;&#19968;&#32452;&#20856;&#22411;&#30340;&#27979;&#35797;&#22330;&#26223;&#20013;&#27979;&#37327;&#23545;&#26032;&#39062;&#31038;&#20132;&#20249;&#20276;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27599;&#31181;&#24773;&#26223;&#23558;&#19968;&#20010;&#29289;&#29702;&#29615;&#22659;&#65288;&#8220;&#22522;&#26495;&#8221;&#65289;&#19982;&#19968;&#32452;&#21442;&#32771;&#21512;&#20316;&#32773;&#65288;&#8220;&#32972;&#26223;&#20154;&#32676;&#8221;&#65289;&#37197;&#23545;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#20010;&#20307;&#38388;&#30456;&#20114;&#20381;&#23384;&#24615;&#30340;&#31038;&#20132;&#24773;&#22659;&#12290;&#20363;&#22914;&#65292;&#19968;&#20123;&#24773;&#24418;&#21463;&#21040;&#20102;&#22522;&#20110;&#21046;&#24230;&#32463;&#27982;&#23398;&#30340;&#33258;&#28982;&#36164;&#28304;&#31649;&#29702;&#21644;&#20844;&#20849;&#29289;&#21697;&#20379;&#32473;&#22256;&#22659;&#30340;&#32771;&#34385;&#30340;&#21551;&#21457;&#65292;&#32780;&#20854;&#20182;&#24773;&#24418;&#21017;&#21463;&#21040;&#20102;&#36827;&#21270;&#29983;&#29289;&#23398;&#12289;&#21338;&#24328;&#35770;&#21644;&#20154;&#24037;&#29983;&#21629;&#31561;&#26041;&#38754;&#30340;&#32771;&#34385;&#25152;&#21551;&#21457;&#12290;Melting Pot&#26088;&#22312;&#28085;&#30422;&#19968;&#32452;&#26368;&#22823;&#22810;&#26679;&#21270;&#30340;&#24773;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent artificial intelligence research promises a path to develop intelligent technologies that are more human-like and more human-compatible than those produced by "solipsistic" approaches, which do not consider interactions between agents. Melting Pot is a research tool developed to facilitate work on multi-agent artificial intelligence, and provides an evaluation protocol that measures generalization to novel social partners in a set of canonical test scenarios. Each scenario pairs a physical environment (a "substrate") with a reference set of co-players (a "background population"), to create a social situation with substantial interdependence between the individuals involved. For instance, some scenarios were inspired by institutional-economics-based accounts of natural resource management and public-good-provision dilemmas. Others were inspired by considerations from evolutionary biology, game theory, and artificial life. Melting Pot aims to cover a maximally diverse set of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#65292;&#21487;&#25214;&#21040;&#21487;&#34892;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2211.13436</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Bilevel Knapsack Problem using Graph Neural Networks. (arXiv:2211.13436v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#65292;&#21487;&#25214;&#21040;&#21487;&#34892;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26159;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#20195;&#29702;&#20154;&#65288;&#39046;&#23548;&#32773;&#21644;&#36861;&#38543;&#32773;&#65289;&#30340;&#23618;&#27425;&#20248;&#21270;&#38382;&#39064;&#12290;&#39046;&#23548;&#32773;&#39318;&#20808;&#20570;&#20986;&#33258;&#24049;&#30340;&#20915;&#31574;&#65292;&#36861;&#38543;&#32773;&#38543;&#21518;&#20570;&#20986;&#26368;&#20339;&#36873;&#25321;&#12290;&#39046;&#23548;&#32773;&#20102;&#35299;&#36861;&#38543;&#32773;&#30340;&#20449;&#24687;&#65292;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#20174;&#39046;&#23548;&#32773;&#30340;&#35282;&#24230;&#32771;&#34385;&#36861;&#38543;&#32773;&#30340;&#21453;&#24212;&#65292;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#23545;&#20110;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26469;&#35828;&#65292;&#27809;&#26377;&#36890;&#29992;&#30340;&#39640;&#25928;&#31639;&#27861;&#25110;&#21830;&#29992;&#27714;&#35299;&#22120;&#21487;&#20197;&#24471;&#21040;&#26368;&#20248;&#35299;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#38382;&#39064;&#20063;&#24456;&#38590;&#24471;&#21040;&#33391;&#22909;&#30340;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#39046;&#23548;&#32773;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23558;&#23618;&#27425;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#23618;&#20248;&#21270;&#38382;&#39064;&#20197;&#33719;&#21462;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21457;&#29616;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36895;&#24230;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bilevel Optimization Problem is a hierarchical optimization problem with two agents, a leader and a follower. The leader make their own decisions first, and the followers make the best choices accordingly. The leader knows the information of the followers, and the goal of the problem is to find the optimal solution by considering the reactions of the followers from the leader's point of view. For the Bilevel Optimization Problem, there are no general and efficient algorithms or commercial solvers to get an optimal solution, and it is very difficult to get a good solution even for a simple problem. In this paper, we propose a deep learning approach using Graph Neural Networks to solve the bilevel knapsack problem. We train the model to predict the leader's solution and use it to transform the hierarchical optimization problem into a single-level optimization problem to get the solution. Our model found the feasible solution that was about 500 times faster than the exact algorithm wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#35270;&#39057;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21327;&#20316;&#25552;&#31034;&#35843;&#25972;&#65288;VoP&#65289;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;VoP&#20855;&#26377;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.12764</link><description>&lt;p&gt;
VoP&#65306;&#29992;&#20110;&#25991;&#26412;&#35270;&#39057;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21327;&#20316;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval. (arXiv:2211.12764v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#35270;&#39057;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21327;&#20316;&#25552;&#31034;&#35843;&#25972;&#65288;VoP&#65289;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;VoP&#20855;&#26377;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#26469;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#37325;&#27169;&#22359;&#26469;&#35843;&#25972;backbone&#20174;&#32780;&#23454;&#29616;&#25991;&#26412;&#35270;&#39057;&#36328;&#27169;&#24577;&#26816;&#32034;&#65292;&#36825;&#19981;&#20165;&#24102;&#26469;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#26356;&#22810;&#30340;&#21442;&#25968;&#65292;&#20063;&#23548;&#33268;&#20102;&#19978;&#28216;&#27169;&#22411;&#30340;&#30693;&#35782;&#36951;&#24536;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;VoP&#65306;&#25991;&#26412;&#35270;&#39057;&#21327;&#20316;&#25552;&#31034;&#35843;&#25972;&#65292;&#20197;&#23454;&#29616;&#23545;&#25991;&#26412;&#35270;&#39057;&#26816;&#32034;&#20219;&#21153;&#30340;&#39640;&#25928;&#35843;&#25972;&#12290;&#25152;&#25552;&#20986;&#30340;VoP&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20855;&#26377;&#24341;&#20837;&#35270;&#39057;&#21644;&#25991;&#26412;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#21487;&#35270;&#20026;&#20855;&#26377;&#20165;0.1&#65285;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24378;&#22823;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#35270;&#39057;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26032;&#22411;&#35270;&#39057;&#25552;&#31034;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#19981;&#21516;&#21487;&#35757;&#32451;&#21442;&#25968;&#35268;&#27169;&#30340;&#24615;&#33021;&#12290;VoP&#22686;&#24378;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20998;&#21035;&#21033;&#29992;&#29305;&#23450;&#30340;&#21487;&#35757;&#32451;&#25552;&#31034;&#26469;&#27169;&#25311;&#24103;&#20301;&#32622;&#65292;&#24103;&#19978;&#19979;&#25991;&#21644;&#23618;&#20989;&#25968;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30456;&#27604;&#65292;&#22686;&#24378;&#30340;VoP&#23454;&#29616;&#20102;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent studies leverage the pre-trained CLIP for text-video cross-modal retrieval by tuning the backbone with additional heavy modules, which not only brings huge computational burdens with much more parameters, but also leads to the knowledge forgetting from upstream models. In this work, we propose the VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the text-video retrieval task. The proposed VoP is an end-to-end framework with both video &amp; text prompts introducing, which can be regarded as a powerful baseline with only 0.1% trainable parameters. Further, based on the spatio-temporal characteristics of videos, we develop three novel video prompt mechanisms to improve the performance with different scales of trainable parameters. The basic idea of the VoP enhancement is to model the frame position, frame context, and layer function with specific trainable prompts, respectively. Extensive experiments show that compared to full fine-tuning, the enhanced VoP achie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#20808;&#35753;&#27169;&#22411;&#23398;&#20064;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#19981;&#26029;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2211.09703</link><description>&lt;p&gt;
&#39640;&#25928;&#35757;&#32451;&#65306;&#25506;&#32034;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26469;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones. (arXiv:2211.09703v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#20808;&#35753;&#27169;&#22411;&#23398;&#20064;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#19981;&#26029;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#30340;&#21331;&#36234;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#26114;&#36149;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65288;&#20363;&#22914;&#35270;&#35273;Transformer&#65289;&#12290;&#26412;&#25991;&#21551;&#21457;&#20110;&#28145;&#24230;&#32593;&#32476;&#30340;&#20869;&#22312;&#23398;&#20064;&#21160;&#21147;&#23398;&#65306;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#22312;&#36739;&#26089;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#27169;&#22411;&#20027;&#35201;&#23398;&#20064;&#22312;&#27599;&#20010;&#31034;&#20363;&#20013;&#35782;&#21035;&#19968;&#20123;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#21028;&#21035;&#27169;&#24335;&#65292;&#20363;&#22914;&#22270;&#20687;&#30340;&#20302;&#39057;&#25104;&#20998;&#21644;&#25968;&#25454;&#22686;&#24191;&#20043;&#21069;&#30340;&#21407;&#22987;&#20449;&#24687;&#12290;&#22522;&#20110;&#27492;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#65292;&#20854;&#20013;&#27169;&#22411;&#24635;&#26159;&#22312;&#27599;&#20010;&#26102;&#26399;&#21033;&#29992;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#35838;&#31243;&#22987;&#20110;&#20165;&#26292;&#38706;&#27599;&#20010;&#31034;&#20363;&#30340;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#24182;&#36880;&#28176;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;1&#65289;&#22312;&#36755;&#20837;&#30340;&#20613;&#37324;&#21494;&#35889;&#20013;&#24341;&#20837;&#19968;&#20010;&#35009;&#21098;&#25805;&#20316;&#65292;&#20351;&#27169;&#22411;&#21482;&#33021;&#20174;&#20302;&#39057;&#32452;&#20998;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The superior performance of modern deep networks usually comes with a costly training procedure. This paper presents a new curriculum learning approach for the efficient training of visual backbones (e.g., vision Transformers). Our work is inspired by the inherent learning dynamics of deep networks: we experimentally show that at an earlier training stage, the model mainly learns to recognize some 'easier-to-learn' discriminative patterns within each example, e.g., the lower-frequency components of images and the original information before data augmentation. Driven by this phenomenon, we propose a curriculum where the model always leverages all the training data at each epoch, while the curriculum starts with only exposing the 'easier-to-learn' patterns of each example, and introduces gradually more difficult patterns. To implement this idea, we 1) introduce a cropping operation in the Fourier spectrum of the inputs, which enables the model to learn from only the lower-frequency compo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#38170;&#28857;&#30340;&#23376;&#31354;&#38388;&#23398;&#20064;&#12289;&#21452;&#21521;&#22270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20248;&#21270;&#23454;&#29616;&#20102;&#21333;&#35270;&#22270;&#21644;&#19968;&#33268;&#24615;&#22270;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2209.04187</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#32479;&#19968;&#31163;&#25955;&#30340;&#21452;&#21521;&#22270;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-view Clustering via Unified and Discrete Bipartite Graph Learning. (arXiv:2209.04187v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#38170;&#28857;&#30340;&#23376;&#31354;&#38388;&#23398;&#20064;&#12289;&#21452;&#21521;&#22270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20248;&#21270;&#23454;&#29616;&#20102;&#21333;&#35270;&#22270;&#21644;&#19968;&#33268;&#24615;&#22270;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20197;&#24448;&#22522;&#20110;&#22270;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#31639;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#24448;&#24448;&#36973;&#21463;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#22256;&#25200;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#36890;&#24120;&#22312;&#21333;&#35270;&#22270;&#32423;&#21035;&#25110;&#35270;&#22270;&#19968;&#33268;&#24615;&#32423;&#21035;&#19978;&#25191;&#34892;&#22270;&#23398;&#20064;&#65292;&#20294;&#24120;&#24120;&#24573;&#30053;&#20102;&#21333;&#35270;&#22270;&#21644;&#19968;&#33268;&#24615;&#22270;&#30340;&#32852;&#21512;&#23398;&#20064;&#21487;&#33021;&#24615;&#12290;&#31532;&#19977;&#65292;&#35768;&#22810;&#31639;&#27861;&#20381;&#36182;&#20110;k-means&#23545;&#35889;&#23884;&#20837;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#20294;&#32570;&#20047;&#30452;&#25509;&#23398;&#20064;&#24102;&#31163;&#25955;&#32858;&#31867;&#32467;&#26500;&#30340;&#22270;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32479;&#19968;&#31163;&#25955;&#30340;&#21452;&#21521;&#22270;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#26041;&#27861;&#65288;UDBGL&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#22522;&#20110;&#38170;&#28857;&#30340;&#23376;&#31354;&#38388;&#23398;&#20064;&#65292;&#20174;&#22810;&#20010;&#35270;&#22270;&#20013;&#23398;&#20064;&#20102;&#35270;&#22270;&#29305;&#23450;&#30340;&#21452;&#21521;&#22270;&#65292;&#28982;&#21518;&#21033;&#29992;&#21452;&#21521;&#22270;&#34701;&#21512;&#23398;&#20064;&#20102;&#35270;&#22270;&#19968;&#33268;&#24615;&#21452;&#21521;&#22270;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;UDBGL&#36890;&#36807;&#37319;&#29992;&#20302;&#31209;&#30697;&#38453;&#36817;&#20284;&#21644;&#21152;&#36895;&#20132;&#26367;&#26041;&#21521;&#20056;&#25968;&#27861;&#36991;&#20813;&#20102;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#32479;&#19968;&#30446;&#26631;&#20989;&#25968;&#65292;&#23427;&#23454;&#29616;&#20102;&#21333;&#35270;&#22270;&#21644;&#19968;&#33268;&#24615;&#22270;&#30340;&#32852;&#21512;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;&#31532;&#19977;&#20010;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#30452;&#25509;&#23398;&#20064;&#25152;&#23398;&#22270;&#30340;&#31163;&#25955;&#32858;&#31867;&#32467;&#26500;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UDBGL&#20248;&#20110;&#20960;&#31181;&#26368;&#26032;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although previous graph-based multi-view clustering algorithms have gained significant progress, most of them are still faced with three limitations. First, they often suffer from high computational complexity, which restricts their applications in large-scale scenarios. Second, they usually perform graph learning either at the single-view level or at the view-consensus level, but often neglect the possibility of the joint learning of single-view and consensus graphs. Third, many of them rely on the k-means for discretization of the spectral embeddings, which lack the ability to directly learn the graph with discrete cluster structure. In light of this, this paper presents an efficient multi-view clustering approach via unified and discrete bipartite graph learning (UDBGL). Specifically, the anchor-based subspace learning is incorporated to learn the view-specific bipartite graphs from multiple views, upon which the bipartite graph fusion is leveraged to learn a view-consensus bipartit
&lt;/p&gt;</description></item><item><title>&#38463;&#23572;&#20271;&#22612;&#35745;&#21010;&#26159;&#19968;&#39033;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#35745;&#21010;&#65292;&#22312;&#38463;&#23572;&#20271;&#22612;&#30340;&#30740;&#31350;&#22242;&#38431;&#21450;&#20840;&#29699;&#24535;&#21516;&#36947;&#21512;&#30340;&#20154;&#22763;&#20013;&#36827;&#34892;&#65292;&#26088;&#22312;&#25506;&#32034;AI&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.11173</link><description>&lt;p&gt;
&#38463;&#23572;&#20271;&#22612;AI&#30740;&#31350;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
The Alberta Plan for AI Research. (arXiv:2208.11173v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11173
&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#20271;&#22612;&#35745;&#21010;&#26159;&#19968;&#39033;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#35745;&#21010;&#65292;&#22312;&#38463;&#23572;&#20271;&#22612;&#30340;&#30740;&#31350;&#22242;&#38431;&#21450;&#20840;&#29699;&#24535;&#21516;&#36947;&#21512;&#30340;&#20154;&#22763;&#20013;&#36827;&#34892;&#65292;&#26088;&#22312;&#25506;&#32034;AI&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38463;&#23572;&#20271;&#22612;&#35745;&#21010;&#12290;&#38463;&#23572;&#20271;&#22612;&#35745;&#21010;&#22312;&#38463;&#23572;&#20271;&#22612;&#30340;&#30740;&#31350;&#22242;&#38431;&#20197;&#21450;&#19990;&#30028;&#21508;&#22320;&#24535;&#21516;&#36947;&#21512;&#30340;&#20154;&#22763;&#20013;&#24320;&#23637;&#12290;&#25105;&#20204;&#27426;&#36814;&#25152;&#26377;&#24895;&#24847;&#21152;&#20837;&#25105;&#20204;&#36825;&#19968;&#36861;&#27714;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Herein we describe our approach to artificial intelligence research, which we call the Alberta Plan. The Alberta Plan is pursued within our research groups in Alberta and by others who are like minded throughout the world. We welcome all who would join us in this pursuit.
&lt;/p&gt;</description></item><item><title>Laser &#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20195;&#29702;&#21644;&#20854;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#30340;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.04589</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Long-term Causal Effects Estimation via Latent Surrogates Representation Learning. (arXiv:2208.04589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04589
&lt;/p&gt;
&lt;p&gt;
Laser &#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20195;&#29702;&#21644;&#20854;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#30340;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#22914;&#33829;&#38144;&#21644;&#21307;&#23398;&#20013;&#65292;&#22522;&#20110;&#30701;&#26399;&#20195;&#29702;&#26469;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#26576;&#20123;&#39046;&#22495;&#20013;&#24050;&#26377;&#25152;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20197;&#19968;&#31181;&#29702;&#24819;&#21270;&#21644;&#31616;&#21270;&#30340;&#26041;&#24335;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24573;&#30053;&#20102;&#30701;&#26399;&#32467;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#23558;&#23427;&#20204;&#20840;&#37096;&#35270;&#20026;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65292;&#20854;&#20013;&#23616;&#37096;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#19982;&#23427;&#20204;&#22312;&#30701;&#26399;&#32467;&#26524;&#20013;&#30340;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Laser&#65292;&#20197;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#65292;&#20854;&#20013;&#35266;&#23519;&#21040;&#20195;&#29702;&#25110;&#20855;&#26377;&#35266;&#23519;&#20195;&#29702;&#12290;&#37492;&#20110;&#20195;&#29702;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#19981;&#21487;&#21306;&#20998;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#35782;&#21035;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;iVAE&#65289;&#22312;&#19981;&#38656;&#35201;&#21306;&#20998;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#25110;&#20808;&#39564;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#25152;&#26377;&#26377;&#25928;&#20195;&#29702;&#20505;&#36873;&#32773;&#19978;&#30340;&#25972;&#20010;&#26377;&#25928;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating long-term causal effects based on short-term surrogates is a significant but challenging problem in many real-world applications, e.g., marketing and medicine. Despite its success in certain domains, most existing methods estimate causal effects in an idealistic and simplistic way - ignoring the causal structure among short-term outcomes and treating all of them as surrogates. However, such methods cannot be well applied to real-world scenarios, in which the partially observed surrogates are mixed with their proxies among short-term outcomes. To this end, we develop our flexible method, Laser, to estimate long-term causal effects in the more realistic situation that the surrogates are observed or have observed proxies.Given the indistinguishability between the surrogates and proxies, we utilize identifiable variational auto-encoder (iVAE) to recover the whole valid surrogates on all the surrogates candidates without the need of distinguishing the observed surrogates or the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26463;&#29702;&#35770;&#27169;&#22411;&#65292;&#20026;&#35821;&#20041;&#27495;&#20041;&#30340;&#20004;&#20010;&#29305;&#24449;&#65288;&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#21644;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#20154;&#31867;&#23545;&#20110;&#36825;&#20123;&#27495;&#20041;&#30340;&#21028;&#26029;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.06807</link><description>&lt;p&gt;
&#35821;&#20041;&#27495;&#20041;&#30340;&#22240;&#26524;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26463;&#29702;&#35770;&#27169;&#22411;&#65292;&#20026;&#35821;&#20041;&#27495;&#20041;&#30340;&#20004;&#20010;&#29305;&#24449;&#65288;&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#21644;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#20154;&#31867;&#23545;&#20110;&#36825;&#20123;&#27495;&#20041;&#30340;&#21028;&#26029;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27495;&#20041;&#26159;&#33258;&#28982;&#35821;&#35328;&#29616;&#35937;&#65292;&#22312;&#19981;&#21516;&#30340;&#35821;&#27861;&#12289;&#35821;&#20041;&#21644;&#35821;&#29992;&#23618;&#38754;&#19978;&#21457;&#29983;&#12290;&#23427;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65307;&#20363;&#22914;&#65292;&#22312;&#24515;&#29702;&#35821;&#35328;&#23398;&#39046;&#22495;&#65292;&#25105;&#20204;&#26377;&#22810;&#31181;&#31454;&#20105;&#24615;&#30340;&#30740;&#31350;&#20154;&#31867;&#28040;&#27495;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30740;&#31350;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#22522;&#20110;&#30524;&#21160;&#36319;&#36394;&#31561;&#27979;&#37327;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20026;&#35821;&#20041;&#27495;&#20041;&#24418;&#24335;&#21270;&#36825;&#20123;&#36827;&#31243;&#65292;&#20854;&#20013;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#29305;&#24449;&#65306;(1)&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#20043;&#38388;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#65292;(2)&#26681;&#25454;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26032;&#22411;&#26463;&#29702;&#35770;&#30830;&#23450;&#22240;&#26524;&#24615;&#27169;&#22411;&#24182;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#24212;&#29992;&#20110;&#20174;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#20351;&#29992;Amazon&#30340;&#26426;&#26800;&#22303;&#32819;&#20854;&#24341;&#25806;&#25910;&#38598;&#30340;&#20154;&#31867;&#21487;&#20449;&#24230;&#21028;&#26029;&#20013;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20854;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12289;&#27495;&#20041;&#27700;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ambiguity is a natural language phenomenon occurring at different levels of syntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics, for instance, we have a variety of competing studies for the human disambiguation processes. These studies are empirical and based on eyetracking measurements. Here we take first steps towards formalizing these processes for semantic ambiguities where we identified the presence of two features: (1) joint plausibility degrees of different possible interpretations, (2) causal structures according to which certain words play a more substantial role in the processes. The novel sheaf-theoretic model of definite causality developed by Gogioso and Pinzani in QPL 2021 offers tools to model and reason about these features. We applied this theory to a dataset of ambiguous phrases extracted from Psycholinguistics literature and their human plausibility judgements collected by us using the Amazon Mechanical Turk engine. We measured the causal fr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#26032;&#21457;&#31958;&#23615;&#30149;&#25104;&#20154;&#24739;&#32773;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ADA&#39118;&#38505;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.02900</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#36827;&#34892;&#26032;&#21457;&#31958;&#23615;&#30149;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
New-Onset Diabetes Assessment Using Artificial Intelligence-Enhanced Electrocardiography. (arXiv:2205.02900v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#26032;&#21457;&#31958;&#23615;&#30149;&#25104;&#20154;&#24739;&#32773;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ADA&#39118;&#38505;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#35786;&#26029;&#30340;&#31958;&#23615;&#30149;&#22312;&#24739;&#32773;&#20013;&#21344;21.4&#65285;&#65292;&#30001;&#20110;&#31579;&#26597;&#29575;&#30340;&#38480;&#21046;&#65292;&#31958;&#23615;&#30149;&#21487;&#33021;&#28508;&#20239;&#26080;&#30151;&#29366;&#32780;&#26410;&#34987;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26469;&#30830;&#23450;&#26032;&#21457;&#31958;&#23615;&#30149;&#30340;&#25104;&#20154;&#24739;&#32773;&#12290; &#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;12&#23548;&#32852;&#24515;&#30005;&#22270;&#21644;&#21487;&#29992;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#26469;&#20272;&#35745;HbA1c&#12290; &#25105;&#20204;&#22238;&#39038;&#24615;&#22320;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;&#26377;&#37197;&#23545;&#30340;ECG&#21644;HbA1c&#25968;&#25454;&#30340;&#30149;&#20154;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ADA&#39118;&#38505;&#26816;&#27979;&#65292;&#22522;&#20110;ECG&#30340;&#35780;&#20272;&#25928;&#26524;&#26356;&#22909;&#12290;AI&#22686;&#24378;&#30340;ECG&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;81&#65285;&#65292;&#28789;&#25935;&#24230;&#20026;80&#65285;&#65292;&#29305;&#24322;&#24615;&#20026;82&#65285;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;ECG&#21487;&#20197;&#25104;&#20026;&#26032;&#21457;&#31958;&#23615;&#30149;&#25104;&#20154;&#24739;&#32773;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#32479;&#31579;&#26597;&#26041;&#27861;&#26377;&#38480;&#30340;&#20154;&#32676;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Undiagnosed diabetes is present in 21.4% of adults with diabetes. Diabetes can remain asymptomatic and undetected due to limitations in screening rates. To address this issue, questionnaires, such as the American Diabetes Association (ADA) Risk test, have been recommended for use by physicians and the public. Based on evidence that blood glucose concentration can affect cardiac electrophysiology, we hypothesized that an artificial intelligence (AI)-enhanced electrocardiogram (ECG) could identify adults with new-onset diabetes. We trained a neural network to estimate HbA1c using a 12-lead ECG and readily available demographics. We retrospectively assembled a dataset comprised of patients with paired ECG and HbA1c data. The population of patients who receive both an ECG and HbA1c may a biased sample of the complete outpatient population, so we adjusted the importance placed on each patient to generate a more representative pseudo-population. We found ECG-based assessment outperforms the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#36710;&#36742;&#21327;&#21516;&#21327;&#35843;&#26694;&#26550;&#65292;&#28041;&#21450;&#30340;&#36710;&#36742;&#20132;&#20986;&#25511;&#21046;&#26435;&#38480;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#38750;&#20984;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#32593;&#32476;&#65288;T3D&#65289;&#31639;&#27861;&#23454;&#29616;&#26368;&#22823;&#21270;&#20132;&#36890;&#36890;&#34892;&#33021;&#21147;&#65292;&#21516;&#26102;&#30830;&#20445;&#39550;&#39542;&#23433;&#20840;&#21644;&#21327;&#35843;&#31995;&#32479;&#30340;&#38271;&#26399;&#31283;&#23450;&#24615;&#12290;&#24191;&#27867;&#30340;&#20223;&#30495;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20132;&#36890;&#36890;&#34892;&#33021;&#21147;&#12289;&#39550;&#39542;&#23433;&#20840;&#21644;&#21327;&#35843;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.01278</link><description>&lt;p&gt;
&#26080;&#20449;&#21495;&#36947;&#36335;&#20132;&#21449;&#21475;&#30340;&#23454;&#26102;&#36710;&#36742;&#21327;&#21516;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Real-time Cooperative Vehicle Coordination at Unsignalized Road Intersections. (arXiv:2205.01278v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01278
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#36710;&#36742;&#21327;&#21516;&#21327;&#35843;&#26694;&#26550;&#65292;&#28041;&#21450;&#30340;&#36710;&#36742;&#20132;&#20986;&#25511;&#21046;&#26435;&#38480;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#38750;&#20984;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#32593;&#32476;&#65288;T3D&#65289;&#31639;&#27861;&#23454;&#29616;&#26368;&#22823;&#21270;&#20132;&#36890;&#36890;&#34892;&#33021;&#21147;&#65292;&#21516;&#26102;&#30830;&#20445;&#39550;&#39542;&#23433;&#20840;&#21644;&#21327;&#35843;&#31995;&#32479;&#30340;&#38271;&#26399;&#31283;&#23450;&#24615;&#12290;&#24191;&#27867;&#30340;&#20223;&#30495;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20132;&#36890;&#36890;&#34892;&#33021;&#21147;&#12289;&#39550;&#39542;&#23433;&#20840;&#21644;&#21327;&#35843;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#36830;&#25509;&#21644;&#33258;&#21160;&#21270;&#36710;&#36742;&#30340;&#34892;&#39542;&#23433;&#20840;&#21644;&#20132;&#36890;&#36890;&#34892;&#33021;&#21147;&#65292;&#26080;&#20449;&#21495;&#36947;&#36335;&#20132;&#21449;&#21475;&#30340;&#21327;&#20316;&#21327;&#35843;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#35201;&#20040;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#29992;&#30340;&#20132;&#21449;&#21475;&#21327;&#35843;&#26694;&#26550;&#65292;&#20854;&#20013;&#28041;&#21450;&#30340;&#36710;&#36742;&#20132;&#20986;&#25511;&#21046;&#26435;&#38480;&#65292;&#24182;&#36981;&#24490;&#26469;&#33258;&#38598;&#20013;&#21327;&#35843;&#22120;&#30340;&#25351;&#20196;&#12290;&#28982;&#21518;&#65292;&#23558;&#21046;&#23450;&#19968;&#20010;&#32479;&#19968;&#30340;&#21327;&#20316;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#26368;&#22823;&#21270;&#20132;&#36890;&#36890;&#34892;&#33021;&#21147;&#65292;&#21516;&#26102;&#30830;&#20445;&#39550;&#39542;&#23433;&#20840;&#21644;&#21327;&#35843;&#31995;&#32479;&#30340;&#38271;&#26399;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#20027;&#35201;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#38750;&#20984;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#37325;&#26032;&#21046;&#23450;&#20026;&#19968;&#20010;&#26080;&#27169;&#22411;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#20004;&#20010;&#24182;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#32593;&#32476;&#65288;T3D&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#39640;&#21487;&#25193;&#23637;&#24615;&#30340;&#23454;&#26102;&#36710;&#36742;&#21327;&#21516;&#21327;&#35843;&#12290;&#24191;&#27867;&#30340;&#20223;&#30495;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20132;&#36890;&#36890;&#34892;&#33021;&#21147;&#12289;&#39550;&#39542;&#23433;&#20840;&#21644;&#21327;&#35843;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative coordination at unsignalized road intersections, which aims to improve the driving safety and traffic throughput for connected and automated vehicles, has attracted increasing interests in recent years. However, most existing investigations either suffer from computational complexity or cannot harness the full potential of the road infrastructure. To this end, we first present a dedicated intersection coordination framework, where the involved vehicles hand over their control authorities and follow instructions from a centralized coordinator. Then a unified cooperative trajectory optimization problem will be formulated to maximize the traffic throughput while ensuring the driving safety and long-term stability of the coordination system. To address the key computational challenges in the real-world deployment, we reformulate this non-convex sequential decision problem into a model-free Markov Decision Process (MDP) and tackle it by devising a Twin Delayed Deep Deterministic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Visual Spatial Reasoning&#65288;VSR&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;10k&#20010;&#33258;&#28982;&#25991;&#26412;-&#22270;&#20687;&#37197;&#23545;&#65292;&#29992;&#20110;&#25512;&#29702;&#21253;&#25324;66&#31181;&#31354;&#38388;&#20851;&#31995;&#65292;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38590;&#20197;&#25429;&#25417;&#20851;&#31995;&#20449;&#24687;&#21644;&#36739;&#23569;&#20851;&#27880;&#29289;&#20307;&#30340;&#26041;&#21521;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2205.00363</link><description>&lt;p&gt;
&#35270;&#35273;&#31354;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Visual Spatial Reasoning. (arXiv:2205.00363v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Visual Spatial Reasoning&#65288;VSR&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;10k&#20010;&#33258;&#28982;&#25991;&#26412;-&#22270;&#20687;&#37197;&#23545;&#65292;&#29992;&#20110;&#25512;&#29702;&#21253;&#25324;66&#31181;&#31354;&#38388;&#20851;&#31995;&#65292;&#30740;&#31350;&#21457;&#29616;&#30446;&#21069;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38590;&#20197;&#25429;&#25417;&#20851;&#31995;&#20449;&#24687;&#21644;&#36739;&#23569;&#20851;&#27880;&#29289;&#20307;&#30340;&#26041;&#21521;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#20851;&#31995;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20197;&#21508;&#31181;&#26041;&#24335;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38590;&#20197;&#25429;&#25417;&#20851;&#31995;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Visual Spatial Reasoning&#65288;VSR&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;10k&#20010;&#33258;&#28982;&#25991;&#26412;-&#22270;&#20687;&#37197;&#23545;&#65292;&#21253;&#25324;66&#31181;&#33521;&#35821;&#30340;&#31354;&#38388;&#20851;&#31995;&#65288;&#22914;&#65306;&#22312;&#19979;&#38754;&#65292;&#22312;&#21069;&#38754;&#21644;&#38754;&#23545;&#65289;&#12290;&#34429;&#28982;&#20351;&#29992;&#20102;&#30475;&#20284;&#31616;&#21333;&#30340;&#27880;&#37322;&#26684;&#24335;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#21253;&#25324;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35821;&#35328;&#29616;&#35937;&#65292;&#20363;&#22914;&#19981;&#21516;&#30340;&#21442;&#32771;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20154;&#31867;&#21644;&#27169;&#22411;&#34920;&#29616;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#65306;&#20154;&#31867;&#20934;&#30830;&#29575;&#39640;&#36798;95%&#20197;&#19978;&#65292;&#32780;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20165;&#33021;&#36798;&#21040;70%&#24038;&#21491;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;VLM&#25353;&#20851;&#31995;&#34920;&#29616;&#30340;&#33021;&#21147;&#19982;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#20960;&#20046;&#27809;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#27979;&#35797;&#30340;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#35782;&#21035;&#28041;&#21450;&#23545;&#35937;&#26041;&#21521;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (such as: under, in front of, and facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: the human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs' by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8220;FixNoise&#8221;&#65292;&#22312;StyleGAN&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#31616;&#21333;&#30340;&#29305;&#24449;&#21305;&#37197;&#25439;&#22833;&#26469;&#25913;&#21892;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#19968;&#37096;&#20998;&#20013;&#20165;&#20445;&#30041;&#28304;&#29305;&#24449;&#20197;&#25511;&#21046;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#39046;&#22495;&#32763;&#35793;&#21644;&#23646;&#24615;&#32534;&#36753;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.14079</link><description>&lt;p&gt;
&#20462;&#27491;&#22122;&#22768;&#65306;Disentangling Source Feature&#29992;&#20110;StyleGAN&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN. (arXiv:2204.14079v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.14079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8220;FixNoise&#8221;&#65292;&#22312;StyleGAN&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#31616;&#21333;&#30340;&#29305;&#24449;&#21305;&#37197;&#25439;&#22833;&#26469;&#25913;&#21892;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#19968;&#37096;&#20998;&#20013;&#20165;&#20445;&#30041;&#28304;&#29305;&#24449;&#20197;&#25511;&#21046;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#39046;&#22495;&#32763;&#35793;&#21644;&#23646;&#24615;&#32534;&#36753;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;StyleGAN&#30340;&#36801;&#31227;&#23398;&#20064;&#22312;&#35299;&#20915;&#19981;&#21516;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#39046;&#22495;&#32763;&#35793;&#19978;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#20132;&#25442;&#25110;&#20923;&#32467;&#26435;&#37325;&#26469;&#21033;&#29992;&#28304;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20294;&#20854;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#25511;&#21046;&#28304;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#29305;&#24449;&#21305;&#37197;&#25439;&#22833;&#26469;&#25913;&#21892;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#8220;FixNoise&#8221;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#20197;&#20165;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#19968;&#37096;&#20998;&#20013;&#20445;&#30041;&#28304;&#29305;&#24449;&#65292;&#20174;&#32780;&#25511;&#21046;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#12290;&#30001;&#20110;&#29305;&#24449;&#31354;&#38388;&#26159;&#20998;&#31163;&#30340;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#24179;&#28369;&#22320;&#25511;&#21046;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39046;&#22495;&#32763;&#35793;&#21644;&#23646;&#24615;&#32534;&#36753;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning of StyleGAN has recently shown great potential to solve diverse tasks, especially in domain translation. Previous methods utilized a source model by swapping or freezing weights during transfer learning, however, they have limitations on visual quality and controlling source features. In other words, they require additional models that are computationally demanding and have restricted control steps that prevent a smooth transition. In this paper, we propose a new approach to overcome these limitations. Instead of swapping or freezing, we introduce a simple feature matching loss to improve generation quality. In addition, to control the degree of source features, we train a target model with the proposed strategy, FixNoise, to preserve the source features only in a disentangled subspace of a target feature space. Owing to the disentangled feature space, our method can smoothly control the degree of the source features in a single model. Extensive experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#35752;&#35770;&#20102;&#35270;&#35273;&#34920;&#24449;&#23398;&#20064;&#20013;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22240;&#26524;&#25512;&#29702;&#33539;&#24335;&#26469;&#23454;&#29616;&#36825;&#20123;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.12037</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#19982;&#35270;&#35273;&#34920;&#24449;&#23398;&#20064;&#30340;&#20132;&#27719;&#65306;&#19968;&#39033;&#21069;&#30651;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning Meets Visual Representation Learning: A Prospective Study. (arXiv:2204.12037v8 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12037
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#35752;&#35770;&#20102;&#35270;&#35273;&#34920;&#24449;&#23398;&#20064;&#20013;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22240;&#26524;&#25512;&#29702;&#33539;&#24335;&#26469;&#23454;&#29616;&#36825;&#20123;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#34920;&#24449;&#23398;&#20064;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#21253;&#25324;&#35270;&#35273;&#29702;&#35299;&#12289;&#35270;&#39057;&#29702;&#35299;&#12289;&#22810;&#27169;&#24577;&#20998;&#26512;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#22478;&#24066;&#35745;&#31639;&#12290;&#30001;&#20110;&#22823;&#25968;&#25454;&#26102;&#20195;&#28044;&#29616;&#30340;&#22823;&#37327;&#22810;&#27169;&#24577;&#24322;&#26500;&#30340;&#26102;&#31354;/&#26102;&#38388;/&#26102;&#31354;&#25968;&#25454;&#65292;&#29616;&#26377;&#35270;&#35273;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#24615;&#32570;&#20047;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20013;&#22823;&#22810;&#25968;&#20542;&#21521;&#20110;&#36866;&#24212;&#21407;&#22987;&#25968;&#25454;/&#21464;&#37327;&#20998;&#24067;&#65292;&#24573;&#30053;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#32972;&#21518;&#30340;&#22522;&#26412;&#22240;&#26524;&#20851;&#31995;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#25351;&#23548;&#21644;&#20998;&#26512;&#65292;&#26080;&#27861;&#35299;&#37322;&#20026;&#20160;&#20040;&#29616;&#20195;&#35270;&#35273;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#23481;&#26131;&#38519;&#20837;&#25968;&#25454;&#20559;&#35265;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#21463;&#21040;&#20154;&#31867;&#32423;&#21035;&#20195;&#29702;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#24456;&#22823;&#30340;&#21162;&#21147;&#24320;&#21457;&#22240;&#26524;&#25512;&#29702;&#33539;&#24335;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual representation learning is ubiquitous in various real-world applications, including visual comprehension, video understanding, multi-modal analysis, human-computer interaction, and urban computing. Due to the emergence of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal data in big data era, the lack of interpretability, robustness, and out-of-distribution generalization are becoming the challenges of the existing visual models. The majority of the existing methods tend to fit the original data/variable distributions and ignore the essential causal relations behind the multi-modal knowledge, which lacks unified guidance and analysis about why modern visual representation learning methods easily collapse into data bias and have limited generalization and cognitive abilities. Inspired by the strong inference ability of human-level agents, recent years have therefore witnessed great effort in developing causal reasoning paradigms to realize robust represe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CgAT&#26041;&#27861;&#65292;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;&#28145;&#24230;Hashing&#32593;&#32476;&#26816;&#32034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.10779</link><description>&lt;p&gt;
CgAT&#65306;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#25552;&#21319;Hashing&#26816;&#32034;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
CgAT: Center-Guided Adversarial Training for Deep Hashing-Based Retrieval. (arXiv:2204.10779v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CgAT&#26041;&#27861;&#65292;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;&#28145;&#24230;Hashing&#32593;&#32476;&#26816;&#32034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;Hashing&#22312;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;min-max&#30340;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65288;CgAT&#65289;&#65292;&#36890;&#36807;&#26368;&#22351;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#25552;&#39640;&#28145;&#24230;Hashing&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#28145;&#24230;Hashing&#26816;&#32034;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#23545;&#25239;&#24615;&#38450;&#24481;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep hashing has been extensively utilized in massive image retrieval because of its efficiency and effectiveness. However, deep hashing models are vulnerable to adversarial examples, making it essential to develop adversarial defense methods for image retrieval. Existing solutions achieved limited defense performance because of using weak adversarial samples for training and lacking discriminative optimization objectives to learn robust features. In this paper, we present a min-max based Center-guided Adversarial Training, namely CgAT, to improve the robustness of deep hashing networks through worst adversarial examples. Specifically, we first formulate the center code as a semantically-discriminative representative of the input image content, which preserves the semantic similarity with positive samples and dissimilarity with negative examples. We prove that a mathematical formula can calculate the center code immediately. After obtaining the center codes in each optimization iterati
&lt;/p&gt;</description></item></channel></rss>