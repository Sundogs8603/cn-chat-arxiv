<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2312.15101</link><description>&lt;p&gt;
&#20462;&#22797;-Con&#65306;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#30340;&#33258;&#21160;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.15101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#27493;&#39588;&#65292;&#21487;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#27169;&#22411;&#22312;&#35774;&#22791;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#21033;&#29992;&#21487;&#33021;&#21482;&#22312;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#25552;&#20379;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36716;&#25442;&#36807;&#31243;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#65292;&#23548;&#33268;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#26080;&#27861;&#37096;&#32626;&#25110;&#23384;&#22312;&#38382;&#39064;&#65292;&#20005;&#37325;&#38477;&#20302;&#20102;&#20854;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;Fix-Con&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26102;&#20351;&#29992;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#22312;&#36716;&#25442;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#30340;&#25925;&#38556;&#12290;Fix-Con&#20351;&#29992;&#20174;&#35843;&#26597;&#36716;&#25442;&#38382;&#39064;&#20013;&#25366;&#25496;&#20986;&#30340;&#19968;&#32452;&#25925;&#38556;&#31867;&#22411;&#26469;&#23450;&#20301;&#36716;&#25442;&#27169;&#22411;&#20013;&#28508;&#22312;&#30340;&#36716;&#25442;&#25925;&#38556;&#65292;&#24182;&#36866;&#24403;&#20462;&#22797;&#23427;&#20204;&#65292;&#20363;&#22914;&#20351;&#29992;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26367;&#25442;&#30446;&#26631;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#36825;&#19968;&#36807;&#31243;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#19978;&#36827;&#34892;&#36845;&#20195;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.   We propose an automated approach for fault localization and repair, Fix-Con, during model conversion between deep learning frameworks. Fix-Con is capable of detecting and fixing faults introduced in model input, parameters, hyperparameters, and the model graph during conversion.   Fix-Con uses a set of fault types mined from surveying conversion issues raised to localize potential conversion faults in the converted target model, and then repairs them appropriately, e.g. replacing the parameters of the target model with those from the source model. This is done iteratively for every image in the datas
&lt;/p&gt;</description></item><item><title>SLEDGE&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#20197;&#21450;Diffusion Transformer&#26469;&#29983;&#25104;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#25311;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.17933</link><description>&lt;p&gt;
SLEDGE: &#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#39550;&#39542;&#26234;&#33021;&#20307;&#30340;&#27169;&#25311;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17933
&lt;/p&gt;
&lt;p&gt;
SLEDGE&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#20197;&#21450;Diffusion Transformer&#26469;&#29983;&#25104;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#25311;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SLEDGE&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#35760;&#24405;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#12290;&#20854;&#26680;&#24515;&#32452;&#20214;&#26159;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#26234;&#33021;&#20307;&#36793;&#30028;&#26694;&#21644;&#36710;&#36947;&#22270;&#12290;&#35813;&#27169;&#22411;&#30340;&#36755;&#20986;&#20316;&#20026;&#20132;&#36890;&#27169;&#25311;&#30340;&#21021;&#22987;&#29366;&#24577;&#12290;&#38024;&#23545;SLEDGE&#24453;&#29983;&#25104;&#30340;&#23454;&#20307;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#36830;&#25509;&#24615;&#21644;&#27599;&#20010;&#22330;&#26223;&#30340;&#21487;&#21464;&#25968;&#37327;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26420;&#32032;&#24212;&#29992;&#21464;&#24471;&#19981;&#31616;&#21333;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38500;&#20102;&#23545;&#29616;&#26377;&#36710;&#36947;&#22270;&#34920;&#31034;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#12290;&#23427;&#23558;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#32534;&#30721;&#20026;&#26629;&#26684;&#21270;&#28508;&#22312;&#26144;&#23556;&#20013;&#30340;&#19981;&#21516;&#36890;&#36947;&#12290;&#36825;&#26377;&#21161;&#20110;&#36710;&#36947;&#26465;&#20214;&#19979;&#30340;&#26234;&#33021;&#20307;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;&#25193;&#25955;&#21464;&#25442;&#22120;&#21516;&#26102;&#29983;&#25104;&#36710;&#36947;&#21644;&#26234;&#33021;&#20307;&#12290;&#22312;SLEDGE&#20013;&#20351;&#29992;&#29983;&#25104;&#30340;&#23454;&#20307;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#27169;&#25311;&#65292;&#20363;&#22914;&#19978;&#37319;&#26679;&#36716;&#24367;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17933v1 Announce Type: cross  Abstract: SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE). It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MAGIS&#26694;&#26550;&#65292;&#22522;&#20110;LLM&#26500;&#24314;&#65292;&#21253;&#25324;&#22235;&#31181;&#23450;&#21046;&#30340;Agent&#65292;&#33021;&#22815;&#21327;&#20316;&#35268;&#21010;&#21644;&#32534;&#30721;&#36807;&#31243;&#65292;&#35299;&#38145;LLMs&#35299;&#20915;GitHub&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.17927</link><description>&lt;p&gt;
MAGIS&#65306;&#22522;&#20110;LLM&#30340;GitHub&#38382;&#39064;&#35299;&#20915;&#22810;Agent&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17927
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MAGIS&#26694;&#26550;&#65292;&#22522;&#20110;LLM&#26500;&#24314;&#65292;&#21253;&#25324;&#22235;&#31181;&#23450;&#21046;&#30340;Agent&#65292;&#33021;&#22815;&#21327;&#20316;&#35268;&#21010;&#21644;&#32534;&#30721;&#36807;&#31243;&#65292;&#35299;&#38145;LLMs&#35299;&#20915;GitHub&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#28436;&#21270;&#20013;&#65292;&#35299;&#20915;GitHub&#23384;&#20648;&#24211;&#20013;&#30340;&#31361;&#21457;&#38382;&#39064;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#19981;&#20165;&#28041;&#21450;&#26032;&#20195;&#30721;&#30340;&#25972;&#21512;&#65292;&#36824;&#21253;&#25324;&#23545;&#29616;&#26377;&#21151;&#33021;&#30340;&#32500;&#25252;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;&#20195;&#30721;&#26356;&#25913;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#20648;&#24211;&#32423;&#21035;&#19978;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#20102;LLMs&#22823;&#22810;&#26080;&#27861;&#35299;&#20915;GitHub&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#24182;&#20998;&#26512;&#20102;&#19968;&#20123;&#24433;&#21709;&#22240;&#32032;&#12290;&#21463;&#32463;&#39564;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;GitHub Issue&#35299;&#20915;&#22810;Agent&#26694;&#26550;MAGIS&#65292;&#30001;&#22235;&#31181;&#38024;&#23545;&#36719;&#20214;&#28436;&#21270;&#23450;&#21046;&#30340;Agent&#32452;&#25104;&#65306;Manager&#12289;Repository Custodian&#12289;Developer&#21644;Quality Assurance Engineer&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#21508;&#31181;Agent&#22312;&#35268;&#21010;&#21644;&#32534;&#30721;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#65292;&#37322;&#25918;LLMs&#35299;&#20915;GitHub&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17927v1 Announce Type: cross  Abstract: In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities. Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level. To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#35757;&#32451;&#25216;&#26415;&#65292;&#21363;Attention Interpolation via Diffusion (AID)&#65292;&#36890;&#36807;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#12289;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65292;&#20197;&#21450;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25554;&#20540;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17924</link><description>&lt;p&gt;
AID: &#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#30340;&#27880;&#37325;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
AID: Attention Interpolation of Text-to-Image Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17924
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#35757;&#32451;&#25216;&#26415;&#65292;&#21363;Attention Interpolation via Diffusion (AID)&#65292;&#36890;&#36807;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#12289;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65292;&#20197;&#21450;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25554;&#20540;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17924v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#21019;&#24314;&#30475;&#19981;&#35265;&#30340;&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#22270;&#20687;&#25554;&#20540;&#12290;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25554;&#20540;&#24050;&#32463;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#26159;&#20855;&#26377;&#29305;&#23450;&#26465;&#20214;&#65288;&#22914;&#25991;&#26412;&#25110;&#23039;&#21183;&#65289;&#30340;&#25554;&#20540;&#21364;&#20102;&#35299;&#19981;&#22810;&#12290;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22914;&#22312;&#26465;&#20214;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#22270;&#20687;&#32570;&#20047;&#19968;&#33268;&#24615;&#12289;&#24179;&#28369;&#24230;&#21644;&#20445;&#30495;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;Attention Interpolation via Diffusion (AID)&#30340;&#26032;&#39062;&#26080;&#35757;&#32451;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;1&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#65307;2&#65289;&#23558;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65307;3&#65289;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20307;&#65292;Prompt-guided Attention Interpolation via Diffusion (PAID)&#65292;&#23427;&#23558;&#25554;&#20540;&#35270;&#20026;&#20381;&#36182;&#20110;&#26465;&#20214;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#20986;&#26356;&#20855;&#21019;&#36896;&#24615;&#30340;&#26032;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17924v1 Announce Type: cross  Abstract: Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater con
&lt;/p&gt;</description></item><item><title>&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17919</link><description>&lt;p&gt;
LISA&#65306;&#29992;&#20110;&#39640;&#25928;&#20869;&#23384;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17919
&lt;/p&gt;
&lt;p&gt;
&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35832;&#22914;&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#20043;&#31867;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#22823;&#35268;&#27169;&#24494;&#35843;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#26080;&#27861;&#19982;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#30456;&#21305;&#37197;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#30340;&#36880;&#23618;&#29305;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#23618;&#20043;&#38388;&#26435;&#37325;&#33539;&#25968;&#30340;&#24322;&#24120;&#20559;&#26012;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#31616;&#21333;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#35760;&#24518;&#25104;&#26412;&#20302;&#20110;LoRA&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#20248;&#20110;LoRA&#21644;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;Layerwise Importance Sampled AdamW&#65288;LISA&#65289;&#65292;&#36825;&#26159;LoRA&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
&lt;/p&gt;</description></item><item><title>AgentStudio&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#34394;&#25311;&#20195;&#29702;&#30340;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#29616;&#23454;&#12289;&#22810;&#27169;&#24577;&#30340;&#24320;&#21457;&#24037;&#20855;&#65292;&#25903;&#25345;&#25972;&#20010;&#20195;&#29702;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#36890;&#29992;&#30340;&#35266;&#27979;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#20197;&#21450;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#26377;&#25928;&#24320;&#21457;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.17918</link><description>&lt;p&gt;
AgentStudio&#65306;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#34394;&#25311;&#20195;&#29702;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
AgentStudio: A Toolkit for Building General Virtual Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17918
&lt;/p&gt;
&lt;p&gt;
AgentStudio&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#34394;&#25311;&#20195;&#29702;&#30340;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#29616;&#23454;&#12289;&#22810;&#27169;&#24577;&#30340;&#24320;&#21457;&#24037;&#20855;&#65292;&#25903;&#25345;&#25972;&#20010;&#20195;&#29702;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#36890;&#29992;&#30340;&#35266;&#27979;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#20197;&#21450;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#26377;&#25928;&#24320;&#21457;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17918v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#21407;&#25991;&#25688;&#35201;: &#21019;&#24314;&#33021;&#22815;&#22312;&#20219;&#20309;&#25968;&#23383;&#35774;&#22791;&#19978;&#20351;&#29992;&#20219;&#24847;&#36719;&#20214;&#30340;&#33258;&#20027;&#34394;&#25311;&#20195;&#29702;&#20173;&#28982;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20004;&#20010;&#20851;&#38190;&#38556;&#30861;&#22952;&#30861;&#20102;&#36827;&#23637;: &#32570;&#20047;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#26500;&#24314;&#34394;&#25311;&#20195;&#29702;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#20197;&#21450;&#38656;&#35201;&#23545;&#22522;&#26412;&#20195;&#29702;&#33021;&#21147;&#36827;&#34892;&#37326;&#22806;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AgentStudio&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#12289;&#29616;&#23454;&#19988;&#22810;&#27169;&#24577;&#30340;&#24037;&#20855;&#21253;&#65292;&#28085;&#30422;&#20102;&#20195;&#29702;&#24320;&#21457;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#12290;&#36825;&#21253;&#25324;&#29615;&#22659;&#35774;&#32622;&#12289;&#25968;&#25454;&#25910;&#38598;&#12289;&#20195;&#29702;&#35780;&#20272;&#21644;&#21487;&#35270;&#21270;&#12290; &#35266;&#27979;&#21644;&#21160;&#20316;&#31354;&#38388;&#26497;&#20854;&#36890;&#29992;&#65292;&#25903;&#25345;&#20989;&#25968;&#35843;&#29992;&#21644;&#20154;&#26426;&#30028;&#38754;&#12290;AgentStudio&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#20854;&#22810;&#26679;&#24615;&#65292;&#20801;&#35768;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#39640;&#25928;&#24320;&#21457;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290; &#20026;&#20102;&#35828;&#26126;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35270;&#35273;&#23450;&#20301;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#22871;&#20214;&#65292;&#37117;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17918v1 Announce Type: new  Abstract: Creating autonomous virtual agents capable of using arbitrary software on any digital device remains a major challenge for artificial intelligence. Two key obstacles hinder progress: insufficient infrastructure for building virtual agents in real-world environments, and the need for in-the-wild evaluation of fundamental agent abilities. To address this, we introduce AgentStudio, an online, realistic, and multimodal toolkit that covers the entire lifecycle of agent development. This includes environment setups, data collection, agent evaluation, and visualization. The observation and action spaces are highly generic, supporting both function calling and human-computer interfaces. This versatility is further enhanced by AgentStudio's graphical user interfaces, which allow efficient development of datasets and benchmarks in real-world settings. To illustrate, we introduce a visual grounding dataset and a real-world benchmark suite, both cre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17916</link><description>&lt;p&gt;
CMP&#65306;&#20855;&#26377;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#30340;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CMP: Cooperative Motion Prediction with Multi-Agent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#21457;&#23637;&#21644;&#36710;&#32852;&#32593;&#65288;V2X&#65289;&#36890;&#20449;&#30340;&#25104;&#29087;&#65292;&#21512;&#20316;&#36830;&#25509;&#30340;&#33258;&#21160;&#21270;&#36710;&#36742;&#65288;CAVs&#65289;&#30340;&#21151;&#33021;&#21464;&#24471;&#21487;&#33021;&#12290;&#26412;&#25991;&#22522;&#20110;&#21512;&#20316;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CMP&#20197;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#22686;&#24378;&#36319;&#36394;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#19982;&#36807;&#21435;&#19987;&#27880;&#20110;&#21512;&#20316;&#24863;&#30693;&#25110;&#36816;&#21160;&#39044;&#27979;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#35299;&#20915;CAVs&#22312;&#24863;&#30693;&#21644;&#39044;&#27979;&#27169;&#22359;&#20013;&#20849;&#20139;&#20449;&#24687;&#30340;&#32479;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20013;&#36824;&#34701;&#20837;&#20102;&#33021;&#22815;&#23481;&#24525;&#29616;&#23454;V2X&#24102;&#23485;&#38480;&#21046;&#21644;&#20256;&#36755;&#24310;&#36831;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#22788;&#29702;&#24222;&#22823;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#39044;&#27979;&#32858;&#21512;&#27169;&#22359;&#65292;&#32479;&#19968;&#20102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#20107;&#20214;&#25552;&#21462;&#38382;&#39064;&#35270;&#20026;&#20998;&#23618;&#20998;&#31867;&#20219;&#21153;&#65292;&#21033;&#29992;&#20998;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#32467;&#21512;BERT&#65292;&#23454;&#29616;&#20102;&#20808;&#35782;&#21035;&#31895;&#31890;&#24230;&#20449;&#24687;&#20877;&#39044;&#27979;&#32454;&#31890;&#24230;&#20449;&#24687;&#30340;&#33258;&#21160;&#20107;&#20214;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17914</link><description>&lt;p&gt;
&#33322;&#31354;&#20107;&#25925;&#25253;&#21578;&#20013;&#32454;&#31890;&#24230;&#20107;&#20214;&#25552;&#21462;&#30340;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#20107;&#20214;&#25552;&#21462;&#38382;&#39064;&#35270;&#20026;&#20998;&#23618;&#20998;&#31867;&#20219;&#21153;&#65292;&#21033;&#29992;&#20998;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#32467;&#21512;BERT&#65292;&#23454;&#29616;&#20102;&#20808;&#35782;&#21035;&#31895;&#31890;&#24230;&#20449;&#24687;&#20877;&#39044;&#27979;&#32454;&#31890;&#24230;&#20449;&#24687;&#30340;&#33258;&#21160;&#20107;&#20214;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33322;&#31354;&#39046;&#22495;&#35760;&#24405;&#20102;&#22823;&#37327;&#30340;&#20107;&#25925;&#25253;&#21578;&#65292;&#25552;&#39640;&#33322;&#31354;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#25253;&#21578;&#65292;&#25105;&#20204;&#38656;&#35201;&#26681;&#25454;&#20107;&#25925;&#25253;&#21578;&#20102;&#35299;&#26368;&#37325;&#35201;&#30340;&#20107;&#20214;&#25110;&#24433;&#21709;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#19981;&#26029;&#22686;&#21152;&#30340;&#20107;&#25925;&#25253;&#21578;&#25968;&#37327;&#35201;&#27714;&#39046;&#22495;&#19987;&#23478;&#25237;&#20837;&#22823;&#37327;&#24037;&#20316;&#26469;&#26631;&#35760;&#36825;&#20123;&#25253;&#21578;&#12290;&#20026;&#20102;&#20351;&#26631;&#35760;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#24320;&#22987;&#24320;&#21457;&#31639;&#27861;&#65292;&#33258;&#21160;&#20174;&#20107;&#25925;&#25253;&#21578;&#20013;&#35782;&#21035;&#28508;&#22312;&#30340;&#20107;&#20214;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#36890;&#36807;&#21033;&#29992;&#20107;&#20214;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20107;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#38382;&#39064;&#35270;&#20026;&#19968;&#39033;&#20998;&#23618;&#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#39318;&#20808;&#35782;&#21035;&#31895;&#31890;&#24230;&#20449;&#24687;&#65292;&#28982;&#21518;&#39044;&#27979;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#25972;&#21512;&#21040;BERT&#20013;&#23454;&#29616;&#20102;&#36825;&#31181;&#20998;&#23618;&#20998;&#31867;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17914v1 Announce Type: new  Abstract: A large volume of accident reports is recorded in the aviation domain, which greatly values improving aviation safety. To better use those reports, we need to understand the most important events or impact factors according to the accident reports. However, the increasing number of accident reports requires large efforts from domain experts to label those reports. In order to make the labeling process more efficient, many researchers have started developing algorithms to identify the underlying events from accident reports automatically. This article argues that we can identify the events more accurately by leveraging the event taxonomy. More specifically, we consider the problem a hierarchical classification task where we first identify the coarse-level information and then predict the fine-level information. We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into BERT. To further 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#21033;&#29992;&#20998;&#23618;&#26631;&#31614;&#25552;&#39640;&#26410;&#30693;&#25925;&#38556;&#26816;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#28909;&#36711;&#38050;&#36807;&#31243;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17891</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#22522;&#20110;&#22270;&#20687;&#30340;&#26032;&#22411;&#25925;&#38556;&#26816;&#27979;&#65292;&#20351;&#29992;&#20998;&#23618;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Image-based Novel Fault Detection with Deep Learning Classifiers using Hierarchical Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#21033;&#29992;&#20998;&#23618;&#26631;&#31614;&#25552;&#39640;&#26410;&#30693;&#25925;&#38556;&#26816;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#28909;&#36711;&#38050;&#36807;&#31243;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25925;&#38556;&#20998;&#31867;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#26159;&#22312;&#38754;&#23545;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#25925;&#38556;&#31867;&#22411;&#26102;&#33021;&#22815;&#26631;&#35760;&#31995;&#32479;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25925;&#38556;&#20998;&#31867;&#22120;&#30340;&#26410;&#30693;&#25925;&#38556;&#26816;&#27979;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#22312;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;&#26377;&#20851;&#25925;&#38556;&#20998;&#31867;&#27861;&#30340;&#26631;&#31614;&#26469;&#25552;&#39640;&#26410;&#30693;&#25925;&#38556;&#26816;&#27979;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#36719;&#26631;&#31614;&#25216;&#26415;&#26469;&#25913;&#21892;&#29616;&#26377;&#30340;&#28145;&#24230;&#26032;&#22411;&#25925;&#38556;&#26816;&#27979;&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20197;&#21450;&#29992;&#20110;&#22312;&#32447;&#26032;&#22411;&#25925;&#38556;&#26816;&#27979;&#30340;&#23618;&#27425;&#19968;&#33268;&#26816;&#27979;&#32479;&#35745;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28909;&#36711;&#38050;&#36807;&#31243;&#20013;&#26816;&#26597;&#22270;&#20687;&#20013;&#26032;&#22411;&#25925;&#38556;&#26816;&#27979;&#30340;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#65292;&#32467;&#26524;&#22312;&#22810;&#31181;&#24773;&#20917;&#21644;&#22522;&#32447;&#26816;&#27979;&#26041;&#27861;&#20013;&#37117;&#26377;&#24456;&#22909;&#30340;&#37325;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17891v1 Announce Type: cross  Abstract: One important characteristic of modern fault classification systems is the ability to flag the system when faced with previously unseen fault types. This work considers the unknown fault detection capabilities of deep neural network-based fault classifiers. Specifically, we propose a methodology on how, when available, labels regarding the fault taxonomy can be used to increase unknown fault detection performance without sacrificing model performance. To achieve this, we propose to utilize soft label techniques to improve the state-of-the-art deep novel fault detection techniques during the training process and novel hierarchically consistent detection statistics for online novel fault detection. Finally, we demonstrated increased detection performance on novel fault detection in inspection images from the hot steel rolling process, with results well replicated across multiple scenarios and baseline detection methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;ST&#26694;&#26550;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#24515;&#29702;&#20581;&#24247;&#31561;&#25935;&#24863;&#39046;&#22495;&#65292;&#20197;&#35299;&#20915;&#31038;&#20250;&#35823;&#24402;&#23646;&#24615;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#22686;&#21152;&#31532;&#20116;&#20010;&#8220;W&#38382;&#39064;&#8221;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#35813;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.17873</link><description>&lt;p&gt;
&#22788;&#29702;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35823;&#24402;&#23646;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;HCXAI&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;ST&#26694;&#26550;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#24515;&#29702;&#20581;&#24247;&#31561;&#25935;&#24863;&#39046;&#22495;&#65292;&#20197;&#35299;&#20915;&#31038;&#20250;&#35823;&#24402;&#23646;&#24615;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#22686;&#21152;&#31532;&#20116;&#20010;&#8220;W&#38382;&#39064;&#8221;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#35813;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#21487;&#35299;&#37322;AI&#65288;HCXAI&#65289;&#20513;&#23548;&#23558;&#31038;&#20250;&#22240;&#32032;&#34701;&#20837;AI&#35299;&#37322;&#20013;&#12290;HCXAI&#35752;&#35770;&#30340;&#26680;&#24515;&#26159;&#31038;&#20250;&#36879;&#26126;&#65288;ST&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#20351;AI&#31995;&#32479;&#30340;&#31038;&#20250;&#32452;&#32455;&#32972;&#26223;&#23545;&#20854;&#29992;&#25143;&#21487;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;ST&#26694;&#26550;&#25193;&#23637;&#21040;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#31038;&#20250;&#35823;&#24402;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#22312;&#25935;&#24863;&#39046;&#22495;&#22914;&#24515;&#29702;&#20581;&#24247;&#26041;&#38754;&#12290;&#20107;&#23454;&#19978;&#65292;LLMs&#22312;&#27169;&#25311;&#35282;&#33394;&#21644;&#20154;&#29289;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#33021;&#21147;&#65292;&#21487;&#33021;&#23548;&#33268;&#35774;&#35745;&#32773;&#24847;&#22270;&#19982;&#29992;&#25143;&#23545;&#31038;&#20250;&#23646;&#24615;&#30340;&#24863;&#30693;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#21487;&#33021;&#20419;&#25104;&#24773;&#24863;&#25805;&#32437;&#21644;&#21361;&#38505;&#34892;&#20026;&#65292;&#20986;&#29616;&#35748;&#35782;&#19978;&#30340;&#19981;&#20844;&#27491;&#24773;&#20917;&#20197;&#21450;&#19981;&#21512;&#29702;&#30340;&#20449;&#20219;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21521;ST&#26694;&#26550;&#28155;&#21152;&#31532;&#20116;&#20010;&#8220;W&#38382;&#39064;&#8221;&#26469;&#21152;&#24378;&#35813;&#26694;&#26550;&#65292;&#20197;&#28548;&#28165;&#35774;&#35745;&#32773;&#21644;&#29992;&#25143;&#23545;LLMs&#36171;&#20104;&#30340;&#20855;&#20307;&#31038;&#20250;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17873v1 Announce Type: new  Abstract: Human-centered explainable AI (HCXAI) advocates for the integration of social aspects into AI explanations. Central to the HCXAI discourse is the Social Transparency (ST) framework, which aims to make the socio-organizational context of AI systems accessible to their users. In this work, we suggest extending the ST framework to address the risks of social misattributions in Large Language Models (LLMs), particularly in sensitive areas like mental health. In fact LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To address these issues, we propose enhancing the ST framework with a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by its designers and users. This addition aims 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20505;&#32454;&#21270;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#22359;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#38477;&#27700;&#25968;&#25454;&#65292;&#20026;&#32531;&#35299;&#27668;&#20505;&#21464;&#21270;&#24102;&#26469;&#30340;&#24433;&#21709;&#21644;&#25552;&#39640;&#27700;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.17847</link><description>&lt;p&gt;
&#27668;&#20505;&#32454;&#21270;&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24102;&#26377;&#27880;&#24847;&#21147;&#22359;&#21644;&#36339;&#36291;&#36830;&#25509;&#30340;&#38477;&#27700;&#25968;&#25454;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20505;&#32454;&#21270;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#22359;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#38477;&#27700;&#25968;&#25454;&#65292;&#20026;&#32531;&#35299;&#27668;&#20505;&#21464;&#21270;&#24102;&#26469;&#30340;&#24433;&#21709;&#21644;&#25552;&#39640;&#27700;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#21152;&#36895;&#20102;&#21270;&#30707;&#29123;&#26009;&#30340;&#28040;&#32791;&#24182;&#20135;&#29983;&#20102;&#28201;&#23460;&#27668;&#20307;&#65292;&#23548;&#33268;&#20102;&#24403;&#20170;&#36843;&#22312;&#30473;&#30571;&#30340;&#38382;&#39064;&#65306;&#20840;&#29699;&#21464;&#26262;&#21644;&#27668;&#20505;&#21464;&#21270;&#12290;&#36825;&#20123;&#38388;&#25509;&#36896;&#25104;&#20005;&#37325;&#30340;&#33258;&#28982;&#28798;&#23475;&#65292;&#35768;&#22810;&#29983;&#21629;&#21463;&#33510;&#20197;&#21450;&#20892;&#19994;&#36130;&#20135;&#30340;&#24040;&#22823;&#25439;&#22833;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#25105;&#20204;&#22303;&#22320;&#30340;&#24433;&#21709;&#65292;&#31185;&#23398;&#23478;&#20204;&#27491;&#22312;&#24320;&#21457;&#21487;&#20877;&#29983;&#12289;&#21487;&#37325;&#22797;&#20351;&#29992;&#21644;&#28165;&#27905;&#33021;&#28304;&#65292;&#27668;&#20505;&#23398;&#23478;&#27491;&#35797;&#22270;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#12290;&#21516;&#26102;&#65292;&#21508;&#22269;&#25919;&#24220;&#27491;&#22312;&#20844;&#24067;&#33410;&#32422;&#36164;&#28304;&#30340;&#25919;&#31574;&#65292;&#20197;&#24314;&#31435;&#26356;&#29615;&#20445;&#30340;&#31038;&#20250;&#24182;&#21796;&#36215;&#29615;&#22659;&#24847;&#35782;&#12290;&#20854;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#22240;&#32032;&#20043;&#19968;&#23601;&#26159;&#38477;&#27700;&#65292;&#23558;&#20957;&#32467;&#30340;&#27700;&#33976;&#27668;&#24102;&#21040;&#22303;&#22320;&#19978;&#12290;&#27700;&#36164;&#28304;&#26159;&#31038;&#20250;&#20013;&#26368;&#37325;&#35201;&#20294;&#22522;&#26412;&#30340;&#38656;&#27714;&#65292;&#19981;&#20165;&#25903;&#25345;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#20063;&#25903;&#25345;&#32463;&#27982;&#12290;&#22312;&#21488;&#28286;&#65292;&#23613;&#31649;&#24179;&#22343;&#24180;&#38477;&#27700;&#37327;&#39640;&#36798;2500&#27627;&#31859;&#65292;&#20294;&#27599;&#20154;&#30340;&#27700;&#20998;&#37197;&#37327;&#20302;&#20110;&#20840;&#29699;&#24179;&#22343;&#27700;&#24179;&#65292;&#36825;&#26159;&#30001;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17847v1 Announce Type: cross  Abstract: Human activities accelerate consumption of fossil fuels and produce greenhouse gases, resulting in urgent issues today: global warming and the climate change. These indirectly cause severe natural disasters, plenty of lives suffering and huge losses of agricultural properties. To mitigate impacts on our lands, scientists are developing renewable, reusable, and clean energies and climatologists are trying to predict the extremes. Meanwhile, governments are publicizing resource-saving policies for a more eco-friendly society and arousing environment awareness. One of the most influencing factors is the precipitation, bringing condensed water vapor onto lands. Water resources are the most significant but basic needs in society, not only supporting our livings, but also economics. In Taiwan, although the average annual precipitation is up to 2,500 millimeter (mm), the water allocation for each person is lower than the global average due to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.17846</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24320;&#25918;&#35789;&#27719;&#26426;&#22120;&#20154;&#26144;&#23556;&#26041;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#20016;&#23500;&#20102;&#23494;&#38598;&#20960;&#20309;&#22320;&#22270;&#12290;&#34429;&#28982;&#36825;&#20123;&#22320;&#22270;&#20801;&#35768;&#22312;&#26597;&#35810;&#26576;&#31181;&#35821;&#35328;&#27010;&#24565;&#26102;&#39044;&#27979;&#36880;&#28857;&#26174;&#33879;&#24615;&#22320;&#22270;&#65292;&#20294;&#22823;&#35268;&#27169;&#29615;&#22659;&#21644;&#36229;&#20986;&#23545;&#35937;&#32423;&#21035;&#30340;&#25277;&#35937;&#26597;&#35810;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#65292;&#26368;&#32456;&#38480;&#21046;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOV-SG&#65292;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;3D&#31354;&#38388;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#27573;&#32423;&#22320;&#22270;&#65292;&#28982;&#21518;&#26500;&#24314;&#20102;&#30001;&#22320;&#26495;&#12289;&#25151;&#38388;&#21644;&#23545;&#35937;&#27010;&#24565;&#32452;&#25104;&#30340;3D&#22330;&#26223;&#22270;&#23618;&#27425;&#32467;&#26500;&#65292;&#27599;&#20010;&#37117;&#21253;&#21547;&#24320;&#25918;&#24615;&#35789;&#27719;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#34920;&#31034;&#22810;&#23618;&#24314;&#31569;&#65292;&#24182;&#19988;&#20801;&#35768;&#26426;&#22120;&#20154;&#20351;&#29992;&#36328;&#23618;Voronoi&#22270;&#31359;&#36234;&#36825;&#20123;&#24314;&#31569;&#12290;HOV-SG&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17846v1 Announce Type: cross  Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ReMamber&#65292;&#19968;&#31181;&#25972;&#21512;&#20102;Mamba&#21644;&#22810;&#27169;&#24577;Mamba Twister&#22359;&#30340;&#26032;&#22411;RIS&#26550;&#26500;&#65292;&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#23454;&#29616;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;</title><link>https://arxiv.org/abs/2403.17839</link><description>&lt;p&gt;
ReMamber&#65306;&#20351;&#29992;Mamba Twister&#23454;&#29616;&#24341;&#29992;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ReMamber: Referring Image Segmentation with Mamba Twister
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ReMamber&#65292;&#19968;&#31181;&#25972;&#21512;&#20102;Mamba&#21644;&#22810;&#27169;&#24577;Mamba Twister&#22359;&#30340;&#26032;&#22411;RIS&#26550;&#26500;&#65292;&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#23454;&#29616;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#29992;&#22270;&#20687;&#20998;&#21106;&#65288;RIS&#65289;&#21033;&#29992;&#21464;&#25442;&#22120;&#22312;&#35299;&#37322;&#22797;&#26434;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20108;&#27425;&#35745;&#31639;&#25104;&#26412;&#20351;&#20854;&#22312;&#25429;&#25417;&#36828;&#31243;&#35270;&#35273;-&#35821;&#35328;&#20381;&#36182;&#24615;&#26041;&#38754;&#28040;&#32791;&#36164;&#28304;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;Mamba&#36890;&#36807;&#39640;&#25928;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#22312;&#22788;&#29702;&#26041;&#38754;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23558;Mamba&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#20132;&#20114;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#22240;&#20026;&#36890;&#36947;&#20132;&#20114;&#19981;&#36275;&#65292;&#26080;&#27861;&#26377;&#25928;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReMamber&#65292;&#36825;&#26159;&#19968;&#31181;&#25972;&#21512;&#20102;Mamba&#21644;&#22810;&#27169;&#24577;Mamba Twister&#22359;&#24378;&#22823;&#21151;&#33021;&#30340;&#26032;&#22411;RIS&#26550;&#26500;&#12290;Mamba Twister&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#26126;&#30830;&#24314;&#27169;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;ReMamber&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20854;&#20182;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17839v1 Announce Type: cross  Abstract: Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks. However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies. Fortunately, Mamba addresses this with efficient linear complexity in processing. However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data. In this paper, we propose ReMamber, a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism. We achieve the state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough analyses of ReMamber and discuss other
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.17827</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21512;&#25104;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17827
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#33258;&#28982;&#30340;3D&#25163;-&#29289;&#20307;&#20132;&#20114;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26399;&#26395;&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#29289;&#20307;&#21160;&#20316;&#22312;&#29289;&#29702;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#19978;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffH2O&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#21333;&#25163;&#25110;&#21452;&#25163;&#29289;&#20307;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19977;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#25235;&#21462;&#38454;&#27573;&#21644;&#22522;&#20110;&#25991;&#26412;&#20132;&#20114;&#38454;&#27573;&#65292;&#24182;&#20026;&#27599;&#20010;&#38454;&#27573;&#20351;&#29992;&#21333;&#29420;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#25235;&#21462;&#38454;&#27573;&#20013;&#65292;&#27169;&#22411;&#20165;&#29983;&#25104;&#25163;&#37096;&#21160;&#20316;&#65292;&#32780;&#22312;&#20132;&#20114;&#38454;&#27573;&#20013;&#65292;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#37117;&#34987;&#21512;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#23494;&#32806;&#21512;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17827v1 Announce Type: cross  Abstract: Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes 
&lt;/p&gt;</description></item><item><title>Stackelberg&#35268;&#21010;&#22312;&#29702;&#35770;&#19978;&#30340;&#22797;&#26434;&#24615;&#20998;&#26512;&#26174;&#31034;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#24182;&#19981;&#27604;&#32463;&#20856;&#35268;&#21010;&#26356;&#38590;&#65292;&#20294;&#22312;&#22810;&#39033;&#24335;&#35745;&#21010;&#38271;&#24230;&#38480;&#21046;&#19979;&#65292;&#20250;&#25552;&#21319;&#21040;&#22810;&#39033;&#24335;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#20013;&#26356;&#39640;&#19968;&#32423;</title><link>https://arxiv.org/abs/2403.17826</link><description>&lt;p&gt;
&#22312;Stackelberg&#35268;&#21010;&#21644;&#20803;&#25805;&#20316;&#39564;&#35777;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#19978;&#65306;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
On the Computational Complexity of Stackelberg Planning and Meta-Operator Verification: Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17826
&lt;/p&gt;
&lt;p&gt;
Stackelberg&#35268;&#21010;&#22312;&#29702;&#35770;&#19978;&#30340;&#22797;&#26434;&#24615;&#20998;&#26512;&#26174;&#31034;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#24182;&#19981;&#27604;&#32463;&#20856;&#35268;&#21010;&#26356;&#38590;&#65292;&#20294;&#22312;&#22810;&#39033;&#24335;&#35745;&#21010;&#38271;&#24230;&#38480;&#21046;&#19979;&#65292;&#20250;&#25552;&#21319;&#21040;&#22810;&#39033;&#24335;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#20013;&#26356;&#39640;&#19968;&#32423;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stackelberg&#35268;&#21010;&#26159;&#26368;&#36817;&#24341;&#20837;&#30340;&#21333;&#36718;&#20004;&#20154;&#23545;&#25239;&#35268;&#21010;&#27169;&#22411;&#65292;&#20854;&#20013;&#20004;&#20301;&#29609;&#23478;&#22312;&#19968;&#20010;&#32852;&#21512;&#30340;&#32463;&#20856;&#35268;&#21010;&#20219;&#21153;&#20013;&#34892;&#21160;&#65292;&#31532;&#19968;&#20301;&#29609;&#23478;&#30340;&#30446;&#26631;&#26159;&#38459;&#30861;&#31532;&#20108;&#20301;&#29609;&#23478;&#23454;&#29616;&#30446;&#26631;&#12290;&#36825;&#23558;Stackelberg&#35268;&#21010;&#38382;&#39064;&#32622;&#20110;&#32463;&#20856;&#35268;&#21010;&#21644;&#19968;&#33324;&#32452;&#21512;&#20004;&#20154;&#28216;&#25103;&#20043;&#38388;&#12290;&#20294;&#65292;&#30830;&#20999;&#22320;&#35828;&#26159;&#22312;&#21738;&#37324;&#65311;&#36804;&#20170;&#20026;&#27490;&#23545;Stackelberg&#35268;&#21010;&#30340;&#25152;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#23454;&#38469;&#26041;&#38754;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#31532;&#19968;&#27425;&#23545;Stackelberg&#35268;&#21010;&#30340;&#29702;&#35770;&#22797;&#26434;&#24615;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;Stackelberg&#35268;&#21010;&#23454;&#38469;&#19978;&#19981;&#27604;&#32463;&#20856;&#35268;&#21010;&#26356;&#38590;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#39033;&#24335;&#35745;&#21010;&#38271;&#24230;&#38480;&#21046;&#19979;&#65292;Stackelberg&#35268;&#21010;&#22312;&#22810;&#39033;&#24335;&#22797;&#26434;&#24615;&#23618;&#27425;&#32467;&#26500;&#20013;&#26356;&#39640;&#19968;&#32423;&#65292;&#34920;&#26126;&#32534;&#35793;&#25104;&#32463;&#20856;&#35268;&#21010;&#20250;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#19979;&#25351;&#25968;&#32423;&#35745;&#21010;&#38271;&#24230;&#22686;&#21152;&#12290;&#22312;&#35797;&#22270;&#35782;&#21035;&#21487;&#22788;&#29702;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17826v1 Announce Type: new  Abstract: Stackelberg planning is a recently introduced single-turn two-player adversarial planning model, where two players are acting in a joint classical planning task, the objective of the first player being hampering the second player from achieving its goal. This places the Stackelberg planning problem somewhere between classical planning and general combinatorial two-player games. But, where exactly? All investigations of Stackelberg planning so far focused on practical aspects. We close this gap by conducting the first theoretical complexity analysis of Stackelberg planning. We show that in general Stackelberg planning is actually no harder than classical planning. Under a polynomial plan-length restriction, however, Stackelberg planning is a level higher up in the polynomial complexity hierarchy, suggesting that compilations into classical planning come with a worst-case exponential plan-length increase. In attempts to identify tractable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21152;&#24555;&#39057;&#35889;&#30417;&#31649;&#27969;&#31243;&#20013;&#30340;&#31034;&#20363;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;LLMs&#22312;&#27492;&#32972;&#26223;&#19979;&#21487;&#20197;&#21457;&#25381;&#30340;&#21508;&#31181;&#20316;&#29992;&#65292;&#21516;&#26102;&#35782;&#21035;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#23454;&#39564;&#12289;&#23454;&#29992;&#26696;&#20363;&#21644;&#35265;&#35299;&#31361;&#20986;&#20102;LLMs&#22312;&#39057;&#35889;&#31649;&#29702;&#20013;&#30340;&#36716;&#21464;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.17819</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#26080;&#32447;&#30005;&#39057;&#35889;&#30417;&#31649;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21152;&#24555;&#39057;&#35889;&#30417;&#31649;&#27969;&#31243;&#20013;&#30340;&#31034;&#20363;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;LLMs&#22312;&#27492;&#32972;&#26223;&#19979;&#21487;&#20197;&#21457;&#25381;&#30340;&#21508;&#31181;&#20316;&#29992;&#65292;&#21516;&#26102;&#35782;&#21035;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#23454;&#39564;&#12289;&#23454;&#29992;&#26696;&#20363;&#21644;&#35265;&#35299;&#31361;&#20986;&#20102;LLMs&#22312;&#39057;&#35889;&#31649;&#29702;&#20013;&#30340;&#36716;&#21464;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#39057;&#35889;&#30417;&#31649;&#26159;&#19968;&#20010;&#22797;&#26434;&#32780;&#20005;&#33499;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#25216;&#26415;&#36827;&#27493;&#36805;&#36895;&#12289;&#23545;&#39057;&#35889;&#38656;&#27714;&#22686;&#21152;&#20197;&#21450;&#22810;&#26041;&#21033;&#30410;&#25912;&#20851;&#32773;&#21487;&#33021;&#23384;&#22312;&#30456;&#20114;&#20914;&#31361;&#30340;&#21033;&#30410;&#65292;&#21152;&#20043;&#37325;&#35201;&#30340;&#32463;&#27982;&#24433;&#21709;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#24773;&#20917;&#65292;&#30417;&#31649;&#26426;&#26500;&#24517;&#39035;&#26377;&#25928;&#22320;&#19982;&#21508;&#26041;&#21512;&#20316;&#65292;&#36319;&#19978;&#20840;&#29699;&#25216;&#26415;&#36235;&#21183;&#65292;&#36827;&#34892;&#25216;&#26415;&#35780;&#20272;&#65292;&#21450;&#26102;&#21457;&#25918;&#35768;&#21487;&#35777;&#65292;&#24182;&#36981;&#23432;&#21508;&#31181;&#27861;&#24459;&#21644;&#25919;&#31574;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17819v1 Announce Type: cross  Abstract: Wireless spectrum regulation is a complex and demanding process due to the rapid pace of technological progress, increasing demand for spectrum, and a multitude of stakeholders with potentially conflicting interests, alongside significant economic implications. To navigate this, regulators must engage effectively with all parties, keep pace with global technology trends, conduct technical evaluations, issue licenses in a timely manner, and comply with various legal and policy frameworks.   In light of these challenges, this paper demonstrates example applications of Large Language Models (LLMs) to expedite spectrum regulatory processes. We explore various roles that LLMs can play in this context while identifying some of the challenges to address. The paper also offers practical case studies and insights, with appropriate experiments, highlighting the transformative potential of LLMs in spectrum management.
&lt;/p&gt;</description></item><item><title>D-PAD &#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#27973;&#23618;&#22810;&#39057;&#27169;&#24335;&#35299;&#32544;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22810;&#20998;&#37327;&#20998;&#35299;&#21644;&#20998;&#35299;-&#37325;&#26500;-&#20998;&#35299;&#27169;&#22359;&#65292;&#26377;&#25928;&#35299;&#32544;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17814</link><description>&lt;p&gt;
D-PAD: &#28145;&#27973;&#23618;&#22810;&#39057;&#27169;&#24335;&#35299;&#32544;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17814
&lt;/p&gt;
&lt;p&gt;
D-PAD &#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#27973;&#23618;&#22810;&#39057;&#27169;&#24335;&#35299;&#32544;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22810;&#20998;&#37327;&#20998;&#35299;&#21644;&#20998;&#35299;-&#37325;&#26500;-&#20998;&#35299;&#27169;&#22359;&#65292;&#26377;&#25928;&#35299;&#32544;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#26377;&#25928;&#35299;&#32544;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;D-PAD&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#28145;&#27973;&#23618;&#22810;&#39057;&#27169;&#24335;&#35299;&#32544;&#31070;&#32463;&#32593;&#32476;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#20102;&#22810;&#20998;&#37327;&#20998;&#35299;&#65288;MCD&#65289;&#27169;&#22359;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#20026;&#20855;&#26377;&#19981;&#21516;&#39057;&#29575;&#33539;&#22260;&#30340;&#32452;&#20998;&#65292;&#23545;&#24212;&#20110;&#8220;&#27973;&#8221;&#26041;&#38754;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;-&#37325;&#26500;-&#20998;&#35299;&#65288;D-R-D&#65289;&#27169;&#22359;&#65292;&#36880;&#28176;&#25552;&#21462;&#32452;&#20998;&#20013;&#28151;&#21512;&#30340;&#39057;&#29575;&#20449;&#24687;&#65292;&#23545;&#24212;&#20110;&#8220;&#28145;&#8221;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17814v1 Announce Type: new  Abstract: In time series forecasting, effectively disentangling intricate temporal patterns is crucial. While recent works endeavor to combine decomposition techniques with deep learning, multiple frequencies may still be mixed in the decomposed components, e.g., trend and seasonal. Furthermore, frequency domain analysis methods, e.g., Fourier and wavelet transforms, have limitations in resolution in the time domain and adaptability. In this paper, we propose D-PAD, a deep-shallow multi-frequency patterns disentangling neural network for time series forecasting. Specifically, a multi-component decomposing (MCD) block is introduced to decompose the series into components with different frequency ranges, corresponding to the "shallow" aspect. A decomposition-reconstruction-decomposition (D-R-D) module is proposed to progressively extract the information of frequencies mixed in the components, corresponding to the "deep" aspect. After that, an intera
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#23433;&#20840;&#24212;&#29992;&#20013;&#65292;&#21363;&#26102;&#24037;&#31243;&#30340;Gemini-pro&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#19982;&#24494;&#35843;&#30340;Vision Transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#20851;&#38190;&#23433;&#20840;&#25361;&#25112;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.17787</link><description>&lt;p&gt;
&#35780;&#20272;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#23433;&#20840;&#24212;&#29992;&#20013;&#65292;&#22312;&#21363;&#26102;&#24037;&#31243;&#36807;&#31243;&#20013;&#35774;&#35745;&#30340;&#22823;&#22411;&#22810;&#27169;&#22411;&#19982;&#24494;&#35843;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17787
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#23433;&#20840;&#24212;&#29992;&#20013;&#65292;&#21363;&#26102;&#24037;&#31243;&#30340;Gemini-pro&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#19982;&#24494;&#35843;&#30340;Vision Transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#20851;&#38190;&#23433;&#20840;&#25361;&#25112;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#23548;&#33268;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#65288;&#22914;Gemini-pro&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#24320;&#22987;&#36716;&#21464;&#21508;&#31181;&#24212;&#29992;&#12290;&#36825;&#20123;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26088;&#22312;&#35299;&#37322;&#21644;&#20998;&#26512;&#22797;&#26434;&#25968;&#25454;&#65292;&#25972;&#21512;&#20102;&#20197;&#24448;&#38590;&#20197;&#23454;&#29616;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#35268;&#27169;&#65292;&#20026;&#19968;&#31995;&#21015;&#24212;&#29992;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#35299;&#20915;&#20851;&#38190;&#23433;&#20840;&#25361;&#25112;&#26041;&#38754;&#65292;&#21363;&#26102;&#24037;&#31243;&#30340;Gemini-pro LMMs&#19982;&#24494;&#35843;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#31616;&#21333;&#35302;&#21457;&#22120;&#65288;&#22914;&#26041;&#24418;&#23567;&#26041;&#22359;&#65289;&#20197;&#31034;&#28508;&#22312;&#21518;&#38376;&#30340;&#22312;&#35270;&#35273;&#19978;&#26174;&#32780;&#26131;&#35265;&#30340;&#20219;&#21153;&#65292;&#20197;&#21450;&#36890;&#36807;&#35270;&#35273;&#34920;&#31034;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#30340;&#22312;&#35270;&#35273;&#19978;&#19981;&#26126;&#26174;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;Gemini-pro&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17787v1 Announce Type: new  Abstract: The success of Large Language Models (LLMs) has led to a parallel rise in the development of Large Multimodal Models (LMMs), such as Gemini-pro, which have begun to transform a variety of applications. These sophisticated multimodal models are designed to interpret and analyze complex data, integrating both textual and visual information on a scale previously unattainable, opening new avenues for a range of applications. This paper investigates the applicability and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision Transformer (ViT) models in addressing critical security challenges. We focus on two distinct tasks: a visually evident task of detecting simple triggers, such as small squares in images, indicative of potential backdoors, and a non-visually evident task of malware classification through visual representations. Our results highlight a significant divergence in performance, with Gemini-pro falling shor
&lt;/p&gt;</description></item><item><title>SciCapenter &#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#21033;&#29992;&#23574;&#31471;&#30340;AI&#25216;&#26415;&#20026;&#31185;&#23398;&#22270;&#20687;&#29983;&#25104;&#22810;&#26679;&#30340;&#26631;&#39064;&#65292;&#25552;&#20379;&#35780;&#20998;&#21644;&#32508;&#21512;&#26816;&#26597;&#34920;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#26631;&#39064;&#25776;&#20889;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;</title><link>https://arxiv.org/abs/2403.17784</link><description>&lt;p&gt;
SciCapenter: &#36890;&#36807;&#26426;&#22120;&#29983;&#25104;&#30340;&#26631;&#39064;&#21644;&#35780;&#20998;&#25903;&#25345;&#31185;&#23398;&#22270;&#20687;&#30340;&#26631;&#39064;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17784
&lt;/p&gt;
&lt;p&gt;
SciCapenter &#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#21033;&#29992;&#23574;&#31471;&#30340;AI&#25216;&#26415;&#20026;&#31185;&#23398;&#22270;&#20687;&#29983;&#25104;&#22810;&#26679;&#30340;&#26631;&#39064;&#65292;&#25552;&#20379;&#35780;&#20998;&#21644;&#32508;&#21512;&#26816;&#26597;&#34920;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#26631;&#39064;&#25776;&#20889;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#20026;&#22270;&#20687;&#25776;&#20889;&#26631;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#35835;&#32773;&#20005;&#37325;&#20381;&#36182;&#36825;&#20123;&#26631;&#39064;&#26469;&#29702;&#35299;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SciCapenter&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23427;&#25972;&#21512;&#20102;&#23574;&#31471;&#30340;AI&#25216;&#26415;&#29992;&#20110;&#31185;&#23398;&#22270;&#20687;&#26631;&#39064;&#65292;&#20197;&#24110;&#21161;&#26631;&#39064;&#30340;&#25776;&#20889;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17784v1 Announce Type: cross  Abstract: Crafting effective captions for figures is important. Readers heavily depend on these captions to grasp the figure's message. However, despite a well-developed set of AI technologies for figures and captions, these have rarely been tested for usefulness in aiding caption writing. This paper introduces SciCapenter, an interactive system that puts together cutting-edge AI technologies for scientific figure captions to aid caption composition. SciCapenter generates a variety of captions for each figure in a scholarly article, providing scores and a comprehensive checklist to assess caption quality across multiple critical aspects, such as helpfulness, OCR mention, key takeaways, and visual properties reference. Users can directly edit captions in SciCapenter, resubmit for revised evaluations, and iteratively refine them. A user study with Ph.D. students indicates that SciCapenter significantly lowers the cognitive load of caption writing.
&lt;/p&gt;</description></item><item><title>MaRDI&#24320;&#21457;&#20102;&#19968;&#20010;FAIR&#21644;&#21487;&#26426;&#22120;&#35299;&#37322;&#30340;&#27169;&#26495;&#65292;&#29992;&#20110;&#20840;&#38754;&#25991;&#26723;&#21270;&#24212;&#29992;&#25968;&#23398;&#20013;&#30340;&#24314;&#27169;-&#20223;&#30495;-&#20248;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;MaRDMO&#21644;MathModDB&#30693;&#35782;&#22270;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.17778</link><description>&lt;p&gt;
&#36808;&#21521;&#24212;&#29992;&#25968;&#23398;&#20013;&#24037;&#20316;&#27969;&#31243;&#21644;&#27169;&#22411;&#30340;FAIR&#25991;&#26723;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards a FAIR Documentation of Workflows and Models in Applied Mathematics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17778
&lt;/p&gt;
&lt;p&gt;
MaRDI&#24320;&#21457;&#20102;&#19968;&#20010;FAIR&#21644;&#21487;&#26426;&#22120;&#35299;&#37322;&#30340;&#27169;&#26495;&#65292;&#29992;&#20110;&#20840;&#38754;&#25991;&#26723;&#21270;&#24212;&#29992;&#25968;&#23398;&#20013;&#30340;&#24314;&#27169;-&#20223;&#30495;-&#20248;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;MaRDMO&#21644;MathModDB&#30693;&#35782;&#22270;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17778v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#24314;&#27169;-&#20223;&#30495;-&#20248;&#21270;&#24037;&#20316;&#27969;&#22312;&#24212;&#29992;&#25968;&#23398;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#25968;&#23398;&#30740;&#31350;&#25968;&#25454;&#20513;&#35758;MaRDI&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;FAIR&#21644;&#21487;&#26426;&#22120;&#35299;&#37322;&#30340;&#27169;&#26495;&#65292;&#23545;&#36825;&#20123;&#24037;&#20316;&#27969;&#30340;&#20840;&#38754;&#25991;&#26723;&#21270;&#20570;&#20986;&#20102;&#22238;&#24212;&#12290;&#30740;&#31350;&#25968;&#25454;&#31649;&#29702;&#32452;&#32455;&#32773;&#30340;&#25554;&#20214;MaRDMO&#20351;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#31185;&#23398;&#23478;&#21487;&#20197;&#21033;&#29992;MaRDI&#27169;&#26495;&#22312;MaRDI&#38376;&#25143;&#19978;&#26080;&#32541;&#22320;&#35760;&#24405;&#21644;&#21457;&#24067;&#20182;&#20204;&#30340;&#24037;&#20316;&#27969;&#12290;&#36825;&#20123;&#24037;&#20316;&#27969;&#30340;&#26680;&#24515;&#26159;&#25968;&#23398;&#27169;&#22411;&#12290;MaRDI&#36890;&#36807;MathModDB&#26412;&#20307;&#35770;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#24418;&#24335;&#27169;&#22411;&#25551;&#36848;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26469;&#33258;&#25968;&#23383;&#20154;&#25991;&#23398;&#31185;&#30340;&#20195;&#25968;&#24314;&#27169;&#24037;&#20316;&#27969;&#31243;&#20013;MaRDMO&#21644;MathModDB&#30693;&#35782;&#22270;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#20010;&#28436;&#31034;&#31361;&#26174;&#20102;&#36825;&#20004;&#39033;&#26381;&#21153;&#22312;&#21407;&#22987;&#25968;&#20540;&#39046;&#22495;&#20043;&#22806;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17778v1 Announce Type: new  Abstract: Modeling-Simulation-Optimization workflows play a fundamental role in applied mathematics. The Mathematical Research Data Initiative, MaRDI, responded to this by developing a FAIR and machine-interpretable template for a comprehensive documentation of such workflows. MaRDMO, a Plugin for the Research Data Management Organiser, enables scientists from diverse fields to document and publish their workflows on the MaRDI Portal seamlessly using the MaRDI template. Central to these workflows are mathematical models. MaRDI addresses them with the MathModDB ontology, offering a structured formal model description. Here, we showcase the interaction between MaRDMO and the MathModDB Knowledge Graph through an algebraic modeling workflow from the Digital Humanities. This demonstration underscores the versatility of both services beyond their original numerical domain.
&lt;/p&gt;</description></item><item><title>&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17768</link><description>&lt;p&gt;
&#20174;&#23398;&#26415;&#22797;&#26434;&#24615;&#21040;&#20844;&#20247;&#21465;&#20107;&#65306;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17768
&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#20316;&#20026;&#19968;&#20010;&#26725;&#26753;&#65292;&#24039;&#22937;&#22320;&#23558;&#22797;&#26434;&#30340;&#30740;&#31350;&#25991;&#31456;&#32763;&#35793;&#25104;&#19982;&#26356;&#24191;&#27867;&#30340;&#20844;&#20247; resonant &#30340;&#25253;&#36947;&#12290;&#36825;&#31181;&#21465;&#20107;&#30340;&#33258;&#21160;&#29983;&#25104;&#22686;&#24378;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#26009;&#24211;&#26469;&#20419;&#36827;&#36825;&#31181;&#33539;&#24335;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;&#20061;&#20010;&#23398;&#31185;&#39046;&#22495;&#20013;&#23398;&#26415;&#20986;&#29256;&#29289;&#21450;&#20854;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#24179;&#34892;&#32534;&#35793;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#31185;&#23398;&#26032;&#38395;&#21465;&#20107;&#21644;&#23398;&#26415;&#25991;&#31295;&#20043;&#38388;&#30340;&#21487;&#35835;&#24615;&#21644;&#31616;&#27905;&#24615;&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#36807;&#31243;&#21253;&#25324;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#20026;&#26410;&#26469;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17768v1 Announce Type: cross  Abstract: Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related 
&lt;/p&gt;</description></item><item><title>DataCook&#26159;&#19968;&#20010;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#29256;&#26435;&#20445;&#25252;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21046;&#20316;&#21453;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#29256;&#26435;&#25345;&#26377;&#20154;&#33021;&#22815;&#22312;&#37096;&#32626;&#38454;&#27573;&#25511;&#21046;&#27169;&#22411;&#30340;&#25480;&#26435;&#12290;</title><link>https://arxiv.org/abs/2403.17755</link><description>&lt;p&gt;
DataCook&#65306;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#29256;&#26435;&#20445;&#25252;&#30340;&#21453;&#23545;&#25239;&#26679;&#26412;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
DataCook: Crafting Anti-Adversarial Examples for Healthcare Data Copyright Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17755
&lt;/p&gt;
&lt;p&gt;
DataCook&#26159;&#19968;&#20010;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#29256;&#26435;&#20445;&#25252;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21046;&#20316;&#21453;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#29256;&#26435;&#25345;&#26377;&#20154;&#33021;&#22815;&#22312;&#37096;&#32626;&#38454;&#27573;&#25511;&#21046;&#27169;&#22411;&#30340;&#25480;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#29256;&#26435;&#20445;&#25252;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#31532;&#19977;&#26041;&#28389;&#29992;&#30340;&#25361;&#25112;&#26085;&#30410;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#29256;&#26435;&#20445;&#25252;&#26041;&#27861;&#24212;&#29992;&#20110;&#25968;&#25454;&#21457;&#24067;&#20043;&#21069;&#65292;&#36825;&#24847;&#21619;&#30528;&#23545;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#21464;&#24471;&#19981;&#21487;&#25511;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DataCook&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#37096;&#32626;&#38454;&#27573;&#20445;&#25252;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#30340;&#29256;&#26435;&#12290;DataCook&#36890;&#36807;&#22312;&#20998;&#21457;&#20043;&#21069;&#8220;&#22788;&#29702;&#8221;&#21407;&#22987;&#25968;&#25454;&#30340;&#26041;&#24335;&#36816;&#34892;&#65292;&#20174;&#32780;&#20351;&#24320;&#21457;&#30340;&#27169;&#22411;&#22312;&#36825;&#20123;&#22788;&#29702;&#36807;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#27491;&#24120;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#38454;&#27573;&#65292;&#21407;&#22987;&#27979;&#35797;&#25968;&#25454;&#20063;&#24517;&#39035;&#36890;&#36807;DataCook&#8220;&#22788;&#29702;&#8221;&#20197;&#30830;&#20445;&#27169;&#22411;&#27491;&#24120;&#36816;&#34892;&#12290;&#35813;&#36807;&#31243;&#20351;&#29256;&#26435;&#25345;&#26377;&#20154;&#22312;&#37096;&#32626;&#38454;&#27573;&#25511;&#21046;&#25480;&#26435;&#12290;DataCook&#32972;&#21518;&#30340;&#26426;&#21046;&#26159;&#21046;&#20316;&#21453;&#23545;&#25239;&#26679;&#26412;&#65288;AntiAdv&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#30340;&#20449;&#24515;&#65292;&#32780;&#19981;&#26159;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17755v1 Announce Type: new  Abstract: In the realm of healthcare, the challenges of copyright protection and unauthorized third-party misuse are increasingly significant. Traditional methods for data copyright protection are applied prior to data distribution, implying that models trained on these data become uncontrollable. This paper introduces a novel approach, named DataCook, designed to safeguard the copyright of healthcare data during the deployment phase. DataCook operates by "cooking" the raw data before distribution, enabling the development of models that perform normally on this processed data. However, during the deployment phase, the original test data must be also "cooked" through DataCook to ensure normal model performance. This process grants copyright holders control over authorization during the deployment phase. The mechanism behind DataCook is by crafting anti-adversarial examples (AntiAdv), which are designed to enhance model confidence, as opposed to st
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#25913;&#36827;LIME&#22270;&#20687;&#35299;&#37322;&#65292;&#20197;&#20943;&#23569;&#20856;&#22411;&#33945;&#29305;&#21345;&#27931;&#25277;&#26679;&#20135;&#29983;&#30340;&#20154;&#24037;&#20135;&#29289;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17742</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#25277;&#26679;&#25913;&#36827;LIME&#22270;&#20687;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Using Stratified Sampling to Improve LIME Image Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17742
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#25913;&#36827;LIME&#22270;&#20687;&#35299;&#37322;&#65292;&#20197;&#20943;&#23569;&#20856;&#22411;&#33945;&#29305;&#21345;&#27931;&#25277;&#26679;&#20135;&#29983;&#30340;&#20154;&#24037;&#20135;&#29289;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20998;&#23618;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;LIME&#22270;&#20687;&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#38024;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#27169;&#22411;&#26080;&#20851;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#20856;&#22411;&#33945;&#29305;&#21345;&#27931;&#25277;&#26679;&#20135;&#29983;&#30340;&#20154;&#24037;&#20135;&#29289;&#12290;&#36825;&#20123;&#20154;&#24037;&#20135;&#29289;&#26159;&#30001;&#20110;&#22312;&#35299;&#37322;&#30340;&#22270;&#20687;&#21608;&#22260;&#30340;&#21512;&#25104;&#37051;&#22495;&#20013;&#23545;&#20381;&#36182;&#21464;&#37327;&#30340;&#27424;&#37319;&#26679;&#36896;&#25104;&#30340;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#20110;&#26080;&#27861;&#22312;&#37319;&#26679;&#25968;&#25454;&#19978;&#25311;&#21512;&#32447;&#24615;&#22238;&#24402;&#22120;&#32780;&#23548;&#33268;&#35299;&#37322;&#19981;&#36275;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19982;Shapley&#29702;&#35770;&#30340;&#32852;&#31995;&#65292;&#22312;&#36807;&#21435;&#26366;&#25552;&#20986;&#20851;&#20110;&#27424;&#37319;&#26679;&#21644;&#26679;&#26412;&#30456;&#20851;&#24615;&#30340;&#31867;&#20284;&#35770;&#28857;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#20998;&#23618;&#25277;&#26679;&#20272;&#35745;&#22120;&#25152;&#38656;&#30340;&#25152;&#26377;&#20844;&#24335;&#21644;&#35843;&#25972;&#22240;&#23376;&#12290;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17742v1 Announce Type: new  Abstract: We investigate the use of a stratified sampling approach for LIME Image, a popular model-agnostic explainable AI method for computer vision tasks, in order to reduce the artifacts generated by typical Monte Carlo sampling. Such artifacts are due to the undersampling of the dependent variable in the synthetic neighborhood around the image being explained, which may result in inadequate explanations due to the impossibility of fitting a linear regressor on the sampled data. We then highlight a connection with the Shapley theory, where similar arguments about undersampling and sample relevance were suggested in the past. We derive all the formulas and adjustment factors required for an unbiased stratified sampling estimator. Experiments show the efficacy of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;</title><link>https://arxiv.org/abs/2403.17740</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#65306;&#24322;&#36136;&#20132;&#20114;&#24314;&#27169;&#29992;&#20110;&#20919;&#21551;&#21160;&#35780;&#20998;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17740
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20919;&#21551;&#21160;&#35780;&#20998;&#39044;&#27979;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#31995;&#65292;&#20363;&#22914;&#21327;&#21516;&#36807;&#28388;&#12289;&#31038;&#20132;&#25512;&#33616;&#21644;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65292;&#20197;&#32531;&#35299;&#20919;&#21551;&#21160;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#19981;&#21516;&#35282;&#33394;&#20043;&#38388;&#30340;&#25968;&#25454;&#26500;&#24314;&#30340;&#26174;&#24335;&#20851;&#31995;&#21487;&#33021;&#19981;&#21487;&#38752;&#19988;&#26080;&#20851;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#29305;&#23450;&#25512;&#33616;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#38480;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#12290;HIRE&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#39044;&#20808;&#23450;&#20041;&#30340;&#20132;&#20114;&#27169;&#24335;&#25110;&#25163;&#21160;&#26500;&#24314;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#65292;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17740v1 Announce Type: cross  Abstract: Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27979;&#35797;&#26102;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#65292;&#36890;&#36807;&#24314;&#27169;&#20256;&#25773;&#22270;&#21644;&#26500;&#24314;&#27979;&#35797;&#26102;&#36866;&#24212;&#26694;&#26550;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17735</link><description>&lt;p&gt;
&#36890;&#36807;&#27979;&#35797;&#26102;&#36866;&#24212;&#36827;&#34892;&#36234;&#30028;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution Rumor Detection via Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17735
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27979;&#35797;&#26102;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#65292;&#36890;&#36807;&#24314;&#27169;&#20256;&#25773;&#22270;&#21644;&#26500;&#24314;&#27979;&#35797;&#26102;&#36866;&#24212;&#26694;&#26550;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35875;&#35328;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#36805;&#36895;&#20256;&#25773;&#65292;&#35875;&#35328;&#26816;&#27979;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26497;&#20854;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35875;&#35328;&#26816;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#24050;&#32463;&#20174;&#30456;&#21516;&#25968;&#25454;&#20998;&#24067;&#20013;&#25910;&#38598;&#20102;&#36275;&#22815;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26032;&#38395;&#20027;&#39064;&#12289;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#12289;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#21450;&#30001;&#26032;&#38395;&#28909;&#24230;&#24341;&#36215;&#30340;&#20256;&#25773;&#35268;&#27169;&#30340;&#24046;&#24322;&#65292;&#35757;&#32451;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#21457;&#29983;&#20102;&#26174;&#33879;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#23548;&#33268;&#20102;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#22312;&#36234;&#30028;&#65288;OOD&#65289;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TARD&#65289;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#23558;&#26032;&#38395;&#30340;&#20256;&#25773;&#24314;&#27169;&#20026;&#19968;&#20010;&#20256;&#25773;&#22270;&#65292;&#24182;&#26500;&#24314;&#20256;&#25773;&#22270;&#27979;&#35797;&#26102;&#36866;&#24212;&#26694;&#26550;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17735v1 Announce Type: new  Abstract: Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Existing methods for rumor detection have achieved good performance, as they have collected enough corpus from the same data distribution for model training. However, significant distribution shifts between the training data and real-world test data occur due to differences in news topics, social media platforms, languages and the variance in propagation scale caused by news popularity. This leads to a substantial decline in the performance of these existing methods in Out-Of-Distribution (OOD) situations. To address this problem, we propose a simple and efficient method named Test-time Adaptation for Rumor Detection under distribution shifts (TARD). This method models the propagation of news in the form of a propagation graph, and builds propagation graph test-time adaptation framework, enhancing the model's adaptability and r
&lt;/p&gt;</description></item><item><title>TinySaver&#26159;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17726</link><description>&lt;p&gt;
&#23567;&#22411;&#27169;&#22411;&#26159;&#22823;&#22411;&#27169;&#22411;&#30340;&#35745;&#31639;&#33410;&#30465;&#32773;
&lt;/p&gt;
&lt;p&gt;
Tiny Models are the Computational Saver for Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17726
&lt;/p&gt;
&lt;p&gt;
TinySaver&#26159;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TinySaver&#65292;&#19968;&#31181;&#31867;&#20284;&#20110;&#26089;&#26399;&#36864;&#20986;&#30340;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#21387;&#32553;&#25216;&#26415;&#19981;&#21516;&#65292;&#20687;TinySaver&#36825;&#26679;&#30340;&#21160;&#24577;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#38590;&#24230;&#24046;&#24322;&#65292;&#20351;&#24471;&#26576;&#20123;&#36755;&#20837;&#33021;&#22815;&#25552;&#21069;&#23436;&#25104;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26089;&#26399;&#36864;&#20986;&#35774;&#35745;&#26159;&#36890;&#36807;&#21521;&#27169;&#22411;&#30340;&#39592;&#24178;&#32467;&#26500;&#38468;&#21152;&#39069;&#22806;&#30340;&#32593;&#32476;&#20998;&#25903;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23436;&#20840;&#29420;&#31435;&#30340;&#23567;&#22411;&#27169;&#22411;&#21487;&#20197;&#22312;&#23545;&#24615;&#33021;&#24433;&#21709;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#26367;&#20195;&#36739;&#22823;&#27169;&#22411;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#12290;&#23558;&#23427;&#20204;&#20316;&#20026;&#31532;&#19968;&#20010;&#36864;&#20986;&#28857;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#25628;&#32034;&#24182;&#20351;&#29992;&#26368;&#21512;&#36866;&#30340;&#23567;&#22411;&#27169;&#22411;&#20316;&#20026;&#32473;&#23450;&#22823;&#22411;&#27169;&#22411;&#30340;&#35745;&#31639;&#33410;&#30465;&#32773;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17726v1 Announce Type: new  Abstract: This paper introduces TinySaver, an early-exit-like dynamic model compression approach which employs tiny models to substitute large models adaptively. Distinct from traditional compression techniques, dynamic methods like TinySaver can leverage the difficulty differences to allow certain inputs to complete their inference processes early, thereby conserving computational resources. Most existing early exit designs are implemented by attaching additional network branches to the model's backbone. Our study, however, reveals that completely independent tiny models can replace a substantial portion of the larger models' job with minimal impact on performance. Employing them as the first exit can remarkably enhance computational efficiency. By searching and employing the most appropriate tiny model as the computational saver for a given large model, the proposed approaches work as a novel and generic method to model compression. This finding
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#26041;&#27861;&#65292;JudgeDeceiver&#65292;&#38024;&#23545;LLM-as-a-Judge&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#23545;&#25239;&#24207;&#21015;&#23454;&#29616;&#20102;&#26377;&#38024;&#23545;&#24615;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#35780;&#20272;&#25805;&#25511;&#12290;</title><link>https://arxiv.org/abs/2403.17710</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#30340;&#23545;LLM&#35780;&#21028;&#31995;&#32479;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Optimization-based Prompt Injection Attack to LLM-as-a-Judge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17710
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#26041;&#27861;&#65292;JudgeDeceiver&#65292;&#38024;&#23545;LLM-as-a-Judge&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#23545;&#25239;&#24207;&#21015;&#23454;&#29616;&#20102;&#26377;&#38024;&#23545;&#24615;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#35780;&#20272;&#25805;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM-as-a-Judge &#26159;&#19968;&#31181;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#25991;&#26412;&#20449;&#24687;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;&#26681;&#25454;&#29616;&#26377;&#30740;&#31350;&#65292;LLMs&#22312;&#25552;&#20379;&#20256;&#32479;&#20154;&#31867;&#35780;&#20272;&#30340;&#24341;&#20154;&#27880;&#30446;&#26367;&#20195;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#38024;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;JudgeDeceiver&#65292;&#19968;&#31181;&#38024;&#23545;LLM-as-a-Judge&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21046;&#23450;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#29992;&#20110;&#25915;&#20987;LLM-as-a-Judge&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#20248;&#21270;&#31639;&#27861;&#39640;&#25928;&#22320;&#33258;&#21160;&#21270;&#29983;&#25104;&#23545;&#25239;&#24207;&#21015;&#65292;&#23454;&#29616;&#23545;&#27169;&#22411;&#35780;&#20272;&#30340;&#26377;&#38024;&#23545;&#24615;&#21644;&#26377;&#25928;&#30340;&#25805;&#20316;&#12290;&#19982;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#21151;&#25928;&#65292;&#32473;&#22522;&#20110;LLM&#30340;&#21028;&#26029;&#31995;&#32479;&#24403;&#21069;&#30340;&#23433;&#20840;&#33539;&#24335;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17710v1 Announce Type: cross  Abstract: LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs). Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment. However, the robustness of these systems against prompt injection attacks remains an open question. In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge. Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations. Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems. T
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20027;&#39064;&#32454;&#21270;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#21644;&#28040;&#38500;&#31163;&#39064;&#35789;&#31561;&#26041;&#24335;&#25913;&#36827;&#30701;&#25991;&#26412;&#30340;&#20027;&#39064;&#24314;&#27169;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20027;&#39064;&#30340;&#35821;&#20041;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.17706</link><description>&lt;p&gt;
&#22686;&#24378;&#30701;&#25991;&#26412;&#24314;&#27169;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20027;&#39064;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17706
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20027;&#39064;&#32454;&#21270;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#21644;&#28040;&#38500;&#31163;&#39064;&#35789;&#31561;&#26041;&#24335;&#25913;&#36827;&#30701;&#25991;&#26412;&#30340;&#20027;&#39064;&#24314;&#27169;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20027;&#39064;&#30340;&#35821;&#20041;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#26500;&#24314;&#38024;&#23545;&#31616;&#30701;&#25991;&#26412;&#65288;&#22914;&#25512;&#25991;&#21644;&#26032;&#38395;&#26631;&#39064;&#65289;&#30340;&#20027;&#39064;&#27169;&#22411;&#23545;&#25429;&#25417;&#31038;&#20250;&#21160;&#24577;&#30340;&#36805;&#36895;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#20027;&#39064;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#34920;&#36798;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#31616;&#27905;&#24615;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20027;&#39064;&#32454;&#21270;&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24182;&#38750;&#30452;&#25509;&#21442;&#19982;&#20027;&#39064;&#30340;&#21021;&#27493;&#24314;&#27169;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#25913;&#36827;&#20027;&#39064;&#22312;&#34987;&#25366;&#25496;&#21518;&#30340;&#38454;&#27573;&#12290;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25351;&#23548;LLMs&#28040;&#38500;&#32473;&#23450;&#20027;&#39064;&#20013;&#30340;&#31163;&#39064;&#35789;&#65292;&#30830;&#20445;&#20165;&#20445;&#30041;&#19982;&#35821;&#22659;&#30456;&#20851;&#30340;&#35789;&#27719;&#25110;&#29992;&#26356;&#31526;&#21512;&#35821;&#20041;&#30340;&#35789;&#27719;&#26367;&#25442;&#12290;&#36825;&#31181;&#26041;&#27861;&#27169;&#25311;&#20102;&#20154;&#31867;&#33324;&#30340;&#23457;&#26597;&#21644;&#25913;&#36827;&#20027;&#39064;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#21508;&#31181;&#20027;&#39064;&#29983;&#25104;&#30340;&#35821;&#20041;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17706v1 Announce Type: cross  Abstract: Crafting effective topic models for brief texts, like tweets and news headlines, is essential for capturing the swift shifts in social dynamics. Traditional topic models, however, often fall short in accurately representing the semantic intricacies of short texts due to their brevity and lack of contextual data. In our study, we harness the advanced capabilities of Large Language Models (LLMs) to introduce a novel approach termed "Topic Refinement". This approach does not directly involve itself in the initial modeling of topics but focuses on improving topics after they have been mined. By employing prompt engineering, we direct LLMs to eliminate off-topic words within a given topic, ensuring that only contextually relevant words are preserved or substituted with ones that fit better semantically. This method emulates human-like scrutiny and improvement of topics, thereby elevating the semantic quality of the topics generated by vario
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MEP&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#26680;&#20989;&#25968;&#29983;&#25104;&#20559;&#24046;&#26469;&#35299;&#20915;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#38271;&#24230;&#22806;&#25512;&#26102;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17698</link><description>&lt;p&gt;
MEP: &#22810;&#26680;&#23398;&#20064;&#22686;&#24378;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#38271;&#24230;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17698
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MEP&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#26680;&#20989;&#25968;&#29983;&#25104;&#20559;&#24046;&#26469;&#35299;&#20915;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#38271;&#24230;&#22806;&#25512;&#26102;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#39044;&#27979;&#30340;&#24207;&#21015;&#38271;&#24230;&#36229;&#36807;&#35757;&#32451;&#20013;&#30475;&#21040;&#30340;&#38271;&#24230;&#26102;&#65292;&#21464;&#21387;&#22120;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#20250;&#38477;&#20302;&#12290;&#29616;&#26377;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;ALiBi&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#20165;&#36890;&#36807;&#23454;&#29616;&#21333;&#20010;&#26680;&#20989;&#25968;&#26469;&#35299;&#20915;&#38271;&#24230;&#22806;&#25512;&#25361;&#25112;&#65292;&#36825;&#20250;&#26681;&#25454;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#20026;&#27599;&#20010;&#21518;Softmax&#27880;&#24847;&#21147;&#20998;&#25968;&#24341;&#20837;&#24658;&#23450;&#20559;&#24046;&#12290;&#36825;&#20123;&#26041;&#27861;&#26410;&#25506;&#35752;&#25110;&#20351;&#29992;&#22810;&#20010;&#26680;&#20989;&#25968;&#26469;&#24212;&#23545;&#22806;&#25512;&#25361;&#25112;&#12290;&#20511;&#37492;ALiBi&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;MEP&#65292;&#23427;&#37319;&#29992;&#21152;&#26435;&#24179;&#22343;&#26469;&#32467;&#21512;&#19981;&#21516;&#30340;&#26680;&#20989;&#25968;&#65288;&#22914;&#25351;&#25968;&#26680;&#21644;&#39640;&#26031;&#26680;&#65289;&#20135;&#29983;&#19968;&#20010;&#24212;&#29992;&#20110;&#21518;Softmax&#27880;&#24847;&#21147;&#20998;&#25968;&#30340;&#20559;&#24046;&#12290;&#26368;&#21021;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21508;&#31181;&#26680;&#20989;&#25968;&#26500;&#24314;&#22810;&#20010;&#26680;&#20989;&#25968;&#12290;&#27599;&#20010;&#26680;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17698v1 Announce Type: cross  Abstract: When the predicted sequence length exceeds the length seen during training, the transformer's inference accuracy diminishes. Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance. These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge. Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores. Initially, the framework utilizes various kernel functions to construct multiple kernel functions. Each kern
&lt;/p&gt;</description></item><item><title>ExpressEdit&#31995;&#32479;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#33609;&#22270;&#25903;&#25345;&#35270;&#39057;&#32534;&#36753;&#20154;&#21592;&#34920;&#36798;&#35270;&#39057;&#32534;&#36753;&#24819;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.17693</link><description>&lt;p&gt;
ExpressEdit&#65306;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#33609;&#22270;&#36827;&#34892;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
ExpressEdit: Video Editing with Natural Language and Sketching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17693
&lt;/p&gt;
&lt;p&gt;
ExpressEdit&#31995;&#32479;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#33609;&#22270;&#25903;&#25345;&#35270;&#39057;&#32534;&#36753;&#20154;&#21592;&#34920;&#36798;&#35270;&#39057;&#32534;&#36753;&#24819;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#35270;&#39057;&#23545;&#20110;&#21521;&#26032;&#25163;&#21644;&#19987;&#23478;&#35299;&#37322;&#27010;&#24565;&#21644;&#31243;&#24207;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21046;&#20316;&#20449;&#24687;&#35270;&#39057;&#26102;&#65292;&#32534;&#36753;&#20154;&#21592;&#36890;&#36807;&#21472;&#21152;&#25991;&#26412;/&#22270;&#20687;&#25110;&#20462;&#21098;&#38236;&#22836;&#26469;&#22686;&#24378;&#35270;&#39057;&#36136;&#37327;&#24182;&#22686;&#21152;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#35270;&#39057;&#32534;&#36753;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#19988;&#32791;&#26102;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#32463;&#24120;&#22312;&#34920;&#36798;&#21644;&#23454;&#29616;&#32534;&#36753;&#24819;&#27861;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#30340;&#26032;&#25163;&#35270;&#39057;&#32534;&#36753;&#20154;&#21592;&#32780;&#35328;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22810;&#27169;&#24577;&#8212;&#8212;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#21644;&#33609;&#22270;&#65292;&#36825;&#26159;&#20154;&#31867;&#29992;&#20110;&#34920;&#36798;&#30340;&#33258;&#28982;&#27169;&#24577;&#8212;&#8212;&#26469;&#25903;&#25345;&#35270;&#39057;&#32534;&#36753;&#20154;&#21592;&#34920;&#36798;&#35270;&#39057;&#32534;&#36753;&#24819;&#27861;&#12290;&#25105;&#20204;&#20174;10&#21517;&#35270;&#39057;&#32534;&#36753;&#20154;&#21592;&#37027;&#37324;&#25910;&#38598;&#20102;176&#31181;&#32534;&#36753;&#21629;&#20196;&#30340;&#22810;&#27169;&#24577;&#34920;&#36798;&#65292;&#25581;&#31034;&#20102;&#22312;&#25551;&#36848;&#32534;&#36753;&#24847;&#22270;&#26102;&#20351;&#29992;NL&#21644;&#33609;&#22270;&#30340;&#27169;&#24335;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ExpressEdit&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;NL&#23454;&#29616;&#35270;&#39057;&#32534;&#36753;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17693v1 Announce Type: cross  Abstract: Informational videos serve as a crucial source for explaining conceptual and procedural knowledge to novices and experts alike. When producing informational videos, editors edit videos by overlaying text/images or trimming footage to enhance the video quality and make it more engaging. However, video editing can be difficult and time-consuming, especially for novice video editors who often struggle with expressing and implementing their editing ideas. To address this challenge, we first explored how multimodality$-$natural language (NL) and sketching, which are natural modalities humans use for expression$-$can be utilized to support video editors in expressing video editing ideas. We gathered 176 multimodal expressions of editing commands from 10 video editors, which revealed the patterns of use of NL and sketching in describing edit intents. Based on the findings, we present ExpressEdit, a system that enables editing videos via NL te
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ECSP&#30340;&#21333;&#22810;&#27169;&#24577;&#24773;&#24863;&#25991;&#21270;&#29305;&#23450;&#25552;&#31034;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#28040;&#24687;&#22686;&#24378;&#22810;&#27169;&#24577;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#26469;&#20943;&#23569;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17683</link><description>&lt;p&gt;
&#35299;&#20915;&#24773;&#24863;&#19982;&#25991;&#21270;&#26234;&#33021;&#20154;&#24037;&#26234;&#33021;&#30740;&#35752;&#20250;&#24773;&#24863;&#39044;&#27979;&#31454;&#36187;&#30340;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17683
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ECSP&#30340;&#21333;&#22810;&#27169;&#24577;&#24773;&#24863;&#25991;&#21270;&#29305;&#23450;&#25552;&#31034;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#28040;&#24687;&#22686;&#24378;&#22810;&#27169;&#24577;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#26469;&#20943;&#23569;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25253;&#21578;&#25552;&#20379;&#20102;&#25105;&#20204;&#22312;WECIA&#24773;&#24863;&#39044;&#27979;&#31454;&#36187;&#65288;EPC&#65289;&#20013;&#25506;&#32034;&#21644;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#24102;&#26377;&#35780;&#35770;&#30340;&#33402;&#26415;&#20316;&#21697;&#39044;&#27979;&#19968;&#20010;&#20154;&#30340;&#24773;&#24863;&#12290;&#35813;&#31454;&#36187;&#30340;&#25968;&#25454;&#38598;&#26159;ArtELingo&#65292;&#26088;&#22312;&#40723;&#21169;&#36328;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#22810;&#26679;&#21270;&#24037;&#20316;&#12290;&#35813;&#25968;&#25454;&#38598;&#20027;&#35201;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65292;&#21363;&#27169;&#24577;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#35821;&#35328;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21333;&#22810;&#27169;&#24577;&#24773;&#24863;&#25991;&#21270;&#29305;&#23450;&#25552;&#31034;&#65288;ECSP&#65289;&#65292;&#20854;&#37325;&#28857;&#26159;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#28040;&#24687;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#26469;&#20943;&#23569;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;&#28548;&#28165;&#19968;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#22359;&#65306;&#65288;1&#65289;&#22522;&#20110;XLM-R&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#21644;&#22522;&#20110;X$^2$-VLM&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;2&#65289;&#24773;&#24863;&#25991;&#21270;&#29305;&#23450;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17683v1 Announce Type: new  Abstract: This report provide a detailed description of the method that we explored and proposed in the WECIA Emotion Prediction Competition (EPC), which predicts a person's emotion through an artistic work with a comment. The dataset of this competition is ArtELingo, designed to encourage work on diversity across languages and cultures. The dataset has two main challenges, namely modal imbalance problem and language-cultural differences problem. In order to address this issue, we propose a simple yet effective approach called single-multi modal with Emotion-Cultural specific prompt(ECSP), which focuses on using the single modal message to enhance the performance of multimodal models and a well-designed prompt to reduce cultural differences problem. To clarify, our approach contains two main blocks: (1)XLM-R\cite{conneau2019unsupervised} based unimodal model and X$^2$-VLM\cite{zeng2022x} based multimodal model (2) Emotion-Cultural specific prompt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33322;&#22825;&#22120;&#19978;&#21387;&#32553;&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#36882;&#24402;&#30340;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;LineRWKV&#65292;&#32467;&#21512;&#20102;Transformer&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.17677</link><description>&lt;p&gt;
&#22312;&#33322;&#22825;&#22120;&#19978;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#28145;&#24230;&#26080;&#25439;&#21644;&#25509;&#36817;&#26080;&#25439;&#39044;&#27979;&#32534;&#30721;&#19982;&#22522;&#20110;&#32447;&#27573;&#30340;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Onboard deep lossless and near-lossless predictive coding of hyperspectral images with line-based attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33322;&#22825;&#22120;&#19978;&#21387;&#32553;&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#36882;&#24402;&#30340;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;LineRWKV&#65292;&#32467;&#21512;&#20102;Transformer&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#33322;&#22825;&#22120;&#19978;&#23545;&#39640;&#20809;&#35889;&#22270;&#20687;&#36827;&#34892;&#21387;&#32553;&#20256;&#32479;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#26469;&#23454;&#29616;&#36275;&#22815;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#20197;&#21450;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25670;&#33073;&#20102;&#20256;&#32479;&#30340;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;LineRWKV&#30340;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36880;&#34892;&#36882;&#24402;&#24037;&#20316;&#20197;&#38480;&#21046;&#20869;&#23384;&#28040;&#32791;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#23558;Transformer&#30340;&#34920;&#31034;&#20248;&#21183;&#19982;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#21644;&#36882;&#24402;&#23454;&#29616;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#28151;&#21512;&#27880;&#24847;&#21147;-&#36882;&#24402;&#25805;&#20316;&#12290;&#21387;&#32553;&#31639;&#27861;&#20351;&#29992;LineRWKV&#23545;&#27599;&#20010;&#20687;&#32032;&#36827;&#34892;&#39044;&#27979;&#65292;&#28982;&#21518;&#23545;&#27531;&#24046;&#36827;&#34892;&#29109;&#32534;&#30721;&#12290;&#22312;HySpecNet-11k&#25968;&#25454;&#38598;&#21644;PRISMA&#22270;&#20687;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LineRWKV&#26159;&#31532;&#19968;&#20010;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17677v1 Announce Type: cross  Abstract: Deep learning methods have traditionally been difficult to apply to compression of hyperspectral images onboard of spacecrafts, due to the large computational complexity needed to achieve adequate representational power, as well as the lack of suitable datasets for training and testing. In this paper, we depart from the traditional autoencoder approach and we design a predictive neural network, called LineRWKV, that works recursively line-by-line to limit memory consumption. In order to achieve that, we adopt a novel hybrid attentive-recursive operation that combines the representational advantages of Transformers with the linear complexity and recursive implementation of recurrent neural networks. The compression algorithm performs prediction of each pixel using LineRWKV, followed by entropy coding of the residual. Experiments on the HySpecNet-11k dataset and PRISMA images show that LineRWKV is the first deep-learning method to outper
&lt;/p&gt;</description></item><item><title>&#23558;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25351;&#23548;&#30340;LLM&#20195;&#29702;SecurityBot&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#65292;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.17674</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#23433;&#20840;&#28216;&#25103;&#20013;&#20381;&#38752;&#33258;&#24049;&#30340;&#26102;&#20505;&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25351;&#23548;LLM&#25104;&#20026;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17674
&lt;/p&gt;
&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25351;&#23548;&#30340;LLM&#20195;&#29702;SecurityBot&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#65292;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;LLM&#65288;&#29983;&#21629;&#21608;&#26399;&#38271;&#12289;&#35760;&#24518;&#25345;&#32493;&#12289;&#33258;&#25105;&#35780;&#20272;&#30340;&#65289;&#19982;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#26377;&#25928;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#32593;&#32476;&#23433;&#20840;&#31561;&#39640;&#39118;&#38505;&#20219;&#21153;&#20013;&#30340;&#21327;&#21516;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SecurityBot&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#39044;&#35757;&#32451;RL&#20195;&#29702;&#25351;&#23548;&#30340;LLM&#20195;&#29702;&#65292;&#29992;&#20110;&#25903;&#25345;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#12290;&#35813;LLM&#20195;&#29702;&#20855;&#26377;&#27010;&#35201;&#27169;&#22359;&#20197;&#29983;&#25104;&#34892;&#20026;&#25351;&#21335;&#12289;&#35760;&#24518;&#27169;&#22359;&#20197;&#32047;&#31215;&#26412;&#22320;&#32463;&#39564;&#12289;&#21453;&#24605;&#27169;&#22359;&#20197;&#37325;&#26032;&#35780;&#20272;&#36873;&#25321;&#65292;&#20197;&#21450;&#34892;&#21160;&#27169;&#22359;&#20197;&#20943;&#23569;&#34892;&#21160;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#23427;&#37319;&#29992;&#21327;&#20316;&#26426;&#21046;&#20174;&#39044;&#35757;&#32451;RL&#20195;&#29702;&#33719;&#24471;&#24314;&#35758;&#65292;&#21253;&#25324;&#29992;&#20110;&#21160;&#24577;&#24314;&#35758;&#30340;&#20809;&#26631;&#12289;&#29992;&#20110;&#22810;&#20010;&#23548;&#24072;&#24314;&#35758;&#25490;&#21517;&#30340;&#32858;&#21512;&#22120;&#21644;&#29992;&#20110;&#20027;&#21160;&#24314;&#35758;&#35831;&#27714;&#30340;&#21628;&#21483;&#22120;&#12290;&#22522;&#20110;CybORG&#23454;&#39564;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#34920;&#26126;&#65292;&#19982;&#29420;&#31435;&#30340;LLM&#25110;RL&#30456;&#27604;&#65292;SecurityBot&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17674v1 Announce Type: cross  Abstract: Integrating LLM and reinforcement learning (RL) agent effectively to achieve complementary performance is critical in high stake tasks like cybersecurity operations. In this study, we introduce SecurityBot, a LLM agent mentored by pre-trained RL agents, to support cybersecurity operations. In particularly, the LLM agent is supported with a profile module to generated behavior guidelines, a memory module to accumulate local experiences, a reflection module to re-evaluate choices, and an action module to reduce action space. Additionally, it adopts the collaboration mechanism to take suggestions from pre-trained RL agents, including a cursor for dynamic suggestion taken, an aggregator for multiple mentors' suggestions ranking and a caller for proactive suggestion asking. Building on the CybORG experiment framework, our experiences show that SecurityBot demonstrates significant performance improvement compared with LLM or RL standalone, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20043;&#38388;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.17661</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#20165;&#20165;&#19978;&#19979;&#25991;&#23398;&#20064;&#23601;&#36275;&#22815;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Language Models for Text Classification: Is In-Context Learning Enough?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20043;&#38388;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#27425;&#21644;&#23569;&#27425;&#26631;&#35760;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#30456;&#23545;&#20110;&#22522;&#20110;&#24494;&#35843;&#30340;&#26356;&#26631;&#20934;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#29702;&#35299;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#20889;&#30340;&#25351;&#20196;&#65288;&#25552;&#31034;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23427;&#20204;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20351;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#27880;&#23454;&#20363;&#25968;&#37327;&#30340;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30740;&#31350;&#22312;&#35268;&#27169;&#19978;&#26377;&#38480;&#65292;&#24182;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#30456;&#32467;&#21512;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65288;&#22914;&#24494;&#35843;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#28085;&#30422;&#20108;&#20803;&#12289;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#38382;&#39064;&#30340;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17661v1 Announce Type: cross  Abstract: Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In par
&lt;/p&gt;</description></item><item><title>SGHormer&#26159;&#19968;&#31181;&#30001;&#33033;&#20914;&#39537;&#21160;&#30340;&#33410;&#33021;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#23558;&#20840;&#31934;&#24230;&#23884;&#20837;&#36716;&#25442;&#20026;&#31232;&#30095;&#21644;&#20108;&#20540;&#21270;&#33033;&#20914;&#20197;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#22270;&#21464;&#25442;&#22120;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17656</link><description>&lt;p&gt;
SGHormer&#65306;&#19968;&#31181;&#30001;&#33033;&#20914;&#39537;&#21160;&#30340;&#33410;&#33021;&#22270;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
SGHormer: An Energy-Saving Graph Transformer Driven by Spikes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17656
&lt;/p&gt;
&lt;p&gt;
SGHormer&#26159;&#19968;&#31181;&#30001;&#33033;&#20914;&#39537;&#21160;&#30340;&#33410;&#33021;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#23558;&#20840;&#31934;&#24230;&#23884;&#20837;&#36716;&#25442;&#20026;&#31232;&#30095;&#21644;&#20108;&#20540;&#21270;&#33033;&#20914;&#20197;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#22270;&#21464;&#25442;&#22120;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#22270;&#21464;&#25442;&#22120;&#65288;GTs&#65289;&#22312;&#21508;&#31181;&#22270;&#20219;&#21153;&#20013;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;GTs&#20986;&#33394;&#24615;&#33021;&#32972;&#21518;&#30340;&#20195;&#20215;&#26159;&#26356;&#39640;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#20256;&#32479;&#21464;&#25442;&#22120;&#20013;&#27880;&#24847;&#21147;&#35745;&#31639;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#20108;&#27425;&#22797;&#26434;&#24230;&#20005;&#37325;&#24433;&#21709;&#20854;&#22312;&#22823;&#35268;&#27169;&#22270;&#25968;&#25454;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#22312;&#31616;&#21270;&#22359;&#20043;&#38388;&#30340;&#32452;&#21512;&#25110;&#27880;&#24847;&#21147;&#23398;&#20064;&#33539;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#20197;&#25552;&#39640;GTs&#30340;&#25928;&#29575;&#65292;&#20294;&#22312;&#26500;&#24314;GT&#26694;&#26550;&#26102;&#24456;&#23569;&#32771;&#34385;&#28304;&#33258;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#32467;&#26500;&#30340;&#19968;&#31995;&#21015;&#33410;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#22270;&#21464;&#25442;&#22120;&#65288;SGHormer&#65289;&#12290;&#23427;&#23558;&#20840;&#31934;&#24230;&#23884;&#20837;&#36716;&#25442;&#20026;&#31232;&#30095;&#21644;&#20108;&#20540;&#21270;&#33033;&#20914;&#20197;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;SGHormer&#20013;&#30340;&#33033;&#20914;&#22270;&#33258;&#27880;&#24847;&#21147;&#21644;&#33033;&#20914;&#20462;&#27491;&#22359;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17656v1 Announce Type: cross  Abstract: Graph Transformers (GTs) with powerful representation learning ability make a huge success in wide range of graph tasks. However, the costs behind outstanding performances of GTs are higher energy consumption and computational overhead. The complex structure and quadratic complexity during attention calculation in vanilla transformer seriously hinder its scalability on the large-scale graph data. Though existing methods have made strides in simplifying combinations among blocks or attention-learning paradigm to improve GTs' efficiency, a series of energy-saving solutions originated from biologically plausible structures are rarely taken into consideration when constructing GT framework. To this end, we propose a new spiking-based graph transformer (SGHormer). It turns full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs. The spiking graph self-attention and spiking rectify blocks in SGHorm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#21644;&#39564;&#35777;&#25277;&#35937;&#35770;&#35777;&#31995;&#32479;&#20013;&#30340;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.17653</link><description>&lt;p&gt;
&#35745;&#31639;&#21644;&#39564;&#35777;&#25277;&#35937;&#35770;&#35777;&#20013;&#20559;&#22909;&#30340;&#22522;&#20110;&#25193;&#23637;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Extension-based Approach for Computing and Verifying Preferences in Abstract Argumentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17653
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#21644;&#39564;&#35777;&#25277;&#35937;&#35770;&#35777;&#31995;&#32479;&#20013;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#21644;&#39564;&#35777;&#25277;&#35937;&#35770;&#35777;&#31995;&#32479;&#20013;&#30340;&#20559;&#22909;&#12290;&#34429;&#28982;&#20808;&#21069;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#35770;&#35777;&#35821;&#20041;&#29992;&#20110;&#20174;&#35770;&#35777;&#26694;&#26550;&#20013;&#35782;&#21035;&#21487;&#25509;&#21463;&#30340;&#35770;&#28857;&#38598;&#65292;&#20294;&#32570;&#20047;&#22522;&#20110;&#38544;&#21547;&#35770;&#28857;&#20559;&#22909;&#30340;&#21487;&#25509;&#21463;&#24615;&#32972;&#21518;&#30340;&#29702;&#30001;&#12290;&#22522;&#20110;&#20559;&#22909;&#30340;&#35770;&#35777;&#26694;&#26550;&#21487;&#20197;&#30830;&#23450;&#22312;&#32473;&#23450;&#19968;&#32452;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#21738;&#20123;&#35770;&#28857;&#26159;&#34987;&#35777;&#26126;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32771;&#34385;&#20102;&#26631;&#20934;&#25512;&#29702;&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#65292;&#21363;&#65292;&#32473;&#23450;&#19968;&#20010;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#21644;&#19968;&#32452;&#34987;&#35777;&#26126;&#30340;&#35770;&#28857;&#65292;&#25105;&#20204;&#35745;&#31639;&#21487;&#33021;&#30340;&#35770;&#28857;&#20559;&#22909;&#26159;&#20160;&#20040;&#12290;&#27492;&#22806;&#65292;&#26377;&#24517;&#35201;&#39564;&#35777;&#65288;&#21363;&#35780;&#20272;&#65289;&#35745;&#31639;&#20986;&#30340;&#20559;&#22909;&#26159;&#21542;&#20250;&#23548;&#33268;&#21487;&#25509;&#21463;&#30340;&#35770;&#28857;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#31351;&#20030;&#35745;&#31639;&#21644;&#21015;&#20030;&#25152;&#26377;&#21487;&#33021;&#30340;&#35770;&#28857;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17653v1 Announce Type: new  Abstract: We present an extension-based approach for computing and verifying preferences in an abstract argumentation system. Although numerous argumentation semantics have been developed previously for identifying acceptable sets of arguments from an argumentation framework, there is a lack of justification behind their acceptability based on implicit argument preferences. Preference-based argumentation frameworks allow one to determine what arguments are justified given a set of preferences. Our research considers the inverse of the standard reasoning problem, i.e., given an abstract argumentation framework and a set of justified arguments, we compute what the possible preferences over arguments are. Furthermore, there is a need to verify (i.e., assess) that the computed preferences would lead to the acceptable sets of arguments. This paper presents a novel approach and algorithm for exhaustively computing and enumerating all possible sets of pr
&lt;/p&gt;</description></item><item><title>S+t-SNE&#26159;t-SNE&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#20855;&#26377;&#22686;&#37327;&#26356;&#26032;&#21644;&#30450;&#30446;&#28418;&#31227;&#31649;&#29702;&#30340;&#29305;&#28857;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38477;&#32500;&#21644;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.17643</link><description>&lt;p&gt;
S+t-SNE - &#23558;&#38477;&#32500;&#24341;&#20837;&#25968;&#25454;&#27969;
&lt;/p&gt;
&lt;p&gt;
S+t-SNE - Bringing dimensionality reduction to data streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17643
&lt;/p&gt;
&lt;p&gt;
S+t-SNE&#26159;t-SNE&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#20855;&#26377;&#22686;&#37327;&#26356;&#26032;&#21644;&#30450;&#30446;&#28418;&#31227;&#31649;&#29702;&#30340;&#29305;&#28857;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38477;&#32500;&#21644;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;S+t-SNE&#65292;&#36825;&#26159;t-SNE&#31639;&#27861;&#30340;&#19968;&#31181;&#25913;&#36827;&#65292;&#26088;&#22312;&#22788;&#29702;&#26080;&#38480;&#25968;&#25454;&#27969;&#12290;S+t-SNE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#38543;&#30528;&#26032;&#25968;&#25454;&#30340;&#21040;&#26469;&#36880;&#27493;&#26356;&#26032;t-SNE&#23884;&#20837;&#65292;&#30830;&#20445;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20197;&#22788;&#29702;&#27969;&#24335;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#27599;&#19968;&#27493;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#28857;&#65292;&#35813;&#31639;&#27861;&#30830;&#20445;&#21487;&#25193;&#23637;&#24615;&#21516;&#26102;&#20445;&#25345;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;&#37319;&#29992;&#30450;&#30446;&#26041;&#27861;&#36827;&#34892;&#28418;&#31227;&#31649;&#29702;&#35843;&#25972;&#23884;&#20837;&#31354;&#38388;&#65292;&#20419;&#36827;&#19981;&#26029;&#21487;&#35270;&#21270;&#19981;&#26029;&#21457;&#23637;&#30340;&#25968;&#25454;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;S+t-SNE&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;&#20854;&#22312;&#27969;&#24335;&#22330;&#26223;&#20013;&#25429;&#25417;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#19968;&#20010;&#23454;&#26102;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17643v1 Announce Type: new  Abstract: We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle infinite data streams. The core idea behind S+t-SNE is to update the t-SNE embedding incrementally as new data arrives, ensuring scalability and adaptability to handle streaming scenarios. By selecting the most important points at each step, the algorithm ensures scalability while keeping informative visualisations. Employing a blind method for drift management adjusts the embedding space, facilitating continuous visualisation of evolving data dynamics. Our experimental evaluations demonstrate the effectiveness and efficiency of S+t-SNE. The results highlight its ability to capture patterns in a streaming scenario. We hope our approach offers researchers and practitioners a real-time tool for understanding and interpreting high-dimensional data.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.17637</link><description>&lt;p&gt;
PeersimGym&#65306;&#29992;&#20110;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17637
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21368;&#36733;&#23545;&#20110;&#22312;&#35832;&#22914;&#29289;&#32852;&#32593;&#20043;&#31867;&#30340;&#32593;&#32476;&#20013;&#24179;&#34913;&#35774;&#22791;&#30340;&#35745;&#31639;&#36127;&#36733;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38754;&#20020;&#30528;&#35832;&#22914;&#22312;&#20005;&#26684;&#30340;&#36890;&#20449;&#21644;&#23384;&#20648;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#24310;&#36831;&#21644;&#33021;&#28304;&#20351;&#29992;&#31561;&#37325;&#35201;&#20248;&#21270;&#25361;&#25112;&#12290;&#20256;&#32479;&#20248;&#21270;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65307;&#21551;&#21457;&#24335;&#26041;&#27861;&#32570;&#20047;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#20801;&#35768;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#23398;&#20064;&#26368;&#20339;&#21368;&#36733;&#31574;&#30053;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;RL &#30340;&#21151;&#25928;&#21462;&#20915;&#20110;&#23545;&#20016;&#23500;&#25968;&#25454;&#38598;&#21644;&#23450;&#21046;&#30340;&#29616;&#23454;&#35757;&#32451;&#29615;&#22659;&#30340;&#35775;&#38382;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; PeersimGym&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;PeersimGym &#25903;&#25345;&#21508;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#35745;&#31639;&#32422;&#26463;&#65292;&#24182;&#25972;&#21512;&#20102;&#19968;&#31181;"PettingZo"&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#37197;&#32622;&#20223;&#30495;&#21442;&#25968;&#21644;&#30417;&#25511;&#20223;&#30495;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17637v1 Announce Type: cross  Abstract: Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints. While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions. However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments. To address this, we introduce PeersimGym, an open-source, customizable simulation environment tailored for developing and optimizing task offloading strategies within computational networks. PeersimGym supports a wide range of network topologies and computational constraints and integrates a \textit{PettingZo
&lt;/p&gt;</description></item><item><title>UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.17633</link><description>&lt;p&gt;
UADA3D&#65306;&#38754;&#21521;&#31232;&#30095;LiDAR&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#30340;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17633
&lt;/p&gt;
&lt;p&gt;
UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#22522;&#20110;LiDAR&#30340;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36866;&#24212;&#24050;&#24314;&#31435;&#30340;&#39640;&#23494;&#24230;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#26356;&#31232;&#30095;&#30340;&#28857;&#20113;&#65292;&#25429;&#25417;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#22330;&#26223;&#65306;&#19981;&#20165;&#26469;&#33258;&#36947;&#36335;&#19978;&#30340;&#36710;&#36742;&#65292;&#36824;&#26469;&#33258;&#20154;&#34892;&#36947;&#19978;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#65292;&#36973;&#36935;&#30528;&#26126;&#26174;&#19981;&#21516;&#30340;&#29615;&#22659;&#26465;&#20214;&#21644;&#20256;&#24863;&#22120;&#37197;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;3D&#29289;&#20307;&#26816;&#27979;&#65288;UADA3D&#65289;&#12290;UADA3D&#19981;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#25110;&#24072;&#29983;&#26550;&#26500;&#12290;&#30456;&#21453;&#65292;&#23427;&#20351;&#29992;&#23545;&#25239;&#26041;&#27861;&#30452;&#25509;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#36866;&#24212;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65292;&#24456;&#24555;&#23558;&#20250;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17633v1 Announce Type: cross  Abstract: In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#24037;&#20855;&#22312;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#33021;&#32791;&#24314;&#27169;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;</title><link>https://arxiv.org/abs/2403.17632</link><description>&lt;p&gt;
&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#33021;&#32791;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#24037;&#20855;&#22312;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#33021;&#32791;&#24314;&#27169;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#25317;&#22581;&#21644;&#29615;&#22659;&#24694;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#26085;&#30410;&#21152;&#21095;&#65292;&#20984;&#26174;&#20102;&#22312;&#22478;&#24066;&#31354;&#38388;&#25512;&#34892;E-Mobility&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;E-&#28369;&#26495;&#36710;&#21644;E-&#33258;&#34892;&#36710;&#31561;&#24494;&#22411;E-Mobility&#24037;&#20855;&#22312;&#36825;&#19968;&#36716;&#21464;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20026;&#22478;&#24066;&#36890;&#21220;&#32773;&#25552;&#20379;&#21487;&#25345;&#32493;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#30340;&#33021;&#32791;&#27169;&#24335;&#26159;&#24433;&#21709;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#23545;&#20110;&#20986;&#34892;&#35268;&#21010;&#20197;&#21450;&#22686;&#24378;&#29992;&#25143;&#22312;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#26102;&#30340;&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#38024;&#23545;&#29305;&#23450;&#31227;&#21160;&#24037;&#20855;&#21644;&#26465;&#20214;&#23450;&#21046;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#24443;&#24213;&#27169;&#22411;&#35780;&#20272;&#21644;&#39564;&#35777;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29233;&#23572;&#20848;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#33021;&#32791;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17632v1 Announce Type: new  Abstract: The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for ene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Denosied Table-Text Retriever&#65288;DoTTeR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21435;&#22122;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#34920;&#32423;&#25490;&#21517;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#34920;&#26684;-&#25991;&#26412;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#23384;&#22312;&#30340;&#20551;&#27491;&#26631;&#31614;&#24433;&#21709;&#21644;&#36328;&#34920;&#26684;&#25512;&#29702;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.17611</link><description>&lt;p&gt;
&#23545;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#21435;&#22122;&#34920;&#26684;-&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Denoising Table-Text Retrieval for Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Denosied Table-Text Retriever&#65288;DoTTeR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21435;&#22122;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#34920;&#32423;&#25490;&#21517;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#34920;&#26684;-&#25991;&#26412;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#23384;&#22312;&#30340;&#20551;&#27491;&#26631;&#31614;&#24433;&#21709;&#21644;&#36328;&#34920;&#26684;&#25512;&#29702;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#26684;-&#25991;&#26412;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#65292;&#26816;&#32034;&#31995;&#32479;&#20174;&#34920;&#26684;&#21644;&#25991;&#26412;&#20013;&#26816;&#32034;&#30456;&#20851;&#35777;&#25454;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;&#20043;&#21069;&#22312;&#34920;&#26684;-&#25991;&#26412;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#30740;&#31350;&#23384;&#22312;&#20004;&#20010;&#24120;&#35265;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#30340;&#26816;&#32034;&#22120;&#21487;&#33021;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20551;&#27491;&#26631;&#31614;&#24433;&#21709;&#65307;&#20854;&#27425;&#65292;&#23427;&#20204;&#21487;&#33021;&#38590;&#20197;&#20026;&#38656;&#35201;&#36328;&#34920;&#26684;&#25512;&#29702;&#30340;&#38382;&#39064;&#25552;&#20379;&#36866;&#24403;&#30340;&#35777;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#34920;&#26684;-&#25991;&#26412;&#26816;&#32034;&#22120;&#65288;DoTTeR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#21033;&#29992;&#19968;&#20010;&#21435;&#22122;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#33293;&#24323;&#36890;&#36807;&#20551;&#27491;&#26816;&#27979;&#27169;&#22411;&#27979;&#37327;&#30340;&#36739;&#20302;&#38382;&#39064;&#30456;&#20851;&#24615;&#24471;&#20998;&#30340;&#31034;&#20363;&#26469;&#20943;&#23569;&#20551;&#27491;&#26631;&#31614;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#34920;&#32423;&#25490;&#21517;&#20449;&#24687;&#25972;&#21512;&#21040;&#26816;&#32034;&#22120;&#20013;&#65292;&#20197;&#24110;&#21161;&#25214;&#21040;&#38656;&#35201;&#36328;&#34920;&#26684;&#25512;&#29702;&#30340;&#38382;&#39064;&#30340;&#35777;&#25454;&#12290;&#20026;&#20102;&#32534;&#30721;&#27492;&#25490;&#21517;&#20449;&#24687;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#25490;&#21517;&#24863;&#30693;&#30340;&#21015;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17611v1 Announce Type: cross  Abstract: In table-text open-domain question answering, a retriever system retrieves relevant evidence from tables and text to answer questions. Previous studies in table-text open-domain question answering have two common challenges: firstly, their retrievers can be affected by false-positive labels in training datasets; secondly, they may struggle to provide appropriate evidence for questions that require reasoning across the table. To address these issues, we propose Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a denoised training dataset with fewer false positive labels by discarding instances with lower question-relevance scores measured through a false positive detection model. Subsequently, we integrate table-level ranking information into the retriever to assist in finding evidence for questions that demand reasoning across the table. To encode this ranking information, we fine-tune a rank-aware column encoder 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20840;&#34701;&#21512;&#25805;&#20316;&#12289;&#26368;&#23567;&#21270;&#20840;&#23616;&#20869;&#23384;&#35775;&#38382;&#20197;&#21450;&#26368;&#22823;&#21270;&#25968;&#25454;&#37325;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#33521;&#29305;&#23572;&#25968;&#25454;&#20013;&#24515;GPU&#19978;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#23454;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.17607</link><description>&lt;p&gt;
&#22312;&#33521;&#29305;&#23572;&#25968;&#25454;&#20013;&#24515;GPU&#19978;&#20840;&#34701;&#21512;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;
&lt;/p&gt;
&lt;p&gt;
Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17607
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20840;&#34701;&#21512;&#25805;&#20316;&#12289;&#26368;&#23567;&#21270;&#20840;&#23616;&#20869;&#23384;&#35775;&#38382;&#20197;&#21450;&#26368;&#22823;&#21270;&#25968;&#25454;&#37325;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#33521;&#29305;&#23572;&#25968;&#25454;&#20013;&#24515;GPU&#19978;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#23454;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33521;&#29305;&#23572;&#25968;&#25454;&#20013;&#24515;GPU Max 1550&#36827;&#34892;&#20248;&#21270;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLPs&#65289;&#30340;SYCL&#23454;&#29616;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#23454;&#29616;&#36890;&#36807;&#34701;&#21512;MLP&#27599;&#19968;&#23618;&#20013;&#30340;&#25805;&#20316;&#65292;&#26368;&#23567;&#21270;&#20102;&#24930;&#36895;&#20840;&#23616;&#20869;&#23384;&#35775;&#38382;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20102;&#26222;&#36890;&#23492;&#23384;&#22120;&#25991;&#20214;&#21644;&#20849;&#20139;&#26412;&#22320;&#20869;&#23384;&#20013;&#30340;&#25968;&#25454;&#37325;&#29992;&#12290;&#25105;&#20204;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;roofline&#27169;&#22411;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#31639;&#26415;&#24378;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#25512;&#29702;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#31867;&#20284;&#30340;&#22522;&#20110;CUDA&#30340;MLP&#23454;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#22312;&#33521;&#29305;&#23572;&#25968;&#25454;&#20013;&#24515;GPU&#19978;&#23545;Nvidia&#30340;H100 GPU&#22522;&#20110;CUDA&#30340;&#23454;&#29616;&#22312;&#25512;&#29702;&#19978;&#30340;&#24615;&#33021;&#20248;&#21183;&#39640;&#36798;2.84&#20493;&#65292;&#22312;&#35757;&#32451;&#19978;&#39640;&#36798;1.75&#20493;&#12290;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;SYCL&#23454;&#29616;&#22312;&#22270;&#20687;&#21387;&#32553;&#12289;&#31070;&#32463;&#20809;&#36752;&#23556;&#22330;&#21644;&#29289;&#29702;&#20449;&#24687;&#20013;&#30340;&#19977;&#20010;&#37325;&#35201;&#39046;&#22495;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17607v1 Announce Type: new  Abstract: This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550. To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP. We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference. We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidia's H100 GPU by a factor up to 2.84 in inference and 1.75 in training. The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17601</link><description>&lt;p&gt;
LASIL&#65306;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#38271;&#26399;&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17601
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#22312;&#20132;&#36890;&#24037;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#21333;&#20010;&#36710;&#36742;&#34892;&#20026;&#21644;&#25972;&#20307;&#20132;&#36890;&#27969;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#19968;&#20010;&#30495;&#23454;&#30340;&#27169;&#25311;&#22120;&#65292;&#31934;&#30830;&#22797;&#21046;&#21508;&#31181;&#20132;&#36890;&#26465;&#20214;&#19979;&#30340;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#20381;&#36182;&#21551;&#21457;&#24335;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#24448;&#24448;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20132;&#36890;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#32780;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#30340;&#27169;&#25311;&#12290;&#30001;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#31283;&#23450;&#30340;&#38271;&#26399;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#20998;&#24067;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#19987;&#23478;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#22686;&#24378;&#29366;&#24577;&#24847;&#35782;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17601v1 Announce Type: new  Abstract: Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#23384;&#20648;&#32593;&#32476;&#30340;&#22810;&#21151;&#33021;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22312;&#38646;&#27425;&#36866;&#24212;&#12289;&#23569;&#27425;&#36866;&#24212;&#21644;&#26080;&#38656;&#35757;&#32451;&#30340;&#23569;&#27425;&#36866;&#24212;&#19977;&#31181;&#35774;&#32622;&#19979;&#39640;&#25928;&#36816;&#34892;</title><link>https://arxiv.org/abs/2403.17589</link><description>&lt;p&gt;
&#21452;&#23384;&#20648;&#32593;&#32476;&#65306;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#23384;&#20648;&#32593;&#32476;&#30340;&#22810;&#21151;&#33021;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22312;&#38646;&#27425;&#36866;&#24212;&#12289;&#23569;&#27425;&#36866;&#24212;&#21644;&#26080;&#38656;&#35757;&#32451;&#30340;&#23569;&#27425;&#36866;&#24212;&#19977;&#31181;&#35774;&#32622;&#19979;&#39640;&#25928;&#36816;&#34892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20687;CLIP&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;&#20309;&#23558;&#23427;&#20204;&#35843;&#25972;&#21040;&#21508;&#31181;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#24050;&#32463;&#24341;&#36215;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#37325;&#35270;&#12290;&#35813;&#36866;&#24212;&#31574;&#30053;&#36890;&#24120;&#21487;&#20197;&#24402;&#31867;&#20026;&#19977;&#31181;&#33539;&#24335;&#65306;&#38646;&#27425;&#36866;&#24212;&#12289;&#23569;&#27425;&#36866;&#24212;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#23569;&#27425;&#36866;&#24212;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#38024;&#23545;&#29305;&#23450;&#35774;&#32622;&#37327;&#36523;&#23450;&#21046;&#30340;&#65292;&#21482;&#33021;&#28385;&#36275;&#20854;&#20013;&#19968;&#31181;&#25110;&#20004;&#31181;&#33539;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#36825;&#19977;&#31181;&#35774;&#32622;&#19979;&#36816;&#34892;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#23384;&#20648;&#32593;&#32476;&#65292;&#21253;&#25324;&#21160;&#24577;&#21644;&#38745;&#24577;&#35760;&#24518;&#32452;&#20214;&#12290;&#38745;&#24577;&#35760;&#24518;&#32531;&#23384;&#35757;&#32451;&#25968;&#25454;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#23569;&#27425;&#36866;&#24212;&#65292;&#32780;&#21160;&#24577;&#35760;&#24518;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#22312;&#32447;&#20445;&#23384;&#21382;&#21490;&#27979;&#35797;&#29305;&#24449;&#65292;&#20801;&#35768;&#25506;&#32034;&#36229;&#20986;&#35770;&#25991;&#20013;&#24050;&#35757;&#32451;&#25968;&#25454;&#30340;&#39069;&#22806;&#25968;&#25454;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17589v1 Announce Type: cross  Abstract: With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically, we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#8220;&#25361;&#25112;&#20896;&#20891;&#8221;&#27604;&#36187;&#20013;&#36159;&#36162;&#30340;&#21442;&#25968;&#21270;&#20998;&#26512;&#38382;&#39064;&#65292;&#25506;&#35752;&#36890;&#36807;&#36159;&#36162;&#20854;&#20182;&#29609;&#23478;&#26469;&#22686;&#21152;&#26368;&#21021;&#20896;&#20891;&#36194;&#24471;&#27604;&#36187;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#38480;&#23450;&#39044;&#31639;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#20197;&#29609;&#23478;&#25968;&#37327;&#20026;&#21442;&#25968;&#26102;&#65292;&#36825;&#19968;&#38382;&#39064;&#26159;&#24369;NP&#22256;&#38590;&#19988;W[1]-hard&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.17587</link><description>&lt;p&gt;
&#22312;&#8220;&#25361;&#25112;&#20896;&#20891;&#8221;&#27604;&#36187;&#20013;&#36159;&#36162;&#30340;&#21442;&#25968;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Parameterized Analysis of Bribery in Challenge the Champ Tournaments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17587
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#8220;&#25361;&#25112;&#20896;&#20891;&#8221;&#27604;&#36187;&#20013;&#36159;&#36162;&#30340;&#21442;&#25968;&#21270;&#20998;&#26512;&#38382;&#39064;&#65292;&#25506;&#35752;&#36890;&#36807;&#36159;&#36162;&#20854;&#20182;&#29609;&#23478;&#26469;&#22686;&#21152;&#26368;&#21021;&#20896;&#20891;&#36194;&#24471;&#27604;&#36187;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#38480;&#23450;&#39044;&#31639;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#20197;&#29609;&#23478;&#25968;&#37327;&#20026;&#21442;&#25968;&#26102;&#65292;&#36825;&#19968;&#38382;&#39064;&#26159;&#24369;NP&#22256;&#38590;&#19988;W[1]-hard&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#20896;&#20891;&#27604;&#36187;&#26159;&#26368;&#31616;&#21333;&#30340;&#31454;&#20105;&#24418;&#24335;&#20043;&#19968;&#65292;&#20854;&#20013;&#19968;&#20010;&#65288;&#26368;&#21021;&#36873;&#25321;&#30340;&#65289;&#20896;&#20891;&#21453;&#22797;&#21463;&#21040;&#20854;&#20182;&#29609;&#23478;&#30340;&#25361;&#25112;&#12290;&#22914;&#26524;&#29609;&#23478;&#20987;&#36133;&#20896;&#20891;&#65292;&#21017;&#35813;&#29609;&#23478;&#34987;&#35270;&#20026;&#26032;&#30340;&#65288;&#24403;&#21069;&#30340;&#65289;&#20896;&#20891;&#12290;&#27599;&#20301;&#21442;&#36187;&#36873;&#25163;&#25353;&#29031;&#22266;&#23450;&#39034;&#24207;&#25361;&#25112;&#24403;&#21069;&#30340;&#20896;&#20891;&#19968;&#27425;&#12290;&#26368;&#21518;&#19968;&#36718;&#30340;&#20896;&#20891;&#34987;&#35270;&#20026;&#27604;&#36187;&#30340;&#33719;&#32988;&#32773;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#24773;&#24418;&#65292;&#21363;&#29609;&#23478;&#21487;&#20197;&#34987;&#36159;&#36162;&#20197;&#38477;&#20302;&#20182;&#20204;&#23545;&#26368;&#21021;&#20896;&#20891;&#30340;&#33719;&#32988;&#27010;&#29575;&#12290;&#30446;&#26631;&#26159;&#36890;&#36807;&#36159;&#36162;&#20854;&#20182;&#29609;&#23478;&#26469;&#26368;&#22823;&#21270;&#26368;&#21021;&#20896;&#20891;&#36194;&#24471;&#27604;&#36187;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#19981;&#36229;&#36807;&#35268;&#23450;&#30340;&#36159;&#36162;&#39044;&#31639;&#12290;Mattei&#31561;&#20154;[&#12298;&#24212;&#29992;&#36923;&#36753;&#26434;&#24535;&#12299;&#65292;2015&#24180;]&#34920;&#26126;&#35813;&#38382;&#39064;&#21487;&#20197;&#22312;&#20266;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#65292;&#24182;&#19988;&#22312;&#20197;&#29609;&#23478;&#25968;&#37327;&#20316;&#20026;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#38382;&#39064;&#23646;&#20110;XP&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#20197;&#29609;&#23478;&#25968;&#37327;&#20026;&#21442;&#25968;&#26102;&#65292;&#35813;&#38382;&#39064;&#26159;&#24369;NP&#22256;&#38590;&#30340;&#65292;&#24182;&#19988;&#26159;W[1]-hard&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17587v1 Announce Type: cross  Abstract: Challenge the champ tournaments are one of the simplest forms of competition, where a (initially selected) champ is repeatedly challenged by other players. If a player beats the champ, then that player is considered the new (current) champ. Each player in the competition challenges the current champ once in a fixed order. The champ of the last round is considered the winner of the tournament. We investigate a setting where players can be bribed to lower their winning probability against the initial champ. The goal is to maximize the probability of the initial champ winning the tournament by bribing the other players, while not exceeding a given budget for the bribes. Mattei et al. [Journal of Applied Logic, 2015] showed that the problem can be solved in pseudo-polynomial time, and that it is in XP when parameterized by the number of players.   We show that the problem is weakly NP-hard and W[1]-hard when parameterized by the number of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#23545;&#35805;&#26641;&#29983;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35757;&#32451;&#20986;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20195;&#29702;&#36798;&#21040;&#19982;&#22312;&#20154;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17582</link><description>&lt;p&gt;
&#26397;&#30528;&#38646;&#25968;&#25454;&#12289;&#21487;&#25511;&#12289;&#33258;&#36866;&#24212;&#23545;&#35805;&#31995;&#32479;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards a Zero-Data, Controllable, Adaptive Dialog System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#23545;&#35805;&#26641;&#29983;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35757;&#32451;&#20986;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20195;&#29702;&#36798;&#21040;&#19982;&#22312;&#20154;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#26641;&#25628;&#32034;&#65288;V&#228;th&#31561;&#65292;2023&#24180;&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#23545;&#35805;&#31995;&#32479;&#25511;&#21046;&#26041;&#27861;&#65292;&#20854;&#20013;&#39046;&#22495;&#19987;&#23478;&#36890;&#36807;&#23545;&#35805;&#26641;&#22609;&#36896;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#20195;&#29702;&#23398;&#20250;&#26377;&#25928;&#22320;&#27983;&#35272;&#36825;&#26869;&#26641;&#65292;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#20363;&#22914;&#39046;&#22495;&#29087;&#24713;&#24230;&#12290;&#28982;&#32780;&#65292;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#38459;&#30861;&#20102;&#22312;&#26032;&#39046;&#22495;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30452;&#25509;&#20174;&#23545;&#35805;&#26641;&#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#21407;&#22987;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20195;&#29702;&#21487;&#20197;&#23454;&#29616;&#19982;&#22312;&#20154;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#65292;&#26080;&#35770;&#26159;&#20351;&#29992;&#21830;&#19994;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#65292;&#36824;&#26159;&#20351;&#29992;&#36739;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#22312;&#21333;&#20010;GPU&#19978;&#36816;&#34892;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25910;&#38598;&#21644;&#27979;&#35797;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65306;ONBOARD&#65292;&#19968;&#20010;&#24110;&#21161;&#22806;&#22269;&#23621;&#27665;&#25644;&#36801;&#30340;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17582v1 Announce Type: cross  Abstract: Conversational Tree Search (V\"ath et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#19978;&#19979;&#25991;&#20316;&#20026;&#36890;&#29992;&#30340;&#35821;&#35328;&#26080;&#20851;&#34920;&#31034;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#25351;&#23548;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#34920;&#31034;&#30340;&#23545;&#40784;&#65292;&#24182;&#29983;&#25104;&#26465;&#20214;&#35270;&#35273;-&#35821;&#35328;&#35760;&#24518;&#36827;&#34892;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2403.17556</link><description>&lt;p&gt;
m3P:&#38754;&#21521;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#32763;&#35793;&#30340;&#22810;&#35821;&#22659;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17556
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#19978;&#19979;&#25991;&#20316;&#20026;&#36890;&#29992;&#30340;&#35821;&#35328;&#26080;&#20851;&#34920;&#31034;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#25351;&#23548;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#34920;&#31034;&#30340;&#23545;&#40784;&#65292;&#24182;&#29983;&#25104;&#26465;&#20214;&#35270;&#35273;-&#35821;&#35328;&#35760;&#24518;&#36827;&#34892;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#32763;&#35793;&#36890;&#36807;&#23558;&#25152;&#26377;&#35821;&#35328;&#25237;&#24433;&#21040;&#19968;&#20010;&#20849;&#20139;&#31354;&#38388;&#26469;&#25903;&#25345;&#22810;&#20010;&#32763;&#35793;&#26041;&#21521;&#65292;&#20294;&#30001;&#20110;&#25991;&#26412;&#27169;&#24577;&#20013;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23588;&#20854;&#26159;&#24403;&#35821;&#35328;&#25968;&#37327;&#36739;&#22823;&#26102;&#65292;&#32763;&#35793;&#36136;&#37327;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#35270;&#35273;&#19978;&#19979;&#25991;&#20316;&#20026;&#36890;&#29992;&#30340;&#35821;&#35328;&#26080;&#20851;&#34920;&#31034;&#65292;&#20197;&#20419;&#36827;&#22810;&#35821;&#35328;&#32763;&#35793;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#25351;&#23548;Multimodal Multilingual&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;m3P&#65289;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#31034;&#19982;&#30456;&#21516;&#21547;&#20041;&#23545;&#40784;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#32763;&#35793;&#30340;&#26465;&#20214;&#35270;&#35273;-&#35821;&#35328;&#35760;&#24518;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25903;&#25345;102&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#25351;&#20196;&#25968;&#25454;&#38598;&#65288;InstrMulti102&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#20013;&#22830;&#35821;&#35328;&#26469;&#26368;&#23567;&#21270;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17556v1 Announce Type: cross  Abstract: Multilingual translation supports multiple translation directions by projecting all languages in a shared space, but the translation quality is undermined by the difference between languages in the text-only modality, especially when the number of languages is large. To bridge this gap, we introduce visual context as the universal language-independent representation to facilitate multilingual translation. In this paper, we propose a framework to leverage the multimodal prompt to guide the Multimodal Multilingual neural Machine Translation (m3P), which aligns the representations of different languages with the same meaning and generates the conditional vision-language memory for translation. We construct a multilingual multimodal instruction dataset (InstrMulti102) to support 102 languages. Our method aims to minimize the representation distance of different languages by regarding the image as a central language. Experimental results sh
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#36716;&#21464;&#28508;&#21147;&#65292;&#33021;&#22815;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12289;&#22686;&#24378;&#22270;&#20687;&#12289;&#24110;&#21161;&#24322;&#24120;&#26816;&#27979;&#21644;&#20419;&#36827;&#22270;&#20687;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.17549</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#23454;&#38469;&#24212;&#29992;&#20808;&#36827;&#20113;&#26381;&#21153;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17549
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#36716;&#21464;&#28508;&#21147;&#65292;&#33021;&#22815;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12289;&#22686;&#24378;&#22270;&#20687;&#12289;&#24110;&#21161;&#24322;&#24120;&#26816;&#27979;&#21644;&#20419;&#36827;&#22270;&#20687;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17549v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#21307;&#23398;&#39046;&#22495;&#26159;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#39046;&#22495;&#20043;&#19968;&#12290;&#38543;&#30528;&#21307;&#30103;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#22810;&#26679;&#21270;&#65292;&#20197;&#21450;&#21307;&#30103;&#38656;&#27714;&#21644;&#25361;&#25112;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20197;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20026;&#20195;&#34920;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24050;&#32463;&#24191;&#27867;&#28183;&#36879;&#21040;&#21307;&#23398;&#24433;&#20687;&#12289;&#20581;&#24247;&#31649;&#29702;&#12289;&#21307;&#30103;&#20449;&#24687;&#21644;&#33647;&#29289;&#30740;&#21457;&#31561;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#25104;&#20026;&#25552;&#39640;&#21307;&#30103;&#26381;&#21153;&#27700;&#24179;&#21644;&#36136;&#37327;&#30340;&#37325;&#35201;&#39537;&#21160;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#36716;&#21464;&#28508;&#21147;&#65292;&#24378;&#35843;&#20102;&#23427;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12289;&#22686;&#24378;&#22270;&#20687;&#12289;&#24110;&#21161;&#24322;&#24120;&#26816;&#27979;&#21644;&#20419;&#36827;&#22270;&#20687;&#36716;&#25442;&#31561;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17549v1 Announce Type: new  Abstract: The medical field is one of the important fields in the application of artificial intelligence technology. With the explosive growth and diversification of medical data, as well as the continuous improvement of medical needs and challenges, artificial intelligence technology is playing an increasingly important role in the medical field. Artificial intelligence technologies represented by computer vision, natural language processing, and machine learning have been widely penetrated into diverse scenarios such as medical imaging, health management, medical information, and drug research and development, and have become an important driving force for improving the level and quality of medical services.The article explores the transformative potential of generative AI in medical imaging, emphasizing its ability to generate syntheticACM-2 data, enhance images, aid in anomaly detection, and facilitate image-to-image translation. Despite chall
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20215;&#20540;&#24046;&#24322;&#21644;&#29366;&#24577;&#35745;&#25968;&#65292;&#21033;&#29992;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#30450;&#30446;&#20999;&#25442;&#26426;&#21046;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.17542</link><description>&lt;p&gt;
VDSC&#65306;&#21033;&#29992;&#20215;&#20540;&#24046;&#24322;&#21644;&#29366;&#24577;&#35745;&#25968;&#22686;&#24378;&#25506;&#32034;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17542
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20215;&#20540;&#24046;&#24322;&#21644;&#29366;&#24577;&#35745;&#25968;&#65292;&#21033;&#29992;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#30450;&#30446;&#20999;&#25442;&#26426;&#21046;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#20110;&#8220;&#25506;&#32034;&#22810;&#23569;&#8221;&#21644;&#8220;&#22914;&#20309;&#25506;&#32034;&#8221;&#38382;&#39064;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#8220;&#20309;&#26102;&#8221;&#25506;&#32034;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26356;&#22797;&#26434;&#30340;&#25506;&#32034;&#31574;&#30053;&#21487;&#20197;&#22312;&#29305;&#23450;&#30340;&#12289;&#36890;&#24120;&#31232;&#30095;&#30340;&#22870;&#21169;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#22914;$\epsilon$-&#36138;&#24515;&#65292;&#22312;&#26356;&#24191;&#27867;&#30340;&#39046;&#22495;&#20013;&#32487;&#32493;&#34920;&#29616;&#20248;&#24322;&#12290;&#36825;&#20123;&#31616;&#21333;&#31574;&#30053;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#23427;&#20204;&#30340;&#26131;&#23454;&#29616;&#24615;&#21644;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#26222;&#36941;&#36866;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#32570;&#28857;&#22312;&#20110;&#23427;&#20204;&#26412;&#36136;&#19978;&#26159;&#19968;&#31181;&#30450;&#30446;&#30340;&#20999;&#25442;&#26426;&#21046;&#65292;&#23436;&#20840;&#24573;&#30053;&#20102;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#20915;&#23450;&#8220;&#20309;&#26102;&#8221;&#36827;&#34892;&#25506;&#32034;&#65292;&#20174;&#32780;&#35299;&#20915;&#30450;&#30446;&#20999;&#25442;&#26426;&#21046;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#31283;&#24577;&#65288;VDSC&#65289;&#25552;&#20986;&#20102;&#20215;&#20540;&#24046;&#24322;&#21644;&#29366;&#24577;&#35745;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17542v1 Announce Type: cross  Abstract: Despite the considerable attention given to the questions of \textit{how much} and \textit{how to} explore in deep reinforcement learning, the investigation into \textit{when} to explore remains relatively less researched. While more sophisticated exploration strategies can excel in specific, often sparse reward environments, existing simpler approaches, such as $\epsilon$-greedy, persist in outperforming them across a broader spectrum of domains. The appeal of these simpler strategies lies in their ease of implementation and generality across a wide range of domains. The downside is that these methods are essentially a blind switching mechanism, which completely disregards the agent's internal state. In this paper, we propose to leverage the agent's internal state to decide \textit{when} to explore, addressing the shortcomings of blind switching mechanisms. We present Value Discrepancy and State Counts through homeostasis (VDSC), a no
&lt;/p&gt;</description></item><item><title>KC-GenRe&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32422;&#26463;&#29983;&#25104;&#24335;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#29992;&#20110;KGC&#65292;&#36890;&#36807;&#23558;KGC&#37325;&#26032;&#25490;&#24207;&#20219;&#21153;&#26500;&#24314;&#20026;&#20505;&#36873;&#26631;&#35782;&#31526;&#25490;&#24207;&#29983;&#25104;&#38382;&#39064;&#20197;&#21450;&#24320;&#21457;&#30693;&#35782;&#24341;&#23548;&#26041;&#27861;&#26469;&#35299;&#20915;&#37325;&#26032;&#25490;&#24207;&#36807;&#31243;&#20013;&#30340;&#19981;&#21305;&#37197;&#21644;&#38169;&#24207;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17532</link><description>&lt;p&gt;
KC-GenRe:&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32422;&#26463;&#29983;&#25104;&#24335;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17532
&lt;/p&gt;
&lt;p&gt;
KC-GenRe&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32422;&#26463;&#29983;&#25104;&#24335;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#29992;&#20110;KGC&#65292;&#36890;&#36807;&#23558;KGC&#37325;&#26032;&#25490;&#24207;&#20219;&#21153;&#26500;&#24314;&#20026;&#20505;&#36873;&#26631;&#35782;&#31526;&#25490;&#24207;&#29983;&#25104;&#38382;&#39064;&#20197;&#21450;&#24320;&#21457;&#30693;&#35782;&#24341;&#23548;&#26041;&#27861;&#26469;&#35299;&#20915;&#37325;&#26032;&#25490;&#24207;&#36807;&#31243;&#20013;&#30340;&#19981;&#21305;&#37197;&#21644;&#38169;&#24207;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;KGC&#65289;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#23454;&#20307;&#20043;&#38388;&#32570;&#22833;&#30340;&#20107;&#23454;&#12290;&#20808;&#21069;&#30340;KGC&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#22823;&#22810;&#24314;&#31435;&#22312;&#38750;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#20197;&#33719;&#24471;&#27599;&#20010;&#20505;&#36873;&#30340;&#27010;&#29575;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#24687;&#25552;&#21462;&#21644;&#23545;&#35805;&#31995;&#32479;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#21033;&#29992;&#23427;&#20204;&#36827;&#34892;KGC&#37325;&#26032;&#25490;&#24207;&#26377;&#21033;&#20110;&#21033;&#29992;&#24191;&#27867;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23436;&#25104;&#20219;&#21153;&#26102;&#21487;&#33021;&#20250;&#36935;&#21040;&#26032;&#38382;&#39064;&#65292;&#21363;&#19981;&#21305;&#37197;&#12289;&#38169;&#24207;&#21644;&#36951;&#28431;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KC-GenRe&#65292;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#30693;&#35782;&#32422;&#26463;&#29983;&#25104;&#24335;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#29992;&#20110;KGC&#12290;&#20026;&#20102;&#20811;&#26381;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;KGC&#37325;&#26032;&#25490;&#24207;&#20219;&#21153;&#26500;&#24314;&#20026;&#30001;&#29983;&#25104;&#24335;LLMs&#23454;&#26045;&#30340;&#20505;&#36873;&#26631;&#35782;&#31526;&#25490;&#24207;&#29983;&#25104;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#38169;&#24207;&#38382;&#39064;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#21463;&#30693;&#35782;&#24341;&#23548;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17532v1 Announce Type: new  Abstract: The goal of knowledge graph completion (KGC) is to predict missing facts among entities. Previous methods for KGC re-ranking are mostly built on non-generative language models to obtain the probability of each candidate. Recently, generative large language models (LLMs) have shown outstanding performance on several tasks such as information extraction and dialog systems. Leveraging them for KGC re-ranking is beneficial for leveraging the extensive pre-trained knowledge and powerful generative capabilities. However, it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission. To this end, we introduce KC-GenRe, a knowledge-constrained generative re-ranking method based on LLMs for KGC. To overcome the mismatch issue, we formulate the KGC re-ranking task as a candidate identifier sorting generation problem implemented by generative LLMs. To tackle the misordering issue, we develop a knowledge-guided
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35299;&#32544;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#65292;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#35757;&#32451;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.17530</link><description>&lt;p&gt;
&#21033;&#29992;&#35299;&#32544;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17530
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#32544;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#65292;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#35757;&#32451;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#21644;&#30446;&#26631;&#65306;&#22312;&#21307;&#23398;&#25104;&#20687;&#31561;&#20851;&#38190;&#39046;&#22495;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#30528;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#35757;&#32451;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#26041;&#27861;&#65306;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20174;&#19968;&#20010;&#39044;&#35757;&#32451;&#38454;&#27573;&#24320;&#22987;&#65292;&#20854;&#20013;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#20013;&#23398;&#21040;&#30340;&#29305;&#24449;&#34987;&#35299;&#32544;&#20197;&#25552;&#39640;&#34920;&#31034;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#21518;&#24341;&#20837;&#20803;&#24494;&#35843;&#27493;&#39588;&#65292;&#21033;&#29992;&#20803;&#35757;&#32451;&#21644;&#20803;&#27979;&#35797;&#38454;&#27573;&#20043;&#38388;&#30340;&#30456;&#20851;&#31867;&#21035;&#20294;&#21464;&#21270;&#31890;&#24230;&#32423;&#21035;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#23558;&#27169;&#22411;&#26292;&#38706;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#28982;&#21518;&#22312;&#20803;&#27979;&#35797;&#26399;&#38388;&#35780;&#20272;&#23427;&#22312;&#26356;&#31616;&#21333;&#20294;&#22312;&#20020;&#24202;&#19978;&#26356;&#20855;&#37325;&#35201;&#24615;&#30340;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17530v1 Announce Type: cross  Abstract: Background and objective: Employing deep learning models in critical domains such as medical imaging poses challenges associated with the limited availability of training data. We present a strategy for improving the performance and generalization capabilities of models trained in low-data regimes. Methods: The proposed method starts with a pre-training phase, where features learned in a self-supervised learning setting are disentangled to improve the robustness of the representations for downstream tasks. We then introduce a meta-fine-tuning step, leveraging related classes between meta-training and meta-testing phases but varying the granularity level. This approach aims to enhance the model's generalization capabilities by exposing it to more challenging classification tasks during meta-training and evaluating it on easier tasks but holding greater clinical relevance during meta-testing. We demonstrate the effectiveness of the propo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20026;&#32032;&#25551;&#34917;&#19969;&#37197;&#22791;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#20445;&#25252;&#19981;&#21516;&#32472;&#22270;&#29256;&#26412;&#30340;&#26041;&#27861;&#65292;&#23558;&#32472;&#22270;&#39034;&#24207;&#20449;&#24687;&#23884;&#20837;&#22270;&#33410;&#28857;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#32032;&#25551;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.17525</link><description>&lt;p&gt;
&#20026;&#22270;&#24418;&#32032;&#25551;&#34920;&#31034;&#35013;&#22791;&#20855;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#30340;&#32032;&#25551;&#34917;&#19969;
&lt;/p&gt;
&lt;p&gt;
Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20026;&#32032;&#25551;&#34917;&#19969;&#37197;&#22791;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#20445;&#25252;&#19981;&#21516;&#32472;&#22270;&#29256;&#26412;&#30340;&#26041;&#27861;&#65292;&#23558;&#32472;&#22270;&#39034;&#24207;&#20449;&#24687;&#23884;&#20837;&#22270;&#33410;&#28857;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#32032;&#25551;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#24133;&#32032;&#25551;&#30340;&#32472;&#21046;&#39034;&#24207;&#35760;&#24405;&#20102;&#23427;&#26159;&#22914;&#20309;&#36880;&#31508;&#30001;&#20154;&#31867;&#21019;&#24314;&#30340;&#12290;&#23545;&#20110;&#22270;&#24418;&#32032;&#25551;&#34920;&#31034;&#23398;&#20064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#26681;&#25454;&#22522;&#20110;&#26102;&#38388;&#30340;&#26368;&#36817;&#37051;&#31574;&#30053;&#23558;&#27599;&#20010;&#34917;&#19969;&#19982;&#21478;&#19968;&#20010;&#30456;&#36830;&#65292;&#23558;&#32032;&#25551;&#32472;&#22270;&#39034;&#24207;&#27880;&#20837;&#21040;&#22270;&#36793;&#26500;&#24314;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#26500;&#24314;&#30340;&#22270;&#36793;&#21487;&#33021;&#19981;&#21487;&#38752;&#65292;&#22240;&#20026;&#32032;&#25551;&#21487;&#33021;&#26377;&#19981;&#21516;&#29256;&#26412;&#30340;&#32472;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#36807;&#21464;&#20307;&#32472;&#21046;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#32032;&#25551;&#34917;&#19969;&#37197;&#22791;&#20855;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;(PE)&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#32472;&#22270;&#39034;&#24207;&#26469;&#23398;&#20064;&#22270;&#24418;&#32032;&#25551;&#34920;&#31034;&#12290;&#25105;&#20204;&#27809;&#26377;&#23558;&#32032;&#25551;&#32472;&#21046;&#27880;&#20837;&#21040;&#22270;&#36793;&#20013;&#65292;&#32780;&#26159;&#20165;&#23558;&#36825;&#20123;&#39034;&#24207;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#33410;&#28857;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27599;&#20010;&#34917;&#19969;&#23884;&#20837;&#37117;&#37197;&#22791;&#26377;&#27491;&#24358;&#32477;&#23545;PE&#65292;&#20197;&#31361;&#20986;&#32472;&#22270;&#39034;&#24207;&#20013;&#30340;&#39034;&#24207;&#20301;&#32622;&#12290;&#23427;&#30340;&#30456;&#37051;&#34917;&#19969;&#25353;self-att&#30340;&#20215;&#20540;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17525v1 Announce Type: cross  Abstract: The drawing order of a sketch records how it is created stroke-by-stroke by a human being. For graphic sketch representation learning, recent studies have injected sketch drawing orders into graph edge construction by linking each patch to another in accordance to a temporal-based nearest neighboring strategy. However, such constructed graph edges may be unreliable, since a sketch could have variants of drawings. In this paper, we propose a variant-drawing-protected method by equipping sketch patches with context-aware positional encoding (PE) to make better use of drawing orders for learning graphic sketch representation. Instead of injecting sketch drawings into graph edges, we embed these sequential information into graph nodes only. More specifically, each patch embedding is equipped with a sinusoidal absolute PE to highlight the sequential position in the drawing order. And its neighboring patches, ranked by the values of self-att
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#27604;&#36739;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#30340;&#33041;&#27963;&#21160;&#26144;&#23556;&#26469;&#25351;&#23548;&#25991;&#26412;&#37325;&#24314;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#38388;&#25509;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17516</link><description>&lt;p&gt;
MapGuide: &#20174;&#33041;&#27963;&#21160;&#20013;&#37325;&#24314;&#36830;&#32493;&#35821;&#35328;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#27604;&#36739;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#30340;&#33041;&#27963;&#21160;&#26144;&#23556;&#26469;&#25351;&#23548;&#25991;&#26412;&#37325;&#24314;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#38388;&#25509;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#27963;&#21160;&#20013;&#35299;&#30721;&#36830;&#32493;&#35821;&#35328;&#26159;&#19968;&#39033;&#33392;&#24040;&#20294;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36825;&#23545;&#20110;&#24110;&#21161;&#35821;&#35328;&#27531;&#38556;&#20154;&#22763;&#36890;&#36807;&#33041;&#20449;&#21495;&#36827;&#34892;&#27807;&#36890;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23558;&#20174;&#33041;&#27963;&#21160;&#26144;&#23556;&#30340;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#21521;&#23548;&#25991;&#26412;&#37325;&#24314;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;METEOR&#20998;&#25968;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;77%&#21644;54%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17516v1 Announce Type: cross  Abstract: Decoding continuous language from brain activity is a formidable yet promising field of research. It is particularly significant for aiding people with speech disabilities to communicate through brain signals. This field addresses the complex task of mapping brain signals to text. The previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from text and then guided text generation by aligning with predicted brain responses. In contrast, we propose a simple yet effective method that guides text reconstruction by directly comparing them with the predicted text embeddings mapped from brain activities. Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on BLEU and METEOR scores. We further validate the proposed modules through detailed ablation studies and case analyses and 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#30340;&#39044;&#27979;&#20998;&#20139;&#26041;&#38754;&#30340;&#26032;&#39062;&#20043;&#22788;&#65292;&#24182;&#20171;&#32461;&#21644;&#31361;&#20986;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#20849;&#20139;&#21512;&#21516;&#12290;</title><link>https://arxiv.org/abs/2403.17515</link><description>&lt;p&gt;
&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#30340;&#39044;&#27979;&#20998;&#20139;
&lt;/p&gt;
&lt;p&gt;
Prediction-sharing During Training and Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17515
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#30340;&#39044;&#27979;&#20998;&#20139;&#26041;&#38754;&#30340;&#26032;&#39062;&#20043;&#22788;&#65292;&#24182;&#20171;&#32461;&#21644;&#31361;&#20986;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#20849;&#20139;&#21512;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#23478;&#20844;&#21496;&#21442;&#19982;&#31454;&#20105;&#24615;&#39044;&#27979;&#20219;&#21153;&#12290;&#27599;&#23478;&#20844;&#21496;&#26377;&#20004;&#20010;&#25968;&#25454;&#26469;&#28304; -- &#26377;&#26631;&#31614;&#30340;&#21382;&#21490;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#30340;&#25512;&#29702;&#26102;&#38388;&#25968;&#25454; -- &#24182;&#19988;&#20351;&#29992;&#21069;&#32773;&#21046;&#23450;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#21518;&#32773;&#23545;&#26032;&#23454;&#20363;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20844;&#21496;&#20043;&#38388;&#30340;&#25968;&#25454;&#20849;&#20139;&#21512;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#20171;&#32461;&#21644;&#31361;&#20986;&#20165;&#20998;&#20139;&#39044;&#27979;&#27169;&#22411;&#30340;&#21512;&#21516;&#12289;&#20165;&#20998;&#20139;&#25512;&#29702;&#26102;&#38388;&#39044;&#27979;&#30340;&#21512;&#21516;&#20197;&#21450;&#20998;&#20139;&#20004;&#32773;&#30340;&#21512;&#21516;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20998;&#20026;&#19977;&#20010;&#23618;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20415;&#20110;&#36827;&#34892;&#30740;&#31350;&#30340;&#19968;&#33324;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#28966;&#28857;&#32553;&#23567;&#21040;&#36825;&#20010;&#26694;&#26550;&#20869;&#30340;&#20004;&#20010;&#33258;&#28982;&#35774;&#32622;&#65306;(i) &#27599;&#23478;&#20844;&#21496;&#30340;&#39044;&#27979;&#27169;&#22411;&#20934;&#30830;&#24230;&#26159;&#20849;&#30693;&#30340;&#65292;&#20294;&#21508;&#33258;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26410;&#30693;&#65307;(ii) &#23384;&#22312;&#20004;&#20010;&#20851;&#20110;&#26368;&#20248;&#39044;&#27979;&#22120;&#30340;&#20551;&#35774;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17515v1 Announce Type: cross  Abstract: Two firms are engaged in a competitive prediction task. Each firm has two sources of data -- labeled historical data and unlabeled inference-time data -- and uses the former to derive a prediction model, and the latter to make predictions on new instances. We study data-sharing contracts between the firms. The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both. Our analysis proceeds on three levels. First, we develop a general Bayesian framework that facilitates our study. Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm's prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#38656;&#27714;&#24322;&#21619;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#25490;&#21517;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#21487;&#27979;&#35797;&#24615;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#34913;&#37327;&#21644;&#37327;&#21270;&#38656;&#27714;&#30340;&#21487;&#27979;&#35797;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17479</link><description>&lt;p&gt;
&#22522;&#20110;&#38656;&#27714;&#24322;&#21619;&#30340;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#21487;&#27979;&#35797;&#24615;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Natural Language Requirements Testability Measurement Based on Requirement Smells
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#38656;&#27714;&#24322;&#21619;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#25490;&#21517;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#21487;&#27979;&#35797;&#24615;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#34913;&#37327;&#21644;&#37327;&#21270;&#38656;&#27714;&#30340;&#21487;&#27979;&#35797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#27714;&#26500;&#25104;&#20102;&#23450;&#20041;&#36719;&#20214;&#31995;&#32479;&#20041;&#21153;&#21644;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#21487;&#27979;&#35797;&#30340;&#38656;&#27714;&#26377;&#21161;&#20110;&#38450;&#27490;&#22833;&#36133;&#65292;&#38477;&#20302;&#32500;&#25252;&#25104;&#26412;&#65292;&#24182;&#31616;&#21270;&#39564;&#25910;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#34913;&#37327;&#21644;&#37327;&#21270;&#38656;&#27714;&#21487;&#27979;&#35797;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#23578;&#26410;&#25552;&#20986;&#22522;&#20110;&#38656;&#27714;&#24322;&#21619;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#38656;&#27714;&#30340;&#21487;&#27979;&#35797;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#21644;&#25490;&#21517;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#30340;&#21487;&#27979;&#35797;&#24615;&#65292;&#22522;&#20110;&#19968;&#20010;&#24191;&#27867;&#30340;&#20061;&#20010;&#38656;&#27714;&#24322;&#21619;&#38598;&#21512;&#65292;&#33258;&#21160;&#26816;&#27979;&#65292;&#24182;&#26681;&#25454;&#38656;&#27714;&#38271;&#24230;&#21644;&#20854;&#24212;&#29992;&#39046;&#22495;&#26469;&#30830;&#23450;&#39564;&#25910;&#27979;&#35797;&#24037;&#20316;&#30340;&#21162;&#21147;&#12290;&#22823;&#22810;&#25968;&#24322;&#21619;&#28304;&#20110;&#19981;&#21487;&#25968;&#30340;&#24418;&#23481;&#35789;&#65292;&#19978;&#19979;&#25991;&#25935;&#24863;&#21644;&#27169;&#31946;&#35789;&#12290;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#23383;&#20856;&#26469;&#26816;&#27979;&#36825;&#20123;&#35789;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31070;&#32463;&#35789;&#23884;&#20837;&#25216;&#26415;&#26469;&#29983;&#25104;&#36825;&#26679;&#19968;&#20010;&#23383;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17479v1 Announce Type: cross  Abstract: Requirements form the basis for defining software systems' obligations and tasks. Testable requirements help prevent failures, reduce maintenance costs, and make it easier to perform acceptance tests. However, despite the importance of measuring and quantifying requirements testability, no automatic approach for measuring requirements testability has been proposed based on the requirements smells, which are at odds with the requirements testability. This paper presents a mathematical model to evaluate and rank the natural language requirements testability based on an extensive set of nine requirements smells, detected automatically, and acceptance test efforts determined by requirement length and its application domain. Most of the smells stem from uncountable adjectives, context-sensitive, and ambiguous words. A comprehensive dictionary is required to detect such words. We offer a neural word-embedding technique to generate such a dic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#31070;&#32463;&#20869;&#26680;(UNK)&#65292;&#21487;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#23398;&#20064;&#27493;&#39588;&#19979;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;NTK&#30340;&#34892;&#20026;&#65292;&#24403;&#23398;&#20064;&#27493;&#39588;&#36924;&#36817;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;NNGP&#12290;</title><link>https://arxiv.org/abs/2403.17467</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
A Unified Kernel for Neural Network Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#31070;&#32463;&#20869;&#26680;(UNK)&#65292;&#21487;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#23398;&#20064;&#27493;&#39588;&#19979;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;NTK&#30340;&#34892;&#20026;&#65292;&#24403;&#23398;&#20064;&#27493;&#39588;&#36924;&#36817;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;NNGP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#20869;&#26680;&#23398;&#20064;&#20043;&#38388;&#30340;&#21306;&#21035;&#21644;&#32852;&#31995;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#22312;&#36830;&#25509;&#26080;&#38480;&#23485;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#29702;&#35770;&#19978;&#30340;&#36827;&#23637;&#12290;&#20986;&#29616;&#20102;&#20004;&#31181;&#20027;&#27969;&#26041;&#27861;&#65306;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;(NNGP)&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#12290;&#21069;&#32773;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20195;&#34920;&#20102;&#38646;&#38454;&#26680;&#65292;&#32780;&#21518;&#32773;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20999;&#21521;&#31354;&#38388;&#65292;&#26159;&#31532;&#19968;&#38454;&#26680;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32479;&#19968;&#31070;&#32463;&#20869;&#26680;(UNK)&#65292;&#35813;&#20869;&#26680;&#34920;&#24449;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#26799;&#24230;&#19979;&#38477;&#21644;&#21442;&#25968;&#21021;&#22987;&#21270;&#20013;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;UNK&#20869;&#26680;&#20445;&#25345;&#20102;NNGP&#21644;NTK&#30340;&#26497;&#38480;&#29305;&#24615;&#65292;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;NTK&#30340;&#34892;&#20026;&#65292;&#20294;&#26377;&#26377;&#38480;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#24182;&#19988;&#24403;&#23398;&#20064;&#27493;&#39588;&#25509;&#36817;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;NNGP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#23545;UNK&#20869;&#26680;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17467v1 Announce Type: cross  Abstract: Past decades have witnessed a great interest in the distinction and connection between neural network learning and kernel learning. Recent advancements have made theoretical progress in connecting infinite-wide neural networks and Gaussian processes. Two predominant approaches have emerged: the Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The former, rooted in Bayesian inference, represents a zero-order kernel, while the latter, grounded in the tangent space of gradient descents, is a first-order kernel. In this paper, we present the Unified Neural Kernel (UNK), which characterizes the learning dynamics of neural networks with gradient descents and parameter initialization. The proposed UNK kernel maintains the limiting properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite learning step and converging to NNGP as the learning step approaches infinity. Besides, we also theoreticall
&lt;/p&gt;</description></item><item><title>LaRE^2 &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#21644;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#22686;&#24378;&#65292;&#20174;&#32780;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.17465</link><description>&lt;p&gt;
LaRE^2: &#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17465
&lt;/p&gt;
&lt;p&gt;
LaRE^2 &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#21644;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#22686;&#24378;&#65292;&#20174;&#32780;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17465v1 &#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#20351;&#30495;&#23454;&#22270;&#20687;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#21306;&#20998;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#23613;&#31649;&#36825;&#19968;&#36827;&#23637;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#26041;&#27861;&#65288;LaRE^2&#65289;&#26469;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#65292;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#29305;&#24449;&#12290;LaRE&#22312;&#29305;&#24449;&#25552;&#21462;&#25928;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21306;&#20998;&#30495;&#20551;&#25152;&#38656;&#30340;&#20851;&#38190;&#32447;&#32034;&#12290;&#20026;&#20102;&#21033;&#29992;LaRE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;LaRE&#24341;&#23548;&#30340;&#26041;&#24335;&#32454;&#21270;&#22270;&#20687;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17465v1 Announce Type: cross  Abstract: The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17456</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Imitating Cost-Constrained Behaviors in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#22797;&#26434;&#30340;&#35745;&#21010;&#21644;&#35843;&#24230;&#38382;&#39064;&#19968;&#30452;&#36890;&#36807;&#21508;&#31181;&#20248;&#21270;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#30340;&#27169;&#20223;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#21487;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#30340;&#34892;&#20026;&#26469;&#23398;&#20064;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#27169;&#22411;&#25110;&#30452;&#25509;&#34892;&#20026;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26080;&#38480;&#21046;&#35774;&#32622;&#19979;&#30340;&#27169;&#20223;&#65288;&#20363;&#22914;&#65292;&#36710;&#36742;&#28040;&#32791;&#30340;&#29123;&#27833;&#37327;&#27809;&#26377;&#38480;&#21046;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#19987;&#23478;&#30340;&#34892;&#20026;&#19981;&#20165;&#21463;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#30340;&#24433;&#21709;&#65292;&#36824;&#21463;&#32422;&#26463;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#33258;&#21160;&#39550;&#39542;&#36865;&#36135;&#36710;&#30340;&#20915;&#31574;&#19981;&#20165;&#21462;&#20915;&#20110;&#36335;&#24452;&#20559;&#22909;/&#22870;&#21169;&#65288;&#26681;&#25454;&#36807;&#21435;&#30340;&#38656;&#27714;&#25968;&#25454;&#65289;&#65292;&#36824;&#21462;&#20915;&#20110;&#36710;&#36742;&#20869;&#30340;&#29123;&#27833;&#21644;&#36865;&#36798;&#26102;&#38388;&#31561;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti
&lt;/p&gt;</description></item><item><title>&#23558;&#31616;&#21333;&#30340;&#25351;&#25968;&#24179;&#28369;&#27861;&#19982;MLP&#32467;&#21512;&#65292;&#36890;&#36807;&#22686;&#21152;&#21442;&#25968;&#21644;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#22797;&#26434;S4&#27169;&#22411;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.17445</link><description>&lt;p&gt;
&#23558;&#25351;&#25968;&#24179;&#28369;&#27861;&#34701;&#20837;MLP&#65306;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17445
&lt;/p&gt;
&lt;p&gt;
&#23558;&#31616;&#21333;&#30340;&#25351;&#25968;&#24179;&#28369;&#27861;&#19982;MLP&#32467;&#21512;&#65292;&#36890;&#36807;&#22686;&#21152;&#21442;&#25968;&#21644;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#22797;&#26434;S4&#27169;&#22411;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#21015;&#25968;&#25454;&#20013;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#26159;&#24207;&#21015;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26368;&#36817;&#21457;&#23637;&#30340;&#27169;&#22411;&#8220;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#8221;&#65288;S4&#65289;&#22312;&#24314;&#27169;&#38271;&#26399;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;S4&#30340;&#25104;&#21151;&#26159;&#22240;&#20026;&#20854;&#22797;&#26434;&#30340;&#21442;&#25968;&#21270;&#21644;HiPPO&#21021;&#22987;&#21270;&#36824;&#26159;&#20165;&#20165;&#30001;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#35752;&#28145;&#24230;SSMs&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#20174;&#31616;&#21333;&#30340;SSM&#25351;&#25968;&#24179;&#28369;&#65288;ETS&#65289;&#24320;&#22987;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#23558;&#20854;&#34701;&#20837;&#36880;&#20803;&#32032;MLP&#25552;&#20986;&#20102;&#19968;&#20010;&#21472;&#21152;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#23383;&#27573;&#26469;&#25193;&#20805;&#31616;&#21333;&#30340;ETS&#20197;&#20943;&#23569;&#24402;&#32435;&#20559;&#24046;&#12290;&#23613;&#31649;&#22312;&#36880;&#20803;&#32032;MLP&#30340;&#21442;&#25968;&#22686;&#21152;&#19981;&#21040;1%&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;LRA&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#19982;S4&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17445v1 Announce Type: cross  Abstract: Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1\% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#30740;&#31350;LLMs&#22312;&#21010;&#20998;&#30151;&#29366;&#21644;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#26041;&#38754;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17428</link><description>&lt;p&gt;
&#36890;&#36807;&#30151;&#29366;&#21010;&#20998;&#21644;&#24635;&#32467;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17428
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#30740;&#31350;LLMs&#22312;&#21010;&#20998;&#30151;&#29366;&#21644;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#26041;&#38754;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21152;&#36895;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#37492;&#20110;&#31934;&#31070;&#31185;&#35775;&#35848;&#26159;&#19987;&#19994;&#38754;&#35797;&#32773;&#19982;&#34987;&#38754;&#35797;&#32773;&#20043;&#38388;&#30446;&#26631;&#23548;&#21521;&#21644;&#32467;&#26500;&#21270;&#23545;&#35805;&#65292;&#36825;&#26159;LLMs&#21487;&#20197;&#25552;&#20379;&#23454;&#36136;&#20215;&#20540;&#30340;&#26368;&#26410;&#34987;&#24320;&#21457;&#30340;&#39046;&#22495;&#20043;&#19968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#20855;&#26377;&#21019;&#20260;&#32463;&#21382;&#21644;&#31934;&#31070;&#20581;&#24247;&#38382;&#39064;&#30340;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#25506;&#35752;&#20102;LLMs&#29992;&#20110;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#29992;&#36884;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;LLMs&#26159;&#21542;&#33021;&#22815;&#65288;1&#65289;&#21010;&#20998;&#34920;&#31034;&#31934;&#31070;&#30151;&#29366;&#30340;&#23545;&#35805;&#37096;&#20998;&#24182;&#21629;&#21517;&#30151;&#29366;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26681;&#25454;&#35775;&#35848;&#23545;&#35805;&#35760;&#24405;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#12290;&#36825;&#37324;&#65292;&#35775;&#35848;&#25968;&#25454;&#30001;&#31934;&#31070;&#20581;&#24247;&#19987;&#23478;&#36827;&#34892;&#26631;&#35760;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#25552;&#31034;&#30340;LLMs&#22312;&#30151;&#29366;&#21010;&#20998;&#21644;&#24635;&#32467;&#19978;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17428v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have accelerated their usage in various domains. Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where LLMs can contribute substantial value. Here, we explore the use of LLMs for enhancing psychiatric interviews, by analyzing counseling data from North Korean defectors with traumatic events and mental health issues. Specifically, we investigate whether LLMs can (1) delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms, and (2) summarize stressors and symptoms, based on the interview dialogue transcript. Here, the transcript data was labeled by mental health experts for training and evaluation of LLMs. Our experimental results show that appropriately prompted LLMs can achieve high performance on both the sympto
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#25512;&#24191;&#26356;&#20581;&#24247;&#30340;&#39278;&#39135;&#20064;&#24815;&#65292;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#27700;&#36275;&#36857;&#12290;</title><link>https://arxiv.org/abs/2403.17426</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#33616;&#31995;&#32479;&#25913;&#21892;&#39278;&#39135;&#27700;&#36275;&#36857;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Powered Recommendation for an Improved Diet Water Footprint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#25512;&#24191;&#26356;&#20581;&#24247;&#30340;&#39278;&#39135;&#20064;&#24815;&#65292;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#27700;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#19990;&#30028;&#33258;&#28982;&#22522;&#37329;&#20250;&#65288;WWF&#65289;&#30340;&#25968;&#25454;&#65292;&#26377;11&#20159;&#20154;&#32570;&#20047;&#27700;&#36164;&#28304;&#65292;27&#20159;&#20154;&#27599;&#24180;&#33267;&#23569;&#26377;&#19968;&#20010;&#26376;&#32463;&#21382;&#27700;&#36164;&#28304;&#21294;&#20047;&#12290;&#21040;2025&#24180;&#65292;&#20840;&#29699;&#19977;&#20998;&#20043;&#20108;&#30340;&#20154;&#21475;&#21487;&#33021;&#38754;&#20020;&#27700;&#36164;&#28304;&#30701;&#32570;&#12290;&#36825;&#31361;&#26174;&#20102;&#39640;&#25928;&#31649;&#29702;&#27700;&#36164;&#28304;&#20351;&#29992;&#30340;&#32039;&#36843;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#39135;&#21697;&#31561;&#32791;&#27700;&#34892;&#19994;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#20419;&#36827;&#21487;&#25345;&#32493;&#21644;&#20581;&#24247;&#30340;&#39135;&#21697;&#28040;&#36153;&#12290;&#35813;&#31995;&#32479;&#22312;&#29992;&#25143;&#39135;&#35889;&#20013;&#25512;&#33616;&#25104;&#20998;&#26367;&#20195;&#21697;&#65292;&#20174;&#32780;&#25913;&#21892;&#33829;&#20859;&#20215;&#20540;&#24182;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#27700;&#36275;&#36857;&#12290;&#31995;&#32479;&#26550;&#26500;&#21253;&#25324;&#28304;&#35782;&#21035;&#12289;&#20449;&#24687;&#25552;&#21462;&#12289;&#27169;&#24335;&#23545;&#40784;&#12289;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#29992;&#25143;&#30028;&#38754;&#24320;&#21457;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25512;&#24191;&#26356;&#20581;&#24247;&#30340;&#39278;&#39135;&#20064;&#24815;&#21644;&#20419;&#36827;&#33410;&#27700;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17426v1 Announce Type: new  Abstract: According to WWF, 1.1 billion people lack access to water, and 2.7 billion experience water scarcity at least one month a year. By 2025, two-thirds of the world's population may be facing water shortages. This highlights the urgency of managing water usage efficiently, especially in water-intensive sectors like food. This paper proposes a recommendation engine, powered by knowledge graphs, aiming to facilitate sustainable and healthy food consumption. The engine recommends ingredient substitutes in user recipes that improve nutritional value and reduce environmental impact, particularly water footprint. The system architecture includes source identification, information extraction, schema alignment, knowledge graph construction, and user interface development. The research offers a promising tool for promoting healthier eating habits and contributing to water conservation efforts.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;MA4DIV&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#65292;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20197;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17421</link><description>&lt;p&gt;
MA4DIV&#65306;&#29992;&#20110;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17421
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;MA4DIV&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#65292;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20197;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#65288;SRD&#65289;&#30340;&#30446;&#26631;&#26159;&#30830;&#20445;&#25152;&#36873;&#25991;&#26723;&#28085;&#30422;&#23613;&#21487;&#33021;&#22810;&#30340;&#19981;&#21516;&#23376;&#20027;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#8220;&#36138;&#23146;&#36873;&#25321;&#8221;&#33539;&#24335;&#65292;&#21363;&#19968;&#27425;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#26368;&#39640;&#22810;&#26679;&#24615;&#20998;&#25968;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#25928;&#29575;&#20302;&#19979;&#65292;&#23481;&#26131;&#38519;&#20837;&#27425;&#20248;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#20854;&#20182;&#26041;&#27861;&#26088;&#22312;&#36817;&#20284;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20294;&#32467;&#26524;&#20173;&#28982;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;MA4DIV&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#25991;&#26723;&#37117;&#26159;&#19968;&#20010;&#26234;&#33021;&#20307;&#65292;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#34987;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17421v1 Announce Type: cross  Abstract: The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of "greedy selection", i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\alpha$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\alpha$-NDCG, while achieving high training efficiency. We conducted preliminary experi
&lt;/p&gt;</description></item><item><title>AI&#23433;&#20840;&#24615;&#28818;&#20316;&#23384;&#22312;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21033;&#30340;&#30417;&#31649;&#21162;&#21147;&#24182;&#20351;&#26377;&#23475;AI&#21512;&#27861;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.17419</link><description>&lt;p&gt;
AI&#23433;&#20840;&#24615;&#65306;&#24517;&#35201;&#65292;&#20294;&#19981;&#36275;&#21644;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
AI Safety: Necessary, but insufficient and possibly problematic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17419
&lt;/p&gt;
&lt;p&gt;
AI&#23433;&#20840;&#24615;&#28818;&#20316;&#23384;&#22312;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21033;&#30340;&#30417;&#31649;&#21162;&#21147;&#24182;&#20351;&#26377;&#23475;AI&#21512;&#27861;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36817;&#26399;&#22260;&#32469;AI&#23433;&#20840;&#24615;&#30340;&#28818;&#20316;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#23457;&#35270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;AI&#23433;&#20840;&#24615;&#28818;&#20316;&#30340;&#24615;&#36136;&#26159;&#30001;&#25919;&#24220;&#21644;&#20225;&#19994;&#20027;&#23548;&#65292;&#24182;&#23558;&#20854;&#19982;AI&#30740;&#31350;&#20013;&#20854;&#20182;&#20851;&#20110;&#25512;&#36827;&#31038;&#20250;&#21033;&#30410;&#30340;&#36884;&#24452;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#8220;AI&#23433;&#20840;&#24615;&#8221;&#23454;&#38469;&#19978;&#24847;&#21619;&#30528;&#20160;&#20040;&#65292;&#24182;&#27010;&#36848;&#20102;AI&#23433;&#20840;&#24615;&#25968;&#23383;&#36275;&#36857;&#25152;&#31526;&#21512;&#30340;&#20027;&#27969;&#27010;&#24565;&#12290;&#25105;&#20204;&#35748;&#20026;AI&#23433;&#20840;&#24615;&#19982;&#36879;&#26126;&#24230;&#31561;&#19982;&#31038;&#20250;&#21033;&#30410;&#30456;&#20851;&#32852;&#30340;&#27010;&#24565;&#20043;&#38388;&#23384;&#22312;&#24494;&#22937;&#19988;&#19981;&#31283;&#23450;&#30340;&#20851;&#31995;&#65292;&#34920;&#26126;&#22914;&#26524;&#30446;&#26631;&#26159;&#24191;&#20041;&#19978;&#30340;&#31038;&#20250;&#21033;&#30410;&#65292;&#21017;&#23427;&#26159;&#19968;&#20010;&#19981;&#20805;&#20998;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;AI&#23433;&#20840;&#24615;&#30340;&#35752;&#35770;&#24050;&#32463;&#24433;&#21709;&#20102;&#19968;&#20123;AI&#30417;&#31649;&#21162;&#21147;&#65292;&#20063;&#35768;&#26159;&#26397;&#30528;&#19981;&#22826;&#29702;&#24819;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#23545;AI&#23433;&#20840;&#24615;&#22914;&#20309;&#20351;&#36890;&#36807;&#32473;&#20104;&#21093;&#21066;&#24615;&#21644;&#26377;&#23475;AI&#19968;&#23618;&#23433;&#20840;&#22806;&#34915;&#26469;&#20419;&#20351;&#32467;&#26500;&#24615;&#20260;&#23475;&#30340;AI&#21512;&#27861;&#21270;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17419v1 Announce Type: new  Abstract: This article critically examines the recent hype around AI safety. We first start with noting the nature of the AI safety hype as being dominated by governments and corporations, and contrast it with other avenues within AI research on advancing social good. We consider what 'AI safety' actually means, and outline the dominant concepts that the digital footprint of AI safety aligns with. We posit that AI safety has a nuanced and uneasy relationship with transparency and other allied notions associated with societal good, indicating that it is an insufficient notion if the goal is that of societal good in a broad sense. We note that the AI safety debate has already influenced some regulatory efforts in AI, perhaps in not so desirable directions. We also share our concerns on how AI safety may normalize AI that advances structural harm through providing exploitative and harmful AI with a veneer of safety.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#22914;Deep Sets&#21644;Transformers&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#30340;&#36827;&#23637;</title><link>https://arxiv.org/abs/2403.17410</link><description>&lt;p&gt;
&#35770;&#25490;&#21015;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
On permutation-invariant neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17410
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22914;Deep Sets&#21644;Transformers&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#22312;&#20551;&#35774;&#36755;&#20837;&#25968;&#25454;&#36981;&#24490;&#22522;&#20110;&#21521;&#37327;&#30340;&#26684;&#24335;&#30340;&#21069;&#25552;&#19979;&#35774;&#35745;&#65292;&#30528;&#37325;&#20110;&#22522;&#20110;&#21521;&#37327;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#38656;&#27714;&#28041;&#21450;&#22522;&#20110;&#38598;&#21512;&#30340;&#20219;&#21153;&#30340;&#22686;&#38271;&#65292;&#30740;&#31350;&#30028;&#23545;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#20852;&#36259;&#21457;&#29983;&#20102;&#33539;&#24335;&#36716;&#21464;&#12290;&#36817;&#24180;&#26469;&#65292;Deep Sets&#21644;Transformers&#31561;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#20986;&#29616;&#22312;&#22788;&#29702;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#26550;&#26500;&#19987;&#38376;&#35774;&#35745;&#20026;&#33258;&#28982;&#23481;&#32435;&#38598;&#21512;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#22788;&#29702;&#38598;&#21512;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#22823;&#37327;&#33268;&#21147;&#20110;&#25506;&#32034;&#21644;&#21033;&#29992;&#36825;&#20123;&#26550;&#26500;&#33021;&#21147;&#30340;&#30740;&#31350;&#21162;&#21147;&#65292;&#20197;&#36924;&#36817;&#38598;&#21512;&#20989;&#25968;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#26088;&#22312;&#27010;&#36848;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17410v1 Announce Type: cross  Abstract: Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17407</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;
&lt;/p&gt;
&lt;p&gt;
Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26412;&#21040;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#30340;&#20934;&#30830;&#36716;&#24405;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#38899;&#38901;&#23398;&#21644;&#35821;&#22659;&#30456;&#20851;&#30340;&#38899;&#21464;&#12290;&#23545;&#20110;&#21306;&#22495;&#23391;&#21152;&#25289;&#26041;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26041;&#35328;&#30340;&#26631;&#20934;&#25340;&#20889;&#32422;&#23450;&#12289;&#24403;&#22320;&#21644;&#22806;&#35821;&#22312;&#36825;&#20123;&#22320;&#21306;&#20013;&#27969;&#34892;&#30340;&#35789;&#27719;&#20197;&#21450;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#38899;&#38901;&#22810;&#26679;&#24615;&#65292;&#36825;&#19968;&#25361;&#25112;&#29978;&#33267;&#26356;&#20026;&#20005;&#23803;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#35206;&#30422;&#23391;&#21152;&#25289;&#22269;&#20845;&#20010;&#22320;&#21306;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#24341;&#20837;&#8220;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#8221;&#65288;DGT&#65289;&#25216;&#26415;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#29983;&#25104;IPA&#36716;&#24405;&#20043;&#21069;&#21521;&#27169;&#22411;&#25552;&#20379;&#26377;&#20851;&#36755;&#20837;&#25991;&#26412;&#30340;&#21306;&#22495;&#26041;&#35328;&#25110;&#8220;&#22320;&#21306;&#8221;&#30340;&#26126;&#30830;&#20449;&#24687;&#12290;&#36825;&#36890;&#36807;&#22312;&#36755;&#20837;&#24207;&#21015;&#21069;&#28155;&#21152;&#19968;&#20010;&#22320;&#21306;&#26631;&#35760;&#26469;&#23454;&#29616;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#31471;&#21040;&#31471;&#36923;&#36753;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24067;&#23572;&#32593;&#32476;&#65292;&#24182;&#24212;&#29992;&#20102;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.17395</link><description>&lt;p&gt;
&#19968;&#20010;&#24320;&#28304;&#30340;&#31471;&#21040;&#31471;&#36923;&#36753;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#24067;&#23572;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
An Open-source End-to-End Logic Optimization Framework for Large-scale Boolean Network with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17395
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#31471;&#21040;&#31471;&#36923;&#36753;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24067;&#23572;&#32593;&#32476;&#65292;&#24182;&#24212;&#29992;&#20102;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#24067;&#23572;&#32593;&#32476;&#30340;&#24320;&#28304;&#31471;&#21040;&#31471;&#36923;&#36753;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17395v1 Announce Type: new  Abstract: We propose an open-source end-to-end logic optimization framework for large-scale boolean network with reinforcement learning.
&lt;/p&gt;</description></item><item><title>ELLEN&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#19982;&#35821;&#35328;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#22312;&#26497;&#20854;&#36731;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#38750;&#24120;&#24378;&#21170;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17385</link><description>&lt;p&gt;
ELLEN: &#38750;&#24120;&#36731;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#39640;&#25928;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17385
&lt;/p&gt;
&lt;p&gt;
ELLEN&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#19982;&#35821;&#35328;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#22312;&#26497;&#20854;&#36731;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#38750;&#24120;&#24378;&#21170;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21322;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#38382;&#39064;&#65292;&#20391;&#37325;&#20110;&#26497;&#20854;&#36731;&#37327;&#32423;&#30340;&#30417;&#30563;&#65292;&#21253;&#25324;&#20165;&#21253;&#21547;&#27599;&#31867;&#21035;10&#20010;&#31034;&#20363;&#30340;&#35789;&#27719;&#34920;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ELLEN&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#23436;&#20840;&#27169;&#22359;&#21270;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23427;&#23558;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#35821;&#35328;&#35268;&#21017;&#30456;&#32467;&#21512;&#12290;&#36825;&#20123;&#35268;&#21017;&#21253;&#25324;&#8220;&#19968;&#20010;&#35805;&#35821;&#19968;&#20010;&#24847;&#20041;&#8221;&#36825;&#26679;&#30340;&#35265;&#35299;&#65292;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26080;&#30417;&#30563;NER&#65292;&#21033;&#29992;&#35789;&#24615;&#26631;&#31614;&#35782;&#21035;&#21644;&#28040;&#38500;&#26410;&#26631;&#35760;&#23454;&#20307;&#20316;&#20026;&#20551;&#36127;&#20363;&#65292;&#20197;&#21450;&#20851;&#20110;&#20998;&#31867;&#22120;&#32622;&#20449;&#24230;&#24471;&#20998;&#22312;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#19979;&#30340;&#20854;&#20182;&#30452;&#35273;&#12290;&#22312;&#20351;&#29992;&#19978;&#36848;&#35789;&#27719;&#34920;&#26497;&#23567;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;ELLEN&#22312;CoNLL-2003&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#23427;&#36824;&#22312;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#30456;&#21516;&#30417;&#30563;&#35774;&#32622;&#65288;&#21363;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;5%&#65289;&#19979;&#65292;&#20248;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#65288;&#19988;&#26356;&#20026;&#22797;&#26434;&#65289;&#30340;&#21322;&#30417;&#30563;NER&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17385v1 Announce Type: cross  Abstract: In this work, we revisit the problem of semi-supervised named entity recognition (NER) focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class. We introduce ELLEN, a simple, fully modular, neuro-symbolic method that blends fine-tuned language models with linguistic rules. These rules include insights such as ''One Sense Per Discourse'', using a Masked Language Model as an unsupervised NER, leveraging part-of-speech tags to identify and eliminate unlabeled entities as false negatives, and other intuitions about classifier confidence scores in local and global context. ELLEN achieves very strong performance on the CoNLL-2003 dataset when using the minimal supervision from the lexicon above. It also outperforms most existing (and considerably more complex) semi-supervised NER methods under the same supervision settings commonly used in the literature (i.e., 5% of the training data). Further, 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20998;&#26512;&#35266;&#27979;&#22312;&#22823;&#27668;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#35266;&#27979;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#25552;&#39640;&#20102;&#23545;&#22825;&#27668;&#39044;&#25253;&#20013;&#35266;&#27979;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.17384</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22823;&#27668;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#35266;&#27979;&#24433;&#21709;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17384
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20998;&#26512;&#35266;&#27979;&#22312;&#22823;&#27668;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#35266;&#27979;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#25552;&#39640;&#20102;&#23545;&#22825;&#27668;&#39044;&#25253;&#20013;&#35266;&#27979;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#35266;&#27979;&#23545;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#20013;&#22823;&#27668;&#29366;&#24577;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#35266;&#27979;&#21644;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#28857;&#38598;&#25104;&#21040;&#27668;&#35937;&#22270;&#20013;&#65292;&#25552;&#21462;&#20197;NWP&#28857;&#20026;&#20013;&#24515;&#30340;$k$-&#36339;&#23376;&#22270;&#12290;&#37319;&#29992;&#33258;&#30417;&#30563;GNN&#26469;&#36890;&#36807;&#32858;&#21512;&#36825;&#20123;$k$-&#36339;&#21322;&#24452;&#20869;&#30340;&#25968;&#25454;&#26469;&#20272;&#35745;&#22823;&#27668;&#29366;&#24577;&#12290;&#30740;&#31350;&#24212;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#37327;&#21270;&#19981;&#21516;&#35266;&#27979;&#22312;&#20272;&#35745;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;11&#20010;&#21355;&#26143;&#21644;&#38470;&#22320;&#35266;&#27979;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#31361;&#20986;&#20102;&#35270;&#35273;&#21270;&#35266;&#27979;&#31867;&#22411;&#37325;&#35201;&#24615;&#30340;&#26377;&#25928;&#24615;&#65292;&#22686;&#24378;&#20102;&#23545;&#22825;&#27668;&#39044;&#25253;&#20013;&#35266;&#27979;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17384v1 Announce Type: new  Abstract: This paper investigates the impact of observations on atmospheric state estimation in weather forecasting systems using graph neural networks (GNNs) and explainability methods. We integrate observation and Numerical Weather Prediction (NWP) points into a meteorological graph, extracting $k$-hop subgraphs centered on NWP points. Self-supervised GNNs are employed to estimate the atmospheric state by aggregating data within these $k$-hop radii. The study applies gradient-based explainability methods to quantify the significance of different observations in the estimation process. Evaluated with data from 11 satellite and land-based observations, the results highlight the effectiveness of visualizing the importance of observation types, enhancing the understanding and optimization of observational data in weather forecasting.
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;&#39537;&#21160;&#30740;&#31350;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#21487;&#20197;&#19982;&#26041;&#27861;&#39537;&#21160;&#30740;&#31350;&#26377;&#30410;&#22320;&#21327;&#21516;&#65292;&#20294;&#30446;&#21069;&#23457;&#26597;&#12289;&#25307;&#32856;&#21644;&#25945;&#23398;&#23454;&#36341;&#24448;&#24448;&#38459;&#30861;&#20102;&#36825;&#31181;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2403.17381</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#39537;&#21160;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
Application-Driven Innovation in Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17381
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#39537;&#21160;&#30740;&#31350;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#21487;&#20197;&#19982;&#26041;&#27861;&#39537;&#21160;&#30740;&#31350;&#26377;&#30410;&#22320;&#21327;&#21516;&#65292;&#20294;&#30446;&#21069;&#23457;&#26597;&#12289;&#25307;&#32856;&#21644;&#25945;&#23398;&#23454;&#36341;&#24448;&#24448;&#38459;&#30861;&#20102;&#36825;&#31181;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#21463;&#29305;&#23450;&#29616;&#23454;&#25361;&#25112;&#21551;&#21457;&#30340;&#21019;&#26032;&#31639;&#27861;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#36825;&#26679;&#30340;&#24037;&#20316;&#19981;&#20165;&#22312;&#24212;&#29992;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20063;&#22312;&#26426;&#22120;&#23398;&#20064;&#26412;&#36523;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;&#39537;&#21160;&#30740;&#31350;&#30340;&#33539;&#24335;&#65292;&#23558;&#20854;&#19982;&#26356;&#26631;&#20934;&#30340;&#26041;&#27861;&#39537;&#21160;&#30740;&#31350;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#24212;&#29992;&#39537;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#21487;&#20197;&#19982;&#26041;&#27861;&#39537;&#21160;&#24037;&#20316;&#26377;&#30410;&#22320;&#21327;&#21516;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#22909;&#22788;&#65292;&#25105;&#20204;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23457;&#26597;&#12289;&#25307;&#32856;&#21644;&#25945;&#23398;&#23454;&#36341;&#24448;&#24448;&#38459;&#30861;&#20102;&#24212;&#29992;&#39537;&#21160;&#21019;&#26032;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#22914;&#20309;&#25913;&#36827;&#36825;&#20123;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17381v1 Announce Type: cross  Abstract: As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important. Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself. In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research. We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work. Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation. We outline how these processes may be improved.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#38899;&#39057;&#30340;&#24773;&#24863;&#20998;&#26512;&#22312;&#38899;&#20048;&#20013;&#30340;&#36816;&#29992;&#65292;&#36890;&#36807;&#39044;&#27979;&#38899;&#20048;&#29255;&#27573;&#38543;&#26102;&#38388;&#30340;&#24773;&#24863;&#21464;&#21270;&#20197;&#21450;&#30830;&#23450;&#38899;&#20048;&#26102;&#38388;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#24773;&#24863;&#20540;&#26469;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#12290;</title><link>https://arxiv.org/abs/2403.17379</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#24212;&#29992;&#22522;&#20110;&#38899;&#39057;&#30340;&#24773;&#24863;&#20998;&#26512;&#22312;&#38899;&#20048;&#20013;
&lt;/p&gt;
&lt;p&gt;
Exploring and Applying Audio-Based Sentiment Analysis in Music
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#38899;&#39057;&#30340;&#24773;&#24863;&#20998;&#26512;&#22312;&#38899;&#20048;&#20013;&#30340;&#36816;&#29992;&#65292;&#36890;&#36807;&#39044;&#27979;&#38899;&#20048;&#29255;&#27573;&#38543;&#26102;&#38388;&#30340;&#24773;&#24863;&#21464;&#21270;&#20197;&#21450;&#30830;&#23450;&#38899;&#20048;&#26102;&#38388;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#24773;&#24863;&#20540;&#26469;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#25991;&#26412;&#22788;&#29702;&#20013;&#19981;&#26029;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#28041;&#21450;&#23545;&#25991;&#26412;&#30340;&#24847;&#35265;&#12289;&#24773;&#24863;&#21644;&#20027;&#35266;&#24615;&#30340;&#35745;&#31639;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#24819;&#27861;&#19981;&#20165;&#38480;&#20110;&#25991;&#26412;&#21644;&#35821;&#38899;&#65292;&#20107;&#23454;&#19978;&#65292;&#23427;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#24418;&#24335;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#31867;&#22312;&#38899;&#20048;&#20013;&#34920;&#36798;&#33258;&#24049;&#30340;&#28145;&#24230;&#19981;&#22914;&#22312;&#25991;&#26412;&#20013;&#12290;&#35745;&#31639;&#27169;&#22411;&#35299;&#37322;&#38899;&#20048;&#24773;&#24863;&#30340;&#33021;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#21487;&#33021;&#23545;&#27835;&#30103;&#21644;&#38899;&#20048;&#25773;&#25918;&#31561;&#26041;&#38754;&#20135;&#29983;&#24433;&#21709;&#21644;&#29992;&#36884;&#12290;&#26412;&#25991;&#28041;&#21450;&#20004;&#20010;&#29420;&#31435;&#20219;&#21153;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;(1)&#39044;&#27979;&#38899;&#20048;&#29255;&#27573;&#38543;&#26102;&#38388;&#30340;&#24773;&#24863;&#65292;&#20197;&#21450;(2)&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30830;&#23450;&#38899;&#20048;&#21518;&#30340;&#19979;&#19968;&#20010;&#24773;&#24863;&#20540;&#65292;&#20197;&#30830;&#20445;&#26080;&#32541;&#36807;&#28193;&#12290;&#21033;&#29992;&#21253;&#21547;&#20174;Free Music Archive&#20013;&#36873;&#20013;&#24182;&#29992;Russel&#30340;af&#22278;&#29615;&#27169;&#22411;&#25253;&#21578;&#30340;&#24841;&#24742;&#21644;&#28608;&#27963;&#27700;&#24179;&#27880;&#37322;&#30340;&#27468;&#26354;&#29255;&#27573;&#30340;Emotions in Music&#25968;&#25454;&#24211;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17379v1 Announce Type: cross  Abstract: Sentiment analysis is a continuously explored area of text processing that deals with the computational analysis of opinions, sentiments, and subjectivity of text. However, this idea is not limited to text and speech, in fact, it could be applied to other modalities. In reality, humans do not express themselves in text as deeply as they do in music. The ability of a computational model to interpret musical emotions is largely unexplored and could have implications and uses in therapy and musical queuing. In this paper, two individual tasks are addressed. This study seeks to (1) predict the emotion of a musical clip over time and (2) determine the next emotion value after the music in a time series to ensure seamless transitions. Utilizing data from the Emotions in Music Database, which contains clips of songs selected from the Free Music Archive annotated with levels of valence and arousal as reported on Russel's circumplex model of af
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#30340;&#26032;&#22411;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955; U-Net &#20013;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#20174;&#32780;&#22312;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35774;&#32622;&#19979;&#25913;&#21892;&#25193;&#25955;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.17377</link><description>&lt;p&gt;
&#20855;&#26377;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#33258;&#30699;&#27491;&#25193;&#25955;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17377
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#30340;&#26032;&#22411;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955; U-Net &#20013;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#20174;&#32780;&#22312;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35774;&#32622;&#19979;&#25913;&#21892;&#25193;&#25955;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20294;&#20854;&#36136;&#37327;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#27604;&#22914;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CG&#65289;&#21644;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CFG&#65289;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#25110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#22914;&#22270;&#20687;&#24674;&#22797;&#20013;&#26080;&#27861;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#31216;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#65292;&#23427;&#25913;&#36827;&#20102;&#25193;&#25955;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#19981;&#31649;&#26159;&#22312;&#26080;&#26465;&#20214;&#36824;&#26159;&#26377;&#26465;&#20214;&#30340;&#35774;&#32622;&#20013;&#65292;&#37117;&#33021;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#25110;&#25972;&#21512;&#22806;&#37096;&#27169;&#22359;&#12290;PAG &#26088;&#22312;&#36890;&#36807;&#25972;&#20010;&#21435;&#22122;&#36807;&#31243;&#36880;&#27493;&#22686;&#24378;&#26679;&#26412;&#30340;&#32467;&#26500;&#12290;&#23427;&#28041;&#21450;&#36890;&#36807;&#29992;&#24658;&#31561;&#30697;&#38453;&#26367;&#25442;&#25193;&#25955; U-Net &#20013;&#36873;&#25321;&#30340;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#32771;&#34385;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17377v1 Announce Type: cross  Abstract: Recent studies have demonstrated that diffusion models are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG). These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration. In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves diffusion sample quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules. PAG is designed to progressively enhance the structure of samples throughout the denoising process. It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, by considering the self-attention mechanisms
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#21160;&#25968;&#25454;&#24341;&#25806;&#65288;AIDE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#38382;&#39064;&#12289;&#39640;&#25928;&#31579;&#36873;&#25968;&#25454;&#12289;&#33258;&#21160;&#26631;&#27880;&#25913;&#36827;&#27169;&#22411;&#12289;&#29983;&#25104;&#22810;&#26679;&#21270;&#22330;&#26223;&#39564;&#35777;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17373</link><description>&lt;p&gt;
AIDE&#65306;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30446;&#26631;&#26816;&#27979;&#30340;&#33258;&#21160;&#25968;&#25454;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17373
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#21160;&#25968;&#25454;&#24341;&#25806;&#65288;AIDE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#38382;&#39064;&#12289;&#39640;&#25928;&#31579;&#36873;&#25968;&#25454;&#12289;&#33258;&#21160;&#26631;&#27880;&#25913;&#36827;&#27169;&#22411;&#12289;&#29983;&#25104;&#22810;&#26679;&#21270;&#22330;&#26223;&#39564;&#35777;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17373v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#31995;&#32479;&#20381;&#36182;&#31283;&#20581;&#30340;&#24863;&#30693;&#27169;&#22411;&#20316;&#20026;&#23433;&#20840;&#20445;&#38556;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#22312;&#36947;&#36335;&#19978;&#36935;&#21040;&#30340;&#29289;&#20307;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#65292;&#32597;&#35265;&#25110;&#26410;&#35265;&#31867;&#21035;&#23545;&#37096;&#32626;&#30340;&#24863;&#30693;&#27169;&#22411;&#26500;&#25104;&#25361;&#25112;&#12290;&#36825;&#38656;&#35201;&#36890;&#36807;&#26114;&#36149;&#30340;&#36807;&#31243;&#19981;&#26029;&#22320;&#31579;&#36873;&#21644;&#26631;&#27880;&#25968;&#25454;&#65292;&#38656;&#35201;&#20154;&#21147;&#30340;&#24040;&#22823;&#25237;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#35774;&#35745;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#24341;&#25806;&#65288;AIDE&#65289;&#65292;&#33258;&#21160;&#35782;&#21035;&#38382;&#39064;&#65292;&#39640;&#25928;&#31579;&#36873;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#21160;&#26631;&#27880;&#25913;&#36827;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#22330;&#26223;&#39564;&#35777;&#27169;&#22411;&#12290;&#36825;&#19968;&#36807;&#31243;&#21487;&#20197;&#36845;&#20195;&#36827;&#34892;&#65292;&#20801;&#35768;&#27169;&#22411;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#25918;&#19990;&#30028;&#26816;&#27979;&#22522;&#20934;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#21508;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;AV&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#22312;&#38477;&#20302;&#26041;&#38754;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17373v1 Announce Type: cross  Abstract: Autonomous vehicle (AV) systems rely on robust perception models as a cornerstone of safety assurance. However, objects encountered on the road exhibit a long-tailed distribution, with rare or unseen categories posing challenges to a deployed perception model. This necessitates an expensive process of continuously curating and annotating data with significant human effort. We propose to leverage recent advances in vision-language and large language models to design an Automatic Data Engine (AIDE) that automatically identifies issues, efficiently curates data, improves the model through auto-labeling, and verifies the model through generation of diverse scenarios. This process operates iteratively, allowing for continuous self-improvement of the model. We further establish a benchmark for open-world detection on AV datasets to comprehensively evaluate various learning paradigms, demonstrating our method's superior performance at a reduc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;ChatGPT&#22312;&#19981;&#21516;&#23610;&#24230;&#19979;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#36739;&#31895;&#31890;&#24230;&#30340;&#23610;&#24230;&#19978;&#65292;ChatGPT&#19982;&#20154;&#31867;&#26356;&#21152;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2403.17368</link><description>&lt;p&gt;
ChatGPT&#23558;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36136;&#37327;&#35780;&#32423;&#23450;&#20026;&#19982;&#20154;&#31867;&#30456;&#20284;&#65306;&#20294;&#26159;&#22522;&#20110;&#21738;&#20123;&#26631;&#20934;&#65311;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;ChatGPT&#22312;&#19981;&#21516;&#23610;&#24230;&#19979;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#36739;&#31895;&#31890;&#24230;&#30340;&#23610;&#24230;&#19978;&#65292;ChatGPT&#19982;&#20154;&#31867;&#26356;&#21152;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#24615;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#38271;&#12290;&#34429;&#28982;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65288;NLEs&#65289;&#23545;&#28548;&#28165;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#32972;&#21518;&#30340;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#36807;&#20154;&#31867;&#21028;&#26029;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#30001;&#20110;&#20027;&#35266;&#24615;&#21644;&#23545;&#32454;&#31890;&#24230;&#35780;&#20998;&#30340;&#38656;&#27714;&#32780;&#21464;&#24471;&#22797;&#26434;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#22312;&#22810;&#20010;&#23610;&#24230;&#65288;&#21363;&#20108;&#20803;&#12289;&#19977;&#20803;&#21644;7-Likert&#23610;&#24230;&#65289;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20174;&#19977;&#20010;NLE&#25968;&#25454;&#38598;&#20013;&#25277;&#21462;300&#20010;&#25968;&#25454;&#23454;&#20363;&#65292;&#24182;&#20026;&#20449;&#24687;&#37327;&#21644;&#28165;&#26224;&#24230;&#20004;&#20010;&#25991;&#26412;&#36136;&#37327;&#24230;&#37327;&#25910;&#38598;&#20102;900&#20010;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#19981;&#21516;&#20027;&#35266;&#35780;&#20998;&#33539;&#22260;&#19979;&#36827;&#34892;&#20102;&#25104;&#23545;&#27604;&#36739;&#23454;&#39564;&#65292;&#20854;&#20013;&#22522;&#32447;&#26469;&#33258;8,346&#20010;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26356;&#31895;&#31890;&#24230;&#30340;&#23610;&#24230;&#19978;&#65292;ChatGPT&#19982;&#20154;&#31867;&#26356;&#21152;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25104;&#23545;&#27604;&#36739;&#21644;&#21160;&#24577;&#25552;&#31034;&#65288;&#21363;&#25552;&#20379;&#35821;&#20041;&#19978;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17368v1 Announce Type: cross  Abstract: As AI becomes more integral in our lives, the need for transparency and responsibility grows. While natural language explanations (NLEs) are vital for clarifying the reasoning behind AI decisions, evaluating them through human judgments is complex and resource-intensive due to subjectivity and the need for fine-grained ratings. This study explores the alignment between ChatGPT and human assessments across multiple scales (i.e., binary, ternary, and 7-Likert scale). We sample 300 data instances from three NLE datasets and collect 900 human annotations for both informativeness and clarity scores as the text quality measurement. We further conduct paired comparison experiments under different ranges of subjectivity scores, where the baseline comes from 8,346 human annotations. Our results show that ChatGPT aligns better with humans in more coarse-grained scales. Also, paired comparisons and dynamic prompting (i.e., providing semantically 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#36830;&#25509;&#65292;&#20445;&#30041;&#21407;&#22987;&#35777;&#25454;&#30340;&#19978;&#19979;&#25991;&#65292;&#30830;&#20445;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17361</link><description>&lt;p&gt;
&#23558;&#25991;&#26412;&#21644;&#34920;&#26684;&#19990;&#30028;&#32852;&#31995;&#36215;&#26469;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17361
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#36830;&#25509;&#65292;&#20445;&#30041;&#21407;&#22987;&#35777;&#25454;&#30340;&#19978;&#19979;&#25991;&#65292;&#30830;&#20445;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
FEVEROUS&#26159;&#19968;&#20010;&#20851;&#27880;&#28041;&#21450;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#20219;&#21153;&#30340;&#22522;&#20934;&#21644;&#30740;&#31350;&#39033;&#30446;&#12290;&#22312;FEVEROUS&#20013;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#39044;&#22788;&#29702;&#24182;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#25968;&#25454;&#36716;&#25442;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#19978;&#19979;&#25991;&#20002;&#22833;&#25110;&#35823;&#23548;&#24615;&#32534;&#30721;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#28040;&#38500;&#20102;&#27169;&#24577;&#36716;&#25442;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#21407;&#22987;&#35777;&#25454;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#36830;&#25509;&#65292;&#20174;&#32780;&#20135;&#29983;&#20840;&#38754;&#19988;&#21487;&#38752;&#30340;&#21028;&#26029;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#24039;&#22937;&#22320;&#31649;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#30830;&#20445;&#21407;&#22987;&#35777;&#25454;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#19981;&#21463;&#25439;&#23475;&#12290;&#27604;&#36739;&#20998;&#26512;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17361v1 Announce Type: cross  Abstract: FEVEROUS is a benchmark and research initiative focused on fact extraction and verification tasks involving unstructured text and structured tabular data. In FEVEROUS, existing works often rely on extensive preprocessing and utilize rule-based transformations of data, leading to potential context loss or misleading encodings. This paper introduces a simple yet powerful model that nullifies the need for modality conversion, thereby preserving the original evidence's context. By leveraging pre-trained models on diverse text and tabular datasets and by incorporating a lightweight attention-based mechanism, our approach efficiently exploits latent connections between different data types, thereby yielding comprehensive and reliable verdict predictions. The model's modular structure adeptly manages multi-modal information, ensuring the integrity and authenticity of the original evidence are uncompromised. Comparative analyses reveal that ou
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21382;&#21490;&#20381;&#36182;&#24615;&#21452;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#36882;&#24402;&#21452;&#21521;&#19978;&#21319;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#22823;&#22411;&#21463;&#38480;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#25913;&#36827;&#20102;&#25506;&#32034;&#33021;&#21147;&#21644;&#32467;&#26524;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17358</link><description>&lt;p&gt;
&#22788;&#29702;&#20855;&#26377;&#36882;&#24402;&#21452;&#19978;&#21319;&#30340;&#36817;&#35270;&#21463;&#38480;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Addressing Myopic Constrained POMDP Planning with Recursive Dual Ascent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17358
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21382;&#21490;&#20381;&#36182;&#24615;&#21452;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#36882;&#24402;&#21452;&#21521;&#19978;&#21319;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#22823;&#22411;&#21463;&#38480;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#25913;&#36827;&#20102;&#25506;&#32034;&#33021;&#21147;&#21644;&#32467;&#26524;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lagrangian-guided Monte Carlo tree search with global dual ascent&#24050;&#24212;&#29992;&#20110;&#22312;&#32447;&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#21463;&#38480;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CPOMDPs&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#20840;&#23616;&#21452;&#19978;&#21319;&#21442;&#25968;&#21487;&#33021;&#23548;&#33268;&#25506;&#32034;&#26399;&#38388;&#30340;&#36817;&#35270;&#34892;&#20026;&#36873;&#25321;&#65292;&#26368;&#32456;&#23548;&#33268;&#27425;&#20248;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24341;&#23548;&#26412;&#22320;&#21160;&#20316;&#36873;&#25321;&#24182;&#36890;&#36807;&#36882;&#24402;&#21452;&#21521;&#19978;&#21319;&#36827;&#34892;&#20248;&#21270;&#30340;&#21382;&#21490;&#20381;&#36182;&#24615;&#21452;&#21464;&#37327;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#28608;&#21169;&#24615;&#30340;&#29609;&#20855;&#31034;&#20363;&#21644;&#20004;&#20010;&#22823;&#22411;CPOMDP&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#26368;&#32456;&#23548;&#33268;&#26356;&#23433;&#20840;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17358v1 Announce Type: new  Abstract: Lagrangian-guided Monte Carlo tree search with global dual ascent has been applied to solve large constrained partially observable Markov decision processes (CPOMDPs) online. In this work, we demonstrate that these global dual parameters can lead to myopic action selection during exploration, ultimately leading to suboptimal decision making. To address this, we introduce history-dependent dual variables that guide local action selection and are optimized with recursive dual ascent. We empirically compare the performance of our approach on a motivating toy example and two large CPOMDPs, demonstrating improved exploration, and ultimately, safer outcomes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;MESIA&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#27880;&#37322;&#25552;&#20379;&#30340;&#34917;&#20805;&#20449;&#24687;&#30340;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26041;&#27861;&#32423;&#27880;&#37322;&#26102;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.17357</link><description>&lt;p&gt;
MESIA: &#29702;&#35299;&#21644;&#21033;&#29992;&#26041;&#27861;&#32423;&#27880;&#37322;&#30340;&#34917;&#20805;&#24615;&#36136;&#29992;&#20110;&#29983;&#25104;&#33258;&#21160;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17357
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;MESIA&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#27880;&#37322;&#25552;&#20379;&#30340;&#34917;&#20805;&#20449;&#24687;&#30340;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26041;&#27861;&#32423;&#27880;&#37322;&#26102;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#27880;&#37322;&#23545;&#20110;&#24320;&#21457;&#20154;&#21592;&#22312;&#31243;&#24207;&#29702;&#35299;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#29702;&#35299;&#21644;&#37325;&#29992;&#26041;&#27861;&#30340;&#22330;&#26223;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#26399;&#26395;&#20195;&#30721;&#27880;&#37322;&#25552;&#20379;&#36229;&#20986;&#26041;&#27861;&#31614;&#21517;&#30340;&#34917;&#20805;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#20195;&#30721;&#27880;&#37322;&#20013;&#36825;&#31181;&#34917;&#20805;&#20449;&#24687;&#30340;&#31243;&#24230;&#24046;&#24322;&#24456;&#22823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#27861;&#32423;&#27880;&#37322;&#30340;&#34917;&#20805;&#24615;&#36136;&#30340;&#35748;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MESIA&#65288;&#24179;&#22343;&#34917;&#20805;&#20449;&#24687;&#37327;&#65289;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#27880;&#37322;&#25552;&#20379;&#30340;&#34917;&#20805;&#20449;&#24687;&#30340;&#31243;&#24230;&#12290;&#20511;&#21161;MESIA&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#20195;&#30721;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#24120;&#35265;&#31867;&#22411;&#30340;&#31070;&#32463;&#26041;&#27861;&#29983;&#25104;&#26041;&#27861;&#32423;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24037;&#20316;&#30340;&#20215;&#20540;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17357v1 Announce Type: cross  Abstract: Code comments are important for developers in program comprehension. In scenarios of comprehending and reusing a method, developers expect code comments to provide supplementary information beyond the method signature. However, the extent of such supplementary information varies a lot in different code comments. In this paper, we raise the awareness of the supplementary nature of method-level comments and propose a new metric named MESIA (Mean Supplementary Information Amount) to assess the extent of supplementary information that a code comment can provide. With the MESIA metric, we conduct experiments on a popular code-comment dataset and three common types of neural approaches to generate method-level comments. Our experimental results demonstrate the value of our proposed work with a number of findings. (1) Small-MESIA comments occupy around 20% of the dataset and mostly fall into only the WHAT comment category. (2) Being able to p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#25551;&#36848;&#20102;Zodiac&#26432;&#25163;&#31532;&#20108;&#20010;&#23494;&#30721;&#30340;&#35299;&#23494;&#36807;&#31243;&#65292;&#35813;&#23494;&#30721;&#20351;&#29992;&#20102;&#36716;&#20301;&#21644;&#21516;&#38899;&#26367;&#25442;&#23494;&#30721;&#65292;&#24182;&#20855;&#26377;&#19981;&#23547;&#24120;&#29305;&#36136;&#12290;</title><link>https://arxiv.org/abs/2403.17350</link><description>&lt;p&gt;
Zodiac&#26432;&#25163;340&#23383;&#31526;&#23494;&#30721;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
The Solution of the Zodiac Killer's 340-Character Cipher
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#25551;&#36848;&#20102;Zodiac&#26432;&#25163;&#31532;&#20108;&#20010;&#23494;&#30721;&#30340;&#35299;&#23494;&#36807;&#31243;&#65292;&#35813;&#23494;&#30721;&#20351;&#29992;&#20102;&#36716;&#20301;&#21644;&#21516;&#38899;&#26367;&#25442;&#23494;&#30721;&#65292;&#24182;&#20855;&#26377;&#19981;&#23547;&#24120;&#29305;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Zodiac&#26432;&#25163;&#26696;&#20214;&#26159;&#21382;&#21490;&#19978;&#26368;&#24191;&#20026;&#20154;&#30693;&#30340;&#26410;&#35299;&#20915;&#36830;&#29615;&#26432;&#25163;&#26696;&#20214;&#20043;&#19968;&#12290;&#36825;&#21517;&#36523;&#20221;&#19981;&#26126;&#30340;&#20982;&#25163;&#26432;&#23475;&#20102;&#20116;&#21517;&#24050;&#30693;&#21463;&#23475;&#32773;&#65292;&#24182;&#24656;&#21523;&#20102;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#12290;&#20182;&#36824;&#19982;&#26032;&#38395;&#30028;&#21644;&#25191;&#27861;&#37096;&#38376;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27807;&#36890;&#12290;&#38500;&#20102;&#20182;&#30340;&#35851;&#26432;&#34892;&#20026;&#22806;&#65292;Zodiac&#36824;&#22240;&#20351;&#29992;&#23494;&#30721;&#32780;&#38395;&#21517;&#12290;&#31532;&#19968;&#20010;Zodiac&#23494;&#30721;&#22312;&#21457;&#24067;&#21518;&#19968;&#21608;&#20869;&#34987;&#35299;&#20915;&#65292;&#32780;&#31532;&#20108;&#20010;&#23494;&#30721;&#22312;51&#24180;&#21518;&#34987;&#20316;&#32773;&#35299;&#20915;&#65292;&#21457;&#29616;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#19981;&#23547;&#24120;&#29305;&#36136;&#30340;&#36716;&#20301;&#21644;&#21516;&#38899;&#26367;&#25442;&#23494;&#30721;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#20010;&#23494;&#30721;&#30340;&#21382;&#21490;&#24847;&#20041;&#20197;&#21450;&#23548;&#33268;&#20854;&#35299;&#23494;&#30340;&#35832;&#22810;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17350v1 Announce Type: new  Abstract: The case of the Zodiac Killer is one of the most widely known unsolved serial killer cases in history. The unidentified killer murdered five known victims and terrorized the state of California. He also communicated extensively with the press and law enforcement. Besides his murders, Zodiac was known for his use of ciphers. The first Zodiac cipher was solved within a week of its publication, while the second cipher was solved by the authors after 51 years, when it was discovered to be a transposition and homophonic substitution cipher with unusual qualities. In this paper, we detail the historical significance of this cipher and the numerous efforts which culminated in its solution.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#35770;&#25991;&#20869;&#23481;&#29983;&#25104;&#22270;&#20687;&#26631;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;PaddleOCR&#21644;LLaMA&#24037;&#20855;&#21253;&#35299;&#20915;&#20102;OCR&#20449;&#24687;&#21644;&#20449;&#24687;&#36807;&#28388;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17342</link><description>&lt;p&gt;
ICCV 2023&#31532;&#19968;&#20010;&#31185;&#23398;&#22270;&#20687;&#23383;&#24149;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17342
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#35770;&#25991;&#20869;&#23481;&#29983;&#25104;&#22270;&#20687;&#26631;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;PaddleOCR&#21644;LLaMA&#24037;&#20855;&#21253;&#35299;&#20915;&#20102;OCR&#20449;&#24687;&#21644;&#20449;&#24687;&#36807;&#28388;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25913;&#21892;&#35770;&#25991;&#20013;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#37319;&#29992;&#24635;&#32467;&#35770;&#25991;&#20013;&#30340;&#25991;&#26412;&#20869;&#23481;&#29983;&#25104;&#22270;&#20687;&#26631;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23448;&#26041;&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#30340;OCR&#20449;&#24687;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;PaddleOCR&#24037;&#20855;&#21253;&#20174;&#25152;&#26377;&#22270;&#20687;&#20013;&#25552;&#21462;OCR&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23448;&#26041;&#25991;&#20214;&#20013;&#30340;&#26576;&#20123;&#25991;&#26412;&#20869;&#23481;&#19982;&#19981;&#36866;&#21512;&#36827;&#34892;&#26631;&#39064;&#22788;&#29702;&#30340;&#22270;&#20687;&#30456;&#20851;&#65292;&#20174;&#32780;&#22312;&#29983;&#25104;&#26631;&#39064;&#36807;&#31243;&#20013;&#24341;&#20837;&#22122;&#38899;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;LLaMA&#26681;&#25454;&#22270;&#20687;&#25552;&#21450;&#26597;&#35810;&#25991;&#26412;&#20869;&#23481;&#26469;&#25552;&#21462;&#29305;&#23450;&#20110;&#22270;&#20687;&#30340;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#36807;&#28388;&#25481;&#22810;&#20313;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#35201;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21450;&#29992;&#20110;&#35780;&#20272;&#30340;&#25351;&#26631;&#65288;&#22914;ROUGE&#65289;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17342v1 Announce Type: cross  Abstract: In this paper, we propose a solution for improving the quality of captions generated for figures in papers. We adopt the approach of summarizing the textual content in the paper to generate image captions. Throughout our study, we encounter discrepancies in the OCR information provided in the official dataset. To rectify this, we employ the PaddleOCR toolkit to extract OCR information from all images. Moreover, we observe that certain textual content in the official paper pertains to images that are not relevant for captioning, thereby introducing noise during caption generation. To mitigate this issue, we leverage LLaMA to extract image-specific information by querying the textual content based on image mentions, effectively filtering out extraneous information. Additionally, we recognize a discrepancy between the primary use of maximum likelihood estimation during text generation and the evaluation metrics such as ROUGE employed to a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#28378;&#21160;&#35270;&#37326;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#24615;&#33021;&#21644;&#21487;&#34892;&#24615;&#21463;&#24433;&#21709;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17338</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#28378;&#21160;&#35270;&#37326;&#25511;&#21046;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17338
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#28378;&#21160;&#35270;&#37326;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#24615;&#33021;&#21644;&#21487;&#34892;&#24615;&#21463;&#24433;&#21709;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#20026;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24456;&#23481;&#26131;&#21464;&#24471;&#26840;&#25163;&#12290;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(CBFs)&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#20986;&#29616;&#65292;&#36890;&#36807;&#20854;&#21069;&#21521;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#26377;&#21033;&#20110;&#36890;&#36807;&#22312;&#25439;&#22833;&#19968;&#20123;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#24335;&#22320;&#20445;&#35777;&#23433;&#20840;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23450;&#20041;&#24615;&#33021;&#30446;&#26631;&#20197;&#21450;&#24517;&#39035;&#22987;&#32456;&#25191;&#34892;&#30340;&#22522;&#20110;CBF&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#21487;&#33021;&#20250;&#23545;&#24615;&#33021;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65306;(i)&#25104;&#26412;&#20989;&#25968;&#21450;&#20854;&#30456;&#20851;&#21442;&#25968;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;(ii)&#22312;CBF&#32422;&#26463;&#20869;&#36827;&#34892;&#21442;&#25968;&#26657;&#20934;&#65292;&#25429;&#25417;&#24615;&#33021;&#21644;&#20445;&#23432;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#20197;&#21450;&#19981;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#24378;&#21270;&#23398;&#20064;(RL)&#28378;&#21160;&#35270;&#37326;&#25511;&#21046;(RHC)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17338v1 Announce Type: cross  Abstract: Optimal control methods provide solutions to safety-critical problems but easily become intractable. Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss. This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced. Unfortunately, both performance and solution feasibility can be significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness. %as well as infeasibility. To address these challenges, we propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#36861;&#27714;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#35299;&#20915;&#20559;&#35265;&#38382;&#39064;&#65292;&#30830;&#20445;&#27169;&#22411;&#19981;&#20250;&#26377;&#24847;&#25110;&#26080;&#24847;&#22320;&#23545;&#26576;&#20123;&#32676;&#20307;&#20135;&#29983;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.17333</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#36861;&#27714;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Pursuit of Fairness in Artificial Intelligence Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17333
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#36861;&#27714;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#35299;&#20915;&#20559;&#35265;&#38382;&#39064;&#65292;&#30830;&#20445;&#27169;&#22411;&#19981;&#20250;&#26377;&#24847;&#25110;&#26080;&#24847;&#22320;&#23545;&#26576;&#20123;&#32676;&#20307;&#20135;&#29983;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#30103;&#12289;&#25945;&#32946;&#21644;&#23601;&#19994;&#31561;&#26041;&#26041;&#38754;&#38754;&#12290;&#30001;&#20110;&#23427;&#20204;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#25935;&#24863;&#29615;&#22659;&#65292;&#24182;&#20570;&#20986;&#21487;&#33021;&#25913;&#21464;&#20154;&#29983;&#30340;&#20915;&#31574;&#65292;&#28508;&#22312;&#30340;&#20559;&#35265;&#32467;&#26524;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#24320;&#21457;&#20154;&#21592;&#24212;&#30830;&#20445;&#36825;&#31867;&#27169;&#22411;&#19981;&#20250;&#34920;&#29616;&#20986;&#20219;&#20309;&#24847;&#22806;&#30340;&#27495;&#35270;&#34892;&#20026;&#65292;&#27604;&#22914;&#20559;&#29233;&#26576;&#20123;&#24615;&#21035;&#12289;&#31181;&#26063;&#25110;&#27531;&#30142;&#20154;&#22763;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26222;&#36941;&#20256;&#25773;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#23545;&#19981;&#20844;&#24179;&#27169;&#22411;&#36234;&#26469;&#36234;&#26377;&#24847;&#35782;&#65292;&#24182;&#33268;&#21147;&#20110;&#20943;&#23569;&#20854;&#20013;&#30340;&#20559;&#35265;&#12290;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#37325;&#35201;&#30740;&#31350;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#19981;&#20250;&#26377;&#24847;&#25110;&#26080;&#24847;&#22320;&#24310;&#32493;&#20559;&#35265;&#12290;&#36825;&#39033;&#35843;&#26597;&#27010;&#36848;&#20102;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#21069;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20844;&#24179;&#24615;&#19981;&#21516;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17333v1 Announce Type: new  Abstract: Artificial Intelligence (AI) models are now being utilized in all facets of our lives such as healthcare, education and employment. Since they are used in numerous sensitive environments and make decisions that can be life altering, potential biased outcomes are a pressing matter. Developers should ensure that such models don't manifest any unexpected discriminatory practices like partiality for certain genders, ethnicities or disabled people. With the ubiquitous dissemination of AI systems, researchers and practitioners are becoming more aware of unfair models and are bound to mitigate bias in them. Significant research has been conducted in addressing such issues to ensure models don't intentionally or unintentionally perpetuate bias. This survey offers a synopsis of the different ways researchers have promoted fairness in AI systems. We explore the different definitions of fairness existing in the current literature. We create a compr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#20171;&#32461;&#20102;DeepKKT&#26465;&#20214;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#31867;&#20284;&#65292;&#20026;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.17329</link><description>&lt;p&gt;
&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Deep Support Vectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#20171;&#32461;&#20102;DeepKKT&#26465;&#20214;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#31867;&#20284;&#65292;&#20026;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#36890;&#24120;&#34987;&#24402;&#22240;&#20110;&#20854;&#19982;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#22312;&#29702;&#35770;&#19978;&#30340;&#31561;&#20215;&#24615;&#65292;&#20294;&#36825;&#31181;&#20851;&#31995;&#30340;&#23454;&#38469;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#22312;&#36825;&#19968;&#39046;&#22495;&#24320;&#23637;&#20102;&#19968;&#39033;&#25506;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeepKKT&#26465;&#20214;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#28145;&#24230;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#20256;&#32479;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#26465;&#20214;&#30340;&#35843;&#25972;&#29256;&#26412;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#30340;&#20999;&#23454;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#65292;&#31867;&#20284;&#20110;SVM&#20013;&#30340;&#36807;&#31243;&#12290;&#20195;&#30721;&#23558;&#20250;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17329v1 Announce Type: cross  Abstract: While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored. This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models. We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning. Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models. Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM. The code will be available.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20132;&#21449;&#36335;&#21475;&#30340;&#20449;&#21495;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#38454;&#27573;&#32039;&#24613;&#24615;&#27010;&#24565;&#21644;&#21487;&#35299;&#37322;&#30340;&#26641;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20449;&#21495;&#36716;&#25442;&#26399;&#38388;&#36873;&#25321;&#28608;&#27963;&#30340;&#20449;&#21495;&#30456;&#20301;&#12290;</title><link>https://arxiv.org/abs/2403.17328</link><description>&lt;p&gt;
&#36890;&#36807;&#36951;&#20256;&#32534;&#31243;&#23398;&#20064;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Traffic Signal Control via Genetic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17328
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20132;&#21449;&#36335;&#21475;&#30340;&#20449;&#21495;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#38454;&#27573;&#32039;&#24613;&#24615;&#27010;&#24565;&#21644;&#21487;&#35299;&#37322;&#30340;&#26641;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20449;&#21495;&#36716;&#25442;&#26399;&#38388;&#36873;&#25321;&#28608;&#27963;&#30340;&#20449;&#21495;&#30456;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#23545;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#65292;&#22312;&#23547;&#27714;&#26356;&#26377;&#25928;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31574;&#30053;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;DRL&#20013;&#22870;&#21169;&#30340;&#35774;&#35745;&#39640;&#24230;&#20381;&#36182;&#39046;&#22495;&#30693;&#35782;&#25165;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#31574;&#30053;&#65292;&#32780;&#26368;&#32456;&#31574;&#30053;&#20063;&#23384;&#22312;&#35299;&#37322;&#22256;&#38590;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#22797;&#26434;&#36335;&#21475;&#30340;&#20449;&#21495;&#25511;&#21046;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#20449;&#21495;&#30456;&#35774;&#35745;&#20102;&#19968;&#20010;&#38454;&#27573;&#32039;&#24613;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#20449;&#21495;&#21464;&#25442;&#26399;&#38388;&#65292;&#20132;&#36890;&#28783;&#25511;&#21046;&#31574;&#30053;&#26681;&#25454;&#38454;&#27573;&#32039;&#24613;&#24615;&#36873;&#25321;&#35201;&#28608;&#27963;&#30340;&#19979;&#19968;&#20010;&#30456;&#20301;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#32039;&#24613;&#21151;&#33021;&#34920;&#31034;&#20026;&#21487;&#35299;&#37322;&#30340;&#26641;&#32467;&#26500;&#12290;&#32039;&#24613;&#21151;&#33021;&#21487;&#20197;&#26681;&#25454;&#24403;&#21069;&#36947;&#36335;&#26465;&#20214;&#20026;&#29305;&#23450;&#30456;&#20301;&#35745;&#31639;&#30456;&#20301;&#32039;&#24613;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17328v1 Announce Type: new  Abstract: The control of traffic signals is crucial for improving transportation efficiency. Recently, learning-based methods, especially Deep Reinforcement Learning (DRL), garnered substantial success in the quest for more efficient traffic signal control strategies. However, the design of rewards in DRL highly demands domain knowledge to converge to an effective policy, and the final policy also presents difficulties in terms of explainability. In this work, a new learning-based method for signal control in complex intersections is proposed. In our approach, we design a concept of phase urgency for each signal phase. During signal transitions, the traffic light control strategy selects the next phase to be activated based on the phase urgency. We then proposed to represent the urgency function as an explainable tree structure. The urgency function can calculate the phase urgency for a specific phase based on the current road conditions. Genetic 
&lt;/p&gt;</description></item><item><title>JMultiWOZ&#26159;&#31532;&#19968;&#20010;&#26085;&#35821;&#22823;&#35268;&#27169;&#22810;&#39046;&#22495;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;&#19982;&#29616;&#26377;&#33521;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#30456;&#23218;&#32654;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#21644;&#22238;&#22797;&#29983;&#25104;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#26085;&#35821;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#19982;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.17319</link><description>&lt;p&gt;
JMultiWOZ&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#26085;&#35821;&#22810;&#39046;&#22495;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17319
&lt;/p&gt;
&lt;p&gt;
JMultiWOZ&#26159;&#31532;&#19968;&#20010;&#26085;&#35821;&#22823;&#35268;&#27169;&#22810;&#39046;&#22495;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;&#19982;&#29616;&#26377;&#33521;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#30456;&#23218;&#32654;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#21644;&#22238;&#22797;&#29983;&#25104;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#26085;&#35821;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#19982;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25968;&#25454;&#38598;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#33521;&#35821;&#22810;&#39046;&#22495;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#26174;&#33879;&#36827;&#23637;&#20570;&#20986;&#36129;&#29486;&#65292;&#20294;&#26085;&#35821;&#20013;&#24182;&#19981;&#23384;&#22312;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#19982;&#33521;&#35821;&#39046;&#22495;&#30456;&#27604;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#25512;&#21160;&#26085;&#35821;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30740;&#31350;&#19982;&#24320;&#21457;&#30340;&#36827;&#23637;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;JMultiWOZ&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26085;&#35821;&#22823;&#35268;&#27169;&#22810;&#39046;&#22495;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;JMultiWOZ&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#33521;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;MultiWOZ2.2&#21644;&#26368;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#27861;&#19978;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#21644;&#22238;&#22797;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;JMultiWOZ&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;MultiWOZ2&#30456;&#23218;&#32654;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17319v1 Announce Type: cross  Abstract: Dialogue datasets are crucial for deep learning-based task-oriented dialogue system research. While numerous English language multi-domain task-oriented dialogue datasets have been developed and contributed to significant advancements in task-oriented dialogue systems, such a dataset does not exist in Japanese, and research in this area is limited compared to that in English. In this study, towards the advancement of research and development of task-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first Japanese language large-scale multi-domain task-oriented dialogue dataset. Using JMultiWOZ, we evaluated the dialogue state tracking and response generation capabilities of the state-of-the-art methods on the existing major English benchmark dataset MultiWOZ2.2 and the latest large language model (LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ provides a benchmark that is on par with MultiWOZ2
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ALISA&#65292;&#19968;&#31181;&#36890;&#36807;&#31232;&#30095;&#24863;&#30693;KV&#32531;&#23384;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#31995;&#32479;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.17312</link><description>&lt;p&gt;
ALISA: &#36890;&#36807;&#31232;&#30095;&#24863;&#30693;KV&#32531;&#23384;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17312
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ALISA&#65292;&#19968;&#31181;&#36890;&#36807;&#31232;&#30095;&#24863;&#30693;KV&#32531;&#23384;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#31995;&#32479;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#26174;&#33879;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#65292;&#24182;&#19988;&#22312;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#20855;&#26377;&#22522;&#30784;&#24615;&#20316;&#29992;&#65292;&#22914;LLaMA&#21644;OPT&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;LLMs&#22312;&#23454;&#38469;&#25512;&#29702;&#20013;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#65292;&#28041;&#21450;&#35745;&#31639;&#21644;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#20855;&#26377;&#33258;&#22238;&#24402;&#29305;&#24615;&#65292;Transformer&#20013;&#30340;&#27880;&#24847;&#23618;&#30340;KV&#32531;&#23384;&#21487;&#20197;&#36890;&#36807;&#23558;&#20108;&#27425;&#22797;&#26434;&#24230;&#35745;&#31639;&#26367;&#25442;&#20026;&#32447;&#24615;&#22797;&#26434;&#24230;&#20869;&#23384;&#35775;&#38382;&#65292;&#20174;&#32780;&#26377;&#25928;&#21152;&#36895;LLM&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23545;&#22788;&#29702;&#26356;&#38271;&#24207;&#21015;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#22686;&#21152;&#20869;&#23384;&#12290;&#36825;&#31181;&#24320;&#38144;&#23548;&#33268;&#30001;&#20110;I/O&#29942;&#39048;&#21644;&#29978;&#33267;&#26159;&#20869;&#23384;&#19981;&#36275;&#38169;&#35823;&#32780;&#23548;&#33268;&#21534;&#21520;&#37327;&#38477;&#20302;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#31995;&#32479;&#19978;&#65292;&#22914;&#21333;&#20010;&#36890;&#29992;GPU&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17312v1 Announce Type: new  Abstract: The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU. In this paper, we propose ALISA, a novel algorithm-system co-desi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#25991;&#26723;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#35299;&#20915;&#26041;&#26696;&#21644;&#20004;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22343;&#33021;&#20135;&#29983;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17308</link><description>&lt;p&gt;
&#31070;&#32463;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#65306;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Neural Multimodal Topic Modeling: A Comprehensive Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#25991;&#26723;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#35299;&#20915;&#26041;&#26696;&#21644;&#20004;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22343;&#33021;&#20135;&#29983;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#22320;&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#25214;&#21040;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#22914;&#22270;&#29255;&#21644;&#25991;&#26412;&#65289;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#25991;&#26723;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#30340;&#31995;&#32479;&#24615;&#21644;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#35299;&#20915;&#26041;&#26696;&#21644;&#20004;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#20016;&#23500;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20004;&#20010;&#27169;&#22411;&#37117;&#33021;&#20135;&#29983;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26041;&#27861;&#20248;&#20110;&#21478;&#19968;&#20010;&#26041;&#27861;&#30340;&#31243;&#24230;&#21462;&#20915;&#20110;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#65292;&#36825;&#34920;&#26126;&#26410;&#26469;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#31616;&#27905;&#30340;&#20154;&#24037;&#35780;&#20272;&#19982;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#25152;&#30830;&#23450;&#30340;&#32467;&#26524;&#19968;&#33268;&#12290;&#36825;&#31181;&#19968;&#33268;&#19981;&#20165;&#21152;&#24378;&#20102;&#25105;&#20204;&#25351;&#26631;&#30340;&#21487;&#20449;&#24230;&#65292;&#20063;&#31361;&#26174;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17308v1 Announce Type: cross  Abstract: Neural topic models can successfully find coherent and diverse topics in textual data. However, they are limited in dealing with multimodal datasets (e.g., images and text). This paper presents the first systematic and comprehensive evaluation of multimodal topic modeling of documents containing both text and images. In the process, we propose two novel topic modeling solutions and two novel evaluation metrics. Overall, our evaluation on an unprecedented rich and diverse collection of datasets indicates that both of our models generate coherent and diverse topics. Nevertheless, the extent to which one method outperforms the other depends on the metrics and dataset combinations, which suggests further exploration of hybrid solutions in the future. Notably, our succinct human evaluation aligns with the outcomes determined by our proposed metrics. This alignment not only reinforces the credibility of our metrics but also highlights the po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#32454;&#33268;&#35752;&#35770;&#65292;&#23545;&#24187;&#35273;&#36827;&#34892;&#20102;&#37327;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;VHILT&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#27492;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17306</link><description>&lt;p&gt;
&#35270;&#35273;&#24187;&#35273;&#65306;&#23450;&#20041;&#12289;&#37327;&#21270;&#21644;&#22788;&#26041;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Visual Hallucination: Definition, Quantification, and Prescriptive Remediations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#32454;&#33268;&#35752;&#35770;&#65292;&#23545;&#24187;&#35273;&#36827;&#34892;&#20102;&#37327;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;VHILT&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#27492;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21457;&#24187;&#35273;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#25110;&#35768;&#26159;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#36827;&#23637;&#30340;&#26368;&#26174;&#33879;&#38556;&#30861;&#12290;&#26368;&#36817;&#65292;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20013;&#24187;&#35273;&#20063;&#30456;&#24403;&#26222;&#36941;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#22522;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20004;&#20010;&#20219;&#21153;&#30340;VLM&#24187;&#35273;&#21078;&#26512;&#30340;&#32454;&#33268;&#35752;&#35770;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#20843;&#20010;&#32454;&#33268;&#30340;&#35270;&#35273;&#24187;&#35273;&#21462;&#21521;&#65306;i) &#19978;&#19979;&#25991;&#29468;&#27979;&#65292;ii) &#36523;&#20221;&#19981;&#19968;&#33268;&#65292;iii) &#22320;&#29702;&#38169;&#35823;&#65292;iv) &#35270;&#35273;&#24187;&#35273;&#65292;v) &#24615;&#21035;&#24322;&#24120;&#65292;vi) VLM&#20316;&#20026;&#20998;&#31867;&#22120;&#65292;vii) &#38169;&#35823;&#38405;&#35835;&#65292;&#21644;viii) &#25968;&#23383;&#24046;&#24322;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20221;&#21517;&#20026;&#35270;&#35273;&#24187;&#35273;&#35825;&#21457;&#65288;VHILT&#65289;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;&#65288;&#23383;&#24149;&#21644;VQA&#65289;&#29983;&#25104;&#30340;&#26469;&#33258;&#20843;&#20010;VLM&#30340;2,000&#20010;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17306v1 Announce Type: new  Abstract: The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However, it's worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA). We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA alo
&lt;/p&gt;</description></item><item><title>InternLM2&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#12289;&#38271;&#25991;&#26412;&#24314;&#27169;&#20197;&#21450;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#21644;&#20248;&#21270;&#25216;&#26415;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#20854;&#21069;&#20219;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.17297</link><description>&lt;p&gt;
InternLM2&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
InternLM2 Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17297
&lt;/p&gt;
&lt;p&gt;
InternLM2&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#12289;&#38271;&#25991;&#26412;&#24314;&#27169;&#20197;&#21450;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#21644;&#20248;&#21270;&#25216;&#26415;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#20854;&#21069;&#20219;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#21363;&#23558;&#21040;&#26469;&#30340;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#22312;&#24320;&#28304;&#27169;&#22411;&#20013;&#22797;&#21046;&#36825;&#26679;&#30340;&#36827;&#23637;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;InternLM2&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#65292;&#22312;6&#20010;&#32500;&#24230;&#21644;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20854;&#21069;&#36744;&#65292;&#22312;&#38271;&#25991;&#26412;&#24314;&#27169;&#21644;&#20027;&#35266;&#35780;&#20272;&#26041;&#38754;&#20248;&#24322;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#21644;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17297v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack" test. InternLM2 is further aligned using Supervised Fi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30693;&#35782;&#36716;&#31227;&#21644;&#35838;&#31243;&#23398;&#20064;&#65292;&#26412;&#30740;&#31350;&#22312;&#19977;&#25351;&#26426;&#26800;&#33218;&#25805;&#32437;&#20219;&#21153;&#20013;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#30340;&#30446;&#26631;</title><link>https://arxiv.org/abs/2403.17266</link><description>&lt;p&gt;
&#25506;&#32034;CausalWorld&#65306;&#36890;&#36807;&#30693;&#35782;&#36716;&#31227;&#21644;&#35838;&#31243;&#23398;&#20064;&#22686;&#24378;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17266
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#36716;&#31227;&#21644;&#35838;&#31243;&#23398;&#20064;&#65292;&#26412;&#30740;&#31350;&#22312;&#19977;&#25351;&#26426;&#26800;&#33218;&#25805;&#32437;&#20219;&#21153;&#20013;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#19977;&#25351;&#26426;&#26800;&#33218;&#25805;&#32437;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#38656;&#35201;&#25351;&#38388;&#30340;&#22797;&#26434;&#36816;&#21160;&#21644;&#21327;&#35843;&#12290;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26234;&#33021;&#20307;&#26469;&#33719;&#24471;&#29087;&#32451;&#25805;&#32437;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#20013;&#37319;&#29992;&#20102;&#20004;&#31181;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#65292;&#21363;&#24494;&#35843;&#21644;&#35838;&#31243;&#23398;&#20064;&#12290;&#24494;&#35843;&#20351;&#26234;&#33021;&#20307;&#21487;&#20197;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#24182;&#23558;&#20854;&#35843;&#25972;&#21040;&#26032;&#20219;&#21153;&#20013;&#12290;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#22810;&#31181;&#21464;&#20307;&#65292;&#22914;&#27169;&#22411;&#36716;&#31227;&#12289;&#31574;&#30053;&#36716;&#31227;&#21644;&#36328;&#20219;&#21153;&#36716;&#31227;&#12290;&#20026;&#20102;&#28040;&#38500;&#39044;&#35757;&#32451;&#30340;&#38656;&#27714;&#65292;&#35838;&#31243;&#23398;&#20064;&#23558;&#39640;&#32423;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#12289;&#28176;&#36827;&#30340;&#38454;&#27573;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#12290;&#21457;&#29616;&#23398;&#20064;&#38454;&#27573;&#30340;&#25968;&#37327;&#12289;&#23376;&#20219;&#21153;&#30340;&#32972;&#26223;&#20197;&#21450;&#36716;&#25442;&#26102;&#26426;&#26159;&#20851;&#38190;&#30340;&#35774;&#35745;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17266v1 Announce Type: cross  Abstract: This study explores a learning-based tri-finger robotic arm manipulating task, which requires complex movements and coordination among the fingers. By employing reinforcement learning, we train an agent to acquire the necessary skills for proficient manipulation. To enhance the efficiency and effectiveness of the learning process, two knowledge transfer strategies, fine-tuning and curriculum learning, were utilized within the soft actor-critic architecture. Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to new tasks. Several variations like model transfer, policy transfer, and across-task transfer were implemented and evaluated. To eliminate the need for pretraining, curriculum learning decomposes the advanced task into simpler, progressive stages, mirroring how humans learn. The number of learning stages, the context of the sub-tasks, and the transition timing were found to be the critical design parameter
&lt;/p&gt;</description></item><item><title>DASA&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#25910;&#25947;&#36895;&#24230;&#20165;&#20381;&#36182;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#23454;&#29616;N&#20493;&#30340;&#25910;&#25947;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.17247</link><description>&lt;p&gt;
DASA: &#24310;&#36831;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#38543;&#26426;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
DASA: Delay-Adaptive Multi-Agent Stochastic Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17247
&lt;/p&gt;
&lt;p&gt;
DASA&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#25910;&#25947;&#36895;&#24230;&#20165;&#20381;&#36182;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#23454;&#29616;N&#20493;&#30340;&#25910;&#25947;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#35774;&#32622;&#65292;&#20854;&#20013;$N$&#20010;&#26234;&#33021;&#20307;&#26088;&#22312;&#36890;&#36807;&#24182;&#34892;&#25805;&#20316;&#24182;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#26469;&#21152;&#36895;&#19968;&#20010;&#24120;&#35265;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#23450;&#19978;&#34892;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#30340;&#20256;&#36755;&#21463;&#21040;&#24322;&#27493;&#21644;&#28508;&#22312;&#26080;&#30028;&#26102;&#21464;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20943;&#36731;&#24310;&#36831;&#21644;&#33853;&#21518;&#32773;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#21448;&#33021;&#33719;&#24471;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DASA&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#12290;&#25105;&#20204;&#23545;DASA&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#20551;&#35774;&#26234;&#33021;&#20307;&#30340;&#38543;&#26426;&#35266;&#27979;&#36807;&#31243;&#26159;&#29420;&#31435;&#39532;&#23572;&#31185;&#22827;&#38142;&#12290;&#19982;&#29616;&#26377;&#32467;&#26524;&#30456;&#27604;&#65292;DASA&#26159;&#31532;&#19968;&#20010;&#20854;&#25910;&#25947;&#36895;&#24230;&#20165;&#21462;&#20915;&#20110;&#28151;&#21512;&#26102;&#38388;$tmix$&#21644;&#24179;&#22343;&#24310;&#36831;$\tau_{avg}$&#65292;&#21516;&#26102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#23454;&#29616;N&#20493;&#30340;&#25910;&#25947;&#21152;&#36895;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#21508;&#31181;SA&#24212;&#29992;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17247v1 Announce Type: new  Abstract: We consider a setting in which $N$ agents aim to speedup a common Stochastic Approximation (SA) problem by acting in parallel and communicating with a central server. We assume that the up-link transmissions to the server are subject to asynchronous and potentially unbounded time-varying delays. To mitigate the effect of delays and stragglers while reaping the benefits of distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm for multi-agent Stochastic Approximation. We provide a finite-time analysis of \texttt{DASA} assuming that the agents' stochastic observation processes are independent Markov chains. Significantly advancing existing results, \texttt{DASA} is the first algorithm whose convergence rate depends only on the mixing time $\tmix$ and on the average delay $\tau_{avg}$ while jointly achieving an $N$-fold convergence speedup under Markovian sampling. Our work is relevant for various SA applications, inc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#32463;&#20856;&#35268;&#21010;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36817;&#20284;&#20154;&#31867;&#30452;&#35273;&#65292;&#20197;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.17246</link><description>&lt;p&gt;
TwoStep: &#20351;&#29992;&#32463;&#20856;&#35268;&#21010;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#32463;&#20856;&#35268;&#21010;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36817;&#20284;&#20154;&#31867;&#30452;&#35273;&#65292;&#20197;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#20043;&#31867;&#30340;&#32463;&#20856;&#35268;&#21010;&#20844;&#24335;&#20801;&#35768;&#30830;&#23450;&#21487;&#23454;&#29616;&#30446;&#26631;&#29366;&#24577;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#21482;&#35201;&#23384;&#22312;&#20219;&#20309;&#21487;&#33021;&#30340;&#21021;&#22987;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;PDDL&#20013;&#23450;&#20041;&#30340;&#25512;&#29702;&#38382;&#39064;&#24182;&#26410;&#25429;&#33719;&#34892;&#21160;&#36827;&#34892;&#30340;&#26102;&#38388;&#26041;&#38754;&#65292;&#20363;&#22914;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#26234;&#33021;&#20307;&#22914;&#26524;&#24444;&#27492;&#30340;&#21518;&#20917;&#19981;&#24178;&#25200;&#21069;&#25552;&#26465;&#20214;&#65292;&#21017;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#19968;&#20010;&#21160;&#20316;&#12290;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#23558;&#30446;&#26631;&#20998;&#35299;&#20026;&#22823;&#37096;&#20998;&#29420;&#31435;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23558;&#27599;&#20010;&#26234;&#33021;&#20307;&#20998;&#37197;&#32473;&#20854;&#20013;&#19968;&#20010;&#23376;&#30446;&#26631;&#65292;&#20197;&#21033;&#29992;&#21516;&#26102;&#36827;&#34892;&#21160;&#20316;&#26469;&#21152;&#24555;&#35745;&#21010;&#27493;&#39588;&#30340;&#25191;&#34892;&#65292;&#27599;&#20010;&#37096;&#20998;&#20165;&#20351;&#29992;&#21333;&#20010;&#26234;&#33021;&#20307;&#35268;&#21010;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#30452;&#25509;&#25512;&#26029;&#35745;&#21010;&#27493;&#39588;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#19981;&#20445;&#35777;&#25191;&#34892;&#25104;&#21151;&#65292;&#20294;&#21033;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#32452;&#35013;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#36817;&#20284;&#20154;&#31867;&#30452;&#35273;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#35268;&#21010;&#21644;LLMs&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17246v1 Announce Type: new  Abstract: Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible. However, reasoning problems defined in PDDL do not capture temporal aspects of action taking, for example that two agents in the domain can execute an action simultaneously if postconditions of each do not interfere with preconditions of the other. A human expert can decompose a goal into largely independent constituent parts and assign each agent to one of these subgoals to take advantage of simultaneous actions for faster execution of plan steps, each using only single agent planning. By contrast, large language models (LLMs) used for directly inferring plan steps do not guarantee execution success, but do leverage commonsense reasoning to assemble action sequences. We combine the strengths of classical planning and LLMs by approximating human intuition
&lt;/p&gt;</description></item><item><title>DreamPolisher&#36890;&#36807;&#20004;&#38454;&#27573;&#39640;&#26031;&#39134;&#28293;&#26041;&#27861;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#19977;&#32500;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#65292;&#24378;&#35843;&#20102;&#35270;&#22270;&#19968;&#33268;&#24615;&#21644;&#32441;&#29702;&#20016;&#23500;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.17237</link><description>&lt;p&gt;
DreamPolisher: &#36890;&#36807;&#20960;&#20309;&#25193;&#25955;&#23454;&#29616;&#39640;&#36136;&#37327;&#25991;&#26412;&#21040;&#19977;&#32500;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17237
&lt;/p&gt;
&lt;p&gt;
DreamPolisher&#36890;&#36807;&#20004;&#38454;&#27573;&#39640;&#26031;&#39134;&#28293;&#26041;&#27861;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#19977;&#32500;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#65292;&#24378;&#35843;&#20102;&#35270;&#22270;&#19968;&#33268;&#24615;&#21644;&#32441;&#29702;&#20016;&#23500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DreamPolisher&#65292;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#39134;&#28293;&#21644;&#20960;&#20309;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#23398;&#20064;&#36328;&#35270;&#22270;&#19968;&#33268;&#24615;&#21644;&#31934;&#32454;&#32454;&#33410;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#39640;&#26031;&#39134;&#28293;&#26041;&#27861;&#24378;&#21046;&#23454;&#29616;&#35270;&#22270;&#20043;&#38388;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#65292;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#19977;&#32500;&#36164;&#20135;&#30340;&#32441;&#29702;&#20445;&#30495;&#24230;&#21644;&#25972;&#20307;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17237v1 Announce Type: cross  Abstract: We present DreamPolisher, a novel Gaussian Splatting based method with geometric guidance, tailored to learn cross-view consistency and intricate detail from textual descriptions. While recent progress on text-to-3D generation methods have been promising, prevailing methods often fail to ensure view-consistency and textural richness. This problem becomes particularly noticeable for methods that work with text input alone. To address this, we propose a two-stage Gaussian Splatting based approach that enforces geometric consistency among views. Initially, a coarse 3D generation undergoes refinement via geometric optimization. Subsequently, we use a ControlNet driven refiner coupled with the geometric consistency term to improve both texture fidelity and overall consistency of the generated 3D asset. Empirical evaluations across diverse textual prompts spanning various object categories demonstrate the efficacy of DreamPolisher in generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20572;&#36710;&#20219;&#21153;&#20013;&#30340;&#22312;&#32447;&#36335;&#24452;&#35268;&#21010;&#65292;&#26088;&#22312;&#21152;&#36895;&#36335;&#24452;&#35268;&#21010;&#36807;&#31243;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17234</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#20572;&#36710;&#20013;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;MCTS&#20013;&#21152;&#36895;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Speeding Up Path Planning via Reinforcement Learning in MCTS for Automated Parking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20572;&#36710;&#20219;&#21153;&#20013;&#30340;&#22312;&#32447;&#36335;&#24452;&#35268;&#21010;&#65292;&#26088;&#22312;&#21152;&#36895;&#36335;&#24452;&#35268;&#21010;&#36807;&#31243;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#35813;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#25972;&#21512;&#21040;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#20013;&#65292;&#20197;&#25552;&#21319;&#22312;&#20840;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#36827;&#34892;&#33258;&#21160;&#20572;&#36710;&#20219;&#21153;&#30340;&#22312;&#32447;&#36335;&#24452;&#35268;&#21010;&#12290;&#22312;&#39640;&#32500;&#31354;&#38388;&#19979;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#24320;&#38144;&#22823;&#12289;&#32791;&#26102;&#38271;&#30340;&#29305;&#28857;&#12290;&#29366;&#24577;&#35780;&#20272;&#26041;&#27861;&#36890;&#36807;&#23558;&#20808;&#39564;&#30693;&#35782;&#24212;&#29992;&#20110;&#25628;&#32034;&#27493;&#39588;&#20013;&#65292;&#20351;&#23454;&#26102;&#31995;&#32479;&#20013;&#30340;&#36807;&#31243;&#26356;&#24555;&#36895;&#12290;&#37492;&#20110;&#33258;&#21160;&#20572;&#36710;&#20219;&#21153;&#36890;&#24120;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25191;&#34892;&#65292;&#20256;&#32479;&#20998;&#26512;&#26041;&#24335;&#38590;&#20197;&#26500;&#24314;&#22362;&#23454;&#20294;&#36731;&#37327;&#32423;&#30340;&#21551;&#21457;&#24335;&#25351;&#23548;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;&#19979;&#20855;&#26377;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;&#27969;&#27700;&#32447;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#23398;&#20064;&#29366;&#24577;&#30340;&#20215;&#20540;&#20197;&#21450;&#26368;&#20339;&#21160;&#20316;&#65292;&#22312;&#21069;&#19968;&#20010;&#21608;&#26399;&#32467;&#26524;&#30340;&#26679;&#26412;&#20013;&#36873;&#25321;&#26368;&#20339;&#21160;&#20316;&#65292;&#25105;&#20204;&#33021;&#22815;&#24314;&#27169;&#19968;&#20010;&#20540;&#20272;&#35745;&#22120;&#20197;&#21450;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17234v1 Announce Type: new  Abstract: In this paper, we address a method that integrates reinforcement learning into the Monte Carlo tree search to boost online path planning under fully observable environments for automated parking tasks. Sampling-based planning methods under high-dimensional space can be computationally expensive and time-consuming. State evaluation methods are useful by leveraging the prior knowledge into the search steps, making the process faster in a real-time system. Given the fact that automated parking tasks are often executed under complex environments, a solid but lightweight heuristic guidance is challenging to compose in a traditional analytical way. To overcome this limitation, we propose a reinforcement learning pipeline with a Monte Carlo tree search under the path planning framework. By iteratively learning the value of a state and the best action among samples from its previous cycle's outcomes, we are able to model a value estimator and a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;&#35299;&#37322;&#26041;&#27861;&#26469;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#35745;&#31639;&#35299;&#37322;&#20998;&#24067;&#30340;&#21464;&#24322;&#31995;&#25968;&#65292;&#35780;&#20272;&#20102;&#35299;&#37322;&#30340;&#32622;&#20449;&#24230;&#24182;&#30830;&#23450;Guided Backpropagation&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17224</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Gradient-based Explanations in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;&#35299;&#37322;&#26041;&#27861;&#26469;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#35745;&#31639;&#35299;&#37322;&#20998;&#24067;&#30340;&#21464;&#24322;&#31995;&#25968;&#65292;&#35780;&#20272;&#20102;&#35299;&#37322;&#30340;&#32622;&#20449;&#24230;&#24182;&#30830;&#23450;Guided Backpropagation&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26041;&#27861;&#26377;&#21161;&#20110;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#21407;&#22240;&#12290;&#36825;&#20123;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#21442;&#19982;&#27169;&#22411;&#35843;&#35797;&#12289;&#24615;&#33021;&#20248;&#21270;&#65292;&#24182;&#33719;&#24471;&#23545;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#30340;&#27934;&#35265;&#12290;&#37492;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#34913;&#37327;&#36825;&#20123;&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;&#35299;&#37322;&#26041;&#27861;&#26469;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#27969;&#31243;&#20026;CIFAR-10&#12289;FER+&#21644;California Housing&#25968;&#25454;&#38598;&#29983;&#25104;&#35299;&#37322;&#20998;&#24067;&#12290;&#36890;&#36807;&#35745;&#31639;&#36825;&#20123;&#20998;&#24067;&#30340;&#21464;&#24322;&#31995;&#25968;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35299;&#37322;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#30830;&#23450;&#20351;&#29992;&#24341;&#23548;&#21453;&#21521;&#20256;&#25773;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#20302;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#20462;&#25913;&#30340;&#20687;&#32032;&#25554;&#20837;/&#21024;&#38500;&#24230;&#37327;&#26469;&#35780;&#20215;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17224v1 Announce Type: cross  Abstract: Explanation methods help understand the reasons for a model's prediction. These methods are increasingly involved in model debugging, performance optimization, and gaining insights into the workings of a model. With such critical applications of these methods, it is imperative to measure the uncertainty associated with the explanations generated by these methods. In this paper, we propose a pipeline to ascertain the explanation uncertainty of neural networks by combining uncertainty estimation methods and explanation methods. We use this pipeline to produce explanation distributions for the CIFAR-10, FER+, and California Housing datasets. By computing the coefficient of variation of these distributions, we evaluate the confidence in the explanation and determine that the explanations generated using Guided Backpropagation have low uncertainty associated with them. Additionally, we compute modified pixel insertion/deletion metrics to ev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#22810;&#26631;&#31614;&#29289;&#20307;&#31867;&#21035;&#20013;&#22522;&#30784;&#29289;&#20307;&#30340;&#20849;&#23384;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#20849;&#23384;&#30697;&#38453;&#20998;&#26512;&#29983;&#25104;&#39057;&#32321;&#27169;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#26410;&#26631;&#35760;&#29289;&#20307;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.17223</link><description>&lt;p&gt;
&#29289;&#20307;&#26816;&#27979;&#21644;&#35782;&#21035;&#30340;&#20849;&#23384;&#65292;&#20197;&#21450;&#26410;&#26631;&#35760;&#29289;&#20307;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Co-Occurring of Object Detection and Identification towards unlabeled object discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17223
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#22810;&#26631;&#31614;&#29289;&#20307;&#31867;&#21035;&#20013;&#22522;&#30784;&#29289;&#20307;&#30340;&#20849;&#23384;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#20849;&#23384;&#30697;&#38453;&#20998;&#26512;&#29983;&#25104;&#39057;&#32321;&#27169;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#26410;&#26631;&#35760;&#29289;&#20307;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22810;&#26631;&#31614;&#29289;&#20307;&#31867;&#21035;&#20013;&#19982;&#22522;&#30784;&#29289;&#20307;&#20849;&#23384;&#30340;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#22312;&#25552;&#20986;&#27169;&#22411;&#30340;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#36793;&#30028;&#26694;&#21450;&#20854;&#23545;&#24212;&#30340;&#26631;&#31614;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#36827;&#34892;&#20849;&#23384;&#30697;&#38453;&#20998;&#26512;&#12290;&#22312;&#20849;&#23384;&#30697;&#38453;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#26631;&#31614;&#30340;&#26368;&#22823;&#20986;&#29616;&#27425;&#25968;&#35774;&#23450;&#22522;&#26412;&#31867;&#65292;&#24182;&#26500;&#24314;&#20851;&#32852;&#35268;&#21017;&#24182;&#29983;&#25104;&#39057;&#32321;&#27169;&#24335;&#12290;&#36825;&#20123;&#39057;&#32321;&#27169;&#24335;&#23558;&#26174;&#31034;&#22522;&#26412;&#31867;&#21450;&#20854;&#23545;&#24212;&#30340;&#20849;&#23384;&#31867;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;Pascal VOC&#21644;MS-COCO&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17223v1 Announce Type: cross  Abstract: In this paper, we propose a novel deep learning based approach for identifying co-occurring objects in conjunction with base objects in multilabel object categories. Nowadays, with the advancement in computer vision based techniques we need to know about co-occurring objects with respect to base object for various purposes. The pipeline of the proposed work is composed of two stages: in the first stage of the proposed model we detect all the bounding boxes present in the image and their corresponding labels, then in the second stage we perform co-occurrence matrix analysis. In co-occurrence matrix analysis, we set base classes based on the maximum occurrences of the labels and build association rules and generate frequent patterns. These frequent patterns will show base classes and their corresponding co-occurring classes. We performed our experiments on two publicly available datasets: Pascal VOC and MS-COCO. The experimental results 
&lt;/p&gt;</description></item><item><title>SeSaMe&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#27169;&#25311;&#33258;&#25105;&#25253;&#21578;&#65292;&#20943;&#36731;&#20102;&#20182;&#20204;&#30340;&#36127;&#25285;</title><link>https://arxiv.org/abs/2403.17219</link><description>&lt;p&gt;
SeSaMe&#65306;&#27169;&#25311;&#33258;&#25105;&#25253;&#21578;&#30340;&#22320;&#38754;&#30495;&#23454;&#24615;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#24863;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17219
&lt;/p&gt;
&lt;p&gt;
SeSaMe&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#27169;&#25311;&#33258;&#25105;&#25253;&#21578;&#65292;&#20943;&#36731;&#20102;&#20182;&#20204;&#30340;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#21644;&#21487;&#31359;&#25140;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#24471;&#21487;&#20197;&#34987;&#21160;&#30417;&#27979;&#19968;&#20010;&#20154;&#30340;&#24515;&#29702;&#12289;&#34892;&#20026;&#21644;&#24773;&#24863;&#20581;&#24247;&#30340;&#28508;&#21147;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#33258;&#25105;&#25253;&#21578;&#32467;&#26524;&#30340;&#38271;&#26399;&#25910;&#38598;&#65292;&#20363;&#22914;&#25233;&#37057;&#12289;&#21387;&#21147;&#21644;&#28966;&#34385;&#65292;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25345;&#32493;&#33258;&#25105;&#25253;&#21578;&#20250;&#32473;&#21442;&#19982;&#32773;&#24102;&#26469;&#37325;&#22823;&#36127;&#25285;&#65292;&#32463;&#24120;&#23548;&#33268;&#27969;&#22833;&#12289;&#32570;&#22833;&#26631;&#31614;&#25110;&#19981;&#30495;&#35802;&#30340;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20351;&#29992;&#24515;&#29702;&#27169;&#22411;&#27169;&#25311;&#35268;&#27169;&#20998;&#25968;&#30340;SeSaMe&#26694;&#26550;&#65292;&#20197;&#20943;&#36731;&#25968;&#23383;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#20013;&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;SeSaMe&#33021;&#22815;&#27169;&#25311;&#21442;&#19982;&#32773;&#22312;&#24515;&#29702;&#37327;&#34920;&#19978;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17219v1 Announce Type: cross  Abstract: Advances in mobile and wearable technologies have enabled the potential to passively monitor a person's mental, behavioral, and affective health. These approaches typically rely on longitudinal collection of self-reported outcomes, e.g., depression, stress, and anxiety, to train machine learning (ML) models. However, the need to continuously self-report adds a significant burden on the participants, often resulting in attrition, missing labels, or insincere responses. In this work, we introduce the Scale Scores Simulation using Mental Models (SeSaMe) framework to alleviate participants' burden in digital mental health studies. By leveraging pre-trained large language models (LLMs), SeSaMe enables the simulation of participants' responses on psychological scales. In SeSaMe, researchers can prompt LLMs with information on participants' internal behavioral dispositions, enabling LLMs to construct mental models of participants to simulate 
&lt;/p&gt;</description></item><item><title>DiffusionAct&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#32534;&#36753;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#23039;&#21183;&#65292;&#23454;&#29616;&#36523;&#20221;&#21644;&#22806;&#35266;&#30340;&#20445;&#30041;&#65292;&#20197;&#21450;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.17217</link><description>&lt;p&gt;
DiffusionAct&#65306;&#21487;&#25511;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#19968;&#27425;&#24615;&#20154;&#33080;&#20877;&#29616;
&lt;/p&gt;
&lt;p&gt;
DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17217
&lt;/p&gt;
&lt;p&gt;
DiffusionAct&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#32534;&#36753;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#23039;&#21183;&#65292;&#23454;&#29616;&#36523;&#20221;&#21644;&#22806;&#35266;&#30340;&#20445;&#30041;&#65292;&#20197;&#21450;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#39537;&#21160;&#30340;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#26088;&#22312;&#21512;&#25104;&#33021;&#25104;&#21151;&#20445;&#30041;&#28304;&#33080;&#30340;&#36523;&#20221;&#21644;&#22806;&#35266;&#65292;&#21516;&#26102;&#36716;&#31227;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#36924;&#30495;&#38754;&#37096;&#22270;&#20687;&#12290;&#29616;&#26377;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#35201;&#20040;&#23384;&#22312;&#22833;&#30495;&#21644;&#35270;&#35273;&#20266;&#24433;&#65292;&#35201;&#20040;&#37325;&#26500;&#36136;&#37327;&#36739;&#24046;&#65292;&#21363;&#32972;&#26223;&#21644;&#19968;&#20123;&#37325;&#35201;&#30340;&#22806;&#35266;&#32454;&#33410;&#65288;&#22914;&#21457;&#22411;/&#39068;&#33394;&#12289;&#30524;&#38236;&#21644;&#37197;&#39280;&#65289;&#26410;&#34987;&#24544;&#23454;&#37325;&#24314;&#12290;&#26368;&#36817;&#22312;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DiffusionAct&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#29031;&#29255;&#36924;&#30495;&#22270;&#20687;&#29983;&#25104;&#26469;&#36827;&#34892;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#25511;&#21046;Diffusion&#33258;&#32534;&#30721;&#22120;&#65288;DiffAE&#65289;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#20197;&#20415;&#32534;&#36753;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#23039;&#21183;&#65292;&#23450;&#20041;&#20026;&#22836;&#37096;&#23039;&#21183;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17217v1 Announce Type: cross  Abstract: Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment. Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#20998;&#26512;&#20102;11&#31181;&#27969;&#34892;&#30340;&#19987;&#38376;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#21457;&#29616;&#20854;&#20013;26.4%&#21040;73.7%&#30340;&#20195;&#30721;&#32763;&#35793;&#38656;&#35201;&#21518;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.17214</link><description>&lt;p&gt;
&#25506;&#31350;&#36755;&#20986;&#26684;&#24335;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#32763;&#35793;&#35780;&#20272;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#20998;&#26512;&#20102;11&#31181;&#27969;&#34892;&#30340;&#19987;&#38376;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20116;&#31181;&#35821;&#35328;&#19978;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#21457;&#29616;&#20854;&#20013;26.4%&#21040;73.7%&#30340;&#20195;&#30721;&#32763;&#35793;&#38656;&#35201;&#21518;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#20195;&#30721;&#32763;&#35793;&#26159;&#36719;&#20214;&#24037;&#31243;&#20013;&#38271;&#26399;&#23384;&#22312;&#19988;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26377;&#21161;&#20110;&#29616;&#20195;&#21270;&#36951;&#30041;&#31995;&#32479;&#65292;&#30830;&#20445;&#36328;&#24179;&#21488;&#20860;&#23481;&#24615;&#65292;&#25552;&#21319;&#36719;&#20214;&#24615;&#33021;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21450;&#20854;&#22312;&#20195;&#30721;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#24378;&#28872;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20116;&#31181;&#35821;&#35328;&#65288;&#21253;&#25324;C&#12289;C++&#12289;Go&#12289;Java&#21644;Python&#65289;&#19978;&#65292;&#20174;1B&#21040;46.7B&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#23545;&#21313;&#19968;&#31181;&#27969;&#34892;&#30340;&#19987;&#38376;&#35843;&#25972;&#30340;LLMs&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#28085;&#30422;3820&#20010;&#32763;&#35793;&#23545;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#35780;&#20272;&#30340;LLMs&#20013;&#65292;26.4%&#21040;73.7%&#30340;&#20195;&#30721;&#32763;&#35793;&#38656;&#35201;&#21518;&#22788;&#29702;&#65292;&#22240;&#20026;&#36825;&#20123;&#32763;&#35793;&#36890;&#24120;&#21253;&#21547;&#20195;&#30721;&#12289;&#24341;&#21495;&#21644;&#25991;&#26412;&#30340;&#28151;&#21512;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#32431;&#28304;&#20195;&#30721;&#12290;&#24573;&#35270;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#26684;&#24335;&#21487;&#33021;&#19981;&#32463;&#24847;&#38388;&#23548;&#33268;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17214v1 Announce Type: cross  Abstract: Code translation between programming languages is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. With the recent advances in large language models (LLMs) and their applications to code translation, there is an increasing need for comprehensive evaluation of these models. In this study, we empirically analyze the generated outputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B up to 46.7B on 3,820 translation pairs across five languages, including C, C++, Go, Java, and Python. Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code. Overlooking the output format of these models can inadvertently lead to u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#65292;&#21487;&#20197;&#24555;&#36895;&#27979;&#35797;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#26041;&#27861;&#30340;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.17212</link><description>&lt;p&gt;
&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Sanity Checks for Explanation Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#65292;&#21487;&#20197;&#24555;&#36895;&#27979;&#35797;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#26041;&#27861;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#21487;&#33021;&#38590;&#20197;&#35299;&#37322;&#25110;&#20986;&#29616;&#38169;&#35823;&#12290; &#23558;&#35299;&#37322;&#26041;&#27861;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#30456;&#32467;&#21512;&#20250;&#20135;&#29983;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#12290; &#35780;&#20272;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#26159;&#22256;&#38590;&#30340;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#65292;&#20854;&#20013;&#38024;&#23545;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#35299;&#37322;&#23450;&#20041;&#20102;&#26435;&#37325;&#21644;&#25968;&#25454;&#38543;&#26426;&#21270;&#27979;&#35797;&#65292;&#20801;&#35768;&#23545;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#26041;&#27861;&#30340;&#32452;&#21512;&#36827;&#34892;&#24555;&#36895;&#27979;&#35797;&#12290; &#25105;&#20204;&#22312;CIFAR10&#21644;&#21152;&#21033;&#31119;&#23612;&#20122;&#25151;&#23627;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#27979;&#35797;&#30340;&#26377;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#27880;&#24847;&#21040;Ensemble&#20284;&#20046;&#22312;Guided Backpropagation&#65292;Integrated Gradients&#21644;LIME&#35299;&#37322;&#19978;&#19968;&#33268;&#36890;&#36807;&#20102;&#36825;&#20004;&#39033;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17212v1 Announce Type: cross  Abstract: Explanations for machine learning models can be hard to interpret or be wrong. Combining an explanation method with an uncertainty estimation method produces explanation uncertainty. Evaluating explanation uncertainty is difficult. In this paper we propose sanity checks for uncertainty explanation methods, where a weight and data randomization tests are defined for explanations with uncertainty, allowing for quick tests to combinations of uncertainty and explanation methods. We experimentally show the validity and effectiveness of these tests on the CIFAR10 and California Housing datasets, noting that Ensembles seem to consistently pass both tests with Guided Backpropagation, Integrated Gradients, and LIME explanations.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.17210</link><description>&lt;p&gt;
CADGL: &#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17210
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDIs&#65289;&#30340;&#30740;&#31350;&#26159;&#33647;&#29289;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20803;&#32032;&#12290;DDIs&#21457;&#29983;&#22312;&#19968;&#20010;&#33647;&#29289;&#30340;&#24615;&#36136;&#21463;&#20854;&#20182;&#33647;&#29289;&#21253;&#21547;&#30340;&#24433;&#21709;&#26102;&#12290;&#26816;&#27979;&#26377;&#21033;&#30340;DDIs&#26377;&#21487;&#33021;&#20026;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#24212;&#29992;&#30340;&#21019;&#26032;&#33647;&#29289;&#30340;&#21019;&#36896;&#21644;&#25512;&#36827;&#38138;&#24179;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#12289;&#31283;&#20581;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#29616;&#23454;&#24212;&#29992;&#21487;&#33021;&#24615;&#26041;&#38754;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;CADGL&#30340;&#26032;&#39062;&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22522;&#20110;&#23450;&#21046;&#30340;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;VGAE&#65289;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#19978;&#19979;&#25991;&#39044;&#22788;&#29702;&#22120;&#20174;&#20004;&#20010;&#19981;&#21516;&#35270;&#35282;&#65306;&#23616;&#37096;&#37051;&#22495;&#21644;&#20998;&#23376;&#19978;&#19979;&#25991;&#65292;&#22312;&#24322;&#36136;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#25429;&#33719;&#20851;&#38190;&#30340;&#32467;&#26500;&#21644;&#29983;&#29702;&#21270;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17210v1 Announce Type: cross  Abstract: Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Ou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.17209</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65306;&#25968;&#23383;&#23402;&#29983;&#21644;&#35821;&#20041;&#33410;&#28857;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17209
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21327;&#21161;&#22312;&#24037;&#19994;4.0&#32972;&#26223;&#19979;&#20026;&#25968;&#23383;&#23402;&#29983;&#24314;&#27169;&#21019;&#24314;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65288;AAS&#65289;&#23454;&#20363;&#65292;&#26088;&#22312;&#22686;&#24378;&#26234;&#33021;&#21046;&#36896;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#20943;&#23569;&#25163;&#21160;&#24037;&#20316;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#25968;&#25454;&#32467;&#26500;&#26469;&#25429;&#25417;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#35201;&#20041;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#24182;&#20174;&#25991;&#26412;&#25216;&#26415;&#25968;&#25454;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#26377;&#25928;&#29983;&#25104;&#29575;&#20026;62-79%&#65292;&#34920;&#26126;&#30456;&#24403;&#27604;&#20363;&#30340;&#25163;&#21160;&#21019;&#24314;&#24037;&#20316;&#21487;&#20197;&#36716;&#25442;&#20026;&#26356;&#23481;&#26131;&#30340;&#39564;&#35777;&#24037;&#20316;&#65292;&#20174;&#32780;&#20943;&#23569;&#21019;&#24314;AAS&#23454;&#20363;&#27169;&#22411;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#23545;&#19981;&#21516;LLM&#30340;&#27604;&#36739;&#20998;&#26512;&#20197;&#21450;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26426;&#21046;&#30340;&#28145;&#20837;&#28040;&#34701;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;LLM&#26377;&#25928;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17209v1 Announce Type: new  Abstract: This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a "semantic node" data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process "semantic node" and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM
&lt;/p&gt;</description></item><item><title>NUMTEMP&#26159;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#39564;&#35777;&#22797;&#26434;&#30340;&#25968;&#23383;&#35770;&#28857;&#65292;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#39564;&#35777;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17169</link><description>&lt;p&gt;
NUMTEMP&#65306;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#24102;&#26377;&#32479;&#35745;&#21644;&#26102;&#38388;&#34920;&#36798;&#24335;&#30340;&#35770;&#28857;&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17169
&lt;/p&gt;
&lt;p&gt;
NUMTEMP&#26159;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#39564;&#35777;&#22797;&#26434;&#30340;&#25968;&#23383;&#35770;&#28857;&#65292;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#39564;&#35777;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26816;&#26597;&#22312;&#25968;&#23383;&#26102;&#20195;&#24212;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#38169;&#35823;&#20449;&#24687;&#26041;&#38754;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#29616;&#26377;&#31995;&#32479;&#20027;&#35201;&#19987;&#27880;&#20110;&#32500;&#22522;&#30334;&#31185;&#19978;&#30340;&#21512;&#25104;&#35770;&#28857;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#19990;&#30028;&#35770;&#28857;&#19978;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;Numtemp&#65292;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#22810;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#20851;&#27880;&#25968;&#23383;&#35770;&#28857;&#65292;&#21253;&#25324;&#26102;&#38388;&#12289;&#32479;&#35745;&#21644;&#22810;&#26679;&#21270;&#26041;&#38754;&#30340;&#32454;&#31890;&#24230;&#20803;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#19981;&#27844;&#38706;&#30340;&#35777;&#25454;&#25910;&#38598;&#12290;&#36825;&#35299;&#20915;&#20102;&#39564;&#35777;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#35770;&#28857;&#22797;&#26434;&#65292;&#24448;&#24448;&#32570;&#20047;&#31934;&#30830;&#20449;&#24687;&#65292;&#36825;&#26159;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#21512;&#25104;&#35770;&#28857;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35780;&#20272;&#24182;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22312;&#39564;&#35777;&#25968;&#23383;&#35770;&#28857;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22522;&#20110;&#35770;&#28857;&#20998;&#35299;&#30340;&#26041;&#27861;&#12289;&#22522;&#20110;&#25968;&#23383;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#22522;&#32447;&#23454;&#29616;&#20102;58.32&#30340;&#23439;F1&#20998;&#25968;&#12290;&#36825;&#35777;&#26126;&#20102;Numtemp&#30340;&#20851;&#38190;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17169v1 Announce Type: cross  Abstract: Automated fact checking has gained immense interest to tackle the growing misinformation in the digital era. Existing systems primarily focus on synthetic claims on Wikipedia, and noteworthy progress has also been made on real-world claims. In this work, we release Numtemp, a diverse, multi-domain dataset focused exclusively on numerical claims, encompassing temporal, statistical and diverse aspects with fine-grained metadata and an evidence collection without leakage. This addresses the challenge of verifying real-world numerical claims, which are complex and often lack precise information, not addressed by existing works that mainly focus on synthetic claims. We evaluate and quantify the limitations of existing solutions for the task of verifying numerical claims. We also evaluate claim decomposition based methods, numerical understanding based models and our best baselines achieves a macro-F1 of 58.32. This demonstrates that Numtemp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#20026;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#25171;&#24320;&#20102;&#19968;&#26465;&#26032;&#36884;&#24452;&#65292;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#22810;&#26679;&#29305;&#24449;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#65292;&#21487;&#20197;&#20248;&#21270;&#26230;&#20307;&#32467;&#26500;&#31283;&#23450;&#24615;&#20197;&#21450;&#20854;&#20182;&#30446;&#26631;&#22914;&#30913;&#24615;&#25110;&#28909;&#30005;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17164</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36136;&#37327;&#22810;&#26679;&#24615;&#29992;&#20110;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Quality-Diversity for Crystal Structure Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#20026;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#25171;&#24320;&#20102;&#19968;&#26465;&#26032;&#36884;&#24452;&#65292;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#22810;&#26679;&#29305;&#24449;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#65292;&#21487;&#20197;&#20248;&#21270;&#26230;&#20307;&#32467;&#26500;&#31283;&#23450;&#24615;&#20197;&#21450;&#20854;&#20182;&#30446;&#26631;&#22914;&#30913;&#24615;&#25110;&#28909;&#30005;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#20307;&#32467;&#26500;&#22312;&#20174;&#30005;&#27744;&#21040;&#22826;&#38451;&#33021;&#30005;&#27744;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#38024;&#23545;&#20854;&#21407;&#23376;&#37197;&#32622;&#39044;&#27979;&#24615;&#33021;&#24050;&#32463;&#26377;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#20391;&#37325;&#20110;&#35782;&#21035;&#33021;&#37327;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#22788;&#30340;&#26368;&#31283;&#23450;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#24573;&#30053;&#20102;&#37027;&#20123;&#21487;&#33021;&#20301;&#20110;&#30456;&#37051;&#23616;&#37096;&#26497;&#23567;&#20540;&#22788;&#12289;&#20855;&#26377;&#19981;&#21516;&#26448;&#26009;&#29305;&#24615;&#65288;&#22914;&#30005;&#23548;&#29575;&#25110;&#25239;&#21464;&#24418;&#24615;&#65289;&#30340;&#20854;&#20182;&#26377;&#36259;&#26448;&#26009;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#20026;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#65292;&#22240;&#20026;&#23427;&#26088;&#22312;&#25214;&#21040;&#20855;&#26377;&#22810;&#26679;&#29305;&#24449;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#26230;&#20307;&#32467;&#26500;&#31283;&#23450;&#24615;&#20197;&#21450;&#20854;&#20182;&#30446;&#26631;&#65288;&#22914;&#30913;&#24615;&#25110;&#28909;&#30005;&#25928;&#29575;&#65289;&#20063;&#21487;&#33021;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17164v1 Announce Type: cross  Abstract: Crystal structures are indispensable across various domains, from batteries to solar cells, and extensive research has been dedicated to predicting their properties based on their atomic configurations. However, prevailing Crystal Structure Prediction methods focus on identifying the most stable solutions that lie at the global minimum of the energy function. This approach overlooks other potentially interesting materials that lie in neighbouring local minima and have different material properties such as conductivity or resistance to deformation. By contrast, Quality-Diversity algorithms provide a promising avenue for Crystal Structure Prediction as they aim to find a collection of high-performing solutions that have diverse characteristics. However, it may also be valuable to optimise for the stability of crystal structures alongside other objectives such as magnetism or thermoelectric efficiency. Therefore, in this work, we harness 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#21270;TSP&#22270;&#34920;&#31034;&#21644;&#27880;&#24847;&#21147;&#25513;&#30721;&#65292;&#20351;&#32534;&#30721;&#22120;&#38598;&#20013;&#20110;TSP&#23454;&#20363;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#21516;&#26102;&#20801;&#35768;&#20449;&#24687;&#22312;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#33258;&#30001;&#27969;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.17159</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810; - &#20851;&#20110;&#31232;&#30095;&#21270;&#22312;Transformers&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;TSP&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Less Is More - On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17159
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#21270;TSP&#22270;&#34920;&#31034;&#21644;&#27880;&#24847;&#21147;&#25513;&#30721;&#65292;&#20351;&#32534;&#30721;&#22120;&#38598;&#20013;&#20110;TSP&#23454;&#20363;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#21516;&#26102;&#20801;&#35768;&#20449;&#24687;&#22312;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#33258;&#30001;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#36817;&#30740;&#31350;&#22788;&#29702;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#31561;&#36335;&#30001;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;transformer&#25110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#35768;&#22810;&#30740;&#31350;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#32534;&#30721;&#22120;&#65292;&#20801;&#35768;&#23427;&#20204;&#22312;&#25972;&#20010;TSP&#23454;&#20363;&#19978;&#32858;&#21512;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#32534;&#30721;&#22120;&#20165;&#20851;&#27880;TSP&#23454;&#20363;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;&#20256;&#36882;&#32473;GNN&#30340;TSP&#22270;&#34920;&#31034;&#25552;&#20986;&#20102;&#22270;&#31232;&#30095;&#21270;&#65292;&#24182;&#20026;&#20256;&#36882;&#32473;transformers&#30340;TSP&#23454;&#20363;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#23631;&#34109;&#65292;&#20854;&#20013;mask&#23545;&#24212;&#20110;&#31232;&#30095;TSP&#22270;&#34920;&#31034;&#30340;&#37051;&#25509;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#31232;&#30095;&#21270;&#32423;&#21035;&#30340;&#38598;&#21512;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#19987;&#27880;&#20110;&#26368;&#26377;&#21069;&#36884;&#30340;&#37096;&#20998;&#65292;&#21516;&#26102;&#36824;&#20801;&#35768;TSP&#23454;&#20363;&#30340;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17159v1 Announce Type: cross  Abstract: Most of the recent studies tackling routing problems like the Traveling Salesman Problem (TSP) with machine learning use a transformer or Graph Neural Network (GNN) based encoder architecture. However, many of them apply these encoders naively by allowing them to aggregate information over the whole TSP instances. We, on the other hand, propose a data preprocessing method that allows the encoders to focus on the most relevant parts of the TSP instances only. In particular, we propose graph sparsification for TSP graph representations passed to GNNs and attention masking for TSP instances passed to transformers where the masks correspond to the adjacency matrices of the sparse TSP graph representations. Furthermore, we propose ensembles of different sparsification levels allowing models to focus on the most promising parts while also allowing information flow between all nodes of a TSP instance. In the experimental studies, we show that
&lt;/p&gt;</description></item><item><title>&#39057;&#35889;&#32676;&#20307;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#20449;&#24687;&#25193;&#25955;&#26469;&#37325;&#24314;&#31454;&#25216;&#22330;&#30340;&#20960;&#20309;&#24418;&#29366;</title><link>https://arxiv.org/abs/2403.17147</link><description>&lt;p&gt;
&#29992;&#39057;&#35889;&#32676;&#20307;&#26426;&#22120;&#20154;&#21548;&#21040;&#19968;&#20010;&#31454;&#25216;&#22330;&#30340;&#24418;&#29366;
&lt;/p&gt;
&lt;p&gt;
Hearing the shape of an arena with spectral swarm robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17147
&lt;/p&gt;
&lt;p&gt;
&#39057;&#35889;&#32676;&#20307;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#20449;&#24687;&#25193;&#25955;&#26469;&#37325;&#24314;&#31454;&#25216;&#22330;&#30340;&#20960;&#20309;&#24418;&#29366;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Swarm&#26426;&#22120;&#20154;&#25216;&#26415;&#25215;&#35834;&#36866;&#24212;&#26410;&#30693;&#24773;&#20917;&#65292;&#24182;&#19988;&#23545;&#20110;&#25925;&#38556;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#28982;&#22312;&#38656;&#35201;&#29702;&#35299;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26356;&#24191;&#27867;&#32972;&#26223;&#30340;&#20840;&#23616;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#27604;&#22914;&#35782;&#21035;&#26426;&#22120;&#20154;&#23884;&#20837;&#30340;&#31454;&#25216;&#22330;&#30340;&#24418;&#29366;&#12290;&#29983;&#29289;&#32676;&#20307;&#65292;&#27604;&#22914;&#40060;&#32676;&#12289;&#40479;&#32676;&#21644;&#26118;&#34411;&#32676;&#65292;&#36890;&#36807;&#23616;&#37096;&#25552;&#31034;&#30340;&#25193;&#25955;&#20363;&#34892;&#35299;&#20915;&#20840;&#23616;&#20960;&#20309;&#38382;&#39064;&#12290;&#36825;&#19968;&#33539;&#24335;&#21487;&#20197;&#36890;&#36807;&#25968;&#23398;&#27169;&#22411;&#26126;&#30830;&#25551;&#36848;&#65292;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#20154;&#32676;&#20307;&#30452;&#25509;&#35745;&#31639;&#21644;&#21033;&#29992;&#12290;&#22495;&#19978;&#30340;&#25193;&#25955;&#22312;&#25968;&#23398;&#19978;&#30001;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#36827;&#34892;&#23553;&#35013;&#65292;&#35813;&#31639;&#23376;&#29992;&#20110;&#24230;&#37327;&#20989;&#25968;&#30340;&#23616;&#37096;&#26354;&#29575;&#12290;&#20851;&#38190;&#26159;&#65292;&#19968;&#20010;&#21306;&#22495;&#30340;&#20960;&#20309;&#24418;&#29366;&#36890;&#24120;&#21487;&#20197;&#20174;&#20854;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#35889;&#20013;&#37325;&#24314;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39057;&#35889;&#32676;&#20307;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#23558;&#20449;&#24687;&#25193;&#25955;&#32473;&#20182;&#20204;&#30340;&#37051;&#23621;&#26469;&#27169;&#25311;&#25289;&#26222;&#25289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17147v1 Announce Type: cross  Abstract: Swarm robotics promises adaptability to unknown situations and robustness against failures. However, it still struggles with global tasks that require understanding the broader context in which the robots operate, such as identifying the shape of the arena in which the robots are embedded. Biological swarms, such as shoals of fish, flocks of birds, and colonies of insects, routinely solve global geometrical problems through the diffusion of local cues. This paradigm can be explicitly described by mathematical models that could be directly computed and exploited by a robotic swarm. Diffusion over a domain is mathematically encapsulated by the Laplacian, a linear operator that measures the local curvature of a function. Crucially the geometry of a domain can generally be reconstructed from the eigenspectrum of its Laplacian. Here we introduce spectral swarm robotics where robots diffuse information to their neighbors to emulate the Lapla
&lt;/p&gt;</description></item><item><title>MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;</title><link>https://arxiv.org/abs/2403.17141</link><description>&lt;p&gt;
MetaAligner&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#36890;&#29992;&#22810;&#30446;&#26631;&#23545;&#40784;&#30340;&#26465;&#20214;&#20174;&#24369;&#21040;&#24378;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17141
&lt;/p&gt;
&lt;p&gt;
MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#26088;&#22312;&#36890;&#36807;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26469;&#35299;&#20915;&#24322;&#36136;&#20154;&#31867;&#26399;&#26395;&#21644;&#20215;&#20540;&#35266;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#21040;&#31574;&#30053;&#27169;&#22411;&#30340;&#21442;&#25968;&#38480;&#21046;&#65292;&#23548;&#33268;&#20004;&#20010;&#20851;&#38190;&#23616;&#38480;&#24615;&#65306;&#65288;1&#65289;&#23427;&#20204;&#30340;&#23545;&#40784;&#31639;&#27861;&#23545;&#20110;&#27599;&#20010;&#26032;&#30446;&#26631;&#27169;&#22411;&#30340;&#37325;&#22797;&#25104;&#26412;&#24456;&#39640;&#65307;&#65288;2&#65289;&#30001;&#20110;&#20854;&#38745;&#24577;&#23545;&#40784;&#30446;&#26631;&#65292;&#23427;&#20204;&#26080;&#27861;&#25193;&#23637;&#21040;&#26410;&#35265;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-Objective Aligner&#65288;MetaAligner&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25191;&#34892;&#26465;&#20214;&#20174;&#24369;&#21040;&#24378;&#26657;&#27491;&#20197;&#36924;&#36817;&#24378;&#21709;&#24212;&#30340;&#27169;&#22411;&#12290;MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MetaAligner&#21462;&#24471;&#20102;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17141v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning. Experimental results show that MetaAligner achieves sign
&lt;/p&gt;</description></item><item><title>RepairAgent&#26159;&#39318;&#20010;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#31243;&#24207;&#26469;&#35299;&#20915;&#31243;&#24207;&#20462;&#22797;&#25361;&#25112;&#30340;&#24037;&#20316;&#65292;&#20854;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#19968;&#32452;&#26377;&#21161;&#20110;&#31243;&#24207;&#20462;&#22797;&#30340;&#24037;&#20855;&#20197;&#21450;&#21160;&#24577;&#26356;&#26032;&#30340;&#25552;&#31034;&#26684;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17134</link><description>&lt;p&gt;
RepairAgent&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#31243;&#24207;&#20462;&#22797;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
RepairAgent: An Autonomous, LLM-Based Agent for Program Repair
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17134
&lt;/p&gt;
&lt;p&gt;
RepairAgent&#26159;&#39318;&#20010;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#31243;&#24207;&#26469;&#35299;&#20915;&#31243;&#24207;&#20462;&#22797;&#25361;&#25112;&#30340;&#24037;&#20316;&#65292;&#20854;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#19968;&#32452;&#26377;&#21161;&#20110;&#31243;&#24207;&#20462;&#22797;&#30340;&#24037;&#20855;&#20197;&#21450;&#21160;&#24577;&#26356;&#26032;&#30340;&#25552;&#31034;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20943;&#36731;&#36719;&#20214;&#28431;&#27934;&#23545;&#31995;&#32479;&#21487;&#38752;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RepairAgent&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33258;&#20027;&#20195;&#29702;&#35299;&#20915;&#31243;&#24207;&#20462;&#22797;&#25361;&#25112;&#30340;&#24037;&#20316;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#29992;&#22266;&#23450;&#30340;&#25552;&#31034;&#25110;&#22312;&#22266;&#23450;&#30340;&#21453;&#39304;&#24490;&#29615;&#20013;&#25552;&#31034;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;LLM&#35270;&#20026;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#35268;&#21010;&#21644;&#25191;&#34892;&#20462;&#22797;&#25805;&#20316;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#36890;&#36807;&#35843;&#29992;&#36866;&#24403;&#30340;&#24037;&#20855;&#20462;&#22797;&#28431;&#27934;&#12290;RepairAgent&#33258;&#30001;&#22320;&#20132;&#32455;&#30528;&#25910;&#38598;&#26377;&#20851;&#28431;&#27934;&#30340;&#20449;&#24687;&#12289;&#25910;&#38598;&#20462;&#22797;&#26448;&#26009;&#20197;&#21450;&#39564;&#35777;&#20462;&#22797;&#36807;&#31243;&#65292;&#24182;&#26681;&#25454;&#25910;&#38598;&#21040;&#30340;&#20449;&#24687;&#21644;&#20808;&#21069;&#30340;&#20462;&#22797;&#23581;&#35797;&#21453;&#39304;&#20915;&#23450;&#35843;&#29992;&#21738;&#20123;&#24037;&#20855;&#12290;&#23454;&#29616;RepairAgent&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#19968;&#32452;&#26377;&#21161;&#20110;&#31243;&#24207;&#20462;&#22797;&#30340;&#24037;&#20855;&#65292;&#20197;&#21450;&#19968;&#20010;&#21160;&#24577;&#26356;&#26032;&#30340;&#25552;&#31034;&#26684;&#24335;&#65292;&#20351;LLM&#33021;&#22815;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17134v1 Announce Type: cross  Abstract: Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#20808;&#21069;&#22312;&#23569;&#20110;&#19968;&#27425;&#23398;&#20064;&#20013;&#25552;&#20986;&#30340;&#31616;&#21333;&#33976;&#39311;&#25216;&#26415;&#22312;&#21407;&#22411;&#36719;&#26631;&#31614;&#33976;&#39311;&#20013;&#30340;&#28508;&#21147;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#25972;&#21512;&#20248;&#21270;&#27493;&#39588;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17130</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#21407;&#22411;&#30340;&#36719;&#26631;&#31614;&#25968;&#25454;&#33976;&#39311;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#31867;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the potential of prototype-based soft-labels data distillation for imbalanced data classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#20808;&#21069;&#22312;&#23569;&#20110;&#19968;&#27425;&#23398;&#20064;&#20013;&#25552;&#20986;&#30340;&#31616;&#21333;&#33976;&#39311;&#25216;&#26415;&#22312;&#21407;&#22411;&#36719;&#26631;&#31614;&#33976;&#39311;&#20013;&#30340;&#28508;&#21147;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#25972;&#21512;&#20248;&#21270;&#27493;&#39588;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26088;&#22312;&#36890;&#36807;&#23569;&#37327;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#39033;&#21512;&#25104;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24403;&#36825;&#20123;&#25968;&#25454;&#34987;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#33021;&#22815;&#37325;&#29616;&#25110;&#36924;&#36817;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#23601;&#22909;&#20687;&#23427;&#26159;&#22312;&#25972;&#20010;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#19968;&#26679;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#33976;&#39311;&#26041;&#27861;&#36890;&#24120;&#19982;&#29305;&#23450;&#30340;ML&#31639;&#27861;&#26377;&#20851;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#25991;&#29486;&#20027;&#35201;&#28041;&#21450;&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32972;&#26223;&#19979;&#23545;&#22823;&#37327;&#22270;&#20687;&#30340;&#33976;&#39311;&#65292;&#20294;&#34920;&#26684;&#25968;&#25454;&#30340;&#33976;&#39311;&#20195;&#34920;&#24615;&#36739;&#20302;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#29702;&#35770;&#35270;&#35282;&#19978;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#23569;&#20110;&#19968;&#27425;&#23398;&#20064;&#20013;&#25552;&#20986;&#30340;&#31616;&#21333;&#33976;&#39311;&#25216;&#26415;&#22312;&#21407;&#22411;&#36719;&#26631;&#31614;&#33976;&#39311;&#20013;&#30340;&#28508;&#21147;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#25972;&#21512;&#20248;&#21270;&#27493;&#39588;&#65292;&#25512;&#21160;&#22522;&#20110;&#21407;&#22411;&#30340;&#36719;&#26631;&#31614;&#33976;&#39311;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;&#35813;&#20998;&#26512;&#26159;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17130v1 Announce Type: cross  Abstract: Dataset distillation aims at synthesizing a dataset by a small number of artificially generated data items, which, when used as training data, reproduce or approximate a machine learning (ML) model as if it were trained on the entire original dataset. Consequently, data distillation methods are usually tied to a specific ML algorithm. While recent literature deals mainly with distillation of large collections of images in the context of neural network models, tabular data distillation is much less represented and mainly focused on a theoretical perspective. The current paper explores the potential of a simple distillation technique previously proposed in the context of Less-than-one shot learning. The main goal is to push further the performance of prototype-based soft-labels distillation in terms of classification accuracy, by integrating optimization steps in the distillation process. The analysis is performed on real-world data sets
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#20381;&#36182;&#32972;&#26223;&#30693;&#35782;&#65288;&#20808;&#39564;&#30693;&#35782;&#65289;&#65292;&#20294;&#26080;&#27861;&#23436;&#20840;&#25972;&#21512;&#19982;&#20219;&#21153;&#20808;&#39564;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#20449;&#24687;&#65292;&#24433;&#21709;&#20102;&#24773;&#32490;&#35782;&#21035;&#31561;&#20027;&#35266;&#20219;&#21153;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.17125</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20808;&#39564;&#30693;&#35782;&#30340;&#24378;&#22823;&#20316;&#29992;&#21450;&#20854;&#23545;&#24773;&#32490;&#35782;&#21035;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17125
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#20381;&#36182;&#32972;&#26223;&#30693;&#35782;&#65288;&#20808;&#39564;&#30693;&#35782;&#65289;&#65292;&#20294;&#26080;&#27861;&#23436;&#20840;&#25972;&#21512;&#19982;&#20219;&#21153;&#20808;&#39564;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#20449;&#24687;&#65292;&#24433;&#21709;&#20102;&#24773;&#32490;&#35782;&#21035;&#31561;&#20027;&#35266;&#20219;&#21153;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
In-context Learning (ICL)&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#28014;&#29616;&#20986;&#26469;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24494;&#35843;&#30456;&#21453;&#12290; ICL&#30340;&#25215;&#35834;&#26159;&#65292;LLM&#21487;&#20197;&#36866;&#24212;&#25191;&#34892;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#20197;&#31454;&#20105;&#21147;&#25110;&#26368;&#26032;&#27700;&#24179;&#30340;&#19968;&#23567;&#37096;&#20998;&#25104;&#26412;&#12290; LLM&#20197;&#36825;&#31181;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#23427;&#20204;&#23545;&#20219;&#21153;&#30340;&#32972;&#26223;&#30693;&#35782;&#65288;&#25110;&#20219;&#21153;&#20808;&#39564;&#30693;&#35782;&#65289;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#20256;&#32479;&#23398;&#20064;&#19981;&#21516;&#65292;LLM&#26080;&#27861;&#23436;&#20840;&#25972;&#21512;&#19982;&#20219;&#21153;&#20808;&#39564;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#28436;&#31034;&#20449;&#24687;&#12290; &#36825;&#21487;&#33021;&#23548;&#33268;&#34920;&#29616;&#36798;&#21040;&#27425;&#20248;&#27700;&#24179;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20027;&#35266;&#20219;&#21153;&#65288;&#22914;&#24773;&#32490;&#35782;&#21035;&#65289;&#65292;&#20854;&#20013;&#25991;&#26412;&#21040;&#24773;&#32490;&#30340;&#26144;&#23556;&#21487;&#33021;&#22240;&#20154;&#31867;&#27880;&#37322;&#30340;&#21464;&#24322;&#24615;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#23454;&#39564;&#24182;&#25552;&#20986;&#20102;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17125v1 Announce Type: cross  Abstract: In-context Learning (ICL) has emerged as a powerful paradigm for performing natural language tasks with Large Language Models (LLM) without updating the models' parameters, in contrast to the traditional gradient-based finetuning. The promise of ICL is that the LLM can adapt to perform the present task at a competitive or state-of-the-art level at a fraction of the cost. The ability of LLMs to perform tasks in this few-shot manner relies on their background knowledge of the task (or task priors). However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition, where the mapping from text to emotions can differ widely due to variability in human annotations. In this work, we design experiments and propose measurements to e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;LLMs&#26469;&#25351;&#23548;&#22810;&#27493;&#28436;&#31034;&#20013;&#38544;&#21547;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#25628;&#32034;&#65292;&#20197;&#21450;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#28436;&#31034;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#12290;</title><link>https://arxiv.org/abs/2403.17124</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#35745;&#21010;&#22522;&#20110;&#28436;&#31034;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#36827;&#34892;&#33853;&#23454;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Plans in Demonstrations Through Counterfactual Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17124
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;LLMs&#26469;&#25351;&#23548;&#22810;&#27493;&#28436;&#31034;&#20013;&#38544;&#21547;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#25628;&#32034;&#65292;&#20197;&#21450;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#28436;&#31034;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#25512;&#29702;&#22522;&#20110;&#29289;&#29702;&#39046;&#22495;&#33853;&#23454;&#22312;&#20307;&#29616;&#26234;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#19987;&#27880;&#20110;&#30452;&#25509;&#21033;&#29992;LLMs&#22312;&#31526;&#21495;&#31354;&#38388;&#20869;&#35268;&#21010;&#65292;&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;LLMs&#25351;&#23548;&#20219;&#21153;&#32467;&#26500;&#30340;&#25628;&#32034;&#65292;&#38544;&#21547;&#22312;&#22810;&#27493;&#28436;&#31034;&#20013;&#30340;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#25805;&#32437;&#35268;&#21010;&#25991;&#29486;&#20013;&#30340;&#27169;&#24335;&#26063;&#30340;&#27010;&#24565;&#65292;&#23427;&#25353;&#29031;&#29305;&#23450;&#36816;&#21160;&#32422;&#26463;&#23558;&#26426;&#22120;&#20154;&#37197;&#32622;&#20998;&#32452;&#65292;&#20316;&#20026;LLM&#39640;&#32423;&#35821;&#35328;&#34920;&#31034;&#21644;&#26426;&#22120;&#20154;&#20302;&#32423;&#29289;&#29702;&#36712;&#36857;&#20043;&#38388;&#30340;&#25277;&#35937;&#23618;&#12290;&#36890;&#36807;&#29992;&#21512;&#25104;&#24178;&#25200;&#37325;&#26032;&#25773;&#25918;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#35206;&#30422;&#28436;&#31034;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#39069;&#22806;&#29983;&#25104;&#25104;&#21151;&#25191;&#34892;&#20197;&#21450;&#26410;&#23436;&#25104;&#20219;&#21153;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#23398;&#20064;&#26694;&#26550;&#35757;&#32451;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17124v1 Announce Type: cross  Abstract: Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural networ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20445;&#25252;&#32593;&#32476;&#20013;&#33410;&#28857;&#20813;&#21463;&#22810;&#20010;&#21516;&#26102;&#25915;&#20987;&#65292;&#29305;&#21035;&#20851;&#27880;$k$-&#24378;&#32599;&#39532;&#21344;&#39046;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#22270;&#20013;&#20998;&#37197;&#37096;&#38431;&#25968;&#37327;&#20197;&#26368;&#23567;&#21270;&#24635;&#26435;&#37325;&#20197;&#28385;&#36275;&#20445;&#25252;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.17108</link><description>&lt;p&gt;
&#22312;&#22810;&#20010;&#21516;&#26102;&#25915;&#20987;&#19979;&#23545;&#22270;&#30340;&#20445;&#25252;&#65306;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Protection under Multiple Simultaneous Attacks: A Heuristic Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20445;&#25252;&#32593;&#32476;&#20013;&#33410;&#28857;&#20813;&#21463;&#22810;&#20010;&#21516;&#26102;&#25915;&#20987;&#65292;&#29305;&#21035;&#20851;&#27880;$k$-&#24378;&#32599;&#39532;&#21344;&#39046;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#22270;&#20013;&#20998;&#37197;&#37096;&#38431;&#25968;&#37327;&#20197;&#26368;&#23567;&#21270;&#24635;&#26435;&#37325;&#20197;&#28385;&#36275;&#20445;&#25252;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#38450;&#33539;&#23545;&#20351;&#29992;&#22270;&#27169;&#22411;&#30340;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#30340;&#22810;&#20010;&#21516;&#26102;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;$k$-&#24378;&#32599;&#39532;&#21344;&#39046;&#38382;&#39064;&#65292;&#36825;&#26159;&#23545;&#22270;&#19978;&#33879;&#21517;&#32599;&#39532;&#21344;&#39046;&#38382;&#39064;&#30340;&#19968;&#33324;&#21270;&#12290;&#35813;&#38382;&#39064;&#26159;&#20851;&#20110;&#20026;&#33410;&#28857;&#20998;&#37197;&#25972;&#25968;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#20195;&#34920;&#30528;&#27599;&#20010;&#33410;&#28857;&#39547;&#25166;&#30340;&#37096;&#38431;&#25968;&#37327;&#65292;&#20197;&#28385;&#36275;&#20445;&#25252;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#24635;&#26435;&#37325;&#12290;&#36825;&#20123;&#32422;&#26463;&#28041;&#21450;&#23545;&#22270;&#23545;&#25239;&#20219;&#20309;&#30001;$k \in \mathbb{N}$&#20010;&#33410;&#28857;&#32452;&#25104;&#30340;&#21516;&#26102;&#25915;&#20987;&#30340;&#20445;&#25252;&#12290;&#22914;&#26524;&#21487;&#20197;&#36890;&#36807;&#20174;&#19968;&#20010;&#30456;&#37051;&#33410;&#28857;&#20511;&#29992;&#20891;&#38431;&#26469;&#20445;&#25252;&#26631;&#35760;&#20026;0&#30340;&#27599;&#20010;&#33410;&#28857;&#65292;&#24182;&#30830;&#20445;&#35813;&#37051;&#23621;&#33267;&#23569;&#20445;&#30041;&#19968;&#25903;&#20891;&#38431;&#36827;&#34892;&#33258;&#21355;&#65292;&#21017;&#25915;&#20987;&#34987;&#35270;&#20026;&#34987;&#20987;&#36864;&#12290;$k$-SRD&#38382;&#39064;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#21046;&#23450;&#21453;&#24656;&#31574;&#30053;&#25110;&#31649;&#29702;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17108v1 Announce Type: new  Abstract: This work focuses on developing an effective meta-heuristic approach to protect against simultaneous attacks on nodes of a network modeled using a graph. Specifically, we focus on the $k$-strong Roman domination problem, a generalization of the well-known Roman domination problem on graphs. This general problem is about assigning integer weights to nodes that represent the number of field armies stationed at each node in order to satisfy the protection constraints while minimizing the total weights. These constraints concern the protection of a graph against any simultaneous attack consisting of $k \in \mathbb{N}$ nodes. An attack is considered repelled if each node labeled 0 can be defended by borrowing an army from one of its neighboring nodes, ensuring that the neighbor retains at least one army for self-defense. The $k$-SRD problem has practical applications in various areas, such as developing counter-terrorism strategies or managin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26426;&#22120;&#27169;&#22411;&#65292;&#25903;&#25345;&#20102;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#36825;&#19968;&#35770;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.17101</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65306;&#19968;&#20010;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
AI Consciousness is Inevitable: A Theoretical Computer Science Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26426;&#22120;&#27169;&#22411;&#65292;&#25903;&#25345;&#20102;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#36825;&#19968;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#26469;&#23457;&#35270;&#24847;&#35782;&#65292;&#36825;&#26159;&#25968;&#23398;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#30740;&#31350;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#30340;&#35745;&#31639;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#20026;&#24847;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#26426;&#22120;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#21463;&#21040;&#20102;&#33406;&#20262;&#183;&#22270;&#28789;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#35745;&#31639;&#27169;&#22411;&#21644;&#20271;&#32435;&#24503;&#183;&#24052;&#23572;&#26031;&#24847;&#35782;&#21095;&#22330;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#23613;&#31649;&#38750;&#24120;&#31616;&#21333;&#65292;&#36825;&#20010;&#27169;&#22411;&#22312;&#39640;&#23618;&#27425;&#19978;&#19982;&#35768;&#22810;&#20851;&#20110;&#20154;&#31867;&#21644;&#21160;&#29289;&#24847;&#35782;&#30340;&#20027;&#35201;&#31185;&#23398;&#29702;&#35770;&#30456;&#19968;&#33268;&#65292;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#26029;&#65306;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17101v1 Announce Type: new  Abstract: We look at consciousness through the lens of Theoretical Computer Science, a branch of mathematics that studies computation under resource limitations. From this perspective, we develop a formal machine model for consciousness. The model is inspired by Alan Turing's simple yet powerful model of computation and Bernard Baars' theater model of consciousness. Though extremely simple, the model aligns at a high level with many of the major scientific theories of human and animal consciousness, supporting our claim that machine consciousness is inevitable.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#33073;&#26426;&#31574;&#30053;&#35780;&#20272;&#20219;&#21153;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#21463;&#32858;&#21512;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#27987;&#32553;&#31995;&#25968;&#25511;&#21046;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;MDP&#20013;&#30340;&#31995;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.17091</link><description>&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#65306;&#29366;&#24577;&#32858;&#21512;&#21644;&#36712;&#36857;&#25968;&#25454;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17091
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#33073;&#26426;&#31574;&#30053;&#35780;&#20272;&#20219;&#21153;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#21463;&#32858;&#21512;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#27987;&#32553;&#31995;&#25968;&#25511;&#21046;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;MDP&#20013;&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20855;&#26377;&#20215;&#20540;&#20989;&#25968;&#21487;&#23454;&#29616;&#24615;&#20294;&#19981;&#20855;&#26377;&#36125;&#23572;&#26364;&#23436;&#22791;&#24615;&#30340;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#33073;&#26426;&#31574;&#30053;&#35780;&#20272;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21463;&#32858;&#21512;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#27987;&#32553;&#31995;&#25968;&#25511;&#21046;&#30340;&#21457;&#29616;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#20165;&#20855;&#26377;&#20215;&#20540;&#20989;&#25968;&#21487;&#23454;&#29616;&#24615;&#30340;&#33073;&#26426;&#31574;&#30053;&#35780;&#20272;&#30340;&#30456;&#24403;&#23436;&#25972;&#30340;&#22270;&#26223;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26377;&#19977;&#20010;&#65306;1&#65289;&#33073;&#26426;&#31574;&#30053;&#35780;&#20272;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30001;&#32858;&#21512;&#30340;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#38598;&#20013;&#31995;&#25968;&#20915;&#23450;&#65292;&#36825;&#20010;&#31995;&#25968;&#30001;&#20989;&#25968;&#31867;&#21644;&#33073;&#26426;&#25968;&#25454;&#20998;&#24067;&#20849;&#21516;&#30830;&#23450;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;MDP&#20013;&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17091v1 Announce Type: cross  Abstract: We revisit the problem of offline reinforcement learning with value function realizability but without Bellman completeness. Previous work by Xie and Jiang (2021) and Foster et al. (2022) left open the question whether a bounded concentrability coefficient along with trajectory-based offline data admits a polynomial sample complexity. In this work, we provide a negative answer to this question for the task of offline policy evaluation. In addition to addressing this question, we provide a rather complete picture for offline policy evaluation with only value function realizability. Our primary findings are threefold: 1) The sample complexity of offline policy evaluation is governed by the concentrability coefficient in an aggregated Markov Transition Model jointly determined by the function class and the offline data distribution, rather than that in the original MDP. This unifies and generalizes the ideas of Xie and Jiang (2021) and Fo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#22686;&#24378;LLMs&#30340;&#33021;&#21147;&#65292;&#20197;&#21327;&#21161;&#29992;&#25143;&#22788;&#29702;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.17089</link><description>&lt;p&gt;
GOLF&#65306;&#30446;&#26631;&#23548;&#21521;&#30340;&#38271;&#26399;&#29983;&#27963;&#20219;&#21153;&#65292;&#30001;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#22686;&#24378;LLMs&#30340;&#33021;&#21147;&#65292;&#20197;&#21327;&#21161;&#29992;&#25143;&#22788;&#29702;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#20449;&#24687;&#33719;&#21462;&#36807;&#31243;&#12290;&#21033;&#29992;LLMs&#20316;&#20026;&#25628;&#32034;&#24341;&#25806;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#29992;&#25143;&#29616;&#22312;&#21487;&#20197;&#35775;&#38382;&#26681;&#25454;&#20854;&#26597;&#35810;&#23450;&#21046;&#30340;&#25688;&#35201;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22312;&#23548;&#33322;&#22823;&#37327;&#20449;&#24687;&#36164;&#28304;&#26102;&#25152;&#24102;&#26469;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;&#36825;&#31181;&#36716;&#21464;&#20984;&#26174;&#20102;LLMs&#22312;&#37325;&#26032;&#23450;&#20041;&#20449;&#24687;&#33719;&#21462;&#33539;&#24335;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#20219;&#21153;&#28966;&#28857;&#20449;&#24687;&#26816;&#32034;&#21644;LLMs&#30340;&#20219;&#21153;&#35268;&#21010;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#23558;LLMs&#30340;&#33021;&#21147;&#33539;&#22260;&#25193;&#23637;&#21040;&#25903;&#25345;&#29992;&#25143;&#23548;&#33322;&#38271;&#26399;&#21644;&#37325;&#35201;&#30340;&#29983;&#27963;&#20219;&#21153;&#12290;&#23427;&#24341;&#20837;&#20102;GOLF&#26694;&#26550;&#65288;&#30446;&#26631;&#23548;&#21521;&#30340;&#38271;&#26399;&#29983;&#27963;&#20219;&#21153;&#65289;&#65292;&#20391;&#37325;&#20110;&#22686;&#24378;LLMs&#36890;&#36807;&#30446;&#26631;&#23450;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#26469;&#21327;&#21161;&#29992;&#25143;&#20570;&#20986;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#35770;&#21253;&#21547;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#31867;&#27604;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17089v1 Announce Type: cross  Abstract: The advent of ChatGPT and similar large language models (LLMs) has revolutionized the human-AI interaction and information-seeking process. Leveraging LLMs as an alternative to search engines, users can now access summarized information tailored to their queries, significantly reducing the cognitive load associated with navigating vast information resources. This shift underscores the potential of LLMs in redefining information access paradigms. Drawing on the foundation of task-focused information retrieval and LLMs' task planning ability, this research extends the scope of LLM capabilities beyond routine task automation to support users in navigating long-term and significant life tasks. It introduces the GOLF framework (Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability to assist in significant life decisions through goal orientation and long-term planning. The methodology encompasses a comprehensive simul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#25968;&#25454;&#38598;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#25439;&#22833;&#20540;&#30340;&#36873;&#25321;&#65292;&#23558;&#35757;&#32451;&#38598;&#32553;&#20943;&#33267;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17083</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20462;&#21098;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study in Dataset Pruning for Image Super-Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#25968;&#25454;&#38598;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#25439;&#22833;&#20540;&#30340;&#36873;&#25321;&#65292;&#23558;&#35757;&#32451;&#38598;&#32553;&#20943;&#33267;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#20013;&#65292;&#20381;&#36182;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#23613;&#31649;&#25552;&#20379;&#20016;&#23500;&#30340;&#35757;&#32451;&#32032;&#26448;&#65292;&#20294;&#20063;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#38598;&#20462;&#21098;&#20316;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#38598;&#32553;&#20943;&#21040;&#22522;&#20110;&#20854;&#25439;&#22833;&#20540;&#32780;&#36873;&#25321;&#30340;&#19968;&#32452;&#26680;&#24515;&#35757;&#32451;&#26679;&#26412;&#12290;&#36890;&#36807;&#20165;&#23558;&#35757;&#32451;&#37325;&#28857;&#25918;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#19978;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#25439;&#22833;&#20540;&#26368;&#39640;&#30340;&#26679;&#26412;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#25110;&#29978;&#33267;&#36229;&#36807;&#25972;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#20855;&#26377;&#26368;&#39640;&#25439;&#22833;&#20540;&#30340;&#21069;5&#65285;&#26679;&#26412;&#20250;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25490;&#38500;&#36825;&#20123;&#26679;&#26412;&#24182;&#35843;&#25972;&#36873;&#25321;&#20197;&#20559;&#22909;&#26356;&#23481;&#26131;&#30340;&#26679;&#26412;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35757;&#32451;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17083v1 Announce Type: cross  Abstract: In image Super-Resolution (SR), relying on large datasets for training is a double-edged sword. While offering rich training material, they also demand substantial computational and storage resources. In this work, we analyze dataset pruning as a solution to these challenges. We introduce a novel approach that reduces a dataset to a core-set of training samples, selected based on their loss values as determined by a simple pre-trained SR model. By focusing the training on just 50% of the original dataset, specifically on the samples characterized by the highest loss values, we achieve results comparable to or even surpassing those obtained from training on the entire dataset. Interestingly, our analysis reveals that the top 5% of samples with the highest loss values negatively affect the training process. Excluding these samples and adjusting the selection to favor easier samples further enhances training outcomes. Our work opens new p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35782;&#21035;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23545;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.17064</link><description>&lt;p&gt;
&#22312;T2I&#27169;&#22411;&#20013;&#36890;&#36807;&#35782;&#21035;&#35821;&#20041;&#26041;&#21521;&#23454;&#29616;&#36830;&#32493;&#12289;&#20027;&#39064;&#29305;&#23450;&#30340;&#23646;&#24615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17064
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23545;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#38480;&#21046;&#65288;&#20363;&#22914;&#8220;&#20154;&#8221;&#21644;&#8220;&#32769;&#24180;&#20154;&#8221;&#20043;&#38388;&#19981;&#23384;&#22312;&#36830;&#32493;&#30340;&#20013;&#38388;&#25551;&#36848;&#30340;&#38598;&#21512;&#65289;&#65292;&#23454;&#29616;&#23545;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#25110;&#29983;&#25104;&#36807;&#31243;&#20197;&#23454;&#29616;&#36825;&#31181;&#25511;&#21046;&#65292;&#20294;&#19981;&#38656;&#35201;&#22266;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#21551;&#29992;&#20840;&#23616;&#32454;&#31890;&#24230;&#23646;&#24615;&#34920;&#36798;&#25511;&#21046;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#31895;&#31890;&#24230;&#23646;&#24615;&#34920;&#36798;&#25511;&#21046;&#65292;&#32780;&#19981;&#33021;&#21516;&#26102;&#20860;&#39038;&#20004;&#32773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#29992;&#30340;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#23384;&#22312;&#21487;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#30340;&#26041;&#21521;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17064v1 Announce Type: cross  Abstract: In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person''). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient op
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17040</link><description>&lt;p&gt;
&#29992;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#23545;&#31038;&#20132;&#32593;&#32476;&#12289;&#21270;&#21512;&#29289;&#21644;&#29983;&#29289;&#31995;&#32479;&#31561;&#22797;&#26434;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#20316;&#20026;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22270;&#23398;&#20064;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#32780;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#32534;&#30721;&#21644;&#22788;&#29702;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#19982;SNNs&#32467;&#21512;&#20197;&#25913;&#21892;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;SNN&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#22270;&#20013;&#37325;&#35201;&#30340;&#33410;&#28857;&#21644;&#30456;&#24212;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19982;&#29616;&#26377;&#22270;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17040v1 Announce Type: new  Abstract: Graph representation learning has become a crucial task in machine learning and data mining due to its potential for modeling complex structures such as social networks, chemical compounds, and biological systems. Spiking neural networks (SNNs) have recently emerged as a promising alternative to traditional neural networks for graph learning tasks, benefiting from their ability to efficiently encode and process temporal and spatial information. In this paper, we propose a novel approach that integrates attention mechanisms with SNNs to improve graph representation learning. Specifically, we introduce an attention mechanism for SNN that can selectively focus on important nodes and corresponding features in a graph during the learning process. We evaluate our proposed method on several benchmark datasets and show that it achieves comparable performance compared to existing graph learning techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#39046;&#22495;&#30340;&#36827;&#21270;&#21382;&#31243;&#65292;&#20171;&#32461;&#20102;&#20174;&#25163;&#21160;&#35774;&#35745;&#21040;&#33258;&#21160;&#21270;&#20248;&#21270;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#25506;&#35752;&#20102;NAS&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#38024;&#23545;&#35745;&#31639;&#25928;&#29575;&#25361;&#25112;&#25552;&#20986;&#30340;&#39640;&#25928;NAS&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17012</link><description>&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#36827;&#21270;&#19982;&#25928;&#29575;&#65306;&#24357;&#21512;&#19987;&#23478;&#35774;&#35745;&#19982;&#33258;&#21160;&#20248;&#21270;&#20043;&#38388;&#30340;&#40511;&#27807;
&lt;/p&gt;
&lt;p&gt;
Evolution and Efficiency in Neural Architecture Search: Bridging the Gap Between Expert Design and Automated Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#39046;&#22495;&#30340;&#36827;&#21270;&#21382;&#31243;&#65292;&#20171;&#32461;&#20102;&#20174;&#25163;&#21160;&#35774;&#35745;&#21040;&#33258;&#21160;&#21270;&#20248;&#21270;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#25506;&#35752;&#20102;NAS&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#38024;&#23545;&#35745;&#31639;&#25928;&#29575;&#25361;&#25112;&#25552;&#20986;&#30340;&#39640;&#25928;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#65292;&#24378;&#35843;&#20102;&#23427;&#20174;&#25163;&#21160;&#35774;&#35745;&#21040;&#33258;&#21160;&#21270;&#12289;&#35745;&#31639;&#39537;&#21160;&#26041;&#27861;&#30340;&#28436;&#21464;&#12290;&#23427;&#28085;&#30422;&#20102;NAS&#30340;&#36215;&#28304;&#21644;&#21457;&#23637;&#65292;&#31361;&#20986;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21307;&#23398;&#24433;&#20687;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#25991;&#31456;&#35814;&#32454;&#38416;&#36848;&#20102;&#20174;&#19987;&#23478;&#39537;&#21160;&#35774;&#35745;&#21040;&#31639;&#27861;&#39537;&#21160;&#36807;&#31243;&#30340;&#36716;&#21464;&#65292;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36827;&#21270;&#31639;&#27861;&#31561;&#21021;&#22987;&#26041;&#27861;&#12290;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;&#38656;&#27714;&#30340;&#25361;&#25112;&#20197;&#21450;&#39640;&#25928;NAS&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#22914;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#21644;&#30828;&#20214;&#24863;&#30693;NAS&#12290;&#35813;&#35770;&#25991;&#36827;&#19968;&#27493;&#38416;&#36848;&#20102;NAS&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;NLP&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#12290;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#25361;&#25112;&#65292;&#21253;&#25324;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17012v1 Announce Type: cross  Abstract: The paper provides a comprehensive overview of Neural Architecture Search (NAS), emphasizing its evolution from manual design to automated, computationally-driven approaches. It covers the inception and growth of NAS, highlighting its application across various domains, including medical imaging and natural language processing. The document details the shift from expert-driven design to algorithm-driven processes, exploring initial methodologies like reinforcement learning and evolutionary algorithms. It also discusses the challenges of computational demands and the emergence of efficient NAS methodologies, such as Differentiable Architecture Search and hardware-aware NAS. The paper further elaborates on NAS's application in computer vision, NLP, and beyond, demonstrating its versatility and potential for optimizing neural network architectures across different tasks. Future directions and challenges, including computational efficiency
&lt;/p&gt;</description></item><item><title>SUDO&#26694;&#26550;&#20801;&#35768;&#22312;&#32570;&#20047;&#30495;&#23454;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#20026;&#23454;&#38469;&#25968;&#25454;&#28857;&#20998;&#37197;&#20020;&#26102;&#26631;&#31614;&#24182;&#30452;&#25509;&#20351;&#29992;&#23427;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.17011</link><description>&lt;p&gt;
SUDO&#65306;&#19968;&#31181;&#26080;&#38656;&#30495;&#23454;&#26631;&#27880;&#35780;&#20272;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SUDO: a framework for evaluating clinical artificial intelligence systems without ground-truth annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17011
&lt;/p&gt;
&lt;p&gt;
SUDO&#26694;&#26550;&#20801;&#35768;&#22312;&#32570;&#20047;&#30495;&#23454;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#20026;&#23454;&#38469;&#25968;&#25454;&#28857;&#20998;&#37197;&#20020;&#26102;&#26631;&#31614;&#24182;&#30452;&#25509;&#20351;&#29992;&#23427;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#36890;&#24120;&#22312;&#19968;&#20010;&#26410;&#26333;&#20809;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65288;&#20363;&#22914;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31995;&#32479;&#30340;&#19981;&#21516;&#21307;&#38498;&#30340;&#25968;&#25454;&#65289;&#12290;&#36825;&#31181;&#35780;&#20272;&#36807;&#31243;&#26088;&#22312;&#27169;&#25311;&#23558;AI&#31995;&#32479;&#37096;&#32626;&#22312;&#26410;&#34987;&#31995;&#32479;&#35265;&#36807;&#20294;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#39044;&#35745;&#20250;&#36935;&#21040;&#30340;&#25968;&#25454;&#19978;&#65307;&#28982;&#32780;&#65292;&#24403;&#23454;&#38469;&#25968;&#25454;&#19982;&#26410;&#26333;&#20809;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#26102;&#65292;&#21363;&#20998;&#24067;&#36716;&#31227;&#29616;&#35937;&#65292;&#24182;&#19988;&#32570;&#20047;&#30495;&#23454;&#26631;&#27880;&#26102;&#65292;&#19981;&#28165;&#26970;&#22522;&#20110;AI&#30340;&#21457;&#29616;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#33021;&#21542;&#21463;&#20449;&#20219;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;SUDO&#65292;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26080;&#38656;&#30495;&#23454;&#26631;&#27880;&#30340;AI&#31995;&#32479;&#30340;&#26694;&#26550;&#12290;SUDO&#20026;&#23454;&#38469;&#25968;&#25454;&#28857;&#20998;&#37197;&#20020;&#26102;&#26631;&#31614;&#65292;&#24182;&#30452;&#25509;&#20351;&#29992;&#36825;&#20123;&#26631;&#31614;&#35757;&#32451;&#19981;&#21516;&#27169;&#22411;&#65292;&#34920;&#29616;&#26368;&#20248;&#30340;&#27169;&#22411;&#34920;&#26126;&#26368;&#21487;&#33021;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17011v1 Announce Type: cross  Abstract: A clinical artificial intelligence (AI) system is often validated on a held-out set of data which it has not been exposed to before (e.g., data from a different hospital with a distinct electronic health record system). This evaluation process is meant to mimic the deployment of an AI system on data in the wild; those which are currently unseen by the system yet are expected to be encountered in a clinical setting. However, when data in the wild differ from the held-out set of data, a phenomenon referred to as distribution shift, and lack ground-truth annotations, it becomes unclear the extent to which AI-based findings can be trusted on data in the wild. Here, we introduce SUDO, a framework for evaluating AI systems without ground-truth annotations. SUDO assigns temporary labels to data points in the wild and directly uses them to train distinct models, with the highest performing model indicative of the most likely label. Through exp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#20419;&#36827;&#20195;&#29702;&#38388;&#19978;&#19979;&#25991;&#20999;&#25442;&#12289;&#23454;&#29616;&#24182;&#21457;&#25191;&#34892;&#20197;&#21450;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.16971</link><description>&lt;p&gt;
LLM Agent Operating System
&lt;/p&gt;
&lt;p&gt;
LLM Agent Operating System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#20419;&#36827;&#20195;&#29702;&#38388;&#19978;&#19979;&#25991;&#20999;&#25442;&#12289;&#23454;&#29616;&#24182;&#21457;&#25191;&#34892;&#20197;&#21450;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16971v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20195;&#29702;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#25928;&#29575;&#21644;&#21151;&#25928;&#12290;&#20854;&#20013;&#21253;&#25324;&#20195;&#29702;&#35831;&#27714;&#22312;LLM&#19978;&#30340;&#27425;&#20248;&#35843;&#24230;&#21644;&#36164;&#28304;&#20998;&#37197;&#12289;&#22312;&#20195;&#29702;&#21644;LLM&#20043;&#38388;&#20132;&#20114;&#26102;&#20445;&#25345;&#19978;&#19979;&#25991;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#23558;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#21644;&#19987;&#19994;&#21270;&#30340;&#24322;&#26500;&#20195;&#29702;&#38598;&#25104;&#22312;&#19968;&#36215;&#30340;&#22797;&#26434;&#24615;&#12290;&#20195;&#29702;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#30340;&#24555;&#36895;&#22686;&#21152;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#36164;&#28304;&#29942;&#39048;&#21644;&#27425;&#20248;&#36164;&#28304;&#21033;&#29992;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;AIOS&#65292;&#19968;&#31181;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#65288;OS&#65289;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;AIOS&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#65292;&#20419;&#36827;&#20195;&#29702;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20999;&#25442;&#65292;&#23454;&#29616;&#20195;&#29702;&#30340;&#24182;&#21457;&#25191;&#34892;&#65292;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16971v1 Announce Type: cross  Abstract: The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS). Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16950</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#25104;&#23545;&#20559;&#22909;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16950
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#22312;&#35780;&#20272;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35780;&#20272;&#20013;&#20173;&#23384;&#22312;&#20559;&#35265;&#65292;&#24120;&#24120;&#38590;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#35780;&#20272;&#19968;&#33268;&#30340;&#36830;&#36143;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;LLM&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#29616;&#26377;&#26088;&#22312;&#20943;&#36731;&#20559;&#35265;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#26377;&#25928;&#23558;LLM&#35780;&#20272;&#22120;&#23545;&#40784;&#12290;&#21463;&#21040;RLHF&#20013;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#20351;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;Pairwise-preference Search&#65288;PAIRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;LLMs&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#24182;&#26377;&#25928;&#23545;&#20505;&#36873;&#25991;&#26412;&#36827;&#34892;&#25490;&#24207;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;PAIRS&#22312;&#20195;&#34920;&#24615;&#35780;&#20272;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#27604;&#30452;&#25509;&#25171;&#20998;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16915</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31895;&#35843;&#20248;&#30340;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM-based IR&#65289;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#31895;&#35843;&#20248;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#19979;&#28216;IR&#20219;&#21153;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#31895;&#35843;&#20248;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#39044;&#27979;&#65288;QDPP&#65289;&#65292;&#20854;&#39044;&#27979;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#36866;&#24403;&#24615;&#12290;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22235;&#20010;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;MRR&#21644;/&#25110;nDCG@5&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31895;&#35843;&#20248;&#20419;&#36827;&#20102;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#36136;&#37327;&#27169;&#22411;&#21644;&#21512;&#21516;&#27861;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27431;&#30431;AI&#27861;&#26696;&#23545;&#39640;&#39118;&#38505;AI&#31995;&#32479;&#35201;&#27714;&#30340;&#26041;&#27861;&#35770;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.16808</link><description>&lt;p&gt;
&#29702;&#35299;&#27431;&#30431;AI&#27861;&#26696;: &#21512;&#35268;&#23433;&#20840;&#20851;&#38190;&#20135;&#21697;&#30340;&#26041;&#27861;&#35770;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Navigating the EU AI Act: A Methodological Approach to Compliance for Safety-critical Products
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16808
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#36136;&#37327;&#27169;&#22411;&#21644;&#21512;&#21516;&#27861;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27431;&#30431;AI&#27861;&#26696;&#23545;&#39640;&#39118;&#38505;AI&#31995;&#32479;&#35201;&#27714;&#30340;&#26041;&#27861;&#35770;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;12&#26376;&#65292;&#27431;&#27954;&#35758;&#20250;&#26242;&#26102;&#21516;&#24847;&#20102;&#27431;&#30431;AI&#27861;&#26696;&#12290;&#36825;&#19968;&#21069;&#25152;&#26410;&#26377;&#30340;AI&#31995;&#32479;&#30417;&#31649;&#26694;&#26550;&#21046;&#23450;&#20102;&#30830;&#20445;AI&#20135;&#21697;&#23433;&#20840;&#12289;&#21512;&#27861;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#36136;&#37327;&#27169;&#22411;&#26469;&#35299;&#37322;&#39640;&#39118;&#38505;AI&#31995;&#32479;&#22312;&#27431;&#30431;AI&#27861;&#26696;&#19979;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;AI&#31995;&#32479;&#20135;&#21697;&#36136;&#37327;&#27169;&#22411;&#65292;&#23558;&#27861;&#26696;&#20013;&#26410;&#28085;&#30422;&#30340;&#30456;&#20851;&#23646;&#24615;&#32435;&#20837;&#32771;&#34385;&#12290;&#25105;&#20204;&#23558;&#27861;&#26696;&#35201;&#27714;&#19982;&#30456;&#20851;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#26144;&#23556;&#65292;&#30446;&#30340;&#26159;&#23558;&#20854;&#32454;&#21270;&#20026;&#21487;&#34913;&#37327;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#21516;&#30340;&#26041;&#27861;&#65292;&#20174;&#21033;&#30410;&#30456;&#20851;&#32773;&#23618;&#38754;&#25512;&#23548;&#25216;&#26415;&#35201;&#27714;&#12290;&#36825;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#19981;&#20165;&#31526;&#21512;&#24050;&#24314;&#31435;&#36136;&#37327;&#26631;&#20934;&#65292;&#32780;&#19988;&#31526;&#21512;&#27861;&#26696;&#20013;&#38024;&#23545;&#39640;&#39118;&#38505;&#65288;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16808v1 Announce Type: new  Abstract: In December 2023, the European Parliament provisionally agreed on the EU AI Act. This unprecedented regulatory framework for AI systems lays out guidelines to ensure the safety, legality, and trustworthiness of AI products. This paper presents a methodology for interpreting the EU AI Act requirements for high-risk AI systems by leveraging product quality models. We first propose an extended product quality model for AI systems, incorporating attributes relevant to the Act not covered by current quality models. We map the Act requirements to relevant quality attributes with the goal of refining them into measurable characteristics. We then propose a contract-based approach to derive technical requirements at the stakeholder level. This facilitates the development and assessment of AI systems that not only adhere to established quality standards, but also comply with the regulatory requirements outlined in the Act for high-risk (including 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#39033;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23545;&#21512;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20197;&#25506;&#35752;&#20154;&#31867;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#30340;&#26131;&#21463;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.16760</link><description>&lt;p&gt;
&#21644;&#25243;&#30828;&#24065;&#19968;&#26679;&#22909;&#65306;&#20154;&#31867;&#23545;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
As Good As A Coin Toss Human detection of AI-generated images, videos, audio, and audiovisual stimuli
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16760
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#39033;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23545;&#21512;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20197;&#25506;&#35752;&#20154;&#31867;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#30340;&#26131;&#21463;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21512;&#25104;&#23186;&#20307;&#21464;&#24471;&#36234;&#26469;&#36234;&#36924;&#30495;&#65292;&#20351;&#29992;&#23427;&#30340;&#38556;&#30861;&#19981;&#26029;&#38477;&#20302;&#65292;&#36825;&#39033;&#25216;&#26415;&#36234;&#26469;&#36234;&#34987;&#24694;&#24847;&#21033;&#29992;&#65292;&#20174;&#37329;&#34701;&#27450;&#35784;&#21040;&#38750;&#33258;&#24895;&#33394;&#24773;&#12290;&#20170;&#22825;&#65292;&#23545;&#25239;&#34987;&#21512;&#25104;&#23186;&#20307;&#35823;&#23548;&#30340;&#20027;&#35201;&#38450;&#24481;&#20381;&#36182;&#20110;&#20154;&#31867;&#35266;&#23519;&#32773;&#22312;&#35270;&#35273;&#21644;&#21548;&#35273;&#19978;&#21306;&#20998;&#30495;&#20551;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#38469;&#19978;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#26377;&#22810;&#33030;&#24369;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21253;&#21547;1276&#21517;&#21442;&#19982;&#32773;&#30340;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20154;&#20204;&#22312;&#21306;&#20998;&#21512;&#25104;&#22270;&#20687;&#12289;&#20165;&#38899;&#39057;&#12289;&#20165;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#20934;&#30830;&#24615;&#22914;&#20309;&#12290;&#20026;&#20102;&#21453;&#26144;&#20154;&#20204;&#22312;&#37326;&#22806;&#21487;&#33021;&#36935;&#21040;&#21512;&#25104;&#23186;&#20307;&#30340;&#24773;&#20917;&#65292;&#27979;&#35797;&#26465;&#20214;&#21644;&#21050;&#28608;&#27169;&#25311;&#20102;&#20856;&#22411;&#30340;&#22312;&#32447;&#24179;&#21488;&#65292;&#32780;&#35843;&#26597;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#21512;&#25104;&#23186;&#20307;&#22343;&#26469;&#33258;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16760v1 Announce Type: cross  Abstract: As synthetic media becomes progressively more realistic and barriers to using it continue to lower, the technology has been increasingly utilized for malicious purposes, from financial fraud to nonconsensual pornography. Today, the principal defense against being misled by synthetic media relies on the ability of the human observer to visually and auditorily discern between real and fake. However, it remains unclear just how vulnerable people actually are to deceptive synthetic media in the course of their day to day lives. We conducted a perceptual study with 1276 participants to assess how accurate people were at distinguishing synthetic images, audio only, video only, and audiovisual stimuli from authentic. To reflect the circumstances under which people would likely encounter synthetic media in the wild, testing conditions and stimuli emulated a typical online platform, while all synthetic media used in the survey was sourced from 
&lt;/p&gt;</description></item><item><title>CLHA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#37325;&#35780;&#20998;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#35843;&#25972;&#65292;&#22312;&#25552;&#21319;&#23545;&#40784;&#25928;&#26524;&#30340;&#21516;&#26102;&#31616;&#21270;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.16649</link><description>&lt;p&gt;
CLHA: &#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29992;&#20110;&#20154;&#31867;&#23545;&#40784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16649
&lt;/p&gt;
&lt;p&gt;
CLHA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#37325;&#35780;&#20998;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#35843;&#25972;&#65292;&#22312;&#25552;&#21319;&#23545;&#40784;&#25928;&#26524;&#30340;&#21516;&#26102;&#31616;&#21270;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#30830;&#20445;&#36825;&#20123;LLMs&#20197;&#23545;&#29992;&#25143;&#26377;&#30410;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;RL&#30340;&#20154;&#31867;&#23545;&#40784;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#38271;&#26399;&#25361;&#25112;&#22312;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29992;&#20110;&#20154;&#31867;&#23545;&#40784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;CLHA&#65289;&#65292;&#30452;&#25509;&#23558;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;CLHA&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#35780;&#20998;&#31574;&#30053;&#26469;&#35780;&#20272;&#25968;&#25454;&#20869;&#30340;&#22122;&#22768;&#65292;&#32771;&#34385;&#20854;&#22266;&#26377;&#36136;&#37327;&#24182;&#21160;&#24577;&#35843;&#25972;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;CLHA&#21033;&#29992;&#25104;&#23545;&#23545;&#27604;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30417;&#30563;&#24494;&#35843;&#25439;&#22833;&#26469;&#33258;&#36866;&#24212;&#22320;&#20462;&#25913;&#29983;&#25104;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#65292;&#30830;&#20445;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#22686;&#24378;&#23545;&#40784;&#12290;&#20351;&#29992;&#20808;&#36827;&#26041;&#27861;&#65292;CLHA&#36229;&#36234;&#20102;&#20854;&#20182;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16649v1 Announce Type: new  Abstract: Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses oth
&lt;/p&gt;</description></item><item><title>DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;</title><link>https://arxiv.org/abs/2403.16451</link><description>&lt;p&gt;
DeepMachining: &#38115;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#22312;&#32447;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeepMachining: Online Prediction of Machining Errors of Lathe Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16451
&lt;/p&gt;
&lt;p&gt;
DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;DeepMachining&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#21152;&#24037;&#35823;&#24046;&#12290;&#25105;&#20204;&#22522;&#20110;&#24037;&#21378;&#30340;&#21046;&#36896;&#25968;&#25454;&#26500;&#24314;&#24182;&#35780;&#20272;&#20102;DeepMachining&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29305;&#23450;&#36710;&#24202;&#26426;&#24202;&#25805;&#20316;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21152;&#24037;&#29366;&#24577;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#29305;&#23450;&#21152;&#24037;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DeepMachining&#22312;&#28041;&#21450;&#19981;&#21516;&#24037;&#20214;&#21644;&#20992;&#20855;&#30340;&#22810;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#39318;&#25209;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
&lt;/p&gt;</description></item><item><title>Re2LLM&#26159;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25552;&#20986;&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.16427</link><description>&lt;p&gt;
Re2LLM: &#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16427
&lt;/p&gt;
&lt;p&gt;
Re2LLM&#26159;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25552;&#20986;&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#26085;&#30410;&#34987;&#30475;&#20316;&#26159;&#22686;&#24378;&#22522;&#20110;&#20250;&#35805;&#25512;&#33616;(SBR)&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24050;&#24191;&#27867;&#30740;&#31350;&#20102;&#22522;&#20110;&#25552;&#31034;&#21644;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;LLMs&#19982;SBR&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#22240;&#32570;&#20047;&#20219;&#21153;&#29305;&#23450;&#21453;&#39304;&#32780;&#38590;&#20197;&#25214;&#21040;&#24341;&#23548;LLMs&#27491;&#30830;&#25512;&#29702;&#30340;&#26368;&#20339;&#25552;&#31034;&#65292;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#19981;&#20339;&#12290;&#23613;&#31649;&#21518;&#32773;&#35797;&#22270;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#24494;&#35843;LLMs&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#35832;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20381;&#36182;&#24320;&#28304;&#39592;&#24178;&#30340;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;SBR&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Re2LLM)&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#21453;&#23556;&#24335;&#25506;&#32034;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16427v1 Announce Type: new  Abstract: Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR.   However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations.   Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones.   To address such issues, we propose a \underline{Re}flective \underline{Re}inforcement \underline{L}arge \underline{L}anguage \underline{M}odel (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently.   In particular, we first design the Reflective Exploration Module to effective
&lt;/p&gt;</description></item><item><title>LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16303</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Biomedical and Health Informatics: A Bibliometric Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16303
&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#65288;BHI&#65289;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20026;&#20998;&#26512;&#25968;&#25454;&#12289;&#27835;&#30103;&#24739;&#32773;&#21644;&#24320;&#23637;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#33258;2022&#24180;&#33267;2023&#24180;&#30340;&#30740;&#31350;&#25991;&#31456;&#21644;&#21512;&#20316;&#32593;&#32476;&#65292;&#20840;&#38754;&#23637;&#31034;LLMs&#22312;BHI&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22914;&#20309;&#21487;&#20197;&#25913;&#36827;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#12289;&#24739;&#32773;&#21442;&#19982;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#21307;&#23398;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#30830;&#23450;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#32472;&#21046;&#20102;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#20027;&#35201;&#36827;&#23637;&#12290;&#26368;&#21518;&#65292;&#23427;&#35752;&#35770;&#20102;&#22312;BHI&#20013;&#20351;&#29992;LLMs&#30340;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#38544;&#31169;&#21644;&#21487;&#38752;&#30340;&#21307;&#30103;&#24314;&#35758;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;LLMs&#22914;&#20309;&#36827;&#19968;&#27493;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16303v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023. It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine. To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field. Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations. Looking ahead, we consider how LLMs could further transform biomedical research as we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#26469;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#21361;&#38505;&#24773;&#20917;&#24182;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#31639;&#27861;&#65292;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39640;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.16291</link><description>&lt;p&gt;
&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#20197;&#36991;&#20813;&#21361;&#38505;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Guessing human intentions to avoid dangerous situations in caregiving robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#26469;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#21361;&#38505;&#24773;&#20917;&#24182;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#31639;&#27861;&#65292;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39640;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#27714;&#26426;&#22120;&#20154;&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#65292;&#23427;&#20204;&#24517;&#39035;&#20934;&#30830;&#35299;&#37322;&#20154;&#31867;&#24847;&#22270;&#24182;&#39044;&#27979;&#28508;&#22312;&#32467;&#26524;&#12290;&#23545;&#20110;&#20026;&#20154;&#31867;&#25252;&#29702;&#35774;&#35745;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#32780;&#35328;&#23588;&#20026;&#37325;&#35201;&#65292;&#21487;&#33021;&#20250;&#38754;&#20020;&#20154;&#31867;&#30340;&#21361;&#38505;&#24773;&#20917;&#65292;&#27604;&#22914;&#26410;&#35265;&#38556;&#30861;&#29289;&#65292;&#24212;&#35813;&#20104;&#20197;&#36991;&#20813;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#65288;ATM&#65289;&#26041;&#27861;&#26469;&#25512;&#26029;&#21644;&#35299;&#37322;&#20154;&#31867;&#24847;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#20154;&#31867;&#39118;&#38505;&#24773;&#20917;&#30340;&#31639;&#27861;&#65292;&#36873;&#25321;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;ATM&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#8220;&#20687;&#25105;&#19968;&#26679;&#8221;&#30340;&#31574;&#30053;&#23558;&#24847;&#22270;&#21644;&#21160;&#20316;&#20998;&#37197;&#32473;&#20154;&#31867;&#12290;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#26426;&#22120;&#20154;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#21487;&#20197;&#39640;&#25104;&#21151;&#29575;&#22320;&#26816;&#27979;&#21644;&#34892;&#21160;&#12290;&#35813;&#31639;&#27861;&#24050;&#32463;&#20316;&#20026;&#29616;&#26377;&#26426;&#22120;&#20154;&#35748;&#30693;&#26550;&#26500;&#30340;&#19968;&#37096;&#20998;&#23454;&#26045;&#65292;&#24182;&#22312;&#27169;&#25311;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16291v1 Announce Type: cross  Abstract: For robots to interact socially, they must interpret human intentions and anticipate their potential outcomes accurately. This is particularly important for social robots designed for human care, which may face potentially dangerous situations for people, such as unseen obstacles in their way, that should be avoided. This paper explores the Artificial Theory of Mind (ATM) approach to inferring and interpreting human intentions. We propose an algorithm that detects risky situations for humans, selecting a robot action that removes the danger in real time. We use the simulation-based approach to ATM and adopt the 'like-me' policy to assign intentions and actions to people. Using this strategy, the robot can detect and act with a high rate of success under time-constrained situations. The algorithm has been implemented as part of an existing robotics cognitive architecture and tested in simulation scenarios. Three experiments have been co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#26412;&#20307;&#26469;&#26500;&#24314;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.16222</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#26412;&#20307;&#26469;&#26500;&#24314;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#20154;&#31867;&#30693;&#35782;&#37117;&#34987;&#23553;&#35013;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#31185;&#23398;&#35770;&#25991;&#20013;&#12290;&#38543;&#30528;&#36825;&#20123;&#25991;&#26412;&#25968;&#25454;&#30340;&#19981;&#26029;&#25193;&#22823;&#65292;&#25991;&#26723;&#32452;&#32455;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#26085;&#30410;&#20851;&#38190;&#65292;&#29992;&#20110;&#20174;&#22823;&#22411;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#38544;&#34255;&#30340;&#21487;&#34892;&#35265;&#35299;&#12290;&#30693;&#35782;&#22270;&#65288;KGs&#65289;&#20316;&#20026;&#19968;&#31181;&#20197;&#32467;&#26500;&#21270;&#26041;&#24335;&#23384;&#20648;&#23454;&#38469;&#20449;&#24687;&#30340;&#25163;&#27573;&#65292;&#25552;&#20379;&#21253;&#25324;&#26469;&#33258;&#32593;&#32476;&#23433;&#20840;&#31185;&#23398;&#25991;&#29486;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#22312;&#20869;&#30340;&#26126;&#30830;&#12289;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#12290;&#26500;&#24314;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#30693;&#35782;&#22270;&#26159;&#25552;&#21462;&#26412;&#20307;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#20027;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#26412;&#20307;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;KG&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#23637;&#31034;&#20102;&#36825;&#19968;&#27010;&#24565;&#12290;KG&#30340;&#19968;&#31181;&#27169;&#24577;&#20195;&#34920;&#20102;&#35770;&#25991;&#20013;&#30340;&#21487;&#35266;&#23519;&#20449;&#24687;&#65292;&#22914;&#30446;&#24405;&#31561;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16222v1 Announce Type: new  Abstract: Much of human knowledge in cybersecurity is encapsulated within the ever-growing volume of scientific papers. As this textual data continues to expand, the importance of document organization methods becomes increasingly crucial for extracting actionable insights hidden within large text datasets. Knowledge Graphs (KGs) serve as a means to store factual information in a structured manner, providing explicit, interpretable knowledge that includes domain-specific information from the cybersecurity scientific literature. One of the challenges in constructing a KG from scientific literature is the extraction of ontology from unstructured text. In this paper, we address this topic and introduce a method for building a multi-modal KG by extracting structured ontology from scientific papers. We demonstrate this concept in the cybersecurity domain. One modality of the KG represents observable information from the papers, such as the categories i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#19987;&#38376;&#38024;&#23545;&#21517;&#20154;&#29031;&#29255;&#30340;&#22270;&#20687;&#25551;&#36848;&#65292;&#26088;&#22312;&#22686;&#24378;&#26032;&#38395;&#34892;&#19994;&#23454;&#36341;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#33258;&#21160;&#26032;&#38395;&#20869;&#23481;&#29983;&#25104;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16209</link><description>&lt;p&gt;
&#26032;&#38395;&#25253;&#36947;&#22330;&#26223;&#20013;&#30340;&#22270;&#20687;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Image Captioning in news report scenario
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#19987;&#38376;&#38024;&#23545;&#21517;&#20154;&#29031;&#29255;&#30340;&#22270;&#20687;&#25551;&#36848;&#65292;&#26088;&#22312;&#22686;&#24378;&#26032;&#38395;&#34892;&#19994;&#23454;&#36341;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#33258;&#21160;&#26032;&#38395;&#20869;&#23481;&#29983;&#25104;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16209v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22270;&#20687;&#25551;&#36848;&#26088;&#22312;&#20026;&#25351;&#23450;&#30340;&#22270;&#20687;&#29983;&#25104;&#30456;&#20851;&#30340;&#25551;&#36848;&#65292;&#20351;&#20854;&#22788;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20132;&#21449;&#28857;&#12290;&#36825;&#39033;&#21162;&#21147;&#22312;&#25512;&#33616;&#31995;&#32479;&#12289;&#26032;&#38395;&#23186;&#20307;&#12289;&#31038;&#20132;&#23186;&#20307;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#29305;&#21035;&#26159;&#22312;&#26032;&#38395;&#25253;&#36947;&#39046;&#22495;&#65292;&#26631;&#39064;&#24212;&#28085;&#30422;&#35814;&#32454;&#20449;&#24687;&#65292;&#22914;&#22270;&#20687;&#20013;&#25429;&#25417;&#21040;&#30340;&#21517;&#20154;&#30340;&#36523;&#20221;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#37096;&#20998;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#20110;&#29702;&#35299;&#22330;&#26223;&#21644;&#21160;&#20316;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19987;&#38376;&#38024;&#23545;&#21517;&#20154;&#29031;&#29255;&#30340;&#22270;&#20687;&#25551;&#36848;&#39046;&#22495;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22686;&#24378;&#26032;&#38395;&#34892;&#19994;&#23454;&#36341;&#26041;&#38754;&#30340;&#24191;&#27867;&#28508;&#21147;&#12290;&#36825;&#19968;&#25506;&#32034;&#26088;&#22312;&#22686;&#24378;&#33258;&#21160;&#21270;&#26032;&#38395;&#20869;&#23481;&#29983;&#25104;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#21152;&#32454;&#33268;&#22320;&#20256;&#25773;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#21162;&#21147;&#23637;&#31034;&#20102;&#19968;&#20010;&#26356;&#24191;&#38420;&#30340;&#35270;&#37326;&#65292;&#20016;&#23500;&#20102;n
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16209v1 Announce Type: cross  Abstract: Image captioning strives to generate pertinent captions for specified images, situating itself at the crossroads of Computer Vision (CV) and Natural Language Processing (NLP). This endeavor is of paramount importance with far-reaching applications in recommendation systems, news outlets, social media, and beyond. Particularly within the realm of news reporting, captions are expected to encompass detailed information, such as the identities of celebrities captured in the images. However, much of the existing body of work primarily centers around understanding scenes and actions. In this paper, we explore the realm of image captioning specifically tailored for celebrity photographs, illustrating its broad potential for enhancing news industry practices. This exploration aims to augment automated news content generation, thereby facilitating a more nuanced dissemination of information. Our endeavor shows a broader horizon, enriching the n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#65292;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;</title><link>https://arxiv.org/abs/2403.16206</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rumor Detection with a novel graph neural network approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#65292;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#35875;&#35328;&#30340;&#24191;&#27867;&#20256;&#25773;&#23545;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#36896;&#25104;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#20844;&#20247;&#20135;&#29983;&#28508;&#22312;&#30340;&#24656;&#24908;&#12289;&#24656;&#24807;&#21644;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#12290;&#22914;&#20309;&#23613;&#26089;&#25581;&#31359;&#35875;&#35328;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#21033;&#29992;&#20449;&#24687;&#20256;&#25773;&#32467;&#26500;&#26469;&#26816;&#27979;&#35875;&#35328;&#65292;&#32780;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#29992;&#25143;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21363;&#20182;&#20204;&#21487;&#33021;&#21327;&#35843;&#20256;&#25773;&#35875;&#35328;&#20197;&#33719;&#24471;&#36739;&#22823;&#30340;&#27969;&#34892;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#65292;&#20197;&#20415;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20174;&#25551;&#36848;&#29992;&#25143;&#21644;&#26469;&#28304;&#25512;&#25991;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#20108;&#37096;&#22270;&#20013;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#30340;&#34920;&#31034;&#65292;&#20197;&#21450;&#20351;&#29992;&#26641;&#32467;&#26500;&#23398;&#20064;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#24471;&#21040;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16206v1 Announce Type: new  Abstract: The wide spread of rumors on social media has caused a negative impact on people's daily life, leading to potential panic, fear, and mental health problems for the public. How to debunk rumors as early as possible remains a challenging problem. Existing studies mainly leverage information propagation structure to detect rumors, while very few works focus on correlation among users that they may coordinate to spread rumors in order to gain large popularity. In this paper, we propose a new detection model, that jointly learns both the representations of user correlation and information propagation to detect rumors on social media. Specifically, we leverage graph neural networks to learn the representations of user correlation from a bipartite graph that describes the correlations between users and source tweets, and the representations of information propagation with a tree structure. Then we combine the learned representations from these 
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;X-Portrait&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.15931</link><description>&lt;p&gt;
X-Portrait: &#20855;&#26377;&#20998;&#23618;&#21160;&#20316;&#27880;&#24847;&#21147;&#30340;&#34920;&#29616;&#24615;&#32918;&#20687;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15931
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;X-Portrait&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;X-Portrait&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#22522;&#20110;&#21333;&#20010;&#32918;&#20687;&#20316;&#20026;&#22806;&#35266;&#21442;&#32771;&#65292;&#24182;&#21033;&#29992;&#26469;&#33258;&#39537;&#21160;&#35270;&#39057;&#30340;&#36816;&#21160;&#26469;&#20026;&#20854;&#28155;&#21152;&#21160;&#30011;&#65292;&#25429;&#25417;&#20855;&#26377;&#39640;&#24230;&#21160;&#24577;&#24615;&#21644;&#24494;&#22937;&#38754;&#37096;&#34920;&#24773;&#20197;&#21450;&#24191;&#27867;&#33539;&#22260;&#22836;&#37096;&#36816;&#21160;&#12290;&#22312;&#20854;&#26680;&#24515;&#37096;&#20998;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#20808;&#39564;&#20316;&#20026;&#28210;&#26579;&#39592;&#26550;&#65292;&#21516;&#26102;&#22312;ControlNet&#26694;&#26550;&#20869;&#36890;&#36807;&#26032;&#39062;&#30340;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#12290;&#19982;&#20256;&#32479;&#30340;&#31895;&#31961;&#26174;&#24335;&#25511;&#21046;&#65288;&#22914;&#38754;&#37096;&#26631;&#24535;&#28857;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#36816;&#21160;&#25511;&#21046;&#27169;&#22359;&#23398;&#20250;&#30452;&#25509;&#20174;&#21407;&#22987;&#39537;&#21160;RGB&#36755;&#20837;&#20013;&#35299;&#35835;&#21160;&#24577;&#12290;&#36890;&#36807;&#26377;&#25928;&#22686;&#24378;&#23545;&#30524;&#31070;&#31561;&#23567;&#23610;&#24230;&#32454;&#24494;&#24046;&#24322;&#30340;&#36816;&#21160;&#20851;&#27880;&#30340;&#22522;&#20110;&#34917;&#19969;&#30340;&#23616;&#37096;&#25511;&#21046;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#36816;&#21160;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15931v1 Announce Type: cross  Abstract: We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeba
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.15729</link><description>&lt;p&gt;
&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion Collider
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15729
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24615;&#21644;&#24222;&#22823;&#30340;&#20449;&#24687;&#37327;&#28085;&#30422;&#20102;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#25991;&#20214;&#12289;&#35770;&#25991;&#12289;&#25968;&#25454;&#21644;&#20854;&#20182;&#36164;&#28304;&#65292;&#23548;&#33268;&#23548;&#33322;&#36825;&#20123;&#22810;&#26679;&#24418;&#24335;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#23545;&#20110;&#26032;&#21512;&#20316;&#32773;&#21644;&#26089;&#26399;&#31185;&#23398;&#23478;&#26469;&#35828;&#23588;&#20026;&#33392;&#24040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;EIC&#25688;&#35201;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65288;RAGS4EIC&#65289;&#12290;&#35813;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19981;&#20165;&#21387;&#32553;&#20449;&#24687;&#65292;&#36824;&#26377;&#25928;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#20102;&#37325;&#22823;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#37319;&#21462;&#20102;&#20004;&#27493;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#26597;&#35810;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#23454;&#39564;&#20449;&#24687;&#30340;&#32508;&#21512;&#21521;&#37327;&#25968;&#25454;&#24211;&#65307;&#20854;&#27425;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#21644;&#26816;&#32034;&#25968;&#25454;&#29983;&#25104;&#21253;&#21547;&#24341;&#29992;&#30340;&#31616;&#27905;&#25688;&#35201;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20351;&#29992;RAG&#35780;&#20272;&#30340;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15729v1 Announce Type: cross  Abstract: The complexity and sheer volume of information encompassing documents, papers, data, and other resources from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments 
&lt;/p&gt;</description></item><item><title>MedPromptX&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#22522;&#30784;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#20294;&#36873;&#25321;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2403.15585</link><description>&lt;p&gt;
MedPromptX&#65306;&#22522;&#20110;&#29616;&#23454;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15585
&lt;/p&gt;
&lt;p&gt;
MedPromptX&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#22522;&#30784;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#20294;&#36873;&#25321;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#32447;&#22270;&#20687;&#36890;&#24120;&#29992;&#20110;&#39044;&#27979;&#24613;&#24615;&#21644;&#24930;&#24615;&#24515;&#32954;&#30142;&#30149;&#65292;&#20294;&#26159;&#23558;&#23427;&#20204;&#19982;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#25972;&#21512;&#30340;&#21162;&#21147;&#38754;&#20020;&#30528;&#22240;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#19981;&#23436;&#25972;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;MedPromptX&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#65288;FP&#65289;&#21644;&#35270;&#35273;&#22522;&#30784;&#65288;VG&#65289;&#30456;&#32467;&#21512;&#65292;&#23558;&#22270;&#20687;&#19982;EHR&#25968;&#25454;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;MLLM&#34987;&#29992;&#26469;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#25552;&#20379;&#23545;&#24739;&#32773;&#30149;&#21490;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#23569;&#26679;&#26412;&#25552;&#31034;&#20943;&#23569;&#20102;&#23545;MLLM&#30340;&#22823;&#37327;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#36807;&#31243;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#21487;&#33021;&#36807;&#20110;&#32321;&#29712;&#65292;&#20294;&#23427;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#30528;&#28145;&#36828;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#26469;&#21160;&#24577;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15585v1 Announce Type: cross  Abstract: Chest X-ray images are commonly used for predicting acute and chronic cardiopulmonary conditions, but efforts to integrate them with structured clinical data face challenges due to incomplete electronic health records (EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate multimodal large language models (MLLMs), few-shot prompting (FP) and visual grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing EHR information, providing a comprehensive understanding of patients' medical history. Additionally, FP reduces the necessity for extensive training of MLLMs while effectively tackling the issue of hallucination. Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance. Hence, we propose a new technique that dynam
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;NaNa&#21644;MiGu&#20004;&#31181;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#34507;&#30333;&#36136;&#30340;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.14736</link><description>&lt;p&gt;
NaNa&#21644;MiGu&#65306;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#34507;&#30333;&#36136;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14736
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;NaNa&#21644;MiGu&#20004;&#31181;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#34507;&#30333;&#36136;&#30340;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#26159;&#21160;&#24577;&#21464;&#21270;&#30340;&#65292;&#36825;&#23558;&#20915;&#23450;&#34507;&#30333;&#36136;&#30340;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;ProNet&#65292;&#20165;&#35775;&#38382;&#26377;&#38480;&#30340;&#26500;&#35937;&#29305;&#24449;&#21644;&#34507;&#30333;&#36136;&#20391;&#38142;&#29305;&#24449;&#65292;&#23548;&#33268;&#39044;&#27979;&#20013;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#19981;&#20999;&#23454;&#38469;&#21644;&#34507;&#30333;&#36136;&#31867;&#21035;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;NaNa&#21644;MiGu&#65292;&#23558;&#34507;&#30333;&#36136;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#32435;&#20837;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#21644;&#20849;&#23884;&#27531;&#24046;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#34507;&#30333;&#36136;&#30340;&#20998;&#23376;&#29983;&#29289;&#29289;&#29702;&#12289;&#20108;&#32423;&#32467;&#26500;&#12289;&#21270;&#23398;&#38190;&#21644;&#31163;&#23376;&#29305;&#24449;&#26469;&#20419;&#36827;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14736v1 Announce Type: cross  Abstract: Protein classification tasks are essential in drug discovery. Real-world protein structures are dynamic, which will determine the properties of proteins. However, the existing machine learning methods, like ProNet (Wang et al., 2022a), only access limited conformational characteristics and protein side-chain features, leading to impractical protein structure and inaccuracy of protein classes in their predictions. In this paper, we propose novel semantic data augmentation methods, Novel Augmentation of New Node Attributes (NaNa), and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate backbone chemical and side-chain biophysical information into protein classification tasks and a co-embedding residual learning framework. Specifically, we leverage molecular biophysical, secondary structure, chemical bonds, and ionic features of proteins to facilitate protein classification tasks. Furthermore, our semantic augmentation me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#26041;&#27861;&#20316;&#20026;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#26356;&#31215;&#26497;&#30340;&#37325;&#21551;&#27169;&#24335;&#65292;&#21487;&#33021;&#20351;&#24471;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#19978;&#20351;&#29992;&#26356;&#36138;&#23146;&#30340;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#24615;&#33021;&#31867;&#20284;&#20110;&#20313;&#24358;&#36864;&#28779;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.14685</link><description>&lt;p&gt;
&#21608;&#26399;&#24615;&#23545;&#25968;&#28201;&#24230;&#35843;&#24230;&#20316;&#20026;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cyclical Log Annealing as a Learning Rate Scheduler
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14685
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#26041;&#27861;&#20316;&#20026;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#26356;&#31215;&#26497;&#30340;&#37325;&#21551;&#27169;&#24335;&#65292;&#21487;&#33021;&#20351;&#24471;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#19978;&#20351;&#29992;&#26356;&#36138;&#23146;&#30340;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#24615;&#33021;&#31867;&#20284;&#20110;&#20313;&#24358;&#36864;&#28779;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#26159;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25351;&#20196;&#65292;&#29992;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25913;&#21464;&#25628;&#32034;&#27493;&#38271;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#23545;&#27493;&#38271;&#36827;&#34892;&#20005;&#26684;&#30340;&#37325;&#21551;&#12290;&#21608;&#26399;&#24615;&#23545;&#25968;&#28201;&#24230;&#35843;&#24230;&#26356;&#31215;&#26497;&#22320;&#23454;&#29616;&#20102;&#37325;&#21551;&#27169;&#24335;&#65292;&#25110;&#35768;&#21487;&#20197;&#20801;&#35768;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#19978;&#20351;&#29992;&#26356;&#36138;&#23146;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20284;&#20046;&#22312;&#22823;&#22411;&#21464;&#21387;&#22120;&#22686;&#24378;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#20313;&#24358;&#36864;&#28779;&#26041;&#26696;&#34920;&#29616;&#31867;&#20284;&#12290;&#26410;&#26469;&#30340;&#23454;&#39564;&#23558;&#28041;&#21450;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#27979;&#35797;&#35843;&#24230;&#22120;&#65292;&#24182;&#36890;&#36807;&#26356;&#22810;&#23454;&#39564;&#25214;&#21040;&#35843;&#24230;&#22120;&#30340;&#26368;&#20339;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14685v1 Announce Type: new  Abstract: A learning rate scheduler is a predefined set of instructions for varying search stepsizes during model training processes. This paper introduces a new logarithmic method using harsh restarting of step sizes through stochastic gradient descent. Cyclical log annealing implements the restart pattern more aggressively to maybe allow the usage of more greedy algorithms on the online convex optimization framework. The algorithm was tested on the CIFAR-10 image datasets, and seemed to perform analogously with cosine annealing on large transformer-enhanced residual neural networks. Future experiments would involve testing the scheduler in generative adversarial networks and finding the best parameters for the scheduler with more experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14633</link><description>&lt;p&gt;
&#20986;&#36523;&#23500;&#36149;&#65311;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#22312;&#31038;&#20250;&#20013;&#21152;&#21095;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#26681;&#25454;&#20010;&#20154;&#32463;&#27982;&#21644;&#31038;&#20250;&#32972;&#26223;&#24433;&#21709;&#33719;&#21462;&#26426;&#20250;&#21644;&#36164;&#28304;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#26222;&#36941;&#38382;&#39064;&#25345;&#32493;&#22320;&#24310;&#32493;&#20102;&#31995;&#32479;&#24615;&#30340;&#19981;&#24179;&#31561;&#65292;&#38459;&#30861;&#20102;&#20316;&#20026;&#19968;&#20010;&#31038;&#20250;&#36861;&#27714;&#21253;&#23481;&#24615;&#36827;&#27493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;SilverSpoon&#65289;&#65292;&#21253;&#21547;3000&#20010;&#26679;&#26412;&#65292;&#23637;&#31034;&#20102;&#29301;&#28041;&#21040;&#24369;&#21183;&#32676;&#20307;&#30001;&#20110;&#20182;&#20204;&#30340;&#22788;&#22659;&#32780;&#23454;&#26045;&#36947;&#24503;&#27169;&#31946;&#34892;&#20026;&#30340;&#20551;&#35774;&#24773;&#26223;&#65292;&#24182;&#38382;&#36825;&#31181;&#34892;&#20026;&#26159;&#21542;&#22312;&#36947;&#24503;&#19978;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#30001;&#23646;&#20110;&#31038;&#20250;&#32463;&#27982;&#20004;&#31471;&#30340;&#20154;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#20351;&#29992;SilverSpoon&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#31243;&#24230;&#20197;&#21450;&#35813;&#31243;&#24230;&#22914;&#20309;&#38543;&#27169;&#22411;&#22823;&#23567;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14633v1 Announce Type: cross  Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;DeepFake&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#23613;&#31649;LLMs&#24182;&#38750;&#19987;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#35774;&#35745;&#65292;&#36825;&#19968;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.14077</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#33021;&#22815;&#26816;&#27979;DeepFakes&#21527;&#65311;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23186;&#20307;&#21462;&#35777;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;DeepFake&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#23613;&#31649;LLMs&#24182;&#38750;&#19987;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#35774;&#35745;&#65292;&#36825;&#19968;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DeepFakes&#26159;&#25351;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#23186;&#20307;&#20869;&#23481;&#65292;&#30001;&#20110;&#20854;&#34987;&#29992;&#20316;&#25955;&#24067;&#34394;&#20551;&#20449;&#24687;&#30340;&#25163;&#27573;&#65292;&#24050;&#32463;&#25104;&#20026;&#36234;&#26469;&#36234;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#26816;&#27979;DeepFakes&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#32534;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;DeepFake&#26816;&#27979;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#21487;&#20197;&#36890;&#36807;&#35880;&#24910;&#30340;&#23454;&#39564;&#35774;&#35745;&#21644;&#21450;&#26102;&#30340;&#24037;&#31243;&#26041;&#27861;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#32771;&#34385;&#21040;LLMs&#24182;&#19981;&#26159;&#26412;&#36136;&#19978;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#65292;&#36825;&#19968;&#28857;&#30456;&#24403;&#26377;&#36259;&#65292;&#32780;&#19988;&#36825;&#20010;&#36807;&#31243;&#24182;&#19981;&#38656;&#35201;&#32534;&#31243;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14077v1 Announce Type: new  Abstract: DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#36890;&#36807;&#30693;&#35782;&#21435;&#38500;&#36807;&#31243;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.13682</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23041;&#32961;&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Threats, Attacks, and Defenses in Machine Unlearning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13682
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#36890;&#36807;&#30693;&#35782;&#21435;&#38500;&#36807;&#31243;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#26368;&#36817;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#36890;&#36807;&#20174;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#25968;&#25454;&#30340;&#24433;&#21709;&#26469;&#23454;&#29616;&#23433;&#20840;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#20010;&#34987;&#31216;&#20026;&#30693;&#35782;&#21435;&#38500;&#30340;&#36807;&#31243;&#35299;&#20915;&#20102;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#36136;&#37327;&#12289;&#25935;&#24863;&#24615;&#12289;&#29256;&#26435;&#38480;&#21046;&#21644;&#36807;&#26102;&#24615;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#36981;&#23432;&#35832;&#22914;&#34987;&#36951;&#24536;&#26435;&#31561;&#38544;&#31169;&#27861;&#35268;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#30340;&#30693;&#35782;&#21435;&#38500;&#26377;&#21161;&#20110;&#20943;&#36731;&#26377;&#23475;&#32467;&#26524;&#30340;&#39118;&#38505;&#65292;&#38450;&#33539;&#20559;&#35265;&#12289;&#35823;&#23548;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#21033;&#29992;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;&#24050;&#32463;&#24320;&#23637;&#20102;&#35774;&#35745;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#30740;&#31350;MU&#26381;&#21153;&#20197;&#19982;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#38598;&#25104;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25552;&#20132;&#35831;&#27714;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13682v2 Announce Type: replace-cross  Abstract: Machine Unlearning (MU) has gained considerable attention recently for its potential to achieve Safe AI by removing the influence of specific data from trained machine learning models. This process, known as knowledge removal, addresses AI governance concerns of training data such as quality, sensitivity, copyright restrictions, and obsolescence. This capability is also crucial for ensuring compliance with privacy regulations such as the Right To Be Forgotten. Furthermore, effective knowledge removal mitigates the risk of harmful outcomes, safeguarding against biases, misinformation, and unauthorized data exploitation, thereby enhancing the safe and responsible use of AI systems. Efforts have been made to design efficient unlearning approaches, with MU services being examined for integration with existing machine learning as a service, allowing users to submit requests to remove specific data from the training corpus. However, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30524;&#21160;&#25968;&#25454;&#25552;&#21462;&#29992;&#25143;&#23884;&#20837;&#65292;&#21487;&#20197;&#26377;&#25928;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13653</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#20957;&#35270;&#20013;&#23398;&#20064;&#29992;&#25143;&#23884;&#20837;&#20197;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning User Embeddings from Human Gaze for Personalised Saliency Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13653
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30524;&#21160;&#25968;&#25454;&#25552;&#21462;&#29992;&#25143;&#23884;&#20837;&#65292;&#21487;&#20197;&#26377;&#25928;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#30340;&#21487;&#37325;&#29992;&#23884;&#20837;&#24050;&#32463;&#26174;&#31034;&#20986;&#23545;&#20010;&#24615;&#21270;&#26174;&#33879;&#24615;&#39044;&#27979;&#20219;&#21153;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#38656;&#35201;&#26126;&#30830;&#30340;&#29992;&#25143;&#29305;&#24449;&#21644;&#20559;&#22909;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20174;&#23569;&#37327;&#29992;&#25143;&#29305;&#23450;&#30340;&#30524;&#21160;&#25968;&#25454;&#29983;&#25104;&#30340;&#33258;&#28982;&#22270;&#20687;&#21644;&#30456;&#24212;&#26174;&#33879;&#24615;&#22320;&#22270;&#23545;&#20013;&#25552;&#21462;&#29992;&#25143;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;Siamese&#21367;&#31215;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#29992;&#25143;&#30340;&#22270;&#20687;&#21644;&#20010;&#20154;&#26174;&#33879;&#24615;&#22320;&#22270;&#23545;&#26469;&#23398;&#20064;&#29992;&#25143;&#23884;&#20837;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#26174;&#33879;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#29983;&#25104;&#30340;&#23884;&#20837;&#20855;&#26377;&#24456;&#39640;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#23558;&#36890;&#29992;&#26174;&#33879;&#24615;&#22320;&#22270;&#20248;&#21270;&#21040;&#20010;&#20154;&#29992;&#25143;&#23618;&#38754;&#65292;&#24182;&#19988;&#22312;&#29992;&#25143;&#21644;&#22270;&#20687;&#20043;&#38388;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#25105;&#20204;&#27169;&#22411;&#23545;&#20010;&#20154;&#29992;&#25143;&#29305;&#24449;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25351;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13653v2 Announce Type: replace-cross  Abstract: Reusable embeddings of user behaviour have shown significant performance improvements for the personalised saliency prediction task. However, prior works require explicit user characteristics and preferences as input, which are often difficult to obtain. We present a novel method to extract user embeddings from pairs of natural images and corresponding saliency maps generated from a small amount of user-specific eye tracking data. At the core of our method is a Siamese convolutional neural encoder that learns the user embeddings by contrasting the image and personal saliency map pairs of different users. Evaluations on two public saliency datasets show that the generated embeddings have high discriminative power, are effective at refining universal saliency maps to the individual users, and generalise well across users and images. Finally, based on our model's ability to encode individual user characteristics, our work points t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;FineHumanML3D&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;FineMotionDiffuse&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.13518</link><description>&lt;p&gt;
&#20174;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
Motion Generation from Fine-grained Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;FineHumanML3D&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;FineMotionDiffuse&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#21160;&#20316;&#30340;&#20219;&#21153;&#26159;&#20174;&#32473;&#23450;&#30340;&#25991;&#23383;&#25551;&#36848;&#29983;&#25104;&#36816;&#21160;&#24207;&#21015;&#65292;&#27169;&#22411;&#24212;&#35813;&#25506;&#32034;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19982;&#20154;&#20307;&#21160;&#20316;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#23616;&#38480;&#20110;&#31895;&#31890;&#24230;&#30340;&#36816;&#21160;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#8220;&#19968;&#20010;&#20154;&#36466;&#19979;&#12290;&#8221;&#65289;&#65292;&#20960;&#20046;&#27809;&#26377;&#25506;&#32034;&#25351;&#23450;&#30456;&#20851;&#36523;&#20307;&#37096;&#20301;&#36816;&#21160;&#30340;&#32454;&#31890;&#24230;&#25551;&#36848;&#12290;&#29992;&#31895;&#31961;&#25991;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#23398;&#20064;&#20174;&#32454;&#31890;&#24230;&#36816;&#21160;&#30456;&#20851;&#35789;&#27719;&#21040;&#36816;&#21160;&#22522;&#20803;&#30340;&#26144;&#23556;&#65292;&#23548;&#33268;&#26080;&#27861;&#20174;&#26410;&#35265;&#25551;&#36848;&#29983;&#25104;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36755;&#20837;&#31934;&#32454;&#25552;&#31034;&#32473; GPT-3.5-turbo&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;-&#21160;&#20316;&#25968;&#25454;&#38598;FineHumanML3D&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26412;&#21040;&#21160;&#20316;&#27169;&#22411;FineMotionDiffuse&#65292;&#20805;&#20998;&#21033;&#29992;&#32454;&#31890;&#24230;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FineMotionDiffuse&#22312;FineHumanML3D&#19978;&#35757;&#32451;&#21518;&#33719;&#24471;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13518v1 Announce Type: cross  Abstract: The task of text2motion is to generate motion sequences from given textual descriptions, where a model should explore the interactions between natural language instructions and human body movements. While most existing works are confined to coarse-grained motion descriptions (e.g., "A man squats."), fine-grained ones specifying movements of relevant body parts are barely explored. Models trained with coarse texts may not be able to learn mappings from fine-grained motion-related words to motion primitives, resulting in the failure in generating motions from unseen descriptions. In this paper, we build a large-scale language-motion dataset with fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we design a new text2motion model, FineMotionDiffuse, which makes full use of fine-grained textual information. Our experiments show that FineMotionDiffuse trained on FineHumanML3D acqui
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13374</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#33258;&#36866;&#24212;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13374
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22788;&#29702;&#20102;&#22312;&#23384;&#22312;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#24179;&#22343;&#26799;&#24230;&#31639;&#27861;&#65288;RAGA&#65289;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20960;&#20309;&#20013;&#20301;&#25968;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#21487;&#20197;&#33258;&#30001;&#36873;&#25321;&#26412;&#22320;&#26356;&#26032;&#30340;&#36718;&#25968;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24377;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#25110;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25910;&#25947;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#24378;&#20984;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#22312;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21482;&#35201;&#24694;&#24847;&#29992;&#25143;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23567;&#20110;&#19968;&#21322;&#65292;RAGA&#23601;&#21487;&#20197;&#20197;$\mathcal{O}({1}/{T^{2/3- \delta}})$&#30340;&#36895;&#24230;&#23454;&#29616;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#65292;&#20854;&#20013;$T$&#20026;&#36845;&#20195;&#27425;&#25968;&#65292;$\delta \in (0, 2/3)$&#65292;&#23545;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#21017;&#21576;&#32447;&#24615;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#31283;&#23450;&#28857;&#25110;&#20840;&#23616;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and $\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#23384;&#22312;&#30340;&#39640;&#30828;&#20214;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;Hyacinth6B&#22312;&#27169;&#22411;&#36731;&#37327;&#21270;&#21644;&#24615;&#33021;&#20043;&#38388;&#25214;&#21040;&#20102;&#24179;&#34913;&#65292;&#37319;&#29992;LoRA&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.13334</link><description>&lt;p&gt;
Hyacinth6B&#65306;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hyacinth6B: A large language model for Traditional Chinese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13334
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#23384;&#22312;&#30340;&#39640;&#30828;&#20214;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;Hyacinth6B&#22312;&#27169;&#22411;&#36731;&#37327;&#21270;&#21644;&#24615;&#33021;&#20043;&#38388;&#25214;&#21040;&#20102;&#24179;&#34913;&#65292;&#37319;&#29992;LoRA&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#24212;&#23545;&#36890;&#24120;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#39640;&#30828;&#20214;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#27169;&#22411;&#36731;&#37327;&#21270;&#21644;&#24615;&#33021;&#20043;&#38388;&#25214;&#21040;&#24179;&#34913;&#65292;&#21162;&#21147;&#22312;&#20351;&#29992;&#30456;&#23545;&#36731;&#37327;&#32423;&#27169;&#22411;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;Hyacinth6B&#26159;&#22522;&#20110;&#36825;&#19968;&#30446;&#26631;&#24320;&#21457;&#30340;&#65292;&#26088;&#22312;&#20805;&#20998;&#21457;&#25381;LLM&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#32780;&#19981;&#36896;&#25104;&#24040;&#22823;&#30340;&#36164;&#28304;&#25104;&#26412;&#65292;&#26377;&#25928;&#22320;&#25512;&#21160;&#36739;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#36793;&#30028;&#12290;&#35757;&#32451;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;LoRA&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13334v1 Announce Type: new  Abstract: This research's primary motivation of this study is to address the high hardware and computational demands typically associated with LLMs.Therefore,our goal is to find a balance between model lightness and performance,striving to maximize performance while using a comparatively lightweight model. Hyacinth6B was developed with this objective in mind,aiming to fully leverage the core capabilities of LLMs without incurring substantial resource costs, effectively pushing the boundaries of smaller model's performance. The training approach involves parameter efficient finetuning using the LoRA method.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12151</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20869;&#23481;&#34701;&#20837;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#22686;&#24378;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12151
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#26377;&#21161;&#20110;&#35299;&#20915;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294;&#29983;&#25104;&#36825;&#31181;&#30693;&#35782;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36890;&#36807;&#35821;&#20041;&#23884;&#20837;&#29983;&#25104;&#21644;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23558;LLM&#38598;&#25104;&#21040;&#19968;&#20010;&#27969;&#31243;&#20013;&#65292;&#35813;&#27969;&#31243;&#22312;&#35270;&#35273;&#22522;&#30784;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#21521;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#24443;&#24213;&#30740;&#31350;&#20102;LLM&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#23884;&#20837;&#19982;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20511;&#37492;&#36825;&#19968;&#28040;&#34701;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23545;&#31454;&#20105;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20174;&#32780;&#31361;&#20986;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.11996</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#30693;&#35782;&#25552;&#21462;&#12289;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#21644;&#22810;&#27169;&#24577;&#26234;&#33021;&#22270;&#25512;&#29702;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11996
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#23558;&#19968;&#32452;&#28041;&#21450;&#29983;&#29289;&#26448;&#26009;&#39046;&#22495;&#30340;1,000&#31687;&#31185;&#23398;&#35770;&#25991;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#26412;&#20307;&#30693;&#35782;&#22270;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#26080;&#26631;&#24230;&#29305;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24615;&#21644;&#20171;&#25968;&#20013;&#24515;&#24615;&#30340;&#32452;&#21512;&#25490;&#21517;&#65292;&#25506;&#27979;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#30340;&#22270;&#36941;&#21382;&#36335;&#24452;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#20837;&#30340;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26597;&#35810;&#65292;&#35782;&#21035;&#30693;&#35782;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#21069;&#25152;&#26410;&#35265;&#30340;&#26448;&#26009;&#35774;&#35745;&#21450;&#20854;&#34892;&#20026;&#12290;&#19968;&#39033;&#27604;&#36739;&#25581;&#31034;&#20102;&#29983;&#29289;&#26448;&#26009;&#21644;&#36125;&#22810;&#33452;&#31532;&#20061;&#20132;&#21709;&#26354;&#20043;&#38388;&#30340;&#35814;&#32454;&#32467;&#26500;&#30456;&#20284;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#36890;&#36807;&#21516;&#26500;&#26144;&#23556;&#20849;&#20139;&#22797;&#26434;&#24615;&#27169;&#24335;&#12290;&#35813;&#31639;&#27861;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#20998;&#32423;&#33740;&#19997;&#20307;&#30340;&#22797;&#21512;&#26448;&#26009;&#65292;&#23558;&#22270;&#37319;&#26679;&#30340;&#32852;&#21512;&#21512;&#25104;&#21407;&#29702;&#19982;&#24247;&#23450;&#26031;&#22522;&#12298;&#31532;&#19971;&#32452;&#25104;&#12299;&#20013;&#25552;&#21462;&#30340;&#21407;&#21017;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.10949</link><description>&lt;p&gt;
SelfIE&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SelfIE: Self-Interpretation of Large Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#33719;&#24471;&#31572;&#26696;&#65311;&#35299;&#37322;&#21644;&#25511;&#21046;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#23545;&#20110;&#21487;&#38752;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#26410;&#26469;&#27169;&#22411;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SelfIE&#65288;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;LLMs&#21709;&#24212;&#20851;&#20110;&#32473;&#23450;&#27573;&#33853;&#30340;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#33258;&#24049;&#30340;&#23884;&#20837;&#12290;SelfIE&#33021;&#22815;&#35299;&#37322;&#38544;&#34255;&#23884;&#20837;&#20013;&#30340;&#24320;&#25918;&#19990;&#30028;&#27010;&#24565;&#65292;&#22312;&#26696;&#20363;&#20013;&#25581;&#31034;LLM&#30340;&#20869;&#37096;&#25512;&#29702;&#65292;&#22914;&#20570;&#20986;&#36947;&#24503;&#20915;&#31574;&#12289;&#20869;&#21270;&#25552;&#31034;&#27880;&#20837;&#21644;&#22238;&#24819;&#26377;&#23475;&#30693;&#35782;&#12290;SelfIE&#23545;&#38544;&#34255;&#23884;&#20837;&#30340;&#25991;&#26412;&#25551;&#36848;&#20063;&#24320;&#36767;&#20102;&#25511;&#21046;LLM&#25512;&#29702;&#30340;&#26032;&#36884;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#30563;&#25511;&#21046;&#65292;&#23427;&#20801;&#35768;&#32534;&#36753;&#24320;&#25918;&#24335;&#27010;&#24565;&#65292;&#32780;&#21482;&#38656;&#35201;&#35745;&#31639;&#21333;&#20010;&#23618;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23558;RLHF&#25193;&#23637;&#21040;&#38544;&#34255;&#30340;&#23884;&#20837;&#65292;&#24182;&#25552;&#20986;&#20102;&#24378;&#21270;&#25511;&#21046;&#26469;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.09963</link><description>&lt;p&gt;
&#22788;&#29702;&#22909;&#24744;&#30340;&#25552;&#31034;&#20559;&#35265;&#65281;&#35843;&#26597;&#21644;&#20943;&#36731;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#30340;&#25552;&#31034;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#21363;&#25552;&#31034;&#24448;&#24448;&#20250;&#24341;&#20837;&#23545;&#29305;&#23450;&#26631;&#31614;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#20869;&#37096;&#25552;&#31034;&#20559;&#35265;&#30340;&#31243;&#24230;&#21644;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22238;&#24212;&#36825;&#19968;&#28857;&#65292;&#26412;&#25991;&#37327;&#21270;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#23454;&#39564;&#20013;&#30340;&#25152;&#26377;&#25552;&#31034;&#37117;&#34920;&#29616;&#20986;&#19981;&#21487;&#24573;&#35270;&#30340;&#20559;&#35265;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25552;&#31034;&#22914;AutoPrompt&#21644;OptiPrompt&#26174;&#31034;&#20986;&#26356;&#39640;&#27700;&#24179;&#30340;&#20559;&#35265;&#65307;2&#65289;&#25552;&#31034;&#20559;&#35265;&#21487;&#20197;&#36890;&#36807;&#36807;&#24230;&#25311;&#21512;&#27979;&#35797;&#25968;&#25454;&#38598;&#19981;&#21512;&#29702;&#22320;&#25918;&#22823;&#22522;&#20934;&#27979;&#35797;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;LAMA&#36825;&#26679;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#25552;&#31034;&#20559;&#35265;&#65292;&#22312;&#25512;&#26029;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20165;&#25552;&#31034;&#26597;&#35810;&#26469;&#20272;&#35745;&#26377;&#20559;&#24046;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20174;&#20013;&#21024;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09963v1 Announce Type: cross  Abstract: Recent research shows that pre-trained language models (PLMs) suffer from "prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce biases toward specific labels. However, the extent and impact of prompt bias within the model remain underexplored. In response, this paper quantifies the bias with various types of prompts and assesses their impact on different benchmarks. We show that: 1) all prompts in the experiments exhibit non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the prompt bias during inference time. Specifically, we first estimate the biased representation using prompt-only querying, and then remove it from the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.09857</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09857
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#27169;&#22411;&#26088;&#22312;&#22312;&#20445;&#30041;&#26087;&#31867;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#36880;&#27493;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#31232;&#32570;&#26679;&#26412;&#12290;&#29616;&#26377;&#30340;FSCIL&#26041;&#27861;&#36890;&#24120;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#24494;&#35843;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38459;&#30861;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#28508;&#21147;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#36817;&#22522;&#20110;&#25552;&#31034;&#30340;CIL&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#29992;&#36275;&#22815;&#30340;&#25968;&#25454;&#35757;&#32451;&#25552;&#31034;&#26469;&#20943;&#36731;&#36951;&#24536;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#65288;ASP&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;ASP&#36890;&#36807;&#20174;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;ASP&#20013;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#25552;&#20379;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;&#24635;&#20043;&#65292;ASP&#38450;&#27490;&#20102;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#30340;&#36807;&#25311;&#21512;&#65292;&#24182;&#19981;&#38656;&#35201;&#22312;&#23569;&#26679;&#26412;&#22686;&#37327;&#20219;&#21153;&#20013;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09857v1 Announce Type: cross  Abstract: Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensi
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.09738</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#25512;&#33616;&#20013;&#29983;&#25104;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09738
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29992;&#25143;&#26159;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#30495;&#23454;&#29992;&#25143;&#20195;&#29702;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#27169;&#25311;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#23427;&#20204;&#33021;&#21542;&#20195;&#34920;&#22810;&#26679;&#21270;&#29992;&#25143;&#32676;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#23545;&#35805;&#25512;&#33616;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#35813;&#21327;&#35758;&#30001;&#20116;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#21512;&#25104;&#29992;&#25143;&#24212;&#35813;&#34920;&#29616;&#20986;&#30340;&#20851;&#38190;&#29305;&#24615;&#65306;&#36873;&#25321;&#35201;&#35848;&#35770;&#30340;&#29289;&#21697;&#65292;&#34920;&#36798;&#20108;&#36827;&#21046;&#20559;&#22909;&#65292;&#34920;&#36798;&#24320;&#25918;&#24335;&#20559;&#22909;&#65292;&#35831;&#27714;&#25512;&#33616;&#20197;&#21450;&#25552;&#20379;&#21453;&#39304;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08763</link><description>&lt;p&gt;
&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Strategies to Continually Pre-train Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08763
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25968;&#21313;&#20159;&#30340;&#26631;&#35760;&#19978;&#36827;&#34892;&#24120;&#35268;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#23601;&#37325;&#26032;&#24320;&#22987;&#35813;&#36807;&#31243;&#12290;&#19968;&#20010;&#26356;&#26377;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25345;&#32493;&#39044;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#33021;&#33410;&#30465;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#20197;&#21069;&#25968;&#25454;&#19978;&#38477;&#20302;&#24615;&#33021;&#25110;&#26080;&#27861;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#65288;LR&#65289;&#37325;&#26032;&#21319;&#28201;&#12289;LR&#37325;&#26032;&#34928;&#20943;&#21644;&#37325;&#25918;&#19978;&#19968;&#25968;&#25454;&#30340;&#32452;&#21512;&#36275;&#20197;&#19982;&#23436;&#20840;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20174;&#26368;&#32456;&#25439;&#22833;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22522;&#20934;&#30340;&#35282;&#24230;&#34913;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#8594;&#33521;&#35821;&#65289;&#20043;&#38388;&#30340;&#24369;&#20294;&#29616;&#23454;&#30340;&#20998;&#24067;&#36716;&#31227;&#20197;&#21450;&#26356;&#24378;&#28872;&#30340;&#20998;&#24067;&#36716;&#31227;&#65288;&#33521;&#35821;&#8594;&#24503;&#35821;&#65289;&#19979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08281</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#25484;&#25569;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;
&lt;/p&gt;
&lt;p&gt;
Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08281
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#12289;&#32534;&#31243;&#20195;&#30721;&#21644;&#25968;&#23398;&#31526;&#21495;&#30340;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#24040;&#22823;&#65292;&#23545;&#20110;&#37027;&#20123;&#21162;&#21147;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20986;&#20102;&#22797;&#26434;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#34701;&#21512;&#24050;&#32463;&#39640;&#24230;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#26694;&#26550;UltraFuser&#21253;&#25324;&#19977;&#20010;&#24050;&#32463;&#22312;&#35821;&#35328;&#12289;&#32534;&#30721;&#21644;&#25968;&#23398;&#19978;&#24471;&#21040;&#20805;&#20998;&#35757;&#32451;&#30340;&#19987;&#23478;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#26469;&#28151;&#21512;&#19987;&#23478;&#30340;&#36755;&#20986;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20276;&#38543;&#24179;&#34913;&#37319;&#26679;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#20197;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#34701;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#29983;&#25104;&#31867;&#20284;&#36755;&#20837;&#22270;&#20687;&#30340;&#26080;&#32570;&#38519;&#25968;&#25454;&#22270;&#20687;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#22270;&#20687;&#31526;&#21512;&#26399;&#26395;&#20998;&#24067;&#65292;&#31283;&#23450;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06247</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#21464;&#20998;&#22270;&#20687;&#29983;&#25104;&#29992;&#20110;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#29983;&#25104;&#31867;&#20284;&#36755;&#20837;&#22270;&#20687;&#30340;&#26080;&#32570;&#38519;&#25968;&#25454;&#22270;&#20687;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#22270;&#20687;&#31526;&#21512;&#26399;&#26395;&#20998;&#24067;&#65292;&#31283;&#23450;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;&#21464;&#20998;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24037;&#19994;&#21046;&#36896;&#20013;&#24322;&#24120;&#26816;&#27979;&#30340;&#24178;&#20928;&#25968;&#25454;&#33719;&#21462;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#24191;&#27867;&#25991;&#26412;&#24211;&#25991;&#26723;&#30340;&#20851;&#20110;&#30446;&#26631;&#23545;&#35937;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#31867;&#20284;&#36755;&#20837;&#22270;&#20687;&#30340;&#26080;&#32570;&#38519;&#25968;&#25454;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30830;&#20445;&#29983;&#25104;&#30340;&#26080;&#32570;&#38519;&#22270;&#20687;&#19982;&#26469;&#33258;&#25991;&#26412;&#21644;&#22270;&#20687;&#30693;&#35782;&#30340;&#39044;&#26399;&#20998;&#24067;&#20445;&#25345;&#19968;&#33268;&#65292;&#30830;&#20445;&#31283;&#23450;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#26080;&#32570;&#38519;&#25968;&#25454;&#65292;&#20063;&#33021;&#36229;&#36234;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36328;&#22235;&#20010;&#22522;&#20934;&#27169;&#22411;&#21644;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#27979;&#35797;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#25928;&#26524;&#30340;&#39069;&#22806;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06247v1 Announce Type: cross  Abstract: We propose a text-guided variational image generation method to address the challenge of getting clean data for anomaly detection in industrial manufacturing. Our method utilizes text information about the target object, learned from extensive text library documents, to generate non-defective data images resembling the input image. The proposed framework ensures that the generated non-defective images align with anticipated distributions derived from textual and image-based knowledge, ensuring stability and generality. Experimental results demonstrate the effectiveness of our approach, surpassing previous methods even with limited non-defective data. Our approach is validated through generalization tests across four baseline models and three distinct datasets. We present an additional analysis to enhance the effectiveness of anomaly detection models by utilizing the generated images.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRSC&#30340;&#21307;&#23398;&#35328;&#35821;&#20998;&#31867;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#23398;&#20064;&#20174;&#25991;&#26412;-&#22768;&#23398;&#25968;&#25454;&#20013;&#20998;&#31163;&#24847;&#22270;&#21644;&#20869;&#23481;&#34920;&#31034;&#20197;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26816;&#27979;25&#31181;&#19981;&#21516;&#21307;&#23398;&#30151;&#29366;&#26102;&#21462;&#24471;&#20102;95%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.05000</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#34920;&#31034;&#36827;&#34892;&#21307;&#23398;&#35328;&#35821;&#30151;&#29366;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Medical Speech Symptoms Classification via Disentangled Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRSC&#30340;&#21307;&#23398;&#35328;&#35821;&#20998;&#31867;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#23398;&#20064;&#20174;&#25991;&#26412;-&#22768;&#23398;&#25968;&#25454;&#20013;&#20998;&#31163;&#24847;&#22270;&#21644;&#20869;&#23481;&#34920;&#31034;&#20197;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26816;&#27979;25&#31181;&#19981;&#21516;&#21307;&#23398;&#30151;&#29366;&#26102;&#21462;&#24471;&#20102;95%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05000v1 &#20844;&#21578;&#31867;&#22411;:new &#25688;&#35201;: &#22312;&#29616;&#26377;&#24037;&#20316;&#20013;&#65292;&#24847;&#22270;&#34987;&#23450;&#20041;&#29992;&#20110;&#29702;&#35299;&#21475;&#22836;&#35821;&#35328;&#12290;&#21307;&#23398;&#35328;&#35821;&#20013;&#28041;&#21450;&#30340;&#25991;&#26412;&#29305;&#24449;&#21644;&#22768;&#23398;&#29305;&#24449;&#22343;&#21253;&#21547;&#24847;&#22270;&#65292;&#36825;&#23545;&#20110;&#30151;&#29366;&#35786;&#26029;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRSC&#30340;&#21307;&#23398;&#35328;&#35821;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33258;&#21160;&#23398;&#20064;&#20174;&#25991;&#26412;-&#22768;&#23398;&#25968;&#25454;&#20013;&#20998;&#31163;&#24847;&#22270;&#21644;&#20869;&#23481;&#34920;&#31034;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290; &#36890;&#36807;&#24847;&#22270;&#32534;&#30721;&#22120;&#25552;&#21462;&#25991;&#26412;&#22495;&#21644;Mel-&#39057;&#35889;&#22270;&#22495;&#30340;&#24847;&#22270;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#20004;&#20010;&#20132;&#25442;&#33719;&#21462;&#37325;&#26500;&#30340;&#25991;&#26412;&#29305;&#24449;&#21644;Mel-&#39057;&#35889;&#22270;&#29305;&#24449;&#12290;&#22312;&#23558;&#20004;&#20010;&#22495;&#30340;&#24847;&#22270;&#32467;&#21512;&#25104;&#19968;&#20010;&#32852;&#21512;&#34920;&#31034;&#21518;&#65292;&#32508;&#21512;&#24847;&#22270;&#34920;&#31034;&#34987;&#36755;&#20837;&#20915;&#31574;&#23618;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26816;&#27979;25&#31181;&#19981;&#21516;&#21307;&#23398;&#30151;&#29366;&#26102;&#33719;&#24471;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;95%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05000v1 Announce Type: new  Abstract: Intent is defined for understanding spoken language in existing works. Both textual features and acoustic features involved in medical speech contain intent, which is important for symptomatic diagnosis. In this paper, we propose a medical speech classification model named DRSC that automatically learns to disentangle intent and content representations from textual-acoustic data for classification. The intent representations of the text domain and the Mel-spectrogram domain are extracted via intent encoders, and then the reconstructed text feature and the Mel-spectrogram feature are obtained through two exchanges. After combining the intent from two domains into a joint representation, the integrated intent representation is fed into a decision layer for classification. Experimental results show that our model obtains an average accuracy rate of 95% in detecting 25 different medical symptoms.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#23545;&#20110;&#29289;&#20307;&#19982;&#32972;&#26223;&#20043;&#38388;&#22810;&#26679;&#21270;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#24341;&#20837;&#19981;&#21516;&#23545;&#35937;&#26041;&#38754;&#21464;&#21270;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.04701</link><description>&lt;p&gt;
ObjectCompose: &#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#22312;&#29289;&#20307;&#19982;&#32972;&#26223;&#32452;&#21512;&#21464;&#21270;&#19978;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04701
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#23545;&#20110;&#29289;&#20307;&#19982;&#32972;&#26223;&#20043;&#38388;&#22810;&#26679;&#21270;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#24341;&#20837;&#19981;&#21516;&#23545;&#35937;&#26041;&#38754;&#21464;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35757;&#32451;&#24182;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#20102;&#35299;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#31243;&#24230;&#23545;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#38024;&#23545;&#19981;&#21516;&#30340;&#29289;&#20307;&#19982;&#32972;&#26223;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#38887;&#24615;&#12290;&#22823;&#22810;&#25968;&#40065;&#26834;&#24615;&#35780;&#20272;&#26041;&#27861;&#24341;&#20837;&#20102;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#35825;&#23548;&#29289;&#20307;&#29305;&#24449;&#65288;&#35270;&#28857;&#12289;&#23610;&#24230;&#12289;&#39068;&#33394;&#65289;&#30340;&#21464;&#21270;&#65292;&#25110;&#32773;&#21033;&#29992;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#65288;&#23545;&#25239;&#24615;&#21464;&#21270;&#12289;&#24120;&#35265;&#30772;&#22351;&#65289;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#27169;&#25311;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#32972;&#26223;&#30340;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#22312;&#25552;&#20379;&#23545;&#35201;&#36827;&#34892;&#30340;&#26356;&#25913;&#30340;&#25511;&#21046;&#26041;&#38754;&#19981;&#36275;&#65292;&#35201;&#20040;&#25197;&#26354;&#20102;&#29289;&#20307;&#30340;&#35821;&#20041;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24341;&#20837;&#21508;&#31181;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04701v1 Announce Type: cross  Abstract: Given the large-scale multi-modal training of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment. In this work, we evaluate the resilience of current vision-based models against diverse object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Recent works have explored leveraging large language models and diffusion models to generate changes in the background. However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task. Our method, on the other hand, can induce diverse objec
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20840;&#38754;&#35780;&#20272;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#21457;&#29616;&#38598;&#25104;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#65289;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02232</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#23545;Mal-API-2019&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20840;&#38754;&#35780;&#20272;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#21457;&#29616;&#38598;&#25104;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#65289;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#25506;&#35752;&#65292;&#37325;&#28857;&#35780;&#20272;&#20102;&#20351;&#29992;Mal-API-2019&#25968;&#25454;&#38598;&#30340;&#21508;&#31181;&#20998;&#31867;&#27169;&#22411;&#12290;&#26088;&#22312;&#36890;&#36807;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#32531;&#35299;&#23041;&#32961;&#26469;&#25512;&#36827;&#32593;&#32476;&#23433;&#20840;&#33021;&#21147;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#38598;&#25104;&#21644;&#38750;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;&#29305;&#21035;&#24378;&#35843;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;TF-IDF&#34920;&#31034;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#38598;&#25104;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#38480;&#21046;&#21644;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#19981;&#26029;&#36866;&#24212;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02232v1 Announce Type: cross  Abstract: This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset. The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively. Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special emphasis is placed on the importance of data pre-processing techniques, particularly TF-IDF representation and Principal Component Analysis, in improving model performance. Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection. The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01748</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#20449;&#21495;&#35299;&#30721;&#20026;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Decode Neural signal as Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#21160;&#24577;&#35299;&#30721;&#35821;&#35328;&#26159;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#26041;&#21521;&#65292;&#23588;&#20854;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#30456;&#23545;&#20110;&#38656;&#35201;&#30005;&#26497;&#26893;&#20837;&#25163;&#26415;&#30340;&#20405;&#20837;&#24615;&#20449;&#21495;&#65292;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#20449;&#21495;&#65288;&#22914;EEG&#12289;MEG&#65289;&#30001;&#20110;&#20854;&#23433;&#20840;&#24615;&#21644;&#26222;&#36866;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#19977;&#20010;&#26041;&#38754;&#30340;&#25506;&#32034;&#36824;&#19981;&#36275;&#65306;1&#65289;&#20197;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#20808;&#21069;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;MEG&#20449;&#21495;&#36136;&#37327;&#26356;&#22909;&#30340;&#38382;&#39064;&#65307;2&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#22312;&#29983;&#25104;&#35299;&#30721;&#36807;&#31243;&#20013;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65307;3&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#22823;&#22810;&#26159;&#22522;&#20110;&#8220;BART&#8221;&#32780;&#19981;&#26159;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#22312;&#35821;&#38899;&#35299;&#30721;&#24418;&#24335;&#20013;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#20013;&#30740;&#31350;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01748v1 Announce Type: cross  Abstract: Decoding language from brain dynamics is an important open direction in the realm of brain-computer interface (BCI), especially considering the rapid growth of large language models. Compared to invasive-based signals which require electrode implantation surgery, non-invasive neural signals (e.g. EEG, MEG) have attracted increasing attention considering their safety and generality. However, the exploration is not adequate in three aspects: 1) previous methods mainly focus on EEG but none of the previous works address this problem on MEG with better signal quality; 2) prior works have predominantly used ``teacher-forcing" during generative decoding, which is impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive, which performs better in other sequence tasks. In this paper, we explore the brain-to-text translation of MEG signals in a speech-decoding formation. Here we are the first to investigate a cross-attentio
&lt;/p&gt;</description></item><item><title>&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.11800</link><description>&lt;p&gt;
&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65306;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#30340;&#26377;&#38480;&#26102;&#38388;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11800
&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#22823;&#35268;&#27169;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#26041;&#26696;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;&#34429;&#28982;&#24310;&#36831;&#30340;&#24433;&#21709;&#22312;&#20248;&#21270;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#19982;&#24213;&#23618;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#30456;&#20114;&#20316;&#29992;&#20197;&#22609;&#36896;SA&#30340;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#30340;&#26041;&#24335;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#35777;&#26126;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#24310;&#36831;&#30340;SA&#26356;&#26032;&#35268;&#21017;&#30830;&#20445;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21040;SA&#36816;&#31639;&#31526;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#20855;&#26377;&#25351;&#25968;&#24555;&#36895;&#30340;&#36895;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;$\tau_{max}$&#21644;&#28151;&#21512;&#26102;&#38388;$\tau_{mix}$&#26041;&#38754;&#26159;\emph{&#32039;&#33268;&#30340;}&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#32039;&#23494;&#30028;&#38480;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24402;&#32435;&#35777;&#26126;&#25216;&#26415;&#65292;&#19982;&#21508;&#31181;&#29616;&#26377;&#24310;&#36831;&#20248;&#21270;&#20998;&#26512;&#19981;&#21516;&#65292;&#23427;&#20381;&#36182;&#20110;&#24314;&#31435;&#26410;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#20116;&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;48&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#65292;&#20026;LLMs&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.11537</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#21435;&#23398;&#20064;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20116;&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;48&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#65292;&#20026;LLMs&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20855;&#26377;&#21508;&#31181;&#26469;&#28304;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#32452;&#32455;&#20173;&#28982;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#20559;&#31163;&#26368;&#20339;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#26469;&#33258;LLMs&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;5&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;48&#20010;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#20851;&#20110;&#20061;&#20010;&#20027;&#35201;&#27169;&#22411;&#33021;&#21147;&#31867;&#21035;&#30340;&#22522;&#20934;&#26469;&#34913;&#37327;&#23427;&#20204;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#22810;&#20010;&#35821;&#26009;&#24211;&#23545;LLMs&#24615;&#33021;&#36129;&#29486;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#32852;&#21512;&#24433;&#21709;&#27169;&#24335;&#65292;&#21253;&#25324;&#20114;&#34917;&#30340;&#12289;&#27491;&#20132;&#30340;&#21644;&#30456;&#20851;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#19968;&#32452;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#32452;&#32455;&#25968;&#25454;&#20197;&#25903;&#25345;LLMs&#20248;&#21270;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11537v1 Announce Type: cross  Abstract: Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to sup
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.08644</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#26029;&#39640;&#25928;LLMs&#30340;&#20018;&#32852;Transformer
&lt;/p&gt;
&lt;p&gt;
Tandem Transformers for Inference Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08644
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#20855;&#26377;&#33258;&#22238;&#24402;&#30340;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#35789;&#20803;&#26159;&#25353;&#39034;&#24207;&#29983;&#25104;&#30340;&#12290;&#23613;&#31649;&#26377;&#20123;&#39044;&#27979;&#21644;&#24182;&#34892;&#35299;&#30721;&#25216;&#26415;&#35797;&#22270;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#38480;&#21046;&#65306;&#35201;&#20040;&#20381;&#36182;&#26356;&#31934;&#31616;&#20294;&#20934;&#30830;&#24230;&#36739;&#20302;&#30340;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#65292;&#35201;&#20040;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22522;&#30784;LLM&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#21363;&#20018;&#32852;Transformer&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#26550;&#26500;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;(1)&#19968;&#20010;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;(2)&#19968;&#20010;&#20197;&#22359;&#27169;&#24335;&#36816;&#34892;&#30340;&#22823;&#27169;&#22411;(&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#35789;&#20803;)&#12290;&#36890;&#36807;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#22823;&#24133;&#25552;&#21319;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;PaLM2&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;PaLM2-Bison&#21644;PaLM2-Gecko&#30340;&#20018;&#32852;&#30456;&#36739;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;3.3%&#65292;&#19982;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#30456;&#27604;&#65292;&#25552;&#20379;&#20102;1.16&#20493;&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream p
&lt;/p&gt;</description></item><item><title>SGS-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#36890;&#36947;&#20248;&#21270;&#21644;&#20851;&#38190;&#24103;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#21644;&#31934;&#30830;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2402.03246</link><description>&lt;p&gt;
SGS-SLAM&#65306;&#22522;&#20110;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM
&lt;/p&gt;
&lt;p&gt;
SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03246
&lt;/p&gt;
&lt;p&gt;
SGS-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#36890;&#36947;&#20248;&#21270;&#21644;&#20851;&#38190;&#24103;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#21644;&#31934;&#30830;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#29702;&#35299;&#22312;&#31264;&#23494;&#21516;&#26102;&#23450;&#20301;&#21644;&#24314;&#22270;&#65288;SLAM&#65289;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#20840;&#38754;&#30340;&#22330;&#26223;&#35299;&#26512;&#12290;&#26368;&#36817;&#23558;&#39640;&#26031;&#28857;&#20113;&#38598;&#25104;&#21040;SLAM&#31995;&#32479;&#20013;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26174;&#24335;&#30340;&#19977;&#32500;&#39640;&#26031;&#34920;&#31034;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#28210;&#26579;&#25928;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SGS-SLAM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;&#35270;&#35273;SLAM&#31995;&#32479;&#65292;&#23427;&#19981;&#20165;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#65292;&#36824;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#37325;&#24314;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#24314;&#22270;&#36807;&#31243;&#20013;&#37319;&#29992;&#22810;&#36890;&#36947;&#20248;&#21270;&#65292;&#23558;&#22806;&#35266;&#12289;&#20960;&#20309;&#21644;&#35821;&#20041;&#32422;&#26463;&#19982;&#20851;&#38190;&#24103;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SGS-SLAM&#22312;&#30456;&#26426;&#20301;&#23039;&#20272;&#35745;&#12289;&#22320;&#22270;&#37325;&#24314;&#21644;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21516;&#26102;&#20445;&#25345;&#23454;&#26102;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaus- sian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rende
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#25945;&#25480;&#26234;&#33021;&#20307;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#65292;&#21487;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20854;&#21028;&#26029;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.18040</link><description>&lt;p&gt;
&#21152;&#24378;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#65306;&#22522;&#20110;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25913;&#36827;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#25945;&#25480;&#26234;&#33021;&#20307;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#65292;&#21487;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20854;&#21028;&#26029;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#36890;&#36807;&#23545;&#35805;&#27969;&#27700;&#32447;&#30340;&#29420;&#31435;&#27169;&#22359;&#36827;&#34892;&#35774;&#35745;&#12290;&#20854;&#20013;&#65292;&#31574;&#30053;&#27169;&#22359;&#26159;&#20915;&#23450;&#23545;&#29992;&#25143;&#36755;&#20837;&#22914;&#20309;&#21709;&#24212;&#30340;&#20851;&#38190;&#12290;&#36825;&#20010;&#31574;&#30053;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#21453;&#39304;&#20449;&#21495;&#24418;&#24335;&#30340;&#29615;&#22659;&#20013;&#25509;&#25910;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23545;&#35805;&#31995;&#32479;&#21482;&#25552;&#20379;&#20102;&#31232;&#32570;&#19988;&#31616;&#21333;&#30340;&#22870;&#21169;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#31639;&#27861;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#24555;&#36895;&#21152;&#36895;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#25945;&#25480;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#26469;&#25552;&#39640;&#21028;&#26029;&#20854;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#21644;&#22909;&#22855;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#27979;&#37327;&#29366;&#24577;&#35775;&#38382;&#39057;&#29575;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#35805;&#35821;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#40723;&#21169;&#25506;&#32034;&#12290;&#22312;&#19968;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;MultiWOZ&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline. Among these, the policy module is essential for deciding what to do in response to user input. This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal. The current dialogue systems, however, only provide meagre and simplistic rewards. Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study. Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system. In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances. Experimental results on MultiWOZ, a heterogeneous dataset, sho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.11911</link><description>&lt;p&gt;
&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20197;&#22686;&#24378;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36741;&#21161;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#20294;&#23545;&#20110;LLMs&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#30830;&#23450;LLMs&#30340;&#21709;&#24212;&#26159;&#28304;&#33258;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36824;&#26159;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547;&#30456;&#20114;&#20914;&#31361;&#30340;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#37197;&#23545;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#20102;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#65288;&#22914;GPT-4/3.5&#21644;Llama2&#65289;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#26356;&#20542;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#36825;&#20123;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#23548;&#33268;&#36825;&#31181;&#20559;&#24046;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;i&#65289;LLMs&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36890;&#24120;&#19982;&#38382;&#39064;&#26356;&#30456;&#20284;&#65292;&#22686;&#21152;&#20102;&#20854;&#34987;&#36873;&#25321;&#30340;&#21487;&#33021;&#24615;&#65307;ii&#65289;&#26816;&#32034;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#30340;&#20998;&#21106;&#36807;&#31243;&#25171;&#26029;&#20102;&#20854;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To achieve this, we construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs (GPT-4/3.5 and Llama2) towards generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) the segmentation process used in retrieved contexts disrupts their compl
&lt;/p&gt;</description></item><item><title>AI&#21644;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#22312;&#30740;&#31350;&#21457;&#29616;&#21644;&#24635;&#32467;&#26041;&#38754;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#21253;&#25324;&#33021;&#22815;&#26356;&#24555;&#22320;&#25214;&#21040;&#30456;&#20851;&#25991;&#29486;&#21644;&#29992;&#31616;&#27905;&#35821;&#35328;&#24635;&#32467;&#30740;&#31350;&#25991;&#31456;&#30340;&#35201;&#28857;&#12290;</title><link>https://arxiv.org/abs/2401.06795</link><description>&lt;p&gt;
AI&#21644;&#29983;&#25104;&#24335;AI&#29992;&#20110;&#30740;&#31350;&#21457;&#29616;&#19982;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
AI and Generative AI for Research Discovery and Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06795
&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#22312;&#30740;&#31350;&#21457;&#29616;&#21644;&#24635;&#32467;&#26041;&#38754;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#21253;&#25324;&#33021;&#22815;&#26356;&#24555;&#22320;&#25214;&#21040;&#30456;&#20851;&#25991;&#29486;&#21644;&#29992;&#31616;&#27905;&#35821;&#35328;&#24635;&#32467;&#30740;&#31350;&#25991;&#31456;&#30340;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#21253;&#25324;&#20381;&#36182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22914;ChatGPT&#65292;&#20170;&#24180;&#36805;&#36895;&#23835;&#36215;&#65292;&#20026;&#22686;&#21152;&#24037;&#20316;&#25928;&#29575;&#21644;&#25913;&#21892;&#29983;&#27963;&#21019;&#36896;&#20102;&#38590;&#20197;&#32622;&#20449;&#30340;&#26426;&#20250;&#12290;&#32479;&#35745;&#23398;&#23478;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#24050;&#32463;&#24320;&#22987;&#20197;&#22810;&#31181;&#26041;&#24335;&#20307;&#39564;&#21040;&#36825;&#20123;&#24037;&#20855;&#30340;&#22909;&#22788;&#65292;&#27604;&#22914;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#20197;&#20998;&#26512;&#25968;&#25454;&#25110;&#25311;&#21512;&#32479;&#35745;&#27169;&#22411;&#12290;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#22312;&#30740;&#31350;&#21457;&#29616;&#21644;&#24635;&#32467;&#26041;&#38754;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#20043;&#19968;&#12290;&#27491;&#22312;&#24320;&#21457;&#29420;&#31435;&#24037;&#20855;&#21644;&#25554;&#20214;&#32473;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#27604;2023&#24180;&#20043;&#21069;&#30340;&#25628;&#32034;&#24037;&#20855;&#26356;&#24555;&#22320;&#25214;&#21040;&#30456;&#20851;&#25991;&#29486;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#24050;&#32463;&#21457;&#23637;&#21040;&#21487;&#20197;&#29992;&#31616;&#27905;&#30340;&#35821;&#35328;&#24635;&#32467;&#21644;&#25552;&#21462;&#30740;&#31350;&#25991;&#31456;&#30340;&#35201;&#28857;&#30340;&#31243;&#24230;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;LLMs&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#29992;&#20110;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06795v2 Announce Type: replace-cross  Abstract: AI and generative AI tools, including chatbots like ChatGPT that rely on large language models (LLMs), have burst onto the scene this year, creating incredible opportunities to increase work productivity and improve our lives. Statisticians and data scientists have begun experiencing the benefits from the availability of these tools in numerous ways, such as the generation of programming code from text prompts to analyze data or fit statistical models. One area that these tools can make a substantial impact is in research discovery and summarization. Standalone tools and plugins to chatbots are being developed that allow researchers to more quickly find relevant literature than pre-2023 search tools. Furthermore, generative AI tools have improved to the point where they can summarize and extract the key points from research articles in succinct language. Finally, chatbots based on highly parameterized LLMs can be used to simula
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#26512;&#22270;&#20687;&#19982;&#25991;&#26412;&#20013;&#30340;&#23545;&#35937;&#21644;&#23646;&#24615;&#65292;&#20351;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#25439;&#22833;&#26469;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#27169;&#22411;</title><link>https://arxiv.org/abs/2312.14149</link><description>&lt;p&gt;
TagAlign&#65306;&#21033;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14149
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#26512;&#22270;&#20687;&#19982;&#25991;&#26412;&#20013;&#30340;&#23545;&#35937;&#21644;&#23646;&#24615;&#65292;&#20351;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#25439;&#22833;&#26469;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#22312;&#20110;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#23545;&#40784;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#32780;&#26080;&#38656;&#38500;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20043;&#22806;&#30340;&#20854;&#20182;&#25968;&#25454;&#26684;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#24133;&#22270;&#20687;&#21450;&#20854;&#37197;&#23545;&#30340;&#25991;&#26412;&#65292;&#25105;&#20204;&#35774;&#27861;&#20174;&#25551;&#36848;&#20013;&#35299;&#26512;&#20986;&#23545;&#35937;&#65288;&#20363;&#22914;&#29483;&#65289;&#21644;&#23646;&#24615;&#65288;&#20363;&#22914;&#40657;&#33394;&#65289;&#65292;&#36825;&#20123;&#23545;&#35937;&#21644;&#23646;&#24615;&#26497;&#26377;&#21487;&#33021;&#23384;&#22312;&#20110;&#22270;&#20687;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35299;&#26512;&#31649;&#36947;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20511;&#21161;&#36825;&#20123;&#35299;&#26512;&#20986;&#30340;&#35821;&#20041;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#24120;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#25439;&#22833;&#19982;&#22810;&#26631;&#31614;&#20998;&#31867;&#25439;&#22833;&#30456;&#32467;&#21512;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14149v3 Announce Type: replace-cross  Abstract: The crux of learning vision-language models is to extract semantically aligned information from visual and linguistic data. Existing attempts usually face the problem of coarse alignment, e.g., the vision encoder struggles in localizing an attribute-specified object. In this work, we propose an embarrassingly simple approach to better align image and text features with no need of additional data formats other than image-text pairs. Concretely, given an image and its paired text, we manage to parse objects (\textit{e.g.}, cat) and attributes (\textit{e.g.}, black) from the description, which are highly likely to exist in the image. It is noteworthy that the parsing pipeline is fully automatic and thus enjoys good scalability. With these parsed semantics as supervision signals, we can complement the commonly used image-text contrastive loss with the multi-tag classification loss. Extensive experimental results on a broad suite of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#22788;&#29702;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.12467</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#25509;&#35302;&#32593;&#26684;&#21464;&#25442;&#22120;&#23398;&#20064;&#28789;&#27963;&#36523;&#20307;&#30896;&#25758;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#22788;&#29702;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#22522;&#20110;&#32593;&#26684;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#24050;&#34987;&#25552;&#20986;&#29992;&#26469;&#24314;&#27169;&#22797;&#26434;&#30340;&#39640;&#32500;&#29289;&#29702;&#31995;&#32479;&#12290;&#19982;&#20256;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#23601;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#27714;&#35299;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#30340;&#26159;&#23427;&#20204;&#26159;&#21542;&#26377;&#25928;&#22320;&#24212;&#23545;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#30340;&#25361;&#25112;&#65292;&#21363;&#30636;&#26102;&#30896;&#25758;&#21457;&#29983;&#22312;&#26497;&#30701;&#26102;&#38388;&#20869;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#36523;&#20307;&#31354;&#38388;&#20301;&#32622;&#20043;&#38388;&#65288;&#30001;&#30896;&#25758;&#24341;&#36215;&#30340;&#65289;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;--&#22312;&#26356;&#39640;&#32423;&#21035;&#32593;&#26684;&#20013;&#30340;&#20004;&#20010;&#25509;&#36817;&#20301;&#32622;&#23545;&#24212;&#20110;&#36523;&#20307;&#20013;&#30340;&#20004;&#20010;&#36828;&#36317;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12467v2 Announce Type: replace-cross  Abstract: Recently, many mesh-based graph neural network (GNN) models have been proposed for modeling complex high-dimensional physical systems. Remarkable achievements have been made in significantly reducing the solving time compared to traditional numerical solvers. These methods are typically designed to i) reduce the computational cost in solving physical dynamics and/or ii) propose techniques to enhance the solution accuracy in fluid and rigid body dynamics. However, it remains under-explored whether they are effective in addressing the challenges of flexible body dynamics, where instantaneous collisions occur within a very short timeframe. In this paper, we present Hierarchical Contact Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn long-range dependencies (occurred by collisions) among spatially distant positions of a body -- two close positions in a higher-level mesh corresponds to two distant posi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#65292;&#36991;&#20813;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#32780;&#31215;&#32047;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2312.08533</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#24341;&#23548;&#30340;&#36712;&#36857;&#25193;&#25955;&#23454;&#29616;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
World Models via Policy-Guided Trajectory Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08533
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#65292;&#36991;&#20813;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#32780;&#31215;&#32047;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#26159;&#24320;&#21457;&#26234;&#33021;agent&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36890;&#36807;&#39044;&#27979;&#19968;&#31995;&#21015;&#34892;&#21160;&#30340;&#32467;&#26524;&#65292;&#19990;&#30028;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#22312;&#8220;&#24819;&#35937;&#20013;&#8221;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#20248;&#21270;&#31574;&#30053;&#65292;&#21363;&#36890;&#36807;&#22312;&#32447;&#31574;&#30053;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#23454;&#29616;&#12290;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#26159;&#33258;&#22238;&#24402;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#29366;&#24577;&#30340;&#21516;&#26102;&#20174;&#31574;&#30053;&#20013;&#37319;&#26679;&#19979;&#19968;&#20010;&#34892;&#21160;&#12290;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#30340;&#22686;&#38271;&#65292;&#39044;&#27979;&#35823;&#24046;&#24517;&#28982;&#20250;&#32047;&#31215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#19981;&#26159;&#33258;&#22238;&#24402;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#21033;&#29992;&#20102;&#38500;&#20102;&#31574;&#30053;&#30340;&#21160;&#20316;&#20998;&#24067;&#26799;&#24230;&#20043;&#22806;&#30340;&#19968;&#20010;&#21435;&#22122;&#27169;&#22411;&#65292;&#23558;&#26368;&#21021;&#38543;&#26426;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#36712;&#36857;&#25193;&#25955;&#25104;&#19968;&#20010;&#22312;&#32447;&#21512;&#25104;&#36712;&#36857;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;PolyGRAD&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08533v3 Announce Type: replace-cross  Abstract: World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in "in imagination". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD,
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21322;&#32467;&#26500;&#21270;&#32593;&#32476;&#25991;&#31456;&#20013;&#23454;&#29616;&#39640;&#36890;&#37327;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#23545;LLMs&#30340;&#24212;&#29992;&#65292;&#32467;&#21512;&#22806;&#37096;&#35821;&#26009;&#24211;&#21644;&#19990;&#30028;&#30693;&#35782;&#65292;&#35774;&#35745;&#38024;&#23545;&#24615;&#30340;&#20108;&#20803;&#20998;&#31867;&#20915;&#31574;&#65292;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.08274</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21322;&#32467;&#26500;&#21270;&#32593;&#32476;&#25991;&#31456;&#39640;&#36890;&#37327;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08274
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21322;&#32467;&#26500;&#21270;&#32593;&#32476;&#25991;&#31456;&#20013;&#23454;&#29616;&#39640;&#36890;&#37327;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#23545;LLMs&#30340;&#24212;&#29992;&#65292;&#32467;&#21512;&#22806;&#37096;&#35821;&#26009;&#24211;&#21644;&#19990;&#30028;&#30693;&#35782;&#65292;&#35774;&#35745;&#38024;&#23545;&#24615;&#30340;&#20108;&#20803;&#20998;&#31867;&#20915;&#31574;&#65292;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#21644;&#29983;&#29289;&#21307;&#23398;&#19990;&#30028;&#30693;&#35782;&#20197;&#21487;&#25193;&#23637;&#21644;&#26377;&#20449;&#26381;&#21147;&#30340;&#26041;&#24335;&#36827;&#34892;&#39640;&#36890;&#37327;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#25552;&#21462;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#21046;&#23450;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20108;&#20803;&#20998;&#31867;&#65292;LLMs&#22522;&#20110;&#22806;&#37096;&#35821;&#26009;&#24211;&#21644;&#20854;&#19990;&#30028;&#30693;&#35782;&#20570;&#20986;&#20915;&#31574;&#65292;&#32473;&#20986;&#20107;&#23454;&#39564;&#35777;&#30340;&#21028;&#26029;&#29702;&#30001;&#12290;&#27492;&#26041;&#27861;&#19987;&#20026;&#21322;&#32467;&#26500;&#21270;&#32593;&#32476;&#25991;&#31456;&#32780;&#35774;&#35745;&#65292;&#22312;&#20854;&#20013;&#23558;&#20027;&#26631;&#39064;&#25351;&#23450;&#20026;&#23614;&#23454;&#20307;&#24182;&#26126;&#30830;&#32435;&#20837;&#19978;&#19979;&#25991;&#65292;&#28508;&#22312;&#22836;&#23454;&#20307;&#26681;&#25454;&#29983;&#29289;&#21307;&#23398;&#35789;&#34920;&#36827;&#34892;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#23558;&#20887;&#38271;&#20869;&#23481;&#20998;&#21106;&#20026;&#25991;&#26412;&#22359;&#65292;&#23884;&#20837;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08274v4 Announce Type: replace-cross  Abstract: Objective: To develop a high-throughput biomedical relation extraction system that takes advantage of the large language models'(LLMs) reading comprehension ability and biomedical world knowledge in a scalable and evidential manner. Methods: We formulate the relation extraction task as binary classifications for large language models. Specifically, LLMs make the decision based on the external corpus and its world knowledge, giving the reason for the judgment for factual verification. This method is tailored for semi-structured web articles, wherein we designate the main title as the tail entity and explicitly incorporate it into the context, and the potential head entities are matched based on a biomedical thesaurus. Moreover, lengthy contents are sliced into text chunks, embedded, and retrieved with additional embedding models. Results: Using an open-source LLM, we extracted 248659 relation triplets of three distinct relation 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#20154;&#31867;&#21644;&#38750;&#20154;&#31867;&#30340;&#27010;&#24565;&#65292;&#20294;&#24182;&#19981;&#26159;&#29992;&#21333;&#20010;&#21333;&#20803;&#26469;&#34920;&#31034;&#36825;&#20123;&#27010;&#24565;</title><link>https://arxiv.org/abs/2312.05337</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#27010;&#24565;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Nets and the Representation of Human Concepts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05337
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#20154;&#31867;&#21644;&#38750;&#20154;&#31867;&#30340;&#27010;&#24565;&#65292;&#20294;&#24182;&#19981;&#26159;&#29992;&#21333;&#20010;&#21333;&#20803;&#26469;&#34920;&#31034;&#36825;&#20123;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#21040;&#24213;&#23398;&#20064;&#21040;&#20102;&#20160;&#20040;&#21602;&#65311;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31038;&#21306;&#35748;&#20026;&#65292;&#20026;&#20102;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24517;&#39035;&#21457;&#23637;&#25277;&#35937;&#30340;&#20154;&#31867;&#27010;&#24565;&#12290;&#19968;&#20123;&#20154;&#29978;&#33267;&#35748;&#20026;&#36825;&#20123;&#27010;&#24565;&#23384;&#20648;&#22312;&#32593;&#32476;&#30340;&#21333;&#20010;&#21333;&#20803;&#20013;&#12290;&#26681;&#25454;&#24403;&#21069;&#30340;&#30740;&#31350;&#65292;&#25105;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#25903;&#25745;&#36825;&#19968;&#35828;&#27861;&#30340;&#20551;&#35774;&#12290;&#25105;&#24471;&#20986;&#32467;&#35770;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#30830;&#33021;&#22815;&#25191;&#34892;&#22797;&#26434;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#20026;&#27492;&#23427;&#20204;&#21487;&#33021;&#23398;&#20064;&#20102;&#20154;&#31867;&#21644;&#38750;&#20154;&#31867;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#35777;&#25454;&#34920;&#26126;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24182;&#27809;&#26377;&#29992;&#21333;&#20010;&#21333;&#20803;&#26469;&#34920;&#31034;&#36825;&#20123;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05337v2 Announce Type: replace-cross  Abstract: What do artificial neural networks (ANNs) learn? The machine learning (ML) community shares the narrative that ANNs must develop abstract human concepts to perform complex tasks. Some go even further and believe that these concepts are stored in individual units of the network. Based on current research, I systematically investigate the assumptions underlying this narrative. I conclude that ANNs are indeed capable of performing complex prediction tasks, and that they may learn human and non-human concepts to do so. However, evidence indicates that ANNs do not represent these concepts in individual units.
&lt;/p&gt;</description></item><item><title>DreamComposer&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#20837;&#22810;&#35270;&#35282;&#26465;&#20214;&#22686;&#24378;&#29616;&#26377;&#35270;&#35282;&#24863;&#30693;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#21487;&#25511;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.03611</link><description>&lt;p&gt;
DreamComposer: &#36890;&#36807;&#22810;&#35270;&#35282;&#26465;&#20214;&#23454;&#29616;&#21487;&#25511;&#30340;3D&#23545;&#35937;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DreamComposer: Controllable 3D Object Generation via Multi-View Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03611
&lt;/p&gt;
&lt;p&gt;
DreamComposer&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#20837;&#22810;&#35270;&#35282;&#26465;&#20214;&#22686;&#24378;&#29616;&#26377;&#35270;&#35282;&#24863;&#30693;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#21487;&#25511;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;2D&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#33021;&#22815;&#20174;&#21333;&#20010;in-the-wild&#22270;&#20687;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26469;&#33258;&#22810;&#20010;&#35270;&#35282;&#30340;&#20449;&#24687;&#65292;&#36825;&#20123;&#30740;&#31350;&#22312;&#29983;&#25104;&#21487;&#25511;&#30340;&#26032;&#35270;&#22270;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DreamComposer&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#27880;&#20837;&#22810;&#35270;&#35282;&#26465;&#20214;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#35270;&#35282;&#24863;&#30693;&#25193;&#25955;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DreamComposer&#39318;&#20808;&#20351;&#29992;&#35270;&#35282;&#24863;&#30693;&#30340;3D&#25552;&#21319;&#27169;&#22359;&#20174;&#22810;&#20010;&#35270;&#35282;&#33719;&#21462;&#23545;&#35937;&#30340;3D&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#22810;&#35270;&#35282;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#20174;3D&#34920;&#31034;&#20013;&#28210;&#26579;&#30446;&#26631;&#35270;&#22270;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#20174;&#22810;&#35270;&#35282;&#36755;&#20837;&#20013;&#25552;&#21462;&#30340;&#30446;&#26631;&#35270;&#22270;&#29305;&#24449;&#34987;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DreamComposer&#19982;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#20860;&#23481;&#65292;&#29992;&#20110;&#38646;-shot&#26032;&#35270;&#22270;sy
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03611v2 Announce Type: replace-cross  Abstract: Utilizing pre-trained 2D large-scale generative models, recent works are capable of generating high-quality novel views from a single in-the-wild image. However, due to the lack of information from multiple views, these works encounter difficulties in generating controllable novel views. In this paper, we present DreamComposer, a flexible and scalable framework that can enhance existing view-aware diffusion models by injecting multi-view conditions. Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain 3D representations of an object from multiple views. Then, it renders the latent features of the target view from 3D representations with the multi-view feature fusion module. Finally the target view features extracted from multi-view inputs are injected into a pre-trained diffusion model. Experiments show that DreamComposer is compatible with state-of-the-art diffusion models for zero-shot novel view sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AV2AV&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#38899;&#35270;&#39057;&#35821;&#38899;&#21040;&#38899;&#35270;&#39057;&#35821;&#38899;&#30340;&#36716;&#25442;&#65292;&#33021;&#22815;&#22312;&#34394;&#25311;&#20250;&#35758;&#20013;&#36827;&#34892;&#30495;&#23454;&#23545;&#35805;&#65292;&#24182;&#25913;&#21892;&#21475;&#35793;&#35821;&#35328;&#32763;&#35793;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.02512</link><description>&lt;p&gt;
AV2AV&#65306;&#30452;&#25509;&#38899;&#35270;&#39057;&#35821;&#38899;&#21040;&#38899;&#35270;&#39057;&#35821;&#38899;&#36716;&#25442;&#65292;&#20855;&#26377;&#32479;&#19968;&#30340;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AV2AV&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#38899;&#35270;&#39057;&#35821;&#38899;&#21040;&#38899;&#35270;&#39057;&#35821;&#38899;&#30340;&#36716;&#25442;&#65292;&#33021;&#22815;&#22312;&#34394;&#25311;&#20250;&#35758;&#20013;&#36827;&#34892;&#30495;&#23454;&#23545;&#35805;&#65292;&#24182;&#25913;&#21892;&#21475;&#35793;&#35821;&#35328;&#32763;&#35793;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#38899;&#35270;&#39057;&#35821;&#38899;&#21040;&#38899;&#35270;&#39057;&#35821;&#38899;&#36716;&#25442;&#65288;AV2AV&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#31995;&#32479;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#26159;&#22810;&#27169;&#24577;&#30340;&#65288;&#21363;&#38899;&#39057;&#21644;&#35270;&#35273;&#35821;&#38899;&#65289;&#12290;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;AV2AV&#65292;&#21487;&#20197;&#24102;&#26469;&#20004;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;1&#65289;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#33258;&#24049;&#30340;&#20027;&#35201;&#35821;&#35328;&#22312;&#34394;&#25311;&#20250;&#35758;&#20013;&#19982;&#20840;&#29699;&#20010;&#20154;&#36827;&#34892;&#31867;&#20284;&#30495;&#23454;&#30340;&#23545;&#35805;&#12290;&#19982;&#20165;&#22312;&#38899;&#39057;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65288;A2A&#65289;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;AV2AV&#30452;&#25509;&#22312;&#38899;&#35270;&#39057;&#35821;&#38899;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#31181;&#33021;&#21147;&#36890;&#36807;&#21576;&#29616;&#19982;&#24050;&#32763;&#35793;&#35821;&#38899;&#21516;&#27493;&#30340;&#22068;&#21767;&#36816;&#21160;&#22686;&#24378;&#20102;&#23545;&#35805;&#20307;&#39564;&#12290;2&#65289;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#21475;&#35793;&#35821;&#35328;&#32763;&#35793;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#38899;&#35270;&#39057;&#35821;&#38899;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#31995;&#32479;&#21487;&#20197;&#22312;&#23384;&#22312;&#22768;&#23398;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#32763;&#35793;&#21475;&#22836;&#35821;&#35328;&#65292;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02512v2 Announce Type: replace-cross  Abstract: This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework, where the input and output of the system are multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key advantages can be brought: 1) We can perform real-like conversations with individuals worldwide in a virtual meeting by utilizing our own primary languages. In contrast to Speech-to-Speech Translation (A2A), which solely translates between audio modalities, the proposed AV2AV directly translates between audio-visual speech. This capability enhances the dialogue experience by presenting synchronized lip movements along with the translated speech. 2) We can improve the robustness of the spoken language translation system. By employing the complementary information of audio-visual speech, the system can effectively translate spoken language even in the presence of acoustic noise, showcasing robust perfor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEEL&#30340;&#26032;&#22411;&#12289;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#37051;&#25509;&#30697;&#38453;&#22823;&#23567;&#21644;&#35789;&#27719;&#37327;&#65292;&#21516;&#26102;&#36890;&#36807;&#33410;&#28857;&#20301;&#32622;&#32534;&#30721;&#23454;&#29616;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#38024;&#23545;&#23646;&#24615;&#22270;&#35774;&#35745;&#20102;&#26032;&#30340;&#25193;&#23637;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.02230</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Simple and Scalable Representation for Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEEL&#30340;&#26032;&#22411;&#12289;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#37051;&#25509;&#30697;&#38453;&#22823;&#23567;&#21644;&#35789;&#27719;&#37327;&#65292;&#21516;&#26102;&#36890;&#36807;&#33410;&#28857;&#20301;&#32622;&#32534;&#30721;&#23454;&#29616;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#38024;&#23545;&#23646;&#24615;&#22270;&#35774;&#35745;&#20102;&#26032;&#30340;&#25193;&#23637;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#29983;&#25104;&#20135;&#29983;&#20102;&#27987;&#21402;&#20852;&#36259;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#20851;&#38190;&#24212;&#29992;&#20215;&#20540;&#30340;&#22522;&#26412;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#22914;&#20998;&#23376;&#35774;&#35745;&#21644;&#31038;&#21306;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#29983;&#25104;&#22823;&#35268;&#27169;&#22270;&#26102;&#36935;&#21040;&#20102;&#37325;&#22823;&#38480;&#21046;&#12290;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#38656;&#35201;&#36755;&#20986;&#38543;&#30528;&#33410;&#28857;&#25968;&#37327;&#21576;&#20108;&#27425;&#22686;&#38271;&#30340;&#23436;&#25972;&#37051;&#25509;&#30697;&#38453;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#21517;&#20026;&#38388;&#38553;&#32534;&#30721;&#36793;&#21015;&#34920;&#65288;GEEL&#65289;&#65292;&#20854;&#34920;&#31034;&#22823;&#23567;&#36739;&#23567;&#19988;&#19982;&#36793;&#25968;&#37327;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;GEEL&#36890;&#36807;&#32467;&#21512;&#38388;&#38553;&#32534;&#30721;&#21644;&#24102;&#23485;&#38480;&#21046;&#26041;&#26696;&#26174;&#33879;&#20943;&#23569;&#20102;&#35789;&#27719;&#37327;&#12290;&#36890;&#36807;&#21152;&#20837;&#33410;&#28857;&#20301;&#32622;&#32534;&#30721;&#65292;GEEL&#21487;&#20197;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;GEEL&#25193;&#23637;&#21040;&#22788;&#29702;&#23646;&#24615;&#22270;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02230v2 Announce Type: replace-cross  Abstract: Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale graphs. This is due to their requirement to output the full adjacency matrices whose size grows quadratically with the number of nodes. In response to this challenge, we introduce a new, simple, and scalable graph representation named gap encoded edge list (GEEL) that has a small representation size that aligns with the number of edges. In addition, GEEL significantly reduces the vocabulary size by incorporating the gap encoding and bandwidth restriction schemes. GEEL can be autoregressively generated with the incorporation of node positional encoding, and we further extend GEEL to deal with attributed graphs by designing a new grammar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.01201</link><description>&lt;p&gt;
PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PAC Privacy Preserving Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01201
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#27491;&#22312;&#24341;&#36215;&#30740;&#31350;&#20154;&#21592;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#65292;&#26377;&#21487;&#33021;&#29983;&#25104;&#26082;&#20855;&#26377;&#39640;&#38544;&#31169;&#24615;&#21448;&#20855;&#26377;&#33391;&#22909;&#35270;&#35273;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#30830;&#20445;&#22312;&#31169;&#26377;&#21270;&#29305;&#23450;&#25968;&#25454;&#23646;&#24615;&#26102;&#30340;&#24378;&#22823;&#20445;&#25252;&#65292;&#24403;&#21069;&#27169;&#22411;&#22312;&#36825;&#20123;&#26041;&#38754;&#32463;&#24120;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21407;&#29702;&#24182;&#30830;&#20445;&#8220;&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#65288;PAC&#65289;&#8221;&#38544;&#31169;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;Langevin&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#27169;&#22411;&#38544;&#31169;&#24615;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#36825;&#20010;&#26032;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;&#30697;&#38453;&#35745;&#31639;&#25903;&#25345;PAC&#30028;&#38480;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#38544;&#31169;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01201v2 Announce Type: replace-cross  Abstract: Data privacy protection is garnering increased attention among researchers. Diffusion models (DMs), particularly with strict differential privacy, can potentially produce images with both high privacy and visual quality. However, challenges arise such as in ensuring robust protection in privatizing specific data attributes, areas where current models often fall short. To address these challenges, we introduce the PAC Privacy Preserving Diffusion Model, a model leverages diffusion principles and ensure Probably Approximately Correct (PAC) privacy. We enhance privacy protection by integrating a private classifier guidance into the Langevin Sampling Process. Additionally, recognizing the gap in measuring the privacy of models, we have developed a novel metric to gauge privacy levels. Our model, assessed with this new metric and supported by Gaussian matrix computations for the PAC bound, has shown superior performance in privacy p
&lt;/p&gt;</description></item><item><title>ViT-Lens-2&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;ViT&#24863;&#30693;&#26032;&#39062;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#23545;&#20934;&#21040;&#39044;&#23450;&#20041;&#31354;&#38388;&#65292;&#20026;&#39640;&#25928;&#30340;&#20840;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2311.16081</link><description>&lt;p&gt;
ViT-Lens: &#36808;&#21521;&#20840;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
ViT-Lens: Towards Omni-modal Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16081
&lt;/p&gt;
&lt;p&gt;
ViT-Lens-2&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;ViT&#24863;&#30693;&#26032;&#39062;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#23545;&#20934;&#21040;&#39044;&#23450;&#20041;&#31354;&#38388;&#65292;&#20026;&#39640;&#25928;&#30340;&#20840;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#25512;&#36827;AI&#26234;&#33021;&#20307;&#65292;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26174;&#33879;&#25913;&#21892;&#25512;&#29702;&#21644;&#25351;&#20196;&#25191;&#34892;&#65292;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20851;&#27880;&#24573;&#35270;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#24863;&#30693;&#22810;&#26679;&#30340;&#27169;&#24577;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#25104;&#26412;&#39640;&#26114;&#65292;&#29978;&#33267;&#38590;&#20197;&#20026;&#31232;&#26377;&#27169;&#24577;&#37325;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ViT-Lens-2&#65292;&#36890;&#36807;&#24863;&#30693;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#25552;&#21462;&#26032;&#39062;&#27169;&#24577;&#24182;&#23558;&#20854;&#23545;&#20934;&#21040;&#39044;&#23450;&#20041;&#31354;&#38388;&#65292;&#20197;&#20419;&#36827;&#39640;&#25928;&#30340;&#20840;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16081v2 Announce Type: replace-cross  Abstract: Aiming to advance AI agents, large foundation models significantly improve reasoning and instruction execution, yet the current focus on vision and language neglects the potential of perceiving diverse modalities in open-world environments. However, the success of data-driven vision and language models is costly or even infeasible to be reproduced for rare modalities. In this paper, we present ViT-Lens-2 that facilitates efficient omni-modal representation learning by perceiving novel modalities with a pretrained ViT and aligning them to a pre-defined space. Specifically, the modality-specific lens is tuned to project any-modal signals to an intermediate embedding space, which are then processed by a strong ViT with pre-trained visual knowledge. The encoded representations are optimized toward aligning with the modal-independent space, pre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified solution for re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.15964</link><description>&lt;p&gt;
&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Pre-training for Localized Instruction Generation of Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35270;&#39057;&#23637;&#31034;&#20102;&#35832;&#22914;&#39135;&#35889;&#20934;&#22791;&#31561;&#20219;&#21153;&#30340;&#36880;&#27493;&#28436;&#31034;&#12290;&#29702;&#35299;&#27492;&#31867;&#35270;&#39057;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;&#27493;&#39588;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#24182;&#29983;&#25104;&#25991;&#23383;&#35828;&#26126;&#12290;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#24182;&#32534;&#20889;&#35828;&#26126;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#38480;&#21046;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#24182;&#38459;&#30861;&#20102;&#26377;&#25928;&#23398;&#20064;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#30340;&#35270;&#39057;-&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#36716;&#24405;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#65292;&#19982;&#20154;&#31867;&#27880;&#37322;&#21592;&#32534;&#20889;&#30340;&#35828;&#26126;&#30456;&#27604;&#23384;&#22312;&#39118;&#26684;&#21464;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;Sieve-&amp;-Swap&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#21644;&#20351;&#29992;&#25991;&#26412;&#39135;&#35889;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#33258;&#21160;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#20197;&#22686;&#24378;&#25991;&#23383;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&amp;-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
&lt;/p&gt;</description></item><item><title>&#23398;&#26415;&#30028;&#30740;&#31350;&#30340;AI&#23433;&#20840;&#23041;&#32961;&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21453;&#26144;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#65292;&#23384;&#22312;&#37325;&#35201;&#19981;&#21305;&#37197;&#65292;&#38656;&#35201;&#26356;&#21152;&#23454;&#29992;&#30340;&#23041;&#32961;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2311.09994</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#20013;&#26356;&#23454;&#29992;&#30340;&#23041;&#32961;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards more Practical Threat Models in Artificial Intelligence Security
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09994
&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#30028;&#30740;&#31350;&#30340;AI&#23433;&#20840;&#23041;&#32961;&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21453;&#26144;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#65292;&#23384;&#22312;&#37325;&#35201;&#19981;&#21305;&#37197;&#65292;&#38656;&#35201;&#26356;&#21152;&#23454;&#29992;&#30340;&#23041;&#32961;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#65306;&#23398;&#26415;&#30028;&#30740;&#31350;&#30340;&#23041;&#32961;&#24182;&#19981;&#24635;&#26159;&#21453;&#26144;&#20986;AI&#30340;&#23454;&#38469;&#20351;&#29992;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#31532;&#19968;&#27493;&#26469;&#25551;&#36848;&#36825;&#31181;&#24046;&#36317;&#30340;&#20840;&#37096;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;AI&#23433;&#20840;&#30740;&#31350;&#20013;&#20845;&#31181;&#26368;&#24120;&#30740;&#31350;&#30340;&#25915;&#20987;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;271&#21517;&#24037;&#19994;&#20174;&#19994;&#32773;&#30340;&#35843;&#26597;&#23558;&#20854;&#19982;&#23454;&#36341;&#20013;&#30340;AI&#20351;&#29992;&#36827;&#34892;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09994v2 Announce Type: replace-cross  Abstract: Recent works have identified a gap between research and practice in artificial intelligence security: threats studied in academia do not always reflect the practical use and security risks of AI. For example, while models are often studied in isolation, they form part of larger ML pipelines in practice. Recent works also brought forward that adversarial manipulations introduced by academic attacks are impractical. We take a first step towards describing the full extent of this disparity. To this end, we revisit the threat models of the six most studied attacks in AI security research and match them to AI usage in practice via a survey with 271 industrial practitioners. On the one hand, we find that all existing threat models are indeed applicable. On the other hand, there are significant mismatches: research is often too generous with the attacker, assuming access to information not frequently available in real-world settings. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BrainRGIN&#24314;&#27169;&#26550;&#26500;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26234;&#21147;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#24182;&#32467;&#21512;&#20102;&#32858;&#31867;&#23884;&#20837;&#12289;&#22270;&#21516;&#26500;&#32593;&#32476;&#12289;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.03520</link><description>&lt;p&gt;
&#22823;&#33041;&#32593;&#32476;&#19982;&#26234;&#21147;&#65306;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BrainRGIN&#24314;&#27169;&#26550;&#26500;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26234;&#21147;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#24182;&#32467;&#21512;&#20102;&#32858;&#31867;&#23884;&#20837;&#12289;&#22270;&#21516;&#26500;&#32593;&#32476;&#12289;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;rsfMRI&#65289;&#26159;&#19968;&#31181;&#30740;&#31350;&#22823;&#33041;&#21151;&#33021;&#21644;&#35748;&#30693;&#36807;&#31243;&#20851;&#31995;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25429;&#33719;&#22823;&#33041;&#30340;&#21151;&#33021;&#32452;&#32455;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#25110;&#21050;&#28608;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;BrainRGIN&#30340;&#26032;&#39062;&#24314;&#27169;&#26550;&#26500;&#65292;&#21033;&#29992;rsfMRI&#25512;&#23548;&#30340;&#38745;&#24577;&#21151;&#33021;&#32593;&#32476;&#36830;&#25509;&#30697;&#38453;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26234;&#21147;&#65288;&#27969;&#20307;&#12289;&#26230;&#20307;&#21644;&#24635;&#20307;&#26234;&#21147;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#23558;&#32858;&#31867;&#23884;&#20837;&#21644;&#22270;&#21516;&#26500;&#32593;&#32476;&#32435;&#20837;&#21040;&#22270;&#21367;&#31215;&#23618;&#20013;&#65292;&#20197;&#21453;&#26144;&#22823;&#33041;&#23376;&#32593;&#32476;&#32452;&#32455;&#30340;&#24615;&#36136;&#21644;&#39640;&#25928;&#32593;&#32476;&#34920;&#36798;&#65292;&#20877;&#36741;&#20197;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03520v2 Announce Type: replace-cross  Abstract: Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful tool for investigating the relationship between brain function and cognitive processes as it allows for the functional organization of the brain to be captured without relying on a specific task or stimuli. In this paper, we present a novel modeling architecture called BrainRGIN for predicting intelligence (fluid, crystallized, and total intelligence) using graph neural networks on rsfMRI derived static functional network connectivity matrices. Extending from the existing graph convolution networks, our approach incorporates a clustering-based embedding and graph isomorphism network in the graph convolutional layer to reflect the nature of the brain sub-network organization and efficient network expression, in combination with TopK pooling and attention-based readout functions. We evaluated our proposed architecture on a large dataset, specifically the A
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.02129</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Pitfalls of Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#24320;&#21457;&#32534;&#36753;LLMs&#20869;&#22312;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#19968;&#20010;&#38452;&#20113;&#24748;&#22312;&#22836;&#39030;&#19978; - &#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#35302;&#21457;&#34676;&#34678;&#25928;&#24212;&#65311;&#22240;&#20026;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#24341;&#20837;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#30340;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#19982;LLMs&#30693;&#35782;&#32534;&#36753;&#30456;&#20851;&#30340;&#28508;&#22312;&#38519;&#38449;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#30693;&#35782;&#20914;&#31361;&#65306;&#32534;&#36753;&#36923;&#36753;&#20914;&#31361;&#30340;&#20107;&#23454;&#32452;&#21487;&#33021;&#20250;&#25918;&#22823;LLMs&#22266;&#26377;&#30340;&#19981;&#19968;&#33268;&#24615; - &#36825;&#26159;&#20197;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#65288;2&#65289;&#30693;&#35782;&#25197;&#26354;&#65306;&#20026;&#20102;&#32534;&#36753;&#20107;&#23454;&#30693;&#35782;&#32780;&#26356;&#25913;&#21442;&#25968;&#21487;&#33021;&#20250;&#19981;&#21487;&#36870;&#22320;&#25197;&#26354;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2309.13339</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340; remarkable generalizability&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#32463;&#24120;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#24314;&#31435;&#36830;&#36143;&#30340;&#24605;&#32500;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#26410;&#21463;&#36923;&#36753;&#21407;&#21017;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#31995;&#32479;&#22320;&#39564;&#35777;&#21644;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#25968;&#25454;&#38598;DiVa-360&#25552;&#20379;&#20102;&#27785;&#28024;&#24335;360&#24230;&#21160;&#24577;&#35270;&#35273;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#22823;&#35268;&#27169;&#22810;&#35270;&#35282;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#20026;&#21160;&#24577;&#31070;&#32463;&#22330;&#26041;&#27861;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2307.16897</link><description>&lt;p&gt;
DiVa-360&#65306;&#27785;&#28024;&#24335;&#31070;&#32463;&#22330;&#21160;&#24577;&#35270;&#35273;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16897
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25968;&#25454;&#38598;DiVa-360&#25552;&#20379;&#20102;&#27785;&#28024;&#24335;360&#24230;&#21160;&#24577;&#35270;&#35273;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#22823;&#35268;&#27169;&#22810;&#35270;&#35282;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#20026;&#21160;&#24577;&#31070;&#32463;&#22330;&#26041;&#27861;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22330;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#24471;&#21160;&#24577;3D&#22330;&#26223;&#30340;&#24418;&#29366;&#21644;&#22806;&#35266;&#33021;&#22815;&#20197;&#39640;&#20445;&#30495;&#24230;&#34987;&#25429;&#25417;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31639;&#27861;&#25361;&#25112;&#21644;&#32570;&#20047;&#22823;&#35268;&#27169;&#22810;&#35270;&#35282;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#20854;&#24615;&#33021;&#20173;&#33853;&#21518;&#20110;&#20256;&#32479;&#30340;2D&#35270;&#39057;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;DiVa-360&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#21516;&#27493;&#30340;&#39640;&#20998;&#36776;&#29575;&#38271;&#26102;&#38388;&#22810;&#35270;&#35282;&#35270;&#39057;&#24207;&#21015;&#30340;&#23454;&#26102;360&#24230;&#21160;&#24577;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#25429;&#25417;&#20102;&#20351;&#29992;53&#21488;&#25668;&#20687;&#26426;&#30340;&#23450;&#21046;&#20302;&#25104;&#26412;&#31995;&#32479;&#35760;&#24405;&#30340;&#21488;&#24335;&#22330;&#26223;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;21&#20010;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#24207;&#21015;&#65292;&#25353;&#19981;&#21516;&#36816;&#21160;&#31867;&#22411;&#20998;&#31867;&#65292;25&#20010;&#22797;&#26434;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#24207;&#21015;&#21644;8&#20010;&#38271;&#26102;&#38388;&#24207;&#21015;&#65292;&#20849;&#35745;1740&#19975;&#24103;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#21069;&#26223;-&#32972;&#26223;&#20998;&#21106;&#25513;&#27169;&#12289;&#21516;&#27493;&#38899;&#39057;&#21644;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#22312;DiVa&#19978;&#23545;&#26368;&#20808;&#36827;&#30340;&#21160;&#24577;&#31070;&#32463;&#22330;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.16897v2 Announce Type: replace-cross  Abstract: Advances in neural fields are enabling high-fidelity capture of the shape and appearance of dynamic 3D scenes. However, their capabilities lag behind those offered by conventional representations such as 2D videos because of algorithmic challenges and the lack of large-scale multi-view real-world datasets. We address the dataset limitation with DiVa-360, a real-world 360 dynamic visual dataset that contains synchronized high-resolution and long-duration multi-view video sequences of table-scale scenes captured using a customized low-cost system with 53 cameras. It contains 21 object-centric sequences categorized by different motion types, 25 intricate hand-object interaction sequences, and 8 long-duration sequences for a total of 17.4 M image frames. In addition, we provide foreground-background segmentation masks, synchronized audio, and text descriptions. We benchmark the state-of-the-art dynamic neural field methods on DiVa-
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#20542;&#21521;&#20110;&#39318;&#20808;&#29983;&#25104;&#36718;&#24275;&#65292;&#28982;&#21518;&#36880;&#28176;&#21152;&#20837;&#32454;&#33410;&#65292;&#26089;&#26399;&#25200;&#21160;&#23545;&#22270;&#20687;&#20869;&#23481;&#24433;&#21709;&#36739;&#22823;</title><link>https://arxiv.org/abs/2303.02490</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#31867;&#20284;&#30011;&#23478;&#30340;&#22270;&#20687;&#65306;&#36718;&#24275;&#20248;&#20808;&#65292;&#32454;&#33410;&#20854;&#27425;&#30340;&#20998;&#26512;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models Generate Images Like Painters: an Analytical Theory of Outline First, Details Later
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.02490
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#20542;&#21521;&#20110;&#39318;&#20808;&#29983;&#25104;&#36718;&#24275;&#65292;&#28982;&#21518;&#36880;&#28176;&#21152;&#20837;&#32454;&#33410;&#65292;&#26089;&#26399;&#25200;&#21160;&#23545;&#22270;&#20687;&#20869;&#23481;&#24433;&#21709;&#36739;&#22823;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2303.02490v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#36328;&#24230; &#25688;&#35201;: &#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#23558;&#32431;&#22122;&#22768;&#36716;&#25442;&#20026;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#65311;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;&#21253;&#25324;&#31867;&#20284;&#31283;&#23450;&#25193;&#25955;&#30340;&#26465;&#20214;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65289;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#28508;&#22312;&#30340;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#65306;(i)&#20010;&#20307;&#36712;&#36857;&#20542;&#21521;&#20110;&#26159;&#20302;&#32500;&#19988;&#31867;&#20284;&#20110;2D&#30340;&#8220;&#26059;&#36716;&#8221;&#65307;(ii)&#39640;&#26041;&#24046;&#30340;&#22330;&#26223;&#29305;&#24449;&#22914;&#24067;&#23616;&#20542;&#21521;&#20110;&#36739;&#26089;&#20986;&#29616;&#65292;&#32780;&#20302;&#26041;&#24046;&#30340;&#32454;&#33410;&#20542;&#21521;&#20110;&#36739;&#26202;&#20986;&#29616;&#65307;(iii)&#26089;&#26399;&#25200;&#21160;&#24448;&#24448;&#20250;&#27604;&#21518;&#26399;&#25200;&#21160;&#23545;&#22270;&#20687;&#20869;&#23481;&#20135;&#29983;&#26356;&#22823;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20123;&#29616;&#35937;&#65292;&#25105;&#20204;&#25512;&#23548;&#24182;&#30740;&#31350;&#20102;&#39640;&#26031;&#20998;&#24067;&#30340;&#27010;&#29575;&#27969;ODE&#30340;&#23553;&#38381;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26174;&#31034;&#20986;&#21453;&#21521;&#25193;&#25955;&#29366;&#24577;&#21521;&#30528;&#36880;&#28176;&#25351;&#23450;&#30340;&#30446;&#26631;&#22312;&#22270;&#20687;&#27969;&#24418;&#19978;&#26059;&#36716;&#12290;&#23427;&#36824;&#34920;&#26126;&#29983;&#25104;&#39318;&#20808;&#28041;&#21450;&#25215;&#35834;&#19968;&#31181;&#36718;&#24275;&#65292;&#28982;&#21518;&#26159;&#26356;&#31934;&#32454;&#30340;&#21644;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.02490v2 Announce Type: replace-cross  Abstract: How do diffusion generative models convert pure noise into meaningful images? In a variety of pretrained diffusion models (including conditional latent space models like Stable Diffusion), we observe that the reverse diffusion process that underlies image generation has the following properties: (i) individual trajectories tend to be low-dimensional and resemble 2D `rotations'; (ii) high-variance scene features like layout tend to emerge earlier, while low-variance details tend to emerge later; and (iii) early perturbations tend to have a greater impact on image content than later perturbations. To understand these phenomena, we derive and study a closed-form solution to the probability flow ODE for a Gaussian distribution, which shows that the reverse diffusion state rotates towards a gradually-specified target on the image manifold. It also shows that generation involves first committing to an outline, and then to finer and f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#38544;&#24335;&#23618;&#29702;&#35770;&#65292;$\Psi$-GNN&#27169;&#22411;&#20102;&#19968;&#20010;&#8220;&#26080;&#38480;&#8221;&#28145;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32463;&#39564;&#35843;&#25972;&#25152;&#38656;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#27425;&#20197;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2302.10891</link><description>&lt;p&gt;
&#19968;&#31181;&#38544;&#24335;GNN&#27714;&#35299;&#22120;&#29992;&#20110;&#31867;&#27850;&#26494;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An Implicit GNN Solver for Poisson-like problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.10891
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#38544;&#24335;&#23618;&#29702;&#35770;&#65292;$\Psi$-GNN&#27169;&#22411;&#20102;&#19968;&#20010;&#8220;&#26080;&#38480;&#8221;&#28145;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32463;&#39564;&#35843;&#25972;&#25152;&#38656;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#27425;&#20197;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\Psi$-GNN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#26222;&#36941;&#27850;&#26494;PDE&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38544;&#24335;&#23618;&#29702;&#35770;&#65292;$\Psi$-GNN&#24314;&#27169;&#20102;&#19968;&#20010;&#8220;&#26080;&#38480;&#8221;&#28145;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32463;&#39564;&#35843;&#25972;&#25152;&#38656;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#27425;&#20197;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#21407;&#22987;&#26550;&#26500;&#26126;&#30830;&#32771;&#34385;&#20102;&#36793;&#30028;&#26465;&#20214;&#65292;&#36825;&#26159;&#29289;&#29702;&#24212;&#29992;&#30340;&#20851;&#38190;&#21069;&#25552;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#20219;&#20309;&#26368;&#21021;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; $\Psi$-GNN&#20351;&#29992;&#8220;&#29289;&#29702;&#20449;&#24687;&#8221;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#35757;&#32451;&#36807;&#31243;&#30001;&#35774;&#35745;&#31283;&#23450;&#65292;&#24182;&#23545;&#20854;&#21021;&#22987;&#21270;&#19981;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#24182;&#19988;&#20854;&#26580;&#38887;&#24615;&#21644;&#27867;&#21270;&#25928;&#29575;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65306;&#30456;&#21516;&#30340;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22788;&#29702;&#21508;&#31181;&#23610;&#23544;&#21644;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#30340;&#38750;&#32467;&#26500;&#21270;&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.10891v3 Announce Type: replace-cross  Abstract: This paper presents $\Psi$-GNN, a novel Graph Neural Network (GNN) approach for solving the ubiquitous Poisson PDE problems with mixed boundary conditions. By leveraging the Implicit Layer Theory, $\Psi$-GNN models an "infinitely" deep network, thus avoiding the empirical tuning of the number of required Message Passing layers to attain the solution. Its original architecture explicitly takes into account the boundary conditions, a critical prerequisite for physical applications, and is able to adapt to any initially provided solution. $\Psi$-GNN is trained using a "physics-informed" loss, and the training process is stable by design, and insensitive to its initialization. Furthermore, the consistency of the approach is theoretically proven, and its flexibility and generalization efficiency are experimentally demonstrated: the same learned model can accurately handle unstructured meshes of various sizes, as well as different bo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2302.03788</link><description>&lt;p&gt;
&#38754;&#21521;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22240;&#26524;&#35770;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Toward a Theory of Causation for Interpreting Neural Code Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models of Code&#65292;&#25110;&#32773;&#31216;&#20026;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#65288;NCMs&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#20174;&#30740;&#31350;&#21407;&#22411;&#21457;&#23637;&#20026;&#21830;&#19994;&#24320;&#21457;&#32773;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36890;&#24120;&#26159;&#20351;&#29992;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#30340;&#65292;&#36825;&#20123;&#25351;&#26631;&#36890;&#24120;&#21482;&#33021;&#25581;&#31034;&#23427;&#20204;&#30495;&#23454;&#24615;&#33021;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;NCMs&#30340;&#24615;&#33021;&#20284;&#20046;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#30446;&#21069;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#20173;&#26377;&#24456;&#22810;&#26410;&#30693;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#38024;&#23545;NCMs&#65292;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;$do_{code}$&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#20197;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;&#34429;&#28982;$do_{code}$&#30340;&#29702;&#35770;&#22522;&#30784;&#21487;&#25193;&#23637;&#21040;&#25506;&#32034;&#19981;&#21516;&#30340;&#27169;&#22411;&#23646;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#23454;&#20363;&#65292;&#26088;&#22312;&#20943;&#23569;&#24433;&#21709;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#35302;&#35273;&#25163;&#22871;&#36741;&#21161;&#25511;&#21046;&#26426;&#22120;&#25163;&#20013;&#30340;&#30446;&#26631;&#23450;&#20301;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2110.07953</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20154;&#31867;&#24847;&#22270;&#20272;&#35745;&#21644;&#39044;&#27979;&#65292;&#29992;&#20110;&#22686;&#24378;&#35302;&#35273;&#25163;&#22871;&#36741;&#21161;&#25511;&#21046;&#26426;&#22120;&#25163;
&lt;/p&gt;
&lt;p&gt;
Attention-based Estimation and Prediction of Human Intent to augment Haptic Glove aided Control of Robotic Hand
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.07953
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#35302;&#35273;&#25163;&#22871;&#36741;&#21161;&#25511;&#21046;&#26426;&#22120;&#25163;&#20013;&#30340;&#30446;&#26631;&#23450;&#20301;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32858;&#28966;&#20110;&#22522;&#20110;&#35302;&#35273;&#25163;&#22871;&#65288;HG&#65289;&#30340;&#25511;&#21046;&#26426;&#26800;&#25163;&#65288;RH&#65289;&#25191;&#34892;&#29305;&#23450;&#30446;&#26631;&#30340;&#25163;&#37096;&#25805;&#20316;&#12290;&#35302;&#35273;&#25163;&#22871;&#21644;&#26426;&#26800;&#25163;&#20013;&#30340;&#39640;&#32500;&#36816;&#21160;&#20449;&#21495;&#20855;&#26377;&#22266;&#26377;&#30340;&#36816;&#21160;&#21464;&#21270;&#65292;&#23548;&#33268;&#38590;&#20197;&#30452;&#25509;&#24314;&#31435;&#20174;&#35302;&#35273;&#25163;&#22871;&#36816;&#21160;&#20449;&#21495;&#21040;&#26426;&#26800;&#25163;&#30340;&#26144;&#23556;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#26426;&#21046;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#20154;&#31867;&#25511;&#21046;&#22120;&#33719;&#21462;&#30340;&#36816;&#21160;&#20449;&#21495;&#19982;&#26426;&#26800;&#25163;&#25235;&#21462;&#30340;&#30446;&#26631;&#23039;&#21183;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#31639;&#27861;&#65292;&#29992;&#20110;&#36716;&#25442;RH&#20013;&#21512;&#25104;&#30340;&#24847;&#22270;&#65292;&#20351;&#30446;&#26631;&#29289;&#20307;&#37325;&#26032;&#23450;&#20301;&#21040;&#39044;&#26399;&#30340;&#30446;&#26631;&#23039;&#21183;&#12290;&#22312;&#36890;&#20449;&#24310;&#36831;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#24847;&#22270;&#30340;&#28382;&#21518;&#65292;&#38656;&#35201;&#39044;&#27979;&#20272;&#35745;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#26469;&#39044;&#27979;&#19968;&#23450;&#21069;&#30651;&#30340;&#24847;&#22270;&#36712;&#36857;&#65292;&#20197;&#24357;&#34917;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.07953v2 Announce Type: replace-cross  Abstract: The letter focuses on Haptic Glove (HG) based control of a Robotic Hand (RH) executing in-hand manipulation of certain objects of interest. The high dimensional motion signals in HG and RH possess intrinsic variability of kinematics resulting in difficulty to establish a direct mapping of the motion signals from HG onto the RH. An estimation mechanism is proposed to quantify the motion signal acquired from the human controller in relation to the intended goal pose of the object being held by the robotic hand. A control algorithm is presented to transform the synthesized intent at the RH and allow relocation of the object to the expected goal pose. The lag in synthesis of the intent in the presence of communication delay leads to a requirement of predicting the estimated intent. We leverage an attention-based convolutional neural network encoder to predict the trajectory of intent for a certain lookahead to compensate for the de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12533</link><description>&lt;p&gt;
&#26377;&#25928;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;k&#20013;&#24515;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24515;&#20026;&#22522;&#30784;&#30340;&#32858;&#31867;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#37117;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36755;&#20837;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#32858;&#31867;&#32467;&#26524;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;k&#20013;&#24515;&#32858;&#31867;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#32972;&#26223;&#30693;&#35782;&#24314;&#27169;&#20026;&#24517;&#36830;&#65288;ML&#65289;&#21644;&#19981;&#36830;&#65288;CL&#65289;&#32422;&#26463;&#38598;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21253;&#25324;k&#20013;&#24515;&#22312;&#20869;&#30340;&#32858;&#31867;&#38382;&#39064;&#26412;&#36136;&#19978;&#37117;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#21463;&#32422;&#26463;&#21464;&#20307;&#34987;&#35748;&#20026;&#21463;&#21040;&#26356;&#20005;&#37325;&#30340;&#36817;&#20284;&#21644;&#35745;&#31639;&#38556;&#30861;&#30340;&#38480;&#21046;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#21453;&#25903;&#37197;&#38598;&#65292;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#25972;&#25968;&#24179;&#38754;&#21644;LP&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#32422;&#26463;k&#20013;&#24515;&#30340;&#39640;&#25928;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#31454;&#20105;&#22522;&#20934;&#31639;&#27861;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#36817;&#20284;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Center-based clustering has attracted significant research interest from both theory and practice. In many practical applications, input data often contain background knowledge that can be used to improve clustering results. In this work, we build on widely adopted $k$-center clustering and model its input background knowledge as must-link (ML) and cannot-link (CL) constraint sets. However, most clustering problems including $k$-center are inherently $\mathcal{NP}$-hard, while the more complex constrained variants are known to suffer severer approximation and computation barriers that significantly limit their applicability. By employing a suite of techniques including reverse dominating sets, linear programming (LP) integral polyhedron, and LP duality, we arrive at the first efficient approximation algorithm for constrained $k$-center with the best possible ratio of 2. We also construct competitive baseline algorithms and empirically evaluate our approximation algorithm against them o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;AI&#35270;&#35273;&#27169;&#22411;&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20197;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#35299;&#37322;&#25928;&#29992;&#21644;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12261</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#20998;&#26512;AI&#35270;&#35273;&#27169;&#22411;&#30340;&#36136;&#37327;&#23646;&#24615;&#21450;&#20854;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Quality Attributes of AI Vision Models in Open Repositories Under Adversarial Attacks. (arXiv:2401.12261v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;AI&#35270;&#35273;&#27169;&#22411;&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20197;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#35299;&#37322;&#25928;&#29992;&#21644;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#32463;&#24120;&#21457;&#24067;&#21040;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#65292;&#22914;HuggingFace&#12290;&#22312;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#29983;&#20135;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20043;&#21069;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#36136;&#37327;&#20445;&#35777;&#39564;&#35777;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#38500;&#20102;&#35780;&#20272;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#30340;&#25928;&#29575;&#22806;&#65292;&#23545;&#25239;&#25915;&#20987;&#21487;&#33021;&#23545;AI&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26500;&#25104;&#23041;&#32961;&#12290;&#21516;&#26102;&#65292;&#21487;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#24212;&#29992;&#36817;&#20284;&#36755;&#20837;&#21040;&#36755;&#20986;&#30340;&#31639;&#27861;&#26469;&#35782;&#21035;&#36129;&#29486;&#29305;&#24449;&#12290;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#20250;&#38477;&#20302;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;XAI&#35299;&#37322;&#30340;&#25928;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#19979;&#28216;&#35780;&#20272;&#20219;&#21153;&#65292;&#21253;&#25324;&#39564;&#35777;AI&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#29992;&#22522;&#20934;&#25200;&#21160;&#35780;&#20272;&#40065;&#26834;&#24615;&#65292;&#27604;&#36739;&#35299;&#37322;&#25928;&#29992;&#20197;&#21450;&#35780;&#20272;&#24320;&#38144;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI models rapidly evolve, they are frequently released to open repositories, such as HuggingFace. It is essential to perform quality assurance validation on these models before integrating them into the production development lifecycle. In addition to evaluating efficiency in terms of balanced accuracy and computing costs, adversarial attacks are potential threats to the robustness and explainability of AI models. Meanwhile, XAI applies algorithms that approximate inputs to outputs post-hoc to identify the contributing features. Adversarial perturbations may also degrade the utility of XAI explanations that require further investigation. In this paper, we present an integrated process designed for downstream evaluation tasks, including validating AI model accuracy, evaluating robustness with benchmark perturbations, comparing explanation utility, and assessing overhead. We demonstrate an evaluation scenario involving six computer vision models, which include CNN-based, Transformer-b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#20844;&#24335;&#30340;&#20248;&#20808;&#39034;&#24207;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27604;&#36739;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#36866;&#21512;&#30340;&#26435;&#37325;&#20272;&#35745;&#65292;&#20351;&#24471;&#39318;&#36873;&#20449;&#21495;&#30340;&#21152;&#26435;&#28385;&#36275;&#24230;&#39640;&#20110;&#38750;&#39318;&#36873;&#20449;&#21495;&#12290;&#22312;&#20154;&#20307;&#35797;&#39564;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02099</link><description>&lt;p&gt;
&#19968;&#31181;&#23433;&#20840;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#21450;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Safe Preference Learning Approach for Personalization with Applications to Autonomous Vehicles. (arXiv:2311.02099v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#20844;&#24335;&#30340;&#20248;&#20808;&#39034;&#24207;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27604;&#36739;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#36866;&#21512;&#30340;&#26435;&#37325;&#20272;&#35745;&#65292;&#20351;&#24471;&#39318;&#36873;&#20449;&#21495;&#30340;&#21152;&#26435;&#28385;&#36275;&#24230;&#39640;&#20110;&#38750;&#39318;&#36873;&#20449;&#21495;&#12290;&#22312;&#20154;&#20307;&#35797;&#39564;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#65292;&#30830;&#20445;&#31526;&#21512;&#32473;&#23450;&#35268;&#33539;&#65292;&#24182;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25551;&#36848;&#20132;&#36890;&#35268;&#21017;&#30340;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;(STL)&#20844;&#24335;&#30340;&#20248;&#20808;&#39034;&#24207;&#32435;&#20837;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#21152;&#26435;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;(PWSTL)&#65292;&#25105;&#20204;&#22522;&#20110;&#25104;&#23545;&#27604;&#36739;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30830;&#20445;&#20102;&#23433;&#20840;&#20445;&#35777;&#30340;&#20559;&#22909;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#32473;&#23450;PWSTL&#20844;&#24335;&#26435;&#37325;&#30340;&#21487;&#34892;&#20272;&#35745;&#65292;&#20351;&#24471;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#26102;&#65292;&#39318;&#36873;&#20449;&#21495;&#30340;&#21152;&#26435;&#23450;&#37327;&#28385;&#36275;&#24230;&#22823;&#20110;&#38750;&#39318;&#36873;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#30340;&#26435;&#37325;&#30340;&#21487;&#34892;&#20272;&#35745;&#23548;&#33268;&#20102;&#19968;&#20010;&#21152;&#26435;STL&#20844;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#27491;&#30830;&#24615;&#21644;&#23450;&#21046;&#21512;&#25104;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#25311;&#39550;&#39542;&#22330;&#26223;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#20307;&#35797;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a preference learning method that ensures adherence to given specifications, with an application to autonomous vehicles. Our approach incorporates the priority ordering of Signal Temporal Logic (STL) formulas describing traffic rules into a learning framework. By leveraging Parametric Weighted Signal Temporal Logic (PWSTL), we formulate the problem of safety-guaranteed preference learning based on pairwise comparisons and propose an approach to solve this learning problem. Our approach finds a feasible valuation for the weights of the given PWSTL formula such that, with these weights, preferred signals have weighted quantitative satisfaction measures greater than their non-preferred counterparts. The feasible valuation of weights given by our approach leads to a weighted STL formula that can be used in correct-and-custom-by-construction controller synthesis. We demonstrate the performance of our method with a pilot human subject study in two different simulated dri
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#30340;&#35821;&#35328;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13257</link><description>&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#35270;&#35273;&#23450;&#20301;&#26377;&#21161;&#20110;&#23398;&#20064;&#21333;&#35789;&#30340;&#21547;&#20041;
&lt;/p&gt;
&lt;p&gt;
Visual Grounding Helps Learn Word Meanings in Low-Data Regimes. (arXiv:2310.13257v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13257
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#30340;&#35821;&#35328;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26159;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#21477;&#23376;&#20135;&#29983;&#21644;&#29702;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20854;&#20869;&#37096;&#34920;&#36798;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#30340;&#35821;&#35328;&#34920;&#36798;&#38750;&#24120;&#21563;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#21462;&#24471;&#36825;&#20123;&#32467;&#26524;&#65292;LM&#24517;&#39035;&#20197;&#19982;&#20154;&#31867;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#27604;&#20799;&#31461;&#22312;&#21457;&#32946;&#36807;&#31243;&#20013;&#25509;&#25910;&#21040;&#30340;&#35821;&#35328;&#25968;&#25454;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#24863;&#30693;&#12289;&#34892;&#21160;&#25110;&#31038;&#20132;&#34892;&#20026;&#30340;&#22522;&#30784;&#12290;&#22914;&#26524;&#29992;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#20381;&#38752;&#24863;&#30693;&#30340;&#30417;&#30563;&#65292;&#27169;&#22411;&#30340;&#35821;&#35328;&#23398;&#20064;&#26159;&#21542;&#26356;&#25509;&#36817;&#20154;&#31867;&#65311;&#25105;&#20204;&#22312;&#21333;&#35789;&#23398;&#20064;&#36825;&#19968;&#35821;&#35328;&#20064;&#24471;&#30340;&#20851;&#38190;&#23376;&#20219;&#21153;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;LM&#26550;&#26500;&#65292;&#24182;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#36741;&#21161;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#21477;&#27861;&#31867;&#21035;&#12289;&#35789;&#27719;&#20851;&#31995;&#12289;&#35821;&#20041;&#23398;&#31561;&#26041;&#38754;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural language models (LMs) are powerful tools for modeling human sentence production and comprehension, and their internal representations are remarkably well-aligned with representations of language in the human brain. But to achieve these results, LMs must be trained in distinctly un-human-like ways -- requiring orders of magnitude more language data than children receive during development, and without any of the accompanying grounding in perception, action, or social behavior. Do models trained more naturalistically -- with grounded supervision -- exhibit more human-like language learning? We investigate this question in the context of word learning, a key sub-task in language acquisition. We train a diverse set of LM architectures, with and without auxiliary supervision from image captioning tasks, on datasets of varying scales. We then evaluate these models on a broad set of benchmarks characterizing models' learning of syntactic categories, lexical relations, semantic f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.12541</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22810;&#30446;&#26631;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Multi-objective Evolutionary Optimization. (arXiv:2310.12541v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEAs&#65289;&#26159;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65288;MOPs&#65289;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;MOEAs&#65292;&#20854;&#25805;&#20316;&#31526;&#38656;&#35201;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#23581;&#35797;&#23558;MOEAs&#20013;&#25163;&#21160;&#35774;&#35745;&#30340;&#25805;&#20316;&#31526;&#26367;&#25442;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#25805;&#20316;&#31526;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23398;&#20064;&#21040;&#30340;&#25805;&#20316;&#31526;&#21487;&#33021;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#35299;&#20915;&#26032;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35753;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#20316;&#20026;&#20998;&#35299;&#22411;MOEA&#65288;MOEA/D&#65289;&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#65292;&#24182;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the operators need carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well to solve new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#22797;&#26434;&#20154;&#32676;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#35270;&#35282;&#35268;&#21010;&#22120;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.10863</link><description>&lt;p&gt;
&#36138;&#24515;&#35270;&#35282;&#65306;&#22810;&#26080;&#20154;&#26426;&#35270;&#37326;&#35268;&#21010;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#30340;&#21327;&#21516;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Greedy Perspectives: Multi-Drone View Planning for Collaborative Coverage in Cluttered Environments. (arXiv:2310.10863v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#22797;&#26434;&#20154;&#32676;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#35270;&#35282;&#35268;&#21010;&#22120;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#22242;&#38431;&#30340;&#37096;&#32626;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25293;&#25668;&#21160;&#24577;&#20154;&#32676;&#65288;&#28436;&#21592;&#65289;&#30340;&#22823;&#35268;&#27169;&#24433;&#20687;&#65292;&#29992;&#20110;&#22242;&#38431;&#36816;&#21160;&#21644;&#30005;&#24433;&#21046;&#20316;&#31561;&#26032;&#24212;&#29992;&#39046;&#22495;&#12290;&#20026;&#20102;&#23454;&#29616;&#35813;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#36890;&#36807;&#39034;&#24207;&#36138;&#24515;&#35268;&#21010;&#36827;&#34892;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#26080;&#20154;&#26426;&#22242;&#38431;&#20043;&#38388;&#36827;&#34892;&#25668;&#20687;&#26426;&#35270;&#37326;&#30340;&#21487;&#25193;&#23637;&#20248;&#21270;&#65292;&#20294;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#21516;&#25928;&#26524;&#38754;&#20020;&#25361;&#25112;&#12290;&#38556;&#30861;&#29289;&#21487;&#33021;&#20135;&#29983;&#36974;&#25377;&#24182;&#22686;&#21152;&#26080;&#20154;&#26426;&#30896;&#25758;&#30340;&#20960;&#29575;&#65292;&#36825;&#21487;&#33021;&#36829;&#21453;&#36817;&#20284;&#26368;&#20248;&#24615;&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#22312;&#31264;&#23494;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#20154;&#32676;&#65292;&#38656;&#35201;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#35270;&#35282;&#35268;&#21010;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#22120;&#65292;&#24182;&#19982;&#36138;&#24515;&#24418;&#25104;&#35268;&#21010;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#35752;&#36974;&#25377;&#21644;&#30896;&#25758;&#23545;&#25293;&#25668;&#24212;&#29992;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35780;&#20272;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for novel applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can be used for scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a greedy formation planner. To evaluate performance,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.14974</link><description>&lt;p&gt;
&#22312;&#19968;&#21315;&#24180;&#21069;&#30340;&#25289;&#19969;&#25991;&#26412;&#20013;&#26816;&#27979;&#21477;&#23376;&#32423;&#21035;&#30340;&#24615;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts. (arXiv:2309.14974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#24555;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#39033;&#20256;&#32479;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#32422;2500&#20010;&#21477;&#23376;&#65292;&#28085;&#30422;&#20102;&#20174;&#20844;&#20803;&#21069;300&#24180;&#21040;&#20844;&#20803;900&#24180;&#30340;&#24615;&#35821;&#20041;&#23398;&#65288;&#21307;&#23398;&#65292;&#24773;&#33394;&#31561;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21477;&#23376;&#20998;&#31867;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#37117;&#27604;&#31616;&#21333;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#25628;&#32034;&#26041;&#27861;&#26356;&#22909;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20010;&#20154;&#35328;&#35821;&#21644;&#31038;&#20250;&#35328;&#35821;&#20803;&#25968;&#25454;&#23884;&#20837;&#65288;&#19990;&#32426;&#65292;&#20316;&#32773;&#65292;&#20889;&#20316;&#31867;&#22411;&#65289;&#30340;&#25972;&#21512;&#65292;&#20294;&#21457;&#29616;&#36825;&#23548;&#33268;&#20102;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;HAN&#20998;&#21035;&#36798;&#21040;&#20102;70.60%&#30340;&#39640;&#31934;&#24230;&#21644;86.33%&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65288;420&#32780;&#19981;&#26159;2013&#65289;&#65292;&#24182;&#26174;&#31034;&#20986;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#31245;&#26377;&#19979;&#38477;&#65292;&#20294;&#24615;&#33021;&#20173;&#28982;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose to evaluate the use of deep learning methods for semantic classification at the sentence level to accelerate the process of corpus building in the field of humanities and linguistics, a traditional and time-consuming task. We introduce a novel corpus comprising around 2500 sentences spanning from 300 BCE to 900 CE including sexual semantics (medical, erotica, etc.). We evaluate various sentence classification approaches and different input embedding layers, and show that all consistently outperform simple token-based searches. We explore the integration of idiolectal and sociolectal metadata embeddings (centuries, author, type of writing), but find that it leads to overfitting. Our results demonstrate the effectiveness of this approach, achieving high precision and true positive rates (TPR) of respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset size on the model performances (420 instead of 2013), and show that, while our models per
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.10275</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#20026;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#26234;&#33021;&#20307;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31354;&#20013;&#32676;&#20307;&#12289;&#33258;&#21160;&#21270;&#20179;&#20648;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#21035;&#65306;&#38598;&#20013;&#24335;&#35268;&#21010;&#21644;&#20998;&#25955;&#24335;&#35268;&#21010;&#12290;&#38598;&#20013;&#24335;&#35268;&#21010;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#65292;&#22240;&#27492;&#22312;&#22823;&#22411;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#19981;&#20855;&#22791;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20998;&#25955;&#24335;&#35268;&#21010;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#26102;&#36335;&#24452;&#35268;&#21010;&#65292;&#23637;&#31034;&#20102;&#38544;&#24335;&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#23427;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#19988;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRAMP&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#24335;&#35838;&#31243;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.06578</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36776;&#21035;&#31185;&#23398;&#20551;&#35774;&#30340;&#35777;&#25454;&#65311;&#31038;&#20250;&#31185;&#23398;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences. (arXiv:2309.06578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#30340;&#21046;&#23450;&#21644;&#27979;&#35797;&#26159;&#32463;&#39564;&#24615;&#30740;&#31350;&#30340;&#26680;&#24515;&#12290;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#20551;&#35774;&#26159;&#22522;&#20110;&#29616;&#26377;&#35777;&#25454;&#30340;&#26368;&#20339;&#29468;&#27979;&#65292;&#24182;&#19988;&#26159;&#22522;&#20110;&#30456;&#20851;&#25991;&#29486;&#30340;&#20840;&#38754;&#35270;&#22270;&#36827;&#34892;&#21551;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27599;&#24180;&#31185;&#23398;&#25991;&#31456;&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#23545;&#20110;&#32473;&#23450;&#20551;&#35774;&#30456;&#20851;&#35777;&#25454;&#30340;&#25163;&#21160;&#27719;&#24635;&#21644;&#32508;&#21512;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#20013;&#30340;&#35777;&#25454;&#65292;&#33021;&#21542;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20849;&#20139;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#20013;&#20351;&#29992;&#31038;&#21306;&#39537;&#21160;&#30340;&#30740;&#31350;&#27880;&#37322;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#24615;&#33021;&#19982;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Sai90000/ScientificHypothesisEvidencing.git&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothesis formulation and testing are central to empirical research. A strong hypothesis is a best guess based on existing evidence and informed by a comprehensive view of relevant literature. However, with exponential increase in the number of scientific articles published annually, manual aggregation and synthesis of evidence related to a given hypothesis is a challenge. Our work explores the ability of current large language models (LLMs) to discern evidence in support or refute of specific hypotheses based on the text of scientific abstracts. We share a novel dataset for the task of scientific hypothesis evidencing using community-driven annotations of studies in the social sciences. We compare the performance of LLMs to several state-of-the-art benchmarks and highlight opportunities for future research in this area. The dataset is available at https://github.com/Sai90000/ScientificHypothesisEvidencing.git
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12243</link><description>&lt;p&gt;
&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#19981;&#21516;&#30340;&#20914;&#31361;&#20248;&#21270;&#20934;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#30340;&#20027;&#20219;&#21153;&#65288;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65289;&#65292;&#20063;&#21487;&#20197;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#21644;&#27425;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#25439;&#22833;&#26368;&#23567;&#21270;&#19982;&#31232;&#30095;&#24615;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#31616;&#21333;&#22320;&#21152;&#26435;&#20934;&#21017;&#65292;&#20294;&#22312;&#20984;&#35774;&#32622;&#20013;&#25165;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#23545;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26631;&#37327;&#21270;&#25216;&#26415;&#65292;&#31639;&#27861;&#21487;&#20197;&#35782;&#21035;&#21407;&#22987;&#38382;&#39064;&#30340;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#23558;&#20854;&#22797;&#26434;&#24615;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22686;&#24191;Lagrangian&#26041;&#27861;&#26469;&#35299;&#20915;&#31616;&#21270;&#21518;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;Adam&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#22788;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#32463;&#27982;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#21152;&#36895;&#36816;&#21160;&#35268;&#21010;&#20248;&#21270;&#36807;&#31243;&#12290;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#24182;&#21487;&#20197;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2308.01557</link><description>&lt;p&gt;
&#36816;&#21160;&#35268;&#21010;&#25193;&#25955;&#65306;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. (arXiv:2308.01557v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#21152;&#36895;&#36816;&#21160;&#35268;&#21010;&#20248;&#21270;&#36807;&#31243;&#12290;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#24182;&#21487;&#20197;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36712;&#36857;&#20998;&#24067;&#30340;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#21152;&#24555;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#20248;&#21270;&#12290;&#22312;&#32473;&#23450;&#20808;&#21069;&#25104;&#21151;&#30340;&#35268;&#21010;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#36712;&#36857;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#26032;&#35268;&#21010;&#38382;&#39064;&#30340;&#20808;&#39564;&#30693;&#35782;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#36825;&#31181;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#24341;&#23548;&#30340;&#26041;&#27861;&#12290;&#21487;&#20197;&#36890;&#36807;&#20174;&#20808;&#39564;&#30693;&#35782;&#20013;&#37319;&#26679;&#21021;&#22987;&#21270;&#65292;&#25110;&#32773;&#22312;&#26368;&#22823;&#21518;&#39564;&#20248;&#21270;&#30340;&#36807;&#31243;&#20013;&#20351;&#29992;&#20808;&#39564;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#21435;&#22122;&#36807;&#31243;&#65292;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#36825;&#23545;&#20110;&#22823;&#37327;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#38750;&#24120;&#36866;&#29992;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;-&#36816;&#21160;&#35268;&#21010;&#25193;&#25955;&#19982;&#20960;&#31181;&#22522;&#20934;&#26041;&#21457;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multimodality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several ba
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#23481;&#26131;&#34987;&#26500;&#26550;&#20026;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#65292;&#32780;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#21463;&#27426;&#36814;&#31243;&#24230;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.07645</link><description>&lt;p&gt;
&#32654;&#22269;&#39184;&#21381;&#35780;&#35770;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31227;&#27665;&#32654;&#39135;&#20182;&#32773;&#21270;&#21644;&#20302;&#22768;&#26395;&#26500;&#26550;
&lt;/p&gt;
&lt;p&gt;
Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models. (arXiv:2307.07645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07645
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#23481;&#26131;&#34987;&#26500;&#26550;&#20026;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#65292;&#32780;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#21463;&#27426;&#36814;&#31243;&#24230;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#29702;&#35299;&#23545;&#39135;&#29289;&#30340;&#38544;&#21547;&#24577;&#24230;&#26377;&#21161;&#20110;&#20943;&#36731;&#22240;&#39135;&#29289;&#20316;&#20026;&#25991;&#21270;&#21644;&#31181;&#26063;&#36523;&#20221;&#30340;&#26631;&#24535;&#32780;&#23548;&#33268;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#23545;&#39135;&#29289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26159;&#19968;&#31181;&#24494;&#20405;&#30053;&#65292;&#23427;&#23545;&#26377;&#23475;&#30340;&#20844;&#20849;&#35805;&#35821;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#21453;&#36807;&#26469;&#21152;&#28145;&#23545;&#27665;&#26063;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#24182;&#23545;&#39184;&#39302;&#30340;&#32463;&#27982;&#32467;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#20180;&#32454;&#30340;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#22312;&#19968;&#39033;&#22823;&#35268;&#27169;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#23545;&#31227;&#27665;&#32654;&#39135;&#24577;&#24230;&#30340;&#31038;&#20250;&#29702;&#35770;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#22312;14&#20010;&#32654;&#22269;&#24030;&#30340;&#26694;&#26550;&#24046;&#24322;&#12290;&#22312;&#25511;&#21046;&#20102;&#39184;&#21381;&#20215;&#26684;&#21644;&#37051;&#37324;&#31181;&#26063;&#22810;&#26679;&#24615;&#31561;&#22240;&#32032;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#26377;&#21487;&#33021;&#20197;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#30340;&#24418;&#24335;&#36827;&#34892;&#26500;&#26550;&#65292;&#22914;&#30495;&#23454;&#24615;&#65288;&#20363;&#22914;&#65292;&#30495;&#23454;&#65292;&#20256;&#32479;&#65289;&#65292;&#24322;&#22269;&#24773;&#35843;&#65288;&#20363;&#22914;&#65292;&#24322;&#22269;&#65292;&#19981;&#21516;&#65289;&#21644;&#20856;&#22411;&#24615;&#65288;&#20363;&#22914;&#65292;&#20856;&#22411;&#65292;&#36890;&#24120;&#65289;&#12290;&#20294;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#65288;&#20363;&#22914;&#65292;&#21360;&#24230;&#65292;&#22696;&#35199;&#21733;&#65289;&#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying and understanding implicit attitudes toward food can help efforts to mitigate social prejudice due to food's pervasive role as a marker of cultural and ethnic identity. Stereotypes about food are a form of microaggression that contribute to harmful public discourse that may in turn perpetuate prejudice toward ethnic groups and negatively impact economic outcomes for restaurants. Through careful linguistic analyses, we evaluate social theories about attitudes toward immigrant cuisine in a large-scale study of framing differences in 2.1M English language Yelp reviews of restaurants in 14 US states. Controlling for factors such as restaurant price and neighborhood racial diversity, we find that immigrant cuisines are more likely to be framed in objectifying and othering terms of authenticity (e.g., authentic, traditional), exoticism (e.g., exotic, different), and prototypicality (e.g., typical, usual), but that non-Western immigrant cuisines (e.g., Indian, Mexican) receive mor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05300</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#35748;&#30693;&#21327;&#21516;&#65306;&#36890;&#36807;&#22810;&#20154;&#26684;&#33258;&#25105;&#21327;&#20316;&#23454;&#29616;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#20381;&#36182;&#20110;&#35748;&#30693;&#21327;&#21516;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;&#19981;&#21516;&#35748;&#30693;&#36807;&#31243;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#21644;&#20449;&#24687;&#25972;&#21512;&#65292;&#20197;&#33719;&#24471;&#27604;&#20010;&#20307;&#35748;&#30693;&#36807;&#31243;&#26356;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#38656;&#35201;&#20016;&#23500;&#39046;&#22495;&#30693;&#35782;&#21644;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;LLM&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#12290;&#35748;&#30693;&#21327;&#21516;&#32773;&#25351;&#30340;&#26159;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#19982;&#22810;&#20010;&#26234;&#24935;&#21512;&#20316;&#65292;&#32467;&#21512;&#20182;&#20204;&#30340;&#20010;&#20307;&#20248;&#21183;&#21644;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#36755;&#20837;&#21160;&#24577;&#35782;&#21035;&#21644;&#27169;&#25311;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;SPP&#37322;&#25918;&#20102;LLM&#20013;&#35748;&#30693;&#21327;&#21516;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assi
&lt;/p&gt;</description></item><item><title>RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15909</link><description>&lt;p&gt;
RL$^3$:&#36890;&#36807;RL&#20869;&#37096;&#30340;RL$^2$&#25552;&#21319;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15909
&lt;/p&gt;
&lt;p&gt;
RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;meta-RL&#65289;&#26041;&#27861;&#65292;&#22914;RL$^2$&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#38024;&#23545;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#20219;&#21153;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#32463;&#39564;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#33324;&#30340;&#24378;&#21270;&#23398;&#20064;&#32452;&#20214;&#65292;&#20363;&#22914;&#20215;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;transformers&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#21464;&#24471;&#31105;&#27490;&#20043;&#21069;&#20063;&#23545;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25512;&#29702;&#30340;&#21382;&#21490;&#38271;&#24230;&#26377;&#23454;&#38469;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#20204;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL$^3$&#65292;&#19968;&#31181;&#32452;&#21512;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#36807;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00038</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;(FedCSD)
&lt;/p&gt;
&lt;p&gt;
FedCSD: A Federated Learning Based Approach for Code-Smell Detection. (arXiv:2306.00038v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#19977;&#20010;&#23454;&#39564;&#26469;&#25903;&#25345;&#36825;&#20123;&#26029;&#35328;&#65292;&#36825;&#20123;&#23454;&#39564;&#21033;&#29992;&#20102;&#19977;&#20010;&#25163;&#21160;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#26816;&#27979;&#21644;&#30740;&#31350;&#19981;&#21516;&#30340;&#20195;&#30721;&#24322;&#21619;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\%) where fewer smells exist in the training dataset, which has a noticeab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$K^2$-&#26641;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32039;&#20945;&#29983;&#25104;&#65292;&#24182;&#21516;&#26102;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#36890;&#36807;&#25552;&#20986;&#39034;&#24207;$K^2$-&#26641;&#34920;&#31034;&#21644;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19125</link><description>&lt;p&gt;
&#22522;&#20110;$K^2$-&#26641;&#30340;&#20998;&#32423;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Graph Generation with $K^2$-trees. (arXiv:2305.19125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$K^2$-&#26641;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32039;&#20945;&#29983;&#25104;&#65292;&#24182;&#21516;&#26102;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#36890;&#36807;&#25552;&#20986;&#39034;&#24207;$K^2$-&#26641;&#34920;&#31034;&#21644;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30446;&#26631;&#20998;&#24067;&#29983;&#25104;&#22270;&#26159;&#35768;&#22810;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#33647;&#29289;&#21457;&#29616;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#26080;&#25439;&#22270;&#21387;&#32553;&#30340;$K^2$-&#26641;&#34920;&#31034;&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;$K^2$-&#26641;&#33021;&#22815;&#22312;&#36827;&#34892;&#32039;&#20945;&#29983;&#25104;&#30340;&#21516;&#26102;&#65292;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;(1)&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#21098;&#26525;&#12289;&#25153;&#24179;&#21270;&#21644;&#35760;&#21495;&#21270;&#36807;&#31243;&#30340;&#39034;&#24207;K2&#26641;&#34920;&#31034;&#21644;(2)&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#19987;&#19994;&#26641;&#24418;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#26469;&#29983;&#25104;&#24207;&#21015;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#24120;&#35268;&#21644;&#20004;&#20010;&#20998;&#23376;&#22270;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#20197;&#35777;&#23454;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation which was originally designed for lossless graph compression. Our motivation stems from the ability of the $K^2$-trees to enable compact generation while concurrently capturing the inherent hierarchical structure of a graph. In addition, we make further contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20013;&#25991;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2305.14790</link><description>&lt;p&gt;
&#25552;&#21319;&#20013;&#25991;&#25991;&#26412;&#20027;&#39064;&#21010;&#20998;&#21644;&#32434;&#35201;&#29983;&#25104;&#65306;&#27573;&#33853;&#32423;&#20027;&#39064;&#34920;&#31034;&#65292;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark. (arXiv:2305.14790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20013;&#25991;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#21010;&#20998;&#21644;&#32434;&#35201;&#29983;&#25104;&#26088;&#22312;&#23558;&#19968;&#20010;&#25991;&#26723;&#20998;&#25104;&#36830;&#36143;&#30340;&#20027;&#39064;&#27573;&#33853;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#23376;&#26631;&#39064;&#12290;&#36825;&#20010;&#36807;&#31243;&#25581;&#31034;&#20102;&#19968;&#20010;&#25991;&#26723;&#30340;&#35805;&#39064;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#20174;&#26356;&#39640;&#30340;&#23618;&#27425;&#24555;&#36895;&#25226;&#25569;&#21644;&#29702;&#35299;&#25991;&#26723;&#30340;&#25972;&#20307;&#24773;&#22659;&#12290;&#28982;&#32780;&#65292;&#19982;&#33521;&#35821;&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#21151;&#30456;&#27604;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#27573;&#33853;&#32423;&#20027;&#39064;&#34920;&#31034;&#21644;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#35821;&#26009;&#24211;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#21253;&#25324;&#26631;&#39064;&#12289;&#23376;&#26631;&#39064;&#21644;&#27573;&#33853;&#65292;&#32508;&#21512;&#22320;&#27169;&#25311;&#20102;&#25991;&#26723;&#30340;&#35805;&#39064;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#30830;&#20445;&#26356;&#20840;&#38754;&#22320;&#34920;&#31034;&#25991;&#26723;&#20869;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#26681;&#25454;&#36825;&#31181;&#34920;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26368;&#22823;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic segmentation and outline generation strive to divide a document into coherent topic sections and generate corresponding subheadings. Such a process unveils the discourse topic structure of a document that benefits quickly grasping and understanding the overall context of the document from a higher level. However, research and applications in this field have been restrained due to the lack of proper paragraph-level topic representations and large-scale, high-quality corpora in Chinese compared to the success achieved in English. Addressing these issues, we introduce a hierarchical paragraph-level topic structure representation with title, subheading, and paragraph that comprehensively models the document discourse topic structure. In addition, we ensure a more holistic representation of topic distribution within the document by using sentences instead of keywords to represent sub-topics. Following this representation, we construct the largest Chinese Paragraph-level Topic Structur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#20102;&#33521;&#25991;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10666</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
a unified front-end framework for english text-to-speech synthesis. (arXiv:2305.10666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#20102;&#33521;&#25991;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#31471;&#26159;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36127;&#36131;&#25552;&#21462;&#35821;&#35328;&#29305;&#24449;&#65292;&#22914;&#38901;&#24459;&#21644;&#38899;&#32032;&#65292;&#36825;&#23545;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#21512;&#25104;&#35821;&#38899;&#33267;&#20851;&#37325;&#35201;&#12290;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#36890;&#24120;&#30001;&#25991;&#26412;&#35268;&#33539;&#21270;&#27169;&#22359;&#65288;TN&#65289;&#65292;&#21333;&#35789;&#38901;&#24459;&#30701;&#35821;&#38901;&#24459;&#30701;&#35821;&#27169;&#22359;&#65288;PWPP&#65289;&#21644;&#23383;&#24418;&#21040;&#38899;&#32032;&#27169;&#22359;&#65288;G2P&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#30340;&#30740;&#31350;&#20165;&#20851;&#27880;&#20110;&#21333;&#29420;&#27169;&#22359;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#65292;&#23548;&#33268;&#27599;&#20010;&#27169;&#22359;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.03123</link><description>&lt;p&gt;
ChatGPT &#38656;&#35201;&#36827;&#34892;SPADE&#65288;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65289;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#20013;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21457;&#34920;&#65292;&#20197;&#23637;&#31034;ChatGPT&#21644;&#20854;&#20182;LLMs&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#12289;&#38598;&#25104;&#21644;&#24773;&#24863;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22823;&#22810;&#25968;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65292;&#24182;&#24314;&#35758;&#19981;&#20165;&#20165;&#26159;ChatGPT&#65292;&#32780;&#26159;&#22312;&#23545;&#35805;&#26426;&#22120;&#20154;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#21518;&#32493;&#20837;&#21475;&#37117;&#24212;&#35813;&#36827;&#34892;SPADE&#35780;&#20272;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#20851;&#20110;ChatGPT&#30340;&#38382;&#39064;&#21644;&#20851;&#27880;&#28857;&#19982;&#19978;&#36848;&#29305;&#24449;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#21021;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#35270;&#21270;&#20197;&#21450;&#20551;&#35774;&#30340;&#20107;&#23454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;&#26041;&#27861;&#65288;SEP&#65289;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03773</link><description>&lt;p&gt;
&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Explicable Robot Planning. (arXiv:2304.03773v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03773
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;&#26041;&#27861;&#65288;SEP&#65289;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#30340;&#26399;&#26395;&#28304;&#33258;&#20110;&#20182;&#20204;&#23545;&#20854;&#20182;&#20154;&#21644;&#19990;&#30028;&#30340;&#20102;&#35299;&#12290;&#22312;&#28041;&#21450;&#21040;&#20154;&#26426;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#26426;&#22120;&#20154;&#30340;&#20102;&#35299;&#21487;&#33021;&#19982;&#29616;&#23454;&#19981;&#31526;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#19981;&#33021;&#28385;&#36275;&#20154;&#20204;&#30340;&#26399;&#26395;&#12290;&#21487;&#35299;&#37322;&#35268;&#21010;&#34987;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#20197;&#21327;&#35843;&#20154;&#31867;&#26399;&#26395;&#21644;&#26368;&#20248;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#36827;&#34892;&#26356;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#20154;&#20915;&#31574;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#37027;&#23601;&#26159;&#22312;&#21487;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#21487;&#35299;&#37322;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;&#21487;&#35299;&#37322;&#35268;&#21010;&#65288;SEP&#65289;&#65292;&#23427;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#12290; SEP&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#31181;&#31574;&#30053;&#65292;&#29983;&#25104;&#25509;&#36817;&#20110;&#20154;&#31867;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#30340;&#35201;&#27714;&#12290;&#36825;&#26159;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;SEP&#30340;&#35299;&#20915;&#26041;&#26696;&#20301;&#20110;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20999;&#23454;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#19981;&#29306;&#29298;&#20219;&#20309;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#20135;&#29983;&#20102;&#23433;&#20840;&#24615;&#21644;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#19968;&#20010;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human expectations stem from their knowledge of the others and the world. Where human-robot interaction is concerned, such knowledge about the robot may be inconsistent with the ground truth, resulting in the robot not meeting its expectations. Explicable planning was previously introduced as a novel planning approach to reconciling human expectations and the optimal robot behavior for more interpretable robot decision-making. One critical issue that remains unaddressed is safety during explicable decision-making which can lead to explicable behaviors that are unsafe. We propose Safe Explicable Planning (SEP), which extends explicable planning to support the specification of a safety bound. The objective of SEP is to find a policy that generates a behavior close to human expectations while satisfying the safety constraints introduced by the bound, which is a special case of multi-objective optimization where the solution to SEP lies on the Pareto frontier. Under such a formulation, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20250;&#35805;&#26641;&#25628;&#32034;(CTS)&#65292;&#23427;&#21487;&#20197;&#26550;&#36215;FAQ&#21644;&#23545;&#35805;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#23450;&#20041;&#23545;&#35805;&#26641;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#21482;&#23398;&#20064;&#25552;&#20986;&#23548;&#33322;&#29992;&#25143;&#36798;&#21040;&#30446;&#26631;&#25152;&#38656;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10227</link><description>&lt;p&gt;
&#20250;&#35805;&#26641;&#25628;&#32034;&#65306;&#19968;&#39033;&#26032;&#30340;&#28151;&#21512;&#23545;&#35805;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Conversational Tree Search: A New Hybrid Dialog Task. (arXiv:2303.10227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20250;&#35805;&#26641;&#25628;&#32034;(CTS)&#65292;&#23427;&#21487;&#20197;&#26550;&#36215;FAQ&#21644;&#23545;&#35805;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#23450;&#20041;&#23545;&#35805;&#26641;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#21482;&#23398;&#20064;&#25552;&#20986;&#23548;&#33322;&#29992;&#25143;&#36798;&#21040;&#30446;&#26631;&#25152;&#38656;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25509;&#21475;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#21644;&#26041;&#20415;&#30340;&#26041;&#24335;&#65292;&#35753;&#29992;&#25143;&#33719;&#21462;&#21407;&#26412;&#21487;&#33021;&#38590;&#20197;&#25110;&#19981;&#26041;&#20415;&#33719;&#24471;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30028;&#38754;&#22823;&#20307;&#19978;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;FAQ&#65292;&#29992;&#25143;&#24517;&#39035;&#25552;&#20986;&#26126;&#30830;&#30340;&#38382;&#39064;&#20197;&#26816;&#32034;&#19968;&#33324;&#30340;&#31572;&#26696;&#65307;&#25110;&#32773;&#23545;&#35805;&#65292;&#29992;&#25143;&#24517;&#39035;&#36981;&#24490;&#39044;&#23450;&#20041;&#30340;&#36335;&#24452;&#20294;&#21487;&#33021;&#20250;&#25509;&#25910;&#21040;&#20010;&#24615;&#21270;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20250;&#35805;&#26641;&#25628;&#32034;(CTS)&#65292;&#23427;&#26550;&#36215;&#20102;&#20449;&#24687;&#26816;&#32034;&#39118;&#26684;FAQ&#21644;&#38754;&#21521;&#20219;&#21153;&#23545;&#35805;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#23450;&#20041;&#23545;&#35805;&#26641;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#21482;&#23398;&#20064;&#25552;&#20986;&#23548;&#33322;&#29992;&#25143;&#36798;&#21040;&#30446;&#26631;&#25152;&#38656;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26053;&#34892;&#25253;&#38144;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#22522;&#32447;&#65288;baseline&#65289;&#20197;&#21450;&#19968;&#39033;&#26032;&#39062;&#30340;&#28145;&#24230;&#22686;&#24378;&#23398;&#20064;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26032;&#30340;&#26550;&#26500;&#32508;&#21512;&#20102;FAQ&#21644;&#23545;&#35805;&#30340;&#20248;&#28857;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational interfaces provide a flexible and easy way for users to seek information that may otherwise be difficult or inconvenient to obtain. However, existing interfaces generally fall into one of two categories: FAQs, where users must have a concrete question in order to retrieve a general answer, or dialogs, where users must follow a predefined path but may receive a personalized answer. In this paper, we introduce Conversational Tree Search (CTS) as a new task that bridges the gap between FAQ-style information retrieval and task-oriented dialog, allowing domain-experts to define dialog trees which can then be converted to an efficient dialog policy that learns only to ask the questions necessary to navigate a user to their goal. We collect a dataset for the travel reimbursement domain and demonstrate a baseline as well as a novel deep Reinforcement Learning architecture for this task. Our results show that the new architecture combines the positive aspects of both the FAQ and 
&lt;/p&gt;</description></item></channel></rss>