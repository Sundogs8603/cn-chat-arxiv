<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31895;&#31890;&#21270;&#20998;&#23376;&#34920;&#31034;&#20013;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#37319;&#26679;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#25928;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01195</link><description>&lt;p&gt;
&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#29992;&#20110;&#31895;&#31890;&#21270;&#20998;&#23376;&#34920;&#31034;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conditional Normalizing Flows for Active Learning of Coarse-Grained Molecular Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31895;&#31890;&#21270;&#20998;&#23376;&#34920;&#31034;&#20013;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#37319;&#26679;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#25928;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#37319;&#26679;&#20998;&#23376;&#31995;&#32479;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#19982;&#29983;&#25104;&#38271;&#26102;&#38388;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#19981;&#21516;&#65292;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22914;&#27491;&#21017;&#21270;&#27969;&#34987;&#29992;&#20110;&#30452;&#25509;&#23398;&#20064;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#65292;&#32780;&#19981;&#38656;&#35201;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#20986;&#29616;&#27169;&#24335;&#23849;&#28291;&#65292;&#22240;&#27492;&#24120;&#24120;&#26080;&#27861;&#25506;&#32034;&#20840;&#37096;&#30340;&#26500;&#22411;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#65292;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#33258;&#30001;&#24230;&#12290;&#22312;&#31895;&#31890;&#21270;&#31354;&#38388;&#19978;&#26465;&#20214;&#21270;&#27491;&#21017;&#21270;&#27969;&#21487;&#20197;&#20135;&#29983;&#20004;&#20010;&#23618;&#27425;&#20043;&#38388;&#30340;&#27010;&#29575;&#36830;&#25509;&#12290;&#20026;&#20102;&#25506;&#32034;&#26500;&#22411;&#31354;&#38388;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31895;&#31890;&#21270;&#27169;&#25311;&#19982;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24517;&#35201;&#26102;&#26356;&#26032;&#27969;&#24182;&#36827;&#34892;&#20840;&#21407;&#23376;&#21183;&#33021;&#35780;&#20272;&#12290;&#20197;&#19993;&#27688;&#37240;&#20108;&#32957;&#20026;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient sampling of the Boltzmann distribution of molecular systems is a long-standing challenge. Recently, instead of generating long molecular dynamics simulations, generative machine learning methods such as normalizing flows have been used to learn the Boltzmann distribution directly, without samples. However, this approach is susceptible to mode collapse and thus often does not explore the full configurational space. In this work, we address this challenge by separating the problem into two levels, the fine-grained and coarse-grained degrees of freedom. A normalizing flow conditioned on the coarse-grained space yields a probabilistic connection between the two levels. To explore the configurational space, we employ coarse-grained simulations with active learning which allows us to update the flow and make all-atom potential energy evaluations only when necessary. Using alanine dipeptide as an example, we show that our methods obtain a speedup to molecular dynamics simulations of
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;</title><link>https://rss.arxiv.org/abs/2402.01030</link><description>&lt;p&gt;
&#21487;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#33021;&#22815;&#28608;&#21457;&#26356;&#20986;&#33394;&#30340;LLM&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Executable Code Actions Elicit Better LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01030
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#25972;&#21512;LLM&#26234;&#33021;&#20307;&#34892;&#21160;&#65292;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#39640;&#36798;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20307;&#20855;&#22791;&#25191;&#34892;&#24191;&#27867;&#34892;&#21160;&#30340;&#33021;&#21147;&#65292;&#22914;&#35843;&#29992;&#24037;&#20855;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#31561;&#65292;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;LLM&#26234;&#33021;&#20307;&#36890;&#24120;&#36890;&#36807;&#29983;&#25104;JSON&#25110;&#25991;&#26412;&#30340;&#39044;&#23450;&#20041;&#26684;&#24335;&#26469;&#20135;&#29983;&#34892;&#21160;&#65292;&#36825;&#36890;&#24120;&#21463;&#38480;&#20110;&#21463;&#38480;&#21046;&#30340;&#34892;&#21160;&#31354;&#38388;&#65288;&#20363;&#22914;&#65292;&#39044;&#23450;&#20041;&#24037;&#20855;&#30340;&#33539;&#22260;&#65289;&#21644;&#21463;&#38480;&#30340;&#28789;&#27963;&#24615;&#65288;&#20363;&#22914;&#65292;&#26080;&#27861;&#32452;&#21512;&#22810;&#20010;&#24037;&#20855;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#23558;LLM&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65288;CodeAct&#65289;&#12290;CodeAct&#19982;Python&#35299;&#37322;&#22120;&#38598;&#25104;&#65292;&#21487;&#20197;&#25191;&#34892;&#20195;&#30721;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#22312;&#26032;&#30340;&#35266;&#23519;&#20013;&#21160;&#24577;&#20462;&#35746;&#20808;&#21069;&#30340;&#34892;&#21160;&#25110;&#21457;&#20986;&#26032;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#23545;17&#20010;LLM&#22312;API-Bank&#21644;&#26032;&#32534;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;CodeAct&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#65288;&#25104;&#21151;&#29575;&#39640;&#20986;20%&#65289;&#12290;CodeAct&#30340;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#28608;&#21169;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19521</link><description>&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20013;&#30340;&#20851;&#38190;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#25152;&#37319;&#29992;&#30340;&#26426;&#21046;&#12290;&#22312;&#38646;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#31867;&#20284;&#8220;&#27861;&#22269;&#30340;&#39318;&#37117;&#26159;&#8221;&#30340;&#25552;&#31034;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#24847;&#21147;&#22836;&#20250;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20027;&#39064;&#23454;&#20307;&#65292;&#22914;&#8220;&#27861;&#22269;&#8221;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21518;&#32493;&#30340;MLP&#20197;&#22238;&#24518;&#25152;&#38656;&#30340;&#31572;&#26696;&#65292;&#22914;&#8220;&#24052;&#40654;&#8221;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;MLP&#30340;&#36755;&#20986;&#20998;&#35299;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36319;&#38543;&#36825;&#20123;&#29305;&#23450;&#20219;&#21153;&#22836;&#30340;MLP&#23618;&#30340;&#21151;&#33021;&#12290;&#22312;&#27531;&#24046;&#27969;&#20013;&#65292;&#23427;&#20250;&#25830;&#38500;&#25110;&#25918;&#22823;&#26469;&#33258;&#21508;&#20010;&#22836;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20250;&#29983;&#25104;&#19968;&#20010;&#32452;&#20214;&#65292;&#23558;&#27531;&#24046;&#27969;&#37325;&#26032;&#23450;&#21521;&#21040;&#39044;&#26399;&#31572;&#26696;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#38646;&#27425;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#29420;&#31435;&#20110;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#20928;&#21270;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16067</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#20928;&#21270;&#30340;&#24378;&#22823;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust Diffusion Models for Adversarial Purification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16067
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#29420;&#31435;&#20110;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#20928;&#21270;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#30340;&#23545;&#25239;&#20928;&#21270;&#65288;AP&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26368;&#26377;&#21147;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23545;&#23545;&#25239;&#25915;&#20987;&#24182;&#19981;&#31283;&#20581;&#36825;&#19968;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25193;&#25955;&#36807;&#31243;&#24456;&#23481;&#26131;&#30772;&#22351;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#21453;&#21521;&#36807;&#31243;&#21518;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#20294;&#19982;&#21407;&#22987;&#36755;&#20837;&#22270;&#20687;&#23436;&#20840;&#19981;&#21516;&#65292;&#23548;&#33268;&#26631;&#20934;&#31934;&#24230;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#28982;&#32780;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#31105;&#27490;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#23545;&#25239;&#24341;&#23548;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#23427;&#29420;&#31435;&#20110;&#32473;&#23450;&#30340;&#39044;&#35757;&#32451;DMs&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;DMs&#12290;&#36825;&#31181;&#24378;&#22823;&#30340;&#24341;&#23548;&#19981;&#20165;&#21487;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#20928;&#21270;&#31034;&#20363;&#20445;&#30041;&#26356;&#22810;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#36824;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16067v1 Announce Type: cross  Abstract: Diffusion models (DMs) based adversarial purification (AP) has shown to be the most powerful alternative to adversarial training (AT). However, these methods neglect the fact that pre-trained diffusion models themselves are not robust to adversarial attacks as well. Additionally, the diffusion process can easily destroy semantic information and generate a high quality image but totally different from the original input image after the reverse process, leading to degraded standard accuracy. To overcome these issues, a natural idea is to harness adversarial training strategy to retrain or fine-tune the pre-trained diffusion model, which is computationally prohibitive. We propose a novel robust reverse process with adversarial guidance, which is independent of given pre-trained DMs and avoids retraining or fine-tuning the DMs. This robust guidance can not only ensure to generate purified examples retaining more semantic content but also m
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20998;&#23618;&#31070;&#32463;&#31526;&#21495;&#33539;&#24335;&#30340;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36339;&#27700;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#20174;&#35270;&#39057;&#25968;&#25454;&#20013;&#25552;&#21462;&#21487;&#35299;&#37322;&#31526;&#21495;&#24182;&#24212;&#29992;&#35268;&#21017;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#65292;&#20248;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#27169;&#22411;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#21160;&#20316;&#35782;&#21035;&#21644;&#26102;&#38388;&#20998;&#21106;&#65292;&#24182;&#29983;&#25104;&#35814;&#32454;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2403.13798</link><description>&lt;p&gt;
&#20998;&#23618;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#29992;&#20110;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hierarchical NeuroSymbolic Approach for Action Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13798
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20998;&#23618;&#31070;&#32463;&#31526;&#21495;&#33539;&#24335;&#30340;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36339;&#27700;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#20174;&#35270;&#39057;&#25968;&#25454;&#20013;&#25552;&#21462;&#21487;&#35299;&#37322;&#31526;&#21495;&#24182;&#24212;&#29992;&#35268;&#21017;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#65292;&#20248;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#27169;&#22411;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#21160;&#20316;&#35782;&#21035;&#21644;&#26102;&#38388;&#20998;&#21106;&#65292;&#24182;&#29983;&#25104;&#35814;&#32454;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#65288;AQA&#65289;&#24212;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#23450;&#37327;&#35780;&#20272;&#20154;&#31867;&#21160;&#20316;&#30340;&#34920;&#29616;&#25110;&#25191;&#34892;&#12290;&#24403;&#21069;&#30340;AQA&#26041;&#27861;&#26159;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#32570;&#20047;&#36879;&#26126;&#24230;&#24182;&#19988;&#26131;&#21463;&#20559;&#35265;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22522;&#20110;&#20027;&#35266;&#20154;&#31867;&#21028;&#26029;&#20316;&#20026;&#22320;&#38754;&#30495;&#30456;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;AQA&#30340;&#31070;&#32463;&#31526;&#21495;&#33539;&#24335;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;&#35270;&#39057;&#25968;&#25454;&#20013;&#25277;&#35937;&#20986;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#65292;&#24182;&#36890;&#36807;&#23558;&#35268;&#21017;&#24212;&#29992;&#20110;&#36825;&#20123;&#31526;&#21495;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#20197;&#36339;&#27700;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#39046;&#22495;&#19987;&#23478;&#26356;&#21916;&#27426;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#20854;&#27604;&#32431;&#31070;&#32463;&#26041;&#27861;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21160;&#20316;&#35782;&#21035;&#21644;&#26102;&#38388;&#20998;&#21106;&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#20102;&#19968;&#20221;&#35814;&#32454;&#25253;&#21578;&#65292;&#23558;&#36339;&#27700;&#20998;&#35299;&#20026;&#20854;&#20803;&#32032;&#65292;&#24182;&#25552;&#20379;&#24102;&#26377;&#35270;&#35273;&#35777;&#25454;&#30340;&#23458;&#35266;&#35780;&#20998;&#12290;&#32463;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13798v1 Announce Type: cross  Abstract: Action quality assessment (AQA) applies computer vision to quantitatively assess the performance or execution of a human action. Current AQA approaches are end-to-end neural models, which lack transparency and tend to be biased because they are trained on subjective human judgements as ground-truth. To address these issues, we introduce a neuro-symbolic paradigm for AQA, which uses neural networks to abstract interpretable symbols from video data and makes quality assessments by applying rules to those symbols. We take diving as the case study. We found that domain experts prefer our system and find it more informative than purely neural approaches to AQA in diving. Our system also achieves state-of-the-art action recognition and temporal segmentation, and automatically generates a detailed report that breaks the dive down into its elements and provides objective scoring with visual evidence. As verified by a group of domain experts, t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#65292;&#24182;&#21253;&#25324;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.06642</link><description>&lt;p&gt;
KELLMRec: &#30693;&#35782;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06642
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#65292;&#24182;&#21253;&#25324;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#34917;&#20805;&#20027;&#27969;&#22522;&#20110;ID&#30340;&#26041;&#27861;&#30340;&#32570;&#22833;&#37096;&#20998;&#12290;&#38543;&#30528;LLM&#30340;&#20852;&#36215;&#65292;&#23427;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#20026;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20351;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#25104;&#20026;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;LLM&#26469;&#22788;&#29702;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#26159;&#19981;&#21487;&#38752;&#21644;&#27425;&#20248;&#30340;&#65292;&#30001;&#20110;&#23384;&#22312;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;LLM&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#12290;&#21463;&#20197;&#19978;&#21160;&#26426;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;LLMRec&#26041;&#27861;&#12290;&#38500;&#20102;&#22312;&#25552;&#31034;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#29992;&#20110;&#35757;&#32451;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#20225;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06642v1 Announce Type: cross  Abstract: The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches. With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction. However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination. A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text. Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training. Experiments on public datasets and in-enter
&lt;/p&gt;</description></item><item><title>Ada-Tracker&#21033;&#29992;&#20809;&#27969;&#25429;&#25417;&#20687;&#32032;&#32423;&#32452;&#32455;&#21464;&#24418;&#24182;&#33258;&#36866;&#24212;&#32416;&#27491;&#36319;&#36394;&#27169;&#26495;&#65292;&#21516;&#26102;&#32467;&#21512;&#24103;&#38388;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#22312;&#25163;&#26415;&#22330;&#26223;&#20013;&#36719;&#32452;&#32455;&#36319;&#36394;&#38754;&#20020;&#30340;&#24418;&#29366;&#21644;&#22806;&#35266;&#25913;&#21464;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06479</link><description>&lt;p&gt;
Ada-Tracker&#65306;&#36890;&#36807;&#24103;&#38388;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#21305;&#37197;&#36827;&#34892;&#36719;&#32452;&#32455;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-Template Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06479
&lt;/p&gt;
&lt;p&gt;
Ada-Tracker&#21033;&#29992;&#20809;&#27969;&#25429;&#25417;&#20687;&#32032;&#32423;&#32452;&#32455;&#21464;&#24418;&#24182;&#33258;&#36866;&#24212;&#32416;&#27491;&#36319;&#36394;&#27169;&#26495;&#65292;&#21516;&#26102;&#32467;&#21512;&#24103;&#38388;&#21305;&#37197;&#21644;&#33258;&#36866;&#24212;&#27169;&#26495;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#22312;&#25163;&#26415;&#22330;&#26223;&#20013;&#36719;&#32452;&#32455;&#36319;&#36394;&#38754;&#20020;&#30340;&#24418;&#29366;&#21644;&#22806;&#35266;&#25913;&#21464;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#32452;&#32455;&#36319;&#36394;&#23545;&#35745;&#31639;&#26426;&#36741;&#21161;&#25163;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#27169;&#26495;&#21644;&#35270;&#39057;&#20013;&#25552;&#21462;&#36776;&#21035;&#29305;&#24449;&#26469;&#24674;&#22797;&#30456;&#24212;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#22312;&#25163;&#26415;&#22330;&#26223;&#20013;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#32452;&#32455;&#22312;&#25972;&#20010;&#25163;&#26415;&#36807;&#31243;&#20013;&#20250;&#25913;&#21464;&#24418;&#29366;&#21644;&#22806;&#35266;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#20809;&#27969;&#26469;&#33258;&#28982;&#25429;&#25417;&#20687;&#32032;&#32423;&#32452;&#32455;&#21464;&#24418;&#65292;&#24182;&#33258;&#36866;&#24212;&#24615;&#22320;&#32416;&#27491;&#36319;&#36394;&#27169;&#26495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23454;&#29616;&#19968;&#20010;&#24103;&#38388;&#21305;&#37197;&#26426;&#21046;&#65292;&#22522;&#20110;&#36830;&#32493;&#24103;&#20043;&#38388;&#30340;&#20809;&#27969;&#25552;&#21462;&#19968;&#20010;&#31895;&#30053;&#24863;&#20852;&#36259;&#21306;&#22495;&#12290;&#20026;&#20102;&#36866;&#24212;&#22806;&#35266;&#21464;&#21270;&#21644;&#20943;&#36731;&#28418;&#31227;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#26495;&#21305;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#26356;&#26032;&#36319;&#36394;&#27169;&#26495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;Ada-Tracker&#36890;&#36807;&#25429;&#25417;&#23616;&#37096;&#21464;&#24418;&#26469;&#20139;&#21463;&#30701;&#26399;&#21160;&#24577;&#24314;&#27169;&#21644;&#38271;&#26399;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06479v1 Announce Type: cross  Abstract: Soft tissue tracking is crucial for computer-assisted interventions. Existing approaches mainly rely on extracting discriminative features from the template and videos to recover corresponding matches. However, it is difficult to adopt these techniques in surgical scenes, where tissues are changing in shape and appearance throughout the surgery. To address this problem, we exploit optical flow to naturally capture the pixel-wise tissue deformations and adaptively correct the tracked template. Specifically, we first implement an inter-frame matching mechanism to extract a coarse region of interest based on optical flow from consecutive frames. To accommodate appearance change and alleviate drift, we then propose an adaptive-template matching method, which updates the tracked template based on the reliability of the estimates. Our approach, Ada-Tracker, enjoys both short-term dynamics modeling by capturing local deformations and long-ter
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.18945</link><description>&lt;p&gt;
Syntactic Ghost&#65306;&#19968;&#31181;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#26080;&#24863;&#30693;&#36890;&#29992;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18945
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#34987;&#21457;&#29616;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21487;&#20197;&#23558;&#28431;&#27934;&#36716;&#31227;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PLM&#21518;&#38376;&#25915;&#20987;&#37319;&#29992;&#26126;&#26174;&#30340;&#35302;&#21457;&#22120;&#65292;&#22312;&#25163;&#21160;&#23545;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#65292;&#22240;&#27492;&#22312;&#25928;&#26524;&#12289;&#38544;&#21311;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#26399;&#26395;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19981;&#21487;&#35265;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#65292;&#31216;&#20026;Syntactic Ghost&#65288;&#31616;&#31216;&#20026;synGhost&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#25932;&#24847;&#22320;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#39044;&#23450;&#20041;&#21477;&#27861;&#32467;&#26500;&#30340;&#27602;&#23475;&#26679;&#26412;&#20316;&#20026;&#38544;&#34109;&#35302;&#21457;&#22120;&#65292;&#28982;&#21518;&#23558;&#21518;&#38376;&#26893;&#20837;&#21040;&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#65292;&#32780;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#30693;&#35782;&#12290;&#27602;&#23475;&#26679;&#26412;&#30340;&#36755;&#20986;&#34920;&#31034;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23613;&#21487;&#33021;&#22343;&#21248;&#22320;&#20998;&#24067;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#24418;&#25104;&#24191;&#27867;&#30340;&#21518;&#38376;&#12290;&#27492;&#22806;&#65292;&#22312;&#20142;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18945v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks. However, existing PLM backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality. In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \textbf{Syntactic Ghost} (synGhost for short). Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to pre-trained representation space without disturbing the primitive knowledge. The output representations of poisoned samples are distributed as uniformly as possible in the feature space via contrastive learning, forming a wide range of backdoors. Additionally, in light 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.18815</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Handle Multilingualism?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18815
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#35821;&#35328;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;LLMs&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#36807;&#31243;&#65306;&#22312;&#21069;&#20960;&#23618;&#20013;&#65292;LLMs&#29702;&#35299;&#38382;&#39064;&#65292;&#23558;&#22810;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#33521;&#35821;&#20197;&#20415;&#20419;&#36827;&#20219;&#21153;&#35299;&#20915;&#38454;&#27573;&#12290;&#22312;&#20013;&#38388;&#23618;&#20013;&#65292;LLMs&#36890;&#36807;&#20197;&#33521;&#35821;&#24605;&#32771;&#24182;&#25972;&#21512;&#22810;&#35821;&#35328;&#30693;&#35782;&#26469;&#36827;&#34892;&#35299;&#20915;&#38382;&#39064;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32467;&#26500;&#65292;&#20998;&#21035;&#33719;&#21462;&#20107;&#23454;&#20869;&#23481;&#12290;&#22312;&#26368;&#21518;&#20960;&#23618;&#20013;&#65292;LLMs&#29983;&#25104;&#19982;&#26597;&#35810;&#30340;&#21407;&#22987;&#35821;&#35328;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#29305;&#23450;&#35821;&#35328;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#26816;&#27979;&#30001;&#36755;&#20837;&#35821;&#35328;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#65292;&#21363;&#20351;&#27809;&#26377;&#26631;&#31614;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#24182;&#34892;&#35821;&#35328;&#29305;&#23450;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18815v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DecisionNCE &#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#21644;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.18137</link><description>&lt;p&gt;
DecisionNCE: &#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DecisionNCE &#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#21644;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#19977;&#22823;&#30446;&#26631;&#65306;1&#65289;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#65307;2&#65289;&#24378;&#21270;&#35270;&#35273;&#34920;&#31034;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65307;3&#65289;&#25429;&#33719;&#36712;&#36857;&#32423;&#35821;&#35328;&#22522;&#30784;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#22823;&#37096;&#20998;&#24050;&#26377;&#26041;&#27861;&#36890;&#36807;&#19981;&#21516;&#30340;&#30446;&#26631;&#26469;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#65292;&#24448;&#24448;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#32479;&#19968;&#30446;&#26631;&#65292;&#21487;&#20197;&#21516;&#26102;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35821;&#35328;&#25351;&#20196;&#26080;&#32541;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#65292;&#22312;&#35270;&#35273;&#36712;&#36857;&#19982;&#20854;&#23545;&#24212;&#30340;&#35821;&#35328;&#25351;&#20196;&#30456;&#27604;&#19981;&#21305;&#37197;&#23545;&#26356;&#22909;&#22320;&#23545;&#40784;&#26102;&#65292;&#27969;&#34892;&#30340; Bradley-Terry &#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#22870;&#21169;&#37325;&#26032;&#21442;&#25968;&#21270;&#32780;&#21464;&#20026;&#34920;&#31034;&#23398;&#20064;&#12290;&#32467;&#26524;&#20135;&#29983;&#30340; DecisionNCE &#26694;&#26550;&#65292;&#31867;&#20284;&#20110; InfoNC
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18137v1 Announce Type: cross  Abstract: Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNC
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#31526;&#21495;&#21270;&#23398;&#20064;&#25216;&#26415;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#24037;&#20316;&#27969;&#29992;&#20110;&#25913;&#36827;&#20195;&#30721;&#27880;&#37322;&#25968;&#25454;&#29983;&#25104;&#21644;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#21463;&#25511;&#21512;&#25104;&#25968;&#25454;&#20462;&#22797;LLM&#29983;&#25104;&#20013;&#30340;&#24369;&#28857;&#65292;&#25552;&#39640;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20195;&#30721;&#27880;&#37322;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16910</link><description>&lt;p&gt;
NeSy&#29369;&#22312;&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#31526;&#21495;&#21270;&#26041;&#27861;&#29992;&#20110;&#25913;&#36827;&#20195;&#30721;&#27880;&#37322;&#25968;&#25454;&#29983;&#25104;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16910
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#31526;&#21495;&#21270;&#23398;&#20064;&#25216;&#26415;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#24037;&#20316;&#27969;&#29992;&#20110;&#25913;&#36827;&#20195;&#30721;&#27880;&#37322;&#25968;&#25454;&#29983;&#25104;&#21644;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#21463;&#25511;&#21512;&#25104;&#25968;&#25454;&#20462;&#22797;LLM&#29983;&#25104;&#20013;&#30340;&#24369;&#28857;&#65292;&#25552;&#39640;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20195;&#30721;&#27880;&#37322;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#65288;NeSy&#65289;&#24037;&#20316;&#27969;&#65292;&#23558;&#22522;&#20110;&#31526;&#21495;&#30340;&#23398;&#20064;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;C&#32534;&#31243;&#35821;&#35328;&#20013;&#20195;&#30721;&#27880;&#37322;&#20998;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#31181;&#24037;&#20316;&#27969;&#29983;&#25104;&#21463;&#25511;&#21512;&#25104;&#25968;&#25454;&#26469;&#20462;&#22797;LLM&#22522;&#20110;&#29983;&#25104;&#30340;&#19968;&#20123;&#26126;&#26174;&#24369;&#28857;&#65292;&#24182;&#25552;&#39640;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20195;&#30721;&#27880;&#37322;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#25968;&#25454;&#22686;&#24378;&#21518;&#23454;&#29616;&#20102;91.412&#65285;&#30340;Macro-F1&#20998;&#25968;&#65292;&#22686;&#21152;&#20102;1.033&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16910v1 Announce Type: cross  Abstract: We present a neuro-symbolic (NeSy) workflow combining a symbolic-based learning technique with a large language model (LLM) agent to generate synthetic data for code comment classification in the C programming language. We also show how generating controlled synthetic data using this workflow fixes some of the notable weaknesses of LLM-based generation and increases the performance of classical machine learning models on the code comment classification task. Our best model, a Neural Network, achieves a Macro-F1 score of 91.412% with an increase of 1.033% after data augmentation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;</title><link>https://arxiv.org/abs/2402.13380</link><description>&lt;p&gt;
&#36808;&#21521;&#21464;&#21387;&#22120;&#65306;&#29992;&#21464;&#21387;&#22120;&#24443;&#24213;&#25913;&#21464;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#23481;&#37327;&#38480;&#21046;&#25209;&#37327;&#29983;&#20135;&#38382;&#39064;&#65288;CLSP&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#21033;&#29992;&#21464;&#21387;&#22120;&#26469;&#39044;&#27979;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#22788;&#29702;&#39034;&#24207;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#38750;&#24120;&#36866;&#21512;&#39044;&#27979;&#27599;&#20010;CLSP&#21608;&#26399;&#20013;&#34920;&#31034;&#29983;&#20135;&#35774;&#32622;&#20915;&#31574;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#21160;&#24577;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;CLSP&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#21518;&#22788;&#29702;&#21464;&#21387;&#22120;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;CPLEX&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13380v1 Announce Type: new  Abstract: In this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP). Our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (MIP) problem. Specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP. This problem is inherently dynamic, and we need to handle sequential decision making under constraints. We present an efficient algorithm in which CLSP solutions are learned through a transformer neural network. The proposed post-processed transformer algorithm surpasses the state-of-the-art solver, CPLEX and Long Short-Term Memory (LSTM) in solution time
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#30340;&#19977;&#31181;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35268;&#21010;&#30340;&#32534;&#35793;&#25216;&#26415;&#65292;&#21487;&#20197;&#29983;&#25104;&#21160;&#20316;&#25104;&#26412;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.09877</link><description>&lt;p&gt;
&#35745;&#31639;&#20855;&#26377;&#32479;&#19968;&#21160;&#20316;&#25104;&#26412;&#30340;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
On Computing Plans with Uniform Action Costs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#30340;&#19977;&#31181;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35268;&#21010;&#30340;&#32534;&#35793;&#25216;&#26415;&#65292;&#21487;&#20197;&#29983;&#25104;&#21160;&#20316;&#25104;&#26412;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#35268;&#21010;&#24212;&#29992;&#20013;&#65292;&#20195;&#29702;&#20154;&#21487;&#33021;&#26377;&#20852;&#36259;&#25214;&#21040;&#21160;&#20316;&#25104;&#26412;&#23613;&#21487;&#33021;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;&#36825;&#26679;&#30340;&#35745;&#21010;&#20026;&#20195;&#29702;&#20154;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#65292;&#36825;&#22312;&#20154;&#31867;&#25191;&#34892;&#35268;&#21010;&#24037;&#20855;&#24314;&#35758;&#30340;&#35745;&#21010;&#26102;&#26159;&#20851;&#38190;&#29305;&#24449;&#12290;&#26412;&#25991;&#23558;&#19977;&#20010;&#19968;&#33268;&#24615;&#24230;&#37327;&#24212;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35268;&#21010;&#30340;&#32534;&#35793;&#25216;&#26415;&#65292;&#20801;&#35768;&#20197;&#21160;&#20316;&#25104;&#26412;&#24635;&#21644;&#21644;&#21160;&#20316;&#25104;&#26412;&#19968;&#33268;&#24615;&#36827;&#34892;&#35789;&#20856;&#25490;&#24207;&#26368;&#20248;&#21270;&#12290;&#22312;&#30693;&#21517;&#21644;&#26032;&#39062;&#30340;&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#37325;&#26500;&#30340;&#20219;&#21153;&#20197;&#29983;&#25104;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09877v1 Announce Type: new  Abstract: In many real-world planning applications, agents might be interested in finding plans whose actions have costs that are as uniform as possible. Such plans provide agents with a sense of stability and predictability, which are key features when humans are the agents executing plans suggested by planning tools. This paper adapts three uniformity metrics to automated planning, and introduce planning-based compilations that allow to lexicographically optimize sum of action costs and action costs uniformity. Experimental results both in well-known and novel planning benchmarks show that the reformulated tasks can be effectively solved in practice to generate uniform plans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#31163;&#36716;&#25442;&#32467;&#26500;&#21644;&#22870;&#21169;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#21518;&#32487;&#24230;&#37327;&#26469;&#25551;&#36848;&#34892;&#20026;&#30340;&#20998;&#24067;&#24335;&#21518;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.08530</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21518;&#32493;&#34920;&#31034;&#30340;&#20998;&#24067;&#24335;&#31867;&#27604;
&lt;/p&gt;
&lt;p&gt;
A Distributional Analogue to the Successor Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#31163;&#36716;&#25442;&#32467;&#26500;&#21644;&#22870;&#21169;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#21518;&#32487;&#24230;&#37327;&#26469;&#25551;&#36848;&#34892;&#20026;&#30340;&#20998;&#24067;&#24335;&#21518;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#36716;&#25442;&#32467;&#26500;&#21644;&#22870;&#21169;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#26126;&#30830;&#30340;&#20998;&#31163;&#12290;&#19982;&#21518;&#32493;&#34920;&#31034;&#65288;SR&#65289;&#25551;&#36848;&#25353;&#29031;&#32473;&#23450;&#31574;&#30053;&#34892;&#20026;&#30340;&#26399;&#26395;&#21518;&#26524;&#31867;&#20284;&#65292;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#21518;&#32487;&#24230;&#37327;&#65288;SM&#65289;&#25551;&#36848;&#20102;&#36825;&#31181;&#34892;&#20026;&#30340;&#20998;&#24067;&#24335;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#20998;&#24067;&#24335;SM&#26500;&#24314;&#20026;&#19968;&#20010;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#20998;&#24067;&#24335;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30456;&#20851;&#30340;&#29702;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20998;&#24067;&#24335;SM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#23618;&#27425;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#19968;&#20123;&#29420;&#31435;&#26377;&#20215;&#20540;&#30340;&#23398;&#20064;&#29366;&#24577;&#29983;&#25104;&#27169;&#22411;&#30340;&#31639;&#27861;&#25216;&#26415;&#12290;&#20316;&#20026;&#20998;&#24067;&#24335;SM&#26377;&#29992;&#24615;&#30340;&#20363;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20351;&#24471;&#38646;&#26679;&#26412;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#22312;&#20197;&#21069;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39564;&#35777;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39564;&#35777;&#22120;&#30340;&#21453;&#39304;&#21644;LLM&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#21512;&#25104;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#19968;&#32452;&#39564;&#35777;&#32534;&#31243;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#20855;&#26377;&#25554;&#20214;&#30340;ChatGPT4&#12290;</title><link>https://arxiv.org/abs/2402.08147</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#36827;&#34892;&#39564;&#35777;&#30340;&#22810;&#27493;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39564;&#35777;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39564;&#35777;&#22120;&#30340;&#21453;&#39304;&#21644;LLM&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#21512;&#25104;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#19968;&#32452;&#39564;&#35777;&#32534;&#31243;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#20855;&#26377;&#25554;&#20214;&#30340;ChatGPT4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22312;Dafny&#12289;Lean&#21644;Coq&#20013;&#39564;&#35777;&#30340;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;VMCTS&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#27493;&#39588;&#26816;&#26597;&#37096;&#20998;&#31243;&#24207;&#26469;&#21033;&#29992;&#25628;&#32034;&#31639;&#27861;&#20013;&#30340;&#39564;&#35777;&#22120;&#12290;&#32467;&#21512;LLM&#20808;&#39564;&#30693;&#35782;&#65292;&#39564;&#35777;&#22120;&#30340;&#21453;&#39304;&#25552;&#39640;&#20102;&#24320;&#28304;&#27169;&#22411;&#30340;&#21512;&#25104;&#33021;&#21147;&#12290;&#22312;&#19968;&#32452;&#20116;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#32534;&#31243;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#22235;&#20010;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#37325;&#26032;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#19968;&#23567;&#26102;&#30340;&#37325;&#26032;&#37319;&#26679;&#65292;&#22522;&#26412;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#32780;VMCTS&#21487;&#20197;&#22312;6&#20998;&#38047;&#20869;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#65292;&#22522;&#26412;&#27169;&#22411;&#21152;&#19978;VMCTS&#29978;&#33267;&#19982;&#20855;&#26377;&#25554;&#20214;&#21644;&#22810;&#27425;&#37325;&#35797;&#30340;ChatGPT4&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#21487;&#22312;https://github.com/namin/llm-verified-with-monte-carlo-tree-search&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq. Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step. In combination with the LLM prior, the verifier feedback raises the synthesis capabilities of open source models. On a set of five verified programming problems, we find that in four problems where the base model cannot solve the question even when re-sampling solutions for one hour, VMCTS can solve the problems within 6 minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented with plugins and multiple re-tries on these problems. Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search .
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#39046;&#22495;&#20013;&#32570;&#20047;&#32467;&#26500;&#21270;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07039</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21327;&#35843;&#25259;&#38706;&#65306;&#36229;&#36234;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Coordinated Disclosure for AI: Beyond Security Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07039
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#39046;&#22495;&#20013;&#32570;&#20047;&#32467;&#26500;&#21270;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20260;&#23475;&#25253;&#21578;&#22312;&#25259;&#38706;&#25110;&#35299;&#20915;&#31639;&#27861;&#32570;&#38519;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#31181;&#20020;&#26102;&#24615;&#30340;&#25805;&#20316;&#65292;&#32570;&#20047;&#32467;&#26500;&#21270;&#30340;&#36807;&#31243;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21327;&#35843;&#28431;&#27934;&#25259;&#38706;&#65288;CVD&#65289;&#30340;&#20262;&#29702;&#21644;&#29983;&#24577;&#31995;&#32479;&#22312;&#36719;&#20214;&#23433;&#20840;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#32654;&#22269;&#30340;&#32972;&#26223;&#19979;&#65292;&#20026;&#20102;&#40723;&#21169;&#31177;&#25345;&#21892;&#24847;&#34892;&#20107;&#30340;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#65292;&#24314;&#31435;&#19968;&#20010;&#23433;&#20840;&#38450;&#25252;&#26465;&#27454;&#20197;&#23545;&#25239;&#35745;&#31639;&#26426;&#27450;&#35784;&#21644;&#28389;&#29992;&#27861;&#26696;&#19968;&#30452;&#23384;&#22312;&#38271;&#26399;&#30340;&#27861;&#24459;&#21644;&#25919;&#31574;&#26007;&#20105;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20013;&#30340;&#31639;&#27861;&#32570;&#38519;&#19982;&#20256;&#32479;&#36719;&#20214;&#28431;&#27934;&#23384;&#22312;&#30528;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#19968;&#31181;&#19987;&#38376;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#29305;&#27530;&#22797;&#26434;&#24615;&#30340;&#19987;&#38376;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#30340;&#23454;&#26045;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;ML&#20013;&#30340;&#25259;&#38706;&#21382;&#21490;&#32972;&#26223;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Harm reporting in the field of Artificial Intelligence (AI) currently operates on an ad hoc basis, lacking a structured process for disclosing or addressing algorithmic flaws. In contrast, the Coordinated Vulnerability Disclosure (CVD) ethos and ecosystem play a pivotal role in software security and transparency. Within the U.S. context, there has been a protracted legal and policy struggle to establish a safe harbor from the Computer Fraud and Abuse Act, aiming to foster institutional support for security researchers acting in good faith. Notably, algorithmic flaws in Machine Learning (ML) models present distinct challenges compared to traditional software vulnerabilities, warranting a specialized approach. To address this gap, we propose the implementation of a dedicated Coordinated Flaw Disclosure (CFD) framework tailored to the intricacies of machine learning and artificial intelligence issues. This paper delves into the historical landscape of disclosures in ML, encompassing the a
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;S3L&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#25972;&#20010;&#20999;&#29255;&#30340;&#34920;&#31034;&#12290;&#23427;&#32467;&#21512;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#37197;&#23545;&#35270;&#22270;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;WSI&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.06188</link><description>&lt;p&gt;
&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#25972;&#20010;&#20999;&#29255;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A self-supervised framework for learning whole slide representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06188
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;S3L&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#25972;&#20010;&#20999;&#29255;&#30340;&#34920;&#31034;&#12290;&#23427;&#32467;&#21512;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#37197;&#23545;&#35270;&#22270;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;WSI&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#20010;&#20999;&#29255;&#25104;&#20687;&#23545;&#20110;&#29983;&#29289;&#21307;&#23398;&#26174;&#24494;&#38236;&#21644;&#35745;&#31639;&#30149;&#29702;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#21315;&#20806;&#20687;&#32032;&#30340;&#22823;&#23567;&#12289;&#22810;&#26679;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#29305;&#24449;&#12289;&#31354;&#38388;&#24322;&#36136;&#24615;&#20197;&#21450;&#26377;&#38480;&#30340;/&#19981;&#23384;&#22312;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687; (WSIs) &#26500;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#31361;&#26174;&#20102;&#20165;&#20381;&#38752;&#30417;&#30563;&#35757;&#32451;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#25972;&#20010;&#20999;&#29255;&#34920;&#31034;&#12290;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21487;&#20197;&#20026;&#19979;&#28216;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;&#30284;&#30151;&#35786;&#26029;&#25110;&#20998;&#23376;&#36951;&#20256;&#39044;&#27979;&#65289;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;WSI&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#25972;&#20010;&#20999;&#29255;&#23398;&#20064;&#65288;S3L&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21315;&#20806;&#20687;&#32032;&#35268;&#27169;&#30340;WSI&#33258;&#30417;&#30563;&#12290;S3L&#23558;&#26469;&#33258;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#25968;&#25454;&#36716;&#25442;&#31574;&#30053;&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#33258;&#30417;&#30563;&#30340;&#37197;&#23545;&#35270;&#22270;&#12290;S3L&#21033;&#29992;&#20869;&#22312;&#30340;&#21306;&#22495;&#24322;&#36136;&#24615;&#12289;&#32452;&#32455;&#23398;&#29305;&#24449;&#30340;&#21487;&#21464;&#24615;&#21644;&#20449;&#24687;&#20887;&#20313;&#24615;
&lt;/p&gt;
&lt;p&gt;
Whole slide imaging is fundamental to biomedical microscopy and computational pathology. However, whole slide images (WSIs) present a complex computer vision challenge due to their gigapixel size, diverse histopathologic features, spatial heterogeneity, and limited/absent data annotations. These challenges highlight that supervised training alone can result in suboptimal whole slide representations. Self-supervised representation learning can achieve high-quality WSI visual feature learning for downstream diagnostic tasks, such as cancer diagnosis or molecular genetic prediction. Here, we present a general self-supervised whole slide learning (S3L) framework for gigapixel-scale self-supervision of WSIs. S3L combines data transformation strategies from transformer-based vision and language modeling into a single unified framework to generate paired views for self-supervision. S3L leverages the inherent regional heterogeneity, histologic feature variability, and information redundancy wi
&lt;/p&gt;</description></item><item><title>Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06187</link><description>&lt;p&gt;
Premier-TACO: &#36890;&#36807;&#26102;&#38388;&#39537;&#21160;&#30340;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#22810;&#20219;&#21153;&#34920;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06187
&lt;/p&gt;
&lt;p&gt;
Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Premier-TACO&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#23569;&#26679;&#26412;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;Premier-TACO&#21033;&#29992;&#19968;&#37096;&#20998;&#22810;&#20219;&#21153;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#29305;&#24449;&#34920;&#31034;&#25429;&#25417;&#20102;&#20851;&#38190;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#25512;&#21160;&#20102;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#65288;TACO&#65289;&#30446;&#26631;&#30340;&#21457;&#23637;&#65292;TACO&#22312;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#26174;&#33879;&#25552;&#39640;TACO&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20351;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#31163;&#32447;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;Deepmind Control Suite&#12289;MetaWorld&#21644;LIBERO&#22312;&#20869;&#30340;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;Premier-TACO&#22312;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO's computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of nove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05967</link><description>&lt;p&gt;
&#26368;&#21518;&#20043;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The last Dance : Robust backdoor attack via diffusion models and bayesian approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#22122;&#38899;&#21644;&#21435;&#22122;&#30340;&#26041;&#24335;&#23398;&#20064;&#27491;&#21521;&#21644;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#29702;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#26088;&#22312;&#27450;&#39575;&#22522;&#20110;&#38899;&#39057;&#30340;DNN&#27169;&#22411;&#65292;&#20363;&#22914;Hugging Face&#26694;&#26550;&#20013;&#30340;&#38899;&#39057;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33410;&#30465;&#26102;&#38388;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Hugging Face&#25512;&#23548;&#20986;&#30340;&#38899;&#39057;Transformer&#19978;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#65288;&#31216;&#20026;`BacKBayDiffMod`&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#20013;&#24320;&#21457;&#30340;&#21518;&#38376;&#25915;&#20987;&#22522;&#20110;&#27602;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28041;&#21450;&#21518;&#38376;&#25193;&#25955;&#37319;&#26679;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20998;&#24067;&#30340;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#65292;&#20174;&#32780;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02992</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Decoding-time Realignment of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#65292;&#20174;&#32780;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#23545;&#20110;&#20943;&#23569;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#21644;&#20559;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#22312;&#20154;&#31867;&#20559;&#22909;&#22870;&#21169;&#21644;&#40723;&#21169;&#20445;&#25345;&#19982;&#26410;&#23545;&#40784;&#27169;&#22411;&#25509;&#36817;&#30340;&#25509;&#36817;&#24615;&#35268;&#21017;&#39033;&#20043;&#38388;&#36827;&#34892;&#20248;&#21270;&#30340;&#26435;&#34913;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#35268;&#21017;&#21270;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#65306;&#35268;&#21017;&#21270;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#22870;&#21169;&#27450;&#39575;&#32780;&#38477;&#20302;&#27169;&#22411;&#33021;&#21147;&#65292;&#32780;&#36807;&#24230;&#35268;&#21017;&#21270;&#21017;&#38459;&#30861;&#23545;&#40784;&#12290;&#20256;&#32479;&#26041;&#27861;&#25214;&#21040;&#26368;&#20339;&#35268;&#21017;&#21270;&#27700;&#24179;&#38656;&#35201;&#20351;&#29992;&#19981;&#21516;&#35268;&#21017;&#21270;&#24378;&#24230;&#37325;&#26032;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#26469;&#35828;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#30721;&#26102;&#38388;&#23545;&#40784;&#65288;DeRa&#65289;&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#21644;&#35780;&#20272;&#19981;&#21516;&#30340;&#35268;&#21017;&#21270;&#24378;&#24230;&#12290;DeRa&#21487;&#20197;&#23545;&#23545;&#40784;&#27169;&#22411;&#30340;&#31243;&#24230;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of al
&lt;/p&gt;</description></item><item><title>MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02342</link><description>&lt;p&gt;
MetaOptimize&#65306;&#19968;&#20010;&#20248;&#21270;&#27493;&#38271;&#21644;&#20854;&#20182;&#20803;&#21442;&#25968;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02342
&lt;/p&gt;
&lt;p&gt;
MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20248;&#21270;&#20803;&#21442;&#25968;&#65288;&#21363;&#36229;&#21442;&#25968;&#65289;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MetaOptimize&#26694;&#26550;&#65292;&#25670;&#33073;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#20256;&#32479;&#20803;&#21442;&#25968;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#20803;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#27493;&#38271;&#65288;&#20063;&#31216;&#20026;&#23398;&#20064;&#29575;&#65289;&#65292;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaOptimize&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26102;&#35843;&#25972;&#27493;&#38271;&#65292;&#36890;&#36807;&#26410;&#26469;&#25439;&#22833;&#30340;&#25240;&#29616;&#24635;&#21644;&#26469;&#26368;&#23567;&#21270;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;MetaOptimize&#30340;&#20302;&#22797;&#26434;&#24230;&#21464;&#20307;&#65292;&#32467;&#21512;&#20854;&#36866;&#24212;&#22810;&#20010;&#20248;&#21270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#65292;&#26088;&#22312;&#35299;&#37322;&#29305;&#23450;&#35270;&#39057;&#20013;&#20154;&#20204;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;SMILE&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2312.09818</link><description>&lt;p&gt;
SMILE&#65306;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#35270;&#39057;&#20013;&#30340;&#31505;&#22768;
&lt;/p&gt;
&lt;p&gt;
SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#65292;&#26088;&#22312;&#35299;&#37322;&#29305;&#23450;&#35270;&#39057;&#20013;&#20154;&#20204;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;SMILE&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26500;&#24314;&#31038;&#20132;&#26234;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20854;&#20013;&#65292;&#31505;&#22768;&#26159;&#20154;&#31867;&#31038;&#20132;&#20114;&#21160;&#20013;&#21457;&#29983;&#30340;&#29420;&#29305;&#34920;&#36798;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38754;&#23545;&#20102;&#26426;&#22120;&#29702;&#35299;&#35270;&#39057;&#20013;&#31505;&#22768;&#32972;&#21518;&#29702;&#30001;&#30340;&#26032;&#25361;&#25112;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#19968;&#26032;&#20219;&#21153;&#65292;&#35299;&#37322;&#20154;&#20204;&#22312;&#29305;&#23450;&#35270;&#39057;&#20013;&#20026;&#20160;&#20040;&#20250;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;SMILE&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#32447;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#21487;&#20197;&#29983;&#25104;&#21487;&#20449;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#22522;&#32447;&#22312;&#25506;&#27979;&#20854;&#20182;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#21644;&#37326;&#22806;&#35270;&#39057;&#26041;&#38754;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09818v2 Announce Type: replace-cross  Abstract: Despite the recent advances of the artificial intelligence, building social intelligence remains a challenge. Among social signals, laughter is one of the distinctive expressions that occurs during social interactions between humans. In this work, we tackle a new challenge for machines to understand the rationale behind laughter in video, Video Laugh Reasoning. We introduce this new task to explain why people laugh in a particular video and a dataset for this task. Our proposed dataset, SMILE, comprises video clips and language descriptions of why people laugh. We propose a baseline by leveraging the reasoning capacity of large language models (LLMs) with textual video representation. Experiments show that our baseline can generate plausible explanations for laughter. We further investigate the scalability of our baseline by probing other video understanding tasks and in-the-wild videos. We release our dataset, code, and model 
&lt;/p&gt;</description></item><item><title>&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.08154</link><description>&lt;p&gt;
&#20877;&#38382;&#19968;&#27425;&#65306;&#33258;&#19968;&#33268;&#24615;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#65288;&#20960;&#20046;&#65289;&#25152;&#26377;&#22330;&#26223;&#20013;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08154
&lt;/p&gt;
&lt;p&gt;
&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;CoT&#25552;&#31034;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;&#36138;&#23146;&#35299;&#30721;&#20250;&#23548;&#33268;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#65292;&#38598;&#25104;&#20248;&#21270;&#23581;&#35797;&#33719;&#24471;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#20197;&#24471;&#21040;&#26368;&#32456;&#31572;&#26696;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#35201;&#20040;&#31616;&#21333;&#22320;&#37319;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#21518;&#22788;&#29702;&#65292;&#27604;&#22914;&#8220;&#33258;&#19968;&#33268;&#24615;&#8221;&#65292;&#35201;&#20040;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#20960;&#20010;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20154;&#31867;&#27880;&#37322;&#30340;&#38468;&#21152;&#27169;&#22411;&#26469;&#22312;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#20013;&#36873;&#25321;&#26368;&#20339;&#36335;&#24452;&#65292;&#20294;&#26410;&#33021;&#25512;&#24191;&#21040;&#29616;&#23454;&#35774;&#32622;&#65292;&#20854;&#20013;&#36755;&#20837;&#38382;&#39064;&#31867;&#22411;&#26410;&#30693;&#25110;&#25512;&#29702;&#36335;&#24452;&#30340;&#31572;&#26696;&#26684;&#24335;&#26410;&#30693;&#12290;&#20026;&#20102;&#36991;&#20813;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#19968;&#33268;&#24615;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#36866;&#29992;&#65292;&#20854;&#20013;&#36755;&#20837;&#38382;&#39064;&#30340;&#31867;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08154v2 Announce Type: replace-cross  Abstract: Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \textit{self-consistency}, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose \textbf{Self-Agreement}, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input question
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25361;&#25112;-&#21709;&#24212;&#26041;&#24335;&#65292;&#38024;&#23545;AI Real-Time Deepfakes&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#23454;&#26102;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2210.06186</link><description>&lt;p&gt;
GOTCHA&#65306;&#36890;&#36807;&#25361;&#25112;-&#21709;&#24212;&#23454;&#29616;&#23454;&#26102;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GOTCHA: Real-Time Video Deepfake Detection via Challenge-Response
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06186
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25361;&#25112;-&#21709;&#24212;&#26041;&#24335;&#65292;&#38024;&#23545;AI Real-Time Deepfakes&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#23454;&#26102;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI-enabled Real-Time Deepfakes&#65288;RTDFs&#65289;&#30340;&#20852;&#36215;&#65292;&#22312;&#32447;&#35270;&#39057;&#20114;&#21160;&#30340;&#23436;&#25972;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#26085;&#30410;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;RTDFs&#29616;&#22312;&#20351;&#24471;&#22312;&#23454;&#26102;&#35270;&#39057;&#20114;&#21160;&#20013;&#23558;&#20882;&#21517;&#39030;&#26367;&#32773;&#30340;&#33080;&#26367;&#25442;&#20026;&#20854;&#21463;&#23475;&#32773;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#28145;&#24230;&#20266;&#36896;&#30340;&#36827;&#27493;&#20063;&#20419;&#20351;&#26816;&#27979;&#36798;&#21040;&#21516;&#26679;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#25216;&#26415;&#26159;&#24322;&#27493;&#30340;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;RTDFs&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#24314;&#31435;&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;-&#21709;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35762;&#35805;&#22836;&#37096;&#39118;&#26684;&#30340;&#35270;&#39057;&#20114;&#21160;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;RTDF&#29983;&#25104;&#31649;&#36947;&#22266;&#26377;&#38480;&#21046;&#30340;&#25361;&#25112;&#20998;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19968;&#20010;&#21253;&#21547;&#20843;&#20010;&#25361;&#25112;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#20998;&#31867;&#20013;&#30340;&#20195;&#34920;&#24615;&#31034;&#20363;&#65292;&#36825;&#20123;&#25361;&#25112;&#19968;&#33268;&#21644;&#26126;&#26174;&#22320;&#38477;&#20302;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#20266;&#36896;&#29983;&#25104;&#22120;&#30340;&#36136;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#24471;&#21040;&#20102;&#20154;&#31867;&#30340;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.06186v3 Announce Type: replace-cross  Abstract: With the rise of AI-enabled Real-Time Deepfakes (RTDFs), the integrity of online video interactions has become a growing concern. RTDFs have now made it feasible to replace an imposter's face with their victim in live video interactions. Such advancement in deepfakes also coaxes detection to rise to the same standard. However, existing deepfake detection techniques are asynchronous and hence ill-suited for RTDFs. To bridge this gap, we propose a challenge-response approach that establishes authenticity in live settings. We focus on talking-head style video interaction and present a taxonomy of challenges that specifically target inherent limitations of RTDF generation pipelines. We evaluate representative examples from the taxonomy by collecting a unique dataset comprising eight challenges, which consistently and visibly degrades the quality of state-of-the-art deepfake generators. These results are corroborated both by humans 
&lt;/p&gt;</description></item><item><title>BlockFusion&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#21644;&#22806;&#25512;&#25216;&#26415;&#29983;&#25104;&#19977;&#32500;&#22330;&#26223;&#30340;&#27169;&#22411;&#65292;&#33021;&#26080;&#32541;&#22320;&#28155;&#21152;&#26032;&#30340;&#22359;&#20197;&#25193;&#23637;&#22330;&#26223;&#12290;&#37319;&#29992;&#28151;&#21512;&#31070;&#32463;&#22330;&#21644;&#28508;&#22312;&#19977;&#24179;&#38754;&#31354;&#38388;&#26469;&#20445;&#35777;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.17053</link><description>&lt;p&gt;
BlockFusion: &#20351;&#29992;&#28508;&#22312;&#19977;&#24179;&#38754;&#22806;&#25512;&#25193;&#23637;&#30340;&#21487;&#25193;&#23637;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation. (arXiv:2401.17053v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17053
&lt;/p&gt;
&lt;p&gt;
BlockFusion&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#21644;&#22806;&#25512;&#25216;&#26415;&#29983;&#25104;&#19977;&#32500;&#22330;&#26223;&#30340;&#27169;&#22411;&#65292;&#33021;&#26080;&#32541;&#22320;&#28155;&#21152;&#26032;&#30340;&#22359;&#20197;&#25193;&#23637;&#22330;&#26223;&#12290;&#37319;&#29992;&#28151;&#21512;&#31070;&#32463;&#22330;&#21644;&#28508;&#22312;&#19977;&#24179;&#38754;&#31354;&#38388;&#26469;&#20445;&#35777;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;BlockFusion&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#20197;&#21333;&#20301;&#22359;&#24418;&#24335;&#29983;&#25104;&#19977;&#32500;&#22330;&#26223;&#65292;&#24182;&#26080;&#32541;&#22320;&#28155;&#21152;&#26032;&#30340;&#22359;&#20197;&#25193;&#23637;&#22330;&#26223;&#12290;BlockFusion&#20351;&#29992;&#20174;&#23436;&#25972;&#30340;&#19977;&#32500;&#22330;&#26223;&#20013;&#38543;&#26426;&#35009;&#21098;&#30340;3D&#22359;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#22359;&#25311;&#21512;&#65292;&#23558;&#25152;&#26377;&#35757;&#32451;&#22359;&#36716;&#25442;&#20026;&#28151;&#21512;&#31070;&#32463;&#22330;&#65306;&#23427;&#21253;&#21547;&#20960;&#20309;&#29305;&#24449;&#30340;&#19977;&#24179;&#38754;&#65292;&#20197;&#21450;&#29992;&#20110;&#35299;&#30721;&#26377;&#31526;&#21495;&#36317;&#31163;&#20540;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#12290;&#37319;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#19977;&#24179;&#38754;&#21387;&#32553;&#21040;&#28508;&#22312;&#19977;&#24179;&#38754;&#31354;&#38388;&#65292;&#24182;&#22312;&#20854;&#19978;&#25191;&#34892;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;&#23545;&#28508;&#22312;&#34920;&#31034;&#24212;&#29992;&#25193;&#25955;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#12290;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25193;&#23637;&#22330;&#26223;&#26102;&#65292;&#21482;&#38656;&#23558;&#31354;&#22359;&#28155;&#21152;&#21040;&#19982;&#24403;&#21069;&#22330;&#26223;&#37325;&#21472;&#65292;&#24182;&#22806;&#25512;&#29616;&#26377;&#30340;&#28508;&#22312;&#19977;&#24179;&#38754;&#20197;&#22635;&#20805;&#26032;&#22359;&#12290;&#22806;&#25512;&#36807;&#31243;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#32422;&#26463;&#26469;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation. To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature 
&lt;/p&gt;</description></item><item><title>&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.10119</link><description>&lt;p&gt;
&#36208;&#21521;&#22522;&#20110;&#21407;&#21017;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10119
&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;k&#32500;Weisfeiler-Leman&#65288;k-WL&#65289;&#23618;&#27425;&#32467;&#26500;&#30340;&#22270;&#24418;&#23398;&#20064;&#26550;&#26500;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#24456;&#22909;&#29702;&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26550;&#26500;&#22312;&#30495;&#23454;&#20219;&#21153;&#20013;&#24448;&#24448;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24433;&#21709;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#22914;&#22270;&#24418;&#21464;&#25442;&#22120;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#23558;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;k-WL&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#22240;&#20026;&#36825;&#20123;&#26550;&#26500;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#26469;&#23454;&#29616;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#36793;&#32536;&#21464;&#25442;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#33410;&#28857;&#23545;&#32780;&#19981;&#26159;&#33410;&#28857;&#19978;&#36827;&#34892;&#25805;&#20316;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36793;&#32536;&#21464;&#25442;&#22120;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29702;&#35770;&#23545;&#40784;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#12290;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;MOCO&#24494;&#35843;&#30340;ResNet50&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.09243</link><description>&lt;p&gt;
DiffClone: &#20351;&#29992;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#26426;&#22120;&#20154;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning. (arXiv:2401.09243v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#12290;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;MOCO&#24494;&#35843;&#30340;ResNet50&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#23494;&#38598;&#19988;&#30828;&#20214;&#29305;&#23450;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#25805;&#20316;&#20195;&#29702;&#65292;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#24335;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;Train-Offline-Test-Online&#65288;TOTO&#65289;&#22522;&#20934;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#24320;&#28304;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#30001;&#19987;&#23478;&#25968;&#25454;&#32452;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#24120;&#35265;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#22522;&#20934;&#20998;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#22312;NeurIPS 2023&#20030;&#21150;&#30340;Train-Offline-Test-Online&#65288;TOTO&#65289;&#22522;&#20934;&#25361;&#25112;&#36187;&#20013;&#30340;&#23448;&#26041;&#25552;&#20132;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#20195;&#29702;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MOCO&#24494;&#35843;&#30340;ResNet50&#30456;&#27604;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning tasks are extremely compute-intensive and hardware-specific. Thus the avenues of tackling these challenges, using a diverse dataset of offline demonstrations that can be used to train robot manipulation agents, is very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a well-curated open-source dataset for offline training comprised mostly of expert data and also benchmark scores of the common offline-RL and behaviour cloning agents. In this paper, we introduce DiffClone, an offline algorithm of enhanced behaviour cloning agent with diffusion-based policy learning, and measured the efficacy of our method on real online physical robots at test time. This is also our official submission to the Train-Offline-Test-Online (TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both pre-trained visual representation and agent policies. In our experiments, we find that MOCO finetuned ResNet50 performs the best in comparison to other finetuned
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01008</link><description>&lt;p&gt;
Text-to-image diffusion models&#20013;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fast Inference Through The Reuse Of Attention Maps In Diffusion Models. (arXiv:2401.01008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#28789;&#27963;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#21333;&#20010;&#22270;&#20687;&#25152;&#38656;&#30340;&#36845;&#20195;&#36807;&#31243;&#26082;&#26114;&#36149;&#21448;&#20855;&#26377;&#36739;&#39640;&#30340;&#24310;&#36831;&#65292;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#30740;&#31350;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#37319;&#26679;&#27493;&#38271;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#37325;&#22797;&#35745;&#31639;&#27880;&#24847;&#21147;&#26144;&#23556;&#26082;&#32791;&#26102;&#21448;&#20887;&#20313;&#65292;&#22240;&#27492;&#25105;&#20204;&#24314;&#35758;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#32467;&#26500;&#21270;&#22320;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#37325;&#29992;&#31574;&#30053;&#21463;&#21040;&#21021;&#32423;ODE&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#35813;&#29702;&#35770;&#35748;&#20026;&#22312;&#37319;&#26679;&#36807;&#31243;&#30340;&#21518;&#26399;&#37325;&#29992;&#26368;&#21512;&#36866;&#12290;&#22312;&#27880;&#24847;&#21040;&#36825;&#31181;&#29702;&#35770;&#26041;&#27861;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models have demonstrated unprecedented abilities at flexible and realistic image synthesis. However, the iterative process required to produce a single image is costly and incurs a high latency, prompting researchers to further investigate its efficiency. Typically, improvements in latency have been achieved in two ways: (1) training smaller models through knowledge distillation (KD); and (2) adopting techniques from ODE-theory to facilitate larger step sizes. In contrast, we propose a training-free approach that does not alter the step-size of the sampler. Specifically, we find the repeated calculation of attention maps to be both costly and redundant; therefore, we propose a structured reuse of attention maps during sampling. Our initial reuse policy is motivated by rudimentary ODE-theory, which suggests that reuse is most suitable late in the sampling procedure. After noting a number of limitations in this theoretical approach, we empirically search for a bet
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24536;&#35760;&#38544;&#31169;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#12290;P2F&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#29255;&#27573;&#24182;&#29983;&#25104;&#34394;&#26500;&#31572;&#26696;&#65292;&#27169;&#31946;&#21270;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#20351;&#29992;&#65292;&#26080;&#38656;&#25163;&#21160;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2401.00870</link><description>&lt;p&gt;
&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24536;&#35760;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Teach Large Language Models to Forget Privacy. (arXiv:2401.00870v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24536;&#35760;&#38544;&#31169;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#12290;P2F&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#29255;&#27573;&#24182;&#29983;&#25104;&#34394;&#26500;&#31572;&#26696;&#65292;&#27169;&#31946;&#21270;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#20351;&#29992;&#65292;&#26080;&#38656;&#25163;&#21160;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20445;&#25252;&#38544;&#31169;&#26041;&#27861;&#65292;&#22914;&#24046;&#20998;&#38544;&#31169;&#21644;&#21516;&#24577;&#21152;&#23494;&#65292;&#22312;&#21482;&#26377;&#40657;&#30418;API&#30340;&#29615;&#22659;&#19979;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#35201;&#27714;&#27169;&#22411;&#36879;&#26126;&#24615;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#24536;&#35760;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#23436;&#25972;&#38382;&#39064;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#29255;&#27573;&#65292;&#29983;&#25104;&#34394;&#26500;&#30340;&#31572;&#26696;&#65292;&#24182;&#20351;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#27169;&#31946;&#21270;&#12290;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#39046;&#22495;&#30340;&#21253;&#21547;&#38544;&#31169;&#25935;&#24863;&#20449;&#24687;&#30340;&#38382;&#39064;&#21019;&#24314;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;P2F&#23454;&#29616;&#20102;&#38646;-shot&#27867;&#21270;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#65292;&#26080;&#38656;&#25163;&#21160;&#35843;&#25972;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;LLM&#35760;&#24518;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#20219;&#20309;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven powerful, but the risk of privacy leakage remains a significant concern. Traditional privacy-preserving methods, such as Differential Privacy and Homomorphic Encryption, are inadequate for black-box API-only settings, demanding either model transparency or heavy computational resources. We propose Prompt2Forget (P2F), the first framework designed to tackle the LLM local privacy challenge by teaching LLM to forget. The method involves decomposing full questions into smaller segments, generating fabricated answers, and obfuscating the model's memory of the original input. A benchmark dataset was crafted with questions containing privacy-sensitive information from diverse fields. P2F achieves zero-shot generalization, allowing adaptability across a wide range of use cases without manual adjustments. Experimental results indicate P2F's robust capability to obfuscate LLM's memory, attaining a forgetfulness score of around 90\% without any utility los
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.10107</link><description>&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs. (arXiv:2310.10107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20110;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#65292;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#30340;&#23398;&#20064;&#30001;&#20110;&#35266;&#23519;&#25968;&#25454;&#38590;&#20197;&#35299;&#35835;&#32780;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#30693;&#36716;&#31227;&#21644;&#35266;&#27979;&#27169;&#22411;&#30340;POMDPs&#20013;&#30340;&#24207;&#21015;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#21518;&#39564;&#37319;&#26679;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PSRL&#65289;&#22312;POMDPs&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35777;&#26126;&#20854;&#36125;&#21494;&#26031;&#36951;&#25022;&#38543;&#30528;&#24207;&#21015;&#30340;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#32780;&#32553;&#23567;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36951;&#25022;&#38543;&#30528;&#26102;&#38388;&#38271;&#24230;$H$&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#19979;&#30028;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;POMDP&#26159;&#27424;&#23436;&#22791;&#19988;&#24369;&#21487;&#35782;&#21035;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#30456;&#27604;&#20110;arXiv:2204.08967&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#25913;&#36827;&#20102;&#36951;&#25022;&#30028;&#32422;$\Omega(H^2\sqrt{SA})$&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to Markov Decision Processes (MDPs), learning in Partially Observable Markov Decision Processes (POMDPs) can be significantly harder due to the difficulty of interpreting observations. In this paper, we consider episodic learning problems in POMDPs with unknown transition and observation models. We consider the Posterior Sampling-based Reinforcement Learning (PSRL) algorithm for POMDPs and show that its Bayesian regret scales as the square root of the number of episodes. In general, the regret scales exponentially with the horizon length $H$, and we show that this is inevitable by providing a lower bound. However, under the condition that the POMDP is undercomplete and weakly revealing, we establish a polynomial Bayesian regret bound that improves the regret bound by a factor of $\Omega(H^2\sqrt{SA})$ over the recent result by arXiv:2204.08967.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#31354;&#38388;-&#26102;&#38388;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#21644;&#20248;&#21270;&#27169;&#22359;&#65292;&#26088;&#22312;&#30740;&#31350;&#21644;&#25913;&#36827;&#20026;&#36135;&#36816;&#21345;&#36710;&#25552;&#20379;&#30005;&#27744;&#26356;&#25442;&#26381;&#21153;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20998;&#26512;&#37325;&#22411;&#21345;&#36710;&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#39044;&#27979;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#26469;&#20915;&#31574;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.04440</link><description>&lt;p&gt;
&#20026;&#36135;&#36816;&#21345;&#36710;&#25552;&#20379;&#30005;&#27744;&#26356;&#25442;&#26381;&#21153;&#30340;&#31354;&#38388;-&#26102;&#38388;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Facilitating Battery Swapping Services for Freight Trucks with Spatial-Temporal Demand Prediction. (arXiv:2310.04440v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#31354;&#38388;-&#26102;&#38388;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#21644;&#20248;&#21270;&#27169;&#22359;&#65292;&#26088;&#22312;&#30740;&#31350;&#21644;&#25913;&#36827;&#20026;&#36135;&#36816;&#21345;&#36710;&#25552;&#20379;&#30005;&#27744;&#26356;&#25442;&#26381;&#21153;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20998;&#26512;&#37325;&#22411;&#21345;&#36710;&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#39044;&#27979;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#26469;&#20915;&#31574;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27668;&#21270;&#37325;&#22411;&#21345;&#36710;&#20026;&#23454;&#29616;&#30899;&#20013;&#21644;&#30340;&#26410;&#26469;&#25552;&#20379;&#20102;&#37325;&#35201;&#26426;&#36935;&#65292;&#28982;&#32780;&#26377;&#38480;&#30005;&#27744;&#33021;&#37327;&#21644;&#37325;&#22411;&#21345;&#36710;&#30340;&#37325;&#37327;&#20351;&#24471;&#32493;&#33322;&#37324;&#31243;&#20943;&#23569;&#21644;&#20805;&#30005;&#26102;&#38388;&#24310;&#38271;&#25104;&#20026;&#22266;&#26377;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30005;&#27744;&#26356;&#25442;&#26381;&#21153;&#25104;&#20026;&#36825;&#20123;&#21345;&#36710;&#30340;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#37319;&#29992;&#21452;&#37325;&#26041;&#27861;&#65292;&#30740;&#31350;&#21644;&#25552;&#39640;&#27492;&#31867;&#26381;&#21153;&#30340;&#25928;&#26524;&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;&#31354;&#38388;-&#26102;&#38388;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#39044;&#27979;&#25509;&#19979;&#26469;&#20960;&#20010;&#23567;&#26102;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#35813;&#39044;&#27979;&#25351;&#23548;&#20248;&#21270;&#27169;&#22359;&#36827;&#34892;&#39640;&#25928;&#30340;&#30005;&#27744;&#20998;&#37197;&#21644;&#37096;&#32626;&#12290;&#36890;&#36807;&#20998;&#26512;2,500&#33521;&#37324;&#30340;&#20844;&#36335;&#32593;&#32476;&#19978;&#30340;&#37325;&#22411;&#21345;&#36710;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20998;&#26512;&#20984;&#26174;&#39044;&#27979;/&#26426;&#22120;&#23398;&#20064;&#22312;&#20419;&#36827;&#26410;&#26469;&#20915;&#31574;&#20013;&#30340;&#20215;&#20540;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23454;&#26045;&#30005;&#27744;&#26356;&#25442;&#30340;&#21021;&#26399;&#38454;&#27573;&#26159;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrifying heavy-duty trucks offers a substantial opportunity to curtail carbon emissions, advancing toward a carbon-neutral future. However, the inherent challenges of limited battery energy and the sheer weight of heavy-duty trucks lead to reduced mileage and prolonged charging durations. Consequently, battery-swapping services emerge as an attractive solution for these trucks. This paper employs a two-fold approach to investigate the potential and enhance the efficacy of such services. Firstly, spatial-temporal demand prediction models are adopted to predict the traffic patterns for the upcoming hours. Subsequently, the prediction guides an optimization module for efficient battery allocation and deployment. Analyzing the heavy-duty truck data on a highway network spanning over 2,500 miles, our model and analysis underscore the value of prediction/machine learning in facilitating future decision-makings. In particular, we find that the initial phase of implementing battery-swappin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20013;&#22686;&#24378;&#26041;&#27861;&#21644;&#19979;&#28216;&#24615;&#33021;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20027;&#35201;&#36890;&#36807;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.03977</link><description>&lt;p&gt;
&#23436;&#32654;&#23545;&#40784;&#21487;&#33021;&#23545;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Perfect Alignment May be Poisonous to Graph Contrastive Learning. (arXiv:2310.03977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20013;&#22686;&#24378;&#26041;&#27861;&#21644;&#19979;&#28216;&#24615;&#33021;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20027;&#35201;&#36890;&#36807;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#23545;&#40784;&#27491;&#26679;&#26412;&#21644;&#20998;&#31163;&#36127;&#26679;&#26412;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#22270;&#24418;&#30340;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#29305;&#23450;&#22686;&#24378;&#26041;&#27861;&#32972;&#21518;&#30340;&#20869;&#22312;&#35268;&#24459;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20160;&#20040;&#26679;&#30340;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#65311;&#23545;&#27604;&#23398;&#20064;&#22914;&#20309;&#23454;&#38469;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#65311;&#20026;&#20160;&#20040;&#22686;&#24378;&#30340;&#24133;&#24230;&#24456;&#37325;&#35201;&#65311;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#24314;&#31435;&#22686;&#24378;&#26041;&#27861;&#21644;&#19979;&#28216;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20197;&#21450;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#36827;&#34892;&#30740;&#31350;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20027;&#35201;&#36890;&#36807;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#32780;&#19981;&#26159;&#32858;&#38598;&#21516;&#19968;&#31867;&#21035;&#30340;&#33410;&#28857;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#20570;&#20986;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#26080;&#27861;&#35299;&#37322;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#21151;&#65292;&#21363;&#20840;&#37096;&#26679;&#26412;&#23436;&#32654;&#23545;&#40784;&#21644;&#22686;&#24378;&#37325;&#21472;&#12290;&#20026;&#20102;&#29702;&#35299;&#22686;&#24378;&#22914;&#20309;&#36741;&#21161;&#23545;&#27604;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning (GCL) aims to learn node representations by aligning positive pairs and separating negative ones. However, limited research has been conducted on the inner law behind specific augmentations used in graph-based learning. What kind of augmentation will help downstream performance, how does contrastive learning actually influence downstream tasks, and why the magnitude of augmentation matters? This paper seeks to address these questions by establishing a connection between augmentation and downstream performance, as well as by investigating the generalization of contrastive learning. Our findings reveal that GCL contributes to downstream tasks mainly by separating different classes rather than gathering nodes of the same class. So perfect alignment and augmentation overlap which draw all intra-class samples the same can not explain the success of contrastive learning. Then in order to comprehend how augmentation aids the contrastive learning process, we conduct 
&lt;/p&gt;</description></item><item><title>HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00113</link><description>&lt;p&gt;
HyperMask: &#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00113
&lt;/p&gt;
&lt;p&gt;
HyperMask&#26159;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#25513;&#30721;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#39034;&#24207;&#35757;&#32451;&#26102;&#65292;&#24448;&#24448;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#26368;&#26377;&#25928;&#30340;&#20043;&#19968;&#26159;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36229;&#32593;&#32476;&#26681;&#25454;&#20219;&#21153;&#30340;&#29305;&#24449;&#29983;&#25104;&#30446;&#26631;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#20027;&#35201;&#38480;&#21046;&#26159;&#36229;&#32593;&#32476;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#21487;&#20197;&#20135;&#29983;&#23436;&#20840;&#19981;&#21516;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#22240;&#27492;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#21333;&#29420;&#35299;&#20915;&#30340;&#12290;&#27169;&#22411;&#22312;&#23398;&#20064;&#21518;&#32493;&#20219;&#21153;&#26102;&#19981;&#20351;&#29992;&#20043;&#21069;&#20219;&#21153;&#25152;&#20851;&#32852;&#30340;&#32593;&#32476;&#20449;&#24687;&#65292;&#24182;&#23454;&#38469;&#19978;&#20135;&#29983;&#20102;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#24425;&#31080;&#31080;&#35777;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#35748;&#20026;&#23384;&#22312;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#65288;&#21363;&#20013;&#22870;&#31080;&#65289;&#65292;&#21487;&#20197;&#20445;&#25345;&#23436;&#25972;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperMask&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#32593;&#32476;&#12290;&#36229;&#32593;&#32476;&#20135;&#29983;&#21322;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#20197;&#33719;&#21462;&#30446;&#26631;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15560</link><description>&lt;p&gt;
&#35782;&#21035;&#24615;&#24456;&#37325;&#35201;&#65306;&#25581;&#31034;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#38544;&#34255;&#30340;&#21487;&#24674;&#22797;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank. (arXiv:2309.15560v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15560
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;(Unbiased Learning to Rank, ULTR)&#22312;&#20174;&#26377;&#20559;&#28857;&#20987;&#26085;&#24535;&#35757;&#32451;&#26080;&#20559;&#25490;&#21517;&#27169;&#22411;&#30340;&#29616;&#20195;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20851;&#38190;&#22312;&#20110;&#26126;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#22522;&#20110;&#26816;&#39564;&#20551;&#35774;&#23545;&#28857;&#20987;&#25968;&#25454;&#36827;&#34892;&#25311;&#21512;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#21482;&#35201;&#28857;&#20987;&#23436;&#20840;&#25311;&#21512;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#24182;&#38750;&#24635;&#26159;&#33021;&#22815;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20174;&#32780;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22238;&#31572;&#30495;&#23454;&#30456;&#20851;&#24615;&#26159;&#21542;&#33021;&#22815;&#20174;&#28857;&#20987;&#25968;&#25454;&#24674;&#22797;&#20986;&#26469;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;ULTR&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#19968;&#20010;&#25490;&#21517;&#27169;&#22411;&#23450;&#20041;&#20026;&#21487;&#35782;&#21035;&#30340;&#65292;&#22914;&#26524;&#23427;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#32553;&#25918;&#21464;&#25442;&#65292;&#36825;&#23545;&#20110;&#25104;&#23545;&#25490;&#21517;&#30446;&#26631;&#26469;&#35828;&#24050;&#36275;&#22815;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#31561;&#20215;&#30340;&#21487;&#35782;&#21035;&#26465;&#20214;&#65292;&#21487;&#20197;&#26032;&#39062;&#22320;&#34920;&#36798;&#20026;&#19968;&#20010;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#38382;&#39064;&#65306;&#24403;&#19988;&#20165;&#24403;&#19968;&#20010;&#22270;&#65288;&#21363;&#21487;&#35782;&#21035;&#24615;&#22270;&#65289;&#36830;&#36890;&#26102;&#65292;&#35813;&#25490;&#21517;&#27169;&#22411;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Unbiased Learning to Rank (ULTR) is widespread in modern systems for training unbiased ranking models from biased click logs. The key is to explicitly model a generation process for user behavior and fit click data based on examination hypothesis. Previous research found empirically that the true latent relevance can be recovered in most cases as long as the clicks are perfectly fitted. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. In this work, we aim to answer if or when the true relevance can be recovered from click data, which is a foundation issue for ULTR field. We first define a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, which is enough for pairwise ranking objective. Then we explore an equivalent condition for identifiability that can be novely expressed as a graph connectivity test problem: if and only if a graph (namely identifi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#20013;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14345</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#20559;&#24046;&#35780;&#20272;&#19982;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Bias Assessment and Mitigation in LLM-based Code Generation. (arXiv:2309.14345v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14345
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#20013;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#32534;&#30721;&#36807;&#31243;&#30340;&#29983;&#20135;&#21147;&#21644;&#25928;&#29575;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;LLM&#22312;&#36719;&#20214;&#32534;&#30721;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#26222;&#21450;&#65292;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#24050;&#32463;&#20986;&#29616;&#65306;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#21253;&#21547;&#19982;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30456;&#20851;&#30340;&#31038;&#20250;&#20559;&#35265;&#65311;&#36825;&#20010;&#38382;&#39064;&#20851;&#31995;&#21040;&#20381;&#36182;&#20110;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#36719;&#20214;&#24212;&#29992;&#30340;&#23436;&#25972;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#36947;&#24503;&#22522;&#30784;&#65292;&#28982;&#32780;&#22312;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#35774;&#35745;&#30340;&#26032;&#39062;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#20559;&#24046;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;9.68\%&#21040;37.37\%&#30340;&#20195;&#30721;&#20989;&#25968;&#30340;&#21151;&#33021;&#20351;
&lt;/p&gt;
&lt;p&gt;
Utilizing state-of-the-art Large Language Models (LLMs), automatic code generation models play a pivotal role in enhancing the productivity and efficiency of software development coding procedures. As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social biases, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models, yet is under-explored in the literature. This paper presents a novel bias assessment framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive evaluation on the bias of nine state-of-the-art LLM-based code generation models. Our findings reveal that first, 31.45\% to 79.93\% code functions generated by our evaluated code generation models are biased, and 9.68\% to 37.37\% code functions' funct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#24037;&#20316;&#39057;&#29575;&#20165;&#21463;&#38480;&#20110;&#26356;&#24555;&#30340;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#32780;&#19981;&#26159;&#36739;&#24930;&#30340;&#29615;&#32469;&#38647;&#36798;&#34701;&#21512;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#65292;&#20197;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#21709;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04806</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#30340;&#21450;&#26102;&#34701;&#21512;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Timely Fusion of Surround Radar/Lidar for Object Detection in Autonomous Driving Systems. (arXiv:2309.04806v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#24037;&#20316;&#39057;&#29575;&#20165;&#21463;&#38480;&#20110;&#26356;&#24555;&#30340;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#32780;&#19981;&#26159;&#36739;&#24930;&#30340;&#29615;&#32469;&#38647;&#36798;&#34701;&#21512;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#65292;&#20197;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#21709;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#20256;&#24863;&#22120;&#25968;&#25454;&#34701;&#21512;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21608;&#22260;&#29615;&#22659;&#37325;&#24314;&#12290;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#25104;&#26412;&#25552;&#20379;360&#24230;&#35270;&#37326;&#37319;&#26679;&#65292;&#26159;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#26377;&#21069;&#26223;&#30340;&#24863;&#30693;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#29289;&#29702;&#38480;&#21046;&#65292;&#29615;&#32469;&#38647;&#36798;&#30340;&#26059;&#36716;&#36895;&#24230;&#21450;&#29983;&#25104;&#38647;&#36798;&#25968;&#25454;&#24103;&#30340;&#39057;&#29575;&#36828;&#20302;&#20110;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#12290;&#29616;&#26377;&#30340;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#34701;&#21512;&#26041;&#27861;&#24517;&#39035;&#20197;&#29615;&#32469;&#38647;&#36798;&#30340;&#20302;&#39057;&#29575;&#24037;&#20316;&#65292;&#32780;&#26080;&#27861;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#39640;&#21709;&#24212;&#24615;&#35201;&#27714;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;MVDNet&#24320;&#21457;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#24037;&#20316;&#39057;&#29575;&#20165;&#21463;&#38480;&#20110;&#26356;&#24555;&#30340;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#32780;&#19981;&#26159;&#36739;&#24930;&#30340;&#29615;&#32469;&#38647;&#36798;&#34701;&#21512;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#22522;&#26412;&#24605;&#36335;&#24456;&#31616;&#21333;&#65306;&#35753;MVDNet&#22788;&#29702;&#26242;&#26102;&#19981;&#23545;&#40784;&#30340;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#24103;&#65292;&#28982;&#21518;&#26681;&#25454;&#24103;&#26102;&#38388;&#20449;&#24687;&#23545;&#34701;&#21512;&#21518;&#30340;&#32467;&#26524;&#36827;&#34892;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusing Radar and Lidar sensor data can fully utilize their complementary advantages and provide more accurate reconstruction of the surrounding for autonomous driving systems. Surround Radar/Lidar can provide 360-degree view sampling with the minimal cost, which are promising sensing hardware solutions for autonomous driving systems. However, due to the intrinsic physical constraints, the rotating speed of surround Radar, and thus the frequency to generate Radar data frames, is much lower than surround Lidar. Existing Radar/Lidar fusion methods have to work at the low frequency of surround Radar, which cannot meet the high responsiveness requirement of autonomous driving systems.This paper develops techniques to fuse surround Radar/Lidar with working frequency only limited by the faster surround Lidar instead of the slower surround Radar, based on the state-of-the-art object detection model MVDNet. The basic idea of our approach is simple: we let MVDNet work with temporally unaligned d
&lt;/p&gt;</description></item><item><title>&#21363;&#20351;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#23384;&#22312;&#20851;&#38190;&#23398;&#20064;&#26399;&#65292;&#36825;&#20123;&#20851;&#38190;&#23398;&#20064;&#26399;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.12221</link><description>&lt;p&gt;
&#21363;&#20351;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#23384;&#22312;&#20851;&#38190;&#23398;&#20064;&#26399;
&lt;/p&gt;
&lt;p&gt;
Critical Learning Periods Emerge Even in Deep Linear Networks. (arXiv:2308.12221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12221
&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#23384;&#22312;&#20851;&#38190;&#23398;&#20064;&#26399;&#65292;&#36825;&#20123;&#20851;&#38190;&#23398;&#20064;&#26399;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#23398;&#20064;&#26399;&#26159;&#25351;&#22312;&#21457;&#32946;&#26089;&#26399;&#65292;&#26242;&#26102;&#30340;&#24863;&#30693;&#32570;&#38519;&#20250;&#23545;&#34892;&#20026;&#21644;&#23398;&#20064;&#34920;&#31034;&#20135;&#29983;&#27704;&#20037;&#24433;&#21709;&#30340;&#26102;&#38388;&#27573;&#12290;&#23613;&#31649;&#29983;&#29289;&#32593;&#32476;&#21644;&#20154;&#24037;&#32593;&#32476;&#20043;&#38388;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#24046;&#24322;&#65292;&#20294;&#20851;&#38190;&#23398;&#20064;&#26399;&#22312;&#20004;&#20010;&#31995;&#32479;&#20013;&#37117;&#26377;&#32463;&#39564;&#35266;&#23519;&#21040;&#12290;&#36825;&#34920;&#26126;&#20851;&#38190;&#23398;&#20064;&#26399;&#21487;&#33021;&#26159;&#23398;&#20064;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#32780;&#19981;&#26159;&#29983;&#29289;&#23398;&#19978;&#30340;&#20598;&#28982;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;&#20851;&#38190;&#23398;&#20064;&#26399;&#20250;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#65292;&#23588;&#20854;&#26159;&#19981;&#28165;&#26970;&#22312;&#20004;&#20010;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#23398;&#20064;&#26399;&#26159;&#21542;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#26550;&#26500;&#25110;&#20248;&#21270;&#32454;&#33410;&#12290;&#20026;&#20102;&#30830;&#23450;&#20851;&#38190;&#30340;&#22522;&#26412;&#22240;&#32032;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#32593;&#32476;&#20063;&#26174;&#31034;&#20986;&#29983;&#29289;&#23398;&#21644;&#20154;&#24037;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#35768;&#22810;&#34892;&#20026;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#36827;&#34892;&#20998;&#26512;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20851;&#38190;&#23398;&#20064;&#26399;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations. Despite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in both systems. This suggests that critical periods may be fundamental to learning and not an accident of biology. Yet, why exactly critical periods emerge in deep networks is still an open question, and in particular it is unclear whether the critical periods observed in both systems depend on particular architectural or optimization details. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and structure of the data distribution. We also show 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#20998;&#23618;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#26426;&#22120;&#20154;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#23558;&#27599;&#20010;&#35268;&#33539;&#20998;&#35299;&#20026;&#21407;&#23376;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#25512;&#26029;&#19981;&#21516;&#35268;&#33539;&#20043;&#38388;&#23376;&#20219;&#21153;&#30340;&#26102;&#24207;&#20851;&#31995;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#38388;&#30340;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.10393</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#35299;&#30340;&#22810;&#26426;&#22120;&#20154;&#20998;&#23618;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#26041;&#27861;&#22312;&#20998;&#23618;&#26102;&#24207;&#36923;&#36753;&#35268;&#33539;&#19979;
&lt;/p&gt;
&lt;p&gt;
Decomposition-based Hierarchical Task Allocation and Planning for Multi-Robots under Hierarchical Temporal Logic Specifications. (arXiv:2308.10393v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#20998;&#23618;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#26426;&#22120;&#20154;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#23558;&#27599;&#20010;&#35268;&#33539;&#20998;&#35299;&#20026;&#21407;&#23376;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#25512;&#26029;&#19981;&#21516;&#35268;&#33539;&#20043;&#38388;&#23376;&#20219;&#21153;&#30340;&#26102;&#24207;&#20851;&#31995;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#38388;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#24102;&#26377;&#26102;&#24207;&#36923;&#36753;&#35268;&#33539;&#30340;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#30740;&#31350;&#20027;&#35201;&#22522;&#20110;&#21333;&#19968;&#20844;&#24335;&#65292;&#29992;&#20110;&#21333;&#20010;&#25110;&#22810;&#20010;&#26426;&#22120;&#20154;&#30340;&#20219;&#21153;&#12290;&#20294;&#26159;&#38543;&#30528;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#26102;&#24207;&#36923;&#36753;&#20844;&#24335;&#19981;&#21487;&#36991;&#20813;&#22320;&#21464;&#24471;&#20887;&#38271;&#65292;&#20351;&#35299;&#37322;&#21644;&#35268;&#33539;&#29983;&#25104;&#21464;&#24471;&#22797;&#26434;&#65292;&#24182;&#19988;&#23545;&#35268;&#21010;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#20135;&#29983;&#21387;&#21147;&#12290;&#26368;&#36817;&#30340;&#21457;&#23637;&#26159;&#25552;&#20986;&#20102;&#26102;&#24207;&#36923;&#36753;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#26102;&#24207;&#36923;&#36753;&#35268;&#33539;&#65292;&#25552;&#20379;&#26356;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#25152;&#25552;&#20986;&#30340;&#35268;&#21010;&#31639;&#27861;&#20551;&#35774;&#27599;&#20010;&#35268;&#33539;&#20013;&#30340;&#26426;&#22120;&#20154;&#26159;&#29420;&#31435;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#22797;&#26434;&#26102;&#38388;&#32422;&#26463;&#30340;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#20998;&#23618;&#26694;&#26550;&#12290;&#22312;&#39640;&#23618;&#65292;&#27599;&#20010;&#35268;&#33539;&#39318;&#20808;&#34987;&#20998;&#35299;&#25104;&#19968;&#32452;&#21407;&#23376;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#26029;&#19981;&#21516;&#35268;&#33539;&#30340;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#26102;&#24207;&#20851;&#31995;&#26469;&#30830;ete&#26426;&#22120;&#20154;&#38388;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past research into robotic planning with temporal logic specifications, notably Linear Temporal Logic (LTL), was largely based on singular formulas for individual or groups of robots. But with increasing task complexity, LTL formulas unavoidably grow lengthy, complicating interpretation and specification generation, and straining the computational capacities of the planners. A recent development has been the hierarchical representation of LTL [1] that contains multiple temporal logic specifications, providing a more interpretable framework. However, the proposed planning algorithm assumes the independence of robots within each specification, limiting their application to multi-robot coordination with complex temporal constraints. In this work, we formulated a decomposition-based hierarchical framework. At the high level, each specification is first decomposed into a set of atomic sub-tasks. We further infer the temporal relations among the sub-tasks of different specifications to const
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TACO&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#25552;&#39640;&#20195;&#29702;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.13229</link><description>&lt;p&gt;
TACO&#65306;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning. (arXiv:2306.13229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TACO&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28508;&#22312;&#21160;&#20316;&#39537;&#21160;&#23545;&#27604;&#25439;&#22833;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#25552;&#39640;&#20195;&#29702;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#21019;&#24314;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#29366;&#24577;&#39044;&#27979;&#20016;&#23500;&#20195;&#29702;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#25511;&#21046;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30446;&#26631;&#36890;&#24120;&#19981;&#36275;&#20197;&#23398;&#20064;&#33021;&#22815;&#34920;&#31034;&#26368;&#20248;&#31574;&#30053;&#25110;&#20540;&#20989;&#25968;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#23427;&#20204;&#36890;&#24120;&#32771;&#34385;&#20855;&#26377;&#23567;&#30340;&#25277;&#35937;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#24573;&#35270;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#20013;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TACO&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#65292;&#20195;&#29702;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#28508;&#22312;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#12290;TACO&#36890;&#36807;&#20248;&#21270;&#37325;&#26032;&#33719;&#24471;&#35266;&#23519;&#19982;&#26368;&#36817;&#30340;&#22810;&#20010;&#20808;&#21069;&#35266;&#23519;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#23398;&#20064;&#29366;&#24577;&#19982;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in reinforcement learning (RL) from raw pixel data, sample inefficiency continues to present a substantial obstacle. Prior works have attempted to address this challenge by creating self-supervised auxiliary tasks, aiming to enrich the agent's learned representations with control-relevant information for future state prediction. However, these objectives are often insufficient to learn representations that can represent the optimal policy or value function, and they often consider tasks with small, abstract discrete action spaces and thus overlook the importance of action representation learning in continuous control. In this paper, we introduce TACO: Temporal Action-driven Contrastive Learning, a simple yet powerful temporal contrastive learning approach that facilitates the concurrent acquisition of latent state and action representations for agents. TACO simultaneously learns a state and an action representation by optimizing the mutual information between re
&lt;/p&gt;</description></item><item><title>PandaLM&#26159;&#19968;&#20010;&#35780;&#20272;LLM&#25351;&#20196;&#35843;&#20248;&#30340;&#33258;&#21160;&#22522;&#20934;&#65292;&#23427;&#33021;&#22815;&#21306;&#20998;&#26368;&#20248;&#27169;&#22411;&#65292;&#24182;&#20851;&#27880;&#20110;&#20027;&#35266;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.05087</link><description>&lt;p&gt;
PandaLM&#65306;LLM&#25351;&#20196;&#35843;&#20248;&#20248;&#21270;&#30340;&#33258;&#21160;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. (arXiv:2306.05087v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05087
&lt;/p&gt;
&lt;p&gt;
PandaLM&#26159;&#19968;&#20010;&#35780;&#20272;LLM&#25351;&#20196;&#35843;&#20248;&#30340;&#33258;&#21160;&#22522;&#20934;&#65292;&#23427;&#33021;&#22815;&#21306;&#20998;&#26368;&#20248;&#27169;&#22411;&#65292;&#24182;&#20851;&#27880;&#20110;&#20027;&#35266;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#22797;&#26434;&#24615;&#21644;&#35780;&#20272;&#35843;&#25972;&#27169;&#22411;&#30340;&#22256;&#38590;&#24615;&#65292;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#25351;&#20196;&#35843;&#20248;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#30830;&#23450;&#26368;&#20339;&#36229;&#21442;&#25968;&#65292;&#38656;&#35201;&#19968;&#20010;&#33258;&#21160;&#30340;&#12289;&#24378;&#22823;&#19988;&#21487;&#38752;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35780;&#20272;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#25361;&#25112;&#65292;&#24314;&#31435;&#36825;&#26679;&#19968;&#20010;&#22522;&#20934;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#27454;&#21517;&#20026;PandaLM&#30340;&#35780;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#33021;&#22815;&#21306;&#20998;&#20986;&#22810;&#20010;LLM&#20013;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;PandaLM&#30340;&#20851;&#27880;&#28857;&#19981;&#20165;&#38480;&#20110;&#20256;&#32479;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#23458;&#35266;&#27491;&#30830;&#24615;&#65292;&#36824;&#28085;&#30422;&#20102;&#35832;&#22914;&#30456;&#23545;&#31616;&#27905;&#24615;&#12289;&#28165;&#26224;&#24230;&#12289;&#36981;&#24490;&#35828;&#26126;&#12289;&#20840;&#38754;&#24615;&#21644;&#24418;&#24335;&#24615;&#31561;&#37325;&#35201;&#20027;&#35266;&#22240;&#32032;&#12290;&#20026;&#30830;&#20445;PandaLM&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#20154;&#24037;&#27880;&#37322;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25152;&#26377;&#19978;&#19979;&#25991;&#37117;&#26159;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated
&lt;/p&gt;</description></item><item><title>Reprompting&#26159;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#26032;&#37197;&#26041;&#35299;&#20915;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#65292;&#27604;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09993</link><description>&lt;p&gt;
Reprompting: &#36890;&#36807;&#21513;&#24067;&#26031;&#37319;&#26679;&#33258;&#21160;&#25512;&#26029;&#24605;&#32500;&#38142;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling. (arXiv:2305.09993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09993
&lt;/p&gt;
&lt;p&gt;
Reprompting&#26159;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#26032;&#37197;&#26041;&#35299;&#20915;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#65292;&#27604;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Reprompting&#65292;&#36825;&#26159;&#19968;&#31181;&#36845;&#20195;&#37319;&#26679;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#32473;&#23450;&#20219;&#21153;&#30340;&#24605;&#32500;&#38142;&#37197;&#26041;&#12290;&#36890;&#36807;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#25105;&#20204;&#25512;&#26029;&#36866;&#29992;&#20110;&#19968;&#32452;&#35757;&#32451;&#26679;&#20363;&#30340;&#24605;&#32500;&#38142;&#37197;&#26041;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20808;&#21069;&#37319;&#26679;&#30340;&#35299;&#20316;&#20026;&#29238;&#25552;&#31034;&#65292;&#36845;&#20195;&#22320;&#37319;&#26679;&#26032;&#30340;&#37197;&#26041;&#26469;&#35299;&#20915;&#20854;&#20182;&#35757;&#32451;&#38382;&#39064;&#12290;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#20116;&#20010;Big-Bench Hard&#20219;&#21153;&#20013;&#65292;Reprompting&#30340;&#34920;&#29616;&#22987;&#32456;&#20248;&#20110;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#22522;&#32447;&#12290;Reprompting&#36824;&#21487;&#20197;&#20419;&#36827;&#30693;&#35782;&#20174;&#19968;&#20010;&#26356;&#24378;&#30340;&#27169;&#22411;&#21040;&#19968;&#20010;&#36739;&#24369;&#30340;&#27169;&#22411;&#30340;&#36716;&#31227;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Reprompting&#30456;&#23545;&#20110;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24102;&#26469;&#20102;&#39640;&#36798;+17&#20010;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, we infer CoT recipes that work consistently well for a set of training samples. Our method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems. On five Big-Bench Hard tasks that require multi-step reasoning, Reprompting achieves consistently better performance than the zero-shot, few-shot, and human-written CoT baselines. Reprompting can also facilitate transfer of knowledge from a stronger model to a weaker model leading to substantially improved performance of the weaker model. Overall, Reprompting brings up to +17 point improvements over the previous state-of-the-art method that uses human-written CoT prompts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.00808</link><description>&lt;p&gt;
&#24179;&#22343;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Average-Constrained Policy Optimization. (arXiv:2302.00808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#21046;&#26465;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36890;&#24120;&#65292;&#24179;&#22343;&#26631;&#20934;&#27604;&#25240;&#25187;&#26631;&#20934;&#26356;&#21512;&#36866;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#24179;&#22343;&#38480;&#21046; CMDP &#30340;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#25240;&#25187;&#38480;&#21046; RL &#38382;&#39064;&#35774;&#35745;&#30340;&#31639;&#27861;&#36890;&#24120;&#22312;&#24179;&#22343; CMDP &#29615;&#22659;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#24179;&#22343;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#65288;ACPO&#65289;&#31639;&#27861;&#30340;&#28789;&#24863;&#26469;&#33258;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#30340;&#33879;&#21517; PPO &#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#22522;&#26412;&#30340;&#24179;&#22343; MDP &#25935;&#24863;&#24615;&#29702;&#35770;&#65292;&#28982;&#21518;&#22312;&#31639;&#27861;&#35774;&#35745;&#20013;&#20351;&#29992;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; MuJoCo &#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#24037;&#20316;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#19982;&#20854;&#20182;&#24120;&#35268;&#31639;&#27861;&#30456;&#27604;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) with constraints is becoming an increasingly important problem for various applications. Often, the average criterion is more suitable than the discounted criterion. Yet, RL for average criterion-constrained MDPs remains a challenging problem. Algorithms designed for discounted constrained RL problems often do not perform well for the average CMDP setting. In this paper, we introduce a new policy optimization with function approximation algorithm for constrained MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by the famed PPO-type algorithms based on trust region methods. We develop basic sensitivity theory for average MDPs, and then use the corresponding bounds in the design of the algorithm. We provide theoretical guarantees on its performance, and through extensive experimental work in various challenging MuJoCo environments, show the superior performance of the algorithm when compared to other sta
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.14041</link><description>&lt;p&gt;
RFold&#65306;&#22522;&#20110;&#35299;&#32806;&#20248;&#21270;&#26041;&#27861;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14041
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#31958;&#26680;&#37240;&#65288;RNA&#65289;&#30340;&#20108;&#32423;&#32467;&#26500;&#27604;&#19977;&#32423;&#32467;&#26500;&#26356;&#31283;&#23450;&#21644;&#26356;&#26131;&#20110;&#22312;&#32454;&#32990;&#20013;&#35775;&#38382;&#65292;&#22240;&#27492;&#23545;&#20110;&#21151;&#33021;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#24615;&#24046;&#21644;&#22797;&#26434;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;RFold&#12290;RFold&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#23558;&#20256;&#32479;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#34892;&#21644;&#36880;&#21015;&#20248;&#21270;&#65292;&#31616;&#21270;&#20102;&#27714;&#35299;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;RFold&#37319;&#29992;&#27880;&#24847;&#21147;&#22320;&#22270;&#20316;&#20026;&#20449;&#24687;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#35774;&#35745;&#25163;&#24037;&#29305;&#24449;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RFold&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#32422;8&#20493;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#21644;Colab&#28436;&#31034;&#21487;&#22312;\href{this http URL}{this http UR}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2207.01955</link><description>&lt;p&gt;
Ask-AC: &#19968;&#31181;&#24490;&#29615;&#20013;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework. (arXiv:2207.01955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#21462;&#24471;&#20102;&#24456;&#22810;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#26696;&#20173;&#28982;&#20381;&#36182;&#20110;&#26469;&#33258;&#39038;&#38382;&#19987;&#23478;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#65292;&#24418;&#24335;&#21253;&#25324;&#25345;&#32493;&#30417;&#25511;&#25110;&#39044;&#23450;&#20041;&#35268;&#21017;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#19968;&#31181;&#40635;&#28902;&#32780;&#26114;&#36149;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#31216;&#20026;Ask-AC&#65292;&#23427;&#29992;&#19968;&#20010;&#21452;&#21521;&#30340;&#23398;&#20064;&#32773;&#20027;&#21160;&#26426;&#21046;&#26367;&#25442;&#20102;&#21333;&#21521;&#30340;&#39038;&#38382;&#25351;&#23548;&#26426;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23398;&#20064;&#32773;&#21644;&#39038;&#38382;&#20043;&#38388;&#30340;&#23450;&#21046;&#21270;&#21644;&#26377;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;Ask-AC &#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#20114;&#34917;&#30340;&#32452;&#20214;&#65292;&#20998;&#21035;&#26159;&#21160;&#20316;&#35831;&#27714;&#32773;&#21644;&#33258;&#36866;&#24212;&#29366;&#24577;&#36873;&#25321;&#22120;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#32435;&#20837;&#21508;&#31181;&#31163;&#25955;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#20013;&#12290;&#21069;&#32773;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#19981;&#30830;&#23450;&#29366;&#24577;&#19979;&#30340;&#39038;&#38382;&#24178;&#39044;&#65292;&#21518;&#32773;&#21017;&#21487;&#20197;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the for
&lt;/p&gt;</description></item></channel></rss>