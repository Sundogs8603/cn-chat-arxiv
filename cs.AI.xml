<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;C-LoRA&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#23450;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#26032;&#27010;&#24565;&#21152;&#20837;&#21518;&#36807;&#21435;&#30456;&#20284;&#27010;&#24565;&#30340;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06027</link><description>&lt;p&gt;
&#25345;&#32493;&#25193;&#25955;&#65306;&#20351;&#29992;C-LoRA&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#30340;&#25345;&#32493;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA. (arXiv:2304.06027v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;C-LoRA&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#23450;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#26032;&#27010;&#24565;&#21152;&#20837;&#21518;&#36807;&#21435;&#30456;&#20284;&#27010;&#24565;&#30340;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21482;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#23450;&#20041;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#26174;&#30528;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#22810;&#20010;&#32454;&#31890;&#24230;&#27010;&#24565;&#20197;&#36830;&#32493;&#26041;&#24335;&#65288;&#21363;&#25345;&#32493;&#24615;&#22320;&#65289;&#33258;&#23450;&#20041;&#36825;&#26679;&#30340;&#27169;&#22411;&#26102;&#65292;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#25216;&#26415;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;C-LoRA&#65292;&#37319;&#29992;&#27969;&#34892;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36328;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#36830;&#32493;&#33258;&#25105;&#27491;&#21017;&#21270;&#20302;&#31209;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21253;&#25324;&#33258;&#23450;&#20041;&#23545;&#35937;&#30340;&#21333;&#35789;&#65288;&#21363;&#8220;&#20154;&#8221;&#29992;&#20110;&#20154;&#33080;&#25968;&#25454;&#38598;&#65289;&#24182;&#21021;&#22987;&#21270;&#20026;&#23436;&#20840;&#38543;&#26426;&#23884;&#20837;&#30340;&#23450;&#21046;&#25552;&#31034;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#24341;&#20837;&#20102;&#24494;&#23567;&#30340;&#39069;&#22806;&#21442;&#25968;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works demonstrate a remarkable ability to customize text-to-image diffusion models while only providing a few example images. What happens if you try to customize such models using multiple, fine-grained concepts in a sequential (i.e., continual) manner? In our work, we show that recent state-of-the-art customization of text-to-image models suffer from catastrophic forgetting when new concepts arrive sequentially. Specifically, when adding a new concept, the ability to generate high quality images of past, similar concepts degrade. To circumvent this forgetting, we propose a new method, C-LoRA, composed of a continually self-regularized low-rank adaptation in cross attention layers of the popular Stable Diffusion model. Furthermore, we use customization prompts which do not include the word of the customized object (i.e., "person" for a human face dataset) and are initialized as completely random embeddings. Importantly, our method induces only marginal additional parameter cost
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#26465;&#20214;&#30340;&#25193;&#25955;&#26041;&#27861;&#26469;&#24314;&#27169;&#36523;&#20307;&#23039;&#24577;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22312;&#20010;&#20154;&#35270;&#35282;&#19979;3D&#22330;&#26223;&#20013;&#30340;&#20154;&#31867;&#23039;&#24577;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#35757;&#32451;&#20013;&#26080;&#38656;&#20998;&#31867;&#22120;&#65292;&#37319;&#26679;&#20855;&#26377;&#19981;&#21516;&#30340;&#26465;&#20214;&#21644;&#22686;&#24378;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06024</link><description>&lt;p&gt;
&#20174;&#20010;&#20154;&#35282;&#24230;&#35270;&#22270;&#20013;&#19977;&#32500;&#22330;&#26223;&#20013;&#24674;&#22797;&#20154;&#20307;&#32593;&#26684;&#30340;&#27010;&#29575;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views. (arXiv:2304.06024v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#26465;&#20214;&#30340;&#25193;&#25955;&#26041;&#27861;&#26469;&#24314;&#27169;&#36523;&#20307;&#23039;&#24577;&#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#22312;&#20010;&#20154;&#35270;&#35282;&#19979;3D&#22330;&#26223;&#20013;&#30340;&#20154;&#31867;&#23039;&#24577;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#35757;&#32451;&#20013;&#26080;&#38656;&#20998;&#31867;&#22120;&#65292;&#37319;&#26679;&#20855;&#26377;&#19981;&#21516;&#30340;&#26465;&#20214;&#21644;&#22686;&#24378;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#33258;&#21160;&#24863;&#30693;&#20154;&#31867;&#31038;&#20132;&#20114;&#21160;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#20174;&#20010;&#20154;&#35270;&#22270;&#20013;&#20272;&#35745;&#21512;&#29702;&#30340;3D&#20154;&#20307;&#23039;&#24577;&#21644;&#24418;&#24577;&#12290;&#36825;&#39033;&#20219;&#21153;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#30001;&#20110;&#20010;&#20154;&#22330;&#26223;&#20013;&#30340;&#36817;&#36317;&#31163;&#23548;&#33268;&#36523;&#20307;&#34987;&#25130;&#26029;&#20005;&#37325;&#65292;&#20174;&#32780;&#23548;&#33268;&#30475;&#19981;&#35265;&#36523;&#20307;&#37096;&#20214;&#30340;&#22823;&#37327;&#23039;&#24577;&#27169;&#31946;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#26465;&#20214;&#30340;&#25193;&#25955;&#26041;&#27861;&#26469;&#27169;&#25311;&#36523;&#20307;&#23039;&#24577;&#20998;&#24067;&#12290;&#22312;3D&#22330;&#26223;&#20960;&#20309;&#26465;&#20214;&#30340;&#32422;&#26463;&#19979;&#65292;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22312;&#21512;&#29702;&#30340;&#20154;-&#22330;&#26223;&#20132;&#20114;&#20013;&#30340;&#36523;&#20307;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#30896;&#25758;&#24471;&#20998;&#30340;&#37319;&#26679;&#26469;&#36827;&#19968;&#27493;&#35299;&#20915;&#20154;-&#22330;&#26223;&#30456;&#20114;&#28183;&#36879;&#38382;&#39064;&#12290;&#26080;&#38656;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#20351;&#24471;&#37319;&#26679;&#20855;&#26377;&#19981;&#21516;&#30340;&#26465;&#20214;&#21644;&#22686;&#24378;&#30340;&#22810;&#26679;&#24615;&#12290;&#19968;&#20010;&#21487;&#35265;&#24615;&#24863;&#30693;&#30340;&#22270;&#21367;&#31215;&#27169;&#22411;&#36890;&#36807;&#27599;&#20010;&#20851;&#33410;&#30340;&#21487;&#35265;&#24230;&#26469;&#25351;&#23548;&#25193;&#25955;&#21435;&#22122;&#22120;&#65292;&#20197;&#21512;&#24182;&#20114;&#20851;&#33410;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic perception of human behaviors during social interactions is crucial for AR/VR applications, and an essential component is estimation of plausible 3D human pose and shape of our social partners from the egocentric view. One of the biggest challenges of this task is severe body truncation due to close social distances in egocentric scenarios, which brings large pose ambiguities for unseen body parts. To tackle this challenge, we propose a novel scene-conditioned diffusion method to model the body pose distribution. Conditioned on the 3D scene geometry, the diffusion model generates bodies in plausible human-scene interactions, with the sampling guided by a physics-based collision score to further resolve human-scene inter-penetrations. The classifier-free training enables flexible sampling with different conditions and enhanced diversity. A visibility-aware graph convolution model guided by per-joint visibility serves as the diffusion denoiser to incorporate inter-joint depende
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36923;&#36753;&#38145;&#23450;&#26469;&#30772;&#22351;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29305;&#27931;&#20234;&#31192;&#38053;&#26469;&#20135;&#29983;&#31070;&#32463;&#29305;&#27931;&#20234;&#24335;&#30340;&#21518;&#38376;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#35825;&#23548;&#22823;&#37327;&#38169;&#35823;&#20998;&#31867;&#29575;&#30340;&#29305;&#27931;&#20234;&#31192;&#38053;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#30340;&#29305;&#27931;&#20234;&#28608;&#27963;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06017</link><description>&lt;p&gt;
&#22522;&#20110;&#36923;&#36753;&#38145;&#23450;&#30340;&#31070;&#32463;&#29305;&#27931;&#20234;&#25915;&#20987;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
Exploiting Logic Locking for a Neural Trojan Attack on Machine Learning Accelerators. (arXiv:2304.06017v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36923;&#36753;&#38145;&#23450;&#26469;&#30772;&#22351;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29305;&#27931;&#20234;&#31192;&#38053;&#26469;&#20135;&#29983;&#31070;&#32463;&#29305;&#27931;&#20234;&#24335;&#30340;&#21518;&#38376;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#35825;&#23548;&#22823;&#37327;&#38169;&#35823;&#20998;&#31867;&#29575;&#30340;&#29305;&#27931;&#20234;&#31192;&#38053;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#30340;&#29305;&#27931;&#20234;&#28608;&#27963;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#38145;&#23450;&#34987;&#25552;&#20986;&#26469;&#20445;&#25252;&#33455;&#29255;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#36923;&#36753;&#38145;&#23450;&#25216;&#26415;&#36890;&#36807;&#20351;&#35774;&#35745;&#20013;&#30340;&#19968;&#37096;&#20998;&#32452;&#21512;&#27169;&#22359;&#20381;&#36182;&#20110;&#20445;&#23494;&#30340;&#31192;&#38053;&#26469;&#20445;&#25252;&#30828;&#20214;IP&#12290;&#22914;&#26524;&#20351;&#29992;&#20102;&#19981;&#27491;&#30830;&#30340;&#31192;&#38053;&#65292;&#38145;&#23450;&#27169;&#22359;&#20250;&#20135;&#29983;&#19968;&#32452;&#30830;&#23450;&#24615;&#38169;&#35823;&#65292;&#38480;&#21046;&#26410;&#32463;&#25480;&#26435;&#30340;&#20351;&#29992;&#12290;&#31070;&#32463;&#21152;&#36895;&#22120;&#26159;&#36923;&#36753;&#38145;&#23450;&#30340;&#24120;&#35265;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#26222;&#21450;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36923;&#36753;&#38145;&#23450;&#26469;&#30772;&#22351;&#23427;&#25152;&#20445;&#25252;&#30340;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#23433;&#20840;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20351;&#29992;&#19981;&#27491;&#30830;&#31192;&#38053;&#25152;&#24341;&#36215;&#30340;&#30830;&#23450;&#24615;&#38169;&#35823;&#26469;&#20135;&#29983;&#31070;&#32463;&#29305;&#27931;&#20234;&#24335;&#30340;&#21518;&#38376;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#19968;&#20010;&#21160;&#26426;&#25915;&#20987;&#22330;&#26223;&#65292;&#20854;&#20013;&#31934;&#24515;&#36873;&#25321;&#30340;&#19981;&#27491;&#30830;&#31192;&#38053;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#29305;&#27931;&#20234;&#31192;&#38053;&#65292;&#22312;&#38145;&#23450;&#30340;&#21152;&#36895;&#22120;&#20013;&#20026;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#36755;&#20837;&#31867;&#21035;&#20135;&#29983;&#20102;&#38169;&#35823;&#20998;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#20026;&#21463;&#36923;&#36753;&#38145;&#23450;&#20445;&#25252;&#30340;&#31070;&#32463;&#21152;&#36895;&#22120;&#35774;&#35745;&#29305;&#27931;&#20234;&#31192;&#38053;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#35825;&#23548;&#22823;&#37327;&#38169;&#35823;&#20998;&#31867;&#29575;&#30340;&#29305;&#27931;&#20234;&#31192;&#38053;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#30340;&#29305;&#27931;&#20234;&#28608;&#27963;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logic locking has been proposed to safeguard intellectual property (IP) during chip fabrication. Logic locking techniques protect hardware IP by making a subset of combinational modules in a design dependent on a secret key that is withheld from untrusted parties. If an incorrect secret key is used, a set of deterministic errors is produced in locked modules, restricting unauthorized use. A common target for logic locking is neural accelerators, especially as machine-learning-as-a-service becomes more prevalent. In this work, we explore how logic locking can be used to compromise the security of a neural accelerator it protects. Specifically, we show how the deterministic errors caused by incorrect keys can be harnessed to produce neural-trojan-style backdoors. To do so, we first outline a motivational attack scenario where a carefully chosen incorrect key, which we call a trojan key, produces misclassifications for an attacker-specified input class in a locked accelerator. We then dev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#30456;&#20284;&#29289;&#20307;&#20132;&#20114;&#65292;&#35825;&#23548;&#29289;&#20307;&#33021;&#21147;&#31751;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#28145;&#24230;&#24863;&#30693;&#23450;&#24615;&#31354;&#38388;&#34920;&#31034;&#27861;&#26500;&#24314;&#27963;&#21160;&#22270;&#24182;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#19981;&#30830;&#23450;&#20132;&#20114;&#24320;&#25918;&#38598;&#21512;&#30340;&#31867;&#21035;&#26080;&#20851;&#29289;&#20307;&#30340;&#33021;&#21147;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.05989</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#22270;&#23884;&#20837;&#36827;&#34892;&#29289;&#20307;&#26080;&#20851;&#33021;&#21147;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Object-agnostic Affordance Categorization via Unsupervised Learning of Graph Embeddings. (arXiv:2304.05989v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#30456;&#20284;&#29289;&#20307;&#20132;&#20114;&#65292;&#35825;&#23548;&#29289;&#20307;&#33021;&#21147;&#31751;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#28145;&#24230;&#24863;&#30693;&#23450;&#24615;&#31354;&#38388;&#34920;&#31034;&#27861;&#26500;&#24314;&#27963;&#21160;&#22270;&#24182;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#19981;&#30830;&#23450;&#20132;&#20114;&#24320;&#25918;&#38598;&#21512;&#30340;&#31867;&#21035;&#26080;&#20851;&#29289;&#20307;&#30340;&#33021;&#21147;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#20851;&#20110;&#29289;&#20307;&#20132;&#20114;&#21644;&#33021;&#21147;&#30340;&#30693;&#35782;&#21487;&#20197;&#20419;&#36827;&#22330;&#26223;&#29702;&#35299;&#21644;&#20154;&#26426;&#21327;&#20316;&#20219;&#21153;&#12290;&#22312;&#26085;&#24120;&#29983;&#27963;&#22330;&#26223;&#20013;&#65292;&#20154;&#20204;&#20542;&#21521;&#20110;&#26681;&#25454;&#22330;&#26223;&#21644;&#29289;&#20307;&#30340;&#21487;&#29992;&#24615;&#20197;&#22810;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#20351;&#29992;&#29289;&#20307;&#12290;&#38024;&#23545;&#23384;&#22312;&#24320;&#25918;&#20114;&#21160;&#21644;&#29289;&#20307;&#30340;&#31867;&#21035;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#29289;&#20307;&#30340;&#33021;&#21147;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#20855;&#26377;&#19981;&#30830;&#23450;&#20132;&#20114;&#24320;&#25918;&#38598;&#21512;&#30340;&#31867;&#21035;&#26080;&#20851;&#29289;&#20307;&#30340;&#33021;&#21147;&#20998;&#31867;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#29289;&#20307;&#20132;&#20114;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#20174;&#32780;&#35825;&#23548;&#29289;&#20307;&#33021;&#21147;&#31751;&#12290;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24863;&#30693;&#23450;&#24615;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#26500;&#24314;&#27963;&#21160;&#22270;&#65288;AGs&#65289;&#65292;&#36825;&#20123;&#22270;&#20174;RGB-D&#35270;&#39057;&#20013;&#25277;&#35937;&#20986;&#26102;&#31354;&#20132;&#20114;&#30340;&#36830;&#32493;&#34920;&#31034;&#12290;&#28982;&#21518;&#23545;&#36825;&#20123;AGs&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#33719;&#21462;&#20855;&#26377;&#31867;&#20284;&#33021;&#21147;&#30340;&#19968;&#32452;&#29289;&#20307;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Acquiring knowledge about object interactions and affordances can facilitate scene understanding and human-robot collaboration tasks. As humans tend to use objects in many different ways depending on the scene and the objects' availability, learning object affordances in everyday-life scenarios is a challenging task, particularly in the presence of an open set of interactions and objects. We address the problem of affordance categorization for class-agnostic objects with an open set of interactions; we achieve this by learning similarities between object interactions in an unsupervised way and thus inducing clusters of object affordances. A novel depth-informed qualitative spatial representation is proposed for the construction of Activity Graphs (AGs), which abstract from the continuous representation of spatio-temporal interactions in RGB-D videos. These AGs are clustered to obtain groups of objects with similar affordances. Our experiments in a real-world scenario demonstrate that o
&lt;/p&gt;</description></item><item><title>KubeEdge-Sedna v0.3&#25552;&#20986;&#20102;&#36793;&#32536;-&#20113;&#21327;&#20316;&#32456;&#36523;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#24067;&#20102;&#34892;&#19994;&#39318;&#20010;&#24320;&#28304;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#36793;&#32536;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#31163;&#32447;&#33258;&#20027;&#24615;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;AI&#24037;&#31243;&#21644;&#33258;&#21160;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.05985</link><description>&lt;p&gt;
KubeEdge-Sedna v0.3&#65306;&#38754;&#21521;&#19979;&#19968;&#20195;&#33258;&#21160;&#23450;&#21046;&#21270;AI&#24037;&#31243;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
KubeEdge-Sedna v0.3: Towards Next-Generation Automatically Customized AI Engineering Scheme. (arXiv:2304.05985v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05985
&lt;/p&gt;
&lt;p&gt;
KubeEdge-Sedna v0.3&#25552;&#20986;&#20102;&#36793;&#32536;-&#20113;&#21327;&#20316;&#32456;&#36523;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#24067;&#20102;&#34892;&#19994;&#39318;&#20010;&#24320;&#28304;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#36793;&#32536;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#31163;&#32447;&#33258;&#20027;&#24615;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;AI&#24037;&#31243;&#21644;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24066;&#22330;&#30340;&#35268;&#27169;&#32487;&#32493;&#22686;&#38271;&#12290;&#30446;&#21069;&#38459;&#30861;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#22823;&#35268;&#27169;&#22797;&#21046;&#30340;&#20027;&#35201;&#25216;&#26415;&#25361;&#25112;&#26159;&#36793;&#32536;&#25968;&#25454;&#30340;&#23567;&#26679;&#26412;&#21644;&#24322;&#26500;&#24615;&#12290;&#27492;&#22806;&#65292;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#23458;&#25143;&#32463;&#24120;&#23545;&#25968;&#25454;&#23433;&#20840;&#21512;&#35268;&#24615;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#31163;&#32447;&#33258;&#20027;&#24615;&#25552;&#20986;&#35201;&#27714;&#12290;&#22522;&#20110;&#23398;&#26415;&#30028;&#20013;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#27425;&#27491;&#24335;&#23450;&#20041;&#20102;&#36793;&#32536;-&#20113;&#21327;&#20316;&#32456;&#36523;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#24067;&#20102;&#19994;&#30028;&#39318;&#20010;&#24320;&#28304;&#30340;&#36793;&#32536;-&#20113;&#21327;&#20316;&#32456;&#36523;&#23398;&#20064;&#12290;&#36793;&#32536;-&#20113;&#21327;&#20316;&#32456;&#36523;&#23398;&#20064;&#36890;&#36807;&#65288;1&#65289;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#26469;&#36866;&#24212;&#19981;&#21516;&#36793;&#32536;&#20301;&#32622;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#20197;&#23454;&#29616;&#8220;&#21315;&#20154;&#21315;&#38754;&#8221;&#30340;&#20934;&#30830;&#39044;&#27979;&#65307;&#65288;2&#65289;&#26410;&#30693;&#20219;&#21153;&#30340;&#22686;&#37327;&#22788;&#29702;&#20351;&#31995;&#32479;&#23398;&#20064;&#26356;&#22810;&#65292;&#26679;&#26412;&#36234;&#23567;&#65292;&#31995;&#32479;&#21464;&#24471;&#36234;&#32874;&#26126;&#65292;&#36880;&#28176;&#23454;&#29616;AI&#24037;&#31243;&#21644;&#33258;&#21160;&#21270;&#65307;&#65288;3&#65289;&#21033;&#29992;&#20113;&#31471;&#30693;&#35782;&#26469;&#25552;&#39640;&#36793;&#32536;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scale of the global edge AI market continues to grow. The current technical challenges that hinder the large-scale replication of edge AI are mainly small samples on the edge and heterogeneity of edge data. In addition, edge AI customers often have requirements for data security compliance and offline autonomy of edge AI services. Based on the lifelong learning method in the academic world, we formally define the problem of edge-cloud collaborative lifelong learning for the first time, and release the industry's first open-source edge-cloud collaborative lifelong learning. Edge-cloud collaborative lifelong learning adapts to data heterogeneity at different edge locations through (1) multi-task transfer learning to achieve accurate prediction of "thousands of people and thousands of faces"; (2) incremental processing of unknown tasks, the more systems learn and the smarter systems are with small samples, gradually realize AI engineering and automation; (3) Use the cloud-side knowled
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#27880;&#24847;&#26862;&#26519;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24341;&#20837;&#21040;&#38543;&#26426;&#26862;&#26519;&#20013;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#33719;&#24471;&#27880;&#24847;&#26435;&#37325;&#65292;&#36827;&#32780;&#35299;&#20915;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.05980</link><description>&lt;p&gt;
&#31070;&#32463;&#27880;&#24847;&#26862;&#26519;&#65306;&#22522;&#20110;Transformer&#30340;&#26862;&#26519;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Neural Attention Forests: Transformer-Based Forest Improvement. (arXiv:2304.05980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#27880;&#24847;&#26862;&#26519;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24341;&#20837;&#21040;&#38543;&#26426;&#26862;&#26519;&#20013;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#33719;&#24471;&#27880;&#24847;&#26435;&#37325;&#65292;&#36827;&#32780;&#35299;&#20915;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#27880;&#24847;&#26862;&#26519;&#65288;NAF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#34920;&#26684;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24341;&#20837;&#21040;&#38543;&#26426;&#26862;&#26519;&#20013;&#65292;&#22312; Nadaraya-Watson &#26680;&#22238;&#24402;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#23558;&#29305;&#23450;&#24418;&#24335;&#30340;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#24471;&#21040;&#30340;&#27880;&#24847;&#26435;&#37325;&#20998;&#37197;&#21040;&#20915;&#31574;&#26641;&#20013;&#30340;&#21494;&#23376;&#25968;&#25454;&#21644;&#38543;&#26426;&#26862;&#26519;&#26412;&#36523;&#20013;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#38543;&#26426;&#26862;&#26519;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#27880;&#24847;&#26435;&#37325;&#21644; Nadaraya-Watson &#22238;&#24402;&#37319;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#34920;&#31034;&#65292;&#20854;&#26435;&#37325;&#21487;&#20197;&#35270;&#20026;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#20849;&#20139;&#26435;&#37325;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#31532;&#19968;&#37096;&#20998;&#29992;&#20110;&#25152;&#26377;&#20915;&#31574;&#26641;&#30340;&#35757;&#32451;&#65292;&#24182;&#35745;&#31639;&#21494;&#23376;&#25968;&#25454;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#31532;&#20108;&#37096;&#20998;&#32858;&#21512;&#26641;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#24182;&#26088;&#22312;&#26368;&#23567;&#21270;&#38543;&#26426;&#26862;&#26519;&#39044;&#27979;&#19982;&#35757;&#32451;&#38598;&#30446;&#26631;&#30495;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new approach called NAF (the Neural Attention Forest) for solving regression and classification tasks under tabular training data is proposed. The main idea behind the proposed NAF model is to introduce the attention mechanism into the random forest by assigning attention weights calculated by neural networks of a specific form to data in leaves of decision trees and to the random forest itself in the framework of the Nadaraya-Watson kernel regression. In contrast to the available models like the attention-based random forest, the attention weights and the Nadaraya-Watson regression are represented in the form of neural networks whose weights can be regarded as trainable parameters. The first part of neural networks with shared weights is trained for all trees and computes attention weights of data in leaves. The second part aggregates outputs of the tree networks and aims to minimize the difference between the random forest prediction and the truth target value from a training set. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#22312;&#35266;&#27979;&#25968;&#25454;&#19979;&#65292;&#36890;&#36807;&#26500;&#36896;&#20004;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#24182;&#20849;&#20139;&#20844;&#20849;&#21442;&#25968;&#26469;&#23545;&#20004;&#32452;&#30340;&#20108;&#20803;&#21709;&#24212;&#21464;&#37327;&#21644;&#21327;&#21464;&#37327;&#36827;&#34892;&#24314;&#27169;&#12290;&#21452;&#39640;&#26031;DAG-probit&#27169;&#22411;&#26159;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#30340;&#65292;&#22312;&#27169;&#22411;&#20013;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#27599;&#20010;&#33410;&#28857;&#30340;&#25928;&#24212;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2304.05976</link><description>&lt;p&gt;
&#21452;&#39640;&#26031;DAG-probit&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Bayesian Causal Inference in Doubly Gaussian DAG-probit Models. (arXiv:2304.05976v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#22312;&#35266;&#27979;&#25968;&#25454;&#19979;&#65292;&#36890;&#36807;&#26500;&#36896;&#20004;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#24182;&#20849;&#20139;&#20844;&#20849;&#21442;&#25968;&#26469;&#23545;&#20004;&#32452;&#30340;&#20108;&#20803;&#21709;&#24212;&#21464;&#37327;&#21644;&#21327;&#21464;&#37327;&#36827;&#34892;&#24314;&#27169;&#12290;&#21452;&#39640;&#26031;DAG-probit&#27169;&#22411;&#26159;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#30340;&#65292;&#22312;&#27169;&#22411;&#20013;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#27599;&#20010;&#33410;&#28857;&#30340;&#25928;&#24212;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#35266;&#23519;&#25968;&#25454;&#19979;&#65292;&#23545;&#20004;&#20010;&#32452;&#30340;&#20108;&#20803;&#21709;&#24212;&#21464;&#37327;&#20197;&#21450;&#19968;&#32452;&#21327;&#21464;&#37327;&#36827;&#34892;&#24314;&#27169;&#12290;&#20998;&#32452;&#21464;&#37327;&#21487;&#20197;&#26159;&#28151;&#28102;&#21464;&#37327;&#65288;&#27835;&#30103;&#21644;&#32467;&#26524;&#30340;&#20849;&#21516;&#21407;&#22240;&#65289;&#65292;&#24615;&#21035;&#65292;&#30149;&#20363;/&#23545;&#29031;&#32452;&#65292;&#31181;&#26063;&#31561;&#12290;&#32473;&#23450;&#21327;&#21464;&#37327;&#21644;&#19968;&#20010;&#20108;&#20803;&#28508;&#21464;&#37327;&#65292;&#30446;&#26631;&#26159;&#26500;&#36896;&#20004;&#20010;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs),&#21516;&#26102;&#20849;&#20139;&#19968;&#20123;&#20844;&#20849;&#21442;&#25968;&#12290;&#34920;&#31034;&#21464;&#37327;&#30340;&#33410;&#28857;&#38598;&#23545;&#20110;&#20004;&#32452;&#26159;&#30456;&#21516;&#30340;&#65292;&#20294;&#34920;&#31034;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#26377;&#21521;&#36793;&#21487;&#20197;&#26377;&#28508;&#22312;&#30340;&#21306;&#21035;&#12290;&#23545;&#20110;&#27599;&#20010;&#32452;&#65292;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#27599;&#20010;&#33410;&#28857;&#30340;&#25928;&#24212;&#22823;&#23567;&#12290;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#32452;&#22312;&#20854;DAG&#19979;&#31526;&#21512;&#39640;&#26031;&#20998;&#24067;&#12290;&#30001;&#20110;DAG&#30340;&#39532;&#23572;&#31185;&#22827;&#24615;&#36136;&#65292;&#32473;&#23450;&#29238;&#33410;&#28857;&#21518;&#65292;DAG&#30340;&#32852;&#21512;&#20998;&#24067;&#26159;&#26465;&#20214;&#29420;&#31435;&#30340;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#32452;&#19979;&#24341;&#20837;&#20102;&#39640;&#26031;DAG-probit&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#22240;&#27492;&#26159;&#21452;&#39640;&#26031;DAG-probit&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider modeling a binary response variable together with a set of covariates for two groups under observational data. The grouping variable can be the confounding variable (the common cause of treatment and outcome), gender, case/control, ethnicity, etc. Given the covariates and a binary latent variable, the goal is to construct two directed acyclic graphs (DAGs), while sharing some common parameters. The set of nodes, which represent the variables, are the same for both groups but the directed edges between nodes, which represent the causal relationships between the variables, can be potentially different. For each group, we also estimate the effect size for each node. We assume that each group follows a Gaussian distribution under its DAG. Given the parent nodes, the joint distribution of DAG is conditionally independent due to the Markov property of DAGs. We introduce the concept of Gaussian DAG-probit model under two groups and hence doubly Gaussian DAG-probit model. To estima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#26426;&#20132;&#20114;&#30340;&#36991;&#38556;&#26041;&#27861;&#65292;&#21487;&#22312;&#22823;&#35268;&#27169;&#19977;&#32500;&#22797;&#26434;&#29615;&#22659;&#20013;&#25511;&#21046;&#26080;&#20154;&#26426;&#23454;&#29616;&#20219;&#24847;&#30446;&#26631;&#28857;&#30340;&#23548;&#33322;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.05959</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#22312;&#20219;&#24847;&#19977;&#32500;&#29615;&#22659;&#20013;&#37319;&#29992;&#20154;&#26426;&#20132;&#20114;&#22686;&#24378;&#36991;&#38556;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UAV Obstacle Avoidance by Human-in-the-Loop Reinforcement in Arbitrary 3D Environment. (arXiv:2304.05959v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#26426;&#20132;&#20114;&#30340;&#36991;&#38556;&#26041;&#27861;&#65292;&#21487;&#22312;&#22823;&#35268;&#27169;&#19977;&#32500;&#22797;&#26434;&#29615;&#22659;&#20013;&#25511;&#21046;&#26080;&#20154;&#26426;&#23454;&#29616;&#20219;&#24847;&#30446;&#26631;&#28857;&#30340;&#23548;&#33322;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#22823;&#35268;&#27169;&#19977;&#32500;&#22797;&#26434;&#29615;&#22659;&#20013;&#20351; UAV &#36798;&#21040;&#20219;&#24847;&#30446;&#26631;&#28857;&#65292;&#19988;&#22312;&#23548;&#33322;&#36807;&#31243;&#20013;&#39134;&#34892;&#39640;&#24230;&#21644;&#36895;&#24230;&#22343;&#21487;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20154;&#26426;&#20132;&#20114;&#30340; DRL &#26041;&#27861;&#65292;&#21487;&#20197;&#20351; UAV &#22312;&#39134;&#34892;&#36807;&#31243;&#20013;&#33258;&#21160;&#36991;&#24320;&#38556;&#30861;&#29289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#20010;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#25351;&#23548; UAV &#23548;&#33322;&#12290;&#20154;&#26426;&#20132;&#20114;&#30340;&#20316;&#29992;&#26159;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#21160;&#24577;&#25913;&#21464; UAV &#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#36991;&#38556;&#12290;&#25105;&#20204;&#22312;&#22478;&#24066;&#12289;&#20892;&#26449;&#21644;&#26862;&#26519;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#25104;&#21151;&#29575;&#21644;&#24179;&#22343;&#27493;&#38271;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20943;&#23569;&#35757;&#32451;&#25910;&#25947;&#26102;&#38388;&#65292;&#25552;&#39640;&#23548;&#33322;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the continuous control of the unmanned aerial vehicle (UAV) based on a deep reinforcement learning method for a large-scale 3D complex environment. The purpose is to make the UAV reach any target point from a certain starting point, and the flying height and speed are variable during navigation. In this work, we propose a deep reinforcement learning (DRL)-based method combined with human-in-the-loop, which allows the UAV to avoid obstacles automatically during flying. We design multiple reward functions based on the relevant domain knowledge to guide UAV navigation. The role of human-in-the-loop is to dynamically change the reward function of the UAV in different situations to suit the obstacle avoidance of the UAV better. We verify the success rate and average step size on urban, rural, and forest scenarios, and the experimental results show that the proposed method can reduce the training convergence time and improve the efficiency and accuracy of navigation tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.05949</link><description>&lt;p&gt;
CMOS + &#38543;&#26426;&#32435;&#31859;&#30913;&#20307;&#65306;&#27010;&#29575;&#25512;&#29702;&#19982;&#23398;&#20064;&#24322;&#26500;&#35745;&#31639;&#26426;
&lt;/p&gt;
&lt;p&gt;
CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning. (arXiv:2304.05949v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#30340;&#25918;&#32531;&#65292;&#21033;&#29992;&#26032;&#20852;&#30340;&#32435;&#31859;&#25216;&#26415;&#65288;X&#65289;&#22686;&#24378;&#20114;&#34917;&#37329;&#23646;&#27687;&#21270;&#29289;&#21322;&#23548;&#20307;&#65288;CMOS&#65289;&#26230;&#20307;&#31649;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#12290;&#23613;&#31649;sMTJs&#35774;&#22791;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#24322;&#26500;&#35745;&#31639;&#26426;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;&#20351;&#29992;CMOS&#39044;&#27979;&#27969;&#31243;&#35774;&#35745;&#22871;&#20214;&#65288;PDK&#65289;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25968;&#23383;CMOS-based p-bits&#27169;&#25311;&#39640;&#36136;&#37327;&#38543;&#26426;&#24615;&#38656;&#35201;&#36229;&#36807;10,000&#20010;&#26230;&#20307;&#31649;&#65292;&#27599;&#29983;&#25104;&#19968;&#20010;&#38543;&#26426;&#25968;&#30340;&#33021;&#37327;&#27604;&#20351;&#29992;&#21482;&#28040;&#32791;2fJ&#30340;sMTJ-based p-bits&#39640;&#32422;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#32553;&#25918;&#21644;&#38598;&#25104;&#29256;&#26412;&#21487;&#20197;&#26174;&#30528;&#25512;&#36827;&#27010;&#29575;&#24615;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the slowing down of Moore's law, augmenting complementary-metal-oxide semiconductor (CMOS) transistors with emerging nanotechnologies (X) is becoming increasingly important. In this paper, we demonstrate how stochastic magnetic tunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combined with versatile Field Programmable Gate Arrays (FPGA) to design an energy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Our heterogeneous computer successfully performs probabilistic inference and asynchronous Boltzmann learning despite device-to-device variations in sMTJs. A comprehensive comparison using a CMOS predictive process design kit (PDK) reveals that digital CMOS-based p-bits emulating high-quality randomness use over 10,000 transistors with the energy per generated random number being roughly two orders of magnitude greater than the sMTJ-based p-bits that dissipate only 2 fJ. Scaled and integrated versions of our approach can significantly advance probabilistic 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#21516;&#22122;&#22768;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;&#39640;&#26031;&#20998;&#24067;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2304.05907</link><description>&lt;p&gt;
&#24102;&#26377;&#23450;&#20301;-&#23610;&#24230;&#22122;&#22768;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion models with location-scale noise. (arXiv:2304.05907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#21516;&#22122;&#22768;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;&#39640;&#26031;&#20998;&#24067;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;(DMs)&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#21040;&#25968;&#25454;&#20013;&#65292;&#24182;&#23398;&#20250;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#24819;&#30830;&#23450;&#21738;&#31181;&#22122;&#22768;&#20998;&#24067;&#65288;&#39640;&#26031;&#25110;&#38750;&#39640;&#26031;&#65289;&#22312;DMs&#20013;&#23548;&#33268;&#26356;&#22909;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;&#30001;&#20110;DMs&#30340;&#35774;&#35745;&#19981;&#36866;&#29992;&#20110;&#38750;&#39640;&#26031;&#22122;&#22768;&#65292;&#22240;&#27492;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20801;&#35768;&#20351;&#29992;&#38750;&#39640;&#26031;&#23450;&#20301;-&#23610;&#24230;&#22122;&#22768;&#26469;&#36870;&#36716;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#26694;&#26550;&#23637;&#31034;&#39640;&#26031;&#20998;&#24067;&#22312;&#21508;&#31181;&#20854;&#23427;&#20998;&#24067;&#65288;&#25289;&#26222;&#25289;&#26031;&#12289;&#22343;&#21248;&#12289;t&#12289;&#24191;&#20041;&#39640;&#26031;&#65289;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) are powerful generative models that add Gaussian noise to the data and learn to remove it. We wanted to determine which noise distribution (Gaussian or non-Gaussian) led to better generated data in DMs. Since DMs do not work by design with non-Gaussian noise, we built a framework that allows reversing a diffusion process with non-Gaussian location-scale noise. We use that framework to show that the Gaussian distribution performs the best over a wide range of other distributions (Laplace, Uniform, t, Generalized-Gaussian).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MusIK&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#22810;&#27493;&#36870;&#36816;&#21160;&#23398;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#21644;&#31995;&#32479;&#25506;&#32034;&#30456;&#32467;&#21512;&#65292;&#36798;&#21040;&#35745;&#31639;&#39640;&#25928;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#26368;&#20248;&#30340;&#25928;&#26524;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#23500;&#35266;&#27979;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.05889</link><description>&lt;p&gt;
&#22810;&#27493;&#36870;&#36816;&#21160;&#23398;&#34920;&#31034;&#23398;&#20064;&#65306;&#23500;&#35266;&#27979;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL. (arXiv:2304.05889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MusIK&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#22810;&#27493;&#36870;&#36816;&#21160;&#23398;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#21644;&#31995;&#32479;&#25506;&#32034;&#30456;&#32467;&#21512;&#65292;&#36798;&#21040;&#35745;&#31639;&#39640;&#25928;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#26368;&#20248;&#30340;&#25928;&#26524;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#23500;&#35266;&#27979;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22359;MDP&#38382;&#39064;&#19979;&#65292;&#38024;&#23545;&#23500;&#26377;&#39640;&#32500;&#24230;&#35266;&#27979;&#32780;&#35774;&#35745;&#30340;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#31639;&#27861;&#23384;&#22312;1&#65289;&#35745;&#31639;&#22797;&#26434;&#24230;&#36807;&#39640;&#65292;2&#65289;&#20855;&#26377;&#19981;&#24517;&#35201;&#30340;&#24378;&#32479;&#35745;&#20551;&#35774;&#65292;&#25110;3&#65289;&#20855;&#26377;&#27425;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#31532;&#19968;&#20010;&#22312;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#26368;&#23567;&#32479;&#35745;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#19982;&#25152;&#38656;&#31934;&#24230;&#27700;&#24179;&#30456;&#23545;&#24212;&#30340;&#36895;&#29575;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;MusIK&#23558;&#31995;&#32479;&#25506;&#32034;&#19982;&#22522;&#20110;&#22810;&#27493;&#36870;&#36816;&#21160;&#23398;&#30340;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#30446;&#26631;&#65292;&#21363;&#20174;&#24403;&#21069;&#35266;&#23519;&#21644;&#65288;&#21487;&#33021;&#36965;&#36828;&#30340;&#65289;&#26410;&#26469;&#35266;&#23519;&#20013;&#39044;&#27979;&#23398;&#20064;&#32773;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;MusIK&#31616;&#21333;&#32780;&#28789;&#27963;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21033;&#29992;&#20102;&#20960;&#31181;&#26032;&#25216;&#26415;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#38142;&#26465;&#35770;&#35777;&#26041;&#27861;&#65292;&#29992;&#20110;&#38480;&#21046;&#26102;&#38388;&#21644;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#20805;&#20998;&#26465;&#20214;&#65292;&#29992;&#20110;&#25429;&#25417;&#20302;&#32500;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;MusIK&#30340;&#20248;&#21183;&#65292;&#26080;&#35770;&#26159;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#36824;&#26159;&#22681;&#38047;&#26102;&#38388;&#26041;&#38754;&#37117;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the design of sample-efficient algorithms for reinforcement learning in the presence of rich, high-dimensional observations, formalized via the Block MDP problem. Existing algorithms suffer from either 1) computational intractability, 2) strong statistical assumptions that are not necessarily satisfied in practice, or 3) suboptimal sample complexity. We address these issues by providing the first computationally efficient algorithm that attains rate-optimal sample complexity with respect to the desired accuracy level, with minimal statistical assumptions. Our algorithm, MusIK, combines systematic exploration with representation learning based on multi-step inverse kinematics, a learning objective in which the aim is to predict the learner's own action from the current observation and observations in the (potentially distant) future. MusIK is simple and flexible, and can efficiently take advantage of general-purpose function approximation. Our analysis leverages several new tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.05874</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;EEG&#25968;&#25454;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21487;&#35299;&#37322;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data. (arXiv:2304.05874v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#20998;&#31867;&#33041;&#30005;&#22270;(EEG)&#25968;&#25454;&#65292;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#22914;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#35786;&#26029;&#20173;&#28982;&#26159;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;AGGCN&#36890;&#36807;&#23558;&#22522;&#20110;&#21367;&#31215;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#19982;&#22522;&#20110;&#21151;&#33021;&#36830;&#25509;&#24615;&#30340;&#33879;&#21517;&#30456;&#20851;&#24230;&#37327;&#30456;&#32467;&#21512;&#26469;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#38376;&#25511;&#22270;&#21367;&#31215;&#21487;&#20197;&#21160;&#24577;&#22320;&#21152;&#26435;&#32771;&#34385;&#21508;&#31181;&#31354;&#38388;&#23610;&#24230;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38381;&#30524;&#21644;&#30529;&#30524;&#29366;&#24577;&#19979;&#22343;&#33021;&#21462;&#24471;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#32467;&#26524;&#30340;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;AGGCN&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;AD&#26368;&#21463;&#24433;&#21709;&#30340;&#33041;&#21306;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) models are increasingly being used for the classification of electroencephalography (EEG) data. However, GNN-based diagnosis of neurological disorders, such as Alzheimer's disease (AD), remains a relatively unexplored area of research. Previous studies have relied on functional connectivity methods to infer brain graph structures and used simple GNN architectures for the diagnosis of AD. In this work, we propose a novel adaptive gated graph convolutional network (AGGCN) that can provide explainable predictions. AGGCN adaptively learns graph structures by combining convolution-based node feature enhancement with a well-known correlation-based measure of functional connectivity. Furthermore, the gated graph convolution can dynamically weigh the contribution of various spatial scales. The proposed model achieves high accuracy in both eyes-closed and eyes-open conditions, indicating the stability of learned representations. Finally, we demonstrate that the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#12289;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24335;&#28023;&#27915;&#24223;&#24323;&#29289;&#28165;&#29702;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#20351;&#24471;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21487;&#20197;&#21327;&#20316;&#31454;&#20105;&#24182;&#23454;&#29616;&#25910;&#38598;&#24223;&#24323;&#29289;&#30340;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.05872</link><description>&lt;p&gt;
&#22312;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#23398;&#20064;&#27807;&#36890;&#21644;&#21327;&#20316;&#20197;&#28165;&#29702;&#28023;&#27915;&#24223;&#24323;&#22609;&#26009;
&lt;/p&gt;
&lt;p&gt;
Learning to Communicate and Collaborate in a Competitive Multi-Agent Setup to Clean the Ocean from Macroplastics. (arXiv:2304.05872v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#12289;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24335;&#28023;&#27915;&#24223;&#24323;&#29289;&#28165;&#29702;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#20351;&#24471;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21487;&#20197;&#21327;&#20316;&#31454;&#20105;&#24182;&#23454;&#29616;&#25910;&#38598;&#24223;&#24323;&#29289;&#30340;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21327;&#20316;&#19982;&#31454;&#20105;&#20043;&#38388;&#30340;&#24179;&#34913;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#24314;&#31435;&#22312;&#19968;&#20010;&#39640;&#24433;&#21709;&#38382;&#39064;&#19978;&#65292;&#36890;&#36807;&#23545;&#28023;&#27915;&#24223;&#24323;&#22609;&#26009;&#30340;&#25910;&#38598;&#23454;&#29616;&#20102;&#21327;&#20316;&#19982;&#31454;&#20105;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#23427;&#22686;&#21152;&#20102;&#20195;&#29702;&#30340;&#35266;&#23519;&#31354;&#38388;&#12290;&#22312;&#25105;&#20204;&#33258;&#23450;&#20041;&#30340;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#25511;&#21046;&#30528;&#25910;&#38598;&#22609;&#26009;&#30340;&#33337;&#21482;&#12290;&#36825;&#31181;&#36890;&#20449;&#26426;&#21046;&#20351;&#20195;&#29702;&#33021;&#22815;&#20351;&#29992;&#20108;&#36827;&#21046;&#20449;&#21495;&#26469;&#24320;&#21457;&#36890;&#20449;&#21327;&#35758;&#12290;&#34429;&#28982;&#20195;&#29702;&#30340;&#38598;&#20307;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#22320;&#28165;&#29702;&#28023;&#27915;&#24223;&#24323;&#22609;&#26009;&#65292;&#20294;&#20195;&#29702;&#20250;&#22240;&#20010;&#20154;&#25910;&#38598;&#21040;&#30340;&#24223;&#24323;&#22609;&#26009;&#25968;&#37327;&#32780;&#33719;&#24471;&#22870;&#21169;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#24517;&#39035;&#23398;&#20250;&#26377;&#25928;&#22320;&#27807;&#36890;&#24182;&#20445;&#25345;&#31454;&#20105;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a balance between collaboration and competition is crucial for artificial agents in many real-world applications. We investigate this using a Multi-Agent Reinforcement Learning (MARL) setup on the back of a high-impact problem. The accumulation and yearly growth of plastic in the ocean cause irreparable damage to many aspects of oceanic health and the marina system. To prevent further damage, we need to find ways to reduce macroplastics from known plastic patches in the ocean. Here we propose a Graph Neural Network (GNN) based communication mechanism that increases the agents' observation space. In our custom environment, agents control a plastic collecting vessel. The communication mechanism enables agents to develop a communication protocol using a binary signal. While the goal of the agent collective is to clean up as much as possible, agents are rewarded for the individual amount of macroplastics collected. Hence agents have to learn to communicate effectively while maintai
&lt;/p&gt;</description></item><item><title>LMR&#26159;&#19968;&#31181;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#27431;&#27663;&#36317;&#31163;&#24230;&#37327;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2304.05869</link><description>&lt;p&gt;
LMR: &#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#36712;&#36857;&#39044;&#27979;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
LMR: Lane Distance-Based Metric for Trajectory Prediction. (arXiv:2304.05869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05869
&lt;/p&gt;
&lt;p&gt;
LMR&#26159;&#19968;&#31181;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#27431;&#27663;&#36317;&#31163;&#24230;&#37327;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#30340;&#24320;&#21457;&#38656;&#35201;&#24230;&#37327;&#26469;&#39564;&#35777;&#21644;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#24050;&#32463;&#30830;&#23450;&#30340;&#24230;&#37327;&#22522;&#20110;&#27431;&#27663;&#36317;&#31163;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#25152;&#26377;&#26041;&#21521;&#19978;&#37117;&#32473;&#20986;&#20102;&#30456;&#21516;&#30340;&#35823;&#24046;&#26435;&#37325;&#12290;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#23545;&#20110;&#20687;&#36947;&#36335;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#22949;&#21892;&#25429;&#25417;&#21040;&#19982;&#24213;&#23618;&#36710;&#36947;&#30456;&#20851;&#30340;&#25805;&#20316;&#21592;&#24847;&#22270;&#12290;&#20026;&#20102;&#38024;&#23545;&#19979;&#28216;&#35268;&#21010;&#20219;&#21153;&#21512;&#29702;&#35780;&#20272;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#65292;&#21363;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#36710;&#36947;&#38169;&#36807;&#29575;&#65288;LMR&#65289;&#12290;&#23545;&#20110;LMR&#30340;&#35745;&#31639;&#65292;&#23558;&#22320;&#38754;&#23454;&#27979;&#21644;&#39044;&#27979;&#31471;&#28857;&#20998;&#37197;&#32473;&#36710;&#36947;&#32447;&#27573;&#65292;&#26356;&#30830;&#20999;&#22320;&#35828;&#26159;&#23427;&#20204;&#30340;&#20013;&#24515;&#32447;&#12290;&#36890;&#36807;&#27839;&#36710;&#36947;&#32447;&#27573;&#30340;&#36317;&#31163;&#27979;&#37327;&#65292;&#39044;&#27979;&#19982;&#23454;&#27979;&#20043;&#38388;&#30340;&#36317;&#31163;&#22312;&#19968;&#23450;&#38408;&#20540;&#33539;&#22260;&#20869;&#30340;&#39044;&#27979;&#34987;&#31216;&#20026;&#21629;&#20013;&#65292;&#21542;&#21017;&#31216;&#20026;&#38169;&#36807;&#12290;LMR&#21017;&#23450;&#20041;&#20026;&#20135;&#29983;&#38169;&#36807;&#30340;&#24207;&#21015;&#30340;&#27604;&#29575;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#27431;&#27663;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;LMR&#26159;&#36866;&#29992;&#20110;&#31867;&#20284;&#36710;&#36947;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#30340;&#36712;&#36857;&#39044;&#27979;&#26356;&#20026;&#21512;&#36866;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of approaches for trajectory prediction requires metrics to validate and compare their performance. Currently established metrics are based on Euclidean distance, which means that errors are weighted equally in all directions. Euclidean metrics are insufficient for structured environments like roads, since they do not properly capture the agent's intent relative to the underlying lane. In order to provide a reasonable assessment of trajectory prediction approaches with regard to the downstream planning task, we propose a new metric that is lane distance-based: Lane Miss Rate (LMR). For the calculation of LMR, the ground-truth and predicted endpoints are assigned to lane segments, more precisely their centerlines. Measured by the distance along the lane segments, predictions that are within a certain threshold distance to the ground-truth count as hits, otherwise they count as misses. LMR is then defined as the ratio of sequences that yield a miss. Our results on three s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.05860</link><description>&lt;p&gt;
&#23398;&#20064;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#34920;&#31034;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Learning Homographic Disambiguation Representation for Neural Machine Translation. (arXiv:2304.05860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#24418;&#24322;&#20041;&#35789;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#19968;&#30452;&#26159;&#38590;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#21516;&#24418;&#24322;&#20041;&#35789;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#8220;HDR-encoder&#8221;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#23398;&#20064;&#36890;&#29992;&#21477;&#23376;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;WordNet&#20013;&#30340;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;HDR-encoder&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;HDR-encoder&#19982;&#22522;&#20110;Transformer&#30340;NMT&#22312;&#19981;&#21516;&#26041;&#26696;&#20013;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#22235;&#20010;&#32763;&#35793;&#26041;&#21521;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#26412;&#26041;&#27861;&#22312;&#22686;&#24378;NMT&#31995;&#32479;&#22788;&#29702;&#21516;&#24418;&#24322;&#20041;&#35789;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Homographs, words with the same spelling but different meanings, remain challenging in Neural Machine Translation (NMT). While recent works leverage various word embedding approaches to differentiate word sense in NMT, they do not focus on the pivotal components in resolving ambiguities of homographs in NMT: the hidden states of an encoder. In this paper, we propose a novel approach to tackle homographic issues of NMT in the latent space. We first train an encoder (aka "HDR-encoder") to learn universal sentence representations in a natural language inference (NLI) task. We further fine-tune the encoder using homograph-based synset sentences from WordNet, enabling it to learn word-level homographic disambiguation representations (HDR). The pre-trained HDR-encoder is subsequently integrated with a transformer-based NMT in various schemes to improve translation accuracy. Experiments on four translation directions demonstrate the effectiveness of the proposed method in enhancing the perfor
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26465;&#20214;&#39550;&#39542;&#34892;&#20026;&#39044;&#27979;&#30340;&#22522;&#20110;&#38598;&#21512;&#27861;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;RESET&#65292;&#21487;&#20197;&#39044;&#27979;&#28789;&#27963;&#25968;&#37327;&#30340;&#36712;&#36857;&#32780;&#19981;&#24433;&#21709;&#36816;&#34892;&#26102;&#38388;&#25110;&#20540;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.05856</link><description>&lt;p&gt;
&#37325;&#35775;&#36712;&#36857;&#38598;&#21512;&#65292;&#29992;&#20110;&#26465;&#20214;&#39550;&#39542;&#34892;&#20026;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RESET: Revisiting Trajectory Sets for Conditional Behavior Prediction. (arXiv:2304.05856v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26465;&#20214;&#39550;&#39542;&#34892;&#20026;&#39044;&#27979;&#30340;&#22522;&#20110;&#38598;&#21512;&#27861;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;RESET&#65292;&#21487;&#20197;&#39044;&#27979;&#28789;&#27963;&#25968;&#37327;&#30340;&#36712;&#36857;&#32780;&#19981;&#24433;&#21709;&#36816;&#34892;&#26102;&#38388;&#25110;&#20540;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#19981;&#21516;&#36712;&#36857;&#26465;&#20214;&#19979;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#21487;&#20197;&#20351;&#24471;&#19979;&#28216;&#30340;&#35268;&#21010;&#22120;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#26465;&#20214;&#34892;&#20026;&#39044;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#22238;&#24402;&#35299;&#30721;&#22120;&#65292;&#36825;&#24847;&#21619;&#30528;&#22352;&#26631;&#25110;&#22810;&#39033;&#24335;&#31995;&#25968;&#20250;&#34987;&#22238;&#24402;&#12290;&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;&#38598;&#21512;&#30340;&#36712;&#36857;&#39044;&#27979;&#65292;&#20854;&#20013;&#39044;&#23450;&#20041;&#36712;&#36857;&#38598;&#21512;&#20013;&#27599;&#31181;&#36712;&#36857;&#30340;&#27010;&#29575;&#30001;&#20998;&#31867;&#27169;&#22411;&#20915;&#23450;&#65292;&#24182;&#39318;&#27425;&#23558;&#20854;&#24212;&#29992;&#20110;&#26465;&#20214;&#39550;&#39542;&#34892;&#20026;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RESET&#65292;&#23427;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#39537;&#21160;&#30340;&#36712;&#36857;&#38598;&#21512;&#29983;&#25104;&#31639;&#27861;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#32534;&#30721;&#22120;&#12290;&#23545;&#20110;&#26080;&#26465;&#20214;&#39044;&#27979;&#65292;RESET&#30340;&#34920;&#29616;&#19982;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#30001;&#20110;&#22522;&#20110;&#38598;&#21512;&#27861;&#30340;&#29305;&#24615;&#65292;&#23427;&#20855;&#26377;&#21487;&#39044;&#27979;&#28789;&#27963;&#25968;&#37327;&#36712;&#36857;&#30340;&#20248;&#21183;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#36816;&#34892;&#26102;&#38388;&#25110;&#20540;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is desirable to predict the behavior of traffic participants conditioned on different planned trajectories of the autonomous vehicle. This allows the downstream planner to estimate the impact of its decisions. Recent approaches for conditional behavior prediction rely on a regression decoder, meaning that coordinates or polynomial coefficients are regressed. In this work we revisit set-based trajectory prediction, where the probability of each trajectory in a predefined trajectory set is determined by a classification model, and first-time employ it to the task of conditional behavior prediction. We propose RESET, which combines a new metric-driven algorithm for trajectory set generation with a graph-based encoder. For unconditional prediction, RESET achieves comparable performance to a regression-based approach. Due to the nature of set-based approaches, it has the advantageous property of being able to predict a flexible number of trajectories without influencing runtime or comple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#20915;&#31574;&#26641;&#31354;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#20915;&#31574;&#26641;&#24182;&#26368;&#20339;&#21270;&#21487;&#35299;&#37322;&#24615;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.05839</link><description>&lt;p&gt;
&#20351;&#29992;&#40657;&#30418;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#31867;&#26641;&#30340;&#26368;&#20339;&#21487;&#35299;&#37322;&#24615; - &#24615;&#33021;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Optimal Interpretability-Performance Trade-off of Classification Trees with Black-Box Reinforcement Learning. (arXiv:2304.05839v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#20915;&#31574;&#26641;&#31354;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#20915;&#31574;&#26641;&#24182;&#26368;&#20339;&#21270;&#21487;&#35299;&#37322;&#24615;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21487;&#20197;&#24314;&#31435;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20449;&#20219;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#29992;&#25143;&#23433;&#20840;&#26816;&#26597;&#12290;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#29305;&#21035;&#25552;&#20379;&#20102;&#20851;&#20110;&#23398;&#20064;&#27169;&#22411;&#30340;&#20840;&#23616;&#35270;&#22270;&#65292;&#24182;&#28165;&#26224;&#22320;&#27010;&#36848;&#20102;&#23545;&#20110;&#20998;&#31867;&#32473;&#23450;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#30340;&#29305;&#24449;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;DT&#22826;&#22823;&#65292;&#21017;&#36825;&#31181;&#21487;&#35299;&#37322;&#24615;&#20250;&#21463;&#21040;&#38459;&#30861;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#26469;&#25506;&#32034;DT&#31354;&#38388;&#20197;&#23398;&#20064;&#32039;&#20945;&#30340;&#26641;&#12290;&#19968;&#20010;&#32473;&#23450;&#30340;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65288;MDP&#65289;&#65292;&#28982;&#21518;&#28155;&#21152;&#20102;&#25910;&#38598;&#20851;&#20110;&#29305;&#24449;&#20449;&#24687;&#30340;&#39069;&#22806;&#21160;&#20316;&#65292;&#30456;&#24403;&#20110;&#26500;&#24314;DT&#12290;&#36890;&#36807;&#36866;&#24403;&#22320;&#24809;&#32602;&#36825;&#20123;&#25805;&#20316;&#65292;RL&#20195;&#29702;&#23398;&#20064;&#26368;&#20339;&#26435;&#34913;DT&#30340;&#22823;&#23567;&#21644;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#35201;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#36825;&#20010;RL&#20195;&#29702;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;MDP&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35777;&#26126;&#65292;&#35299;&#20915;&#19968;&#20010;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#38382;&#39064;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#20248;&#21270;&#21487;&#35299;&#37322;&#24615; - &#24615;&#33021;&#26435;&#34913;&#30340;DT&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of AI models allows for user safety checks to build trust in these models. In particular, decision trees (DTs) provide a global view on the learned model and clearly outlines the role of the features that are critical to classify a given data. However, interpretability is hindered if the DT is too large. To learn compact trees, a Reinforcement Learning (RL) framework has been recently proposed to explore the space of DTs. A given supervised classification task is modeled as a Markov decision problem (MDP) and then augmented with additional actions that gather information about the features, equivalent to building a DT. By appropriately penalizing these actions, the RL agent learns to optimally trade-off size and performance of a DT. However, to do so, this RL agent has to solve a partially observable MDP. The main contribution of this paper is to prove that it is sufficient to solve a fully observable problem to learn a DT optimizing the interpretability-performance tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992; DARTS &#26041;&#27861;&#23545;&#26631;&#20934; RNN &#21333;&#20803;&#36827;&#34892;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110; ReNet &#26550;&#26500;&#30340;&#26032;&#30340; RNN &#21333;&#20803;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22312; CIFAR-10 &#21644; SVHN &#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05838</link><description>&lt;p&gt;
DartsReNet&#65306;&#22312;ReNet&#26550;&#26500;&#20013;&#25506;&#32034;&#26032;&#30340;RNN&#21333;&#20803;
&lt;/p&gt;
&lt;p&gt;
DartsReNet: Exploring new RNN cells in ReNet architectures. (arXiv:2304.05838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992; DARTS &#26041;&#27861;&#23545;&#26631;&#20934; RNN &#21333;&#20803;&#36827;&#34892;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110; ReNet &#26550;&#26500;&#30340;&#26032;&#30340; RNN &#21333;&#20803;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22312; CIFAR-10 &#21644; SVHN &#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861; DARTS&#65292;&#20026;&#22270;&#20687;&#20998;&#31867;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21333;&#20803;&#65292;&#35813;&#21333;&#20803;&#29992;&#20110; ReNet &#26550;&#26500;&#12290;&#25105;&#20204;&#23545; ReNet &#26550;&#26500;&#24863;&#20852;&#36259;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110; RNN &#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#21367;&#31215;&#21644;&#27744;&#21270;&#27493;&#39588;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#20351;&#29992; DARTS &#26469;&#21457;&#29616;&#26032;&#30340;&#21333;&#20803;&#35774;&#35745;&#20197;&#20811;&#26381;&#26631;&#20934; RNN &#21333;&#20803;&#38024;&#23545;&#19968;&#32500;&#24207;&#21015;&#25968;&#25454;&#32780;&#38750;&#22270;&#20687;&#20998;&#31867;&#36825;&#31181;&#20108;&#32500;&#25968;&#25454;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#20351;&#29992; GRU &#21644; LSTM &#21333;&#20803;&#30340; ReNet &#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#30340;&#26032;&#21333;&#20803;&#22312; CIFAR-10 &#21644; SVHN &#19978;&#20248;&#20110;&#26631;&#20934; RNN &#21333;&#20803;&#65292;&#32780;&#23545; SVHN &#32467;&#26524;&#30340;&#25913;&#36827;&#34920;&#26126;&#20854;&#20855;&#26377;&#25512;&#24191;&#24615;&#65292;&#22240;&#20026;&#25105;&#20204;&#26159;&#20174; CIFAR-10 &#25512;&#23548;&#20986; RNN &#21333;&#20803;&#35774;&#35745;&#30340;&#65292;&#32780;&#27809;&#26377;&#38024;&#23545; SVHN &#36827;&#34892;&#26032;&#30340;&#21333;&#20803;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new Recurrent Neural Network (RNN) cells for image classification using a Neural Architecture Search (NAS) approach called DARTS. We are interested in the ReNet architecture, which is a RNN based approach presented as an alternative for convolutional and pooling steps. ReNet can be defined using any standard RNN cells, such as LSTM and GRU. One limitation is that standard RNN cells were designed for one dimensional sequential data and not for two dimensions like it is the case for image classification. We overcome this limitation by using DARTS to find new cell designs. We compare our results with ReNet that uses GRU and LSTM cells. Our found cells outperform the standard RNN cells on CIFAR-10 and SVHN. The improvements on SVHN indicate generalizability, as we derived the RNN cell designs from CIFAR-10 without performing a new cell search for SVHN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05836</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Game-theoretic Framework for Federated Learning. (arXiv:2304.05836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#33391;&#24615;&#21442;&#19982;&#32773;&#26088;&#22312;&#21327;&#21516;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#21322;&#35802;&#23454;&#30340;&#23545;&#25163;&#26102;&#65292;\textit{&#38544;&#31169;&#27844;&#28431;}&#30340;&#39118;&#38505;&#26159;&#19981;&#21487;&#24573;&#35270;&#30340;&#12290;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#35774;&#35745;&#20445;&#25252;&#26426;&#21046;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#21457;&#26126;&#25915;&#20987;&#26426;&#21046;&#12290;&#34429;&#28982;&#20445;&#25252;&#32773;&#19982;&#25915;&#20987;&#32773;&#20043;&#38388;&#30340;&#26007;&#20105;&#20284;&#20046;&#27704;&#26080;&#27490;&#22659;&#65292;&#20294;&#25105;&#20204;&#20851;&#24515;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#20107;&#20808;&#39044;&#38450;&#28508;&#22312;&#30340;&#25915;&#20987;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;FL&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#30456;&#24212;&#25910;&#30410;&#65292;&#20854;&#20013;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#27492;&#28216;&#25103;&#31216;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#65292;&#22312;&#20854;&#20013;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#37117;&#19981;&#30693;&#36947;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#22266;&#26377;&#30340;\textit{&#19981;&#23436;&#20840;&#20449;&#24687;}&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;FLSG&#19982;&#19968;&#20010;\textit{oracle}&#30456;&#20851;&#32852;&#65292;&#35813;oracle&#20855;&#26377;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#30693;&#35782;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#21508;&#31181;&#25928;&#29992;&#20989;&#25968;&#21644;&#25915;&#20987;&#27169;&#22411;&#32452;&#21512;&#19979;FLSG&#30340;&#32435;&#20160;&#22343;&#34913;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#23454;&#39564;&#32467;&#26524;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;FL&#22330;&#26223;&#20013;&#30340;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, benign participants aim to optimize a global model collaboratively. However, the risk of \textit{privacy leakage} cannot be ignored in the presence of \textit{semi-honest} adversaries. Existing research has focused either on designing protection mechanisms or on inventing attacking mechanisms. While the battle between defenders and attackers seems never-ending, we are concerned with one critical question: is it possible to prevent potential attacks in advance? To address this, we propose the first game-theoretic framework that considers both FL defenders and attackers in terms of their respective payoffs, which include computational costs, FL model utilities, and privacy leakage risks. We name this game the Federated Learning Security Game (FLSG), in which neither defenders nor attackers are aware of all participants' payoffs.  To handle the \textit{incomplete information} inherent in this situation, we propose associating the FLSG with an \textit{oracle} that ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#38480;&#21046;&#21644;&#23637;&#26395;&#12290;</title><link>http://arxiv.org/abs/2304.05832</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;: &#26041;&#27861;&#19982;&#25361;&#25112;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Few Shot Semantic Segmentation: a review of methodologies and open challenges. (arXiv:2304.05832v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#38480;&#21046;&#21644;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#21487;&#20026;&#22270;&#20687;&#20013;&#30340;&#27599;&#20010;&#20687;&#32032;&#36171;&#20104;&#20998;&#31867;&#26631;&#31614;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#30340;&#31361;&#30772;&#25552;&#20379;&#20102;&#22865;&#26426;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#24230;&#20934;&#30830;&#24615;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#19968;&#20123;&#39046;&#22495;&#30001;&#20110;&#25968;&#25454;&#31232;&#32570;&#12289;&#38544;&#31169;&#38382;&#39064;&#21644;&#38656;&#35201;&#29087;&#32451;&#26631;&#27880;&#20154;&#21592;&#31561;&#21407;&#22240;&#65292;&#38590;&#20197;&#26500;&#24314;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#24050;&#20316;&#20026;&#19968;&#31181;&#20801;&#35768;&#27169;&#22411;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26032;&#30740;&#31350;&#27969;&#27966;&#32780;&#20986;&#29616;&#12290;&#26412;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#21516;&#26102;&#25551;&#36848;&#20102;&#24403;&#21069;&#30340;&#38480;&#21046;&#21644;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation assigns category labels to each pixel in an image, enabling breakthroughs in fields such as autonomous driving and robotics. Deep Neural Networks have achieved high accuracies in semantic segmentation but require large training datasets. Some domains have difficulties building such datasets due to rarity, privacy concerns, and the need for skilled annotators. Few-Shot Learning (FSL) has emerged as a new research stream that allows models to learn new tasks from a few samples. This contribution provides an overview of FSL in semantic segmentation (FSS), proposes a new taxonomy, and describes current limitations and outlooks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2304.05805</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65306;&#38271;&#26399;&#35760;&#24518;&#26377;&#22810;&#22823;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
GDP nowcasting with artificial neural networks: How much does long-term memory matter?. (arXiv:2304.05805v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#19981;&#21516;&#30340;&#32479;&#35745;&#27169;&#22411;&#24212;&#29992;&#20110;&#32654;&#22269;&#32463;&#27982;&#23395;&#24230;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65288;GDP&#65289;&#22686;&#38271;&#39044;&#27979;&#12290;&#20351;&#29992;&#27599;&#26376;&#30340;FRED-MD&#25968;&#25454;&#24211;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#65288;DFM&#65289;&#21644;&#22235;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#39044;&#27979;&#34920;&#29616;&#65306;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;1D CNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#12290;&#23454;&#35777;&#20998;&#26512;&#21576;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#35780;&#20272;&#21608;&#26399;&#30340;&#32467;&#26524;&#12290;&#31532;&#19968;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2019&#24180;&#31532;4&#23395;&#24230;&#65289;&#20855;&#26377;&#24179;&#34913;&#30340;&#32463;&#27982;&#22686;&#38271;&#65292;&#32780;&#31532;&#20108;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2022&#24180;&#31532;3&#23395;&#24230;&#65289;&#36824;&#21253;&#25324;COVID-19&#34928;&#36864;&#26399;&#38388;&#30340;&#26102;&#38388;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#30456;&#23545;&#36739;&#20302;&#30340;&#38408;&#20540;&#20540;&#65288;&#32422;&#20845;&#20010;&#23395;&#24230;&#25110;&#21313;&#20843;&#20010;&#26376;&#65289;&#20197;&#21518;&#65292;&#36825;&#31181;&#25928;&#24212;&#20250;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26399;&#65288;&#22914;COVID-19&#34928;&#36864;&#26399;&#38388;&#65289;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#20250;&#21464;&#24471;&#36739;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our study, we apply different statistical models to nowcast quarterly GDP growth for the US economy. Using the monthly FRED-MD database, we compare the nowcasting performance of the dynamic factor model (DFM) and four artificial neural networks (ANNs): the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the results from two distinctively different evaluation periods. The first (2010:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. According to our results, longer input sequences result in more accurate nowcasts in periods of balanced economic growth. However, this effect ceases above a relatively low threshold value of around six quarters (eighteen months). During periods of economic turbulence (e.g., during the COVID-19 recession), long
&lt;/p&gt;</description></item><item><title>Proximity Forest 2.0&#26159;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05800</link><description>&lt;p&gt;
Proximity Forest 2.0&#65306;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Proximity Forest 2.0: A new effective and scalable similarity-based classifier for time series. (arXiv:2304.05800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05800
&lt;/p&gt;
&lt;p&gt;
Proximity Forest 2.0&#26159;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#30001;&#20110;&#21487;&#33021;&#19982;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21253;&#25324;&#36235;&#21183;&#12289;&#26041;&#24046;&#12289;&#39057;&#29575;&#12289;&#24133;&#24230;&#21644;&#21508;&#31181;&#27169;&#24335;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#31867;&#21035;&#65292;&#21253;&#25324;&#22522;&#20110;&#30456;&#20284;&#24615;&#12289;&#29305;&#24449;&#21644;&#38388;&#38548;&#12289;&#24418;&#29366;&#12289;&#23383;&#20856;&#12289;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;Proximity Forest&#29256;&#26412;2.0&#65288;PF 2.0&#65289;&#65292;&#23427;&#22312;UCR&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26368;&#36866;&#21512;&#20351;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;PF 2.0 &#21512;&#24182;&#20102;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;&#26368;&#36817;&#30340;&#19977;&#20010;&#36827;&#23637;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Time series classification (TSC) is a challenging task due to the diversity of types of feature that may be relevant for different classification tasks, including trends, variance, frequency, magnitude, and various patterns. To address this challenge, several alternative classes of approach have been developed, including similarity-based, features and intervals, shapelets, dictionary, kernel, neural network, and hybrid approaches. While kernel, neural network, and hybrid approaches perform well overall, some specialized approaches are better suited for specific tasks. In this paper, we propose a new similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), which outperforms previous state-of-the-art similarity-based classifiers across the UCR benchmark and outperforms state-of-the-art kernel, neural network, and hybrid methods on specific datasets in the benchmark that are best addressed by similarity-base methods. PF 2.0 incorporates three recent advances in time series simi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GPToLODS&#30340;&#21407;&#22411;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#25968;&#30334;&#20010;RDF&#30693;&#35782;&#22270;&#35889;&#20026;ChatGPT&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#21644;&#35777;&#25454;&#65292;&#20016;&#23500;ChatGPT&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2304.05774</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20010;RDF&#30693;&#35782;&#22270;&#35889;&#20016;&#23500;ChatGPT&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Using Multiple RDF Knowledge Graphs for Enriching ChatGPT Responses. (arXiv:2304.05774v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GPToLODS&#30340;&#21407;&#22411;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#25968;&#30334;&#20010;RDF&#30693;&#35782;&#22270;&#35889;&#20026;ChatGPT&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#21644;&#35777;&#25454;&#65292;&#20016;&#23500;ChatGPT&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36234;&#26469;&#36234;&#27969;&#34892;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;ChatGPT&#32842;&#22825;&#26694;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#35814;&#32454;&#22238;&#22797;&#21644;&#26126;&#30830;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#24456;&#22810;&#24773;&#20917;&#19979;&#65292;&#23427;&#36820;&#22238;&#21548;&#36215;&#26469;&#24456;&#21512;&#29702;&#20294;&#26159;&#19981;&#27491;&#30830;&#25110;&#19981;&#20934;&#30830;&#30340;&#22238;&#22797;&#65292;&#24182;&#19988;&#23427;&#19981;&#25552;&#20379;&#35777;&#25454;&#12290;&#22240;&#27492;&#65292;&#20219;&#20309;&#29992;&#25143;&#37117;&#24517;&#39035;&#36827;&#19968;&#27493;&#25628;&#32034;&#20197;&#26816;&#26597;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;/&#25110;&#26597;&#25214;&#26377;&#20851;&#21709;&#24212;&#23454;&#20307;&#30340;&#26356;&#22810;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#22312;&#20219;&#20309;&#23454;&#38469;&#39046;&#22495;&#37117;&#23384;&#22312;&#22823;&#37327;&#30340;RDF&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#20026;&#20102;&#23454;&#29616;ChatGPT&#21644;RDF KG&#30340;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GPToLODS&#30340;&#30740;&#31350;&#21407;&#22411;&#65292;&#33021;&#22815;&#20174;&#25968;&#30334;&#20010;RDF KG&#20013;&#20026;&#20219;&#20309;ChatGPT&#21709;&#24212;&#28155;&#21152;&#26356;&#22810;&#20449;&#24687;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#35782;&#21035;&#24182;&#27880;&#37322;&#27599;&#20010;&#21709;&#24212;&#23454;&#20307;&#30340;&#32479;&#35745;&#25968;&#25454;&#21644;&#25351;&#21521;LODsyndesis KG&#65288;&#21253;&#21547;&#26469;&#33258;400&#22810;&#20010;RDF KG&#21644;&#36229;&#36807;412&#30334;&#19975;&#23454;&#20307;&#30340;&#38598;&#25104;&#25968;&#25454;&#65289;&#30340;&#36229;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a recent trend for using the novel Artificial Intelligence ChatGPT chatbox, which provides detailed responses and articulate answers across many domains of knowledge. However, in many cases it returns plausible-sounding but incorrect or inaccurate responses, whereas it does not provide evidence. Therefore, any user has to further search for checking the accuracy of the answer or/and for finding more information about the entities of the response. At the same time there is a high proliferation of RDF Knowledge Graphs (KGs) over any real domain, that offer high quality structured data. For enabling the combination of ChatGPT and RDF KGs, we present a research prototype, called GPToLODS, which is able to enrich any ChatGPT response with more information from hundreds of RDF KGs. In particular, it identifies and annotates each entity of the response with statistics and hyperlinks to LODsyndesis KG (which contains integrated data from 400 RDF KGs and over 412 million entities). In 
&lt;/p&gt;</description></item><item><title>&#21069;&#20154;&#30340;&#20154;&#33080;&#38450;&#20266;&#65288;FAS&#65289;&#25216;&#26415;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#24403;&#21069;&#20844;&#24320;&#30340;FAS&#25968;&#25454;&#38598;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#19981;&#36275;&#65292;&#24341;&#36215;&#36807;&#25311;&#21512;&#21644;&#22330;&#26223;&#35823;&#21028;&#12290;&#36890;&#36807;&#24341;&#20837;&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#65288;WFAS&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#30740;&#31350;&#25552;&#39640;&#20102;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#31243;&#24230;&#65292;&#20419;&#36827;&#20102;FAS&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.05753</link><description>&lt;p&gt;
&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#25361;&#25112;&#36187;2023&#65306;&#22522;&#20934;&#21644;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results. (arXiv:2304.05753v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05753
&lt;/p&gt;
&lt;p&gt;
&#21069;&#20154;&#30340;&#20154;&#33080;&#38450;&#20266;&#65288;FAS&#65289;&#25216;&#26415;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#24403;&#21069;&#20844;&#24320;&#30340;FAS&#25968;&#25454;&#38598;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#19981;&#36275;&#65292;&#24341;&#36215;&#36807;&#25311;&#21512;&#21644;&#22330;&#26223;&#35823;&#21028;&#12290;&#36890;&#36807;&#24341;&#20837;&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#65288;WFAS&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#30740;&#31350;&#25552;&#39640;&#20102;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#31243;&#24230;&#65292;&#20419;&#36827;&#20102;FAS&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#38450;&#20266;&#65288;FAS&#65289;&#26159;&#20445;&#25252;&#33258;&#21160;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#23436;&#25972;&#24615;&#30340;&#37325;&#35201;&#26426;&#21046;&#12290;&#23613;&#31649;&#26377;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23558;&#29616;&#26377;&#26041;&#27861;&#25512;&#24191;&#21040;&#23454;&#38469;&#24212;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#20197;&#24402;&#22240;&#20110;&#20844;&#24320;&#21487;&#29992;&#30340;FAS&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#35757;&#32451;&#26399;&#38388;&#36807;&#25311;&#21512;&#25110;&#27979;&#35797;&#26399;&#38388;&#39281;&#21644;&#12290;&#23601;&#25968;&#37327;&#32780;&#35328;&#65292;&#27450;&#35784;&#20027;&#20307;&#30340;&#25968;&#37327;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20165;&#21253;&#25324;&#23569;&#20110;2,000&#20010;&#21463;&#35797;&#32773;&#12290;&#23601;&#22810;&#26679;&#24615;&#32780;&#35328;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30001;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#20351;&#29992;&#37325;&#22797;&#26426;&#26800;&#21270;&#36807;&#31243;&#25910;&#38598;&#30340;&#27450;&#35784;&#26679;&#26412;&#32452;&#25104;&#12290;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23548;&#33268;&#21516;&#36136;&#21270;&#26679;&#26412;&#21644;&#22330;&#26223;&#22810;&#26679;&#24615;&#30340;&#21294;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#65288;WFAS&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;FAS&#25968;&#25454;&#38598;&#65292;&#21487;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face anti-spoofing (FAS) is an essential mechanism for safeguarding the integrity of automated face recognition systems. Despite substantial advancements, the generalization of existing approaches to real-world applications remains challenging. This limitation can be attributed to the scarcity and lack of diversity in publicly available FAS datasets, which often leads to overfitting during training or saturation during testing. In terms of quantity, the number of spoof subjects is a critical determinant. Most datasets comprise fewer than 2,000 subjects. With regard to diversity, the majority of datasets consist of spoof samples collected in controlled environments using repetitive, mechanical processes. This data collection methodology results in homogenized samples and a dearth of scenario diversity. To address these shortcomings, we introduce the Wild Face Anti-Spoofing (WFAS) dataset, a large-scale, diverse FAS dataset collected in unconstrained settings. Our dataset encompasses 853
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36827;&#32780;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05749</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation. (arXiv:2304.05749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36827;&#32780;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#23545;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#65288;CTDGNs&#65289;&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#65288;LTF&#65289;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#24314;&#27169;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;CTDGN&#23545;&#20110;&#24314;&#27169;&#26102;&#38388;&#22270;&#25968;&#25454;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20294;&#30001;&#20110;&#23545;&#21382;&#21490;&#25968;&#25454;&#30340;&#23454;&#36136;&#35201;&#27714;&#65292;&#23427;&#20204;&#22312;LTF&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20063;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#30452;&#35266;&#30340;&#26041;&#27861;&#26159;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#24341;&#20837;CTDGN&#30340;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#65292;&#24182;&#36827;&#34892;&#25513;&#34109;&#28151;&#21512;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#29992;&#20110;&#26356;&#22810;&#24773;&#20917;&#65292;&#32780;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#25554;&#20837;&#21040;&#20219;&#24847;CTDGN&#20013;&#65292;&#32780;&#19981;&#22686;&#21152;&#21442;&#25968;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;UmmU&#22312;LTF&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on long-term forecasting (LTF) on continuous-time dynamic graph networks (CTDGNs), which is important for real-world modeling. Existing CTDGNs are effective for modeling temporal graph data due to their ability to capture complex temporal dependencies but perform poorly on LTF due to the substantial requirement for historical data, which is not practical in most cases. To relieve this problem, a most intuitive way is data augmentation. In this study, we propose \textbf{\underline{U}ncertainty \underline{M}asked \underline{M}ix\underline{U}p (UmmU)}: a plug-and-play module that conducts uncertainty estimation to introduce uncertainty into the embedding of intermediate layer of CTDGNs, and perform masked mixup to further enhance the uncertainty of the embedding to make it generalize to more situations. UmmU can be easily inserted into arbitrary CTDGNs without increasing the number of parameters. We conduct comprehensive experiments on three real-world dynamic graph dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05727</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#39044;&#38450;&#24615;&#20462;&#21098;Clever Hans&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#24050;&#25104;&#20026;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#19982;&#29992;&#25143;&#30340;&#39046;&#22495;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65288;&#20363;&#22914;Clever Hans&#25928;&#24212;&#65289;&#20063;&#34987;&#35748;&#20026;&#26159;&#25913;&#36827;&#38169;&#35823;&#27169;&#22411;&#30340;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#35299;&#37322;&#36798;&#25104;&#19968;&#33268;&#26102;&#65292;&#35201;&#24590;&#20040;&#20570;&#23601;&#19981;&#37027;&#20040;&#28165;&#26970;&#20102;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#29992;&#25143;&#25509;&#21463;&#35299;&#37322;&#24182;&#19981;&#20445;&#35777;ML&#27169;&#22411;&#30340;&#33391;&#22909;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#19968;&#20123;&#38544;&#34255;&#30340;Clever Hans&#25928;&#24212;&#21487;&#33021;&#20173;&#28982;&#26410;&#34987;&#21457;&#29616;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#36129;&#29486;&#19968;&#20010;&#26032;&#26041;&#27861;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#12290;&#33258;&#28982;&#30011;&#20687;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#27169;&#22411;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;&#30340;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#22240;&#27492;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI has become a popular tool for validating machine learning models. Mismatches between the explained model's decision strategy and the user's domain knowledge (e.g. Clever Hans effects) have also been recognized as a starting point for improving faulty models. However, it is less clear what to do when the user and the explanation agree. In this paper, we demonstrate that acceptance of explanations by the user is not a guarantee for a ML model to function well, in particular, some Clever Hans effects may remain undetected. Such hidden flaws of the model can nevertheless be mitigated, and we demonstrate this by contributing a new method, Explanation-Guided Exposure Minimization (EGEM), that premptively prunes variations in the ML model that have not been the subject of positive explanation feedback. Experiments on natural image data demonstrate that our approach leads to models that strongly reduce their reliance on hidden Clever Hans strategies, and consequently achieve hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#21462;&#20154;&#30340;&#21160;&#24577;&#29305;&#24449;&#26469;&#33258;&#21160;&#35843;&#25972;DMP&#26694;&#26550;&#21442;&#25968;&#65292;&#20197;&#22686;&#24378;&#26426;&#22120;&#20154;&#36712;&#36857;&#35268;&#21010;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.05703</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#36816;&#21160;&#21407;&#29702;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#20154;&#26426;&#25216;&#33021;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Human-Robot Skill Transfer with Enhanced Compliance via Dynamic Movement Primitives. (arXiv:2304.05703v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#21462;&#20154;&#30340;&#21160;&#24577;&#29305;&#24449;&#26469;&#33258;&#21160;&#35843;&#25972;DMP&#26694;&#26550;&#21442;&#25968;&#65292;&#20197;&#22686;&#24378;&#26426;&#22120;&#20154;&#36712;&#36857;&#35268;&#21010;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25214;&#21040;&#19968;&#20010;&#39640;&#25928;&#30340;&#36866;&#24212;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#26041;&#24335;&#26159;&#25552;&#39640;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;&#20248;&#20808;&#20219;&#21153;&#65292;&#20854;&#20013;&#36890;&#36807;&#28436;&#31034;&#23398;&#20064;&#65288;LfD&#65289;&#26041;&#27861;&#23558;&#31867;&#20284;&#20110;&#20154;&#30340;&#25216;&#33021;&#20256;&#36882;&#32473;&#26426;&#22120;&#20154;&#26159;&#19968;&#31181;&#36712;&#36857;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#30340;&#21160;&#20316;&#36890;&#24120;&#38024;&#23545;&#20154;&#30340;&#26426;&#20307;&#20248;&#21270;&#32780;&#19981;&#26159;&#26426;&#22120;&#20154;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#29983;&#29289;&#21147;&#23398;&#19982;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#21160;&#24577;&#36816;&#21160;&#21407;&#29702;&#65288;DMP&#65289;&#26694;&#26550;&#26159;LfD&#36825;&#19968;&#38480;&#21046;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#38656;&#35201;&#35843;&#33410;&#20844;&#24335;&#20013;&#30340;&#20108;&#38454;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#24341;&#20837;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#20174;&#20154;&#30340;&#28436;&#31034;&#20013;&#25552;&#21462;&#21160;&#24577;&#29305;&#24449;&#26469;&#33258;&#21160;&#35843;&#25972;DMP&#26694;&#26550;&#20013;&#30340;&#21442;&#25968;&#12290;&#38500;&#20102;&#19982;LfD&#19968;&#36215;&#20351;&#29992;&#20043;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#29992;&#36884;&#26159;&#65292;&#23427;&#21487;&#20197;&#31435;&#21363;&#19982;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19968;&#36215;&#29992;&#20110;&#26426;&#22120;&#20154;&#35757;&#32451;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25552;&#21462;&#30340;&#29305;&#24449;&#26377;&#21161;&#20110;&#25552;&#39640;&#26426;&#22120;&#20154;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding an efficient way to adapt robot trajectory is a priority to improve overall performance of robots. One approach for trajectory planning is through transferring human-like skills to robots by Learning from Demonstrations (LfD). The human demonstration is considered the target motion to mimic. However, human motion is typically optimal for human embodiment but not for robots because of the differences between human biomechanics and robot dynamics. The Dynamic Movement Primitives (DMP) framework is a viable solution for this limitation of LfD, but it requires tuning the second-order dynamics in the formulation. Our contribution is introducing a systematic method to extract the dynamic features from human demonstration to auto-tune the parameters in the DMP framework. In addition to its use with LfD, another utility of the proposed method is that it can readily be used in conjunction with Reinforcement Learning (RL) for robot training. In this way, the extracted features facilitate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#30524;&#37096;&#22270;&#20687;&#20013;&#20934;&#30830;&#23450;&#20301;&#35282;&#33180;&#21453;&#23556;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#23545;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#20165;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#19988;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.05673</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#30524;&#37096;&#22270;&#20687;&#20013;&#20934;&#30830;&#23450;&#20301;&#35282;&#33180;&#21453;&#23556;
&lt;/p&gt;
&lt;p&gt;
Precise localization of corneal reflections in eye images using deep learning trained on synthetic data. (arXiv:2304.05673v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05673
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#30524;&#37096;&#22270;&#20687;&#20013;&#20934;&#30830;&#23450;&#20301;&#35282;&#33180;&#21453;&#23556;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#23545;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#20165;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#19988;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#22320;&#23450;&#20301;&#21333;&#20010;&#30524;&#37096;&#22270;&#20687;&#20013;&#35282;&#33180;&#21453;&#23556;&#30340;&#20013;&#24515;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#32431;&#31929;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#20351;&#29992;&#21482;&#26377;&#27169;&#25311;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#22909;&#22788;&#26159;&#23436;&#20840;&#36991;&#24320;&#20102;&#38656;&#35201;&#23545;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#30340;&#32321;&#29712;&#27880;&#37322;&#36807;&#31243;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#25918;&#32622;&#22312;&#19981;&#21516;&#32972;&#26223;&#20013;&#21644;&#23884;&#20837;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20174;&#30495;&#23454;&#30524;&#30555;&#20013;&#25293;&#25668;&#30340;&#39640;&#36136;&#37327;&#35270;&#39057;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#31354;&#38388;&#31934;&#24230;&#26041;&#38754;&#38477;&#20302;&#20102;35&#65285;&#65292;&#24182;&#22312;&#27169;&#25311;&#22270;&#20687;&#26041;&#38754;&#20197;&#31354;&#38388;&#20934;&#30830;&#24615;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#35282;&#33180;&#21453;&#23556;&#20013;&#24515;&#23450;&#20301;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning method for accurately localizing the center of a single corneal reflection (CR) in an eye image. Unlike previous approaches, we use a convolutional neural network (CNN) that was trained solely using simulated data. Using only simulated data has the benefit of completely sidestepping the time-consuming process of manual annotation that is required for supervised training on real eye images. To systematically evaluate the accuracy of our method, we first tested it on images with simulated CRs placed on different backgrounds and embedded in varying levels of noise. Second, we tested the method on high-quality videos captured from real eyes. Our method outperformed state-of-the-art algorithmic methods on real eye images with a 35% reduction in terms of spatial precision, and performed on par with state-of-the-art on simulated images in terms of spatial accuracy.We conclude that our method provides a precise method for CR center localization and provides a solutio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#21435;&#38500;&#35270;&#35273;Transformer&#20013;&#30340;Token Mixer&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#35270;&#35273;Backbone&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#37197;&#22791;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#31574;&#30053;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#35270;&#35273;Backbone&#24182;&#33719;&#24471;&#20196;&#20154;&#40723;&#33310;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#25512;&#26029;&#26399;&#38388;&#20139;&#21463;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05659</link><description>&lt;p&gt;
RIFormer&#65306;&#31227;&#38500;Token Mixer&#21518;&#20445;&#25345;&#35270;&#35273;Backbone&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
RIFormer: Keep Your Vision Backbone Effective While Removing Token Mixer. (arXiv:2304.05659v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#21435;&#38500;&#35270;&#35273;Transformer&#20013;&#30340;Token Mixer&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#35270;&#35273;Backbone&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#37197;&#22791;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#31574;&#30053;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#35270;&#35273;Backbone&#24182;&#33719;&#24471;&#20196;&#20154;&#40723;&#33310;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#25512;&#26029;&#26399;&#38388;&#20139;&#21463;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#21435;&#38500;&#22522;&#26412;&#26500;&#24314;&#22359;&#20013;&#30340;Token Mixer&#30340;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;Backbone&#30340;&#26377;&#25928;&#24615;&#12290;Token Mixer&#20316;&#20026;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#30340;&#33258;&#27880;&#24847;&#21147;&#65292;&#26088;&#22312;&#25191;&#34892;&#19981;&#21516;&#31354;&#38388;Token&#20043;&#38388;&#30340;&#20449;&#24687;&#36890;&#20449;&#65292;&#20294;&#20250;&#23548;&#33268;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#21024;&#38500;&#23427;&#20204;&#23558;&#23548;&#33268;&#19981;&#23436;&#25972;&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#22240;&#27492;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20986;Reparameterization Idea&#30340;RepIdentityFormer&#26469;&#30740;&#31350;&#26080;Token Mixer&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25913;&#36827;&#30340;&#23398;&#20064;&#33539;&#24335;&#26469;&#25171;&#30772;&#31616;&#21333;&#26080;Token Mixer&#30340;Backbone&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#23558;&#32463;&#39564;&#23454;&#36341;&#24635;&#32467;&#20026;5&#26465;&#20934;&#21017;&#12290;&#37197;&#22791;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#35270;&#35273;Backbone&#24182;&#33719;&#24471;&#20196;&#20154;&#40723;&#33310;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#25512;&#26029;&#26399;&#38388;&#20139;&#21463;&#39640;&#25928;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#21078;&#26512;&#20998;&#26512;&#20063;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies how to keep a vision backbone effective while removing token mixers in its basic building blocks. Token mixers, as self-attention for vision transformers (ViTs), are intended to perform information communication between different spatial tokens but suffer from considerable computational cost and latency. However, directly removing them will lead to an incomplete model structure prior, and thus brings a significant accuracy drop. To this end, we first develop an RepIdentityFormer base on the re-parameterizing idea, to study the token mixer free model architecture. And we then explore the improved learning paradigm to break the limitation of simple token mixer free backbone, and summarize the empirical practice into 5 guidelines. Equipped with the proposed optimization strategy, we are able to build an extremely simple vision backbone with encouraging performance, while enjoying the high efficiency during inference. Extensive experiments and ablative analysis also demo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#20114;&#24800;&#65288;PR&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#23450;&#20041;&#37051;&#25509;&#31354;&#38388;&#21644;&#35774;&#35745;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#22312;&#19981;&#21305;&#37197;&#30340;&#29366;&#24577;&#19979;&#20805;&#20998;&#21033;&#29992;&#20132;&#21449;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#29702;&#35770;&#20445;&#38556;&#65292;&#33021;&#22312;&#20010;&#20307;&#24863;&#30693;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#25910;&#25947;&#20110;&#26368;&#20248;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.05632</link><description>&lt;p&gt;
&#20855;&#26377;&#29702;&#35770;&#20445;&#38556;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#20114;&#24800;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Policy Reciprocity with Theoretical Guarantee. (arXiv:2304.05632v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#20114;&#24800;&#65288;PR&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#23450;&#20041;&#37051;&#25509;&#31354;&#38388;&#21644;&#35774;&#35745;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#22312;&#19981;&#21305;&#37197;&#30340;&#29366;&#24577;&#19979;&#20805;&#20998;&#21033;&#29992;&#20132;&#21449;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#29702;&#35770;&#20445;&#38556;&#65292;&#33021;&#22312;&#20010;&#20307;&#24863;&#30693;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#25910;&#25947;&#20110;&#26368;&#20248;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#38382;&#39064;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20132;&#21449;&#26234;&#33021;&#20307;&#30693;&#35782;&#26469;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#20114;&#24800;&#65288;PR&#65289;&#26694;&#26550;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#20132;&#21449;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#19981;&#21305;&#37197;&#30340;&#29366;&#24577;&#19979;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#19981;&#21305;&#37197;&#29366;&#24577;&#30340;&#37051;&#25509;&#31354;&#38388;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20215;&#20540;&#36845;&#20195;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#20351;&#26234;&#33021;&#20307;&#21487;&#20197;&#25512;&#26029;&#26356;&#31934;&#30830;&#30340;&#22238;&#25253;&#12290;&#20026;&#20102;&#25552;&#39640;PR&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#28145;&#23618;PR&#31639;&#27861;&#12290;&#21478;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20010;&#20307;&#24863;&#30693;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#28176;&#36827;&#22320;&#36798;&#25104;&#20849;&#35782;&#24182;&#25910;&#25947;&#20110;&#26368;&#20248;&#20540;&#20989;&#25968;&#65292;&#36825;&#20998;&#21035;&#24847;&#21619;&#30528;PR&#30340;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PR&#22312;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-agent reinforcement learning (RL) algorithms hold great potential for solving a variety of real-world problems. However, they do not fully exploit cross-agent knowledge to reduce sample complexity and improve performance. Although transfer RL supports knowledge sharing, it is hyperparameter sensitive and complex. To solve this problem, we propose a novel multi-agent policy reciprocity (PR) framework, where each agent can fully exploit cross-agent policies even in mismatched states. We then define an adjacency space for mismatched states and design a plug-and-play module for value iteration, which enables agents to infer more precise returns. To improve the scalability of PR, deep PR is proposed for continuous control tasks. Moreover, theoretical analysis shows that agents can asymptotically reach consensus through individual perceived rewards and converge to an optimal value function, which implies the stability and effectiveness of PR, respectively. Experimental results o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DESMIL&#65292;&#19968;&#20010;&#26032;&#30340;&#22810;&#20852;&#36259;&#32593;&#32476;&#65292;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#20013;&#35299;&#20915;&#36328;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#21435;&#30456;&#20851;&#25552;&#21462;&#30340;&#22810;&#20010;&#20852;&#36259;&#21521;&#37327;&#65292;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.05615</link><description>&lt;p&gt;
&#22810;&#20852;&#36259;&#28145;&#24230;&#31283;&#23450;&#23398;&#20064;&#29992;&#20110;&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Stable Multi-Interest Learning for Out-of-distribution Sequential Recommendation. (arXiv:2304.05615v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05615
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DESMIL&#65292;&#19968;&#20010;&#26032;&#30340;&#22810;&#20852;&#36259;&#32593;&#32476;&#65292;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#20013;&#35299;&#20915;&#36328;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#21435;&#30456;&#20851;&#25552;&#21462;&#30340;&#22810;&#20010;&#20852;&#36259;&#21521;&#37327;&#65292;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#21033;&#30410;&#27169;&#22411;&#34987;&#29992;&#20316;&#25552;&#21462;&#29992;&#25143;&#22810;&#20010;&#34920;&#31034;&#21521;&#37327;&#30340;&#20852;&#36259;&#65292; &#23545;&#20110;&#24207;&#21015;&#25512;&#33616;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#30340;&#22810;&#20852;&#36259;&#25512;&#33616;&#27169;&#22411;&#37117;&#27809;&#26377;&#32771;&#34385;&#20852;&#36259;&#20998;&#24067;&#21487;&#33021;&#25913;&#21464;&#24102;&#26469;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#29992;&#25143;&#22810;&#20010;&#20852;&#36259;&#36890;&#24120;&#39640;&#24230;&#30456;&#20851;&#65292;&#27169;&#22411;&#26377;&#26426;&#20250;&#23398;&#20064;&#21040;&#22024;&#26434;&#20852;&#36259;&#21644;&#30446;&#26631;&#39033;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#65292;&#20852;&#36259;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#34394;&#20551;&#30456;&#20851;&#24615;&#20250;&#35823;&#23548;&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#36328;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#21033;&#30410;&#32593;&#32476;&#65292;&#21517;&#20026;DESMIL&#65292;&#35813;&#32593;&#32476;&#35797;&#22270;&#22312;&#27169;&#22411;&#20013;&#21435;&#30456;&#20851;&#25552;&#21462;&#30340;&#21033;&#30410;&#65292;&#20174;&#32780;&#21487;&#20197;&#28040;&#38500;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#12290;DESMIL&#24212;&#29992;&#19968;&#20010;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#25552;&#21462;&#22810;&#20010;&#21033;&#30410;&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;&#26469;&#23545;&#23427;&#20204;&#36827;&#34892;&#32534;&#30721;&#65292;&#19968;&#20010;&#21435;&#30456;&#20851;&#27169;&#22359;&#26469;&#21435;&#38500;&#30456;&#20851;&#24615;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DESMIL&#22312;in-distribution&#21644;out-of-distribution&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, multi-interest models, which extract interests of a user as multiple representation vectors, have shown promising performances for sequential recommendation. However, none of existing multi-interest recommendation models consider the Out-Of-Distribution (OOD) generalization problem, in which interest distribution may change. Considering multiple interests of a user are usually highly correlated, the model has chance to learn spurious correlations between noisy interests and target items. Once the data distribution changes, the correlations among interests may also change, and the spurious correlations will mislead the model to make wrong predictions. To tackle with above OOD generalization problem, we propose a novel multi-interest network, named DEep Stable Multi-Interest Learning (DESMIL), which attempts to de-correlate the extracted interests in the model, and thus spurious correlations can be eliminated. DESMIL applies an attentive module to extract multiple interests, an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;22&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;ChatGPT&#22312;&#21508;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#19968;&#33268;&#24615;&#21644;&#21151;&#25928;&#65292;&#36825;&#20026;&#22810;&#35821;&#35328;&#23398;&#20064;&#21644;LLMs&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.05613</link><description>&lt;p&gt;
ChatGPT&#36229;&#36234;&#33521;&#35821;&#65306;&#26397;&#30528;&#23545;&#22810;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning. (arXiv:2304.05613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;22&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;ChatGPT&#22312;&#21508;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#19968;&#33268;&#24615;&#21644;&#21151;&#25928;&#65292;&#36825;&#20026;&#22810;&#35821;&#35328;&#23398;&#20064;&#21644;LLMs&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#26368;&#37325;&#35201;&#30340;&#31361;&#30772;&#20043;&#19968;&#65292;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;ChatGPT&#26159;&#26368;&#36817;&#24320;&#21457;&#30340;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;LLM&#31995;&#32479;&#20043;&#19968;&#65292;&#23637;&#31034;&#20102;&#23545;&#35821;&#35328;&#29983;&#25104;&#30340;&#20986;&#33394;&#25216;&#33021;&#65292;&#24182;&#21463;&#21040;&#20102;&#20844;&#20247;&#30340;&#39640;&#24230;&#20851;&#27880;&#12290;&#22312;&#21457;&#29616;ChatGPT&#22312;&#33521;&#35821;&#20013;&#30340;&#21508;&#31181;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#21644;&#29983;&#25104;&#22810;&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#22810;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#12290;&#32771;&#34385;&#21040;ChatGPT&#22312;&#19981;&#21516;&#38382;&#39064;&#21644;&#39046;&#22495;&#30340;&#33521;&#35821;&#20013;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;ChatGPT&#26159;&#21542;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#36824;&#26159;&#38656;&#35201;&#24320;&#21457;&#26356;&#22810;&#30340;&#35821;&#35328;&#29305;&#23450;&#25216;&#26415;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#38656;&#35201;&#23545;ChatGPT&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#21363;&#36229;&#20986;&#20102;&#25253;&#36947;&#30340;&#36726;&#20107;&#65289;&#65292;&#36825;&#22312;&#24403;&#21069;&#30340;&#30740;&#31350;&#20013;&#20173;&#28982;&#32570;&#20047;&#25110;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;ChatGPT&#30340;&#20840;&#38754;&#19968;&#32452;22&#31181;&#35821;&#35328;&#65292;&#20174;&#19981;&#21516;&#30340;&#23478;&#26063;&#20013;&#24320;&#21457;&#19968;&#20010;&#22810;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#21508;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#19968;&#33268;&#24615;&#21644;&#21151;&#25928;&#65292;&#36825;&#20026;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#27491;&#38754;&#31572;&#26696;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#23558;&#20419;&#36827;&#26410;&#26469;&#30340;&#22810;&#35821;&#35328;&#23398;&#20064;&#21644;LLMs&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#20256;&#32479;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#20043;&#33021;&#22815;&#25429;&#25417;&#36229;&#36807;&#20154;&#31867;&#35268;&#33539;&#33539;&#30068;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#27010;&#24565;&#34920;&#31034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.05591</link><description>&lt;p&gt;
FLAN-T5&#20013;&#35821;&#20041;&#29305;&#24449;&#39564;&#35777;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semantic Feature Verification in FLAN-T5. (arXiv:2304.05591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#20256;&#32479;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#20043;&#33021;&#22815;&#25429;&#25417;&#36229;&#36807;&#20154;&#31867;&#35268;&#33539;&#33539;&#30068;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#27010;&#24565;&#34920;&#31034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#26041;&#38754;&#30340;&#28508;&#21147;&#8212;&#8212;&#36825;&#26159;&#35780;&#20215;&#35748;&#30693;&#31185;&#23398;&#20013;&#27010;&#24565;&#32467;&#26500;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#25105;&#20204;&#22522;&#20110;&#29616;&#26377;&#30340;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#26500;&#24314;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#39564;&#35777;&#30340;&#35268;&#33539;&#25429;&#25417;&#20102;&#27010;&#24565;&#32467;&#26500;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#36229;&#36234;&#20102;&#20165;&#32771;&#34385;&#20154;&#31867;&#35268;&#33539;&#25152;&#33021;&#28085;&#30422;&#30340;&#33539;&#22260;&#65292;&#24182;&#19988;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#22312;&#36828;&#36317;&#31163;&#30456;&#20851;&#30340;&#39033;&#30446;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#20154;&#31867;&#21028;&#26029;&#12290;&#35813;&#32467;&#26524;&#34920;&#26126;LLM&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#20256;&#32479;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#39564;&#35777;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#27010;&#24565;&#34920;&#31034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the potential of a large language model for aiding in generation of semantic feature norms - a critical tool for evaluating conceptual structure in cognitive science. Building from an existing human-generated dataset, we show that machine-verified norms capture aspects of conceptual structure beyond what is expressed in human norms alone, and better explain human judgments of semantic similarity amongst items that are distally related. The results suggest that LLMs can greatly enhance traditional methods of semantic feature norm verification, with implications for our understanding of conceptual representation in humans and machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20248;&#20110;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#21644;&#20854;&#20182;AL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.05578</link><description>&lt;p&gt;
&#20449;&#24687;&#37327;&#26159;&#21542;&#37325;&#35201;&#65311;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Does Informativeness Matter? Active Learning for Educational Dialogue Act Classification. (arXiv:2304.05578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20248;&#20110;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#21644;&#20854;&#20182;AL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#35805;&#34892;&#20026;&#65288;DA&#65289;&#65292;&#21487;&#20197;&#35299;&#37322;&#19987;&#23478;&#23548;&#24072;&#22312;&#36741;&#23548;&#36807;&#31243;&#20013;&#20570;&#20102;&#20160;&#20040;&#20197;&#21450;&#23398;&#29983;&#30693;&#36947;&#20160;&#20040;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23454;&#35777;&#30740;&#31350;&#22810;&#37319;&#29992;&#38543;&#26426;&#25277;&#26679;&#27861;&#26469;&#33719;&#24471;&#25163;&#21160;&#27880;&#37322;DA&#30340;&#21477;&#23376;&#26679;&#26412;&#65292;&#28982;&#21518;&#29992;&#20110;&#22521;&#35757;DA&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#30340;&#20449;&#24687;&#26679;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#21644;&#20854;&#20182;&#26368;&#26032;&#30340;AL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue Acts (DAs) can be used to explain what expert tutors do and what students know during the tutoring process. Most empirical studies adopt the random sampling method to obtain sentence samples for manual annotation of DAs, which are then used to train DA classifiers. However, these studies have paid little attention to sample informativeness, which can reflect the information quantity of the selected samples and inform the extent to which a classifier can learn patterns. Notably, the informativeness level may vary among the samples and the classifier might only need a small amount of low informative samples to learn the patterns. Random sampling may overlook sample informativeness, which consumes human labelling costs and contributes less to training the classifiers. As an alternative, researchers suggest employing statistical sampling methods of Active Learning (AL) to identify the informative samples for training the classifiers. However, the use of AL methods in educational D
&lt;/p&gt;</description></item><item><title>DIFFSTE&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#22330;&#26223;&#25991;&#23383;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#35774;&#35745;&#65292;&#21253;&#25324;&#23383;&#31526;&#32534;&#30721;&#22120;&#20197;&#25552;&#39640;&#25991;&#26412;&#28165;&#26224;&#24230;&#21644;&#25351;&#20196;&#32534;&#30721;&#22120;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#26679;&#24335;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#28210;&#26579;&#27491;&#30830;&#25991;&#26412;&#21644;&#25511;&#21046;&#25991;&#26412;&#26679;&#24335;&#26041;&#38754;&#30340;&#22256;&#38590;</title><link>http://arxiv.org/abs/2304.05568</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#32534;&#30721;&#22120;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#22330;&#26223;&#25991;&#23383;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Improving Diffusion Models for Scene Text Editing with Dual Encoders. (arXiv:2304.05568v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05568
&lt;/p&gt;
&lt;p&gt;
DIFFSTE&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#22330;&#26223;&#25991;&#23383;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#35774;&#35745;&#65292;&#21253;&#25324;&#23383;&#31526;&#32534;&#30721;&#22120;&#20197;&#25552;&#39640;&#25991;&#26412;&#28165;&#26224;&#24230;&#21644;&#25351;&#20196;&#32534;&#30721;&#22120;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#26679;&#24335;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#28210;&#26579;&#27491;&#30830;&#25991;&#26412;&#21644;&#25511;&#21046;&#25991;&#26412;&#26679;&#24335;&#26041;&#38754;&#30340;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#25991;&#23383;&#32534;&#36753;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22312;&#22270;&#20687;&#20013;&#20462;&#25913;&#25110;&#25554;&#20837;&#25351;&#23450;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#33258;&#28982;&#21644;&#36924;&#30495;&#30340;&#22806;&#35266;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#39118;&#26684;&#36716;&#31227;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21306;&#22495;&#35009;&#21098;&#20986;&#26469;&#24182;&#23558;&#23427;&#20204;&#39304;&#20837;&#22270;&#20687;&#36716;&#31227;&#27169;&#22411;&#65288;&#22914;GAN&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25913;&#21464;&#25991;&#26412;&#26679;&#24335;&#21644;&#25554;&#20837;&#25991;&#26412;&#21040;&#22270;&#20687;&#20013;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#26465;&#20214;&#19979;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#28210;&#26579;&#27491;&#30830;&#30340;&#25991;&#26412;&#21644;&#25511;&#21046;&#25991;&#26412;&#26679;&#24335;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#35774;&#35745;&#26469;&#25913;&#21892;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340; DIFFSTE&#65292;&#20854;&#20013;&#21253;&#25324;&#29992;&#20110;&#26356;&#22909;&#25991;&#26412;&#28165;&#26224;&#24230;&#30340;&#23383;&#31526;&#32534;&#30721;&#22120;&#21644;&#29992;&#20110;&#26356;&#22909;&#26679;&#24335;&#25511;&#21046;&#30340;&#25351;&#20196;&#32534;&#30721;&#22120;&#12290;&#24341;&#20837;&#25351;&#20196;&#35843;&#25972;&#26694;&#26550;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scene text editing is a challenging task that involves modifying or inserting specified texts in an image while maintaining its natural and realistic appearance. Most previous approaches to this task rely on style-transfer models that crop out text regions and feed them into image transfer models, such as GANs. However, these methods are limited in their ability to change text style and are unable to insert texts into images. Recent advances in diffusion models have shown promise in overcoming these limitations with text-conditional image editing. However, our empirical analysis reveals that state-of-the-art diffusion models struggle with rendering correct text and controlling text style. To address these problems, we propose DIFFSTE to improve pre-trained diffusion models with a dual encoder design, which includes a character encoder for better text legibility and an instruction encoder for better style control. An instruction tuning framework is introduced to train our model to learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;VAL-PAT&#65292;&#22312;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#30417;&#30563;&#19979;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#34892;&#20154;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#21508;&#31181;&#34892;&#20154;&#20998;&#26512;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#25105;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12289;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#23398;&#20064;&#21644;&#22810;&#23646;&#24615;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.05554</link><description>&lt;p&gt;
&#20174;&#22810;&#27169;&#24577;&#20449;&#24687;&#30417;&#30563;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#34892;&#20154;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Transferable Pedestrian Representation from Multimodal Information Supervision. (arXiv:2304.05554v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;VAL-PAT&#65292;&#22312;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#30417;&#30563;&#19979;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#34892;&#20154;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#21508;&#31181;&#34892;&#20154;&#20998;&#26512;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#25105;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12289;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#23398;&#20064;&#21644;&#22810;&#23646;&#24615;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26080;&#30417;&#30563;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;(reID)&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26410;&#26631;&#35760;&#30340;&#20154;&#29289;&#22270;&#20687;&#19978;&#39044;&#35757;&#32451;&#27604;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#23545;&#19979;&#28216;reID&#20219;&#21153;&#30340;&#24615;&#33021;&#20135;&#29983;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39044;&#20808;&#35757;&#32451;&#30340;&#26041;&#27861;&#26159;&#20026;reID&#19987;&#38376;&#35774;&#35745;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#20854;&#20182;&#34892;&#20154;&#20998;&#26512;&#20219;&#21153;&#30340;&#28789;&#27963;&#36866;&#24212;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;VAL-PAT&#65292;&#23427;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#65292;&#20197;&#22810;&#27169;&#24577;&#20449;&#24687;&#22686;&#24378;&#21508;&#31181;&#34892;&#20154;&#20998;&#26512;&#20219;&#21153;&#12290;&#20026;&#20102;&#35757;&#32451;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#23398;&#20064;&#30446;&#26631;&#65292;&#21363;&#33258;&#25105;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12289;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#23398;&#20064;&#21644;&#22810;&#23646;&#24615;&#20998;&#31867;&#12290;&#33258;&#25105;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#26377;&#21161;&#20110;&#23398;&#20064;&#22266;&#26377;&#30340;&#34892;&#20154;&#29305;&#24615;&#65292;&#32780;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#23398;&#20064;&#25351;&#23548;&#27169;&#22411;&#20851;&#27880;&#34892;&#20154;&#30340;&#22806;&#35266;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#22810;&#23646;&#24615;&#20998;&#31867;&#40723;&#21169;&#27169;&#22411;&#35782;&#21035;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent researches on unsupervised person re-identification~(reID) have demonstrated that pre-training on unlabeled person images achieves superior performance on downstream reID tasks than pre-training on ImageNet. However, those pre-trained methods are specifically designed for reID and suffer flexible adaption to other pedestrian analysis tasks. In this paper, we propose VAL-PAT, a novel framework that learns transferable representations to enhance various pedestrian analysis tasks with multimodal information. To train our framework, we introduce three learning objectives, \emph{i.e.,} self-supervised contrastive learning, image-text contrastive learning and multi-attribute classification. The self-supervised contrastive learning facilitates the learning of the intrinsic pedestrian properties, while the image-text contrastive learning guides the model to focus on the appearance information of pedestrians.Meanwhile, multi-attribute classification encourages the model to recognize attr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DynamicDet&#30340;&#29289;&#20307;&#26816;&#27979;&#21160;&#24577;&#26694;&#26550;&#65292;&#23427;&#20855;&#26377;&#33258;&#36866;&#24212;&#36335;&#30001;&#22120;&#12289;&#36864;&#20986;&#20934;&#21017;&#21644;&#21487;&#21464;&#36895;&#24230;&#25512;&#29702;&#31574;&#30053;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21487;&#23454;&#29616;&#22312;&#19981;&#21516;&#31934;&#24230;&#21644;&#36895;&#24230;&#20043;&#38388;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.05552</link><description>&lt;p&gt;
DynamicDet:&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#32479;&#19968;&#21160;&#24577;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
DynamicDet: A Unified Dynamic Architecture for Object Detection. (arXiv:2304.05552v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05552
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DynamicDet&#30340;&#29289;&#20307;&#26816;&#27979;&#21160;&#24577;&#26694;&#26550;&#65292;&#23427;&#20855;&#26377;&#33258;&#36866;&#24212;&#36335;&#30001;&#22120;&#12289;&#36864;&#20986;&#20934;&#21017;&#21644;&#21487;&#21464;&#36895;&#24230;&#25512;&#29702;&#31574;&#30053;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21487;&#23454;&#29616;&#22312;&#19981;&#21516;&#31934;&#24230;&#21644;&#36895;&#24230;&#20043;&#38388;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#30740;&#31350;&#35838;&#39064;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#25512;&#29702;&#65292;&#21160;&#24577;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#24378;&#22823;&#30340;&#21160;&#24577;&#26816;&#27979;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#32570;&#20047;&#36866;&#21512;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#21160;&#24577;&#26550;&#26500;&#21644;&#36864;&#20986;&#20934;&#21017;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DynamicDet&#30340;&#29289;&#20307;&#26816;&#27979;&#21160;&#24577;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26681;&#25454;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#24615;&#36136;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#26550;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36335;&#30001;&#22120;&#65292;&#29992;&#20110;&#20998;&#26512;&#22810;&#23610;&#24230;&#20449;&#24687;&#24182;&#33258;&#21160;&#20915;&#23450;&#25512;&#29702;&#36335;&#24452;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#27979;&#25439;&#22833;&#30340;&#36864;&#20986;&#20934;&#21017;&#30340;&#26032;&#22411;&#20248;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#25105;&#20204;&#30340;&#21160;&#24577;&#26816;&#27979;&#22120;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#21464;&#36895;&#24230;&#25512;&#29702;&#31574;&#30053;&#65292;&#24110;&#21161;&#21482;&#29992;&#19968;&#20010;&#21160;&#24577;&#26816;&#27979;&#22120;&#23454;&#29616;&#24191;&#27867;&#30340;&#31934;&#24230; - &#36895;&#24230;&#26435;&#34913;&#12290;&#22312;COCO&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic neural network is an emerging research topic in deep learning. With adaptive inference, dynamic models can achieve remarkable accuracy and computational efficiency. However, it is challenging to design a powerful dynamic detector, because of no suitable dynamic architecture and exiting criterion for object detection. To tackle these difficulties, we propose a dynamic framework for object detection, named DynamicDet. Firstly, we carefully design a dynamic architecture based on the nature of the object detection task. Then, we propose an adaptive router to analyze the multi-scale information and to decide the inference route automatically. We also present a novel optimization strategy with an exiting criterion based on the detection losses for our dynamic detectors. Last, we present a variable-speed inference strategy, which helps to realize a wide range of accuracy-speed trade-offs with only one dynamic detector. Extensive experiments conducted on the COCO benchmark demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CLCLSA&#65292;&#21487;&#29992;&#20110;&#19981;&#23436;&#25972;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#19968;&#20307;&#21270;&#65292;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05542</link><description>&lt;p&gt;
CLCLSA&#65306;&#20132;&#21449;&#32452;&#23398;&#38142;&#25509;&#30340;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#22810;&#32452;&#23398;&#25968;&#25454;&#19981;&#23436;&#25972;&#24773;&#20917;&#19979;&#19968;&#20307;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLCLSA: Cross-omics Linked embedding with Contrastive Learning and Self Attention for multi-omics integration with incomplete multi-omics data. (arXiv:2304.05542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CLCLSA&#65292;&#21487;&#29992;&#20110;&#19981;&#23436;&#25972;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#19968;&#20307;&#21270;&#65292;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#24322;&#36136;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#25972;&#21512;&#23545;&#20110;&#29702;&#35299;&#36951;&#20256;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#26410;&#37197;&#23545;&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#30001;&#20110;&#20202;&#22120;&#28789;&#25935;&#24230;&#21644;&#25104;&#26412;&#25152;&#23548;&#33268;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CLCLSA&#65292;&#21487;&#29992;&#20110;&#19981;&#23436;&#25972;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#19968;&#20307;&#21270;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#37319;&#29992;&#20132;&#21449;&#32452;&#23398;&#33258;&#32534;&#30721;&#22120;&#36328;&#36234;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#29289;&#25968;&#25454;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#65292;&#28982;&#21518;&#24341;&#20837;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#32852;&#21512;&#23398;&#20064;&#24050;&#37197;&#23545;&#21644;&#26410;&#37197;&#23545;&#25968;&#25454;&#30340;&#20844;&#20849;&#34920;&#31034;&#65292;&#26377;&#25928;&#22320;&#38598;&#25104;&#19981;&#23436;&#25972;&#25968;&#25454;&#12290;CLCLSA&#22312;&#22810;&#20010;&#22522;&#20934;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integration of heterogeneous and high-dimensional multi-omics data is becoming increasingly important in understanding genetic data. Each omics technique only provides a limited view of the underlying biological process and integrating heterogeneous omics layers simultaneously would lead to a more comprehensive and detailed understanding of diseases and phenotypes. However, one obstacle faced when performing multi-omics data integration is the existence of unpaired multi-omics data due to instrument sensitivity and cost. Studies may fail if certain aspects of the subjects are missing or incomplete. In this paper, we propose a deep learning method for multi-omics integration with incomplete data by Cross-omics Linked unified embedding with Contrastive Learning and Self Attention (CLCLSA). Utilizing complete multi-omics data as supervision, the model employs cross-omics autoencoders to learn the feature representation across different types of biological data. The multi-omics contrastive
&lt;/p&gt;</description></item><item><title>MoMo&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#20849;&#20139;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#22791;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21333;&#19968;&#30340;&#21464;&#21387;&#22120;&#21644;&#38454;&#27573;&#24615;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#20445;&#30041;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#19982;&#24378;&#27169;&#22411;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05523</link><description>&lt;p&gt;
MoMo: &#19968;&#20010;&#20849;&#20139;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MoMo: A shared encoder Model for text, image and multi-Modal representations. (arXiv:2304.05523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05523
&lt;/p&gt;
&lt;p&gt;
MoMo&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#20849;&#20139;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#22791;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21333;&#19968;&#30340;&#21464;&#21387;&#22120;&#21644;&#38454;&#27573;&#24615;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#20445;&#30041;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#19982;&#24378;&#27169;&#22411;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#20849;&#20139;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#23427;&#22312;&#20960;&#20010;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#25968;&#25454;&#12289;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#25928;&#29575;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#30456;&#27604;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#21464;&#21387;&#22120;&#65292;&#25152;&#26377;&#32534;&#30721;&#22120;&#23618;&#22788;&#29702;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20854;&#20013;&#27169;&#22411;&#39318;&#20808;&#22312;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#21333;&#27169;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#26368;&#21518;&#22312;&#25991;&#26412;&#21644;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#22312;&#20004;&#31181;&#27169;&#24335;&#19979;&#20445;&#30041;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35757;&#32451;&#31649;&#36947;&#65292;&#23427;&#22312;&#27599;&#20010;&#35757;&#32451;&#26356;&#26032;&#27493;&#39588;&#26102;&#21516;&#26102;&#20174;&#19981;&#21516;&#27169;&#24577;&#30340;&#26799;&#24230;&#26356;&#26032;&#20013;&#23398;&#20064;&#12290;&#19979;&#28216;&#30340;&#32431;&#25991;&#26412;&#12289;&#32431;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20960;&#20010;&#24378;&#27169;&#22411;&#31454;&#20105;&#65292;&#21516;&#26102;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;MoMo&#22312;&#19982;FLAVA&#30340;&#31454;&#20105;&#20013;&#34920;&#29616;&#24471;&#24456;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a self-supervised shared encoder model that achieves strong results on several visual, language and multimodal benchmarks while being data, memory and run-time efficient. We make three key contributions. First, in contrast to most existing works, we use a single transformer with all the encoder layers processing both the text and the image modalities. Second, we propose a stage-wise training strategy where the model is first trained on images, then jointly with unimodal text and image datasets and finally jointly with text and text-image datasets. Third, to preserve information across both the modalities, we propose a training pipeline that learns simultaneously from gradient updates of different modalities at each training update step. The results on downstream text-only, image-only and multimodal tasks show that our model is competitive with several strong models while using fewer parameters and lesser pre-training data. For example, MoMo performs competitively with FLAVA 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#19981;&#21464;&#38598;&#30340;&#24212;&#29992;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.05509</link><description>&lt;p&gt;
&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#22312;&#36807;&#31243;&#25511;&#21046;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#24182;&#20445;&#35777;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Control invariant set enhanced reinforcement learning for process control: improved sampling efficiency and guaranteed stability. (arXiv:2304.05509v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#19981;&#21464;&#38598;&#30340;&#24212;&#29992;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22240;&#20854;&#22788;&#29702;&#23454;&#38469;&#24212;&#29992;&#20013;&#20851;&#38190;&#30340;&#23433;&#20840;&#24615;&#32422;&#26463;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#12290;&#23427;&#21033;&#29992;&#25511;&#21046;&#19981;&#21464;&#38598;&#30340;&#20248;&#28857;&#26469;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#31163;&#32447;&#38454;&#27573;&#21644;&#22312;&#32447;&#38454;&#27573;&#12290;&#31163;&#32447;&#38454;&#27573;&#23558;&#25511;&#21046;&#19981;&#21464;&#38598;&#32435;&#20837;&#22870;&#21169;&#35774;&#35745;&#12289;&#21021;&#22987;&#29366;&#24577;&#37319;&#26679;&#21644;&#29366;&#24577;&#37325;&#32622;&#31243;&#24207;&#20013;&#12290;&#22312;&#22312;&#32447;&#38454;&#27573;&#65292;&#24403;&#29366;&#24577;&#22312;&#25511;&#21046;&#19981;&#21464;&#38598;&#20043;&#22806;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20197;&#28385;&#36275;&#31283;&#23450;&#24615;&#26631;&#20934;&#12290;&#21033;&#29992;&#25511;&#21046;&#19981;&#21464;&#38598;&#30340;&#26174;&#24335;&#24418;&#24335;&#24471;&#21040;&#20102;&#19968;&#20010;&#22791;&#20221;&#34920;&#26469;&#20445;&#35777;&#22312;&#32447;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#27169;&#25311;&#21270;&#23398;&#21453;&#24212;&#22120;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31163;&#32447;&#35757;&#32451;&#20013;&#37319;&#26679;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is an area of significant research interest, and safe RL in particular is attracting attention due to its ability to handle safety-driven constraints that are crucial for real-world applications of RL algorithms. This work proposes a novel approach to RL training, called control invariant set (CIS) enhanced RL, which leverages the benefits of CIS to improve stability guarantees and sampling efficiency. The approach consists of two learning stages: offline and online. In the offline stage, CIS is incorporated into the reward design, initial state sampling, and state reset procedures. In the online stage, RL is retrained whenever the state is outside of CIS, which serves as a stability criterion. A backup table that utilizes the explicit form of CIS is obtained to ensure the online stability. To evaluate the proposed approach, we apply it to a simulated chemical reactor. The results show a significant improvement in sampling efficiency during offline training 
&lt;/p&gt;</description></item><item><title>DistHD&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#23398;&#20064;&#24863;&#30693;&#21160;&#24577;&#32534;&#30721;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#37325;&#26500;&#24433;&#21709;&#23398;&#20064;&#36136;&#37327;&#30340;&#32500;&#24230;&#65292;&#20197;&#26174;&#33879;&#36739;&#20302;&#30340;&#32500;&#24230;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.05503</link><description>&lt;p&gt;
DistHD: &#19968;&#31181;&#36866;&#29992;&#20110;&#36229;&#39640;&#32500;&#20998;&#31867;&#30340;&#23398;&#20064;&#24863;&#30693;&#21160;&#24577;&#32534;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DistHD: A Learner-Aware Dynamic Encoding Method for Hyperdimensional Classification. (arXiv:2304.05503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05503
&lt;/p&gt;
&lt;p&gt;
DistHD&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#23398;&#20064;&#24863;&#30693;&#21160;&#24577;&#32534;&#30721;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#37325;&#26500;&#24433;&#21709;&#23398;&#20064;&#36136;&#37327;&#30340;&#32500;&#24230;&#65292;&#20197;&#26174;&#33879;&#36739;&#20302;&#30340;&#32500;&#24230;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21551;&#21457;&#20110;&#20154;&#33041;&#30340;&#36229;&#39640;&#32500;&#35745;&#31639;&#25216;&#26415;(HDC)&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#26377;&#21069;&#36884;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#38745;&#24577;&#32534;&#30721;&#22120;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20174;&#19981;&#26356;&#26032;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#38750;&#24120;&#39640;&#30340;&#32500;&#24230;&#25165;&#33021;&#33719;&#24471;&#36275;&#22815;&#30340;&#20934;&#30830;&#24615;&#65292;&#20005;&#37325;&#38477;&#20302;&#32534;&#30721;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;DistHD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#32534;&#30721;&#25216;&#26415;&#65292;&#29992;&#20110;HDC&#33258;&#36866;&#24212;&#23398;&#20064;&#65292;&#33021;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#37325;&#26500;&#23545;&#20998;&#31867;&#20135;&#29983;&#35823;&#23548;&#24182;&#24433;&#21709;&#23398;&#20064;&#36136;&#37327;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;DistHD&#25104;&#21151;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20197;&#26174;&#33879;&#36739;&#20302;&#30340;&#32500;&#24230;&#36798;&#21040;&#20102;&#25152;&#26399;&#26395;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired hyperdimensional computing (HDC) has been recently considered a promising learning approach for resource-constrained devices. However, existing approaches use static encoders that are never updated during the learning process. Consequently, it requires a very high dimensionality to achieve adequate accuracy, severely lowering the encoding and training efficiency. In this paper, we propose DistHD, a novel dynamic encoding technique for HDC adaptive learning that effectively identifies and regenerates dimensions that mislead the classification and compromise the learning quality. Our proposed algorithm DistHD successfully accelerates the learning process and achieves the desired accuracy with considerably lower dimensionality.
&lt;/p&gt;</description></item><item><title>GraphGANFed&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;GCN&#12289;GAN&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21161;&#21147;&#22522;&#20110;&#22270;&#30340;&#20998;&#23376;&#30340;&#33647;&#29289;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05498</link><description>&lt;p&gt;
GraphGANFed: &#38024;&#23545;&#22522;&#20110;&#22270;&#30340;&#20998;&#23376;&#30340;&#32852;&#21512;&#29983;&#25104;&#26694;&#26550;&#65292;&#21161;&#21147;&#39640;&#25928;&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GraphGANFed: A Federated Generative Framework for Graph-Structured Molecules Towards Efficient Drug Discovery. (arXiv:2304.05498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05498
&lt;/p&gt;
&lt;p&gt;
GraphGANFed&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;GCN&#12289;GAN&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21161;&#21147;&#22522;&#20110;&#22270;&#30340;&#20998;&#23376;&#30340;&#33647;&#29289;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21152;&#36895;&#20102;&#23427;&#22312;&#21508;&#20010;&#39046;&#22495;&#24212;&#29992;&#65292;&#22914;&#32454;&#32990;&#22270;&#20687;&#20998;&#26512;&#21644;&#20998;&#23376;&#21457;&#29616;&#12290;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22240;&#20854;&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#22823;&#22411;&#20998;&#23376;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24182;&#29983;&#25104;&#20445;&#30041;&#30456;&#20284;&#24615;&#36136;&#30340;&#26032;&#20998;&#23376;&#30340;&#33021;&#21147;&#32780;&#25104;&#20026;&#39318;&#36873;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#21046;&#33647;&#20844;&#21496;&#21487;&#33021;&#19981;&#24895;&#24847;&#25110;&#26080;&#27861;&#20849;&#20139;&#20854;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#30001;&#20110;&#20998;&#23376;&#25968;&#25454;&#38598;&#30340;&#22320;&#29702;&#20998;&#24067;&#21644;&#25935;&#24863;&#24615;&#36136;&#65292;&#36825;&#20351;&#24471;&#22312;&#38598;&#20013;&#24335;&#26041;&#24335;&#20013;&#35757;&#32451;GAN&#21464;&#24471;&#19981;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#12289;GAN&#21644;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#25972;&#20307;&#31995;&#32479;&#30340;GraphGANFed&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have accelerated its use in various applications, such as cellular image analysis and molecular discovery. In molecular discovery, a generative adversarial network (GAN), which comprises a discriminator to distinguish generated molecules from existing molecules and a generator to generate new molecules, is one of the premier technologies due to its ability to learn from a large molecular data set efficiently and generate novel molecules that preserve similar properties. However, different pharmaceutical companies may be unwilling or unable to share their local data sets due to the geo-distributed and sensitive nature of molecular data sets, making it impossible to train GANs in a centralized manner. In this paper, we propose a Graph convolutional network in Generative Adversarial Networks via Federated learning (GraphGANFed) framework, which integrates graph convolutional neural Network (GCN), GAN, and federated learning (FL) as a whole system to genera
&lt;/p&gt;</description></item><item><title>KGS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#22240;&#26524;&#36793;&#32536;&#20449;&#24687;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.05493</link><description>&lt;p&gt;
KGS&#65306;&#21033;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#36138;&#23146;&#31561;&#20215;&#25628;&#32034;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
KGS: Causal Discovery Using Knowledge-guided Greedy Equivalence Search. (arXiv:2304.05493v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05493
&lt;/p&gt;
&lt;p&gt;
KGS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#22240;&#26524;&#36793;&#32536;&#20449;&#24687;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#33021;&#25552;&#20379;&#20851;&#20110;&#28508;&#22312;&#30340;&#22240;&#26524;&#26426;&#21046;&#21644;&#21487;&#33021;&#30340;&#22240;&#26524;&#22270;&#31354;&#38388;&#30340;&#36275;&#22815;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#26041;&#27861;&#25628;&#32034;&#22270;&#31561;&#20215;&#31867;&#30340;&#31354;&#38388;&#65292;&#22914;&#36138;&#23146;&#31561;&#20215;&#25628;&#32034;&#65288;GES&#65289;&#65292;&#25628;&#32034;&#31354;&#38388;&#36890;&#24120;&#20250;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20808;&#21069;&#30340;&#22240;&#26524;&#20449;&#24687;&#65292;&#20363;&#22914;&#26377;&#26080;&#22240;&#26524;&#36793;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#25351;&#23548;&#21457;&#29616;&#36807;&#31243;&#65292;&#20351;&#20854;&#36208;&#21521;&#26356;&#20026;&#21463;&#38480;&#19988;&#20934;&#30830;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;KGS&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#30340;&#36138;&#23146;&#20998;&#25968;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#21644;&#32467;&#26500;&#20808;&#39564;&#65288;&#22240;&#26524;&#36793;&#65289;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;KGS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#30693;&#35782;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#20004;&#20010;&#21464;&#37327;&#20043;&#38388;&#30340;&#20808;&#21069;&#36793;&#32536;&#20449;&#24687;&#65292;&#21253;&#25324;&#26377;&#21521;&#36793;&#65292;&#26080;&#36793;&#21644;&#26080;&#21521;&#36793;&#30340;&#23384;&#22312;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning causal relationships solely from observational data provides insufficient information about the underlying causal mechanism and the search space of possible causal graphs. As a result, often the search space can grow exponentially for approaches such as Greedy Equivalence Search (GES) that uses a score-based approach to search the space of equivalence classes of graphs. Prior causal information such as the presence or absence of a causal edge can be leveraged to guide the discovery process towards a more restricted and accurate search space. In this study, we present KGS, a knowledge-guided greedy score-based causal discovery approach that uses observational data and structural priors (causal edges) as constraints to learn the causal graph. KGS is a novel application of knowledge constraints that can leverage any of the following prior edge information between any two variables: the presence of a directed edge, the absence of an edge, and the presence of an undirected edge. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#35302;&#21457;&#30340;&#24322;&#26500;&#22270;&#32593;&#32476;&#30340;&#20004;&#38454;&#27573;&#21463;&#20247;&#25193;&#23637;&#26041;&#26696;&#65292;&#21487;&#20197;&#32771;&#34385;&#19981;&#21516;&#30340;&#21452;&#38754;&#20132;&#20114;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.05474</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#35302;&#21457;&#24322;&#26500;&#22270;&#32593;&#32476;&#30340;&#22810;&#33410;&#30446;&#21457;&#34892;&#30340;&#21463;&#20247;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Audience Expansion for Multi-show Release Based on an Edge-prompted Heterogeneous Graph Network. (arXiv:2304.05474v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#35302;&#21457;&#30340;&#24322;&#26500;&#22270;&#32593;&#32476;&#30340;&#20004;&#38454;&#27573;&#21463;&#20247;&#25193;&#23637;&#26041;&#26696;&#65292;&#21487;&#20197;&#32771;&#34385;&#19981;&#21516;&#30340;&#21452;&#38754;&#20132;&#20114;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#24179;&#21488;&#19978;&#65292;&#38024;&#23545;&#26032;&#33410;&#30446;&#36827;&#34892;&#21463;&#20247;&#23450;&#20301;&#21644;&#25193;&#23637;&#30340;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#29983;&#25104;&#23427;&#20204;&#30340;&#23884;&#20837;&#12290;&#24212;&#35813;&#20174;&#29992;&#25143;&#21644;&#33410;&#30446;&#30340;&#35282;&#24230;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36861;&#27714;&#21363;&#26102;&#65288;&#28857;&#20987;&#65289;&#21644;&#38271;&#26399;&#65288;&#35266;&#30475;&#26102;&#38388;&#65289;&#22870;&#21169;&#65292;&#20197;&#21450;&#26032;&#33410;&#30446;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#38382;&#39064;&#36866;&#21512;&#36890;&#36807;&#24322;&#26500;&#22270;&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#65292;&#22240;&#20026;&#25968;&#25454;&#20855;&#26377;&#33258;&#28982;&#30340;&#22270;&#32467;&#26500;&#12290;&#20294;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32593;&#32476;&#36890;&#24120;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#33410;&#28857;&#21644;&#21508;&#31181;&#31867;&#22411;&#30340;&#36793;&#32536;&#12290;&#24456;&#23569;&#26377;&#29616;&#26377;&#26041;&#27861;&#19987;&#27880;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#24182;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#36793;&#32536;&#65292;&#29305;&#21035;&#26159;&#21518;&#32773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#35302;&#21457;&#30340;&#24322;&#26500;&#22270;&#32593;&#32476;&#30340;&#20004;&#38454;&#27573;&#21463;&#20247;&#25193;&#23637;&#26041;&#26696;&#65292;&#21487;&#20197;&#32771;&#34385;&#19981;&#21516;&#30340;&#21452;&#38754;&#20132;&#20114;&#21644;&#29305;&#24449;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;&#36873;&#25321;&#29992;&#25143;ID&#21644;&#26174;&#31034;&#22120;&#30340;&#29305;&#23450;&#36793;&#32536;&#20449;&#24687;&#32452;&#21512;&#26469;&#26500;&#24314;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the user targeting and expanding of new shows on a video platform, the key point is how their embeddings are generated. It's supposed to be personalized from the perspective of both users and shows. Furthermore, the pursue of both instant (click) and long-time (view time) rewards, and the cold-start problem for new shows bring additional challenges. Such a problem is suitable for processing by heterogeneous graph models, because of the natural graph structure of data. But real-world networks usually have billions of nodes and various types of edges. Few existing methods focus on handling large-scale data and exploiting different types of edges, especially the latter. In this paper, we propose a two-stage audience expansion scheme based on an edge-prompted heterogeneous graph network which can take different double-sided interactions and features into account. In the offline stage, to construct the graph, user IDs and specific side information combinations of the shows are chosen to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29289;&#29702;&#28210;&#26579;&#22120;&#65292;&#20351;&#29992;&#20809;&#37319;&#26679;&#22330;&#21644;BRDF&#34920;&#31034;&#26469;&#27169;&#25311;&#30452;&#25509;&#21644;&#38388;&#25509;&#20809;&#29031;&#65292;&#24182;&#23454;&#29616;&#22797;&#26434;&#23545;&#35937;&#30340;&#28210;&#26579;&#12290;</title><link>http://arxiv.org/abs/2304.05472</link><description>&lt;p&gt;
&#22522;&#20110;&#20809;&#37319;&#26679;&#22330;&#21644;BRDF&#34920;&#31034;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#31070;&#32463;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
Light Sampling Field and BRDF Representation for Physically-based Neural Rendering. (arXiv:2304.05472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29289;&#29702;&#28210;&#26579;&#22120;&#65292;&#20351;&#29992;&#20809;&#37319;&#26679;&#22330;&#21644;BRDF&#34920;&#31034;&#26469;&#27169;&#25311;&#30452;&#25509;&#21644;&#38388;&#25509;&#20809;&#29031;&#65292;&#24182;&#23454;&#29616;&#22797;&#26434;&#23545;&#35937;&#30340;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#28210;&#26579;&#23545;&#20110;&#23637;&#31034;&#35745;&#31639;&#26426;&#22270;&#24418;&#36164;&#20135;&#20013;&#30340;&#35814;&#32454;&#36924;&#30495;&#30340;&#22330;&#26223;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#21046;&#20316;&#29289;&#29702;&#28210;&#26579;&#25928;&#26524;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29289;&#29702;&#28210;&#26579;&#22120;&#65292;&#20197;&#28040;&#38500;&#35774;&#22791;&#20381;&#36182;&#24615;&#24182;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#23398;&#20064;&#30340;&#20809;&#37319;&#26679;&#22330;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#29031;&#34920;&#31034;&#26041;&#27861;&#26469;&#27169;&#25311;&#30452;&#25509;&#21644;&#38388;&#25509;&#20809;&#29031;&#65292;&#24182;&#25552;&#20986;&#20102;BRDF&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#23545;&#35937;&#30340;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physically-based rendering (PBR) is key for immersive rendering effects used widely in the industry to showcase detailed realistic scenes from computer graphics assets. A well-known caveat is that producing the same is computationally heavy and relies on complex capture devices. Inspired by the success in quality and efficiency of recent volumetric neural rendering, we want to develop a physically-based neural shader to eliminate device dependency and significantly boost performance. However, no existing lighting and material models in the current neural rendering approaches can accurately represent the comprehensive lighting models and BRDFs properties required by the PBR process. Thus, this paper proposes a novel lighting representation that models direct and indirect light locally through a light sampling strategy in a learned light sampling field. We also propose BRDF models to separately represent surface/subsurface scattering details to enable complex objects such as translucent 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#65292;&#35774;&#35745;&#20102;&#19977;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#25286;&#20998;&#20219;&#21153;&#24182;&#35780;&#20272;ChatGPT&#65292;&#23454;&#39564;&#34920;&#26126;ChatGPT&#30340;&#34920;&#29616;&#19982;&#30417;&#30563;&#26041;&#27861;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#20294;&#23427;&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#25512;&#26029;&#20986;&#26356;&#22810;&#30340;&#23567;&#20851;&#31995;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.05454</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Temporal Relation Extraction with ChatGPT. (arXiv:2304.05454v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;ChatGPT&#30340;&#38646;&#26679;&#26412;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#65292;&#35774;&#35745;&#20102;&#19977;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#25286;&#20998;&#20219;&#21153;&#24182;&#35780;&#20272;ChatGPT&#65292;&#23454;&#39564;&#34920;&#26126;ChatGPT&#30340;&#34920;&#29616;&#19982;&#30417;&#30563;&#26041;&#27861;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#20294;&#23427;&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#25512;&#26029;&#20986;&#26356;&#22810;&#30340;&#23567;&#20851;&#31995;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#25991;&#26723;&#20013;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290; &#30417;&#30563;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;ChatGPT&#22312;&#38646;&#26679;&#26412;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#25286;&#20998;&#20219;&#21153;&#24182;&#35780;&#20272;ChatGPT&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19982;&#30417;&#30563;&#26041;&#27861;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#32780;&#19988;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25552;&#31034;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;ChatGPT&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#25512;&#26029;&#20986;&#26356;&#22810;&#30340;&#23567;&#20851;&#31995;&#31867;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;ChatGPT&#22312;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#29616;&#26377;&#32570;&#38519;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#22312;&#26102;&#38388;&#25512;&#26029;&#36807;&#31243;&#20013;&#26080;&#27861;&#20445;&#25345;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#22312;&#20027;&#21160;&#38271;&#20381;&#36182;&#26102;&#38388;&#25512;&#26029;&#20013;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of temporal relation extraction is to infer the temporal relation between two events in the document. Supervised models are dominant in this task. In this work, we investigate ChatGPT's ability on zero-shot temporal relation extraction. We designed three different prompt techniques to break down the task and evaluate ChatGPT. Our experiments show that ChatGPT's performance has a large gap with that of supervised methods and can heavily rely on the design of prompts. We further demonstrate that ChatGPT can infer more small relation classes correctly than supervised methods. The current shortcomings of ChatGPT on temporal relation extraction are also discussed in this paper. We found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency temporal inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#26426;&#26800;&#23646;&#24615;&#20197;&#23454;&#29616;&#21453;&#21521;&#35774;&#35745;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.05422</link><description>&lt;p&gt;
&#21487;&#21306;&#20998;&#30340;&#22270;&#32467;&#26500;&#27169;&#22411;&#29992;&#20110;&#26230;&#26684;&#26448;&#26009;&#21453;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Differentiable graph-structured models for inverse design of lattice materials. (arXiv:2304.05422v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#26426;&#26800;&#23646;&#24615;&#20197;&#23454;&#29616;&#21453;&#21521;&#35774;&#35745;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#20110;&#28145;&#31354;&#24694;&#21155;&#29615;&#22659;&#20013;&#33021;&#22815;&#26681;&#25454;&#38656;&#35201;&#33258;&#36866;&#24212;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#30340;&#26448;&#26009;&#23558;&#22312;&#23450;&#20041;&#26410;&#26469;&#30340;&#31354;&#38388;&#25506;&#32034;&#26041;&#38754;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#33258;&#28982;&#30028;&#20013;&#65292;&#24494;&#22937;&#30340;&#24494;&#35266;&#32467;&#26500;&#21644;&#26684;&#23376;&#20960;&#20309;&#24418;&#29366;&#26159;&#35774;&#35745;&#36866;&#24212;&#20110;&#29305;&#23450;&#29615;&#22659;&#26448;&#26009;&#30340;&#20196;&#20154;&#20852;&#22859;&#30340;&#28789;&#24863;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#19981;&#35268;&#21017;&#25299;&#25169;&#35206;&#30422;&#30340;&#24040;&#22823;&#35774;&#35745;&#31354;&#38388;&#65292;&#22312;&#20998;&#26512;&#19978;&#36827;&#34892;&#25506;&#32034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#21512;&#25104;&#26230;&#26684;&#26448;&#26009;&#37117;&#26159;&#22522;&#20110;&#21608;&#26399;&#24615;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#23545;&#35268;&#21017;&#21644;&#19981;&#35268;&#21017;&#26230;&#26684;&#26448;&#26009;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#21147;&#23398;&#24615;&#36136;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#26469;&#35843;&#25972;&#21333;&#20010;&#26230;&#26684;&#20803;&#32032;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#23646;&#24615;&#65292;&#20174;&#32780;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26448;&#26009;&#12290;&#24341;&#20837;&#23545;&#26230;&#26684;&#32467;&#26500;&#21644;&#26448;&#26009;&#23646;&#24615;&#30340;&#38544;&#24335;&#21487;&#23398;&#20064;&#20960;&#20309;&#34920;&#31034;&#65292;&#32467;&#21512;&#21453;&#35774;&#35745;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Materials possessing flexible physico-chemical properties that adapt on-demand to the hostile environmental conditions of deep space will become essential in defining the future of space exploration. A promising venue for inspiration towards the design of environment-specific materials is in the intricate micro-architectures and lattice geometry found throughout nature. However, the immense design space covered by such irregular topologies is challenging to probe analytically. For this reason, most synthetic lattice materials have to date been based on periodic architectures instead. Here, we propose a computational approach using a graph representation for both regular and irregular lattice materials. Our method uses differentiable message passing algorithms to calculate mechanical properties, and therefore allows using automatic differentiation to adjust both the geometric structure and attributes of individual lattice elements to design materials with desired properties. The introdu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;DARTS&#26041;&#27861;&#30340;&#36129;&#29486;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.05405</link><description>&lt;p&gt;
&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#39640;&#25928;&#33258;&#21160;&#21270;:&#19968;&#39033;&#27010;&#36848;&#30740;&#31350;(arXiv: 2304.05405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search. (arXiv:2304.05405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;DARTS&#26041;&#27861;&#30340;&#36129;&#29486;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;DNAS&#65289;&#36805;&#36895;&#25104;&#20026;&#33258;&#21160;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290; &#36825;&#31181;&#23835;&#36215;&#20027;&#35201;&#24402;&#21151;&#20110;DARTS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#37325;&#35201;&#30340;DNAS&#26041;&#27861;&#20043;&#19968;&#12290; &#19982;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25110;&#36827;&#21270;&#31639;&#27861;&#30340;&#20197;&#21069;&#30340;&#20316;&#21697;&#30456;&#27604;&#65292;DNAS&#36895;&#24230;&#24555;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#12290; &#22312;&#36825;&#31687;&#20840;&#38754;&#30340;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#19987;&#38376;&#20851;&#27880;DNAS&#24182;&#23457;&#26597;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;DNAS&#26041;&#27861;&#12290; &#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36807;&#21435;&#20960;&#24180;&#23545;DNAS&#24102;&#26469;&#30340;&#36129;&#29486;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;NAS&#39046;&#22495;&#30340;&#24433;&#21709;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#26469;&#20570;&#20986;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, Differentiable Neural Architecture Search (DNAS) rapidly imposed itself as the trending approach to automate the discovery of deep neural network architectures. This rise is mainly due to the popularity of DARTS, one of the first major DNAS methods. In contrast with previous works based on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster by several orders of magnitude and uses fewer computational resources. In this comprehensive survey, we focus specifically on DNAS and review recent approaches in this field. Furthermore, we propose a novel challenge-based taxonomy to classify DNAS methods. We also discuss the contributions brought to DNAS in the past few years and its impact on the global NAS field. Finally, we conclude by giving some insights into future research directions for the DNAS field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.05265</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#25991;&#26412;&#20026;&#24341;&#23548;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#24403;&#24341;&#23548;&#20449;&#24687;&#21253;&#21547;&#29992;&#25143;&#23450;&#20041;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25110;&#38271;&#23614;&#27010;&#24565;&#26631;&#35760;&#26102;&#65292;&#25991;&#26412;&#21453;&#36716;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#29983;&#25104;&#25216;&#26415;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#23637;&#31034;&#20102;&#25991;&#26412;&#21453;&#36716;&#30340;&#37096;&#32626;&#20173;&#20805;&#28385;&#20102;&#8220;&#40657;&#39764;&#27861;&#8221;&#65292;&#20363;&#22914;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#20005;&#33499;&#35201;&#27714;&#65292;&#22312;&#24490;&#29615;&#20013;&#38656;&#35201;&#33392;&#33510;&#30340;&#20154;&#21147;&#25104;&#26412;&#21644;&#32570;&#20047;&#40065;&#26834;&#24615;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;&#30340;&#22823;&#22823;&#22686;&#24378;&#29256;&#21453;&#36716;&#65292;&#35299;&#20915;&#20102;&#25152;&#26377;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#21453;&#36807;&#26469;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;COTI&#30340;&#26680;&#24515;&#26159;&#22522;&#20110;&#29702;&#35770;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#20855;&#26377;&#20840;&#38754;&#21644;&#26032;&#39062;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#30001;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#25152;&#25552;&#21462;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;COTI&#30340;&#24615;&#33021;&#27604;&#20043;&#21069;&#25216;&#26415;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#65292;&#20197;&#20811;&#26381;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#21644;&#31435;&#27861;&#26426;&#26500;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#36880;&#27493;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#24688;&#24403;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2304.04914</link><description>&lt;p&gt;
&#30417;&#31649;&#24066;&#22330;&#65306;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Regulatory Markets: The Future of AI Governance. (arXiv:2304.04914v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#65292;&#20197;&#20811;&#26381;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#21644;&#31435;&#27861;&#26426;&#26500;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#36880;&#27493;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#24688;&#24403;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24688;&#24403;&#22320;&#30417;&#31649;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26085;&#30410;&#32039;&#36843;&#30340;&#25919;&#31574;&#25361;&#25112;&#12290;&#31435;&#27861;&#26426;&#26500;&#21644;&#30417;&#31649;&#26426;&#26500;&#32570;&#20047;&#32763;&#35793;&#20844;&#20247;&#38656;&#27714;&#20026;&#27861;&#24459;&#35201;&#27714;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#26410;&#33021;&#20351;AI&#31995;&#32479;&#30340;&#29983;&#20135;&#32773;&#21644;&#20351;&#29992;&#32773;&#23545;&#27665;&#20027;&#35201;&#27714;&#36127;&#36131;&#12290;&#25552;&#20986;&#20102;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20811;&#26381;&#21629;&#20196;&#21644;&#25511;&#21046;&#30417;&#31649;&#21644;&#33258;&#25105;&#30417;&#31649;&#30340;&#23616;&#38480;&#24615;&#12290;&#30417;&#31649;&#24066;&#22330;&#21487;&#20197;&#20351;&#25919;&#24220;&#20026;AI&#30417;&#31649;&#24314;&#31435;&#25919;&#31574;&#20248;&#20808;&#32423;&#65292;&#21516;&#26102;&#20381;&#38752;&#24066;&#22330;&#21147;&#37327;&#21644;&#34892;&#19994;&#30740;&#21457;&#21162;&#21147;&#26469;&#24320;&#21019;&#26368;&#33021;&#23454;&#29616;&#25919;&#31574;&#21046;&#23450;&#32773;&#22768;&#26126;&#30446;&#26631;&#30340;&#30417;&#31649;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Appropriately regulating artificial intelligence is an increasingly urgent policy challenge. Legislatures and regulators lack the specialized knowledge required to best translate public demands into legal requirements. Overreliance on industry self-regulation fails to hold producers and users of AI systems accountable to democratic demands. Regulatory markets, in which governments require the targets of regulation to purchase regulatory services from a private regulator, are proposed. This approach to AI regulation could overcome the limitations of both command-and-control regulation and self-regulation. Regulatory market could enable governments to establish policy priorities for the regulation of AI, whilst relying on market forces and industry R&amp;D efforts to pioneer the methods of regulation that best achieve policymakers' stated objectives.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;COhort Representation lEarning&#65288;CORE&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.04468</link><description>&lt;p&gt;
&#23454;&#29616;&#38431;&#21015;&#26234;&#33021;&#21270;&#65306;&#19968;&#31181;&#38024;&#23545;&#30005;&#23376;&#30149;&#21382;&#20998;&#26512;&#30340;&#36890;&#29992;&#32676;&#20307;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis. (arXiv:2304.04468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04468
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;COhort Representation lEarning&#65288;CORE&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#26159;&#20174;&#20020;&#24202;&#24120;&#35268;&#25252;&#29702;&#20013;&#29983;&#25104;&#30340;&#65292;&#35760;&#24405;&#20102;&#24191;&#27867;&#30340;&#30149;&#20154;&#20154;&#32676;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20026;&#25913;&#21892;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#30149;&#20154;&#31649;&#29702;&#21644;&#24178;&#39044;&#31574;&#30053;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#21033;&#29992;EHR&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#27969;&#34892;&#30340;EHR&#25968;&#25454;&#20998;&#26512;&#33539;&#24335;&#26159;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#23427;&#39318;&#20808;&#21033;&#29992;&#21333;&#20010;&#30149;&#20154;&#30340;EHR&#25968;&#25454;&#36890;&#36807;&#19968;&#20010;&#20027;&#24178;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#25903;&#25345;&#24314;&#31435;&#22312;&#36825;&#20123;&#34920;&#31034;&#30340;&#22810;&#26679;&#21270;&#30340;&#21307;&#30103;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#26080;&#27861;&#28145;&#20837;&#20998;&#26512;&#30149;&#20154;&#30340;&#30456;&#20851;&#24615;&#65292;&#36890;&#24120;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#34987;&#31216;&#20026;&#38431;&#21015;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21516;&#19968;&#38431;&#21015;&#20013;&#30340;&#30149;&#20154;&#20542;&#21521;&#20110;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#65292;&#34920;&#26126;&#20182;&#20204;&#22312;&#21307;&#30103;&#26465;&#20214;&#65288;&#22914;&#30151;&#29366;&#25110;&#30142;&#30149;&#65289;&#26041;&#38754;&#20855;&#26377;&#30456;&#20284;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;COhort Representation lEarning (CORE)&#26694;&#26550;&#26469;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38431;&#21015;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23545;&#32676;&#20307;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#24182;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHR) are generated from clinical routine care recording valuable information of broad patient populations, which provide plentiful opportunities for improving patient management and intervention strategies in clinical practice. To exploit the enormous potential of EHR data, a popular EHR data analysis paradigm in machine learning is EHR representation learning, which first leverages the individual patient's EHR data to learn informative representations by a backbone, and supports diverse health-care downstream tasks grounded on the representations. Unfortunately, such a paradigm fails to access the in-depth analysis of patients' relevance, which is generally known as cohort studies in clinical practice. Specifically, patients in the same cohort tend to share similar characteristics, implying their resemblance in medical conditions such as symptoms or diseases. In this paper, we propose a universal COhort Representation lEarning (CORE) framework to augment EHR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.04133</link><description>&lt;p&gt;
&#22522;&#20110;NeRF&#25216;&#26415;&#30340;&#21355;&#26143;&#22270;&#20687;&#34920;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#26159;&#23545;&#26368;&#36817;&#24341;&#20837;&#30340;S-NeRF&#27169;&#22411;&#30340;&#20462;&#25913;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#31232;&#30095;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22270;&#29255;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#31934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#65292;&#36825;&#23545;&#21355;&#26143;&#35266;&#27979;&#24212;&#29992;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;S-NeRF&#26041;&#27861;&#25913;&#36827;&#20102;&#26631;&#20934;&#30340;NeRF&#26041;&#27861;&#65292;&#23558;&#36752;&#23556;&#24378;&#24230;&#32771;&#34385;&#20026;&#39640;&#21453;&#23556;&#29575;&#21644;&#20837;&#23556;&#36752;&#29031;&#24230;&#30340;&#20989;&#25968;&#12290;&#36825;&#20004;&#20010;&#37327;&#37117;&#26159;&#27169;&#22411;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26525;&#26465;&#30340;&#36755;&#20986;&#65292;&#32780;&#21518;&#32773;&#21017;&#34987;&#35270;&#20026;&#26469;&#33258;&#22826;&#38451;&#30340;&#30452;&#25509;&#20809;&#32447;&#21644;&#26469;&#33258;&#22825;&#31354;&#30340;&#28459;&#21453;&#23556;&#39068;&#33394;&#20989;&#25968;&#12290;&#35813;&#23454;&#29616;&#22522;&#20110;&#29992;&#32553;&#25918;-&#35009;&#21098;&#25216;&#26415;&#22686;&#24378;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23545;NeRF&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#23545;&#26367;&#20195;&#21697;&#30340;&#25490;&#21517;&#65292;&#36890;&#36807;&#31639;&#27861;&#25214;&#21040;&#20102;&#19968;&#31181;&#20960;&#20046;&#26368;&#20248;&#30340;&#25805;&#32437;&#26041;&#24335;&#65292;&#20197;&#30830;&#23450;&#22312;&#32473;&#23450;&#24773;&#20917;&#19979;&#25805;&#32437;&#30340;&#38590;&#26131;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.03060</link><description>&lt;p&gt;
&#19968;&#23545;&#26367;&#20195;&#21697;&#30340;&#20960;&#20046;&#26368;&#20248;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Almost optimal manipulation of a pair of alternatives. (arXiv:2304.03060v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#23545;&#26367;&#20195;&#21697;&#30340;&#25490;&#21517;&#65292;&#36890;&#36807;&#31639;&#27861;&#25214;&#21040;&#20102;&#19968;&#31181;&#20960;&#20046;&#26368;&#20248;&#30340;&#25805;&#32437;&#26041;&#24335;&#65292;&#20197;&#30830;&#23450;&#22312;&#32473;&#23450;&#24773;&#20917;&#19979;&#25805;&#32437;&#30340;&#38590;&#26131;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#36807;&#31243;&#20013;&#19987;&#23478;&#30340;&#35282;&#33394;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26368;&#32456;&#30340;&#24314;&#35758;&#21462;&#20915;&#20110;&#20182;&#20204;&#30340;&#24577;&#24230;&#12289;&#22836;&#33041;&#28165;&#26224;&#31243;&#24230;&#12289;&#32463;&#39564;&#21644;&#23545;&#38382;&#39064;&#30340;&#20102;&#35299;&#12290;&#20294;&#26159;&#65292;&#24314;&#35758;&#36824;&#21462;&#20915;&#20110;&#20182;&#20204;&#30340;&#35802;&#23454;&#12290;&#22914;&#26524;&#19987;&#23478;&#19981;&#35802;&#23454;&#24590;&#20040;&#21150;&#65311;&#37027;&#20040;&#65292;&#22312;&#32473;&#23450;&#24773;&#20917;&#19979;&#25805;&#32437;&#26159;&#22810;&#20040;&#22256;&#38590;&#23601;&#21464;&#24471;&#24456;&#37325;&#35201;&#20102;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#21040;&#20102;&#36890;&#36807;&#27604;&#36739;&#19968;&#23545;&#26367;&#20195;&#21697;&#33719;&#24471;&#30340;&#25490;&#21517;&#30340;&#25805;&#32437;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#20132;&#25442;&#36873;&#23450;&#30340;&#20004;&#20010;&#26367;&#20195;&#21697;&#20301;&#32622;&#30340;&#20960;&#20046;&#26368;&#20248;&#26041;&#24335;&#12290;&#30001;&#27492;&#65292;&#23601;&#21487;&#20197;&#30830;&#23450;&#22312;&#32473;&#23450;&#24773;&#20917;&#19979;&#25805;&#32437;&#26159;&#22810;&#20040;&#22256;&#38590;&#30340;&#20102;&#12290;&#29702;&#35770;&#32771;&#34385;&#36890;&#36807;&#19968;&#20010;&#23454;&#38469;&#20363;&#23376;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The role of an expert in the decision-making process is crucial, as the final recommendation depends on his disposition, clarity of mind, experience, and knowledge of the problem. However, the recommendation also depends on their honesty. But what if the expert is dishonest? Then, the answer on how difficult it is to manipulate in a given case becomes essential. In the presented work, we consider manipulation of a ranking obtained by comparing alternatives in pairs. More specifically, we propose an algorithm for finding an almost optimal way to swap the positions of two selected alternatives. Thanks to this, it is possible to determine how difficult such manipulation is in a given case. Theoretical considerations are illustrated by a practical example.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PiCL&#12290;</title><link>http://arxiv.org/abs/2304.03031</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#30340;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Revisiting Dense Retrieval with Unanswerable Counterfactuals. (arXiv:2304.03031v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PiCL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#20013;&#65292;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#26694;&#26550;&#24456;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#26816;&#32034;&#22120;&#20174;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#20026;&#38405;&#35835;&#22120;&#25277;&#21462;&#19968;&#32452;&#30456;&#20851;&#30340;&#20505;&#36873;&#27573;&#33853;&#12290;&#36825;&#31181;&#26041;&#27861;&#32972;&#21518;&#30340;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#65292;&#20174;&#26816;&#32034;&#22120;&#24471;&#21040;&#30340;&#39640;&#30456;&#20851;&#24615;&#20998;&#25968;&#21487;&#33021;&#34920;&#26126;&#20174;&#38405;&#35835;&#22120;&#33719;&#21462;&#31572;&#26696;&#30340;&#21487;&#33021;&#24615;&#24456;&#39640;&#65292;&#36825;&#24847;&#21619;&#30528;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24456;&#21487;&#33021;&#21253;&#21547;&#32473;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#23454;&#35777;&#39539;&#26021;&#20102;&#36825;&#31181;&#35266;&#28857;&#65292;&#24182;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#23494;&#38598;&#26816;&#32034;&#20013;&#36825;&#31181;&#23545;&#31572;&#26696;&#26080;&#24863;&#30693;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23547;&#27714;&#20351;&#29992;&#21453;&#20107;&#23454;&#26679;&#26412;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#36164;&#28304;&#65292;&#20197;&#26356;&#22909;&#22320;&#21516;&#27493;DPR&#30340;&#30456;&#20851;&#24615;&#27979;&#37327;&#21644;&#38382;&#39064;-&#27573;&#33853;&#23545;&#30340;&#21487;&#31572;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;Pivoting&#23545;&#27604;&#23398;&#20064;&#65288;PiCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The retriever-reader framework is popular for open-domain question answering (ODQA), where a retriever samples for the reader a set of relevant candidate passages from a large corpus. A key assumption behind this method is that high relevance scores from the retriever likely indicate high answerability from the reader, which implies a high probability that the retrieved passages contain answers to a given question. In this work, we empirically dispel this belief and observe that recent dense retrieval models based on DPR often rank unanswerable counterfactual passages higher than their answerable original passages. To address such answer-unawareness in dense retrievers, we seek to use counterfactual samples as additional training resources to better synchronize the relevance measurement of DPR with the answerability of question-passage pairs. Specifically, we present counterfactually-Pivoting Contrastive Learning (PiCL), a novel representation learning approach for passage retrieval th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38053;&#21273;&#65306;&#29992;GPT&#35299;&#23494;&#26448;&#26009;&#31185;&#23398;&#30340;&#31192;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#20449;&#24687;&#25552;&#21462;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#38041;&#38043;&#30719;&#22826;&#38451;&#33021;&#30005;&#27744;FAIR&#25968;&#25454;&#38598;&#23545;GPT-3&#36827;&#34892;&#24494;&#35843;&#65292;&#33719;&#24471;&#20102;91.8 F1&#24471;&#20998;&#65292;&#24182;&#26356;&#26032;&#20102;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36804;&#20170;&#20026;&#27490;&#25152;&#26377;&#30456;&#20851;&#31185;&#23398;&#35770;&#25991;&#12290;&#25152;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#24050;&#34987;&#26684;&#24335;&#21270;&#21644;&#26631;&#20934;&#21270;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#30452;&#25509;&#20316;&#20026;&#21518;&#32493;&#25968;&#25454;&#20998;&#26512;&#30340;&#36755;&#20837;&#12290;&#36825;&#20010;&#29305;&#24615;&#23558;&#20351;&#26448;&#26009;&#31185;&#23398;&#23478;&#36890;&#36807;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#35780;&#35770;&#25991;&#31456;&#26469;&#24320;&#21457;&#20854;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;DFT&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20687;&#26448;&#26009;&#23398;&#23478;&#19968;&#26679;&#35780;&#21028;&#26448;&#26009;&#21644;&#35774;&#35745;&#26032;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#35299;&#37322;&#26041;&#27861;&#8212;&#8212;BODEM&#65292;&#23427;&#37319;&#29992;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#29983;&#25104;&#22810;&#20010;&#29256;&#26412;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#27604;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.17249</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#35937;&#26816;&#27979;&#30340;&#27169;&#22411;&#26080;&#20851;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic explainable artificial intelligence for object detection in image data. (arXiv:2303.17249v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#35299;&#37322;&#26041;&#27861;&#8212;&#8212;BODEM&#65292;&#23427;&#37319;&#29992;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#29983;&#25104;&#22810;&#20010;&#29256;&#26412;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#27604;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#36890;&#36807;&#24320;&#21457;&#22823;&#22411;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#36879;&#26126;&#24230;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#22952;&#30861;&#36825;&#20123;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#20013;&#24320;&#21457;&#26041;&#27861;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#30340;&#34892;&#20026;&#12289;&#20915;&#31574;&#36923;&#36753;&#21644;&#28431;&#27934;&#12290;&#26412;&#25991;&#20026;&#20102;&#35299;&#37322;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23545;&#35937;&#26816;&#27979;&#31995;&#32479;&#35774;&#35745;&#21644;&#23454;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;Black-box Object Detection Explanation by Masking&#65288;BODEM&#65289;&#30340;&#40657;&#30418;&#35828;&#26126;&#26041;&#27861;&#65292;&#37319;&#29992;&#26032;&#30340;&#25513;&#34109;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#26469;&#29983;&#25104;&#36755;&#20837;&#22270;&#20687;&#30340;&#22810;&#20010;&#29256;&#26412;&#12290;&#23616;&#37096;&#25513;&#34109;&#29992;&#20110;&#24178;&#25200;&#30446;&#26631;&#23545;&#35937;&#20869;&#30340;&#20687;&#32032;&#65292;&#20197;&#20102;&#35299;&#23545;&#35937;&#26816;&#27979;&#22120;&#23545;&#36825;&#20123;&#21464;&#21270;&#30340;&#21453;&#24212;&#65292;&#32780;&#36828;&#31243;&#25513;&#34109;&#21017;&#29992;&#20110;&#30740;&#31350;&#23545;&#35937;&#26816;&#27979;&#22120;&#22312;&#22270;&#20687;&#32972;&#26223;&#19978;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;BODEM&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection is a fundamental task in computer vision, which has been greatly progressed through developing large and intricate deep learning models. However, the lack of transparency is a big challenge that may not allow the widespread adoption of these models. Explainable artificial intelligence is a field of research where methods are developed to help users understand the behavior, decision logics, and vulnerabilities of AI-based systems. Black-box explanation refers to explaining decisions of an AI system without having access to its internals. In this paper, we design and implement a black-box explanation method named Black-box Object Detection Explanation by Masking (BODEM) through adopting a new masking approach for AI-based object detection systems. We propose local and distant masking to generate multiple versions of an input image. Local masks are used to disturb pixels within a target object to figure out how the object detector reacts to these changes, while distant ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20803;&#23431;&#23449;&#20013;&#20351;&#29992;ChatGPT&#30340;&#21033;&#24330;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#21160;&#24577;&#21644;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#65292;&#20294;&#20063;&#24517;&#39035;&#32771;&#34385;&#38544;&#31169;&#12289;&#20559;&#35265;&#21644;&#36947;&#24503;&#31561;&#30456;&#20851;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13856</link><description>&lt;p&gt;
ChatGPT&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#24212;&#29992;&#65306;&#25327;&#25937;&#32773;&#36824;&#26159;&#27585;&#28781;&#32773;?
&lt;/p&gt;
&lt;p&gt;
Unleasing ChatGPT on the Metaverse: Savior or Destroyer?. (arXiv:2303.13856v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20803;&#23431;&#23449;&#20013;&#20351;&#29992;ChatGPT&#30340;&#21033;&#24330;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#21160;&#24577;&#21644;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#65292;&#20294;&#20063;&#24517;&#39035;&#32771;&#34385;&#38544;&#31169;&#12289;&#20559;&#35265;&#21644;&#36947;&#24503;&#31561;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#8220;&#20803;&#23431;&#23449;&#8221;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#30340;&#20132;&#20114;&#26041;&#24335;&#21644;&#27785;&#28024;&#20307;&#39564;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#32780;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#20854;&#20013;&#65292;&#19968;&#20010;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#26159;ChatGPT&#65292;&#36825;&#26159;OpenAI&#35757;&#32451;&#30340;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#35814;&#32454;&#25506;&#35752;&#20102;&#22312;&#20803;&#23431;&#23449;&#20013;&#24341;&#20837;ChatGPT&#30340;&#21033;&#24330;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#23089;&#20048;&#12289;&#20010;&#24615;&#21270;&#21644;&#25903;&#25345;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;&#36825;&#39033;&#25216;&#26415;&#21487;&#20197;&#25552;&#20379;&#21160;&#24577;&#21644;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#65292;&#20294;&#20063;&#24517;&#39035;&#32771;&#34385;&#38544;&#31169;&#12289;&#20559;&#35265;&#21644;&#36947;&#24503;&#31561;&#30456;&#20851;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#36825;&#20123;&#26426;&#36935;&#21644;&#38556;&#30861;&#65292;&#24110;&#21161;&#35835;&#32773;&#29702;&#35299;ChatGPT&#23545;&#20803;&#23431;&#23449;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#23427;&#21019;&#24314;&#26356;&#21152;&#27785;&#28024;&#21644;&#26377;&#36259;&#30340;&#34394;&#25311;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
The incorporation of artificial intelligence (AI) technology, and in particular natural language processing (NLP), is becoming increasingly vital for the development of immersive and interactive metaverse experiences. One such artificial intelligence tool that is gaining traction in the metaverse is ChatGPT, a large language model trained by OpenAI. The article delves into the pros and cons of utilizing ChatGPT for metaverse-based education, entertainment, personalization, and support. Dynamic and personalized experiences are possible with this technology, but there are also legitimate privacy, bias, and ethical issues to consider. This article aims to help readers understand the possible influence of ChatGPT on the metaverse and how it may be used to effectively create a more immersive and engaging virtual environment by evaluating these opportunities and obstacles.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#21161;&#20110;&#21307;&#23398;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13375</link><description>&lt;p&gt;
GPT-4&#22312;&#21307;&#23398;&#25361;&#25112;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Capabilities of GPT-4 on Medical Challenge Problems. (arXiv:2303.13375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#21161;&#20110;&#21307;&#23398;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21253;&#25324;&#21307;&#23398;&#12290;&#25105;&#20204;&#23545;&#19968;&#39033;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;GPT-4&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#27809;&#26377;&#32463;&#36807;&#38024;&#23545;&#21307;&#23398;&#38382;&#39064;&#30340;&#35757;&#32451;&#25110;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#32654;&#22269;&#20020;&#24202;&#33021;&#21147;&#35780;&#20272;&#21644;&#25480;&#26435;&#32771;&#26680;&#35745;&#21010;&#65288;USMLE&#65289;&#30340;&#20004;&#32452;&#23448;&#26041;&#32451;&#20064;&#26448;&#26009;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22312;MultiMedQA&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#38500;&#20102;&#27979;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#27979;&#35797;&#38382;&#39064;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25506;&#32034;&#35757;&#32451;&#26399;&#38388;&#20869;&#23481;&#35760;&#24518;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#30740;&#31350;&#27010;&#29575;&#26657;&#20934;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#65292;GPT-4&#30340;&#34920;&#29616;&#21450;&#20854;&#23545;&#24120;&#35782;&#30693;&#35782;&#30340;&#22788;&#29702;&#21644;&#25972;&#21512;&#36807;&#31243;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#25105;&#20204;&#20063;&#21457;&#29616;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11436</link><description>&lt;p&gt;
&#24515;&#28789;&#19982;&#26426;&#22120;: &#35299;&#24320;GPT-4&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
Mind meets machine: Unravelling GPT-4's cognitive psychology. (arXiv:2303.11436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#65292;GPT-4&#30340;&#34920;&#29616;&#21450;&#20854;&#23545;&#24120;&#35782;&#30693;&#35782;&#30340;&#22788;&#29702;&#21644;&#25972;&#21512;&#36807;&#31243;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#25105;&#20204;&#20063;&#21457;&#29616;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#25104;&#20998;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#29615;&#22659;&#35266;&#23519;&#25512;&#26029;&#32467;&#35770;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#25104;&#20026;&#36234;&#26469;&#36234;&#33021;&#22815;&#25191;&#34892;&#20154;&#31867;&#32423;&#20219;&#21153;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;GPT-4&#21450;&#20854;&#22312;&#21307;&#23398;&#32771;&#35797;&#12289;&#24459;&#24072;&#32771;&#35797;&#31561;&#20154;&#31867;&#38590;&#20197;&#23436;&#25104;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#30340;&#25104;&#21151;&#65292;&#22686;&#21152;&#20102;LLMs&#25104;&#20026;&#23436;&#32654;&#26234;&#33021;&#24037;&#20855;&#30340;&#20449;&#24515;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;GPT-4&#35770;&#25991;&#21521;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#26576;&#20123;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20294;&#23545;GPT-4&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#29305;&#21035;&#26159;&#29616;&#26377;&#30340;&#24050;&#32463;&#30830;&#31435;&#22909;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#36824;&#26159;&#32570;&#22833;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20851;&#27880;GPT-4&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#21450;&#20854;&#35748;&#30693;&#24515;&#29702;&#23398;&#24037;&#20855;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#33021;&#22815;&#29702;&#35299;GPT-4&#22914;&#20309;&#22312;&#20854;&#35821;&#35328;&#29983;&#25104;&#36807;&#31243;&#20013;&#22788;&#29702;&#21644;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#65292;&#20197;&#21450;&#20854;&#22312;&#36825;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning is a basic ingredient of intelligence in humans, empowering the ability to deduce conclusions based on the observations of surroundings. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans such as medical exam, bar exam and others has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Though, the GPT-4 paper has shown performance on some common sense reasoning tasks, a comprehensive assessment of GPT-4 on common sense reasoning tasks, particularly on the existing well-established datasets is missing. In this study, we focus on the evaluation of GPT-4's performance on a set of common sense reasoning questions from the widely used CommonsenseQA dataset along with tools from cognitive psychology. In doing so, we understand how GPT-4 processes and integrates common sense k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#30340;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.08622</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#25193;&#25955;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer. (arXiv:2303.08622v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#30340;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#38543;&#26426;&#24615;&#32780;&#23384;&#22312;&#39118;&#26684;&#36716;&#25442;&#21644;&#20869;&#23481;&#20445;&#25252;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#35745;&#31639;&#23494;&#38598;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#25110;&#38468;&#21152;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#65292;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#36741;&#21161;&#32593;&#32476;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#29983;&#25104;&#26679;&#26412;&#21644;&#21407;&#22987;&#22270;&#20687;&#23884;&#20837;&#20043;&#38388;&#30340;&#22270;&#22359;&#23545;&#27604;&#25439;&#22833;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#19982;&#28304;&#22270;&#20687;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#30041;&#20869;&#23481;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#21516;&#26102;&#65292;&#22312;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#12289;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#21644;&#25805;&#20316;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown great promise in text-guided image style transfer, but there is a trade-off between style transformation and content preservation due to their stochastic nature. Existing methods require computationally expensive fine-tuning of diffusion models or additional neural network. To address this, here we propose a zero-shot contrastive loss for diffusion models that doesn't require additional fine-tuning or auxiliary networks. By leveraging patch-wise contrastive loss between generated samples and original image embeddings in the pre-trained diffusion model, our method can generate images with the same semantic content as the source image in a zero-shot manner. Our approach outperforms existing methods while preserving content and requiring no additional training, not only for image style transfer but also for image-to-image translation and manipulation. Our experimental results validate the effectiveness of our proposed method.
&lt;/p&gt;</description></item><item><title>&#24037;&#20316;&#22330;&#25152;&#24863;&#30693;&#25216;&#26415;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#20135;&#21147;&#21644;&#31119;&#31049;&#65292;&#20294;&#22914;&#20309;&#33719;&#21462;&#24037;&#20154;&#30340;&#26377;&#25928;&#21516;&#24847;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#38590;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#24037;&#20154;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.07242</link><description>&lt;p&gt;
&#24037;&#20154;&#26159;&#21542;&#33021;&#22815;&#26377;&#25928;&#21516;&#24847;&#24037;&#20316;&#22330;&#25152;&#20581;&#24247;&#31185;&#25216;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Workers Meaningfully Consent to Workplace Wellbeing Technologies?. (arXiv:2303.07242v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07242
&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#22330;&#25152;&#24863;&#30693;&#25216;&#26415;&#30340;&#24212;&#29992;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#20135;&#21147;&#21644;&#31119;&#31049;&#65292;&#20294;&#22914;&#20309;&#33719;&#21462;&#24037;&#20154;&#30340;&#26377;&#25928;&#21516;&#24847;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#38590;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#24037;&#20154;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#22330;&#25152;&#37096;&#32626;&#30340;&#24863;&#30693;&#25216;&#26415;&#21487;&#20197;&#25910;&#38598;&#20010;&#20154;&#27963;&#21160;&#21644;&#32676;&#20307;&#20114;&#21160;&#30340;&#35814;&#32454;&#25968;&#25454;&#65292;&#21542;&#21017;&#24456;&#38590;&#25429;&#25417;&#12290;&#36825;&#20123;&#25216;&#26415;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#26159;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#20225;&#19994;&#21644;&#24037;&#20154;&#20248;&#21270;&#29983;&#20135;&#21147;&#21644;&#31119;&#31049;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#24037;&#20316;&#22330;&#25152;&#20013;&#30340;&#20869;&#22312;&#21644;&#32467;&#26500;&#24615;&#26435;&#21147;&#21160;&#24577;&#65292;&#25509;&#21463;&#21547;&#33988;&#30340;&#36981;&#20174;&#20197;&#30417;&#27979;&#24037;&#20316;&#27963;&#21160;&#32780;&#19981;&#26159;&#23547;&#27714;&#24037;&#20154;&#30340;&#26377;&#24847;&#20041;&#30340;&#21516;&#24847;&#30340;&#26222;&#36941;&#20570;&#27861;&#24341;&#21457;&#20102;&#38544;&#31169;&#21644;&#20262;&#29702;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#24037;&#20154;&#22312;&#21516;&#24847;&#24037;&#20316;&#22330;&#25152;&#20581;&#24247;&#31185;&#25216;&#26041;&#38754;&#38754;&#20020;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;&#21033;&#29992;&#19968;&#20010;&#34394;&#25311;&#26696;&#20363;&#65292;&#24341;&#23548;15&#20301;&#21442;&#19982;&#32773;&#21442;&#19982;&#20102;6&#20010;&#22810;&#21033;&#30410;&#30456;&#20851;&#26041;&#28966;&#28857;&#23567;&#32452;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21442;&#19982;&#32773;&#21516;&#24847;&#24037;&#20316;&#22330;&#25152;&#24863;&#30693;&#25216;&#26415;&#30340;&#26399;&#26395;&#21644;&#33021;&#21147;&#12290;&#25105;&#20204;&#25551;&#32472;&#20102;&#21487;&#33021;&#26356;&#22909;&#22320;&#25903;&#25345;&#26356;&#26377;&#24847;&#20041;&#30340;&#21516;&#24847;&#24037;&#20316;&#22330;&#25152;&#20581;&#24247;&#31185;&#25216;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#36825;&#20123;&#25514;&#26045; drawing on critical
&lt;/p&gt;
&lt;p&gt;
Sensing technologies deployed in the workplace can collect detailed data about individual activities and group interactions that are otherwise difficult to capture. A hopeful application of these technologies is that they can help businesses and workers optimize productivity and wellbeing. However, given the inherent and structural power dynamics in the workplace, the prevalent approach of accepting tacit compliance to monitor work activities rather than seeking workers' meaningful consent raises privacy and ethical concerns. This paper unpacks a range of challenges that workers face when consenting to workplace wellbeing technologies. Using a hypothetical case to prompt reflection among six multi-stakeholder focus groups involving 15 participants, we explored participants' expectations and capacity to consent to workplace sensing technologies. We sketched possible interventions that could better support more meaningful consent to workplace wellbeing technologies by drawing on critical
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23384;&#27963;&#30340;&#29983;&#29289;&#21487;&#20197;&#23398;&#20064;&#21040;&#20248;&#20110; L\'evy walks &#30340;&#35269;&#39135;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#35269;&#39135;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06050</link><description>&lt;p&gt;
&#26368;&#20248;&#35269;&#39135;&#31574;&#30053;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#24182;&#19988;&#20248;&#20110; L\'evy walks&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal foraging strategies can be learned and outperform L\'evy walks. (arXiv:2303.06050v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06050
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23384;&#27963;&#30340;&#29983;&#29289;&#21487;&#20197;&#23398;&#20064;&#21040;&#20248;&#20110; L\'evy walks &#30340;&#35269;&#39135;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#35269;&#39135;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
L\'evy walks &#21644;&#20854;&#20182;&#20105;&#35758;&#35770;&#30340;&#26368;&#20248;&#35269;&#39135;&#27169;&#22411;&#25104;&#21151;&#22320;&#34987;&#29992;&#20110;&#25551;&#36848;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#65292;&#21560;&#24341;&#20102;&#32463;&#27982;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#29983;&#24577;&#23398;&#12289;&#36827;&#21270;&#29983;&#29289;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20173;&#19981;&#28165;&#26970;&#21738;&#20123;&#31574;&#30053;&#21487;&#20197;&#26368;&#22823;&#21270;&#35269;&#39135;&#25928;&#29575;&#65292;&#36825;&#20123;&#31574;&#30053;&#26159;&#21542;&#21487;&#20197;&#34987;&#29983;&#29289;&#23398;&#20064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#35269;&#39135;&#32773;&#24314;&#27169;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#31561;&#21516;&#20110;&#20248;&#21270;&#35269;&#39135;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#23398;&#20064;&#20102;&#20248;&#20110;&#24050;&#30693;&#31574;&#30053;&#22914; L\'evy walks &#30340;&#35269;&#39135;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
L\'evy walks and other theoretical models of optimal foraging have been successfully used to describe real-world scenarios, attracting attention in several fields such as economy, physics, ecology, and evolutionary biology. However, it remains unclear in most cases which strategies maximize foraging efficiency and whether such strategies can be learned by living organisms. To address these questions, we model foragers as reinforcement learning agents. We first prove theoretically that maximizing rewards in our reinforcement learning model is equivalent to optimizing foraging efficiency. We then show with numerical experiments that our agents learn foraging strategies which outperform the efficiency of known strategies such as L\'evy walks.
&lt;/p&gt;</description></item><item><title>CVT-SLR&#26159;&#19968;&#31181;&#26032;&#30340;&#25163;&#35821;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#21644;&#21464;&#20998;&#23545;&#40784;&#30340;&#26041;&#27861;&#26469;&#20805;&#20998;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#65292;&#20026;&#35299;&#20915;&#25163;&#35821;&#35782;&#21035;&#20013;&#32570;&#20047;&#22823;&#35268;&#27169;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.05725</link><description>&lt;p&gt;
CVT-SLR&#65306;&#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#19982;&#21464;&#20998;&#23545;&#40784;&#30340;&#25163;&#35821;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment. (arXiv:2303.05725v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05725
&lt;/p&gt;
&lt;p&gt;
CVT-SLR&#26159;&#19968;&#31181;&#26032;&#30340;&#25163;&#35821;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#21644;&#21464;&#20998;&#23545;&#40784;&#30340;&#26041;&#27861;&#26469;&#20805;&#20998;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#65292;&#20026;&#35299;&#20915;&#25163;&#35821;&#35782;&#21035;&#20013;&#32570;&#20047;&#22823;&#35268;&#27169;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#35782;&#21035; (SLR) &#26159;&#19968;&#39033;&#24369;&#30417;&#30563;&#20219;&#21153;&#65292;&#21487;&#20197;&#23558;&#25163;&#35821;&#35270;&#39057;&#27880;&#37322;&#20026;&#25991;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#21487;&#29992;&#30340;&#25163;&#35821;&#25968;&#25454;&#38598;&#32780;&#23548;&#33268;&#30340;&#19981;&#20805;&#20998;&#35757;&#32451;&#25104;&#20026; SLR &#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968; SLR &#30340;&#24037;&#20316;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22359;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#31181;&#20027;&#27969;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#27169;&#22411; CVT-SLR&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language recognition (SLR) is a weakly supervised task that annotates sign videos as textual glosses. Recent studies show that insufficient training caused by the lack of large-scale available sign language datasets becomes the main bottleneck for SLR. The majority of SLR works thereby adopt pretrained visual modules and develop two mainstream solutions. The multi-stream architectures extend multi-cue visual features, yielding the current SOTA performances but requiring complex designs and might introduce potential noise. Alternatively, the advanced single-cue SLR frameworks using explicit cross-modal alignment between visual and textual modalities are simple and effective, potentially competitive with the multi-cue framework. In this work, we propose a novel contrastive visual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained knowledge of both the visual and language modalities. Based on the single-cue cross-modal alignment framework, we propose a variation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PromptAttack&#30340;&#32452;&#21512;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#34987;&#20302;&#39057;&#29575;&#35206;&#30422;&#30340;&#23376;&#32676;&#20307;&#20013;&#25628;&#32034;&#25214;&#21040;&#30446;&#26631;&#27169;&#22411;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#23376;&#32676;&#20307;&#65292;&#20197;&#27492;&#35782;&#21035;&#20986;&#20998;&#31867;&#22120;&#21487;&#33021;&#23384;&#22312;&#30340;&#31995;&#32479;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05072</link><description>&lt;p&gt;
&#21457;&#29616;&#22270;&#20687;&#20998;&#31867;&#22120;&#22312;&#32597;&#35265;&#23376;&#32676;&#20307;&#19978;&#30340;&#31995;&#32479;&#24615;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Identification of Systematic Errors of Image Classifiers on Rare Subgroups. (arXiv:2303.05072v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05072
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PromptAttack&#30340;&#32452;&#21512;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#34987;&#20302;&#39057;&#29575;&#35206;&#30422;&#30340;&#23376;&#32676;&#20307;&#20013;&#25628;&#32034;&#25214;&#21040;&#30446;&#26631;&#27169;&#22411;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#23376;&#32676;&#20307;&#65292;&#20197;&#27492;&#35782;&#21035;&#20986;&#20998;&#31867;&#22120;&#21487;&#33021;&#23384;&#22312;&#30340;&#31995;&#32479;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#22120;&#20855;&#26377;&#20986;&#33394;&#30340;&#24179;&#22343;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#34987;&#20805;&#20998;&#35206;&#30422;&#30340;&#35821;&#20041;&#36830;&#36143;&#30340;&#23376;&#32676;&#20307;&#19978;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#20123;&#31995;&#32479;&#35823;&#24046;&#21487;&#33021;&#20250;&#24433;&#21709;&#27665;&#26063;&#23569;&#25968;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#21450;&#22312;&#39046;&#22495;&#36716;&#31227;&#26102;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#25628;&#32034;&#25968;&#25454;&#20013;&#34987;&#20302;&#39057;&#29575;&#35206;&#30422;&#30340;&#23376;&#32676;&#20307;&#65288;&#8220;&#25552;&#31034;&#8221;&#65289;&#65292;&#25214;&#21040;&#30446;&#26631;&#27169;&#22411;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#23376;&#32676;&#20307;&#65292;&#20197;&#27492;&#35782;&#21035;&#20986;&#20998;&#31867;&#22120;&#21487;&#33021;&#23384;&#22312;&#30340;&#31995;&#32479;&#35823;&#24046;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#32452;&#21512;&#27979;&#35797;&#35299;&#20915;&#20102;&#23376;&#32676;&#20307;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;PromptAttack&#12290;&#25105;&#20204;&#22312;&#21463;&#25511;&#30340;&#23454;&#39564;&#29615;&#22659;&#19979;&#30740;&#31350;&#20102;PromptAttack&#30340;&#23376;&#32676;&#20307;&#28085;&#30422;&#29575;&#21644;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite excellent average-case performance of many image classifiers, their performance can substantially deteriorate on semantically coherent subgroups of the data that were under-represented in the training data. These systematic errors can impact both fairness for demographic minority groups as well as robustness and safety under domain shift. A major challenge is to identify such subgroups with subpar performance when the subgroups are not annotated and their occurrence is very rare. We leverage recent advances in text-to-image models and search in the space of textual descriptions of subgroups ("prompts") for subgroups where the target model has low performance on the prompt-conditioned synthesized data. To tackle the exponentially growing number of subgroups, we employ combinatorial testing. We denote this procedure as PromptAttack as it can be interpreted as an adversarial attack in a prompt space. We study subgroup coverage and identifiability with PromptAttack in a controlled 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#26041;&#38754;&#25152;&#21462;&#24471;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04150</link><description>&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Reinforcement Learning: A Survey. (arXiv:2303.04150v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04150
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#26041;&#38754;&#25152;&#21462;&#24471;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#35757;&#32451;&#26234;&#33021;&#20307;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#26827;&#30424;&#28216;&#25103;&#12289;&#34903;&#26426;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#31561;&#21508;&#31181;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#25935;&#24863;&#36229;&#21442;&#25968;&#23548;&#33268;&#30340;&#33030;&#24369;&#25910;&#25947;&#29305;&#24615;&#65292;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#26102;&#38388;&#20998;&#37197;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#24615;&#25506;&#32034;&#19981;&#36275;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#22256;&#38590;&#20197;&#21450;&#22870;&#21169;&#20914;&#31361;&#30446;&#26631;&#12290;&#36827;&#21270;&#35745;&#31639;&#32500;&#25252;&#30528;&#19968;&#32676;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24050;&#23637;&#29616;&#20986;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#38598;&#25104;&#36827;&#21270;&#35745;&#31639;&#30340;&#26368;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a machine learning approach that trains agents to maximize cumulative rewards through interactions with environments. The integration of RL with deep learning has recently resulted in impressive achievements in a wide range of challenging tasks, including board games, arcade games, and robot control. Despite these successes, there remain several crucial challenges, including brittle convergence properties caused by sensitive hyperparameters, difficulties in temporal credit assignment with long time horizons and sparse rewards, a lack of diverse exploration, especially in continuous search space scenarios, difficulties in credit assignment in multi-agent reinforcement learning, and conflicting objectives for rewards. Evolutionary computation (EC), which maintains a population of learning agents, has demonstrated promising performance in addressing these limitations. This article presents a comprehensive survey of state-of-the-art methods for integrating EC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.07731</link><description>&lt;p&gt;
AI&#23545;&#25239;AI&#65306;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25171;&#20987;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#39184;&#21381;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media. (arXiv:2302.07731v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;GPT&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21046;&#36896;&#20986;&#38590;&#20197;&#21306;&#20998;&#30340;&#34394;&#20551;&#39038;&#23458;&#35780;&#35770;&#65292;&#20174;&#32780;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26816;&#27979;&#36825;&#20123;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#36896;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;Yelp&#39564;&#35777;&#30340;&#39640;&#36136;&#37327;&#30340;&#31934;&#33521;&#39184;&#21381;&#35780;&#35770;&#26469;&#29983;&#25104;OpenAI GPT&#35780;&#35770;&#29983;&#25104;&#22120;&#30340;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#26368;&#32456;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#34394;&#20551;&#35780;&#35770;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#65288;&#22914;&#35780;&#35770;&#12289;&#29992;&#25143;&#21644;&#39184;&#21381;&#29305;&#24449;&#20197;&#21450;&#20889;&#20316;&#39118;&#26684;&#65289;&#19978;&#35782;&#21035;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#19981;&#26029;&#38754;&#20020;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#30340;&#25361;&#25112;&#65292;&#23613;&#31649;&#20182;&#20204;&#21487;&#33021;&#23454;&#26045;&#26816;&#27979;&#31995;&#32479;&#20197;&#36807;&#28388;&#20986;&#21487;&#30097;&#30340;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative models such as GPT may be used to fabricate indistinguishable fake customer reviews at a much lower cost, thus posing challenges for social media platforms to detect these machine-generated fake reviews. We propose to leverage the high-quality elite restaurant reviews verified by Yelp to generate fake reviews from the OpenAI GPT review creator and ultimately fine-tune a GPT output detector to predict fake reviews that significantly outperform existing solutions. We further apply the model to predict non-elite reviews and identify the patterns across several dimensions, such as review, user and restaurant characteristics, and writing style. We show that social media platforms are continuously challenged by machine-generated fake reviews, although they may implement detection systems to filter out suspicious reviews.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#36965;&#24863;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;Viewmaker&#32593;&#32476;&#65292;&#26412;&#25991;&#21457;&#29616;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#20135;&#29983;&#35270;&#22270;&#65292;&#24182;&#22312;&#22235;&#20010;&#22810;&#20809;&#35889;&#25104;&#20687;&#38382;&#39064;&#19978;&#23454;&#29616;&#20248;&#20110;&#22522;&#20110;&#35009;&#21098;&#21644;&#21453;&#23556;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.05757</link><description>&lt;p&gt;
&#22810;&#20809;&#35889;&#23545;&#27604;&#23398;&#20064;&#19982;Viewmaker&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multispectral Contrastive Learning with Viewmaker Networks. (arXiv:2302.05757v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#36965;&#24863;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;Viewmaker&#32593;&#32476;&#65292;&#26412;&#25991;&#21457;&#29616;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#20135;&#29983;&#35270;&#22270;&#65292;&#24182;&#22312;&#22235;&#20010;&#22810;&#20809;&#35889;&#25104;&#20687;&#38382;&#39064;&#19978;&#23454;&#29616;&#20248;&#20110;&#22522;&#20110;&#35009;&#21098;&#21644;&#21453;&#23556;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#39046;&#22495;&#21644;&#27169;&#24577;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#25968;&#25454;&#28857;&#30340;&#30456;&#20284;&#8220;&#35270;&#22270;&#8221;&#12290;&#28982;&#32780;&#65292;&#19987;&#19994;&#30340;&#31185;&#23398;&#27169;&#24577;&#23545;&#36825;&#31181;&#33539;&#24335; pose challenge&#65292;&#22240;&#20026;&#20026;&#27599;&#20010;&#31185;&#23398;&#20202;&#22120;&#35782;&#21035;&#22909;&#30340;&#35270;&#22270;&#26159;&#22797;&#26434;&#21644;&#32791;&#26102;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23558;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#36965;&#24863;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Viewmaker&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#29983;&#25104;&#35270;&#22270;&#30340;&#26041;&#27861;&#65292;&#24456;&#26377;&#21069;&#36884;&#22320;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#30693;&#35782;&#21644;&#35797;&#38169;&#30340;&#24773;&#20917;&#19979;&#22312;&#36825;&#19968;&#39046;&#22495;&#20869;&#20135;&#29983;&#35270;&#22270;&#12290;&#25105;&#20204;&#23558;Viewmaker&#24212;&#29992;&#20110;&#22235;&#20010;&#22810;&#20809;&#35889;&#25104;&#20687;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#20855;&#26377;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21457;&#29616;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#35780;&#20272;&#20013;&#65292;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;Viewmaker&#37117;&#21487;&#20197;&#20248;&#20110;&#22522;&#20110;&#35009;&#21098;&#21644;&#21453;&#23556;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#23545;&#27604;&#23398;&#20064;&#25193;&#23637;&#21040;&#23454;&#38469;&#30340;&#31185;&#23398;&#24212;&#29992;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning methods have been applied to a range of domains and modalities by training models to identify similar "views" of data points. However, specialized scientific modalities pose a challenge for this paradigm, as identifying good views for each scientific instrument is complex and time-intensive. In this paper, we focus on applying contrastive learning approaches to a variety of remote sensing datasets. We show that Viewmaker networks, a recently proposed method for generating views, are promising for producing views in this setting without requiring extensive domain knowledge and trial and error. We apply Viewmaker to four multispectral imaging problems, each with a different format, finding that Viewmaker can outperform croppingand reflection-based methods for contrastive learning in every case when evaluated on downstream classification tasks. This provides additional evidence that domain-agnostic methods can empower contrastive learning to scale to real-world scie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#35821;&#26009;&#24211;&#26469;&#36880;&#27493;&#36866;&#24212;&#39046;&#22495;&#20197;&#25552;&#39640;LM&#22312;&#39046;&#22495;&#20869;&#30340;&#32456;&#31471;&#20219;&#21153;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#30452;&#25509;&#25511;&#21046;LM&#30340;&#26356;&#26032;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#26469;&#20445;&#30041;LM&#20013;&#30340;&#25972;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#23545;&#27604;&#24050;&#23398;&#20064;&#39046;&#22495;&#30693;&#35782;&#21644;&#24403;&#21069;&#20840;&#32593;&#32476;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#30693;&#35782;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2302.03241</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Continual Pre-training of Language Models. (arXiv:2302.03241v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#35821;&#26009;&#24211;&#26469;&#36880;&#27493;&#36866;&#24212;&#39046;&#22495;&#20197;&#25552;&#39640;LM&#22312;&#39046;&#22495;&#20869;&#30340;&#32456;&#31471;&#20219;&#21153;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#30452;&#25509;&#25511;&#21046;LM&#30340;&#26356;&#26032;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#26469;&#20445;&#30041;LM&#20013;&#30340;&#25972;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#23545;&#27604;&#24050;&#23398;&#20064;&#39046;&#22495;&#30693;&#35782;&#21644;&#24403;&#21069;&#20840;&#32593;&#32476;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#30693;&#35782;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24555;&#36895;&#21457;&#23637;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#30740;&#31350;LMs&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#29305;&#21035;&#26159;&#25345;&#32493;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;&#25110;&#25345;&#32493;DAP&#35757;&#32451;&#65289;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;LMs&#20197;&#20351;&#20854;&#36866;&#24212;&#20110;&#39046;&#22495;&#65292;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#35821;&#26009;&#24211;&#26469;&#25345;&#32493;DAP&#35757;&#32451;LMs&#65292;&#20197;&#20351;&#20854;&#36866;&#24212;&#20110;&#36825;&#20123;&#39046;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#30340;&#32456;&#31471;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#19968;&#31181;&#36719;&#25513;&#34109;&#26426;&#21046;&#65292;&#21487;&#30452;&#25509;&#25511;&#21046;LMs&#30340;&#26356;&#26032;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#26469;&#20445;&#30041;&#21407;&#22987;LMs&#20013;&#30340;&#26222;&#36890;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23427;&#23545;&#20808;&#21069;&#23398;&#20064;&#30340;&#39046;&#22495;&#30693;&#35782;&#65288;&#21253;&#25324;&#39044;&#20808;&#35757;&#32451;&#30340;LMs&#20013;&#30340;&#26222;&#36890;&#30693;&#35782;&#65289;&#21644;&#26469;&#33258;&#24403;&#21069;&#20840;&#32593;&#32476;&#30340;&#30693;&#35782;&#36827;&#34892;&#23545;&#27604;&#65292;&#20197;&#23454;&#29616;&#30693;&#35782;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#23376;&#27169;&#20215;&#20540;&#30340;&#20195;&#29702;&#20154;&#20043;&#38388;&#20844;&#24179;&#20998;&#37197;&#19981;&#21487;&#20998;&#21106;&#29289;&#21697;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;leximin&#12289;max Nash welfare&#65288;MNW&#65289;&#21644;$p$-mean welfare&#26368;&#22823;&#21270;&#20998;&#37197;&#12290;&#22312;$ab$&#21487;&#25972;&#38500;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#27714;&#35299;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#34917;&#20805;&#20102;&#29616;&#26377;&#32467;&#26524;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35813;&#25991;&#30740;&#31350;&#20102;leximin&#21644;MNW&#20998;&#37197;&#22312;&#26080;&#23241;&#22930;&#21644;&#26368;&#22823;&#26368;&#23567;&#20221;&#39069;&#20445;&#35777;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.03087</link><description>&lt;p&gt;
&#21033;&#29992;&#23376;&#27169;&#20215;&#20540;&#23558;&#29289;&#21697;&#20998;&#37197;&#32473;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
Dividing Good and Better Items Among Agents with Submodular Valuations. (arXiv:2302.03087v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#23376;&#27169;&#20215;&#20540;&#30340;&#20195;&#29702;&#20154;&#20043;&#38388;&#20844;&#24179;&#20998;&#37197;&#19981;&#21487;&#20998;&#21106;&#29289;&#21697;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;leximin&#12289;max Nash welfare&#65288;MNW&#65289;&#21644;$p$-mean welfare&#26368;&#22823;&#21270;&#20998;&#37197;&#12290;&#22312;$ab$&#21487;&#25972;&#38500;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#27714;&#35299;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#34917;&#20805;&#20102;&#29616;&#26377;&#32467;&#26524;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35813;&#25991;&#30740;&#31350;&#20102;leximin&#21644;MNW&#20998;&#37197;&#22312;&#26080;&#23241;&#22930;&#21644;&#26368;&#22823;&#26368;&#23567;&#20221;&#39069;&#20445;&#35777;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20108;&#20803;&#23376;&#27169;&#20215;&#20540;&#30340;&#20195;&#29702;&#20154;&#20043;&#38388;&#20844;&#24179;&#20998;&#37197;&#19968;&#32452;&#19981;&#21487;&#20998;&#21106;&#30340;&#29289;&#21697;&#30340;&#38382;&#39064;&#12290;&#27599;&#20010;&#29289;&#21697;&#37117;&#25552;&#20379;$a$&#25110;$b$&#30340;&#36793;&#38469;&#25910;&#30410;&#65288;$a&lt;b$&#65289;&#65292;&#19988;&#29289;&#21697;&#30340;&#36793;&#38469;&#25910;&#30410;&#36882;&#20943;&#12290;&#36825;&#26159;&#20004;&#31181;&#24191;&#27867;&#30740;&#31350;&#30340;&#35780;&#20272;&#31867;&#21035;&#8212;&#8212;&#20108;&#20803;&#21152;&#24615;&#35780;&#20272;&#21644;&#20108;&#20803;&#23376;&#27169;&#35780;&#20272;&#30340;&#33258;&#28982;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#31639;&#27861;&#26694;&#26550;&#65292;&#22522;&#20110;&#26368;&#36817;&#20171;&#32461;&#30340;&#8220;&#25196;&#22522;&#20132;&#25442;&#8221;&#26426;&#21046;&#65292;&#21487;&#20197;&#36866;&#24212;&#35745;&#31639;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#21253;&#25324;leximin&#12289;max Nash welfare&#65288;MNW&#65289;&#21644;&#24403;$a$&#38500;&#20197;$b$&#26102;$p$-mean welfare&#26368;&#22823;&#21270;&#20998;&#37197;&#12290;&#36825;&#20010;&#32467;&#26524;&#34917;&#20805;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#32467;&#26524;&#65292;&#21363;&#24403;$a$&#19981;&#33021;&#34987;$b$&#25972;&#38500;&#26102;&#65292;leximin&#21644;MNW&#20998;&#37197;&#30340;&#35745;&#31639;&#26080;&#27861;&#22797;&#26434;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22312;&#20004;&#20010;&#30693;&#21517;&#23646;&#24615;&#8212;&#8212;&#26080;&#23241;&#22930;&#21644;&#26368;&#22823;&#26368;&#23567;&#20221;&#39069;&#20445;&#35777;&#26041;&#38754;&#30340;leximin&#21644;MNW&#20998;&#37197;&#12290;&#22312;&#26080;&#23241;&#22930;&#26041;&#38754;&#65292;&#25105;&#20204;&#34920;&#26126;leximin&#21644;MNW&#37117;&#19981;&#28385;&#36275;
&lt;/p&gt;
&lt;p&gt;
We study the problem of fairly allocating a set of indivisible goods among agents with bivalued submodular valuations -- each good provides a marginal gain of either $a$ or $b$ ($a &lt; b$) and goods have decreasing marginal gains. This is a natural generalization of two well-studied valuation classes -bivalued additive valuations and binary submodular valuations. We present a simple sequential algorithmic framework, based on the recently introduced Yankee Swap mechanism, that can be adapted to compute a variety of solution concepts, including leximin, max Nash welfare (MNW) and $p$-mean welfare maximizing allocations when $a$ divides $b$. This result is complemented by an existing result on the computational intractability of leximin and MNW allocations when $a$ does not divide $b$.  We further examine leximin and MNW allocations with respect to two well-known properties -- envy freeness and the maximin share guarantee. On envy freeness, we show that neither the leximin nor the MNW all
&lt;/p&gt;</description></item><item><title>LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2301.09799</link><description>&lt;p&gt;
LDMIC&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LDMIC: Learning-based Distributed Multi-view Image Coding. (arXiv:2301.09799v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09799
&lt;/p&gt;
&lt;p&gt;
LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
LDMIC is a learning-based distributed multi-view image coding framework that captures global inter-view correlations through independent encoders and a joint context transfer module based on the cross-attention mechanism, which is insensitive to geometric relations.
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#22270;&#20687;&#21387;&#32553;&#22312;3D&#30456;&#20851;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#39044;&#27979;&#32534;&#30721;&#26550;&#26500;&#65292;&#38656;&#35201;&#32852;&#21512;&#32534;&#30721;&#21387;&#32553;&#30456;&#24212;&#30340;&#35270;&#24046;&#21644;&#27531;&#24046;&#20449;&#24687;&#12290;&#36825;&#35201;&#27714;&#30456;&#26426;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#65292;&#24182;&#24378;&#21046;&#25191;&#34892;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#26497;&#32447;&#20960;&#20309;&#32422;&#26463;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#38543;&#26426;&#37325;&#21472;&#35270;&#37326;&#30340;&#20998;&#24067;&#24335;&#30456;&#26426;&#31995;&#32479;&#20013;&#37096;&#32626;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#29702;&#35770;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#21644;&#32852;&#21512;&#35299;&#30721;&#23454;&#29616;&#30456;&#20851;&#28304;&#30340;&#39640;&#25928;&#25968;&#25454;&#21387;&#32553;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#35774;&#35745;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#65288;LDMIC&#65289;&#26694;&#26550;&#30340;&#21160;&#26426;&#12290;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#65292;LDMIC&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Mystique&#65292;&#19968;&#20010;&#20934;&#30830;&#21487;&#25193;&#23637;&#30340;&#29983;&#20135;AI&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;PyTorch&#25191;&#34892;&#36319;&#36394;&#26469;&#25429;&#33719;AI&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#20449;&#24687;&#21644;&#20803;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#22320;&#29983;&#25104;&#20855;&#26377;&#20195;&#34920;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#30340;AI&#22522;&#20934;&#27979;&#35797;&#65292;&#19988;&#20855;&#26377;&#20302;&#36816;&#34892;&#26102;&#36127;&#33655;&#21644;&#20202;&#22120;&#21270;&#24037;&#20316;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2301.04122</link><description>&lt;p&gt;
Mystique: &#23454;&#29616;&#31934;&#30830;&#21487;&#25193;&#23637;&#30340;&#29983;&#20135;AI&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Mystique: Enabling Accurate and Scalable Generation of Production AI Benchmarks. (arXiv:2301.04122v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Mystique&#65292;&#19968;&#20010;&#20934;&#30830;&#21487;&#25193;&#23637;&#30340;&#29983;&#20135;AI&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;PyTorch&#25191;&#34892;&#36319;&#36394;&#26469;&#25429;&#33719;AI&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#20449;&#24687;&#21644;&#20803;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#22320;&#29983;&#25104;&#20855;&#26377;&#20195;&#34920;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#30340;AI&#22522;&#20934;&#27979;&#35797;&#65292;&#19988;&#20855;&#26377;&#20302;&#36816;&#34892;&#26102;&#36127;&#33655;&#21644;&#20202;&#22120;&#21270;&#24037;&#20316;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#24555;&#36895;&#22686;&#38271;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#37327;&#65292;&#26500;&#24314;&#22823;&#22411;AI&#38598;&#32676;&#25104;&#20026;&#29616;&#20195;&#20113;&#25552;&#20379;&#21830;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#20934;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#22312;&#35774;&#35745;&#24555;&#33410;&#22863;&#36719;&#20214;&#21644;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Mystique&#65292;&#36825;&#26159;&#19968;&#20010;&#31934;&#30830;&#21487;&#25193;&#23637;&#30340;&#29983;&#20135;AI&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#20811;&#26381;&#21487;&#25193;&#23637;&#24615;&#30340;&#20004;&#20010;&#20027;&#35201;&#38590;&#39064;&#65306;&#65288;i&#65289;&#24037;&#20316;&#36127;&#36733;&#30340;&#20195;&#34920;&#24615;&#65307;&#65288;ii&#65289;&#24555;&#36895;&#23558;&#38598;&#32676;&#21464;&#26356;&#32435;&#20837;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;Mystique&#21033;&#29992;PyTorch&#25191;&#34892;&#36319;&#36394;&#65288;ET&#65289;&#21151;&#33021;&#65292;&#20197;&#36816;&#31639;&#31526;&#30340;&#24418;&#24335;&#65292;&#20197;&#22270;&#24418;&#26684;&#24335;&#25429;&#33719;AI&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#20449;&#24687;&#21644;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#26469;&#28304;&#20110;&#38598;&#32676;ET&#30340;&#26500;&#24314;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#20195;&#34920;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#30340;AI&#22522;&#20934;&#27979;&#35797;&#12290;&#36731;&#37327;&#32423;&#25968;&#25454;&#25910;&#38598;&#12289;&#20302;&#36816;&#34892;&#26102;&#36127;&#33655;&#21644;&#20202;&#22120;&#21270;&#24037;&#20316;&#30340;Mystique&#34920;&#31034;&#26159;&#21487;&#25193;&#23637;&#30340;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building large AI fleets to support the rapidly growing DL workloads is an active research topic for modern cloud providers. Generating accurate benchmarks plays an essential role in designing the fast-paced software and hardware solutions in this space. Two fundamental challenges to make this scalable are (i) workload representativeness and (ii) the ability to quickly incorporate changes to the fleet into the benchmarks.  To overcome these issues, we propose Mystique, an accurate and scalable framework for production AI benchmark generation. It leverages the PyTorch execution trace (ET), a new feature that captures the runtime information of AI models at the granularity of operators, in a graph format, together with their metadata. By sourcing fleet ETs, we can build AI benchmarks that are portable and representative. Mystique is scalable, due to its lightweight data collection, in terms of runtime overhead and instrumentation effort. It is also adaptive because ET composability allow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20123;&#26497;&#31471;&#30340;&#22270;&#20687;&#21464;&#25442;&#65292;&#21457;&#29616;&#26426;&#22120;&#21644;&#20154;&#31867;&#22312;&#36825;&#20123;&#21464;&#25442;&#19979;&#30340;&#34920;&#29616;&#24046;&#24322;&#36739;&#22823;&#65292;&#26426;&#22120;&#22312;&#26576;&#20123;&#21464;&#25442;&#19979;&#27604;&#20154;&#31867;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#20154;&#31867;&#23481;&#26131;&#22788;&#29702;&#30340;&#21464;&#25442;&#19979;&#34920;&#29616;&#19981;&#22914;&#20154;&#31867;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#25913;&#21892;&#26426;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.13967</link><description>&lt;p&gt;
&#26497;&#31471;&#22270;&#20687;&#21464;&#25442;&#22312;&#20154;&#31867;&#21644;&#26426;&#22120;&#19978;&#20135;&#29983;&#19981;&#21516;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Extreme Image Transformations Affect Humans and Machines Differently. (arXiv:2212.13967v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20123;&#26497;&#31471;&#30340;&#22270;&#20687;&#21464;&#25442;&#65292;&#21457;&#29616;&#26426;&#22120;&#21644;&#20154;&#31867;&#22312;&#36825;&#20123;&#21464;&#25442;&#19979;&#30340;&#34920;&#29616;&#24046;&#24322;&#36739;&#22823;&#65292;&#26426;&#22120;&#22312;&#26576;&#20123;&#21464;&#25442;&#19979;&#27604;&#20154;&#31867;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#20154;&#31867;&#23481;&#26131;&#22788;&#29702;&#30340;&#21464;&#25442;&#19979;&#34920;&#29616;&#19981;&#22914;&#20154;&#31867;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#25913;&#21892;&#26426;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#26368;&#36817;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANN)&#22768;&#31216;&#27169;&#25311;&#28789;&#38271;&#31867;&#21160;&#29289;&#31070;&#32463;&#21644;&#20154;&#31867;&#30340;&#34920;&#29616;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#23545;&#35937;&#35782;&#21035;&#26041;&#38754;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#20197;&#20154;&#31867;&#19981;&#24120;&#37319;&#29992;&#30340;&#26041;&#24335;&#21033;&#29992;&#20302;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#35270;&#35273;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;ANN&#26469;&#35828;&#65292;&#36229;&#20986;&#20998;&#24067;&#25110;&#23545;&#25239;&#24615;&#36755;&#20837;&#32463;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#23398;&#20064;&#25277;&#35937;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#23545;&#35768;&#22810;&#26497;&#31471;&#22270;&#20687;&#30072;&#21464;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#22270;&#20687;&#21464;&#25442;&#65292;&#21463;&#31070;&#32463;&#29983;&#29702;&#23398;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#24182;&#22312;&#23545;&#35937;&#35782;&#21035;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#20154;&#31867;&#21644;ANN&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#21464;&#25442;&#65292;&#26426;&#22120;&#30340;&#34920;&#29616;&#27604;&#20154;&#31867;&#26356;&#22909;&#65292;&#32780;&#23545;&#20110;&#26576;&#20123;&#23545;&#20154;&#31867;&#31616;&#21333;&#30340;&#21464;&#25442;&#65292;&#26426;&#22120;&#38590;&#20197;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#65292;&#24182;&#20026;&#25105;&#20204;&#30340;&#21464;&#25442;&#25968;&#25454;&#25214;&#21040;&#20102;&#19968;&#20010;&#38590;&#24230;&#25490;&#21517;&#12290;&#25105;&#20204;&#36824;&#24314;&#35758;&#22914;&#20309;&#25913;&#21464;&#20154;&#31867;&#35270;&#35273;&#22788;&#29702;&#30340;&#26576;&#20123;&#29305;&#24449;&#20197;&#25552;&#39640;ANN&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some recent artificial neural networks (ANNs) claim to model aspects of primate neural and human performance data. Their success in object recognition is, however, dependent on exploiting low-level features for solving visual tasks in a way that humans do not. As a result, out-of-distribution or adversarial input is often challenging for ANNs. Humans instead learn abstract patterns and are mostly unaffected by many extreme image distortions. We introduce a set of novel image transforms inspired by neurophysiological findings and evaluate humans and ANNs on an object recognition task. We show that machines perform better than humans for certain transforms and struggle to perform at par with humans on others that are easy for humans. We quantify the differences in accuracy for humans and machines and find a ranking of difficulty for our transforms for human data. We also suggest how certain characteristics of human visual processing can be adapted to improve the performance of ANNs for o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#21462;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#30340;&#31572;&#26696;&#27010;&#29575;&#65292;&#33021;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;</title><link>http://arxiv.org/abs/2212.05762</link><description>&lt;p&gt;
&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Momentum Contrastive Pre-training for Question Answering. (arXiv:2212.05762v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#21462;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#30340;&#31572;&#26696;&#27010;&#29575;&#65292;&#33021;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25277;&#21462;&#24335;&#38382;&#31572;&#65288;QA&#65289;&#39044;&#35757;&#32451;&#26041;&#27861;&#29983;&#25104;&#31867;&#20284;&#20110;&#22635;&#31354;&#39064;&#30340;&#26597;&#35810;&#65292;&#20854;&#35821;&#27861;&#32467;&#26500;&#19982;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19981;&#21516;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#31616;&#21333;&#30340;&#20851;&#38190;&#35789;&#21305;&#37197;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#37327;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;MCROSS&#65288;Momentum Contrastive pRe-training fOr queStion anSwering&#65289;&#29992;&#20110;&#25277;&#21462;&#24335;&#30340;&#38382;&#39064;&#22238;&#31572;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MCROSS&#24341;&#20837;&#20102;&#21160;&#37327;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#31572;&#26696;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;QA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#30417;&#30563;&#21644;&#38646;-shot&#22330;&#26223;&#19979;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing pre-training methods for extractive Question Answering (QA) generate cloze-like queries different from natural questions in syntax structure, which could overfit pre-trained models to simple keyword matching. In order to address this problem, we propose a novel Momentum Contrastive pRe-training fOr queStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS introduces a momentum contrastive learning framework to align the answer probability between cloze-like and natural query-passage sample pairs. Hence, the pre-trained models can better transfer the knowledge learned in cloze-like samples to answering natural questions. Experimental results on three benchmarking QA datasets show that our method achieves noticeable improvement compared with all baselines in both supervised and zero-shot scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20154;&#24615;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#21457;&#29616;&#24403;&#21069;AI&#39550;&#39542;&#21592;&#19981;&#33021;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#65292;&#38656;&#35201;&#22312;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.02908</link><description>&lt;p&gt;
&#36808;&#21521;&#20154;&#31867;&#20860;&#23481;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65306;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#30340;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v5 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20154;&#24615;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#21457;&#29616;&#24403;&#21069;AI&#39550;&#39542;&#21592;&#19981;&#33021;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#65292;&#38656;&#35201;&#22312;&#24773;&#24863;&#36807;&#28193;&#27169;&#22411;&#20013;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#36208;&#21521;&#26080;&#38656;&#21452;&#25163;&#30340;&#29983;&#27963;&#26041;&#24335;&#26102;&#65292;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#65292;&#22914;&#26524;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33021;&#22815;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#39550;&#39542;&#65292;&#20154;&#20204;&#20250;&#26356;&#23481;&#26131;&#25509;&#21463;&#23427;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#23569;&#37327;&#30740;&#31350;&#20174;&#20056;&#23458;&#35282;&#24230;&#30340;&#33258;&#28982;&#20307;&#39564;&#26469;&#26816;&#39564;&#30446;&#21069;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#39033;&#30495;&#23454;&#36947;&#36335;&#29615;&#22659;&#19979;&#30340;&#35797;&#39564;&#65292;&#27979;&#35797;&#20102;69&#20301;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#65292;&#23581;&#35797;&#20102;&#35299;AI&#39550;&#39542;&#20154;&#21592;&#33021;&#21542;&#20026;&#20056;&#23458;&#21019;&#36896;&#31867;&#20284;&#20154;&#31867;&#30340;&#39550;&#20056;&#20307;&#39564;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#39550;&#20056;&#20307;&#39564;&#30340;&#38750;&#35821;&#35328;&#22270;&#28789;&#27979;&#35797;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20316;&#20026;&#20056;&#23458;&#20056;&#22352;AI&#39550;&#39542;&#20154;&#21592;&#25110;&#20154;&#31867;&#39550;&#39542;&#20154;&#21592;&#39550;&#39542;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#24182;&#21028;&#26029;&#21496;&#26426;&#26159;&#20154;&#31867;&#36824;&#26159;AI&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20805;&#24403;AI&#39550;&#39542;&#21592;&#30340;&#27773;&#36710;&#26410;&#33021;&#36890;&#36807;&#25105;&#20204;&#30340;&#27979;&#35797;&#65292;&#22240;&#20026;&#20056;&#23458;&#33021;&#22815;&#36229;&#36807;&#38543;&#26426;&#29468;&#27979;&#22320;&#35782;&#21035;&#20986;AI&#39550;&#39542;&#21592;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#39550;&#39542;&#21592;&#39550;&#39542;&#36710;&#36742;&#26102;&#65292;&#20056;&#23458;&#30340;&#21028;&#26029;&#32467;&#26524;&#32422;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#38468;&#36817;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20154;&#31867;&#20056;&#23458;&#22312;&#39550;&#39542;&#36807;&#31243;&#20013;&#32473;&#20104;&#20102;&#21738;&#20123;&#20154;&#24615;&#21270;&#29305;&#24449;&#30340;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous cars are indispensable when humans go further down the hands-free route. Although existing literature highlights that the acceptance of the autonomous car will increase if it drives in a human-like manner, sparse research offers the naturalistic experience from a passenger's seat perspective to examine the human likeness of current autonomous cars. The present study tested whether the AI driver could create a human-like ride experience for passengers based on 69 participants' feedback in a real-road scenario. We designed a ride experience-based version of the non-verbal Turing test for automated driving. Participants rode in autonomous cars (driven by either human or AI drivers) as a passenger and judged whether the driver was human or AI. The AI driver failed to pass our test because passengers detected the AI driver above chance. In contrast, when the human driver drove the car, the passengers' judgement was around chance. We further investigated how human passengers ascri
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#30693;&#35782;&#24211;&#36827;&#34892;&#34917;&#20840;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.01923</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#30693;&#35782;&#24211;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph. (arXiv:2212.01923v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01923
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#30693;&#35782;&#24211;&#36827;&#34892;&#34917;&#20840;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#30693;&#35782;&#24211;&#24050;&#32463;&#34987;&#26500;&#24314;&#26469;&#23384;&#20648;&#22823;&#37327;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30693;&#35782;&#24211;&#39640;&#24230;&#19981;&#23436;&#25972;&#65292;&#20363;&#22914;Freebase&#20013;&#26377;70%&#30340;&#20154;&#27809;&#26377;&#20986;&#29983;&#22320;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#12289;&#22522;&#20110;&#26597;&#35810;&#39537;&#21160;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#31995;&#32479;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#34701;&#21512;&#26469;&#33258;Web&#30340;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#21644;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22522;&#20110;&#38382;&#31572;&#21644;&#35268;&#21017;&#25512;&#29702;&#26500;&#24314;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#20013;&#22522;&#20110;&#19981;&#21516;&#30340;&#36335;&#24452;&#23545;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#21462;&#24471;&#20102;&#27604;&#38382;&#31572;&#12289;&#35268;&#21017;&#25512;&#29702;&#21644;&#22522;&#32447;&#34701;&#21512;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#20943;&#23569;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#20026;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#24555;&#36895;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete, for example, over 70% of people in Freebase have no known place of birth. To solve this problem, we propose a query-driven knowledge base completion system with multimodal fusion of unstructured and structured information. To effectively fuse unstructured information from the Web and structured information in knowledge bases to achieve good performance, our system builds multimodal knowledge graphs based on question answering and rule inference. We propose a multimodal path fusion algorithm to rank candidate answers based on different paths in the multimodal knowledge graphs, achieving much better performance than question answering, rule inference and a baseline fusion algorithm. To improve system efficiency, query-driven techniques are utilized to reduce the runtime of our system, providing fast responses to user queries. Ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#30340;&#30693;&#35782;&#22270;&#35889;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#36716;&#21270;&#20026;&#20004;&#20010;KG&#20043;&#38388;&#30340;&#23545;&#25239;&#24615;&#38382;&#31572;&#28216;&#25103;&#65292;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#21407;&#22987;&#25968;&#25454;&#21644;&#26356;&#22810;&#32771;&#34385;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#33021;&#21147;&#23618;&#38754;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2212.00994</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#20449;&#24687;&#19979;&#30340;&#30693;&#35782;&#22270;&#35889;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Quality Evaluation under Incomplete Information. (arXiv:2212.00994v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#30340;&#30693;&#35782;&#22270;&#35889;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#36716;&#21270;&#20026;&#20004;&#20010;KG&#20043;&#38388;&#30340;&#23545;&#25239;&#24615;&#38382;&#31572;&#28216;&#25103;&#65292;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#21407;&#22987;&#25968;&#25454;&#21644;&#26356;&#22810;&#32771;&#34385;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#33021;&#21147;&#23618;&#38754;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#20855;&#26377;&#26681;&#26412;&#20316;&#29992;&#65292;&#30693;&#35782;&#22270;&#35889;(KGs)&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#23545;KG&#30340;&#36136;&#37327;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#19988;&#19981;&#21487;&#25110;&#32570;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#32500;&#24230;&#25552;&#20986;&#26032;&#30340;&#36136;&#37327;&#24230;&#37327;&#25110;&#22312;KG&#26500;&#24314;&#38454;&#27573;&#34913;&#37327;&#24615;&#33021;&#26469;&#35780;&#20272;KG&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;KG&#20013;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;KG&#30340;&#20869;&#37096;&#20449;&#24687;&#22312;&#36136;&#37327;&#35780;&#20272;&#26399;&#38388;&#26292;&#38706;&#20986;&#26469;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#26356;&#22810;&#22320;&#32771;&#34385;&#25968;&#25454;&#23618;&#38754;&#30340;&#36136;&#37327;&#65292;&#32780;&#19981;&#26159;&#33021;&#21147;&#23618;&#38754;&#30340;&#36136;&#37327;&#65292;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#26469;&#35828;&#65292;&#21518;&#32773;&#26356;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#23436;&#25972;&#20449;&#24687;&#19979;&#30340;&#30693;&#35782;&#22270;&#35889;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;(QEII)&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#19981;&#26292;&#38706;&#20219;&#20309;&#21407;&#22987;&#25968;&#25454;&#65292;&#32780;&#23558;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#36716;&#21270;&#20026;&#20004;&#20010;KG&#20043;&#38388;&#30340;&#23545;&#25239;&#24615;&#38382;&#31572;&#28216;&#25103;&#12290;&#28216;&#25103;&#32988;&#32773;&#22240;&#27492;&#34987;&#35748;&#20026;&#20855;&#26377;&#26356;&#22909;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have attracted more and more attentions because of their fundamental roles in many tasks. Quality evaluation for KGs is thus crucial and indispensable. Existing methods in this field evaluate KGs by either proposing new quality metrics from different dimensions or measuring performances at KG construction stages. However, there are two major issues with those methods. First, they highly rely on raw data in KGs, which makes KGs' internal information exposed during quality evaluation. Second, they consider more about the quality at data level instead of ability level, where the latter one is more important for downstream applications. To address these issues, we propose a knowledge graph quality evaluation framework under incomplete information (QEII). The quality evaluation task is transformed into an adversarial Q&amp;A game between two KGs. Winner of the game is thus considered to have better qualities. During the evaluation process, no raw data is exposed, which en
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#21464;&#37327;&#25490;&#24207;&#31574;&#30053;&#65292;&#21487;&#26681;&#25454;&#19981;&#21516;&#38382;&#39064;&#23454;&#20363;&#30340;&#29305;&#24449;&#23450;&#21046;&#21464;&#37327;&#25490;&#24207;&#31574;&#30053;&#20197;&#25552;&#39640;&#32422;&#26463;&#32534;&#31243;&#30340;&#27714;&#35299;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14492</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#22686;&#24378;&#32422;&#26463;&#32534;&#31243;&#22312;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing Constraint Programming via Supervised Learning for Job Shop Scheduling. (arXiv:2211.14492v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#21464;&#37327;&#25490;&#24207;&#31574;&#30053;&#65292;&#21487;&#26681;&#25454;&#19981;&#21516;&#38382;&#39064;&#23454;&#20363;&#30340;&#29305;&#24449;&#23450;&#21046;&#21464;&#37327;&#25490;&#24207;&#31574;&#30053;&#20197;&#25552;&#39640;&#32422;&#26463;&#32534;&#31243;&#30340;&#27714;&#35299;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#32534;&#31243;&#26159;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#12290;&#22312;&#32422;&#26463;&#32534;&#31243;&#27714;&#35299;&#22120;&#20013;&#65292;&#36873;&#21462;&#21464;&#37327;&#30340;&#39034;&#24207;&#23545;&#27714;&#35299;&#22120;&#30340;&#26377;&#25928;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#21464;&#37327;&#25490;&#24207;&#31574;&#30053;&#65292;&#22312;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#38382;&#39064;&#23454;&#20363;&#30340;&#26368;&#20248;&#35299;&#26469;&#25490;&#24207;&#32422;&#26463;&#32534;&#31243;&#27714;&#35299;&#22120;&#20013;&#30340;&#21464;&#37327;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#21464;&#37327;&#25490;&#24207;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#38382;&#39064;&#23454;&#20363;&#30340;&#29305;&#24449;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#30456;&#24212;&#22320;&#23450;&#21046;&#21464;&#37327;&#25490;&#24207;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38750;&#24120;&#39640;&#25928;&#19988;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23398;&#21040;&#30340;&#21464;&#37327;&#25490;&#24207;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#20063;&#24456;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constraint programming (CP) is a powerful technique for solving constraint satisfaction and optimization problems. In CP solvers, the variable ordering strategy used to select which variable to explore first in the solving process has a significant impact on solver effectiveness. To address this issue, we propose a novel variable ordering strategy based on supervised learning, which we evaluate in the context of job shop scheduling problems. Our learning-based methods predict the optimal solution of a problem instance and use the predicted solution to order variables for CP solvers. \added[]{Unlike traditional variable ordering methods, our methods can learn from the characteristics of each problem instance and customize the variable ordering strategy accordingly, leading to improved solver performance.} Our experiments demonstrate that training machine learning models is highly efficient and can achieve high accuracy. Furthermore, our learned variable ordering methods perform competit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#30340;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#26469;&#36798;&#21040;&#26356;&#39640;&#25928;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.07098</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#23436;&#25104;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion. (arXiv:2211.07098v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#30340;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#26469;&#36798;&#21040;&#26356;&#39640;&#25928;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#30693;&#35782;&#24211;&#24050;&#32463;&#24314;&#31435;&#26469;&#23384;&#20648;&#22823;&#37327;&#30693;&#35782;&#65292;&#28982;&#32780;&#36825;&#20123;&#30693;&#35782;&#24211;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#20026;&#20102;&#21033;&#29992;&#32593;&#32476;&#19978;&#30340;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#23436;&#25104;&#30693;&#35782;&#24211;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#26469;&#25552;&#21462;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#20165;&#20165;&#36890;&#36807;&#38750;&#24120;&#23569;&#30340;&#38382;&#39064;&#23601;&#21487;&#20197;&#36798;&#21040;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#38382;&#31572;&#31995;&#32479;&#36824;&#20351;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#27604;&#22914;&#23454;&#20307;&#31867;&#22411;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#20197;&#24110;&#21161;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete. To solve this problem, we propose a web-based question answering system system with multimodal fusion of unstructured and structured information, to fill in missing information for knowledge bases. To utilize unstructured information from the Web for knowledge base completion, we design a web-based question answering system using multimodal features and question templates to extract missing facts, which can achieve good performance with very few questions. To help improve extraction quality, the question answering system employs structured information from knowledge bases, such as entity types and entity-to-entity relatedness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#35821;&#20041;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#35780;&#20272;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#27604;&#36739;&#19981;&#21516;PLM&#30340;&#21487;&#38752;&#24615;&#65292;&#20445;&#35777;&#20854;&#22312;&#30456;&#21516;&#25110;&#30456;&#20284;&#30340;&#25552;&#31034;&#19979;&#29983;&#25104;&#30340;&#36755;&#20986;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2211.05853</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#19968;&#33268;&#24615;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring Reliability of Large Language Models through Semantic Consistency. (arXiv:2211.05853v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#35821;&#20041;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#35780;&#20272;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#27604;&#36739;&#19981;&#21516;PLM&#30340;&#21487;&#38752;&#24615;&#65292;&#20445;&#35777;&#20854;&#22312;&#30456;&#21516;&#25110;&#30456;&#20284;&#30340;&#25552;&#31034;&#19979;&#29983;&#25104;&#30340;&#36755;&#20986;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26497;&#39640;&#30340;&#27969;&#30021;&#24615;&#21644;&#24615;&#33021;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#34920;&#29616;&#33391;&#22909;&#30340;PLMs&#38750;&#24120;&#25935;&#24863;&#65292;&#23545;&#20110;&#36755;&#20837;&#30340;&#25552;&#31034;&#38750;&#24120;&#25935;&#24863;&#12290;&#21363;&#20351;&#25552;&#31034;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#21516;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#33021;&#32473;&#20986;&#38750;&#24120;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;&#24403;&#32771;&#34385;PLMs&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#36182;&#37096;&#32626;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#23427;&#20204;&#22312;&#24847;&#24605;&#30456;&#21516;&#25110;&#34920;&#36798;&#30456;&#21516;&#24847;&#22270;&#30340;&#25552;&#31034;&#19979;&#30340;&#36755;&#20986;&#26159;&#19968;&#33268;&#30340;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#26368;&#20808;&#36827;&#30340;PLMs&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#20294;&#23427;&#20204;&#20165;&#38480;&#20110;&#35780;&#20272;&#21333;&#20010;&#25110;&#22810;&#20010;&#21333;&#35789;&#31572;&#26696;&#30340;&#35789;&#27719;&#31561;&#20215;&#24615;&#65292;&#32780;&#19981;&#28041;&#21450;&#29983;&#25104;&#25991;&#26412;&#24207;&#21015;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#29702;&#35299;&#22312;&#29983;&#25104;&#25991;&#26412;&#35774;&#32622;&#19979;PLMs&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35821;&#20041;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#20801;&#35768;&#27604;&#36739;&#24320;&#25918;&#24335;&#25991;&#26412;&#36755;&#20986;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#20960;&#20010;&#29256;&#26412;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#35780;&#20272;&#22810;&#20010;PLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large pretrained language models (PLMs) demonstrate incredible fluency and performance on many natural language tasks, recent work has shown that well-performing PLMs are very sensitive to what prompts are feed into them. Even when prompts are semantically identical, language models may give very different answers. When considering safe and trustworthy deployments of PLMs we would like their outputs to be consistent under prompts that mean the same thing or convey the same intent. While some work has looked into how state-of-the-art PLMs address this need, they have been limited to only evaluating lexical equality of single- or multi-word answers and do not address consistency of generative text sequences. In order to understand consistency of PLMs under text generation settings, we develop a measure of semantic consistency that allows the comparison of open-ended text outputs. We implement several versions of this consistency metric to evaluate the performance of a number of PLM
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.05207</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26469;&#35782;&#21035;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#19979;&#30340;&#33041;&#30005;&#22270;&#22270;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Machine Learning System to Identify EEG Patterns on the Ictal-Interictal-Injury Continuum. (arXiv:2211.05207v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#65292;&#20154;&#20204;&#21628;&#21505;&#22312;&#29992;&#20110;&#20020;&#24202;&#24037;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#65288;&#30315;&#30187;&#12289;LPD&#12289;GPD&#12289;LRDA&#12289;GRDA&#12289;&#20854;&#20182;&#65289;&#30340;&#23384;&#22312;&#12290;&#27599;&#20010;&#39044;&#27979;&#37117;&#37197;&#26377;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#20511;&#21161;&#20110;&#19987;&#38376;&#30340;&#29992;&#25143;&#30028;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;&#27492;&#26032;&#22411;&#27169;&#22411;&#26550;&#26500;&#23398;&#20064;&#20102;&#19968;&#32452;&#21407;&#22411;&#31034;&#20363;&#65288;&#8220;&#21407;&#22411;&#8221;&#65289;&#65292;&#24182;&#36890;&#36807;&#23558;&#26032;&#30340;EEG&#29255;&#27573;&#19982;&#36825;&#20123;&#21407;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#20570;&#20986;&#20915;&#31574;&#12290;&#36825;&#20123;&#21407;&#22411;&#21487;&#20197;&#26159;&#21333;&#31867;&#65288;&#20165;&#19982;&#19968;&#20010;&#31867;&#30456;&#20851;&#65289;&#25110;&#21452;&#31867;&#65288;&#19982;&#20004;&#20010;&#31867;&#30456;&#20851;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65306;1&#65289;&#20351;&#29992;&#20840;&#23616;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#65292;&#23558;1275&#32500;cEEG&#28508;&#22312;&#29305;&#24449;&#26144;&#23556;&#21040;&#20108;&#32500;&#31354;&#38388;&#20013;&#65292;&#21487;&#35270;&#21270;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20854;&#39640;&#32500;&#32467;&#26500;&#12290;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#35299;&#37322;&#26041;&#27861;&#65292;&#20351;&#20154;&#31867;&#19987;&#23478;&#33021;&#22815;&#26597;&#35810;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#25509;&#25910;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#35299;&#37322;&#12290;3&#65289;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#26576;&#20010;&#20915;&#31574;&#30340;&#36755;&#20837;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#65292;&#20801;&#35768;&#35814;&#32454;&#26816;&#26597;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#24615;&#27169;&#22411;&#20998;&#31867;EEG&#22270;&#26696;&#21644;&#25552;&#20379;&#19987;&#23478;&#21451;&#22909;&#30340;&#35299;&#37322;&#30340;&#23454;&#29992;&#24615;&#65292;&#36825;&#20004;&#20010;&#26041;&#38754;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many medical subfields, there is a call for greater interpretability in the machine learning systems used for clinical work. In this paper, we design an interpretable deep learning model to predict the presence of 6 types of brainwave patterns (Seizure, LPD, GPD, LRDA, GRDA, other) commonly encountered in ICU EEG monitoring. Each prediction is accompanied by a high-quality explanation delivered with the assistance of a specialized user interface. This novel model architecture learns a set of prototypical examples (``prototypes'') and makes decisions by comparing a new EEG segment to these prototypes. These prototypes are either single-class (affiliated with only one class) or dual-class (affiliated with two classes).  We present three main ways of interpreting the model: 1) Using global-structure preserving methods, we map the 1275-dimensional cEEG latent features to a 2D space to visualize the ictal-interictal-injury continuum and gain insight into its high-dimensional structure. 2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23436;&#20840;&#23433;&#20840;&#30340;&#38544;&#20889;&#26415;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;&#26368;&#23567;&#29109;&#32806;&#21512;&#36807;&#31243;&#25165;&#26159;&#26368;&#39640;&#25928;&#30340;&#65292;&#25552;&#20986;&#20102;&#39640;&#21487;&#20280;&#32553;&#24615;&#30340;&#23436;&#32654;&#23433;&#20840;&#38544;&#20889;&#31639;&#27861;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.14889</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#23567;&#29109;&#32806;&#21512;&#30340;&#23436;&#32654;&#23433;&#20840;&#38544;&#20889;&#26415;
&lt;/p&gt;
&lt;p&gt;
Perfectly Secure Steganography Using Minimum Entropy Coupling. (arXiv:2210.14889v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23436;&#20840;&#23433;&#20840;&#30340;&#38544;&#20889;&#26415;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;&#26368;&#23567;&#29109;&#32806;&#21512;&#36807;&#31243;&#25165;&#26159;&#26368;&#39640;&#25928;&#30340;&#65292;&#25552;&#20986;&#20102;&#39640;&#21487;&#20280;&#32553;&#24615;&#30340;&#23436;&#32654;&#23433;&#20840;&#38544;&#20889;&#31639;&#27861;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#20889;&#26415;&#26159;&#23558;&#31192;&#23494;&#20449;&#24687;&#32534;&#30721;&#21040;&#26080;&#23475;&#30340;&#20869;&#23481;&#20013;&#65292;&#20197;&#20415;&#23545;&#25163;&#26080;&#27861;&#24847;&#35782;&#21040;&#38544;&#34255;&#30340;&#21547;&#20041;&#30340;&#23454;&#36341;&#12290;&#26412;&#25991;&#38024;&#23545;&#38544;&#20889;&#26415;&#30340;&#23433;&#20840;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26368;&#23567;&#29109;&#32806;&#21512;&#23454;&#29616;&#23436;&#20840;&#23433;&#20840;&#30340;&#38544;&#20889;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#23436;&#20840;&#23433;&#20840;&#30340;&#38544;&#20889;&#36807;&#31243;&#20013;&#65292;&#20165;&#20165;&#26159;&#26368;&#23567;&#29109;&#32806;&#21512;&#30340;&#36807;&#31243;&#26159;&#26368;&#39640;&#25928;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#23454;&#29616;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#39640;&#21487;&#20280;&#32553;&#24615;&#30340;&#23436;&#32654;&#23433;&#20840;&#20445;&#38556;&#30340;&#38544;&#20889;&#31639;&#27861;&#25552;&#20379;&#20102;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steganography is the practice of encoding secret information into innocuous content in such a manner that an adversarial third party would not realize that there is hidden meaning. While this problem has classically been studied in security literature, recent advances in generative models have led to a shared interest among security and machine learning researchers in developing scalable steganography techniques. In this work, we show that a steganography procedure is perfectly secure under Cachin (1998)'s information theoretic-model of steganography if and only if it is induced by a coupling. Furthermore, we show that, among perfectly secure procedures, a procedure is maximally efficient if and only if it is induced by a minimum entropy coupling. These insights yield what are, to the best of our knowledge, the first steganography algorithms to achieve perfect security guarantees with non-trivial efficiency; additionally, these algorithms are highly scalable. To provide empirical valid
&lt;/p&gt;</description></item><item><title>LittleBird&#26159;&#19968;&#20010;&#22522;&#20110;BigBird&#30340;&#38382;&#31572;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#20301;&#32622;&#34920;&#31034;&#26041;&#27861;ALiBi&#21644;&#26367;&#25442;&#20840;&#23616;&#20449;&#24687;&#34920;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#23427;&#21487;&#20197;&#22312;&#30701;&#36755;&#20837;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#38271;&#36755;&#20837;&#36827;&#34892;&#24037;&#20316;&#24182;&#19988;&#22312;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.11870</link><description>&lt;p&gt;
LittleBird&#65306;&#29992;&#20110;&#38382;&#31572;&#30340;&#26356;&#24555;&#26356;&#38271;Transformer
&lt;/p&gt;
&lt;p&gt;
LittleBird: Efficient Faster &amp; Longer Transformer for Question Answering. (arXiv:2210.11870v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11870
&lt;/p&gt;
&lt;p&gt;
LittleBird&#26159;&#19968;&#20010;&#22522;&#20110;BigBird&#30340;&#38382;&#31572;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#20301;&#32622;&#34920;&#31034;&#26041;&#27861;ALiBi&#21644;&#26367;&#25442;&#20840;&#23616;&#20449;&#24687;&#34920;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#23427;&#21487;&#20197;&#22312;&#30701;&#36755;&#20837;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#38271;&#36755;&#20837;&#36827;&#34892;&#24037;&#20316;&#24182;&#19988;&#22312;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BERT&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#25104;&#26524;&#12290;&#20294;&#30001;&#20110;&#20854;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;Longformer&#12289;ETC&#21644;BigBird&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20108;&#27425;&#20381;&#36182;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#24182;&#19981;&#36275;&#22815;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;LittleBird&#65292;&#23427;&#26159;&#22522;&#20110;BigBird&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#36895;&#24230;&#21644;&#26356;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#12289;&#26356;&#26377;&#25928;&#30340;&#20301;&#32622;&#34920;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;Attention with Linear Biases (ALiBi)&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22312;BigBird&#20013;&#29992;&#20110;&#34920;&#31034;&#20840;&#23616;&#20449;&#24687;&#30340;&#26041;&#27861;&#21487;&#20197;&#26367;&#25442;&#20026;&#25171;&#21253;&#21644;&#35299;&#21253;&#27880;&#24847;&#21147;&#65292;&#36825;&#26356;&#26377;&#25928;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#29992;&#20110;&#30701;&#36755;&#20837;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#38271;&#36755;&#20837;&#36827;&#34892;&#24037;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#37325;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#39640;&#25928;&#22320;&#35757;&#32451;&#30701;&#36755;&#20837;&#12290;&#36825;&#23545;&#20110;&#33719;&#21462;&#22823;&#37327;&#38271;&#25991;&#26412;&#25968;&#25454;&#22256;&#38590;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#32780;&#35328;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism. Longformer, ETC and BigBird addressed this issue and effectively solved the quadratic dependency problem. However we find that these models are not sufficient, and propose LittleBird, a novel model based on BigBird with improved speed and memory footprint while maintaining accuracy. In particular, we devise a more flexible and efficient position representation method based on Attention with Linear Biases (ALiBi). We also show that replacing the method of global information represented in the BigBird with pack and unpack attention is more effective. The proposed model can work on long inputs even after being pre-trained on short inputs, and can be trained efficiently reusing existing pre-trained language model for short inputs. This is a significant benefit for low-resource languages where large amounts of long text data are difficult to obtain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20840;&#23616;&#36923;&#36753;GNN&#35299;&#37322;&#22120;GLGExplainer&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20197;&#20219;&#24847;&#24067;&#23572;&#32452;&#21512;&#30340;&#24418;&#24335;&#29983;&#25104;GNN&#23398;&#20064;&#30340;&#22270;&#24418;&#27010;&#24565;&#30340;&#35299;&#37322;&#22120;&#12290;GLGExplainer&#25552;&#20379;&#20102;&#20934;&#30830;&#21487;&#35299;&#37322;&#30340;&#20840;&#23616;&#35299;&#37322;&#65292;&#19982;GNN&#30340;&#32452;&#21512;&#24615;&#36136;&#30456;&#19968;&#33268;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20998;&#31867;&#25110;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.07147</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#27010;&#24565;&#30340;&#36923;&#36753;&#32452;&#21512;&#65292;&#20840;&#23616;&#35299;&#37322;GNNs
&lt;/p&gt;
&lt;p&gt;
Global Explainability of GNNs via Logic Combination of Learned Concepts. (arXiv:2210.07147v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20840;&#23616;&#36923;&#36753;GNN&#35299;&#37322;&#22120;GLGExplainer&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20197;&#20219;&#24847;&#24067;&#23572;&#32452;&#21512;&#30340;&#24418;&#24335;&#29983;&#25104;GNN&#23398;&#20064;&#30340;&#22270;&#24418;&#27010;&#24565;&#30340;&#35299;&#37322;&#22120;&#12290;GLGExplainer&#25552;&#20379;&#20102;&#20934;&#30830;&#21487;&#35299;&#37322;&#30340;&#20840;&#23616;&#35299;&#37322;&#65292;&#19982;GNN&#30340;&#32452;&#21512;&#24615;&#36136;&#30456;&#19968;&#33268;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20998;&#31867;&#25110;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;GNN&#30340;&#23454;&#20363;&#32423;&#35299;&#37322;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#24050;&#32463;&#24320;&#21457;&#20102;&#24456;&#22810;&#26041;&#27861;&#65292;&#20294;&#26159;&#25552;&#20379;GNN&#34892;&#20026;&#30340;&#20840;&#23616;&#35299;&#37322;&#21364;&#24456;&#23569;&#34987;&#25506;&#35752;&#65292;&#23613;&#31649;&#23427;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#35843;&#35797;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20165;&#21015;&#20986;&#32473;&#23450;&#31867;&#21035;&#30340;&#23616;&#37096;&#35299;&#37322;&#65292;&#35201;&#20040;&#29983;&#25104;&#19968;&#20010;&#20855;&#26377;&#32473;&#23450;&#31867;&#21035;&#26368;&#22823;&#20998;&#25968;&#30340;&#21512;&#25104;&#21407;&#22411;&#22270;&#65292;&#23436;&#20840;&#24573;&#30053;&#20102;GNN&#21487;&#33021;&#24050;&#32463;&#23398;&#20064;&#30340;&#20219;&#20309;&#32452;&#21512;&#24615;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GLGExplainer&#65288;&#22522;&#20110;&#20840;&#23616;&#36923;&#36753;&#30340;GNN&#35299;&#37322;&#22120;&#65289;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#23398;&#20064;&#36807;&#30340;&#22270;&#24418;&#27010;&#24565;&#30340;&#20219;&#24847;&#24067;&#23572;&#32452;&#21512;&#30340;&#35299;&#37322;&#30340;&#20840;&#23616;&#35299;&#37322;&#22120;&#12290;GLGExplainer&#26159;&#19968;&#20010;&#20840;&#21487;&#24494;&#26550;&#26500;&#65292;&#23427;&#20197;&#23616;&#37096;&#35299;&#37322;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#22522;&#20110;&#22270;&#24418;&#27010;&#24565;&#30340;&#36923;&#36753;&#20844;&#24335;&#65292;&#34920;&#31034;&#20026;&#23616;&#37096;&#35299;&#37322;&#30340;&#38598;&#32676;&#12290;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#21453;&#65292;GLGExplainer&#25552;&#20379;&#20102;&#20934;&#30830;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#20840;&#23616;&#35299;&#37322;&#65292;&#19982;GNN&#30340;&#32452;&#21512;&#24615;&#36136;&#30456;&#19968;&#33268;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20998;&#31867;&#25110;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;GLGExplainer&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20854;&#20013;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned. In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#38388;&#27880;&#24847;&#21147;&#21333;&#20803;&#65288;TAU&#65289;&#20197;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#23558;&#26102;&#38388;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#24103;&#20869;&#38745;&#24577;&#27880;&#24847;&#21147;&#21644;&#24103;&#38388;&#21160;&#24577;&#27880;&#24847;&#21147;&#12290;&#21516;&#26102;&#24341;&#20837;&#24046;&#20998;&#31163;&#25955;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32771;&#34385;&#24103;&#38388;&#21464;&#21270;&#65292;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2206.12126</link><description>&lt;p&gt;
&#26102;&#38388;&#27880;&#24847;&#21147;&#21333;&#20803;&#65306;&#38754;&#21521;&#39640;&#25928;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning. (arXiv:2206.12126v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#38388;&#27880;&#24847;&#21147;&#21333;&#20803;&#65288;TAU&#65289;&#20197;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#23558;&#26102;&#38388;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#24103;&#20869;&#38745;&#24577;&#27880;&#24847;&#21147;&#21644;&#24103;&#38388;&#21160;&#24577;&#27880;&#24847;&#21147;&#12290;&#21516;&#26102;&#24341;&#20837;&#24046;&#20998;&#31163;&#25955;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32771;&#34385;&#24103;&#38388;&#21464;&#21270;&#65292;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#24103;&#26469;&#29983;&#25104;&#26410;&#26469;&#24103;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#31354;&#38388;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#25429;&#25417;&#24103;&#20869;&#29305;&#24449;&#65292;&#20013;&#38388;&#30340;&#26102;&#38388;&#27169;&#22359;&#25429;&#25417;&#24103;&#38388;&#30456;&#20851;&#24615;&#12290;&#34429;&#28982;&#20027;&#27969;&#26041;&#27861;&#37319;&#29992;&#36882;&#24402;&#21333;&#20803;&#26469;&#25429;&#33719;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#30001;&#20110;&#26080;&#27861;&#24182;&#34892;&#21270;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#24182;&#34892;&#21270;&#26102;&#38388;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#27880;&#24847;&#21147;&#21333;&#20803;&#65288;TAU&#65289;&#65292;&#23558;&#26102;&#38388;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#24103;&#20869;&#38745;&#24577;&#27880;&#24847;&#21147;&#21644;&#24103;&#38388;&#21160;&#24577;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20391;&#37325;&#20110;&#24103;&#20869;&#35823;&#24046;&#65292;&#20294;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#31163;&#25955;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#32771;&#34385;&#24103;&#38388;&#21464;&#21270;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#27966;&#29983;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal predictive learning aims to generate future frames by learning from historical frames. In this paper, we investigate existing methods and present a general framework of spatiotemporal predictive learning, in which the spatial encoder and decoder capture intra-frame features and the middle temporal module catches inter-frame correlations. While the mainstream methods employ recurrent units to capture long-term temporal dependencies, they suffer from low computational efficiency due to their unparallelizable architectures. To parallelize the temporal module, we propose the Temporal Attention Unit (TAU), which decomposes the temporal attention into intra-frame statical attention and inter-frame dynamical attention. Moreover, while the mean squared error loss focuses on intra-frame errors, we introduce a novel differential divergence regularization to take inter-frame variations into account. Extensive experiments demonstrate that the proposed method enables the derived mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#27979;&#21644;&#38450;&#24481;&#25200;&#21160;&#22522;&#30784;&#35299;&#37322;&#22120;&#30340;&#24694;&#24847;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24615;&#30340;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2205.14772</link><description>&lt;p&gt;
&#26080;&#27450;&#39575;&#24615;&#22522;&#20110;&#25200;&#21160;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unfooling Perturbation-Based Post Hoc Explainers. (arXiv:2205.14772v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#27979;&#21644;&#38450;&#24481;&#25200;&#21160;&#22522;&#30784;&#35299;&#37322;&#22120;&#30340;&#24694;&#24847;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24615;&#30340;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24040;&#22823;&#36827;&#27493;&#21560;&#24341;&#20102;&#21307;&#29983;&#12289;&#36151;&#27454;&#20154;&#12289;&#27861;&#23448;&#21644;&#20854;&#20182;&#19987;&#19994;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29087;&#24713;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20154;&#23545;&#20854;&#20915;&#31574;&#36807;&#31243;&#32570;&#20047;&#36879;&#26126;&#24230;&#34920;&#31034;&#25285;&#24551;&#12290;&#25200;&#21160;&#22522;&#30784;&#20107;&#21518;&#35299;&#37322;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#36825;&#20123;&#31995;&#32479;&#36827;&#34892;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#26597;&#35810;&#32423;&#21035;&#35775;&#38382;&#26435;&#38480;&#21363;&#21487;&#35299;&#37322;&#20219;&#20309;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#35299;&#37322;&#22120;&#21487;&#20197;&#34987;&#24694;&#24847;&#25915;&#20987;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#23457;&#35745;&#21592;&#12289;&#30417;&#31649;&#26426;&#26500;&#21644;&#20854;&#20182;&#30417;&#30563;&#32773;&#20135;&#29983;&#20102;&#20005;&#37325;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#20960;&#20010;&#33258;&#28982;&#38382;&#39064;&#26174;&#32780;&#26131;&#35265;&#8212;&#8212;&#25105;&#20204;&#22914;&#20309;&#23457;&#35745;&#36825;&#20123;&#40657;&#21283;&#23376;&#31995;&#32479;&#65311;&#25105;&#20204;&#22914;&#20309;&#30830;&#23450;&#23457;&#35745;&#23545;&#35937;&#26159;&#21542;&#26159;&#30495;&#35802;&#37197;&#21512;&#23457;&#35745;&#30340;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20005;&#26684;&#35268;&#33539;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#25269;&#24481;&#25200;&#21160;&#22522;&#30784;&#35299;&#37322;&#22120;&#30340;&#24694;&#24847;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26816;&#27979;&#65288;CAD-Detect&#65289;&#21644;&#38450;&#24481;&#65288;CAD-Defense&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#20986;&#24694;&#24847;&#25915;&#20987;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#25269;&#24481;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24615;&#25552;&#20379;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monumental advancements in artificial intelligence (AI) have lured the interest of doctors, lenders, judges, and other professionals. While these high-stakes decision-makers are optimistic about the technology, those familiar with AI systems are wary about the lack of transparency of its decision-making processes. Perturbation-based post hoc explainers offer a model agnostic means of interpreting these systems while only requiring query-level access. However, recent work demonstrates that these explainers can be fooled adversarially. This discovery has adverse implications for auditors, regulators, and other sentinels. With this in mind, several natural questions arise - how can we audit these black box systems? And how can we ascertain that the auditee is complying with the audit in good faith? In this work, we rigorously formalize this problem and devise a defense against adversarial attacks on perturbation-based explainers. We propose algorithms for the detection (CAD-Detect) and de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35748;&#20026;CNN&#22312;&#23398;&#20064;&#22270;&#20687;&#22359;&#21518;&#36991;&#20813;&#20102;&#32500;&#25968;&#28798;&#38590;&#65292;&#24182;&#25552;&#20379;&#20102;&#25903;&#25345;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2205.10760</link><description>&lt;p&gt;
CNN&#22312;&#23398;&#20064;&#22270;&#20687;&#22359;&#21518;&#36991;&#20813;&#20102;&#32500;&#25968;&#28798;&#38590;
&lt;/p&gt;
&lt;p&gt;
CNNs Avoid Curse of Dimensionality by Learning on Patches. (arXiv:2205.10760v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35748;&#20026;CNN&#22312;&#23398;&#20064;&#22270;&#20687;&#22359;&#21518;&#36991;&#20813;&#20102;&#32500;&#25968;&#28798;&#38590;&#65292;&#24182;&#25552;&#20379;&#20102;&#25903;&#25345;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#24182;&#20855;&#26377;&#38750;&#20961;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#20026;&#27490;&#65292;&#26377;&#20851;&#39044;&#27979;CNN&#27867;&#21270;&#38169;&#35823;&#30340;&#23581;&#35797;&#21482;&#38480;&#20110;&#20107;&#21518;&#20998;&#26512;&#12290;&#20107;&#20808;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#29702;&#35770;&#22823;&#22810;&#24573;&#30053;&#20102;&#21367;&#31215;&#24615;&#26041;&#38754;&#65292;&#24182;&#19988;&#19981;&#25351;&#26126;CNN&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#20854;&#20013;&#22270;&#20687;&#32500;&#25968;&#20026;&#25968;&#21315;&#65289;&#20013;&#20026;&#20309;&#33021;&#22815;&#20284;&#20046;&#20811;&#26381;&#32500;&#25968;&#28798;&#38590;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35797;&#22270;&#22312;&#20551;&#35774;CNN&#22312;&#22270;&#20687;&#22359;&#30340;&#22495;&#19978;&#36816;&#20316;&#30340;&#24773;&#20917;&#19979;&#35299;&#37322;CNN&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#20026;CNN&#30340;&#27867;&#21270;&#35823;&#24046;&#25512;&#23548;&#20808;&#39564;&#35823;&#24046;&#30028;&#30340;&#24037;&#20316;&#65292;&#24182;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35777;&#25454;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#22359;&#30340;&#29702;&#35770;&#36824;&#20026;&#20160;&#20040;&#25968;&#25454;&#22686;&#24378;&#34920;&#29616;&#20986;&#21487;&#38752;&#24615;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of convolutional neural networks (CNNs) in numerous computer vision tasks and their extraordinary generalization performances, several attempts to predict the generalization errors of CNNs have only been limited to a posteriori analyses thus far. A priori theories explaining the generalization performances of deep neural networks have mostly ignored the convolutionality aspect and do not specify why CNNs are able to seemingly overcome curse of dimensionality on computer vision tasks like image classification where the image dimensions are in thousands. Our work attempts to explain the generalization performance of CNNs on image classification under the hypothesis that CNNs operate on the domain of image patches. Ours is the first work we are aware of to derive an a priori error bound for the generalization error of CNNs and we present both quantitative and qualitative evidences in the support of our theory. Our patch-based theory also offers explanation for why data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35299;&#30721;&#32447;&#24615;&#20998;&#32452;&#30721;&#20013;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#21040;BP&#35299;&#30721;&#22120;&#20013;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36825;&#20123;&#21442;&#25968;&#21463;&#21040;&#20195;&#30721;&#30701;&#21608;&#26399;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#23398;&#20064;&#26435;&#37325;&#30340;&#35299;&#30721;&#22120;&#27604;&#37027;&#20123;&#32463;&#36807;&#35299;&#26512;&#20248;&#21270;&#30340;&#26435;&#37325;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#22797;&#26434;&#20449;&#36947;&#19978;&#26377;&#26174;&#33879;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2205.10684</link><description>&lt;p&gt;
&#35299;&#35835;&#31070;&#32463;&#26368;&#23567;&#24635;&#21644;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpreting Neural Min-Sum Decoders. (arXiv:2205.10684v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35299;&#30721;&#32447;&#24615;&#20998;&#32452;&#30721;&#20013;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#21040;BP&#35299;&#30721;&#22120;&#20013;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36825;&#20123;&#21442;&#25968;&#21463;&#21040;&#20195;&#30721;&#30701;&#21608;&#26399;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#23398;&#20064;&#26435;&#37325;&#30340;&#35299;&#30721;&#22120;&#27604;&#37027;&#20123;&#32463;&#36807;&#35299;&#26512;&#20248;&#21270;&#30340;&#26435;&#37325;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#22797;&#26434;&#20449;&#36947;&#19978;&#26377;&#26174;&#33879;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24615;&#20998;&#32452;&#30721;&#30340;&#35299;&#30721;&#20013;&#65292;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#21040;&#32622;&#20449;&#20256;&#25773;&#65288;BP&#65289;&#35299;&#30721;&#22120;&#20013;&#21487;&#20197;&#23454;&#29616;&#26126;&#26174;&#30340;&#21487;&#38752;&#24615;&#22686;&#30410;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#26377;&#20004;&#20010;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#26159;&#23545;&#23398;&#20064;&#26435;&#37325;&#32570;&#20047;&#35299;&#37322;&#65292;&#21478;&#19968;&#20010;&#26159;&#23545;&#38750;AWGN&#20449;&#36947;&#32570;&#20047;&#20998;&#26512;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#23545;&#23398;&#20064;&#26435;&#37325;&#21450;&#20854;&#19982;&#22522;&#30784;&#30721;&#30340;&#32467;&#26500;&#30340;&#20851;&#31995;&#30340;&#35265;&#35299;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26435;&#37325;&#21463;&#21040;&#20195;&#30721;&#30701;&#21608;&#26399;&#20998;&#24067;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#35299;&#30721;&#22120;&#22312;&#38750;AWGN&#20449;&#36947;&#19978;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#21512;&#25104;&#20449;&#36947;&#21644;&#31354;&#20013;&#20449;&#36947;&#65292;&#24182;&#30740;&#31350;&#20102;&#22797;&#26434;&#24615;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#35777;&#26126;&#22686;&#21152;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#22312;&#22797;&#26434;&#20449;&#36947;&#19978;&#26377;&#26174;&#33879;&#30340;&#24110;&#21161;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#23398;&#20064;&#26435;&#37325;&#30340;&#35299;&#30721;&#22120;&#27604;&#37027;&#20123;&#32463;&#36807;&#35299;&#26512;&#20248;&#21270;&#30340;&#26435;&#37325;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In decoding linear block codes, it was shown that noticeable reliability gains can be achieved by introducing learnable parameters to the Belief Propagation (BP) decoder. Despite the success of these methods, there are two key open problems. The first is the lack of interpretation of the learned weights, and the other is the lack of analysis for non-AWGN channels. In this work, we aim to bridge this gap by providing insights into the weights learned and their connection to the structure of the underlying code. We show that the weights are heavily influenced by the distribution of short cycles in the code. We next look at the performance of these decoders in non-AWGN channels, both synthetic and over-the-air channels, and study the complexity vs. performance trade-offs, demonstrating that increasing the number of parameters helps significantly in complex channels. Finally, we show that the decoders with learned weights achieve higher reliability than those with weights optimized analyti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20845;&#31181;AutoML&#26694;&#26550;&#22312;100&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#21457;&#29616;&#19981;&#21516;&#26694;&#26550;&#22312;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#21508;&#26377;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#26694;&#26550;&#30340;&#36873;&#25321;&#24212;&#26681;&#25454;&#29305;&#23450;&#20219;&#21153;&#35201;&#27714;&#26469;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2204.08358</link><description>&lt;p&gt;
AutoMLBench&#65306;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#20840;&#38754;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AutoMLBench: A Comprehensive Experimental Evaluation of Automated Machine Learning Frameworks. (arXiv:2204.08358v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20845;&#31181;AutoML&#26694;&#26550;&#22312;100&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#21457;&#29616;&#19981;&#21516;&#26694;&#26550;&#22312;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#21508;&#26377;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#26694;&#26550;&#30340;&#36873;&#25321;&#24212;&#26681;&#25454;&#29305;&#23450;&#20219;&#21153;&#35201;&#27714;&#26469;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#38656;&#27714;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#24050;&#32463;&#24847;&#35782;&#21040;&#26377;&#36275;&#22815;&#30693;&#35782;&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#25968;&#37327;&#26080;&#27861;&#36319;&#38543;&#25968;&#23383;&#19990;&#30028;&#20013;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#37327;&#21644;&#24212;&#29992;&#38656;&#27714;&#32780;&#25193;&#23637;&#12290;&#20026;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20960;&#31181;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26469;&#22635;&#34917;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#24046;&#36317;&#12290;&#27599;&#20010;&#26694;&#26550;&#37117;&#24102;&#26377;&#19981;&#21516;&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20845;&#31181;&#27969;&#34892;&#30340;AutoML&#26694;&#26550;&#65288;AutoWeka&#12289;AutoSKlearn&#12289;TPOT&#12289;Recipe&#12289;ATM&#21644;SmartML&#65289;&#22312;&#26469;&#33258;&#24050;&#24314;&#31435;&#30340;AutoML&#22522;&#20934;&#22871;&#20214;&#30340;100&#20010;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#32771;&#34385;&#20102;&#19981;&#21516;&#26041;&#38754;&#30340;&#27604;&#36739;&#65292;&#21253;&#25324;&#22810;&#20010;&#35774;&#35745;&#20915;&#31574;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#22914;&#26102;&#38388;&#39044;&#31639;&#12289;&#25628;&#32034;&#31354;&#38388;&#30340;&#22823;&#23567;&#12289;&#20803;&#23398;&#20064;&#21644;&#38598;&#25104;&#26500;&#24314;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#27599;&#20010;AutoML&#26694;&#26550;&#22312;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#37117;&#26377;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#26694;&#26550;&#30340;&#36873;&#25321;&#21487;&#33021;&#21462;&#20915;&#20110;&#25163;&#22836;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the booming demand for machine learning applications, it has been recognized that the number of knowledgeable data scientists can not scale with the growing data volumes and application needs in our digital world. In response to this demand, several automated machine learning (AutoML) frameworks have been developed to fill the gap of human expertise by automating the process of building machine learning pipelines. Each framework comes with different heuristics-based design decisions. In this study, we present a comprehensive evaluation and comparison of the performance characteristics of six popular AutoML frameworks, namely, AutoWeka, AutoSKlearn, TPOT, Recipe, ATM, and SmartML, across 100 data sets from established AutoML benchmark suites. Our experimental evaluation considers different aspects for its comparison, including the performance impact of several design decisions, including time budget, size of search space, meta-learning, and ensemble construction. The results of our
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#20449;&#26465;&#27169;&#22411;&#65292;&#21487;&#20197;&#20419;&#20351;&#26234;&#33021;&#20307;&#20026;&#23427;&#25152;&#23646;&#30340;&#22242;&#38431;&#25171;&#36896;&#20248;&#21270;&#34892;&#20026;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#38431;&#21451;&#25110;&#25972;&#20010;&#31995;&#32479;&#30340;&#21033;&#30410;&#19981;&#19968;&#23450;&#23436;&#20840;&#19968;&#33268;&#65292;&#20063;&#33021;&#23454;&#29616;&#20840;&#23616;&#26377;&#30410;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.07471</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#20449;&#26465;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Credo in Multiagent Learning. (arXiv:2204.07471v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#20449;&#26465;&#27169;&#22411;&#65292;&#21487;&#20197;&#20419;&#20351;&#26234;&#33021;&#20307;&#20026;&#23427;&#25152;&#23646;&#30340;&#22242;&#38431;&#25171;&#36896;&#20248;&#21270;&#34892;&#20026;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#38431;&#21451;&#25110;&#25972;&#20010;&#31995;&#32479;&#30340;&#21033;&#30410;&#19981;&#19968;&#23450;&#23436;&#20840;&#19968;&#33268;&#65292;&#20063;&#33021;&#23454;&#29616;&#20840;&#23616;&#26377;&#30410;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#27169;&#22411;&#65292;&#20063;&#23601;&#26159;&#21483;&#20570;&#20449;&#26465;&#65292;&#29992;&#20110;&#31995;&#32479;&#20013;&#37197;&#32622;&#25104;&#22810;&#20010;&#23567;&#32452;&#65288;&#21363;&#22242;&#38431;&#65289;&#30340;&#26234;&#33021;&#20307;&#12290;&#20449;&#26465;&#27169;&#22411;&#35268;&#33539;&#20102;&#26234;&#33021;&#20307;&#20026;&#23427;&#25152;&#23646;&#30340;&#22242;&#38431;&#20248;&#21270;&#20854;&#34892;&#20026;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#25361;&#25112;&#24615;&#30340;&#31038;&#20250;&#22256;&#22659;&#20013;&#35780;&#20272;&#20449;&#26465;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#38431;&#21451;&#25110;&#25972;&#20010;&#31995;&#32479;&#30340;&#21033;&#30410;&#19981;&#19968;&#23450;&#23436;&#20840;&#19968;&#33268;&#65292;&#20063;&#33021;&#23454;&#29616;&#20840;&#23616;&#26377;&#30410;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#31181;&#27809;&#26377;&#23436;&#20840;&#20849;&#21516;&#21033;&#30410;&#30340;&#22330;&#26223;&#65292;&#36825;&#20123;&#22330;&#26223;&#23454;&#29616;&#20102;&#39640;&#24179;&#31561;&#21644;&#26174;&#33879;&#39640;&#20110;&#25152;&#26377;&#26234;&#33021;&#20307;&#21033;&#30410;&#19968;&#33268;&#30340;&#24179;&#22343;&#20154;&#21475;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model for multi-objective optimization, a credo, for agents in a system that are configured into multiple groups (i.e., teams). Our model of credo regulates how agents optimize their behavior for the groups they belong to. We evaluate credo in the context of challenging social dilemmas with reinforcement learning agents. Our results indicate that the interests of teammates, or the entire system, are not required to be fully aligned for achieving globally beneficial outcomes. We identify two scenarios without full common interest that achieve high equality and significantly higher mean population rewards compared to when the interests of all agents are aligned.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35757;&#32451;&#35823;&#24046;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#26512;&#20844;&#24335;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#23485;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22343;&#34892;&#20026;&#12290;&#25105;&#20204;&#39044;&#27979;&#24182;&#34920;&#24449;&#20102;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#27531;&#20313;&#35757;&#32451;&#35823;&#24046;&#20316;&#20026;&#31995;&#32479;&#21442;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#12290;</title><link>http://arxiv.org/abs/2203.16711</link><description>&lt;p&gt;
&#23485;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#20998;&#26512;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Analytic theory for the dynamics of wide quantum neural networks. (arXiv:2203.16711v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35757;&#32451;&#35823;&#24046;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#26512;&#20844;&#24335;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#23485;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22343;&#34892;&#20026;&#12290;&#25105;&#20204;&#39044;&#27979;&#24182;&#34920;&#24449;&#20102;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#27531;&#20313;&#35757;&#32451;&#35823;&#24046;&#20316;&#20026;&#31995;&#32479;&#21442;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#37327;&#23376;&#30005;&#36335;&#21487;&#29992;&#20316;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26377;&#28508;&#21147;&#22312;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#26102;&#20248;&#20110;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#20851;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#38382;&#39064;&#19978;&#34920;&#29616;&#30340;&#32467;&#26524;&#26159;&#21551;&#21457;&#24335;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25910;&#25947;&#29575;&#36824;&#27809;&#26377;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#24577;&#65292;&#30740;&#31350;&#19968;&#31867;&#21487;&#21464;&#37327;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#35823;&#24046;&#12290;&#25105;&#20204;&#23558;&#23485;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20026;&#24102;&#26377;&#22823;&#37327;&#37327;&#23376;&#20301;&#21644;&#21487;&#21464;&#21442;&#25968;&#30340;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#26497;&#38480;&#12290;&#25105;&#20204;&#28982;&#21518;&#21457;&#29616;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#26512;&#20844;&#24335;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#23427;&#20204;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22343;&#34892;&#20026;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#30740;&#31350;&#30340;&#32467;&#26524;&#30340;&#21518;&#26524;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#65292;&#25105;&#20204;&#39044;&#27979;&#24182;&#34920;&#24449;&#20102;&#27531;&#20313;&#35757;&#32451;&#35823;&#24046;&#20316;&#20026;&#31995;&#32479;&#21442;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#12290;&#25105;&#20204;&#26368;&#32456;&#36890;&#36807;&#21508;&#31181;&#37327;&#23376;&#30005;&#36335;&#30340;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#25105;&#20204;&#30340;&#39044;&#27979;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterized quantum circuits can be used as quantum neural networks and have the potential to outperform their classical counterparts when trained for addressing learning problems. To date, much of the results on their performance on practical problems are heuristic in nature. In particular, the convergence rate for the training of quantum neural networks is not fully understood. Here, we analyze the dynamics of gradient descent for the training error of a class of variational quantum machine learning models. We define wide quantum neural networks as parameterized quantum circuits in the limit of a large number of qubits and variational parameters. We then find a simple analytic formula that captures the average behavior of their loss function and discuss the consequences of our findings. For example, for random quantum circuits, we predict and characterize an exponential decay of the residual training error as a function of the parameters of the system. We finally validate our analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#24182;&#21457;&#38543;&#26426;&#21338;&#24328;&#65288;NS-CSG&#65289;&#30340;&#24314;&#27169;&#24418;&#24335;&#65292;&#24182;&#38024;&#23545;&#20855;&#26377; Borel &#29366;&#24577;&#31354;&#38388;&#30340; NS-CSG &#31867;&#23637;&#31034;&#20102;&#38646;&#21644;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#20215;&#20540;&#20989;&#25968;&#30340;&#23384;&#22312;&#24615;&#21644;&#21487;&#27979;&#24615;&#12290;&#39318;&#27425;&#25552;&#20986;&#20102;&#21487;&#23454;&#26045;&#30340;&#20540;&#36845;&#20195;&#65288;VI&#65289;&#21644;&#31574;&#30053;&#36845;&#20195;&#65288;PI&#65289;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#22312;&#22522;&#20934; NS-CSG &#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.06255</link><description>&lt;p&gt;
&#38646;&#21644;&#31070;&#32463;&#31526;&#21495;&#24182;&#21457;&#38543;&#26426;&#21338;&#24328;&#30340;&#31574;&#30053;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Strategy Synthesis for Zero-Sum Neuro-Symbolic Concurrent Stochastic Games. (arXiv:2202.06255v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#24182;&#21457;&#38543;&#26426;&#21338;&#24328;&#65288;NS-CSG&#65289;&#30340;&#24314;&#27169;&#24418;&#24335;&#65292;&#24182;&#38024;&#23545;&#20855;&#26377; Borel &#29366;&#24577;&#31354;&#38388;&#30340; NS-CSG &#31867;&#23637;&#31034;&#20102;&#38646;&#21644;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#20215;&#20540;&#20989;&#25968;&#30340;&#23384;&#22312;&#24615;&#21644;&#21487;&#27979;&#24615;&#12290;&#39318;&#27425;&#25552;&#20986;&#20102;&#21487;&#23454;&#26045;&#30340;&#20540;&#36845;&#20195;&#65288;VI&#65289;&#21644;&#31574;&#30053;&#36845;&#20195;&#65288;PI&#65289;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#22312;&#22522;&#20934; NS-CSG &#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#32463;&#20856;&#31526;&#21495;&#25216;&#26415;&#30340;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#38656;&#35201;&#27491;&#24335;&#30340;&#26041;&#27861;&#26469;&#25512;&#29702;&#20854;&#27491;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24314;&#27169;&#24418;&#24335;&#65292;&#31216;&#20026;&#31070;&#32463;&#31526;&#21495;&#24182;&#21457;&#38543;&#26426;&#21338;&#24328;&#65288;NS-CSG&#65289;&#65292;&#30001;&#22312;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#24863;&#30693;&#26426;&#21046;&#35266;&#23519;&#21040;&#30340;&#20849;&#20139;&#36830;&#32493;&#29366;&#24577;&#29615;&#22659;&#20013;&#30456;&#20114;&#20316;&#29992;&#30340;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#20195;&#29702;&#32452;&#25104;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20855;&#26377; Borel &#29366;&#24577;&#31354;&#38388;&#30340; NS-CSG &#31867;&#65292;&#35777;&#26126;&#20102;&#22312;&#23545;&#36825;&#31867;&#27169;&#22411;&#30340;&#32452;&#20214;&#26045;&#21152;&#20998;&#27573;&#24120;&#25968;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#38646;&#21644;&#25240;&#25187;&#32047;&#31215;&#22238;&#25253;&#30340;&#20215;&#20540;&#20989;&#25968;&#30340;&#23384;&#22312;&#24615;&#21644;&#21487;&#27979;&#24615;&#12290;&#20026;&#20102;&#35745;&#31639;&#20540;&#21644;&#21512;&#25104;&#31574;&#30053;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#21487;&#23454;&#26045;&#30340;&#20540;&#36845;&#20195;&#65288;VI&#65289;&#21644;&#31574;&#30053;&#36845;&#20195;&#65288;PI&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#31867;&#36830;&#32493;&#29366;&#24577; CSG&#38382;&#39064;&#12290;&#36825;&#20123;&#38656;&#35201;&#23545;&#21160;&#21147;&#23398;&#19979;&#30340;&#29615;&#22659;&#30340;&#21069;&#20687;&#36827;&#34892;&#26377;&#38480;&#34920;&#31034;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#24230;&#35770;&#30340;&#26631;&#20934;&#25216;&#26415;&#26469;&#26500;&#36896;&#36825;&#20010;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;NS-CSG&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic approaches to artificial intelligence, which combine neural networks with classical symbolic techniques, are growing in prominence, necessitating formal approaches to reason about their correctness. We propose a novel modelling formalism called neuro-symbolic concurrent stochastic games (NS-CSGs), which comprise probabilistic finite-state agents interacting in a shared continuous-state environment observed through perception mechanisms implemented as neural networks (NNs). We focus on the class of NS-CSGs with Borel state spaces and prove the existence and measurability of the value function for zero-sum discounted cumulative rewards under piecewise-constant restrictions on the components of this class of models. To compute values and synthesise strategies, we present, for the first time, implementable value iteration (VI) and policy iteration (PI) algorithms to solve a class of continuous-state CSGs. These require a finite representation of the pre-image of the environm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#19987;&#23478;&#24341;&#23548;&#30340;&#23545;&#31216;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#38024;&#23545;&#38750;&#30830;&#23450;&#24615;MDP&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26816;&#27979;&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#32452;&#22522;&#20934;&#20219;&#21153;&#19978;&#26126;&#26174;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.09943</link><description>&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#24341;&#23548;&#30340;&#23545;&#31216;&#26816;&#27979;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#20197;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation through Expert-guided Symmetry Detection to Improve Performance in Offline Reinforcement Learning. (arXiv:2112.09943v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#19987;&#23478;&#24341;&#23548;&#30340;&#23545;&#31216;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#38024;&#23545;&#38750;&#30830;&#23450;&#24615;MDP&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26816;&#27979;&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#32452;&#22522;&#20934;&#20219;&#21153;&#19978;&#26126;&#26174;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#31163;&#32447;&#21160;&#24577;&#27169;&#22411;&#20272;&#35745;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23398;&#20064;&#38454;&#27573;&#21487;&#29992;&#30340;&#25968;&#25454;&#12290;&#26377;&#26102;&#65292;&#27169;&#22411;&#30340;&#21160;&#24577;&#29305;&#24615;&#19982;&#24403;&#21069;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#26576;&#20123;&#21464;&#25442;&#26159;&#19981;&#21464;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#30340;&#19987;&#23478;&#24341;&#23548;&#27969;&#27700;&#32447;&#65292;&#22914;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24402;&#19968;&#21270;&#27969;&#65292;&#21487;&#26377;&#25928;&#26816;&#27979;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#30340;&#36825;&#31181;&#32467;&#26500;&#65292;&#21253;&#25324;&#31867;&#21035;&#21644;&#36830;&#32493;&#20540;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#22686;&#24378;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#26368;&#32456;&#23548;&#33268;&#30495;&#23454;&#27169;&#22411;&#21644;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#20943;&#23569;&#12290;&#36825;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21021;&#27493;&#36807;&#31243;&#65292;&#22312;&#37319;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#20043;&#21069;&#25191;&#34892;&#65292;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#33539;&#20363;&#25193;&#23637;&#21040;&#35299;&#20915;&#38750;&#30830;&#23450;&#24615;MDP&#65292;&#29305;&#21035;&#26159;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26816;&#27979;&#31639;&#27861;&#26469;&#35782;&#21035;&#21644;&#21033;&#29992;&#23545;&#31216;&#24615;&#65292;2&#65289;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#32452;&#22522;&#20934;&#20219;&#21153;&#19978;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline estimation of the dynamical model of a Markov Decision Process (MDP) is a non-trivial task that greatly depends on the data available in the learning phase. Sometimes the dynamics of the model is invariant with respect to some transformations of the current state and action. Recent works showed that an expert-guided pipeline relying on Density Estimation methods as Deep Neural Network based Normalizing Flows effectively detects this structure in deterministic environments, both categorical and continuous-valued. The acquired knowledge can be exploited to augment the original data set, leading eventually to a reduction in the distributional shift between the true and the learned model. Such data augmentation technique can be exploited as a preliminary process to be executed before adopting an Offline Reinforcement Learning architecture, increasing its performance. In this work we extend the paradigm to also tackle non-deterministic MDPs, in particular, 1) we propose a detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35780;&#35770;&#25991;&#26412;&#26469;&#36827;&#34892;&#39046;&#22495;&#35299;&#32544;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19977;&#20010;&#25991;&#26412;&#20998;&#26512;&#27169;&#22359;&#65292;&#30001;&#21333;&#19968;&#39046;&#22495;&#21028;&#21035;&#22120;&#25351;&#24341;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#39046;&#22495;&#35299;&#32544;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#32534;&#30721;&#32593;&#32476;&#20174;&#21333;&#20010;&#39046;&#22495;&#21040;&#22810;&#20010;&#39046;&#22495;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12289;&#31283;&#20581;&#21644;&#21487;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2110.12648</link><description>&lt;p&gt;
&#26080;&#37325;&#22797;&#29992;&#25143;&#25110;&#19978;&#19979;&#25991;&#30340;&#22522;&#20110;&#35780;&#35770;&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#30340;&#39046;&#22495;&#35299;&#32544;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Review-Based Domain Disentanglement without Duplicate Users or Contexts for Cross-Domain Recommendation. (arXiv:2110.12648v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35780;&#35770;&#25991;&#26412;&#26469;&#36827;&#34892;&#39046;&#22495;&#35299;&#32544;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19977;&#20010;&#25991;&#26412;&#20998;&#26512;&#27169;&#22359;&#65292;&#30001;&#21333;&#19968;&#39046;&#22495;&#21028;&#21035;&#22120;&#25351;&#24341;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#39046;&#22495;&#35299;&#32544;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#32534;&#30721;&#32593;&#32476;&#20174;&#21333;&#20010;&#39046;&#22495;&#21040;&#22810;&#20010;&#39046;&#22495;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12289;&#31283;&#20581;&#21644;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#25512;&#33616;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#19987;&#27880;&#20110;&#39046;&#22495;&#21487;&#20849;&#20139;&#20449;&#24687;&#65288;&#37325;&#21472;&#29992;&#25143;&#25110;&#30456;&#21516;&#30340;&#19978;&#19979;&#25991;&#65289;&#29992;&#20110;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#19988;&#27809;&#26377;&#36825;&#26679;&#30340;&#35201;&#27714;&#23601;&#24456;&#38590;&#36827;&#34892;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#23545;&#22823;&#22810;&#25968;&#30005;&#23376;&#21830;&#21153;&#31995;&#32479;&#36890;&#29992;&#30340;&#35780;&#35770;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;&#21517;&#20026;SER&#65289;&#20351;&#29992;&#19977;&#20010;&#25991;&#26412;&#20998;&#26512;&#27169;&#22359;&#65292;&#30001;&#21333;&#19968;&#39046;&#22495;&#21028;&#21035;&#22120;&#25351;&#23548;&#36827;&#34892;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#35299;&#32544;&#30340;&#36136;&#37327;&#65292;&#24182;&#21066;&#24369;&#28304;&#39046;&#22495;&#30340;&#19981;&#33391;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32534;&#30721;&#32593;&#32476;&#20174;&#21333;&#20010;&#39046;&#22495;&#25193;&#23637;&#21040;&#22810;&#20010;&#39046;&#22495;&#65292;&#36825;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#22522;&#20110;&#35780;&#35770;&#30340;&#25512;&#33616;&#31995;&#32479;&#38750;&#24120;&#24378;&#22823;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
A cross-domain recommendation has shown promising results in solving data-sparsity and cold-start problems. Despite such progress, existing methods focus on domain-shareable information (overlapped users or same contexts) for a knowledge transfer, and they fail to generalize well without such requirements. To deal with these problems, we suggest utilizing review texts that are general to most e-commerce systems. Our model (named SER) uses three text analysis modules, guided by a single domain discriminator for disentangled representation learning. Here, we suggest a novel optimization strategy that can enhance the quality of domain disentanglement, and also debilitates detrimental information of a source domain. Also, we extend the encoding network from a single to multiple domains, which has proven to be powerful for review-based recommender systems. Extensive experiments and ablation studies demonstrate that our method is efficient, robust, and scalable compared to the state-of-the-a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#29366;&#24577;&#35775;&#38382;&#27169;&#24335;&#23545;&#29366;&#24577;&#36827;&#34892;&#32534;&#30721;&#30340;&#32487;&#25215;&#34920;&#31034;&#65288;SR&#65289;&#21487;&#20197;&#34987;&#30475;&#20316;&#21457;&#29616;&#21644;&#20351;&#29992;&#26102;&#38388;&#25277;&#35937;&#30340;&#33258;&#28982;&#22522;&#36136;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#26377;&#21161;&#20110;&#36827;&#34892;&#26242;&#26102;&#25193;&#23637;&#25506;&#32034;&#25110;&#35268;&#21010;&#30340;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2110.05740</link><description>&lt;p&gt;
&#22522;&#20110;&#32487;&#25215;&#34920;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Temporal Abstraction in Reinforcement Learning with the Successor Representation. (arXiv:2110.05740v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#29366;&#24577;&#35775;&#38382;&#27169;&#24335;&#23545;&#29366;&#24577;&#36827;&#34892;&#32534;&#30721;&#30340;&#32487;&#25215;&#34920;&#31034;&#65288;SR&#65289;&#21487;&#20197;&#34987;&#30475;&#20316;&#21457;&#29616;&#21644;&#20351;&#29992;&#26102;&#38388;&#25277;&#35937;&#30340;&#33258;&#28982;&#22522;&#36136;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#26377;&#21161;&#20110;&#36827;&#34892;&#26242;&#26102;&#25193;&#23637;&#25506;&#32034;&#25110;&#35268;&#21010;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#27425;&#30340;&#26102;&#38388;&#25277;&#35937;&#26159;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#36890;&#24120;&#36890;&#36807;&#34987;&#31216;&#20026;&#36873;&#39033;&#30340;&#26242;&#26102;&#24615;&#34892;&#21160;&#26469;&#24314;&#27169;&#12290;&#36873;&#39033;&#20801;&#35768;&#26234;&#33021;&#20307;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#39044;&#27979;&#24182;&#22312;&#19981;&#21516;&#30340;&#25277;&#35937;&#23618;&#27425;&#19978;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36873;&#39033;&#26694;&#26550;&#30340;&#26041;&#27861;&#36890;&#24120;&#20174;&#24050;&#30693;&#30340;&#21512;&#29702;&#36873;&#39033;&#38598;&#24320;&#22987;&#20551;&#35774;&#12290;&#24403;&#27809;&#26377;&#36825;&#31181;&#24773;&#20917;&#26102;&#65292;&#23545;&#20110;&#24212;&#35813;&#32771;&#34385;&#21738;&#20123;&#36873;&#39033;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22522;&#20110;&#29366;&#24577;&#35775;&#38382;&#27169;&#24335;&#23545;&#29366;&#24577;&#36827;&#34892;&#32534;&#30721;&#30340;&#32487;&#25215;&#34920;&#31034;&#65288;SR&#65289;&#21487;&#20197;&#34987;&#35270;&#20026;&#21457;&#29616;&#21644;&#20351;&#29992;&#26102;&#38388;&#25277;&#35937;&#30340;&#33258;&#28982;&#22522;&#36136;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#20174;&#36817;&#26399;&#30740;&#31350;&#30340;&#20840;&#23616;&#35270;&#35282;&#20986;&#21457;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;SR&#21457;&#29616;&#26377;&#21161;&#20110;&#36827;&#34892;&#26242;&#26102;&#25193;&#23637;&#25506;&#32034;&#25110;&#35268;&#21010;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#32467;&#21512;&#21040;&#19968;&#20010;&#23436;&#25972;&#30340;&#23454;&#39564;&#20013;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation (SR), which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the SR can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these re
&lt;/p&gt;</description></item></channel></rss>