<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25511;&#21046;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#39044;&#27979;&#22522;&#20934;&#65292;&#35780;&#20272;&#32473;&#23450;&#27169;&#22411;&#22312;&#36890;&#36807;&#37319;&#26679;&#35268;&#21010;&#23545;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;&#26377;11&#20010;&#20219;&#21153;&#31867;&#21035;&#21644;310&#20010;&#20219;&#21153;&#23454;&#20363;&#23450;&#20041;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#21450;&#23436;&#25972;&#30340;&#35268;&#21010;&#23454;&#29616;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25351;&#26631;&#22312;&#39044;&#27979;&#20219;&#21153;&#25191;&#34892;&#25104;&#21151;&#26041;&#38754;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13723</link><description>&lt;p&gt;
&#19968;&#31181;&#20197;&#25511;&#21046;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#39044;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Control-Centric Benchmark for Video Prediction. (arXiv:2304.13723v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25511;&#21046;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#39044;&#27979;&#22522;&#20934;&#65292;&#35780;&#20272;&#32473;&#23450;&#27169;&#22411;&#22312;&#36890;&#36807;&#37319;&#26679;&#35268;&#21010;&#23545;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;&#26377;11&#20010;&#20219;&#21153;&#31867;&#21035;&#21644;310&#20010;&#20219;&#21153;&#23454;&#20363;&#23450;&#20041;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#21450;&#23436;&#25972;&#30340;&#35268;&#21010;&#23454;&#29616;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25351;&#26631;&#22312;&#39044;&#27979;&#20219;&#21153;&#25191;&#34892;&#25104;&#21151;&#26041;&#38754;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26159;&#23398;&#20064;&#19990;&#30028;&#21160;&#24577;&#27169;&#22411;&#30340;&#20307;&#29616;&#20195;&#29702;&#20154;&#30340;&#26377;&#24076;&#26395;&#30340;&#30693;&#35782;&#26469;&#28304;&#12290;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#22312;&#33258;&#25105;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36234;&#26469;&#36234;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#35270;&#39057;&#25968;&#25454;&#65292;&#35780;&#20272;&#22522;&#20110;&#20154;&#31867;&#24863;&#30693;&#30456;&#20284;&#24615;&#25110;&#20687;&#32032;&#27604;&#36739;&#30340;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25351;&#26631;&#26159;&#21542;&#20934;&#30830;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#20110;&#35268;&#21010;&#26426;&#22120;&#20154;&#25805;&#20316;&#26469;&#35828;&#65292;&#29616;&#26377;&#25351;&#26631;&#22312;&#39044;&#27979;&#20219;&#21153;&#25191;&#34892;&#25104;&#21151;&#26041;&#38754;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#21160;&#26465;&#20214;&#19979;&#30340;&#35270;&#39057;&#39044;&#27979;&#22522;&#20934;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#35268;&#21010;&#23545;&#32473;&#23450;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#25511;&#21046;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35270;&#35273;&#35268;&#21010;&#30340;&#35270;&#39057;&#39044;&#27979; ($VP^2$)&#65292;&#21253;&#25324;11&#20010;&#20219;&#21153;&#31867;&#21035;&#21644;310&#20010;&#20219;&#21153;&#23454;&#20363;&#23450;&#20041;&#30340;&#27169;&#25311;&#29615;&#22659;&#12289;&#23436;&#25972;&#30340;&#35268;&#21010;&#23454;&#29616;&#21644;&#21253;&#21547;&#33050;&#26412;&#20132;&#20114;&#36712;&#36857;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video is a promising source of knowledge for embodied agents to learn models of the world's dynamics. Large deep networks have become increasingly effective at modeling complex video data in a self-supervised manner, as evaluated by metrics based on human perceptual similarity or pixel-wise comparison. However, it remains unclear whether current metrics are accurate indicators of performance on downstream tasks. We find empirically that for planning robotic manipulation, existing metrics can be unreliable at predicting execution success. To address this, we propose a benchmark for action-conditioned video prediction in the form of a control benchmark that evaluates a given model for simulated robotic manipulation through sampling-based planning. Our benchmark, Video Prediction for Visual Planning ($VP^2$), includes simulated environments with 11 task categories and 310 task instance definitions, a full planning implementation, and training datasets containing scripted interaction traje
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13714</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#30340;&#23454;&#38469;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#24403;&#21069;&#30340;&#25506;&#32034;&#24182;&#26410;&#35780;&#20272;LLMs&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;LLM&#26159;&#21542;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#30001;&#21307;&#29983;&#25552;&#20132;&#30340;&#20449;&#24687;&#38656;&#27714;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;66&#20010;&#26469;&#33258;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#38382;&#39064;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#25552;&#20132;&#32473;GPT-3.5&#21644;GPT-4&#12290;12&#21517;&#21307;&#29983;&#35780;&#20272;&#20102;LLM&#21709;&#24212;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#29616;&#26377;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#21307;&#29983;&#30340;&#35780;&#20272;&#22522;&#20110;&#22810;&#25968;&#31080;&#27719;&#24635;&#12290;&#23545;&#20110;&#27809;&#26377;&#20219;&#20309;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#21307;&#29983;&#35748;&#20026;&#20219;&#20309;&#19968;&#20010;LLM&#21709;&#24212;&#37117;&#19981;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#23545;&#20110;GPT-3.5&#65292;8&#20010;&#38382;&#39064;&#30340;&#21709;&#24212;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#25253;&#21578;&#19968;&#33268;&#65292;20&#20010;&#19981;&#19968;&#33268;&#65292;9&#20010;&#26080;&#27861;&#35780;&#20272;&#12290;&#26377;29&#20010;&#21709;&#24212;&#27809;&#26377;&#22810;&#25968;&#31080;&#34920;&#31034;&#8220;&#21516;&#24847;&#8221;&#12289;&#8220;&#19981;&#21516;&#24847;&#8221;&#21644;&#8220;&#26080;&#27861;&#35780;&#20272;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.13712</link><description>&lt;p&gt;
&#21457;&#25381;LLMs&#22312;&#23454;&#36341;&#20013;&#30340;&#21147;&#37327;&#65306;ChatGPT&#21450;&#20854;&#24212;&#29992;&#30340;&#32508;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;LLMs&#30340;&#20351;&#29992;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20174;&#20107;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#23454;&#29992;&#30340;&#25351;&#21335;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;Large Language Models&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;LLMs&#30340;&#20351;&#29992;&#35752;&#35770;&#21644;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24403;&#21069;&#30340;GPT&#21644;BERT&#26679;&#24335;&#30340;LLMs&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20363;&#22914;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12289;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12289;&#32039;&#24613;&#33021;&#21147;&#20197;&#21450;&#29305;&#23450;&#20219;&#21153;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#21508;&#31181;&#20351;&#29992;&#21644;&#38750;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#35828;&#26126;LLMs&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#23454;&#38469;&#24212;&#29992;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#35797;&#22270;&#20102;&#35299;&#25968;&#25454;&#23545;&#20110;LLMs&#24212;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of dat
&lt;/p&gt;</description></item><item><title>HeySQuAD &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#24182;&#22238;&#31572;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#36827;&#34892;&#35757;&#32451;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13689</link><description>&lt;p&gt;
HeySQuAD: &#19968;&#20010;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HeySQuAD: A Spoken Question Answering Dataset. (arXiv:2304.13689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13689
&lt;/p&gt;
&lt;p&gt;
HeySQuAD &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#24182;&#22238;&#31572;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#36827;&#34892;&#35757;&#32451;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#23545;&#20110;&#35780;&#20272;&#21475;&#35821;&#38382;&#31572;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#25968;&#23383;&#21161;&#25163;&#31561;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#31038;&#21306;&#20849;&#20139;&#30340;&#21475;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598; HeySQuAD&#65292;&#23427;&#30001;76k&#20010;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#12289;97k&#20010;&#26426;&#22120;&#29983;&#25104;&#30340;&#38382;&#39064;&#20197;&#21450;&#30456;&#24212;&#30340;&#25991;&#26412;&#31572;&#26696;&#32452;&#25104;&#65292;&#36825;&#20123;&#31572;&#26696;&#28304;&#33258; SQuAD QA &#25968;&#25454;&#38598;&#12290;HeySQuAD &#30340;&#30446;&#26631;&#26159;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#24182;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#20154;&#31867;&#21475;&#35821;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#37327;&#21270;&#26469;&#33258;&#20004;&#26041;&#38754;&#22122;&#22768;&#30340;&#24046;&#24322;&#21450;&#23545;&#27169;&#22411;&#21644;&#22238;&#31572;&#20934;&#30830;&#24230;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#21475;&#35821;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#22238;&#31572;&#30340;&#26159;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#21644;&#21407;&#22987; SQuAD &#38382;&#39064;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#65288;12.51%&#65289;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#26159;&#20165;&#20351;&#29992;&#21407;&#22987; SQuAD &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-spoken questions are critical to evaluating the performance of spoken question answering (SQA) systems that serve several real-world use cases including digital assistants. We present a new large-scale community-shared SQA dataset, HeySQuAD that consists of 76k human-spoken questions and 97k machine-generated questions and corresponding textual answers derived from the SQuAD QA dataset. The goal of HeySQuAD is to measure the ability of machines to understand noisy spoken questions and answer the questions accurately. To this end, we run extensive benchmarks on the human-spoken and machine-generated questions to quantify the differences in noise from both sources and its subsequent impact on the model and answering accuracy. Importantly, for the task of SQA, where we want to answer human-spoken questions, we observe that training using the transcribed human-spoken and original SQuAD questions leads to significant improvements (12.51%) over training using only the original SQuAD te
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#23396;&#23707;&#20013;&#21019;&#24314;AI&#27169;&#22411;&#65292;&#25361;&#25112;&#22312;&#20110;&#24314;&#31435;&#22810;&#26041;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#21270;&#20102;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#21644;&#19994;&#21153;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2304.13688</link><description>&lt;p&gt;
&#21512;&#20316;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#21147;&#37322;&#25918;&#65306;&#20851;&#20110;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Collaborative AI -- On the Socio-technical Challenges of Federated Machine Learning. (arXiv:2304.13688v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13688
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#23396;&#23707;&#20013;&#21019;&#24314;AI&#27169;&#22411;&#65292;&#25361;&#25112;&#22312;&#20110;&#24314;&#31435;&#22810;&#26041;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#21270;&#20102;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#21644;&#19994;&#21153;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#30340;&#39072;&#35206;&#24615;&#28508;&#21147;&#28304;&#20110;&#22823;&#25968;&#25454;&#30340;&#20986;&#29616;&#65292;&#20294;&#26159;&#24456;&#22823;&#19968;&#37096;&#20998;&#25968;&#25454;&#20998;&#25955;&#22312;&#25968;&#25454;&#23396;&#23707;&#20013;&#65292;&#20854;&#28508;&#21147;&#26410;&#33021;&#24471;&#21040;&#37322;&#25918;&#12290;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#21487;&#20197;&#20174;&#20998;&#25955;&#30340;&#12289;&#28508;&#22312;&#30340;&#25968;&#25454;&#23396;&#23707;&#20013;&#21019;&#24314;AI&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#22312;&#25216;&#26415;&#19978;&#21487;&#20197;&#25171;&#24320;&#25968;&#25454;&#23396;&#23707;&#65292;&#20174;&#32780;&#37322;&#25918;&#32463;&#27982;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22810;&#20010;&#25317;&#26377;&#25968;&#25454;&#23396;&#23707;&#30340;&#26041;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;&#24314;&#31435;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#26159;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#24403;&#21069;&#30340;&#25991;&#29486;&#32570;&#20047;&#25104;&#21151;&#23454;&#29616;&#21512;&#20316;AI&#39033;&#30446;&#25152;&#24517;&#39035;&#32771;&#34385;&#30340;&#25351;&#21335;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#22238;&#39038;&#12289;&#28966;&#28857;&#23567;&#32452;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#30340;&#25361;&#25112;&#21644;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#21644;&#25193;&#23637;&#30340;&#19994;&#21153;&#27169;&#24335;&#65292;&#20197;&#23454;&#29616;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
The disruptive potential of AI systems roots in the emergence of big data. Yet, a significant portion is scattered and locked in data silos, leaving its potential untapped. Federated Machine Learning is a novel AI paradigm enabling the creation of AI models from decentralized, potentially siloed data. Hence, Federated Machine Learning could technically open data silos and therefore unlock economic potential. However, this requires collaboration between multiple parties owning data silos. Setting up collaborative business models is complex and often a reason for failure. Current literature lacks guidelines on which aspects must be considered to successfully realize collaborative AI projects. This research investigates the challenges of prevailing collaborative business models and distinct aspects of Federated Machine Learning. Through a systematic literature review, focus group, and expert interviews, we provide a systemized collection of socio-technical challenges and an extended Busin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#19982;&#26041;&#27861;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.13671</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#30340;&#22810;&#30446;&#26631;&#29289;&#27969;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multiobjective Logistics Optimization for Automated ATM Cash Replenishment Process. (arXiv:2304.13671v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#19982;&#26041;&#27861;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26102;&#20195;&#65292;&#23558;&#25968;&#23383;&#25216;&#26415;&#25972;&#21512;&#21040;&#38134;&#34892;&#36816;&#33829;&#30340;&#21508;&#20010;&#26041;&#38754;&#21487;&#20197;&#25913;&#21892;&#27969;&#31243;&#33258;&#21160;&#21270;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#26381;&#21153;&#27700;&#24179;&#25552;&#21319;&#12290;&#34429;&#28982;ATM&#29616;&#37329;&#29289;&#27969;&#26159;&#24433;&#21709;&#36816;&#33829;&#25104;&#26412;&#21644;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20294;&#21364;&#24456;&#23569;&#26377;&#21162;&#21147;&#26469;&#21152;&#20197;&#25913;&#36827;&#12290;&#29305;&#21035;&#26159;&#22312;&#36234;&#21335;&#65292;&#25317;&#26377;&#36229;&#36807;2&#19975;&#21488;ATM&#30340;&#24066;&#22330;&#19978;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#21644;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ATM&#29616;&#37329;&#34917;&#20805;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#36827;&#34892;&#20102;&#27010;&#25324;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the digital transformation era, integrating digital technology into every aspect of banking operations improves process automation, cost efficiency, and service level improvement. Although logistics for ATM cash is a crucial task that impacts operating costs and consumer satisfaction, there has been little effort to enhance it. Specifically, in Vietnam, with a market of more than 20,000 ATMs nationally, research and technological solutions that can resolve this issue remain scarce. In this paper, we generalized the vehicle routing problem for ATM cash replenishment, suggested a mathematical model and then offered a tool to evaluate various situations. When being evaluated on the simulated dataset, our proposed model and method produced encouraging results with the benefits of cutting ATM cash operating costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GEN&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#19968;&#23567;&#32452;&#21477;&#23376;/&#38382;&#39064;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#20174;&#29992;&#25143;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#25552;&#39640;&#29983;&#25104;&#38382;&#39064;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.13664</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#24335;&#21453;&#39304;&#25552;&#39640;&#38382;&#31572;&#29983;&#25104;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Using Implicit Feedback to Improve Question Generation. (arXiv:2304.13664v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GEN&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#19968;&#23567;&#32452;&#21477;&#23376;/&#38382;&#39064;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#20174;&#29992;&#25143;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#25552;&#39640;&#29983;&#25104;&#38382;&#39064;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#29983;&#25104;(QG)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#19968;&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#12290;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20174;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#20013;&#21463;&#30410;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#36873;&#25321;&#25110;&#32534;&#36753;&#36825;&#20123;&#38382;&#39064;&#26469;&#20351;&#20854;&#26356;&#20934;&#30830;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GEN&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20174;&#36825;&#20123;&#65288;&#38544;&#24335;&#65289;&#21453;&#39304;&#20013;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#20110;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#23567;&#32452;&#21477;&#23376;/&#38382;&#39064;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#21019;&#24314;&#27169;&#24335;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21477;&#23376;&#12290;&#27599;&#20010;&#30001;&#29992;&#25143;&#32416;&#27491;&#21518;&#29983;&#25104;&#30340;&#38382;&#39064;&#37117;&#20316;&#20026;&#19979;&#19968;&#27425;&#36845;&#20195;&#30340;&#26032;&#31181;&#23376;&#20351;&#29992;&#65292;&#22240;&#27492;&#27599;&#27425;&#37117;&#20250;&#21019;&#24314;&#26356;&#22810;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#29992;&#25143;&#25152;&#20570;&#30340;&#26356;&#27491;&#26469;&#35780;&#20998;&#27169;&#24335;&#65292;&#20174;&#32780;&#23545;&#29983;&#25104;&#30340;&#38382;&#39064;&#36827;&#34892;&#25490;&#24207;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#20248;&#20110;&#29616;&#26377;&#30340;QG&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Generation (QG) is a task of Natural Language Processing (NLP) that aims at automatically generating questions from text. Many applications can benefit from automatically generated questions, but often it is necessary to curate those questions, either by selecting or editing them. This task is informative on its own, but it is typically done post-generation, and, thus, the effort is wasted. In addition, most existing systems cannot incorporate this feedback back into them easily. In this work, we present a system, GEN, that learns from such (implicit) feedback. Following a pattern-based approach, it takes as input a small set of sentence/question pairs and creates patterns which are then applied to new unseen sentences. Each generated question, after being corrected by the user, is used as a new seed in the next iteration, so more patterns are created each time. We also take advantage of the corrections made by the user to score the patterns and therefore rank the generated qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PVP&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32479;&#35745;&#20449;&#24687;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#21021;&#22987;&#21270;&#25552;&#31034;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#26497;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#65288;&#27599;&#31867;&#19968;&#21040;&#20004;&#20010;&#31034;&#20363;&#65289;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13639</link><description>&lt;p&gt;
PVP: &#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PVP: Pre-trained Visual Parameter-Efficient Tuning. (arXiv:2304.13639v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PVP&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32479;&#35745;&#20449;&#24687;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#21021;&#22987;&#21270;&#25552;&#31034;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#26497;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#65288;&#27599;&#31867;&#19968;&#21040;&#20004;&#20010;&#31034;&#20363;&#65289;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23436;&#20840;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#26497;&#22823;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#27425;&#36890;&#36807;&#32463;&#39564;&#25506;&#31350;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;PETuning&#26041;&#27861;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PVP&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#21021;&#22987;&#21270;&#25552;&#31034;&#27169;&#22359;&#12290;PVP&#21487;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#19978;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#65288;&#20363;&#22914;&#27599;&#31867;&#19968;&#21040;&#20004;&#20010;&#31034;&#20363;&#65289;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained transformers have demonstrated remarkable success in various computer vision tasks. However, it is still highly challenging to fully fine-tune these models for downstream tasks due to their high computational and storage costs. Recently, Parameter-Efficient Tuning (PETuning) techniques, e.g., Visual Prompt Tuning (VPT) and Low-Rank Adaptation (LoRA), have significantly reduced the computation and storage cost by inserting lightweight prompt modules into the pre-trained models and tuning these prompt modules with a small number of trainable parameters, while keeping the transformer backbone frozen. Although only a few parameters need to be adjusted, most PETuning methods still require a significant amount of downstream task training data to achieve good results. The performance is inadequate on low-data regimes, especially when there are only one or two examples per class. To this end, we first empirically identify the poor performance is mainly due to the inappr
&lt;/p&gt;</description></item><item><title>AutoCure&#26159;&#19968;&#31181;&#26080;&#38656;&#37197;&#32622;&#30340;&#25968;&#25454;&#25972;&#29702;&#31649;&#36947;&#65292;&#21487;&#25552;&#39640;&#34920;&#26684;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#30340;&#35823;&#24046;&#26816;&#27979;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#65292;&#20174;&#21512;&#20046;&#35268;&#33539;&#30340;&#25968;&#25454;&#37096;&#20998;&#20013;&#22686;&#24378;&#20102;&#28165;&#26224;&#25968;&#25454;&#20998;&#25968;&#30340;&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13636</link><description>&lt;p&gt;
AutoCure&#65306;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#31649;&#32447;&#30340;&#33258;&#21160;&#34920;&#26684;&#25968;&#25454;&#25972;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
AutoCure: Automated Tabular Data Curation Technique for ML Pipelines. (arXiv:2304.13636v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13636
&lt;/p&gt;
&lt;p&gt;
AutoCure&#26159;&#19968;&#31181;&#26080;&#38656;&#37197;&#32622;&#30340;&#25968;&#25454;&#25972;&#29702;&#31649;&#36947;&#65292;&#21487;&#25552;&#39640;&#34920;&#26684;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#30340;&#35823;&#24046;&#26816;&#27979;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#65292;&#20174;&#21512;&#20046;&#35268;&#33539;&#30340;&#25968;&#25454;&#37096;&#20998;&#20013;&#22686;&#24378;&#20102;&#28165;&#26224;&#25968;&#25454;&#20998;&#25968;&#30340;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#22810;&#20010;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#25968;&#25454;&#20934;&#22791;&#20173;&#28982;&#26159;&#21457;&#23637;&#20934;&#30830;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#26102;&#38388;&#25237;&#36164;&#26469;&#25628;&#32034;&#36866;&#21512;&#30340;&#25968;&#25454;&#25972;&#29702;&#21644;&#36716;&#25442;&#24037;&#20855;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoCure&#65292;&#19968;&#31181;&#26032;&#39062;&#19988;&#26080;&#38656;&#37197;&#32622;&#30340;&#25968;&#25454;&#25972;&#29702;&#31649;&#36947;&#65292;&#21487;&#25552;&#39640;&#34920;&#26684;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#19982;&#20256;&#32479;&#30340;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#19981;&#21516;&#65292;AutoCure&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#30340;&#35823;&#24046;&#26816;&#27979;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#65292;&#20174;&#21512;&#20046;&#35268;&#33539;&#30340;&#25968;&#25454;&#37096;&#20998;&#20013;&#22686;&#24378;&#20102;&#28165;&#26224;&#25968;&#25454;&#20998;&#25968;&#30340;&#23494;&#24230;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;AutoCure&#21487;&#20197;&#19982;&#24320;&#28304;&#24037;&#20855;&#65288;&#20363;&#22914;Auto-sklearn&#12289;H2O&#21644;TPOT&#65289;&#38598;&#25104;&#65292;&#20197;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#27665;&#20027;&#21270;&#12290;&#20316;&#20026;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;AutoCure&#19982;28&#31181;&#20256;&#32479;&#25968;&#25454;&#25972;&#29702;&#24037;&#20855;&#32452;&#21512;&#30340;&#27604;&#36739;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have become increasingly prevalent in multiple domains, such as autonomous driving, healthcare, and finance. In such domains, data preparation remains a significant challenge in developing accurate models, requiring significant expertise and time investment to search the huge search space of well-suited data curation and transformation tools. To address this challenge, we present AutoCure, a novel and configuration-free data curation pipeline that improves the quality of tabular data. Unlike traditional data curation methods, AutoCure synthetically enhances the density of the clean data fraction through an adaptive ensemble-based error detection method and a data augmentation module. In practice, AutoCure can be integrated with open source tools, e.g., Auto-sklearn, H2O, and TPOT, to promote the democratization of machine learning. As a proof of concept, we provide a comparative evaluation of AutoCure against 28 combinations of traditional data curation tool
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#24182;&#19981;&#21482;&#29992;&#20110;&#20195;&#34920;&#26576;&#20010;&#27010;&#24565;&#65292;&#36824;&#21487;&#20316;&#20026;&#22806;&#37096;&#21644;&#20869;&#37096;&#30340;&#27807;&#36890;&#24037;&#20855;&#65292;&#25552;&#39640;&#30693;&#35782;&#20256;&#36882;&#25928;&#29575;&#65292;&#24182;&#23545;&#23398;&#20064;&#19990;&#30028;&#25552;&#20379;&#26377;&#30410;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2304.13626</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#31526;&#21495;&#30340;&#20316;&#29992;&#65306;&#23427;&#20204;&#24182;&#19981;&#26159;&#20320;&#35748;&#20026;&#30340;&#37027;&#26679;&#65281;
&lt;/p&gt;
&lt;p&gt;
The Roles of Symbols in Neural-based AI: They are Not What You Think!. (arXiv:2304.13626v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13626
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#24182;&#19981;&#21482;&#29992;&#20110;&#20195;&#34920;&#26576;&#20010;&#27010;&#24565;&#65292;&#36824;&#21487;&#20316;&#20026;&#22806;&#37096;&#21644;&#20869;&#37096;&#30340;&#27807;&#36890;&#24037;&#20855;&#65292;&#25552;&#39640;&#30693;&#35782;&#20256;&#36882;&#25928;&#29575;&#65292;&#24182;&#23545;&#23398;&#20064;&#19990;&#30028;&#25552;&#20379;&#26377;&#30410;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#31526;&#21495;&#39318;&#20808;&#26159;&#26234;&#33021;&#20307;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#22806;&#37096;&#27807;&#36890;&#24037;&#20855;&#65292;&#27604;&#36215;&#30452;&#25509;&#32463;&#39564;&#19990;&#30028;&#65292;&#20351;&#29992;&#31526;&#21495;&#21487;&#20197;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;&#20294;&#31526;&#21495;&#20063;&#21487;&#20197;&#22312;&#26234;&#33021;&#20307;&#20869;&#37096;&#36890;&#36807;&#33258;&#25105;&#27807;&#36890;&#30340;&#24418;&#24335;&#20351;&#29992;&#65292;&#26469;&#24110;&#21161;&#25551;&#36848;&#21644;&#35777;&#26126;&#30495;&#27491;&#23454;&#26045;&#24605;&#32771;&#30340;&#27425;&#31526;&#21495;&#27169;&#24335;&#30340;&#31070;&#32463;&#27963;&#21160;&#12290;&#31526;&#21495;&#21644;&#20351;&#29992;&#31526;&#21495;&#30340;&#35821;&#35328;&#19981;&#20165;&#20801;&#35768;&#25105;&#20204;&#21521;&#33258;&#24049;&#21644;&#20182;&#20154;&#38416;&#36848;&#25105;&#20204;&#30340;&#24605;&#32500;&#65292;&#36824;&#20026;&#23398;&#20064;&#19990;&#30028;&#25552;&#20379;&#26377;&#30410;&#30340;&#32422;&#26463;&#65288;&#24341;&#23548;&#20559;&#24046;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#31070;&#32463;&#31185;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#35265;&#35299;&#65292;&#20171;&#32461;&#20102;&#20154;&#31867;&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#21450;&#20854;&#25152;&#25351;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#29616;&#20195;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23454;&#29616;&#30456;&#21516;&#30340;&#21151;&#33021;&#12290;&#25509;&#30528;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#20551;&#35774;&#21644;&#21487;&#33021;&#30340;&#26234;&#33021;&#20307;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#27425;&#31526;&#21495;&#34920;&#31034;&#21644;&#31526;&#21495;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose that symbols are first and foremost external communication tools used between intelligent agents that allow knowledge to be transferred in a more efficient and effective manner than having to experience the world directly. But, they are also used internally within an agent through a form of self-communication to help formulate, describe and justify subsymbolic patterns of neural activity that truly implement thinking. Symbols, and our languages that make use of them, not only allow us to explain our thinking to others and ourselves, but also provide beneficial constraints (inductive bias) on learning about the world. In this paper we present relevant insights from neuroscience and cognitive science, about how the human brain represents symbols and the concepts they refer to, and how today's artificial neural networks can do the same. We then present a novel neuro-symbolic hypothesis and a plausible architecture for intelligent agents that combines subsymbolic representations
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;</title><link>http://arxiv.org/abs/2304.13567</link><description>&lt;p&gt;
&#20301;&#32622;&#20559;&#24046;&#23545;token&#20998;&#31867;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13567
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#25110;&#35789;&#24615;&#26631;&#27880;&#31561;&#19979;&#28216;&#20219;&#21153;&#24050;&#30693;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#36127;&#31034;&#20363;&#30340;&#27604;&#20363;&#21644;&#31867;&#19981;&#24179;&#34913;&#26041;&#38754;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#65292;&#21363;token&#20998;&#31867;&#20219;&#21153;&#20013;&#27491;&#31034;&#20363;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;Token&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20301;&#32622;&#20559;&#24046;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;CoNLL03&#21644;OntoNote5.0&#29992;&#20110;NER&#65292;English Tree Bank UD_en&#21644;TweeBank&#29992;&#20110;POS&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;Transformer&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#20687;BERT&#12289;ERNIE&#12289;ELECTRA&#36825;&#26679;&#30340;&#32534;&#30721;&#22120;&#21644;&#20687;GPT2 &#21644;BLOOM&#36825;&#26679;&#30340;&#35299;&#30721;&#22120;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#22522;&#20110;&#37327;&#21270;&#21516;&#26102;&#23454;&#29616;&#36890;&#35759;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#19988;&#21521;&#22343;&#21248;&#37327;&#21270;&#30340;&#26799;&#24230;&#28155;&#21152;&#20108;&#39033;&#22122;&#22768;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#24046;&#20998;&#38544;&#31169;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.13545</link><description>&lt;p&gt;
&#19968;&#31661;&#21452;&#38613;&#65306;&#37327;&#21270;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#19982;&#36890;&#35759;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Killing Two Birds with One Stone: Quantization Achieves Privacy in Distributed Learning. (arXiv:2304.13545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#22522;&#20110;&#37327;&#21270;&#21516;&#26102;&#23454;&#29616;&#36890;&#35759;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#19988;&#21521;&#22343;&#21248;&#37327;&#21270;&#30340;&#26799;&#24230;&#28155;&#21152;&#20108;&#39033;&#22122;&#22768;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#24046;&#20998;&#38544;&#31169;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36890;&#35759;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#26159;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20998;&#24320;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#21487;&#33021;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#24212;&#29992;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#36890;&#35759;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#36890;&#35759;&#21644;&#38544;&#31169;&#30456;&#20851;&#24615;&#36136;&#30340;&#26032;&#35265;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21521;&#22343;&#21248;&#37327;&#21270;&#30340;&#26799;&#24230;&#28155;&#21152;&#20108;&#39033;&#22122;&#22768;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#24046;&#20998;&#38544;&#31169;&#32423;&#21035;&#65292;&#20174;&#32780;&#22312;&#31245;&#24494;&#29306;&#29298;&#36890;&#35759;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477; (SGD) &#26694;&#26550;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#25429;&#25417;&#20102;&#36890;&#35759;&#12289;&#38544;&#31169;&#21644;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#26032;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication efficiency and privacy protection are two critical issues in distributed machine learning. Existing methods tackle these two issues separately and may have a high implementation complexity that constrains their application in a resource-limited environment. We propose a comprehensive quantization-based solution that could simultaneously achieve communication efficiency and privacy protection, providing new insights into the correlated nature of communication and privacy. Specifically, we demonstrate the effectiveness of our proposed solutions in the distributed stochastic gradient descent (SGD) framework by adding binomial noise to the uniformly quantized gradients to reach the desired differential privacy level but with a minor sacrifice in communication efficiency. We theoretically capture the new trade-offs between communication, privacy, and learning performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20845;&#31181;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#26367;&#25442;&#20026;&#20302;&#31209;&#24352;&#37327;&#36924;&#36817;&#30340;&#26041;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#19988;&#36866;&#21512;&#20110;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.13539</link><description>&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Tensor Decomposition for Model Reduction in Neural Networks: A Review. (arXiv:2304.13539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20845;&#31181;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#26367;&#25442;&#20026;&#20302;&#31209;&#24352;&#37327;&#36924;&#36817;&#30340;&#26041;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#19988;&#36866;&#21512;&#20110;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#23427;&#20204;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#65292;&#38656;&#35201;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#19968;&#31181;&#30452;&#25509;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#19981;&#21516;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#23558;&#32593;&#32476;&#30340;&#23618;&#26367;&#25442;&#20026;&#20854;&#20302;&#31209;&#24352;&#37327;&#36817;&#20284;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20845;&#31181;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#24182;&#38416;&#36848;&#20102;&#23427;&#20204;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#27169;&#22411;&#21442;&#25968;&#21387;&#32553;&#20013;&#30340;&#33021;&#21147;&#12290;&#19968;&#20123;&#21387;&#32553;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#29978;&#33267;&#21487;&#20197;&#39640;&#20110;&#21407;&#22987;&#29256;&#26412;&#12290;&#35780;&#20272;&#34920;&#26126;&#65292;&#24352;&#37327;&#20998;&#35299;&#21487;&#20197;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#23454;&#29616;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#21512;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural networks have revolutionized the fields of computer vision (CV) and Natural Language Processing (NLP). They are widely used for solving complex CV tasks and NLP tasks such as image classification, image generation, and machine translation. Most state-of-the-art neural networks are over-parameterized and require a high computational cost. One straightforward solution is to replace the layers of the networks with their low-rank tensor approximations using different tensor decomposition methods. This paper reviews six tensor decomposition methods and illustrates their ability to compress model parameters of convolutional neural networks (CNNs), recurrent neural networks (RNNs) and Transformers. The accuracy of some compressed models can be higher than the original versions. Evaluations indicate that tensor decompositions can achieve significant reductions in model size, run-time and energy consumption, and are well suited for implementing neural networks on edge devices.
&lt;/p&gt;</description></item><item><title>ElegansNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;Caenorhabditis elegans&#30340;&#36830;&#25509;&#32452;&#30340;&#32467;&#26500;&#20316;&#20026;&#21442;&#32771;&#65292;&#35774;&#35745;&#29983;&#25104;&#20102;&#20855;&#26377;&#31867;&#20284;&#20110;&#33258;&#28982;&#32593;&#32476;&#25299;&#25169;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#22312;&#25928;&#29575;&#12289;&#24615;&#33021;&#19978;&#22343;&#32988;&#36807;&#38543;&#26426;&#36830;&#32447;&#32593;&#32476;&#21644;&#20154;&#24037;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.13538</link><description>&lt;p&gt;
ElegansNet&#65306;&#19968;&#31687;&#31616;&#35201;&#30340;&#31185;&#23398;&#25253;&#21578;&#21644;&#21021;&#27493;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
ElegansNet: a brief scientific report and initial experiments. (arXiv:2304.13538v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13538
&lt;/p&gt;
&lt;p&gt;
ElegansNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;Caenorhabditis elegans&#30340;&#36830;&#25509;&#32452;&#30340;&#32467;&#26500;&#20316;&#20026;&#21442;&#32771;&#65292;&#35774;&#35745;&#29983;&#25104;&#20102;&#20855;&#26377;&#31867;&#20284;&#20110;&#33258;&#28982;&#32593;&#32476;&#25299;&#25169;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#22312;&#25928;&#29575;&#12289;&#24615;&#33021;&#19978;&#22343;&#32988;&#36807;&#38543;&#26426;&#36830;&#32447;&#32593;&#32476;&#21644;&#20154;&#24037;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25253;&#21578;&#20171;&#32461;&#20102;ElegansNet&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#27169;&#20223;&#29616;&#23454;&#19990;&#30028;&#30340;&#31070;&#32463;&#32593;&#32476;&#30005;&#36335;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#36830;&#36890;&#24615;&#25299;&#25169;&#21644;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#29289;&#31070;&#32463;&#30005;&#36335;&#30340;&#24378;&#22823;&#34920;&#36798;&#33021;&#21147;&#26469;&#35774;&#35745;&#21644;&#29983;&#25104;&#20855;&#26377;&#31867;&#20284;&#20110;&#33258;&#28982;&#32593;&#32476;&#25299;&#25169;&#30340;&#25913;&#36827;&#22411;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#12290;&#30001;&#20110;&#20854;&#23436;&#25972;&#24615;&#12289;&#21512;&#29702;&#30340;&#22823;&#23567;&#21644;&#21151;&#33021;&#31070;&#32463;&#20803;&#31867;&#21035;&#27880;&#37322;&#65292;Caenorhabditis elegans&#30340;&#36830;&#25509;&#32452;&#34987;&#29992;&#20316;&#21442;&#32771;&#12290;&#35777;&#26126;&#20102;&#31616;&#21333;&#29983;&#29289;&#30340;&#36830;&#36890;&#22270;&#22312;&#31070;&#32463;&#20803;&#20043;&#38388;&#20855;&#26377;&#29305;&#23450;&#30340;&#21151;&#33021;&#20851;&#31995;&#65292;&#19968;&#26086;&#36716;&#21270;&#20026;&#21487;&#23398;&#20064;&#30340;&#24352;&#37327;&#32593;&#32476;&#24182;&#38598;&#25104;&#21040;&#29616;&#20195;&#26550;&#26500;&#20013;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#29983;&#29289;&#21487;&#34892;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#38543;&#26426;&#36830;&#32447;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#19982;&#25353;&#20840;&#29699;&#22522;&#20934;&#25490;&#21517;&#30340;&#20154;&#24037;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research report introduces ElegansNet, a neural network that mimics real-world neuronal network circuitry, with the goal of better understanding the interplay between connectome topology and deep learning systems. The proposed approach utilizes the powerful representational capabilities of living beings' neuronal circuitry to design and generate improved deep learning systems with a topology similar to natural networks. The Caenorhabditis elegans connectome is used as a reference due to its completeness, reasonable size, and functional neuron classes annotations. It is demonstrated that the connectome of simple organisms exhibits specific functional relationships between neurons, and once transformed into learnable tensor networks and integrated into modern architectures, it offers bio-plausible structures that efficiently solve complex tasks. The performance of the models is demonstrated against randomly wired networks and compared to artificial networks ranked on global benchmar
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25163;&#20889;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Transformer&#65292;&#22312;&#21516;&#19968;&#27169;&#22411;&#20013;&#32467;&#21512;&#20102;&#29305;&#24449;&#25552;&#21462;&#12289;&#25163;&#20889;&#35782;&#21035;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27493;&#39588;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39044;&#20808;&#20998;&#21106;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#24211;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.13530</link><description>&lt;p&gt;
&#20174;&#20840;&#25163;&#20889;&#39029;&#38754;&#20013;&#25552;&#21462;&#38190;&#20540;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Key-value information extraction from full handwritten pages. (arXiv:2304.13530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13530
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25163;&#20889;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Transformer&#65292;&#22312;&#21516;&#19968;&#27169;&#22411;&#20013;&#32467;&#21512;&#20102;&#29305;&#24449;&#25552;&#21462;&#12289;&#25163;&#20889;&#35782;&#21035;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27493;&#39588;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39044;&#20808;&#20998;&#21106;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#24211;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25968;&#23383;&#21270;&#30340;&#25163;&#20889;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#30446;&#21069;&#30001;&#29420;&#31435;&#27169;&#22411;&#25191;&#34892;&#30340;&#19981;&#21516;&#27493;&#39588;&#65306;&#29305;&#24449;&#25552;&#21462;&#12289;&#25163;&#20889;&#35782;&#21035;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20043;&#21069;&#36827;&#34892;&#25163;&#20889;&#35782;&#21035;&#65292;&#24182;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#21576;&#29616;&#32467;&#26524;&#65306;&#34892;&#12289;&#27573;&#33853;&#21644;&#39029;&#38754;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24212;&#29992;&#20110;&#25972;&#20010;&#39029;&#38754;&#26102;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#29305;&#21035;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#20808;&#20998;&#21106;&#27493;&#39588;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#20174;&#38190;&#20540;&#27880;&#37322;&#20013;&#36827;&#34892;&#23398;&#20064;&#65306;&#21363;&#37325;&#35201;&#21333;&#35789;&#21644;&#30456;&#24212;&#21629;&#21517;&#23454;&#20307;&#30340;&#21015;&#34920;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#24211;&#65288;IAM&#12289;ESPOSALLES&#21644;POPP&#65289;&#30340;&#26368;&#26032;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22312;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Transformer-based approach for information extraction from digitized handwritten documents. Our approach combines, in a single model, the different steps that were so far performed by separate models: feature extraction, handwriting recognition and named entity recognition. We compare this integrated approach with traditional two-stage methods that perform handwriting recognition before named entity recognition, and present results at different levels: line, paragraph, and page. Our experiments show that attention-based models are especially interesting when applied on full pages, as they do not require any prior segmentation step. Finally, we show that they are able to learn from key-value annotations: a list of important words with their corresponding named entities. We compare our models to state-of-the-art methods on three public databases (IAM, ESPOSALLES, and POPP) and outperform previous performances on all three datasets.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#24207;&#21015;&#20998;&#35299;&#65292;&#36890;&#36807;&#20998;&#26512;&#31243;&#24207;&#20043;&#38388;&#30340;Green&#20851;&#31995;&#65292;&#20026;&#36923;&#36753;&#32534;&#31243;&#20195;&#25968;&#29702;&#35770;&#36827;&#19968;&#27493;&#21457;&#23637;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.13522</link><description>&lt;p&gt;
&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#24207;&#21015;&#20998;&#35299;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sequential decomposition of propositional logic programs. (arXiv:2304.13522v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13522
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#24207;&#21015;&#20998;&#35299;&#65292;&#36890;&#36807;&#20998;&#26512;&#31243;&#24207;&#20043;&#38388;&#30340;Green&#20851;&#31995;&#65292;&#20026;&#36923;&#36753;&#32534;&#31243;&#20195;&#25968;&#29702;&#35770;&#36827;&#19968;&#27493;&#21457;&#23637;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#20102;&#21629;&#39064;&#36923;&#36753;&#31243;&#24207;&#30340;&#24207;&#21015;&#32452;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31243;&#24207;&#20043;&#38388;&#30340; Green &#20851;&#31995; $\mathcal{L,R,J}$&#65292;&#20174;&#32780;&#30740;&#31350;&#31243;&#24207;&#30340;&#24207;&#21015;&#20998;&#35299;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#65292;&#26412;&#25991;&#26159;&#36923;&#36753;&#32534;&#31243;&#20195;&#25968;&#29702;&#35770;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sequential composition of propositional logic programs has been recently introduced. This paper studies the sequential {\em decomposition} of programs by studying Green's relations $\mathcal{L,R,J}$ -- well-known in semigroup theory -- between programs. In a broader sense, this paper is a further step towards an algebraic theory of logic programming.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#30149;&#29702;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31751;&#29109;&#26469;&#36873;&#25321;&#26377;&#25928;&#30340;WSI&#24182;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21322;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13513</link><description>&lt;p&gt;
&#31751;&#29109;&#65306;&#30149;&#29702;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20027;&#21160;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Cluster Entropy: Active Domain Adaptation in Pathological Image Segmentation. (arXiv:2304.13513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#30149;&#29702;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31751;&#29109;&#26469;&#36873;&#25321;&#26377;&#25928;&#30340;WSI&#24182;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21322;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30149;&#29702;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#22495;&#20559;&#31227;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#28304;&#22495;&#65288;&#22312;&#29305;&#23450;&#21307;&#38498;&#25910;&#38598;&#30340;&#22270;&#20687;&#65289;&#35757;&#32451;&#30340;&#32593;&#32476;&#22312;&#30446;&#26631;&#22495;&#65288;&#26469;&#33258;&#19981;&#21516;&#21307;&#38498;&#65289;&#20013;&#30001;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#29305;&#24449;&#34920;&#29616;&#19981;&#20339;&#12290;&#30001;&#20110;&#30149;&#29702;&#23398;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#19981;&#21516;&#31867;&#21035;&#20808;&#39564;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#20856;&#22411;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#19981;&#33021;&#36890;&#36807;&#23545;&#40784;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#20998;&#24067;&#26469;&#24456;&#22909;&#22320;&#22788;&#29702;&#35813;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31751;&#29109;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#29992;&#20110;&#21322;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#30340;&#26377;&#25928;WSI&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#31751;&#30340;&#29109;&#26469;&#24230;&#37327;WSI&#30340;&#22270;&#20687;&#29305;&#24449;&#22914;&#20309;&#35206;&#30422;&#30446;&#26631;&#22495;&#30340;&#25972;&#20010;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;&#20004;&#23478;&#21307;&#38498;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The domain shift in pathological segmentation is an important problem, where a network trained by a source domain (collected at a specific hospital) does not work well in the target domain (from different hospitals) due to the different image features. Due to the problems of class imbalance and different class prior of pathology, typical unsupervised domain adaptation methods do not work well by aligning the distribution of source domain and target domain. In this paper, we propose a cluster entropy for selecting an effective whole slide image (WSI) that is used for semi-supervised domain adaptation. This approach can measure how the image features of the WSI cover the entire distribution of the target domain by calculating the entropy of each cluster and can significantly improve the performance of domain adaptation. Our approach achieved competitive results against the prior arts on datasets collected from two hospitals.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35752;&#35770;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#25216;&#26415;&#21644;&#20020;&#24202;&#35270;&#35282;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#23398;&#31185;&#21512;&#20316;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13493</link><description>&lt;p&gt;
&#23454;&#29616;&#20020;&#24202;AI&#20844;&#24179;&#24615;: &#19968;&#20010;&#32763;&#35793;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards clinical AI fairness: A translational perspective. (arXiv:2304.13493v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13493
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35752;&#35770;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#25216;&#26415;&#21644;&#20020;&#24202;&#35270;&#35282;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#23398;&#31185;&#21512;&#20316;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#27934;&#35265;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#21033;&#30410;&#39046;&#22495;&#65292;&#20844;&#24179;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#23613;&#31649;&#22312;&#31639;&#27861;&#24320;&#21457;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35752;&#35770;&#21644;&#21162;&#21147;&#65292;&#20294;AI&#20844;&#24179;&#24615;&#21644;&#20020;&#24202;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;AI&#20844;&#24179;&#24615;&#30340;&#25216;&#26415;&#21644;&#20020;&#24202;&#35270;&#35282;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#24378;&#35843;&#20102;AI&#20844;&#24179;&#24615;&#22312;&#36716;&#21270;&#20026;&#21307;&#30103;&#20445;&#20581;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#38556;&#30861;&#65292;&#20027;&#24352;&#36328;&#23398;&#31185;&#21512;&#20316;&#20197;&#24357;&#34917;&#30693;&#35782;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#20851;&#20110;AI&#20844;&#24179;&#24615;&#30340;&#20020;&#24202;&#38382;&#39064;&#30340;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has demonstrated the ability to extract insights from data, but the issue of fairness remains a concern in high-stakes fields such as healthcare. Despite extensive discussion and efforts in algorithm development, AI fairness and clinical concerns have not been adequately addressed. In this paper, we discuss the misalignment between technical and clinical perspectives of AI fairness, highlight the barriers to AI fairness' translation to healthcare, advocate multidisciplinary collaboration to bridge the knowledge gap, and provide possible solutions to address the clinical concerns pertaining to AI fairness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20808;&#39564;&#20449;&#24687;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#65292;&#24341;&#20837;&#20102;&#20248;&#20808;&#39118;&#38505;&#27010;&#24565;&#65292;&#24182;&#20026;&#32479;&#35745;&#20272;&#35745;&#38382;&#39064;&#25552;&#20379;&#20102;&#19979;&#30028;&#65292;&#23637;&#29616;&#20102;&#26694;&#26550;&#22312;&#19981;&#21516;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.13479</link><description>&lt;p&gt;
&#20808;&#39564;&#20449;&#24687;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Fundamental Tradeoffs in Learning with Prior Information. (arXiv:2304.13479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20808;&#39564;&#20449;&#24687;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#65292;&#24341;&#20837;&#20102;&#20248;&#20808;&#39118;&#38505;&#27010;&#24565;&#65292;&#24182;&#20026;&#32479;&#35745;&#20272;&#35745;&#38382;&#39064;&#25552;&#20379;&#20102;&#19979;&#30028;&#65292;&#23637;&#29616;&#20102;&#26694;&#26550;&#22312;&#19981;&#21516;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#23398;&#20064;&#32773;&#22312;&#25152;&#23398;&#38382;&#39064;&#19978;&#30340;&#20808;&#39564;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#21644;&#20854;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20248;&#20808;&#39118;&#38505;&#30340;&#27010;&#24565;&#65292;&#23427;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#26497;&#23567;&#26497;&#22823;&#21644;&#36125;&#21494;&#26031;&#39118;&#38505;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#30740;&#31350;&#29616;&#23454;&#19981;&#19968;&#23450;&#31526;&#21512;&#23398;&#20064;&#32773;&#20808;&#39564;&#30340;&#24773;&#20917;&#19979;&#36825;&#20123;&#22522;&#26412;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32553;&#20943;&#30340;&#26041;&#27861;&#26469;&#25193;&#23637;&#32463;&#20856;&#30340;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#25216;&#26415;&#65292;&#20197;&#20415;&#20026;&#32479;&#35745;&#20272;&#35745;&#38382;&#39064;&#30340;&#20248;&#20808;&#39118;&#38505;&#25552;&#20379;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#35834;&#19981;&#31561;&#24335;&#30340;&#25512;&#24191;&#65288;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#65289;&#65292;&#29992;&#20110;&#22312;&#28041;&#21450;&#26080;&#38480;&#25439;&#22833;&#30340;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#19979;&#65292;&#19979;&#30028;&#20248;&#20808;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25581;&#31034;&#20102;&#22312;&#20272;&#35745;&#12289;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#20808;&#39564;&#20449;&#24687;&#19982;&#23398;&#20064;&#24615;&#33021;&#20043;&#38388;&#26435;&#34913;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to understand fundamental tradeoffs between the accuracy of prior information that a learner has on a given problem and its learning performance. We introduce the notion of prioritized risk, which differs from traditional notions of minimax and Bayes risk by allowing us to study such fundamental tradeoffs in settings where reality does not necessarily conform to the learner's prior. We present a general reduction-based approach for extending classical minimax lower-bound techniques in order to lower bound the prioritized risk for statistical estimation problems. We also introduce a novel generalization of Fano's inequality (which may be of independent interest) for lower bounding the prioritized risk in more general settings involving unbounded losses. We illustrate the ability of our framework to provide insights into tradeoffs between prior information and learning performance for problems in estimation, regression, and reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23433;&#25490;&#22320;&#38081;&#26102;&#21051;&#34920;&#21644;&#35843;&#25972;&#21015;&#36710;&#30340;&#20572;&#38752;&#26102;&#38388;&#21644;&#24033;&#33322;&#36895;&#24230;&#65292;&#20248;&#21270;&#25200;&#21160;&#19979;&#30340;&#22320;&#38081;&#31995;&#32479;&#33021;&#28304;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#36798;&#38477;&#20302;10.9%&#30340;&#29301;&#24341;&#33021;&#37327;&#28040;&#32791;&#21644;&#26368;&#39640;&#36798;&#25552;&#39640;47.9%&#30340;&#20877;&#29983;&#21046;&#21160;&#33021;&#37327;&#21033;&#29992;&#29575;&#65292;&#20026;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#30340;&#33410;&#33021;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.13443</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22320;&#38081;&#31995;&#32479;&#33021;&#28304;&#25928;&#29575;&#22312;&#19981;&#30830;&#23450;&#25200;&#21160;&#19979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning. (arXiv:2304.13443v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23433;&#25490;&#22320;&#38081;&#26102;&#21051;&#34920;&#21644;&#35843;&#25972;&#21015;&#36710;&#30340;&#20572;&#38752;&#26102;&#38388;&#21644;&#24033;&#33322;&#36895;&#24230;&#65292;&#20248;&#21270;&#25200;&#21160;&#19979;&#30340;&#22320;&#38081;&#31995;&#32479;&#33021;&#28304;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#36798;&#38477;&#20302;10.9%&#30340;&#29301;&#24341;&#33021;&#37327;&#28040;&#32791;&#21644;&#26368;&#39640;&#36798;&#25552;&#39640;47.9%&#30340;&#20877;&#29983;&#21046;&#21160;&#33021;&#37327;&#21033;&#29992;&#29575;&#65292;&#20026;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#30340;&#33410;&#33021;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#20132;&#36890;&#39046;&#22495;&#65292;&#22320;&#38081;&#31995;&#32479;&#26159;&#20851;&#38190;&#30340;&#21487;&#25345;&#32493;&#20844;&#20849;&#20132;&#36890;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24040;&#22823;&#33021;&#28304;&#28040;&#32791;&#23545;&#21487;&#25345;&#32493;&#24615;&#30446;&#26631;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#24310;&#35823;&#21644;&#20056;&#23458;&#27969;&#21464;&#21270;&#31561;&#25200;&#21160;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#23545;&#22320;&#38081;&#31995;&#32479;&#30340;&#33021;&#28304;&#25928;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#21015;&#36710;&#30340;&#20572;&#38752;&#26102;&#38388;&#21644;&#24033;&#33322;&#36895;&#24230;&#65292;&#37325;&#26032;&#23433;&#25490;&#22320;&#38081;&#26102;&#21051;&#34920;&#65292;&#24182;&#20248;&#21270;&#21463;&#25200;&#21160;&#24433;&#21709;&#30340;&#22320;&#38081;&#31995;&#32479;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;10.9&#65285;&#30340;&#29301;&#24341;&#33021;&#37327;&#28040;&#32791;&#38477;&#20302;&#21644;&#26368;&#39640;&#36798;47.9&#65285;&#30340;&#20877;&#29983;&#21046;&#21160;&#33021;&#37327;&#21033;&#29992;&#29575;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#20026;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#33410;&#33021;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of urban transportation, metro systems serve as crucial and sustainable means of public transit. However, their substantial energy consumption poses a challenge to the goal of sustainability. Disturbances such as delays and passenger flow changes can further exacerbate this issue by negatively affecting energy efficiency in metro systems. To tackle this problem, we propose a policy-based reinforcement learning approach that reschedules the metro timetable and optimizes energy efficiency in metro systems under disturbances by adjusting the dwell time and cruise speed of trains. Our experiments conducted in a simulation environment demonstrate the superiority of our method over baseline methods, achieving a traction energy consumption reduction of up to 10.9% and an increase in regenerative braking energy utilization of up to 47.9%. This study provides an effective solution to the energy-saving problem of urban rail transit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23545;&#36229;&#20986;&#20998;&#24067;&#30340;&#8220;&#21487;&#25511;&#8221;&#29366;&#24577;&#30340;&#8220;&#25509;&#21147;&#27867;&#21270;&#8221;&#24615;&#33021;&#12290;&#36890;&#36807;&#35753;&#27979;&#35797;&#20195;&#29702;&#20174;&#20854;&#20182;&#38476;&#29983;&#20195;&#29702;&#30340;&#36712;&#36857;&#20013;&#38388;&#24320;&#22987;&#65292;&#21457;&#29616;&#36825;&#31181;&#27867;&#21270;&#26222;&#36941;&#23384;&#22312;&#27867;&#21270;&#22833;&#25928;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13424</link><description>&lt;p&gt;
&#19982;&#38476;&#29983;&#20154;&#19968;&#36215;&#36305;&#25509;&#21147;&#36187;&#65311;&#24378;&#21270;&#23398;&#20064;&#22312;&#36229;&#20986;&#20998;&#24067;&#36712;&#36857;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories. (arXiv:2304.13424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23545;&#36229;&#20986;&#20998;&#24067;&#30340;&#8220;&#21487;&#25511;&#8221;&#29366;&#24577;&#30340;&#8220;&#25509;&#21147;&#27867;&#21270;&#8221;&#24615;&#33021;&#12290;&#36890;&#36807;&#35753;&#27979;&#35797;&#20195;&#29702;&#20174;&#20854;&#20182;&#38476;&#29983;&#20195;&#29702;&#30340;&#36712;&#36857;&#20013;&#38388;&#24320;&#22987;&#65292;&#21457;&#29616;&#36825;&#31181;&#27867;&#21270;&#26222;&#36941;&#23384;&#22312;&#27867;&#21270;&#22833;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#25913;&#36827;&#21508;&#31181;&#29366;&#24577;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#23545;&#36229;&#20986;&#20998;&#24067;&#30340;&#8220;&#21487;&#25511;&#8221;&#29366;&#24577;&#30340;&#8220;&#25509;&#21147;&#27867;&#21270;&#8221;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#27979;&#35797;&#20195;&#29702;&#20174;&#20854;&#20182;&#29420;&#31435;&#35757;&#32451;&#33391;&#22909;&#30340;&#8220;&#38476;&#29983;&#8221;&#20195;&#29702;&#30340;&#36712;&#36857;&#30340;&#20013;&#38388;&#24320;&#22987;&#65292;&#25105;&#20204;&#23454;&#38469;&#35780;&#20272;&#20102;&#36825;&#31181;&#27867;&#21270;&#31867;&#22411;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26469;&#33258;&#38476;&#29983;&#20195;&#29702;&#30340;&#21487;&#25511;&#29366;&#24577;&#20960;&#20046;&#26222;&#36941;&#23384;&#22312;&#27867;&#21270;&#22833;&#25928;&#12290;&#20363;&#22914;&#65292;&#22312;&#20154;&#24418;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;PPO&#20195;&#29702;&#65292;&#22312;&#27491;&#24120;&#27979;&#35797;&#26399;&#38388;&#21482;&#26377;3.9&#65285;&#30340;&#22833;&#36133;&#29575;&#65292;&#20294;&#22312;10&#20010;&#38476;&#29983;&#20195;&#29702;&#30340;&#36712;&#36857;&#20013;&#65292;&#22833;&#36133;&#29575;&#21319;&#39640;&#21040;31.4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we define, evaluate, and improve the ``relay-generalization'' performance of reinforcement learning (RL) agents on the out-of-distribution ``controllable'' states. Ideally, an RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. For example, a self-driving system should be able to take over the control from humans in the middle of driving and must continue to drive the car safely. To practically evaluate this type of generalization, we start the test agent from the middle of other independently well-trained \emph{stranger} agents' trajectories. With extensive experimental evaluation, we show the prevalence of \emph{generalization failure} on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\% failure rate during regular testing, failed on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#32467;&#26597;&#35810;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#29305;&#24449;&#27169;&#22411;&#37197;&#32622;&#20219;&#21153;&#65292;&#20351;&#24471;&#33021;&#22815;&#21033;&#29992;&#25968;&#25454;&#24211;&#25216;&#26415;&#26469;&#35299;&#20915;&#37197;&#32622;&#38382;&#39064;&#24182;&#25552;&#20379;&#26032;&#30340;&#31639;&#27861;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13422</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#32467;&#26597;&#35810;&#30340;&#29305;&#24449;&#27169;&#22411;&#37197;&#32622;&#32422;&#26463;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Conjunctive Query Based Constraint Solving For Feature Model Configuration. (arXiv:2304.13422v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#32467;&#26597;&#35810;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#29305;&#24449;&#27169;&#22411;&#37197;&#32622;&#20219;&#21153;&#65292;&#20351;&#24471;&#33021;&#22815;&#21033;&#29992;&#25968;&#25454;&#24211;&#25216;&#26415;&#26469;&#35299;&#20915;&#37197;&#32622;&#38382;&#39064;&#24182;&#25552;&#20379;&#26032;&#30340;&#31639;&#27861;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#27169;&#22411;&#37197;&#32622;&#21487;&#20197;&#22522;&#20110;&#21508;&#31181;&#25512;&#29702;&#26041;&#27861;&#36827;&#34892;&#25903;&#25345;&#12290;&#20363;&#22914;&#65292;SAT&#27714;&#35299;&#12289;&#32422;&#26463;&#27714;&#35299;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;(ASP)&#12290;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#25216;&#26415;&#19987;&#19994;&#30693;&#35782;&#26469;&#23450;&#20041;&#21644;&#35299;&#20915;&#24213;&#23618;&#37197;&#32622;&#38382;&#39064;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#36890;&#24120;&#30001;&#24403;&#20170;&#30340;&#20851;&#31995;&#25968;&#25454;&#24211;&#31995;&#32479;&#25903;&#25345;&#30340;&#32852;&#32467;&#26597;&#35810;&#26469;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;(CSP)&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#26159;&#35299;&#20915;&#29305;&#24449;&#27169;&#22411;&#37197;&#32622;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#24212;&#29992;&#24191;&#27867;&#30340;&#25968;&#25454;&#24211;&#25216;&#26415;&#26469;&#35299;&#20915;&#37197;&#32622;&#20219;&#21153;&#65292;&#24182;&#22312;&#35782;&#21035;&#21644;&#35299;&#20915;&#19981;&#19968;&#33268;&#24615;&#26102;&#25552;&#20379;&#20102;&#26032;&#30340;&#31639;&#27861;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature model configuration can be supported on the basis of various types of reasoning approaches. Examples thereof are SAT solving, constraint solving, and answer set programming (ASP). Using these approaches requires technical expertise of how to define and solve the underlying configuration problem. In this paper, we show how to apply conjunctive queries typically supported by today's relational database systems to solve constraint satisfaction problems (CSP) and -- more specifically -- feature model configuration tasks. This approach allows the application of a wide-spread database technology to solve configuration tasks and also allows for new algorithmic approaches when it comes to the identification and resolution of inconsistencies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#36830;&#32493;&#23618;&#20013;&#30340;&#28388;&#27874;&#22120;&#30456;&#20284;&#24230;&#36827;&#34892;&#28388;&#27874;&#22120;&#21098;&#26525;&#30340;&#26041;&#27861;&#65288;FSCL&#65289;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#36816;&#31639;&#37327;&#31561;&#26041;&#38754;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.13397</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#32493;&#23618;&#20013;&#30340;&#28388;&#27874;&#22120;&#30456;&#20284;&#24230;&#36827;&#34892;&#28388;&#27874;&#22120;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Filter Pruning via Filters Similarity in Consecutive Layers. (arXiv:2304.13397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#36830;&#32493;&#23618;&#20013;&#30340;&#28388;&#27874;&#22120;&#30456;&#20284;&#24230;&#36827;&#34892;&#28388;&#27874;&#22120;&#21098;&#26525;&#30340;&#26041;&#27861;&#65288;FSCL&#65289;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#20934;&#30830;&#24615;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#36816;&#31639;&#37327;&#31561;&#26041;&#38754;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28388;&#27874;&#22120;&#21098;&#26525;&#26159;&#21387;&#32553;&#21644;&#21152;&#36895;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#24191;&#27867;&#37319;&#29992;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#24573;&#30053;&#20102;&#19981;&#21516;&#23618;&#20013;&#28388;&#27874;&#22120;&#21644;&#36890;&#36947;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29420;&#31435;&#22788;&#29702;&#27599;&#23618;&#26410;&#33021;&#21033;&#29992;&#36328;&#23618;&#20043;&#38388;&#30340;&#21327;&#20316;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30452;&#35266;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#21033;&#29992;&#36830;&#32493;&#23618;&#20013;&#30340;&#28388;&#27874;&#22120;&#30456;&#20284;&#24615;&#65288;FSCL&#65289;&#26469;&#21387;&#32553;&#27169;&#22411;&#12290;FSCL&#36890;&#36807;&#21098;&#26525;&#37027;&#20123;&#22312;&#27169;&#22411;&#20013;&#23545;&#24212;&#29305;&#24449;&#26356;&#21152;&#19981;&#37325;&#35201;&#30340;&#28388;&#27874;&#22120;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;FSCL&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#22312;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#22312;&#20934;&#30830;&#24615;&#65292;FLOPs&#21644;&#21442;&#25968;&#20943;&#23569;&#26041;&#38754;&#20135;&#29983;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Filter pruning is widely adopted to compress and accelerate the Convolutional Neural Networks (CNNs), but most previous works ignore the relationship between filters and channels in different layers. Processing each layer independently fails to utilize the collaborative relationship across layers. In this paper, we intuitively propose a novel pruning method by explicitly leveraging the Filters Similarity in Consecutive Layers (FSCL). FSCL compresses models by pruning filters whose corresponding features are more worthless in the model. The extensive experiments demonstrate the effectiveness of FSCL, and it yields remarkable improvement over state-of-the-art on accuracy, FLOPs and parameter reduction on several benchmark models and datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VGOS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#31232;&#30095;&#36755;&#20837;&#65288;3-10&#20010;&#35270;&#22270;&#65289;&#20013;&#37325;&#24314;&#25918;&#23556;&#22330;&#65292;&#20197;&#35299;&#20915;&#20307;&#32032;&#32593;&#26684;&#26356;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#37327;&#20307;&#32032;&#35757;&#32451;&#31574;&#30053;&#26469;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.13386</link><description>&lt;p&gt;
VGOS&#65306;&#26469;&#33258;&#31232;&#30095;&#36755;&#20837;&#30340;&#20307;&#32032;&#32593;&#26684;&#20248;&#21270;&#35270;&#35282;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs. (arXiv:2304.13386v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VGOS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#31232;&#30095;&#36755;&#20837;&#65288;3-10&#20010;&#35270;&#22270;&#65289;&#20013;&#37325;&#24314;&#25918;&#23556;&#22330;&#65292;&#20197;&#35299;&#20915;&#20307;&#32032;&#32593;&#26684;&#26356;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#37327;&#20307;&#32032;&#35757;&#32451;&#31574;&#30053;&#26469;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25918;&#23556;&#22330;&#65288;NeRF&#65289;&#30001;&#20110;&#20854;&#20248;&#24322;&#30340;&#36136;&#37327;&#21644;&#28789;&#27963;&#24615;&#22312;&#26032;&#39062;&#35270;&#35282;&#21512;&#25104;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;NeRF&#38656;&#35201;&#23494;&#38598;&#30340;&#36755;&#20837;&#35270;&#22270;&#65288;&#20960;&#21313;&#21040;&#20960;&#30334;&#20010;&#65289;&#21644;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#26102;&#38388;&#65288;&#20960;&#23567;&#26102;&#21040;&#20960;&#22825;&#65289;&#25165;&#33021;&#20026;&#21333;&#20010;&#22330;&#26223;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#12290;&#34429;&#28982;&#20351;&#29992;&#20307;&#32032;&#32593;&#26684;&#34920;&#31034;&#25918;&#23556;&#22330;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#65292;&#20294;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#22312;&#31232;&#30095;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#20307;&#32032;&#32593;&#26684;&#26356;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#35270;&#22270;&#65292;&#20250;&#20135;&#29983;&#31354;&#27934;&#21644;&#28418;&#28014;&#29289;&#65292;&#23548;&#33268;&#20266;&#24433;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VGOS&#65292;&#19968;&#31181;&#20174;&#31232;&#30095;&#36755;&#20837;&#65288;3-10&#20010;&#35270;&#22270;&#65289;&#24555;&#36895;&#65288;3-5&#20998;&#38047;&#65289;&#37325;&#24314;&#25918;&#23556;&#22330;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#25913;&#21892;&#22522;&#20110;&#20307;&#32032;&#30340;&#25918;&#23556;&#22330;&#22312;&#31232;&#30095;&#36755;&#20837;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;a&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#22686;&#37327;&#20307;&#32032;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#37325;&#24314;&#26089;&#26399;&#25233;&#21046;&#21608;&#36793;&#20307;&#32032;&#30340;&#20248;&#21270;&#65292;&#20197;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRF) has shown great success in novel view synthesis due to its state-of-the-art quality and flexibility. However, NeRF requires dense input views (tens to hundreds) and a long training time (hours to days) for a single scene to generate high-fidelity images. Although using the voxel grids to represent the radiance field can significantly accelerate the optimization process, we observe that for sparse inputs, the voxel grids are more prone to overfitting to the training views and will have holes and floaters, which leads to artifacts. In this paper, we propose VGOS, an approach for fast (3-5 minutes) radiance field reconstruction from sparse inputs (3-10 views) to address these issues. To improve the performance of voxel-based radiance field in sparse input scenarios, we propose two methods: (a) We introduce an incremental voxel training strategy, which prevents overfitting by suppressing the optimization of peripheral voxels in the early stage of reconstructio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35282;&#33394;&#30340;&#35775;&#38382;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21306;&#22359;&#38142;&#21644;&#26234;&#33021;&#21512;&#32422;&#20013;&#23454;&#29616;&#26469;&#20445;&#38556;&#22522;&#20110;&#20113;&#30340;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13379</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#26234;&#33021;&#24037;&#19994;&#31649;&#29702;&#31995;&#32479;&#35775;&#38382;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Blockchain-based Access Control for Secure Smart Industry Management Systems. (arXiv:2304.13379v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35282;&#33394;&#30340;&#35775;&#38382;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21306;&#22359;&#38142;&#21644;&#26234;&#33021;&#21512;&#32422;&#20013;&#23454;&#29616;&#26469;&#20445;&#38556;&#22522;&#20110;&#20113;&#30340;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#28041;&#21450;&#22823;&#37327;&#20114;&#32852;&#35774;&#22791;&#65292;&#23548;&#33268;&#22823;&#37327;&#25968;&#25454;&#29983;&#25104;&#12290;&#20113;&#35745;&#31639;&#25216;&#26415;&#36817;&#24180;&#26469;&#22312;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20419;&#36827;&#25104;&#26412;&#25928;&#30410;&#26381;&#21153;&#30340;&#25552;&#20379;&#21644;&#22823;&#37327;&#25968;&#25454;&#30340;&#31649;&#29702;&#12290;&#22312;&#22522;&#20110;&#20113;&#30340;&#21046;&#36896;&#31995;&#32479;&#20013;&#65292;&#30830;&#20445;&#23545;&#25968;&#25454;&#30340;&#25480;&#26435;&#35775;&#38382;&#33267;&#20851;&#37325;&#35201;&#12290;&#20113;&#24179;&#21488;&#30001;&#21333;&#19968;&#26435;&#23041;&#36816;&#33829;&#12290;&#22240;&#27492;&#65292;&#20113;&#24179;&#21488;&#23481;&#26131;&#25104;&#20026;&#21333;&#28857;&#25925;&#38556;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#21183;&#21147;&#30340;&#25915;&#20987;&#12290;&#20869;&#37096;&#25110;&#22806;&#37096;&#25932;&#23545;&#21183;&#21147;&#21487;&#20197;&#36731;&#26131;&#20462;&#25913;&#29992;&#25143;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#20197;&#20801;&#35768;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#35775;&#38382;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35282;&#33394;&#30340;&#35775;&#38382;&#25511;&#21046;&#26041;&#27861;&#65292;&#20511;&#21161;&#21306;&#22359;&#38142;&#21644;&#26234;&#33021;&#21512;&#32422;&#22312;&#22522;&#20110;&#20113;&#30340;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#38450;&#27490;&#20462;&#25913;&#25915;&#20987;&#12290;&#35813;&#22522;&#20110;&#35282;&#33394;&#30340;&#35775;&#38382;&#25511;&#21046;&#34987;&#24320;&#21457;&#20986;&#26469;&#26469;&#30830;&#23450;&#26234;&#33021;&#21512;&#32422;&#20013;&#29992;&#25143;&#30340;&#35282;&#33394;&#21644;&#26435;&#38480;&#12290;&#28982;&#21518;&#23558;&#26234;&#33021;&#21512;&#32422;&#37096;&#32626;&#21040;&#31169;&#26377;&#21306;&#22359;&#38142;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart manufacturing systems involve a large number of interconnected devices resulting in massive data generation. Cloud computing technology has recently gained increasing attention in smart manufacturing systems for facilitating cost-effective service provisioning and massive data management. In a cloud-based manufacturing system, ensuring authorized access to the data is crucial. A cloud platform is operated under a single authority. Hence, a cloud platform is prone to a single point of failure and vulnerable to adversaries. An internal or external adversary can easily modify users' access to allow unauthorized users to access the data. This paper proposes a role-based access control to prevent modification attacks by leveraging blockchain and smart contracts in a cloud-based smart manufacturing system. The role-based access control is developed to determine users' roles and rights in smart contracts. The smart contracts are then deployed to the private blockchain network. We evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;F^3&#65292;&#20351;&#29992;&#24310;&#36831;&#30340;&#35823;&#24046;&#20449;&#24687;&#26469;&#32553;&#25918;&#26799;&#24230;&#20174;&#32780;&#25552;&#39640;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20026;&#20302;&#33021;&#37327;&#35757;&#32451;&#21644;&#24182;&#34892;&#21270;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.13372</link><description>&lt;p&gt;
&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Feed-Forward Optimization With Delayed Feedback for Neural Networks. (arXiv:2304.13372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;F^3&#65292;&#20351;&#29992;&#24310;&#36831;&#30340;&#35823;&#24046;&#20449;&#24687;&#26469;&#32553;&#25918;&#26799;&#24230;&#20174;&#32780;&#25552;&#39640;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20026;&#20302;&#33021;&#37327;&#35757;&#32451;&#21644;&#24182;&#34892;&#21270;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#29983;&#29289;&#23398;&#19978;&#30340;&#25209;&#35780;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#33258;&#28982;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21487;&#34892;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#21363;&#26435;&#37325;&#20256;&#36755;&#21644;&#26356;&#26032;&#38145;&#23450;&#65292;&#20197;&#23454;&#29616;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#65288;F^3&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#24310;&#36831;&#30340;&#35823;&#24046;&#20449;&#24687;&#20316;&#20026;&#26679;&#26412;&#32423;&#32553;&#25918;&#22240;&#23376;&#26469;&#26356;&#20934;&#30830;&#22320;&#36817;&#20284;&#26799;&#24230;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;F^3&#23558;&#29983;&#29289;&#21487;&#34892;&#24615;&#35757;&#32451;&#31639;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#20043;&#38388;&#30340;&#39044;&#27979;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#20102;&#39640;&#36798;96&#65285;&#12290;&#36825;&#35777;&#26126;&#20102;&#29983;&#29289;&#21487;&#34892;&#24615;&#35757;&#32451;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20026;&#20302;&#33021;&#37327;&#35757;&#32451;&#21644;&#24182;&#34892;&#21270;&#24320;&#36767;&#20102;&#26377; promising &#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation has long been criticized for being biologically implausible, relying on concepts that are not viable in natural learning processes. This paper proposes an alternative approach to solve two core issues, i.e., weight transport and update locking, for biological plausibility and computational efficiency. We introduce Feed-Forward with delayed Feedback (F$^3$), which improves upon prior work by utilizing delayed error information as a sample-wise scaling factor to approximate gradients more accurately. We find that F$^3$ reduces the gap in predictive performance between biologically plausible training algorithms and backpropagation by up to 96%. This demonstrates the applicability of biologically plausible training and opens up promising new avenues for low-energy training and parallelization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#20010;&#22522;&#20110;LoRaWAN&#30340;&#26234;&#33021;&#26657;&#22253;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;k&#36817;&#37051;&#21644;LSTM&#26041;&#27861;&#22788;&#29702;&#20002;&#22833;&#20540;&#21644;&#39044;&#27979;&#26410;&#26469;&#35835;&#25968;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#25151;&#38388;&#20869;&#20154;&#25968;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;95&#65285;&#12290;</title><link>http://arxiv.org/abs/2304.13366</link><description>&lt;p&gt;
&#22522;&#20110;LoRaWAN&#30340;&#26234;&#33021;&#26657;&#22253;&#65306;&#25968;&#25454;&#38598;&#21644;&#20154;&#27969;&#35745;&#25968;&#22120;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
LoRaWAN-enabled Smart Campus: The Dataset and a People Counter Use Case. (arXiv:2304.13366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#20010;&#22522;&#20110;LoRaWAN&#30340;&#26234;&#33021;&#26657;&#22253;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;k&#36817;&#37051;&#21644;LSTM&#26041;&#27861;&#22788;&#29702;&#20002;&#22833;&#20540;&#21644;&#39044;&#27979;&#26410;&#26469;&#35835;&#25968;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#25151;&#38388;&#20869;&#20154;&#25968;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;95&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#22312;&#26234;&#33021;&#26657;&#22253;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;LoRaWAN&#30340;&#26234;&#33021;&#26657;&#22253;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#25551;&#36848;&#12290;LoRaWAN&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#21487;&#20197;&#20026;&#25968;&#30334;&#20010;&#29289;&#32852;&#32593;&#35774;&#22791;&#25552;&#20379;&#26381;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;&#35774;&#22791;&#36830;&#25509;&#21040;&#26381;&#21153;&#22120;&#30340;LoRa&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20002;&#22833;&#30340;&#20256;&#36755;&#24182;&#25552;&#20986;&#20102;k&#36817;&#37051;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#32570;&#22833;&#20540;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#35835;&#25968;&#12290;&#26368;&#21518;&#65292;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#22522;&#20110;&#25152;&#36873;&#20256;&#24863;&#22120;&#30340;&#35835;&#25968;&#39044;&#27979;&#25151;&#38388;&#20869;&#20154;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#39044;&#27979;&#20154;&#25968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;95&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#65292;&#24182;&#19988;&#26377;&#35814;&#32454;&#35828;&#26126;&#65292;&#36825;&#26159;&#25506;&#32034;&#20854;&#20182;&#21151;&#33021;&#21644;&#24212;&#29992;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoT has a significant role in the smart campus. This paper presents a detailed description of the Smart Campus dataset based on LoRaWAN. LoRaWAN is an emerging technology that enables serving hundreds of IoT devices. First, we describe the LoRa network that connects the devices to the server. Afterward, we analyze the missing transmissions and propose a k-nearest neighbor solution to handle the missing values. Then, we predict future readings using a long short-term memory (LSTM). Finally, as one example application, we build a deep neural network to predict the number of people inside a room based on the selected sensor's readings. Our results show that our model achieves an accuracy of $95 \: \%$ in predicting the number of people. Moreover, the dataset is openly available and described in detail, which is opportunity for exploration of other features and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#36328;&#36234;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#23398;&#26041;&#27861;&#36827;&#34892;&#38646;&#26679;&#26412;&#36328;&#32534;&#31243;&#35821;&#35328;&#20195;&#30721;&#25628;&#32034;&#30340;&#20013;&#38388;&#34920;&#31034;&#26041;&#24335;&#65292;&#24182;&#22312;COBOL&#32534;&#31243;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#20351;&#24471;&#20195;&#30721;&#20811;&#38534;&#20219;&#21153;&#30340;&#24615;&#33021;&#33719;&#24471;&#20102;12.85 MAP @ 2&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.13350</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#20013;&#38388;&#34920;&#31034;&#30340;&#31070;&#32463;&#31526;&#21495;&#24335;&#38646;&#26679;&#26412;&#20195;&#30721;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic Zero-Shot Code Cloning with Cross-Language Intermediate Representation. (arXiv:2304.13350v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#36328;&#36234;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#23398;&#26041;&#27861;&#36827;&#34892;&#38646;&#26679;&#26412;&#36328;&#32534;&#31243;&#35821;&#35328;&#20195;&#30721;&#25628;&#32034;&#30340;&#20013;&#38388;&#34920;&#31034;&#26041;&#24335;&#65292;&#24182;&#22312;COBOL&#32534;&#31243;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#20351;&#24471;&#20195;&#30721;&#20811;&#38534;&#20219;&#21153;&#30340;&#24615;&#33021;&#33719;&#24471;&#20102;12.85 MAP @ 2&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COBOL&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20195;&#30721;&#35821;&#20041;&#30456;&#20284;&#30340;&#20811;&#38534;&#20219;&#21153;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#65292;&#37319;&#29992;&#22312;C&#21644;COBOL&#20013;&#30340;&#20195;&#30721;&#37117;&#21487;&#29992;&#30340;ASTs&#65288;&#25277;&#35937;&#35821;&#27861;&#26641;&#65289;&#20316;&#20026;&#20013;&#38388;&#34920;&#31034;&#12290;&#20351;&#29992;&#22522;&#20110;&#32467;&#26500;&#36941;&#21382;&#65288;SBT&#65289;&#30340;&#26041;&#27861;&#23558;IRs&#32447;&#24615;&#21270;&#65292;&#20197;&#21019;&#24314;&#39034;&#24207;&#36755;&#20837;&#12290;&#36827;&#19968;&#27493;&#23545;UnixCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;CodeNet&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;SBT IR&#30340;C&#20195;&#30721;&#23545;&#36827;&#34892;&#38646;&#26679;&#26412;&#36328;&#32534;&#31243;&#35821;&#35328;&#20195;&#30721;&#25628;&#32034;&#30340;&#20219;&#21153;&#65292;&#20197;&#36827;&#34892;&#20195;&#30721;&#20811;&#38534;&#12290;&#20351;&#29992;&#36825;&#20010;&#31934;&#35843;&#30340;UnixCoder&#65292;&#30456;&#23545;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;UniXCoder&#27169;&#22411;&#65292;&#22312;COBOL&#27979;&#35797;&#25968;&#25454;&#20013;&#33719;&#24471;&#20102;12.85 MAP @ 2&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we define a neuro-symbolic approach to address the task of finding semantically similar clones for the codes of the legacy programming language COBOL, without training data. We define a meta-model that is instantiated to have an Intermediate Representation (IR) in the form of Abstract Syntax Trees (ASTs) common across codes in C and COBOL. We linearize the IRs using Structure Based Traversal (SBT) to create sequential inputs. We further fine-tune UnixCoder, the best-performing model for zero-shot cross-programming language code search, for the Code Cloning task with the SBT IRs of C code-pairs, available in the CodeNet dataset. This allows us to learn latent representations for the IRs of the C codes, which are transferable to the IRs of the COBOL codes. With this fine-tuned UnixCoder, we get a performance improvement of 12.85 MAP@2 over the pre-trained UniXCoder model, in a zero-shot setting, on the COBOL test split synthesized from the CodeNet dataset. This demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#24322;&#24341;&#23548;&#30340;&#22270;&#20687;&#31713;&#25913;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#23545;&#31713;&#25913;&#25935;&#24863;&#19988;&#20855;&#26377;&#32039;&#20945;&#35270;&#35273;&#27169;&#24335;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20855;&#26377;&#36739;&#24191;&#27867;&#30340;&#25512;&#24191;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13349</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#24322;&#24341;&#23548;&#37325;&#24314;&#23398;&#20064;&#30340;&#22270;&#20687;&#31713;&#25913;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discrepancy-Guided Reconstruction Learning for Image Forgery Detection. (arXiv:2304.13349v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#24322;&#24341;&#23548;&#30340;&#22270;&#20687;&#31713;&#25913;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#23545;&#31713;&#25913;&#25935;&#24863;&#19988;&#20855;&#26377;&#32039;&#20945;&#35270;&#35273;&#27169;&#24335;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20855;&#26377;&#36739;&#24191;&#27867;&#30340;&#25512;&#24191;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#31713;&#25913;&#26816;&#27979;&#33539;&#24335;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26082;&#25935;&#24863;&#20110;&#31713;&#25913;&#21448;&#20855;&#26377;&#32039;&#20945;&#35270;&#35273;&#27169;&#24335;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#31713;&#25913;&#29305;&#23450;&#27169;&#24335;&#65288;&#20363;&#22914;&#22122;&#22768;&#12289;&#32441;&#29702;&#21644;&#39057;&#29575;&#65289;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20855;&#26377;&#24191;&#27867;&#30340;&#25512;&#24191;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#24046;&#24322;&#24341;&#23548;&#32534;&#30721;&#22120;&#65288;DisGE&#65289;&#26469;&#25552;&#21462;&#25935;&#24863;&#20110;&#31713;&#25913;&#30340;&#35270;&#35273;&#27169;&#24335;&#12290;DisGE&#30001;&#20004;&#20010;&#20998;&#25903;&#32452;&#25104;&#65292;&#20854;&#20013;&#20027;&#27969;&#30340;&#39592;&#24178;&#20998;&#25903;&#29992;&#20110;&#25552;&#21462;&#19968;&#33324;&#35821;&#20041;&#29305;&#24449;&#65292;&#32780;&#36741;&#21161;&#30340;&#24046;&#24322;&#24615;&#22806;&#37096;&#27880;&#24847;&#21147;&#20998;&#25903;&#29992;&#20110;&#25552;&#21462;&#26126;&#30830;&#30340;&#31713;&#25913;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#22836;&#37325;&#24314;&#65288;DouHR&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;&#19981;&#21516;&#39063;&#31890;&#31354;&#38388;&#20013;&#30340;&#30495;&#23454;&#32039;&#20945;&#35270;&#35273;&#27169;&#24335;&#12290;&#22312;DouHR&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#24046;&#24322;&#32858;&#21512;&#26816;&#27979;&#22120;&#65288;DisAD&#65289;&#26469;&#32858;&#21512;&#36825;&#20123;&#30495;&#23454;&#32039;&#20945;&#30340;&#35270;&#35273;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#26410;&#30693;&#27169;&#24335;&#30340;&#31713;&#25913;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel image forgery detection paradigm for boosting the model learning capacity on both forgery-sensitive and genuine compact visual patterns. Compared to the existing methods that only focus on the discrepant-specific patterns (\eg, noises, textures, and frequencies), our method has a greater generalization. Specifically, we first propose a Discrepancy-Guided Encoder (DisGE) to extract forgery-sensitive visual patterns. DisGE consists of two branches, where the mainstream backbone branch is used to extract general semantic features, and the accessorial discrepant external attention branch is used to extract explicit forgery cues. Besides, a Double-Head Reconstruction (DouHR) module is proposed to enhance genuine compact visual patterns in different granular spaces. Under DouHR, we further introduce a Discrepancy-Aggregation Detector (DisAD) to aggregate these genuine compact visual patterns, such that the forgery detection capability on unknown patterns can
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#22312;HAR&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#22312;&#20808;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13327</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#20272;&#65306;&#24212;&#29992;&#20110;HAR
&lt;/p&gt;
&lt;p&gt;
Evaluation of Regularization-based Continual Learning Approaches: Application to HAR. (arXiv:2304.13327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#22312;HAR&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#22312;&#20808;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36866;&#35745;&#31639;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#39046;&#22495;&#20013;&#25552;&#20379;&#26381;&#21153;&#65292;&#21253;&#25324;&#20581;&#24247;&#21644;&#31119;&#21033;&#36825;&#20010;&#30456;&#20851;&#19988;&#21160;&#24577;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#22312;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#28436;&#36827;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#38500;&#38750;&#36827;&#34892;&#23436;&#25972;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36830;&#32493;&#23398;&#20064;&#30340;&#27010;&#24565;&#22312;&#20170;&#22825;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#38750;&#24120;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#31616;&#21333;&#24182;&#19988;&#25104;&#26412;&#20302;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38750;&#24120;&#19987;&#19994;&#21270;&#24182;&#19988;&#38590;&#20197;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#22312;HAR&#39046;&#22495;&#30340;&#20840;&#38754;&#27604;&#36739;&#65292;&#24182;&#31361;&#20986;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#22312;&#20808;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pervasive computing allows the provision of services in many important areas, including the relevant and dynamic field of health and well-being. In this domain, Human Activity Recognition (HAR) has gained a lot of attention in recent years. Current solutions rely on Machine Learning (ML) models and achieve impressive results. However, the evolution of these models remains difficult, as long as a complete retraining is not performed. To overcome this problem, the concept of Continual Learning is very promising today and, more particularly, the techniques based on regularization. These techniques are particularly interesting for their simplicity and their low cost. Initial studies have been conducted and have shown promising outcomes. However, they remain very specific and difficult to compare. In this paper, we provide a comprehensive comparison of three regularization-based methods that we adapted to the HAR domain, highlighting their strengths and limitations. Our experiments were con
&lt;/p&gt;</description></item><item><title>&#25506;&#31350;&#20102;AI&#22312;&#21019;&#36896;&#24615;&#34920;&#36798;&#20013;&#34920;&#36798;&#24773;&#24863;&#26041;&#38754;&#30340;&#28508;&#21147;&#19982;&#23616;&#38480;&#65292;AI&#21487;&#20197;&#20419;&#36827;&#21019;&#36896;&#21147;&#21644;&#24773;&#24863;&#30340;&#33258;&#25105;&#34920;&#36798;&#12290;</title><link>http://arxiv.org/abs/2304.13324</link><description>&lt;p&gt;
&#24773;&#24863;&#30340;&#30011;&#20687;&#65306;&#36890;&#36807;AI&#29983;&#25104;&#30340;&#33402;&#26415;&#36171;&#20104;&#33258;&#25105;&#34920;&#36798;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Portrait of Emotion: Empowering Self-Expression through AI-Generated Art. (arXiv:2304.13324v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13324
&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#20102;AI&#22312;&#21019;&#36896;&#24615;&#34920;&#36798;&#20013;&#34920;&#36798;&#24773;&#24863;&#26041;&#38754;&#30340;&#28508;&#21147;&#19982;&#23616;&#38480;&#65292;AI&#21487;&#20197;&#20419;&#36827;&#21019;&#36896;&#21147;&#21644;&#24773;&#24863;&#30340;&#33258;&#25105;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#36890;&#36807;&#21019;&#36896;&#24615;&#34920;&#36798;&#21453;&#26144;&#20316;&#32773;&#30340;&#35748;&#30693;&#36807;&#31243;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#37325;&#28857;&#22312;&#20110;AI&#29983;&#25104;&#30340;&#33402;&#26415;&#20316;&#21697;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65288;&#23545;&#40784;&#65289;&#21644;&#26681;&#25454;&#21019;&#24847;&#12289;&#32654;&#24863;&#12289;&#26032;&#39062;&#24615;&#12289;&#23089;&#20048;&#24615;&#21644;&#28145;&#24230;&#31561;&#26631;&#20934;&#35270;&#35273;&#21270;&#34920;&#29616;&#24773;&#24863;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20316;&#32773;&#24773;&#24863;&#25551;&#36848;&#30340;&#22270;&#20687;&#22312;&#20107;&#20214;&#20013;&#30340;&#34920;&#29616;&#26041;&#38754;&#26356;&#21463;&#27426;&#36814;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#36807;&#20998;&#24378;&#35843;&#26576;&#20123;&#20803;&#32032;&#25110;&#21051;&#26495;&#21360;&#35937;&#30340;&#22270;&#20687;&#20250;&#23545;AI&#30340;&#23545;&#40784;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#21487;&#20197;&#20419;&#36827;&#21019;&#36896;&#21147;&#21644;&#24773;&#24863;&#30340;&#33258;&#25105;&#34920;&#36798;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26694;&#26550;&#21487;&#29992;&#20110;&#35774;&#35745;&#30456;&#20851;&#39046;&#22495;&#65288;&#20363;&#22914;&#24515;&#29702;&#20581;&#24247;&#25945;&#32946;&#12289;&#27835;&#30103;&#21644;&#21672;&#35810;&#65289;&#20013;&#22522;&#20110;AI&#30340;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigated the potential and limitations of generative artificial intelligence (AI) in reflecting the authors' cognitive processes through creative expression. The focus is on the AI-generated artwork's ability to understand human intent (alignment) and visually represent emotions based on criteria such as creativity, aesthetic, novelty, amusement, and depth. Results show a preference for images based on the descriptions of the authors' emotions over the main events. We also found that images that overrepresent specific elements or stereotypes negatively impact AI alignment. Our findings suggest that AI could facilitate creativity and the self-expression of emotions. Our research framework with generative AIs can help design AI-based interventions in related fields (e.g., mental health education, therapy, and counseling).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#20934;&#30830;&#19988;&#31616;&#26126;&#22320;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#38024;&#23545;&#27492;&#30446;&#30340;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#35774;&#35745;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13312</link><description>&lt;p&gt;
&#25216;&#26415;&#31508;&#35760;&#65306;&#23450;&#20041;&#21644;&#37327;&#21270;DNN&#30340;AND-OR&#20132;&#20114;&#20197;&#36827;&#34892;&#20934;&#30830;&#21644;&#31616;&#26126;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs. (arXiv:2304.13312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#20934;&#30830;&#19988;&#31616;&#26126;&#22320;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#38024;&#23545;&#27492;&#30446;&#30340;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#35774;&#35745;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#37325;&#26032;&#24605;&#32771;&#20132;&#20114;&#30340;&#23450;&#20041;&#65292;&#28982;&#21518;&#27491;&#24335;&#23450;&#20041;&#20102;&#22522;&#20110;&#20132;&#20114;&#30340;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#12290;&#38024;&#23545;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AND&#65288;OR&#65289;&#20132;&#20114;&#22312;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;AND&#65288;OR&#65289;&#20851;&#31995;&#25928;&#24212;&#26041;&#38754;&#30340;&#21807;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;AND-OR&#20132;&#20114;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;DNN&#30340;&#25512;&#29702;&#36923;&#36753;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#31526;&#21495;&#27010;&#24565;&#20934;&#30830;&#32780;&#31616;&#26126;&#22320;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical note, we aim to explain a deep neural network (DNN) by quantifying the encoded interactions between input variables, which reflects the DNN's inference logic. Specifically, we first rethink the definition of interactions, and then formally define faithfulness and conciseness for interaction-based explanation. To this end, we propose two kinds of interactions, i.e., the AND interaction and the OR interaction. For faithfulness, we prove the uniqueness of the AND (OR) interaction in quantifying the effect of the AND (OR) relationship between input variables. Besides, based on AND-OR interactions, we design techniques to boost the conciseness of the explanation, while not hurting the faithfulness. In this way, the inference logic of a DNN can be faithfully and concisely explained by a set of symbolic concepts.
&lt;/p&gt;</description></item><item><title>HiQ&#26159;&#19968;&#31181;&#21487;&#36879;&#26126;&#30417;&#25511;Python&#31243;&#24207;&#36816;&#34892;&#26102;&#20449;&#24687;&#30340;&#31995;&#32479;&#65292;&#20855;&#26377;&#38750;&#20405;&#20837;&#24615;&#21644;&#21160;&#24577;&#24615;&#65292;&#21487;&#24212;&#29992;&#20110;&#31163;&#32447;/&#22312;&#32447;&#24212;&#29992;&#31243;&#24207;&#21644;&#20998;&#24067;&#24335;&#31995;&#32479;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#25429;&#25417;&#29942;&#39048;&#65292;&#32780;&#19981;&#24433;&#21709;&#20195;&#30721;&#30340;&#24178;&#20928;&#31243;&#24230;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13302</link><description>&lt;p&gt;
HiQ -- &#19968;&#31181;&#22768;&#26126;&#24615;&#12289;&#38750;&#20405;&#20837;&#24335;&#12289;&#21160;&#24577;&#21644;&#36879;&#26126;&#30340;&#21487;&#35266;&#23519;&#24615;&#21644;&#20248;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
HiQ -- A Declarative, Non-intrusive, Dynamic and Transparent Observability and Optimization System. (arXiv:2304.13302v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13302
&lt;/p&gt;
&lt;p&gt;
HiQ&#26159;&#19968;&#31181;&#21487;&#36879;&#26126;&#30417;&#25511;Python&#31243;&#24207;&#36816;&#34892;&#26102;&#20449;&#24687;&#30340;&#31995;&#32479;&#65292;&#20855;&#26377;&#38750;&#20405;&#20837;&#24615;&#21644;&#21160;&#24577;&#24615;&#65292;&#21487;&#24212;&#29992;&#20110;&#31163;&#32447;/&#22312;&#32447;&#24212;&#29992;&#31243;&#24207;&#21644;&#20998;&#24067;&#24335;&#31995;&#32479;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#25429;&#25417;&#29942;&#39048;&#65292;&#32780;&#19981;&#24433;&#21709;&#20195;&#30721;&#30340;&#24178;&#20928;&#31243;&#24230;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;HiQ&#8221;&#30340;&#38750;&#20405;&#20837;&#24335;&#12289;&#22768;&#26126;&#24615;&#12289;&#21160;&#24577;&#21644;&#36879;&#26126;&#31995;&#32479;&#65292;&#29992;&#20110;&#36319;&#36394;Python&#31243;&#24207;&#30340;&#36816;&#34892;&#26102;&#20449;&#24687;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#36816;&#34892;&#26102;&#31995;&#32479;&#24615;&#33021;&#21644;&#25439;&#22833;&#27934;&#23519;&#21147;&#12290; HiQ&#21487;&#20197;&#29992;&#20110;&#21333;&#29255;&#21644;&#20998;&#24067;&#24335;&#31995;&#32479;&#12289;&#31163;&#32447;&#21644;&#22312;&#32447;&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#24050;&#32463;&#23558;&#20854;&#29992;&#20110;&#20248;&#21270;&#20351;&#29992;Python&#32534;&#20889;&#30340;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#24182;&#21487;&#25512;&#24191;&#21040;&#20219;&#20309;Python&#31243;&#24207;&#25110;&#20998;&#24067;&#24335;&#31995;&#32479;&#65292;&#29978;&#33267;&#26159;Java&#31561;&#20854;&#20182;&#35821;&#35328;&#12290; &#25105;&#20204;&#24050;&#32463;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;HiQ&#65292;&#24182;&#37319;&#29992;&#23427;&#26469;&#25429;&#25417;&#29942;&#39048;&#65292;&#21516;&#26102;&#20445;&#25345;&#25105;&#20204;&#30340;&#29983;&#20135;&#20195;&#30721;&#24178;&#20928;&#19988;&#39640;&#24615;&#33021;&#12290; &#25105;&#20204;&#30340;&#23454;&#29616;&#24050;&#32463;&#22312; [https://github.com/oracle/hiq](https://github.com/oracle/hiq) &#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a non-intrusive, declarative, dynamic and transparent system called `HiQ` to track Python program runtime information without compromising on the run-time system performance and losing insight. HiQ can be used for monolithic and distributed systems, offline and online applications. HiQ is developed when we optimize our large deep neural network (DNN) models which are written in Python, but it can be generalized to any Python program or distributed system, or even other languages like Java. We have implemented the system and adopted it in our deep learning model life cycle management system to catch the bottleneck while keeping our production code clean and highly performant. The implementation is open-sourced at: [https://github.com/oracle/hiq](https://github.com/oracle/hiq).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;&#30340;&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#38480;&#21046;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13301</link><description>&lt;p&gt;
&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL. (arXiv:2304.13301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;&#30340;&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#38480;&#21046;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Codex&#12289;ChatGPT&#21644;GPT-4&#65289;&#22312;AI&#31038;&#21306;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#12290;&#19968;&#20123;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#20204;&#26377;&#28508;&#21147;&#29983;&#25104;SQL&#26597;&#35810;&#65292;&#20294;&#26159;&#23427;&#20204;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#65288;&#20363;&#22914;&#31616;&#21333;&#30340;&#32467;&#26500;&#25110;&#38543;&#26426;&#25277;&#26679;&#65289;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#19981;&#24517;&#35201;&#25110;&#26080;&#20851;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CBR-ApSQL&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#30340;&#26694;&#26550;&#65292;&#19982;GPT-3.5&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#23545;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#36827;&#34892;&#31934;&#30830;&#25511;&#21046;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#20197;&#28789;&#27963;&#35843;&#25972;GPT-3.5&#30340;&#36755;&#20837;&#65292;&#20854;&#20013;&#28041;&#21450;&#65288;1&#65289;&#36890;&#36807;&#21435;&#35821;&#20041;&#21270;&#36755;&#20837;&#38382;&#39064;&#26469;&#33258;&#36866;&#24212;&#26816;&#32034;&#26696;&#20363;&#65292;&#26681;&#25454;&#38382;&#39064;&#24847;&#22270;&#65292;&#20197;&#21450;&#65288;2&#65289;&#33258;&#36866;&#24212;&#22238;&#36864;&#26426;&#21046;&#65292;&#20197;&#30830;&#20445;&#25552;&#31034;&#30340;&#20449;&#24687;&#37327;&#21644;&#26696;&#20363;&#19982;&#25552;&#31034;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#21435;&#35821;&#20041;&#21270;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Semantic D
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs), such as Codex, ChatGPT and GPT-4 have significantly impacted the AI community, including Text-to-SQL tasks. Some evaluations and analyses on LLMs show their potential to generate SQL queries but they point out poorly designed prompts (e.g. simplistic construction or random sampling) limit LLMs' performance and may cause unnecessary or irrelevant outputs. To address these issues, we propose CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5 for precise control over case-relevant and case-irrelevant knowledge in Text-to-SQL tasks. We design adaptive prompts for flexibly adjusting inputs for GPT-3.5, which involves (1) adaptively retrieving cases according to the question intention by de-semantizing the input question, and (2) an adaptive fallback mechanism to ensure the informativeness of the prompt, as well as the relevance between cases and the prompt. In the de-semanticization phase, we designed Semantic D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#21644;&#21019;&#24847;&#35774;&#35745;&#22312;&#20854;&#20013;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.13269</link><description>&lt;p&gt;
&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Game-based Platforms for Artificial Intelligence Research. (arXiv:2304.13269v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#21644;&#21019;&#24847;&#35774;&#35745;&#22312;&#20854;&#20013;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#20855;&#26377;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#24191;&#27867;&#29305;&#24449;&#65292;&#25104;&#20026;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#29702;&#24819;&#27979;&#35797;&#22522;&#22320;&#65292;&#20849;&#21516;&#30340;&#30740;&#31350;&#39046;&#22495;&#21253;&#25324;&#23398;&#20064;&#21644;&#20248;&#21270;&#12289;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#12289;&#21338;&#24328;&#35770;&#12289;&#35745;&#21010;&#19982;&#25490;&#31243;&#12289;&#35774;&#35745;&#21644;&#25945;&#32946;&#31561;&#12290;&#24050;&#23454;&#26045;&#20102;&#35768;&#22810;&#24320;&#28304;&#28216;&#25103;&#25110;&#22522;&#20110;&#28216;&#25103;&#30340;&#29615;&#22659;&#29992;&#20110;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#12290;&#38500;&#20102;&#21333;&#20154;&#25110;&#22810;&#20154;&#12289;&#21512;&#20316;&#25110;&#23545;&#25239;&#24615;&#28216;&#25103;&#22806;&#65292;&#22312;&#21019;&#24847;&#35774;&#35745;&#26041;&#38754;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#24179;&#21488;&#20026;&#25506;&#32034;&#21644;&#27604;&#36739;&#20154;&#24037;&#26234;&#33021;&#30340;&#24605;&#24819;&#21644;&#25216;&#26415;&#25552;&#20379;&#20102;&#29702;&#24819;&#22522;&#20934;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#30001;&#36825;&#20123;&#24179;&#21488;&#28436;&#21464;&#24341;&#36215;&#30340;&#30740;&#31350;&#36235;&#21183;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Games have been the perfect test-beds for artificial intelligence research for the characteristics that widely exist in real-world scenarios. Learning and optimisation, decision making in dynamic and uncertain environments, game theory, planning and scheduling, design and education are common research areas shared between games and real-world problems. Numerous open-sourced games or game-based environments have been implemented for studying artificial intelligence. In addition to single- or multi-player, collaborative or adversarial games, there has also been growing interest in implementing platforms for creative design in recent years. Those platforms provide ideal benchmarks for exploring and comparing artificial intelligence ideas and techniques. This paper reviews the game-based platforms for artificial intelligence research, discusses the research trend induced by the evolution of those platforms, and gives an outlook.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#20998;&#31867;&#65292;&#21253;&#25324;&#23458;&#25143;&#31471;&#12289;&#26381;&#21153;&#22120;&#31471;&#21644;&#22522;&#20110;FL&#30340;BFL&#26041;&#27861;&#12290;BFL&#26159;&#35299;&#20915;&#29616;&#26377;FL&#26041;&#27861;&#20013;&#21463;&#38480;&#21644;&#21160;&#24577;&#30340;&#25968;&#25454;&#21644;&#26465;&#20214;&#12289;&#24322;&#36136;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#20998;&#26512;&#35299;&#37322;&#33021;&#21147;&#25361;&#25112;&#30340;&#26377;&#21069;&#36884;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13267</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Bayesian Federated Learning: A Survey. (arXiv:2304.13267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#20998;&#31867;&#65292;&#21253;&#25324;&#23458;&#25143;&#31471;&#12289;&#26381;&#21153;&#22120;&#31471;&#21644;&#22522;&#20110;FL&#30340;BFL&#26041;&#27861;&#12290;BFL&#26159;&#35299;&#20915;&#29616;&#26377;FL&#26041;&#27861;&#20013;&#21463;&#38480;&#21644;&#21160;&#24577;&#30340;&#25968;&#25454;&#21644;&#26465;&#20214;&#12289;&#24322;&#36136;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#20998;&#26512;&#35299;&#37322;&#33021;&#21147;&#25361;&#25112;&#30340;&#26377;&#21069;&#36884;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20197;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#24335;&#25972;&#21512;&#20102;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#12289;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;FL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#33021;&#21147;&#21463;&#21040;&#26377;&#38480;&#21644;&#21160;&#24577;&#30340;&#25968;&#25454;&#21644;&#26465;&#20214;&#65292;&#21253;&#25324;&#24322;&#36136;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#20998;&#26512;&#35299;&#37322;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#23545;BFL&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#22522;&#26412;&#27010;&#24565;&#65292;&#20854;&#22312;FL&#19978;&#19979;&#25991;&#20013;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20174;&#36125;&#21494;&#26031;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;BFL&#36827;&#34892;&#20998;&#31867;&#21644;&#35752;&#35770;&#12290;&#25105;&#20204;&#23558;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#30340;BFL&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;FL&#30340;BFL&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#35752;&#35770;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;BFL&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;BFL&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#20197;&#36827;&#19968;&#27493;&#28385;&#36275;&#29616;&#23454;&#29983;&#27963;&#20013;FL&#24212;&#29992;&#30340;&#22797;&#26434;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) demonstrates its advantages in integrating distributed infrastructure, communication, computing and learning in a privacy-preserving manner. However, the robustness and capabilities of existing FL methods are challenged by limited and dynamic data and conditions, complexities including heterogeneities and uncertainties, and analytical explainability. Bayesian federated learning (BFL) has emerged as a promising approach to address these issues. This survey presents a critical overview of BFL, including its basic concepts, its relations to Bayesian learning in the context of FL, and a taxonomy of BFL from both Bayesian and federated perspectives. We categorize and discuss client- and server-side and FL-based BFL methods and their pros and cons. The limitations of the existing BFL methods and the future directions of BFL research further address the intricate requirements of real-life FL applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BSDE&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#29616;&#26377;&#30340;&#20998;&#25968;&#20989;&#25968;&#30830;&#23450;&#21040;&#36798;&#25152;&#38656;&#32456;&#31471;&#20998;&#24067;&#25152;&#38656;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#20026;&#25193;&#25955;&#24314;&#27169;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#25193;&#25955;&#21453;&#28436;&#65292;&#26465;&#20214;&#25193;&#25955;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.13224</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;BSDE&#25193;&#25955;&#27169;&#22411;&#65306;&#21453;&#28436;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation. (arXiv:2304.13224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BSDE&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#29616;&#26377;&#30340;&#20998;&#25968;&#20989;&#25968;&#30830;&#23450;&#21040;&#36798;&#25152;&#38656;&#32456;&#31471;&#20998;&#24067;&#25152;&#38656;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#20026;&#25193;&#25955;&#24314;&#27169;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#25193;&#25955;&#21453;&#28436;&#65292;&#26465;&#20214;&#25193;&#25955;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;BSDE&#30340;&#25193;&#25955;&#27169;&#22411;&#20026;&#25193;&#25955;&#24314;&#27169;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#19982;&#20256;&#32479;&#30340;SDE-based&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#29616;&#26377;&#30340;&#20998;&#25968;&#20989;&#25968;&#30830;&#23450;&#21040;&#36798;&#25152;&#38656;&#32456;&#31471;&#20998;&#24067;&#25152;&#38656;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20351;&#29992;Lipschitz&#32593;&#32476;&#36827;&#34892;&#20998;&#25968;&#21305;&#37197;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#22312;&#25193;&#25955;&#21453;&#28436;&#65292;&#26465;&#20214;&#25193;&#25955;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20026;&#35299;&#20915;&#29616;&#23454;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proposed BSDE-based diffusion model represents a novel approach to diffusion modeling, which extends the application of stochastic differential equations (SDEs) in machine learning. Unlike traditional SDE-based diffusion models, our model can determine the initial conditions necessary to reach a desired terminal distribution by adapting an existing score function. We demonstrate the theoretical guarantees of the model, the benefits of using Lipschitz networks for score matching, and its potential applications in various areas such as diffusion inversion, conditional diffusion, and uncertainty quantification. Our work represents a contribution to the field of score-based generative learning and offers a promising direction for solving real-world problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29615;&#22659;&#23436;&#20840;&#26080;&#30693;&#21644;&#23436;&#32654;&#30693;&#35782;&#20043;&#38388;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23616;&#37096;&#27169;&#22411;&#21644;&#20445;&#25345;&#25968;&#25454;&#39537;&#21160;&#35843;&#25972;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.13223</link><description>&lt;p&gt;
&#20855;&#26377;&#23616;&#37096;&#21442;&#25968;&#27169;&#22411;&#30693;&#35782;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Partial Parametric Model Knowledge. (arXiv:2304.13223v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29615;&#22659;&#23436;&#20840;&#26080;&#30693;&#21644;&#23436;&#32654;&#30693;&#35782;&#20043;&#38388;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23616;&#37096;&#27169;&#22411;&#21644;&#20445;&#25345;&#25968;&#25454;&#39537;&#21160;&#35843;&#25972;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#36830;&#32493;&#25511;&#21046;&#65292;&#20197;&#22635;&#34917;&#22312;&#29615;&#22659;&#23436;&#20840;&#26080;&#30693;&#21644;&#23436;&#32654;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Partial Knowledge Least Squares Policy Iteration(PLSPI)&#65292;&#26082;&#20511;&#37492;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20063;&#20511;&#37492;&#20102;&#27169;&#22411;&#22522;&#30784;&#25511;&#21046;&#12290;&#23427;&#21033;&#29992;&#23616;&#37096;&#27169;&#22411;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#65292;&#24182;&#20445;&#30041;&#24378;&#21270;&#23398;&#20064;&#26397;&#21521;&#26368;&#20248;&#24615;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#35843;&#25972;&#12290;&#25105;&#20204;&#20197;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#20026;&#26696;&#20363;&#30740;&#31350;&#65307;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We adapt reinforcement learning (RL) methods for continuous control to bridge the gap between complete ignorance and perfect knowledge of the environment. Our method, Partial Knowledge Least Squares Policy Iteration (PLSPI), takes inspiration from both model-free RL and model-based control. It uses incomplete information from a partial model and retains RL's data-driven adaption towards optimal performance. The linear quadratic regulator provides a case study; numerical experiments demonstrate the effectiveness and resulting benefits of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;Pascal VOC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35821;&#20041;&#20998;&#21106;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;FCN&#20316;&#20026;&#22522;&#20934;&#24182;&#25913;&#36827;&#20102;&#20854;&#38382;&#39064;&#65292;&#26368;&#32456;&#21457;&#29616;&#20351;&#29992;ResNet&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2304.13216</link><description>&lt;p&gt;
&#21033;&#29992;CNN&#36827;&#34892;Pascal VOC&#30340;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Exploiting CNNs for Semantic Segmentation with Pascal VOC. (arXiv:2304.13216v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;Pascal VOC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35821;&#20041;&#20998;&#21106;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;FCN&#20316;&#20026;&#22522;&#20934;&#24182;&#25913;&#36827;&#20102;&#20854;&#38382;&#39064;&#65292;&#26368;&#32456;&#21457;&#29616;&#20351;&#29992;ResNet&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;Pascal VOC&#25968;&#25454;&#38598;&#19978;&#23545;&#35821;&#20041;&#20998;&#21106;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#20840;&#21367;&#31215;&#32593;&#32476;&#65288;FCN&#65289;&#20316;&#20026;&#22522;&#20934;&#65292;&#24182;&#38024;&#23545;&#20854;&#38382;&#39064;&#36827;&#34892;&#25913;&#36827;&#65292;&#21253;&#25324;&#20313;&#24358;&#36864;&#28779;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;ResNet&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a comprehensive study on semantic segmentation with the Pascal VOC dataset. Here, we have to label each pixel with a class which in turn segments the entire image based on the objects/entities present. To tackle this, we firstly use a Fully Convolution Network (FCN) baseline which gave 71.31% pixel accuracy and 0.0527 mean IoU. We analyze its performance and working and subsequently address the issues in the baseline with three improvements: a) cosine annealing learning rate scheduler(pixel accuracy: 72.86%, IoU: 0.0529), b) data augmentation(pixel accuracy: 69.88%, IoU: 0.0585) c) class imbalance weights(pixel accuracy: 68.98%, IoU: 0.0596). Apart from these changes in training pipeline, we also explore three different architectures: a) Our proposed model -- Advanced FCN (pixel accuracy: 67.20%, IoU: 0.0602) b) Transfer Learning with ResNet (Best performance) (pixel accuracy: 71.33%, IoU: 0.0926 ) c) U-Net(pixel accuracy: 72.15%, IoU: 0.0649). We observe that
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#24515;&#29702;&#20581;&#24247;&#20250;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#26500;&#24314;&#36131;&#20219;VMHA&#65292;&#20197;&#25552;&#20986;&#21518;&#32493;&#38382;&#39064;&#25110;&#25552;&#20379;&#30693;&#24773;&#22238;&#24212;&#65292;&#20016;&#23500;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.13191</link><description>&lt;p&gt;
&#38754;&#21521;&#24515;&#29702;&#20581;&#24247;&#30340;&#21487;&#35299;&#37322;&#21644;&#23433;&#20840;&#30340;&#20250;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable and Safe Conversational Agents for Mental Health: A Survey. (arXiv:2304.13191v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13191
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#24515;&#29702;&#20581;&#24247;&#20250;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#26500;&#24314;&#36131;&#20219;VMHA&#65292;&#20197;&#25552;&#20986;&#21518;&#32493;&#38382;&#39064;&#25110;&#25552;&#20379;&#30693;&#24773;&#22238;&#24212;&#65292;&#20016;&#23500;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#24515;&#29702;&#20581;&#24247;&#21161;&#25163;&#65288;VMHA&#65289;&#22312;&#25345;&#32493;&#25512;&#36827;&#65292;&#20197;&#25903;&#25345;&#27599;&#24180;6000&#19975;&#27425;&#21021;&#32423;&#21307;&#30103;&#20445;&#20581;&#23601;&#35786;&#21644;600&#19975;&#27425;&#24613;&#35786;&#23460;&#23601;&#35786;&#30340;&#36127;&#25285;&#36807;&#37325;&#30340;&#20840;&#29699;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#26159;&#30001;&#20020;&#24202;&#24515;&#29702;&#23398;&#23478;&#12289;&#31934;&#31070;&#31185;&#21307;&#24072;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30740;&#31350;&#20154;&#21592;&#20026;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#65288;CBT&#65289;&#26500;&#24314;&#30340;&#12290;&#30446;&#21069;&#65292;VMHA&#30340;&#20316;&#29992;&#26159;&#36890;&#36807;&#20449;&#24687;&#25552;&#20379;&#24773;&#24863;&#25903;&#25345;&#65292;&#37325;&#28857;&#19981;&#22312;&#19982;&#24739;&#32773;&#36827;&#34892;&#28145;&#20837;&#30340;&#21453;&#24605;&#23545;&#35805;&#12290;&#38656;&#35201;&#26356;&#20840;&#38754;&#12289;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#36127;&#36131;&#20219;&#30340;VMHA&#65292;&#20197;&#25552;&#20986;&#21518;&#32493;&#38382;&#39064;&#25110;&#25552;&#20379;&#30693;&#24773;&#22238;&#24212;&#12290;&#36825;&#39033;&#35843;&#26597;&#23545;&#29616;&#26377;&#30340;&#24515;&#29702;&#20581;&#24247;&#20250;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25209;&#21028;&#24615;&#23457;&#26597;&#65292;&#38543;&#21518;&#25552;&#20986;&#20102;&#20851;&#20110;VMHA&#25913;&#36827;&#30340;&#26032;&#35265;&#35299;&#65292;&#21253;&#25324;&#29615;&#22659;&#30693;&#35782;&#12289;&#25968;&#25454;&#38598;&#21644;&#23427;&#20204;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#20013;&#30340;&#26032;&#20852;&#35282;&#33394;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20016;&#23500;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual Mental Health Assistants (VMHAs) are seeing continual advancements to support the overburdened global healthcare system that gets 60 million primary care visits, and 6 million Emergency Room (ER) visits annually. These systems are built by clinical psychologists, psychiatrists, and Artificial Intelligence (AI) researchers for Cognitive Behavioral Therapy (CBT). At present, the role of VMHAs is to provide emotional support through information, focusing less on developing a reflective conversation with the patient. A more comprehensive, safe and explainable approach is required to build responsible VMHAs to ask follow-up questions or provide a well-informed response. This survey offers a systematic critical review of the existing conversational agents in mental health, followed by new insights into the improvements of VMHAs with contextual knowledge, datasets, and their emerging role in clinical decision support. We also provide new directions toward enriching the user experience
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25253;&#21578;&#20102;&#20960;&#20010;&#22522;&#20110;GPT-4&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#32534;&#31243;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#24403;&#21069;&#24037;&#20855;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#30721;&#29983;&#25104;&#34429;&#28982;&#24378;&#22823;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#39564;&#35777;&#20197;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;GPT-4&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#29616;&#26377;&#20195;&#30721;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#36136;&#37327;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#23454;&#36136;&#24615;&#35206;&#30422;&#33539;&#22260;&#30340;&#27979;&#35797;&#65292;&#20294;&#24212;&#29992;&#20110;&#30456;&#20851;&#20195;&#30721;&#26102;&#65292;&#35768;&#22810;&#27979;&#35797;&#22833;&#36133;&#20102;&#12290;</title><link>http://arxiv.org/abs/2304.13187</link><description>&lt;p&gt;
&#22522;&#20110;GPT-4&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#32534;&#31243;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
AI-assisted coding: Experiments with GPT-4. (arXiv:2304.13187v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25253;&#21578;&#20102;&#20960;&#20010;&#22522;&#20110;GPT-4&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#32534;&#31243;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#24403;&#21069;&#24037;&#20855;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#30721;&#29983;&#25104;&#34429;&#28982;&#24378;&#22823;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#39564;&#35777;&#20197;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;GPT-4&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#29616;&#26377;&#20195;&#30721;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#36136;&#37327;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#23454;&#36136;&#24615;&#35206;&#30422;&#33539;&#22260;&#30340;&#27979;&#35797;&#65292;&#20294;&#24212;&#29992;&#20110;&#30456;&#20851;&#20195;&#30721;&#26102;&#65292;&#35768;&#22810;&#27979;&#35797;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22312;&#26576;&#20123;&#35745;&#31639;&#26426;&#32534;&#31243;&#20219;&#21153;&#19978;&#24050;&#32463;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20960;&#20010;&#20351;&#29992;GPT-4&#29983;&#25104;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#23454;&#39564;&#12290;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#24403;&#21069;&#24037;&#20855;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#30721;&#29983;&#25104;&#34429;&#28982;&#24378;&#22823;&#65292;&#20294;&#38656;&#35201;&#20154;&#31867;&#30340;&#22823;&#37327;&#39564;&#35777;&#26469;&#30830;&#20445;&#20854;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;GPT-4&#37325;&#26500;&#29616;&#26377;&#20195;&#30721;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35813;&#20195;&#30721;&#22312;&#20960;&#20010;&#24050;&#26377;&#20195;&#30721;&#36136;&#37327;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;GPT-4&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#23454;&#36136;&#24615;&#35206;&#30422;&#33539;&#22260;&#30340;&#27979;&#35797;&#65292;&#20294;&#26159;&#24212;&#29992;&#20110;&#30456;&#20851;&#20195;&#30721;&#26102;&#65292;&#35768;&#22810;&#27979;&#35797;&#22833;&#36133;&#20102;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#32534;&#30721;&#24037;&#20855;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#20204;&#20173;&#38656;&#35201;&#20154;&#31867;&#22312;&#20854;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#20197;&#30830;&#20445;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) tools based on large language models have acheived human-level performance on some computer programming tasks. We report several experiments using GPT-4 to generate computer code. These experiments demonstrate that AI code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance. We also demonstrate that GPT-4 refactoring of existing code can significantly improve that code along several established metrics for code quality, and we show that GPT-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code. These findings suggest that while AI coding tools are very powerful, they still require humans in the loop to ensure validity and accuracy of the results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#35745;&#31639;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#32467;&#26500;&#21098;&#26525;&#65292;&#20351;&#20854;&#22312;&#26368;&#23567;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;20%&#20197;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.13164</link><description>&lt;p&gt;
&#36808;&#21521;&#35745;&#31639;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Compute-Optimal Transfer Learning. (arXiv:2304.13164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#35745;&#31639;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#32467;&#26500;&#21098;&#26525;&#65292;&#20351;&#20854;&#22312;&#26368;&#23567;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;20%&#20197;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#36801;&#31227;&#23398;&#20064;&#39046;&#22495;&#21457;&#29983;&#20102;&#37325;&#22823;&#21464;&#38761;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#36866;&#24212;&#24615;&#12290;&#20294;&#26159;&#65292;&#24494;&#35843;&#25110;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#35201;&#27714;&#21487;&#33021;&#20250;&#38459;&#30861;&#23427;&#20204;&#30340;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#35745;&#31639;&#25928;&#29575;&#19982;&#28176;&#36817;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#23398;&#20064;&#31639;&#27861;&#22312;&#35745;&#31639;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#23454;&#29616;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#32467;&#26500;&#21098;&#26525;&#65292;&#21487;&#20197;&#20351;&#23427;&#20204;&#22312;&#26368;&#23567;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#25552;&#20379;&#22810;&#31181;&#36801;&#31227;&#22330;&#26223;&#30340;Nevis'22&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20302;&#35745;&#31639;&#33539;&#22260;&#20869;&#65292;&#21098;&#26525;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#21367;&#31215;&#36807;&#28388;&#22120;&#21487;&#20197;&#24102;&#26469;&#36229;&#36807;20%&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of transfer learning is undergoing a significant shift with the introduction of large pretrained models which have demonstrated strong adaptability to a variety of downstream tasks. However, the high computational and memory requirements to finetune or use these models can be a hindrance to their widespread use. In this study, we present a solution to this issue by proposing a simple yet effective way to trade computational efficiency for asymptotic performance which we define as the performance a learning algorithm achieves as compute tends to infinity. Specifically, we argue that zero-shot structured pruning of pretrained models allows them to increase compute efficiency with minimal reduction in performance. We evaluate our method on the Nevis'22 continual learning benchmark that offers a diverse set of transfer scenarios. Our results show that pruning convolutional filters of pretrained models can lead to more than 20% performance improvement in low computational regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#19968;&#21442;&#25968;&#30340;&#31574;&#30053;Roll-Drop&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;dropout&#26469;&#32771;&#34385;&#35266;&#27979;&#22122;&#22768;&#65292;&#32780;&#19981;&#38656;&#35201;&#26174;&#24335;&#24314;&#27169;&#22122;&#22768;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#25311;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.13150</link><description>&lt;p&gt;
Roll-Drop&#65306;&#20351;&#29992;&#21333;&#19968;&#21442;&#25968;&#32771;&#34385;&#35266;&#27979;&#22122;&#22768;&#30340;&#26041;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Roll-Drop: accounting for observation noise with a single parameter. (arXiv:2304.13150v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#19968;&#21442;&#25968;&#30340;&#31574;&#30053;Roll-Drop&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;dropout&#26469;&#32771;&#34385;&#35266;&#27979;&#22122;&#22768;&#65292;&#32780;&#19981;&#38656;&#35201;&#26174;&#24335;&#24314;&#27169;&#22122;&#22768;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#25311;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Roll-Drop&#30340;&#31616;&#21333;&#31574;&#30053;&#65292;&#23427;&#22312;&#27169;&#25311;&#20013;&#20351;&#29992;dropout&#26469;&#32771;&#34385;&#37096;&#32626;&#26399;&#38388;&#35266;&#27979;&#22122;&#22768;&#32780;&#19981;&#38656;&#35201;&#26174;&#24335;&#22320;&#27169;&#25311;&#27599;&#20010;&#29366;&#24577;&#19979;&#30340;&#22122;&#22768;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#25311;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#27880;&#20837;&#20102;&#39640;&#36798;25%&#30340;&#35266;&#27979;&#22122;&#22768;&#26102;&#65292;&#35813;&#26041;&#27861;&#30340;&#25104;&#21151;&#29575;&#39640;&#36798;80%&#65292;&#40065;&#26834;&#24615;&#27604;&#22522;&#32447;&#26041;&#27861;&#39640;&#20986;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple strategy for sim-to-real in Deep-Reinforcement Learning (DRL) -- called Roll-Drop -- that uses dropout during simulation to account for observation noise during deployment without explicitly modelling its distribution for each state. DRL is a promising approach to control robots for highly dynamic and feedback-based manoeuvres, and accurate simulators are crucial to providing cheap and abundant data to learn the desired behaviour. Nevertheless, the simulated data are noiseless and generally show a distributional shift that challenges the deployment on real machines where sensor readings are affected by noise. The standard solution is modelling the latter and injecting it during training; while this requires a thorough system identification, Roll-Drop enhances the robustness to sensor noise by tuning only a single parameter. We demonstrate an 80% success rate when up to 25% noise is injected in the observations, with twice higher robustness than the baseline
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MBIB&#65292;&#19968;&#20010;&#23558;&#19981;&#21516;&#31867;&#22411;&#23186;&#20307;&#20559;&#35265;&#20998;&#20026;&#20849;&#21516;&#26694;&#26550;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#20197;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#25216;&#26415;&#65292;&#32467;&#26524;&#26174;&#31034;&#27809;&#26377;&#21333;&#19968;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#21516;&#26102;&#21457;&#29616;&#30740;&#31350;&#20852;&#36259;&#21644;&#36164;&#28304;&#20998;&#37197;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2304.13148</link><description>&lt;p&gt;
MBIB--&#39318;&#20010;&#23186;&#20307;&#20559;&#35265;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#38598;&#21512;&#30340;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
Introducing MBIB -- the first Media Bias Identification Benchmark Task and Dataset Collection. (arXiv:2304.13148v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MBIB&#65292;&#19968;&#20010;&#23558;&#19981;&#21516;&#31867;&#22411;&#23186;&#20307;&#20559;&#35265;&#20998;&#20026;&#20849;&#21516;&#26694;&#26550;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#20197;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#25216;&#26415;&#65292;&#32467;&#26524;&#26174;&#31034;&#27809;&#26377;&#21333;&#19968;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#65292;&#21516;&#26102;&#21457;&#29616;&#30740;&#31350;&#20852;&#36259;&#21644;&#36164;&#28304;&#20998;&#37197;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#26469;&#20998;&#32452;&#36825;&#20123;&#35780;&#20272;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23186;&#20307;&#20559;&#35265;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#65288;MBIB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#23186;&#20307;&#20559;&#35265;&#65288;&#20363;&#22914;&#65292;&#35821;&#35328;&#12289;&#35748;&#30693;&#12289;&#25919;&#27835;&#65289;&#20998;&#20026;&#19968;&#20010;&#20849;&#21516;&#30340;&#26694;&#26550;&#65292;&#20197;&#27979;&#35797;&#39044;&#27979;&#26816;&#27979;&#25216;&#26415;&#30340;&#27010;&#25324;&#21270;&#31243;&#24230;&#12290;&#22312;&#35780;&#20272;&#20102;115&#20010;&#25968;&#25454;&#38598;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;9&#20010;&#20219;&#21153;&#65292;&#20180;&#32454;&#25552;&#20986;&#20102;22&#20010;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#25216;&#26415;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;Transformer&#25216;&#26415;&#65288;&#20363;&#22914;T5&#12289;BART&#65289;&#35780;&#20272;MBIB&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20167;&#24680;&#35328;&#35770;&#12289;&#31181;&#26063;&#20559;&#35265;&#21644;&#24615;&#21035;&#20559;&#35265;&#26356;&#23481;&#26131;&#26816;&#27979;&#65292;&#20294;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#26576;&#20123;&#20559;&#35265;&#31867;&#22411;&#65292;&#20363;&#22914;&#65292;&#35748;&#30693;&#21644;&#25919;&#27835;&#20559;&#35265;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#21333;&#19968;&#25216;&#26415;&#21487;&#20197;&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#30740;&#31350;&#20852;&#36259;&#21644;&#36164;&#28304;&#20998;&#37197;&#22312;&#23186;&#20307;&#20559;&#35265;&#35782;&#21035;&#30340;&#20010;&#21035;&#20219;&#21153;&#19978;&#23384;&#22312;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although media bias detection is a complex multi-task problem, there is, to date, no unified benchmark grouping these evaluation tasks. We introduce the Media Bias Identification Benchmark (MBIB), a comprehensive benchmark that groups different types of media bias (e.g., linguistic, cognitive, political) under a common framework to test how prospective detection techniques generalize. After reviewing 115 datasets, we select nine tasks and carefully propose 22 associated datasets for evaluating media bias detection techniques. We evaluate MBIB using state-of-the-art Transformer techniques (e.g., T5, BART). Our results suggest that while hate speech, racial bias, and gender bias are easier to detect, models struggle to handle certain bias types, e.g., cognitive and political bias. However, our results show that no single technique can outperform all the others significantly. We also find an uneven distribution of research interest and resource allocation to the individual tasks in media 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#33021;&#26681;&#25454;&#31227;&#21160;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#23545;&#26223;&#35266;&#36827;&#34892;&#20998;&#23618;&#65292;&#36890;&#36807;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#23454;&#29616;&#20102;&#23545;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#24314;&#27169;&#65292;&#36866;&#29992;&#20110;&#20998;&#31867;&#23621;&#27665;&#21306;&#21644;&#21830;&#19994;&#21306;&#31561;&#19981;&#21516;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.13143</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#26102;&#31354;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Temporal Analysis of Spatiotemporal Data. (arXiv:2304.13143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#33021;&#26681;&#25454;&#31227;&#21160;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#23545;&#26223;&#35266;&#36827;&#34892;&#20998;&#23618;&#65292;&#36890;&#36807;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#23454;&#29616;&#20102;&#23545;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#24314;&#27169;&#65292;&#36866;&#29992;&#20110;&#20998;&#31867;&#23621;&#27665;&#21306;&#21644;&#21830;&#19994;&#21306;&#31561;&#19981;&#21516;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#27963;&#21160;&#30340;&#26102;&#38388;&#27169;&#24335;&#19982;&#29992;&#22320;&#31867;&#22411;&#20043;&#38388;&#23384;&#22312;&#30528;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#26681;&#25454;&#31227;&#21160;&#27963;&#21160;&#26102;&#38388;&#24207;&#21015;&#23545;&#26223;&#35266;&#36827;&#34892;&#20998;&#23618;&#12290;&#39318;&#20808;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#28982;&#21518;&#36890;&#36807;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#23558;&#20854;&#21387;&#32553;&#20026;&#20219;&#21153;&#26080;&#20851;&#30340;&#26102;&#38388;&#23884;&#20837;&#65292;&#35813;&#23884;&#20837;&#20445;&#30041;&#20102;&#26102;&#38388;&#24207;&#21015;&#20013;&#35266;&#23519;&#21040;&#30340;&#21608;&#26399;&#24615;&#26102;&#38388;&#27169;&#24335;&#12290;&#20687;&#32032;&#32423;&#30340;&#23884;&#20837;&#34987;&#36716;&#25442;&#20026;&#31867;&#20284;&#22270;&#20687;&#30340;&#36890;&#36947;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#20219;&#21153;&#30340;&#19979;&#28216;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#24314;&#27169;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26102;&#38388;&#23884;&#20837;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#34920;&#24449;&#65292;&#24182;&#19988;&#23545;&#20110;&#20998;&#31867;&#23621;&#27665;&#21306;&#21644;&#21830;&#19994;&#21306;&#31561;&#19981;&#21516;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exists a correlation between geospatial activity temporal patterns and type of land use. A novel self-supervised approach is proposed to stratify landscape based on mobility activity time series. First, the time series signal is transformed to the frequency domain and then compressed into task-agnostic temporal embeddings by a contractive autoencoder, which preserves cyclic temporal patterns observed in time series. The pixel-wise embeddings are converted to image-like channels that can be used for task-based, multimodal modeling of downstream geospatial tasks using deep semantic segmentation. Experiments show that temporal embeddings are semantically meaningful representations of time series data and are effective across different tasks such as classifying residential area and commercial areas.
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#38024;&#23545;&#22686;&#26448;&#21046;&#36896;&#35797;&#20214;&#30340;&#34920;&#38754;&#31895;&#31961;&#24230;&#20351;&#29992;&#19977;&#31181;&#37327;&#23376;&#31639;&#27861;&#65288;QNN&#12289;Q-Forest&#21644;VQC&#65289;&#36827;&#34892;&#22238;&#24402;&#39044;&#27979;&#65292;&#20854;&#20013;Q-Forest&#31639;&#27861;&#34920;&#29616;&#26368;&#20248;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;MSE&#21644;MAE&#21644;&#36739;&#39640;&#30340;EVS&#12290;</title><link>http://arxiv.org/abs/2304.13142</link><description>&lt;p&gt;
&#38754;&#21521;&#22686;&#26448;&#21046;&#36896;&#35797;&#20214;&#34920;&#38754;&#31895;&#31961;&#24230;&#39044;&#27979;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning Approach for the Prediction of Surface Roughness in Additive Manufactured Specimens. (arXiv:2304.13142v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13142
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#38024;&#23545;&#22686;&#26448;&#21046;&#36896;&#35797;&#20214;&#30340;&#34920;&#38754;&#31895;&#31961;&#24230;&#20351;&#29992;&#19977;&#31181;&#37327;&#23376;&#31639;&#27861;&#65288;QNN&#12289;Q-Forest&#21644;VQC&#65289;&#36827;&#34892;&#22238;&#24402;&#39044;&#27979;&#65292;&#20854;&#20013;Q-Forest&#31639;&#27861;&#34920;&#29616;&#26368;&#20248;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;MSE&#21644;MAE&#21644;&#36739;&#39640;&#30340;EVS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#38754;&#31895;&#31961;&#24230;&#26159;&#24433;&#21709;&#22686;&#26448;&#21046;&#36896;&#38646;&#20214;&#24615;&#33021;&#21644;&#21151;&#33021;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20934;&#30830;&#39044;&#27979;&#34920;&#38754;&#31895;&#31961;&#24230;&#23545;&#20110;&#20248;&#21270;&#21046;&#36896;&#36807;&#31243;&#21644;&#30830;&#20445;&#26368;&#32456;&#20135;&#21697;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#37327;&#23376;&#35745;&#31639;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#21644;&#21019;&#24314;&#31934;&#30830;&#39044;&#27979;&#27169;&#22411;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#38024;&#23545;&#22686;&#26448;&#21046;&#36896;&#35797;&#20214;&#30340;&#34920;&#38754;&#31895;&#31961;&#24230;&#36827;&#34892;&#20102;&#19977;&#31181;&#37327;&#23376;&#31639;&#27861;&#65292;&#21253;&#25324;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#12289;&#37327;&#23376;&#26862;&#26519;&#65288;Q-Forest&#65289;&#21644;&#21464;&#20998;&#37327;&#23376;&#20998;&#31867;&#22120;&#65288;VQC&#65289;&#30340;&#22238;&#24402;&#36866;&#24212;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#12289;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#21644;&#35299;&#37322;&#26041;&#24046;&#24471;&#20998;&#65288;EVS&#65289;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;Q-Forest&#31639;&#27861;&#36229;&#36234;&#20102;&#20854;&#20182;&#31639;&#27861;&#65292;MSE&#20026;56.905&#65292;MAE&#20026;7.479&#65292;EVS&#20026;0.2957&#12290;
&lt;/p&gt;
&lt;p&gt;
Surface roughness is a crucial factor influencing the performance and functionality of additive manufactured components. Accurate prediction of surface roughness is vital for optimizing manufacturing processes and ensuring the quality of the final product. Quantum computing has recently gained attention as a potential solution for tackling complex problems and creating precise predictive models. In this research paper, we conduct an in-depth comparison of three quantum algorithms i.e. the Quantum Neural Network (QNN), Quantum Forest (Q-Forest), and Variational Quantum Classifier (VQC) adapted for regression for predicting surface roughness in additive manufactured specimens for the first time. We assess the algorithms performance using Mean Squared Error (MSE), Mean Absolute Error (MAE), and Explained Variance Score (EVS) as evaluation metrics. Our findings show that the Q-Forest algorithm surpasses the other algorithms, achieving an MSE of 56.905, MAE of 7.479, and an EVS of 0.2957. I
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13138</link><description>&lt;p&gt;
&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#30340;&#26356;&#26032;&#31561;&#20215;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Update Equivalence Framework for Decision-Time Planning. (arXiv:2304.13138v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26827;&#31867;&#28216;&#25103;&#31561;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#20013;&#65292;&#21363;&#26102;&#20462;&#27491;&#65288;&#25110;&#26500;&#24314;&#65289;&#31574;&#30053;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26159;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#30340;&#20851;&#38190;&#12290;&#19968;&#20123;&#30740;&#31350;&#23558;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#25193;&#23637;&#21040;&#26356;&#26222;&#36941;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25169;&#20811;&#20013;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#32771;&#34385;&#38543;&#30528;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#30340;&#22686;&#21152;&#32780;&#24555;&#36895;&#22686;&#38271;&#30340;&#23376;&#28216;&#25103;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#36739;&#22823;&#26102;&#19981;&#36215;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#32780;&#19981;&#26159;&#23376;&#28216;&#25103;&#27010;&#24565;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#27169;&#25311;&#21516;&#27493;&#23398;&#20064;&#31639;&#27861;&#30340;&#26356;&#26032;&#12290;&#36825;&#20010;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#24341;&#20837;&#19968;&#31995;&#21015;&#21407;&#21017;&#19978;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#24182;&#20026;&#26032;&#30340;&#19968;&#20010;&#31995;&#21015;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of revising (or constructing) a policy immediately prior to execution -- known as decision-time planning -- is key to achieving superhuman performance in perfect-information settings like chess and Go. A recent line of work has extended decision-time planning to more general imperfect-information settings, leading to superhuman performance in poker. However, these methods requires considering subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on subgames but rather on the notion of update equivalence. In this framework, decision-time planning algorithms simulate updates of synchronous learning algorithms. This framework enables us to introduce a new family of principled decision-time planning algorithms that do not rely on public information, opening the door to sound and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26041;&#26696;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20107;&#20214;&#36880;&#20107;&#20214;&#22810;&#26222;&#21202;&#20462;&#27491;&#65292;&#23454;&#29616;&#23545;&#24555;&#36895;&#12289;&#28909;&#30340;&#24322;&#20301;&#21516;&#20301;&#32032;&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#28608;&#20809;&#20809;&#35889;&#23398;&#30740;&#31350;&#65292;&#22312;&#26497;&#31471;&#28201;&#24230;&#19979;&#20173;&#33021;&#23454;&#29616;kHz&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13120</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20107;&#20214;&#36880;&#20107;&#20214;&#22810;&#26222;&#21202;&#20462;&#27491;&#36827;&#34892;&#24555;&#36895;&#12289;&#28909;&#30340;&#24322;&#20301;&#21516;&#20301;&#32032;&#30340;&#39640;&#31934;&#24230;&#20809;&#35889;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Precision Spectroscopy of Fast, Hot Exotic Isotopes Using Machine Learning Assisted Event-by-Event Doppler Correction. (arXiv:2304.13120v1 [nucl-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13120
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26041;&#26696;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20107;&#20214;&#36880;&#20107;&#20214;&#22810;&#26222;&#21202;&#20462;&#27491;&#65292;&#23454;&#29616;&#23545;&#24555;&#36895;&#12289;&#28909;&#30340;&#24322;&#20301;&#21516;&#20301;&#32032;&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#28608;&#20809;&#20809;&#35889;&#23398;&#30740;&#31350;&#65292;&#22312;&#26497;&#31471;&#28201;&#24230;&#19979;&#20173;&#33021;&#23454;&#29616;kHz&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#26041;&#26696;,&#29992;&#20110;&#22312;&#24555;&#36895;&#30340;&#24322;&#20301;&#21516;&#20301;&#32032;&#19978;&#36827;&#34892;&#25935;&#24863;&#12289;&#39640;&#31934;&#24230;&#30340;&#28608;&#20809;&#20809;&#35889;&#23398;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#30005;&#22330;&#20869;&#35825;&#23548;&#21407;&#23376;&#30340;&#36880;&#27493;&#20849;&#25391;&#30005;&#31163;,&#28982;&#21518;&#26816;&#27979;&#31163;&#23376;&#21644;&#30456;&#24212;&#30340;&#30005;&#23376;,&#21487;&#20197;&#36827;&#34892;&#26102;&#38388;&#21644;&#20301;&#32622;&#25935;&#24863;&#30340;&#32467;&#26524;&#31890;&#23376;&#30340;&#27979;&#37327;&#12290;&#20351;&#29992;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476; (MDN),&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#23545;&#21333;&#20010;&#21407;&#23376;&#30340;&#21021;&#22987;&#33021;&#37327;&#36827;&#34892;&#39044;&#27979;,&#20174;&#32780;&#22312;&#20107;&#20214;&#36880;&#20107;&#20214;&#30340;&#22522;&#30784;&#19978;&#24212;&#29992;&#22810;&#26222;&#21202;&#20462;&#27491;&#25152;&#35266;&#23519;&#21040;&#30340;&#36291;&#36801;&#39057;&#29575;&#12290;&#25105;&#20204;&#23545;&#35813;&#25552;&#35758;&#30340;&#23454;&#39564;&#26041;&#26696;&#36827;&#34892;&#25968;&#20540;&#27169;&#25311;,&#24182;&#34920;&#26126;&#21487;&#20197;&#23454;&#29616;kHz&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;,&#23545;&#22312;&#26497;&#31471;&#28201;&#24230; ($&gt; 10^8$ K) &#19979;&#20135;&#29983;&#30340;&#31163;&#23376;&#26463;&#36827;&#34892;&#65292;&#22312;&#33021;&#37327;&#20998;&#25955;&#26368;&#22823;&#21487;&#36798;10 keV&#21644;&#36895;&#24230;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#24773;&#20917;&#19979;&#12290;&#33021;&#22815;&#30452;&#25509;&#22312;&#39640;&#33021;&#26463;&#19978;&#36827;&#34892;&#39134;&#34892;&#35889;&#23398;&#30740;&#31350;,&#20026;&#30740;&#31350;&#30701;&#23551;&#21629;&#30340;&#31163;&#23376;&#30340;&#29289;&#29702;&#21644;&#21270;&#23398;&#24615;&#36136;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an experimental scheme for performing sensitive, high-precision laser spectroscopy studies on fast exotic isotopes. By inducing a step-wise resonant ionization of the atoms travelling inside an electric field and subsequently detecting the ion and the corresponding electron, time- and position-sensitive measurements of the resulting particles can be performed. Using a Mixture Density Network (MDN), we can leverage this information to predict the initial energy of individual atoms and thus apply a Doppler correction of the observed transition frequencies on an event-by-event basis. We conduct numerical simulations of the proposed experimental scheme and show that kHz-level uncertainties can be achieved for ion beams produced at extreme temperatures ($&gt; 10^8$ K), with energy spreads as large as $10$ keV and non-uniform velocity distributions. The ability to perform in-flight spectroscopy, directly on highly energetic beams, offers unique opportunities to studying short-lived i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#26377;&#38480;CSI&#30340;&#24773;&#20917;&#19979;&#36805;&#36895;&#36827;&#34892;THz&#27874;&#26463;&#25628;&#32034;&#65292;&#20197;&#20811;&#26381;THz&#20449;&#21495;&#30340;&#20256;&#25773;&#34928;&#20943;&#12290;</title><link>http://arxiv.org/abs/2304.13109</link><description>&lt;p&gt;
&#26377;&#38480;CSI&#19979;&#30340;THz&#27874;&#26463;&#25628;&#32034;&#30340;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Deep Reinforcement Learning for THz-Beam Search with Limited CSI. (arXiv:2304.13109v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#26377;&#38480;CSI&#30340;&#24773;&#20917;&#19979;&#36805;&#36895;&#36827;&#34892;THz&#27874;&#26463;&#25628;&#32034;&#65292;&#20197;&#20811;&#26381;THz&#20449;&#21495;&#30340;&#20256;&#25773;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
THz&#36890;&#20449;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#39640;&#25968;&#25454;&#29575;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#65292;&#28982;&#32780;&#20854;&#20005;&#37325;&#30340;&#20256;&#25773;&#34928;&#20943;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;FDRL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36816;&#33829;&#21830;&#32593;&#32476;&#20013;&#65292;&#30001;&#36793;&#32536;&#26381;&#21153;&#22120;&#21327;&#35843;&#30340;&#22810;&#20010;&#22522;&#31449;&#65288;BS&#65289;&#36805;&#36895;&#25191;&#34892;THz&#27874;&#26463;&#25628;&#32034;&#12290;&#25152;&#26377;BS&#37117;&#36827;&#34892;&#22522;&#20110;DDPG&#65288;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65289;&#30340;DRL&#20197;&#33719;&#24471;&#20855;&#26377;&#26377;&#38480;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;THz&#27874;&#26463;&#25104;&#24418;&#31574;&#30053;&#12290;&#20182;&#20204;&#20351;&#29992;&#38544;&#34255;&#20449;&#24687;&#26356;&#26032;&#20182;&#20204;&#30340;DDPG&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#36328;&#23567;&#21306;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Terahertz (THz) communication with ultra-wide available spectrum is a promising technique that can achieve the stringent requirement of high data rate in the next-generation wireless networks, yet its severe propagation attenuation significantly hinders its implementation in practice. Finding beam directions for a large-scale antenna array to effectively overcome severe propagation attenuation of THz signals is a pressing need. This paper proposes a novel approach of federated deep reinforcement learning (FDRL) to swiftly perform THz-beam search for multiple base stations (BSs) coordinated by an edge server in a cellular network. All the BSs conduct deep deterministic policy gradient (DDPG)-based DRL to obtain THz beamforming policy with limited channel state information (CSI). They update their DDPG models with hidden information in order to mitigate inter-cell interference. We demonstrate that the cell network can achieve higher throughput as more THz CSI and hidden neurons of DDPG a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#25552;&#21462;&#20154;&#20307;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#26102;&#38388;-selective&#29305;&#24449;&#25552;&#21462;&#31639;&#27861;&#21306;&#20998;&#26377;&#30452;&#35273;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.13107</link><description>&lt;p&gt;
&#22522;&#20110;WiFi CSI&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#30340;&#26102;&#38388;&#36873;&#25321;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Time-Selective RNN for Device-Free Multi-Room Human Presence Detection Using WiFi CSI. (arXiv:2304.13107v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#25552;&#21462;&#20154;&#20307;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#26102;&#38388;-selective&#29305;&#24449;&#25552;&#21462;&#31639;&#27861;&#21306;&#20998;&#26377;&#30452;&#35273;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23384;&#22312;&#26816;&#27979;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#21253;&#25324;&#23478;&#23621;&#33258;&#21160;&#21270;&#12289;&#23433;&#20840;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#37319;&#29992;&#22522;&#20110;&#25668;&#20687;&#26426;&#30340;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#20294;&#20250;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21830;&#29992;WiFi&#25509;&#20837;&#28857;&#25552;&#20379;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#26041;&#27861;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#20449;&#36947;&#29305;&#24449;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#36873;&#25321;&#24615;&#26465;&#20214;&#21452;&#29305;&#24449;&#25552;&#21462;&#36882;&#24402;&#32593;&#32476;(TCD-FERN)&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26088;&#22312;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;(DaS)&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#22312;&#26465;&#20214;&#20154;&#20307;&#29305;&#24449;&#19979;&#25429;&#25417;&#37325;&#35201;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#25552;&#21462;&#20154;&#30340;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#24182;&#21306;&#20998;&#26377;&#30452;&#25509;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#20943;&#23569;&#25151;&#38388;&#38548;&#26029;&#36896;&#25104;&#30340;&#29305;&#24449;&#34928;&#20943;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110; LSTM &#30340; NCoV-DaS &#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human presence detection is a crucial technology for various applications, including home automation, security, and healthcare. While camera-based systems have traditionally been used for this purpose, they raise privacy concerns. To address this issue, recent research has explored the use of channel state information (CSI) approaches that can be extracted from commercial WiFi access points (APs) and provide detailed channel characteristics. In this thesis, we propose a device-free human presence detection system for multi-room scenarios using a time-selective conditional dual feature extract recurrent Network (TCD-FERN). Our system is designed to capture significant time features with the condition on current human features using a dynamic and static (DaS) data preprocessing technique to extract moving and spatial features of people and differentiate between line-of-sight (LoS) path blocking and non-blocking cases. To mitigate the feature attenuation problem caused by room partitions,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13105</link><description>&lt;p&gt;
&#21033;&#29992;&#23460;&#20869;WiFi&#31995;&#32479;&#36827;&#34892;&#26080;&#35774;&#22791;&#31359;&#22681;&#23384;&#22312;&#26816;&#27979;&#30340;&#27880;&#24847;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20934;&#30830;&#26816;&#27979;&#20154;&#21592;&#23384;&#22312;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#33021;&#28304;&#31649;&#29702;&#21644;&#23433;&#20840;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#30340;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21517;&#20026;&#27880;&#24847;&#21147;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#65288;ALPD&#65289;&#65292;&#37319;&#29992;&#20851;&#27880;&#26426;&#21046;&#20174;CSI&#25968;&#25454;&#20013;&#33258;&#21160;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#23376;&#36733;&#27874;&#65292;&#24182;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#25429;&#25417;CSI&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#38745;&#24577;&#29305;&#24449;&#26469;&#25552;&#39640;&#38745;&#24577;&#29366;&#24577;&#19979;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#19968;&#23545;WiFi&#25509;&#20837;&#28857;&#65288;AP&#65289;&#26469;&#25910;&#38598;CSI&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;ALPD&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36827;&#19968;&#27493;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ALPD&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#21452;&#21521;&#20256;&#36755;&#25968;&#25454;&#19981;&#20250;&#24433;&#21709;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate detection of human presence in indoor environments is important for various applications, such as energy management and security. In this paper, we propose a novel system for human presence detection using the channel state information (CSI) of WiFi signals. Our system named attention-enhanced deep learning for presence detection (ALPD) employs an attention mechanism to automatically select informative subcarriers from the CSI data and a bidirectional long short-term memory (LSTM) network to capture temporal dependencies in CSI. Additionally, we utilize a static feature to improve the accuracy of human presence detection in static states. We evaluate the proposed ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI dataset, which is further compared with several benchmarks. The results demonstrate that our ALPD system outperforms the benchmarks in terms of accuracy, especially in the presence of interference. Moreover, bidirectional transmission data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LSTM&#31070;&#32463;&#32593;&#32476;&#22312;&#24494;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#20013;&#21463;&#21040;&#22122;&#22768;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#20302;&#36890;&#28388;&#27874;&#22120;&#28040;&#38500;&#20102;&#25915;&#20987;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13104</link><description>&lt;p&gt;
&#22522;&#20110;LSTM&#30340;&#24494;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#23545;&#25239;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LSTM-based Load Forecasting Robustness Against Noise Injection Attack in Microgrid. (arXiv:2304.13104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LSTM&#31070;&#32463;&#32593;&#32476;&#22312;&#24494;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#20013;&#21463;&#21040;&#22122;&#22768;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#20302;&#36890;&#28388;&#27874;&#22120;&#28040;&#38500;&#20102;&#25915;&#20987;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LSTM&#31070;&#32463;&#32593;&#32476;&#22312;&#29702;&#24819;&#24494;&#30005;&#32593;&#20013;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#20013;&#23545;&#25239;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604;&#19979;&#36827;&#34892;&#30340;&#40657;&#30418;&#39640;&#26031;&#22122;&#22768;&#25915;&#20987;&#19979;LSTM&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20551;&#35774;&#25915;&#20987;&#32773;&#21482;&#33021;&#35775;&#38382;LSTM&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22122;&#22768;&#25915;&#20987;&#24433;&#21709;&#20102;LSTM&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#27491;&#24120;&#39044;&#27979;&#65292;&#36127;&#33655;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.047 MW&#65292;&#32780;&#23545;&#20110;&#20449;&#22122;&#27604;&#20026;6 dB&#30340;&#39640;&#26031;&#22122;&#22768;&#25554;&#20837;&#65292;&#35813;&#20540;&#22686;&#21152;&#21040;&#20102;0.097 MW&#12290;&#20026;&#20102;&#20351;LSTM&#27169;&#22411;&#23545;&#22122;&#22768;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#23558;&#26368;&#20339;&#25130;&#27490;&#39057;&#29575;&#30340;&#20302;&#36890;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#27169;&#22411;&#30340;&#36755;&#20837;&#20197;&#28040;&#38500;&#22122;&#22768;&#25915;&#20987;&#12290;&#35813;&#28388;&#27874;&#22120;&#22312;&#20302;&#20449;&#22122;&#27604;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#23545;&#20110;&#23567;&#22122;&#22768;&#30340;&#25928;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the robustness of an LSTM neural network against noise injection attacks for electric load forecasting in an ideal microgrid. The performance of the LSTM model is investigated under a black-box Gaussian noise attack with different SNRs. It is assumed that attackers have just access to the input data of the LSTM model. The results show that the noise attack affects the performance of the LSTM model. The load prediction means absolute error (MAE) is 0.047 MW for a healthy prediction, while this value increases up to 0.097 MW for a Gaussian noise insertion with SNR= 6 dB. To robustify the LSTM model against noise attack, a low-pass filter with optimal cut-off frequency is applied at the model's input to remove the noise attack. The filter performs better in case of noise with lower SNR and is less promising for small noises.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HyMo &#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102; FastText &#35789;&#23884;&#20837;&#25216;&#26415;&#21644; BiGRU &#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#26234;&#33021;&#21512;&#32422;&#28431;&#27934;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.13103</link><description>&lt;p&gt;
HyMo: &#19968;&#31181;&#26032;&#22411;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#22312;&#26234;&#33021;&#21512;&#32422;&#28431;&#27934;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
HyMo: Vulnerability Detection in Smart Contracts using a Novel Multi-Modal Hybrid Model. (arXiv:2304.13103v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HyMo &#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102; FastText &#35789;&#23884;&#20837;&#25216;&#26415;&#21644; BiGRU &#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#26234;&#33021;&#21512;&#32422;&#28431;&#27934;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26234;&#33021;&#21512;&#32422;&#24050;&#25104;&#20026;&#37329;&#34701;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#20445;&#38505;&#21644;&#28216;&#25103;&#31561;&#35768;&#22810;&#34892;&#19994;&#20013;&#30340;&#24120;&#29992;&#24037;&#20855;&#12290;&#26234;&#33021;&#21512;&#32422;&#25968;&#37327;&#24050;&#32463;&#25104;&#20493;&#22686;&#21152;&#65292;&#21516;&#26102;&#30001;&#20110;&#26234;&#33021;&#21512;&#32422;&#28431;&#27934;&#24102;&#26469;&#30340;&#32463;&#27982;&#25439;&#22833;&#65292;&#26234;&#33021;&#21512;&#32422;&#30340;&#23433;&#20840;&#24615;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#20998;&#26512;&#25216;&#26415;&#33021;&#22815;&#35782;&#21035;&#22823;&#37327;&#30340;&#26234;&#33021;&#21512;&#32422;&#23433;&#20840;&#28431;&#27934;&#65292;&#20294;&#23427;&#20204;&#36807;&#20110;&#20381;&#36182;&#19987;&#23478;&#24314;&#31435;&#30340;&#21018;&#24615;&#26631;&#20934;&#65292;&#26816;&#27979;&#36807;&#31243;&#38543;&#30528;&#26234;&#33021;&#21512;&#32422;&#30340;&#22797;&#26434;&#31243;&#24230;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#21152;&#32791;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; HyMo &#20316;&#20026;&#19968;&#31181;&#22810;&#27169;&#24577;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#26234;&#33021;&#22320;&#32771;&#34385;&#22810;&#31181;&#36755;&#20837;&#34920;&#31034;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;&#65292;&#24182;&#20351;&#29992; FastText &#35789;&#23884;&#20837;&#25216;&#26415;&#23558;&#27599;&#20010;&#21333;&#35789;&#34920;&#31034;&#20026;&#23383;&#31526;&#30340; n-gram&#65292;&#19982;&#30001;&#20004;&#20010; GRUs &#32452;&#25104;&#30340;&#24207;&#21015;&#22788;&#29702;&#27169;&#22411;&#30340; BiGRU &#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25645;&#37197;&#65292;&#20197;&#23454;&#29616;&#26234;&#33021;&#21512;&#32422;&#28431;&#27934;&#26816;&#27979;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With blockchain technology rapidly progress, the smart contracts have become a common tool in a number of industries including finance, healthcare, insurance and gaming. The number of smart contracts has multiplied, and at the same time, the security of smart contracts has drawn considerable attention due to the monetary losses brought on by smart contract vulnerabilities. Existing analysis techniques are capable of identifying a large number of smart contract security flaws, but they rely too much on rigid criteria established by specialists, where the detection process takes much longer as the complexity of the smart contract rises. In this paper, we propose HyMo as a multi-modal hybrid deep learning model, which intelligently considers various input representations to consider multimodality and FastText word embedding technique, which represents each word as an n-gram of characters with BiGRU deep learning technique, as a sequence processing model that consists of two GRUs to achiev
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#65292;&#20026;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.13081</link><description>&lt;p&gt;
&#26032;&#20852;&#25216;&#26415;&#30340;&#32452;&#32455;&#27835;&#29702;&#65306;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Organizational Governance of Emerging Technologies: AI Adoption in Healthcare. (arXiv:2304.13081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#65292;&#20026;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#33829;&#21644;&#20844;&#20849;&#37096;&#38376;&#30340;&#32467;&#26500;&#21644;&#35268;&#33539;&#31934;&#32454;&#21270;&#20102;&#26032;&#20852;&#25216;&#26415;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#23613;&#31649;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;AI&#37319;&#29992;&#26041;&#24335;&#65292;&#20294;&#26159;&#20854;&#20351;&#29992;&#21644;&#25972;&#21512;&#21608;&#22260;&#30340;&#32452;&#32455;&#27835;&#29702;&#24448;&#24448;&#34987;&#35748;&#20026;&#19981;&#21487;&#34892;&#12290;&#20581;&#24247;AI&#21512;&#20316;&#20249;&#20276;&#20851;&#31995;&#65288;HAIP&#65289;&#26088;&#22312;&#36890;&#36807;&#27492;&#30740;&#31350;&#26356;&#22909;&#22320;&#23450;&#20041;&#21307;&#30103;&#20445;&#20581;&#20013;AI&#31995;&#32479;&#30340;&#20805;&#20998;&#32452;&#32455;&#27835;&#29702;&#35201;&#27714;&#65292;&#24182;&#25903;&#25345;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#35201;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#37319;&#29992;&#30340;&#26631;&#20934;&#22914;&#20309;&#26131;&#20110;&#20351;&#29992;&#21644;&#39640;&#25928;&#36816;&#20316;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#29305;&#23450;&#30340;&#21355;&#29983;&#31995;&#32479;&#20013;&#65292;&#32472;&#21046;&#20986;&#23454;&#38469;&#26426;&#26500;&#37319;&#29992;AI&#25216;&#26415;&#30340;&#20855;&#20307;&#20915;&#31574;&#28857;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#31181;&#21512;&#20316;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working w
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#26641;&#33683;&#27966;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#20462;&#21098;&#25216;&#26415;&#21644;&#27169;&#22411;&#21442;&#25968;&#32467;&#26500;&#20248;&#21270;&#65292;&#20197;&#36866;&#24212;&#20854;&#30828;&#20214;&#29305;&#28857;&#24182;&#25552;&#39640;&#33021;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.13039</link><description>&lt;p&gt;
&#38024;&#23545;&#26641;&#33683;&#27966;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Optimizing Deep Learning Models For Raspberry Pi. (arXiv:2304.13039v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13039
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26641;&#33683;&#27966;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#20462;&#21098;&#25216;&#26415;&#21644;&#27169;&#22411;&#21442;&#25968;&#32467;&#26500;&#20248;&#21270;&#65292;&#20197;&#36866;&#24212;&#20854;&#30828;&#20214;&#29305;&#28857;&#24182;&#25552;&#39640;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#36825;&#31867;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#20351;&#24471;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#36816;&#34892;&#65288;&#22914;&#26641;&#33683;&#27966;&#65289;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20462;&#21098;&#65288;pruning&#65289;&#25216;&#26415;&#21644;&#20248;&#21270;&#27169;&#22411;&#20197;&#36866;&#21512;&#26641;&#33683;&#27966;&#31561;&#30828;&#20214;&#26550;&#26500;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#25110;&#35757;&#32451;&#21518;&#20462;&#21098;&#21487;&#20197;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#35753;&#27169;&#22411;&#26356;&#39640;&#25928;&#12290;&#20248;&#21270;&#27169;&#22411;&#21017;&#21253;&#25324;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#21644;&#32467;&#26500;&#36866;&#24212;&#26641;&#33683;&#27966;&#30828;&#20214;&#29305;&#28857;&#65292;&#20363;&#22914;&#26641;&#33683;&#27966;&#30340;CPU&#21644;GPU&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#35745;&#31639;&#37327;&#20197;&#23454;&#29616;&#33021;&#32791;&#30340;&#20248;&#21270;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have become increasingly popular for a wide range of applications, including computer vision, natural language processing, and speech recognition. However, these models typically require large amounts of computational resources, making them challenging to run on low-power devices such as the Raspberry Pi. One approach to addressing this challenge is to use pruning techniques to reduce the size of the deep learning models. Pruning involves removing unimportant weights and connections from the model, resulting in a smaller and more efficient model. Pruning can be done during training or after the model has been trained. Another approach is to optimize the deep learning models specifically for the Raspberry Pi architecture. This can include optimizing the model's architecture and parameters to take advantage of the Raspberry Pi's hardware capabilities, such as its CPU and GPU. Additionally, the model can be optimized for energy efficiency by minimizing the amount of c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#24418;&#24335;&#65292;&#26500;&#36896;&#21508;&#31181;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#22270;&#23884;&#20837;&#65292;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.13032</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#27880;&#37322;&#22270;&#24418;&#25968;&#25454;&#24182;&#24212;&#29992;&#20110;&#36719;&#20214;&#20195;&#30721;&#24615;&#33021;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction. (arXiv:2304.13032v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13032
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#24418;&#24335;&#65292;&#26500;&#36896;&#21508;&#31181;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#22270;&#23884;&#20837;&#65292;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#31243;&#24207;&#65292;&#21253;&#25324;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#24037;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#21644;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20123;&#21487;&#33021;&#20107;&#20808;&#24182;&#19981;&#21487;&#29992;&#12290;&#33719;&#21462;&#27880;&#37322;&#36890;&#24120;&#38656;&#35201;&#26174;&#30528;&#30340;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20351;&#24471;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#19987;&#38376;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#24320;&#22987;&#65292;&#28982;&#21518;&#29992;&#25968;&#25454;&#21644;&#25511;&#21046;&#27969;&#36793;&#26469;&#22686;&#24378;&#23427;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#28304;&#20195;&#30721;&#30340;&#26641;&#24418;&#34920;&#31034;&#36716;&#25442;&#20026;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#65288;FA-AST&#65289;&#34920;&#31034;&#27861;&#12290;&#22522;&#20110;&#22270;&#24418;&#34920;&#31034;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#22270;&#23884;&#20837;&#65288;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#65289;&#26500;&#36896;&#25104;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#12290;&#37492;&#20110;&#36825;&#26679;&#30340;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#21464;&#24471;&#20219;&#21153;&#19981;&#21487;&#30693;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#22238;&#24402;&#26041;&#27861;&#21644;&#36866;&#29992;&#20110;&#22238;&#24402;&#30340;&#26597;&#35810;&#31574;&#30053;&#26469;&#25191;&#34892;&#20027;&#21160;&#23398;&#20064;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#29992;&#20110;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#31574;&#30053;&#21644;&#22238;&#24402;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning and data analytics applications, including performance engineering in software systems, require a large number of annotations and labelled data, which might not be available in advance. Acquiring annotations often requires significant time, effort, and computational resources, making it challenging. We develop a unified active learning framework, specializing in software performance prediction, to address this task. We begin by parsing the source code to an Abstract Syntax Tree (AST) and augmenting it with data and control flow edges. Then, we convert the tree representation of the source code to a Flow Augmented-AST graph (FA-AST) representation. Based on the graph representation, we construct various graph embeddings (unsupervised and supervised) into a latent space. Given such an embedding, the framework becomes task agnostic since active learning can be performed using any regression method and query strategy suited for regression. Within this framework, we in
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#24335;&#20027;&#20041;&#26469;&#25805;&#20316;&#21644;&#20998;&#26512;&#35777;&#26126;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#30701;&#30340;&#35777;&#26126;&#21644;&#20943;&#23569;&#25628;&#32034;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.12827</link><description>&lt;p&gt;
&#35777;&#26126;&#32467;&#26500;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigations into Proof Structures. (arXiv:2304.12827v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12827
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#24335;&#20027;&#20041;&#26469;&#25805;&#20316;&#21644;&#20998;&#26512;&#35777;&#26126;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#30701;&#30340;&#35777;&#26126;&#21644;&#20943;&#23569;&#25628;&#32034;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#19968;&#31181;&#26032;&#22411;&#24418;&#24335;&#20027;&#20041;&#26469;&#25805;&#20316;&#21644;&#20998;&#26512;&#35777;&#26126;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#30340;&#23545;&#35937;&#12290;&#22312;&#36825;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#65292;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20165;&#38480;&#20110;&#30001;&#27987;&#32553;&#25512;&#23548;&#29305;&#24449;&#30340;&#19968;&#38454;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#19968;&#20010;&#20840;&#38754;&#30340;&#24418;&#24335;&#37325;&#26500;&#21644;&#20998;&#26512;&#21382;&#21490;&#19978;{\L}ukasiewicz&#24191;&#27867;&#30740;&#31350;&#36807;&#30340;&#38382;&#39064;&#30340;&#35777;&#26126;&#20026;&#20363;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#22312;&#35777;&#26126;&#25628;&#32034;&#36807;&#31243;&#20013;&#29983;&#25104;&#24341;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#25628;&#32034;&#24037;&#20316;&#37327;&#24182;&#25214;&#21040;&#26356;&#30701;&#30340;&#35777;&#26126;&#12290;&#22312;&#36825;&#26465;&#36335;&#32447;&#19978;&#25253;&#21578;&#20102;&#35768;&#22810;&#23454;&#39564;&#65292;&#20854;&#20013;&#33258;&#21160;&#21457;&#29616;&#20102;&#19968;&#20010;&#35777;&#26126;{\L}ukasiewicz&#30340;&#38382;&#39064;&#65292;&#23427;&#27604;&#20197;&#21069;&#20219;&#20309;&#30001;&#20154;&#25110;&#26426;&#22120;&#21457;&#29616;&#30340;&#35777;&#26126;&#37117;&#35201;&#30701;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and elaborate a novel formalism for the manipulation and analysis of proofs as objects in a global manner. In this first approach the formalism is restricted to first-order problems characterized by condensed detachment. It is applied in an exemplary manner to a coherent and comprehensive formal reconstruction and analysis of historical proofs of a widely-studied problem due to {\L}ukasiewicz. The underlying approach opens the door towards new systematic ways of generating lemmas in the course of proof search to the effects of reducing the search effort and finding shorter proofs. Among the numerous reported experiments along this line, a proof of {\L}ukasiewicz's problem was automatically discovered that is much shorter than any proof found before by man or machine.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#33268;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#32422;&#26463;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#32454;&#21270;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#22833;&#30495;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12591</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#33268;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#32422;&#26463;&#36827;&#34892;&#26080;&#30417;&#30563;&#21512;&#25104;&#22270;&#20687;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Synthetic Image Refinement via Contrastive Learning and Consistent Semantic and Structure Constraints. (arXiv:2304.12591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#33268;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#32422;&#26463;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#32454;&#21270;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#22833;&#30495;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#35745;&#31639;&#26426;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#30340;&#30495;&#23454;&#24615;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#35821;&#20041;&#20998;&#24067;&#65292;&#22240;&#27492;&#21512;&#25104;&#21644;&#32454;&#21270;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#36827;&#32780;&#23548;&#33268;&#35821;&#20041;&#22833;&#30495;&#12290;&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#23558;&#30456;&#20851;&#34917;&#19969;&#25289;&#22312;&#19968;&#36215;&#24182;&#23558;&#19981;&#30456;&#20851;&#30340;&#34917;&#19969;&#25512;&#24320;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21512;&#25104;&#21644;&#31934;&#32454;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;CL&#26469;&#20943;&#23569;&#35821;&#20041;&#22833;&#30495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#30828;&#36127;&#37319;&#26679;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#24615;&#21644;&#23450;&#37327;&#25514;&#26045;&#27604;&#36739;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#20960;&#31181;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring the realism of computer-generated synthetic images is crucial to deep neural network (DNN) training. Due to different semantic distributions between synthetic and real-world captured datasets, there exists semantic mismatch between synthetic and refined images, which in turn results in the semantic distortion. Recently, contrastive learning (CL) has been successfully used to pull correlated patches together and push uncorrelated ones apart. In this work, we exploit semantic and structural consistency between synthetic and refined images and adopt CL to reduce the semantic distortion. Besides, we incorporate hard negative mining to improve the performance furthermore. We compare the performance of our method with several other benchmarking methods using qualitative and quantitative measures and show that our method offers the state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#24449;&#30456;&#20851;&#24615;&#22312;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#35823;&#23548;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11597</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning Partial Correlation based Deep Visual Representation for Image Classification. (arXiv:2304.11597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#24449;&#30456;&#20851;&#24615;&#22312;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#35823;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35270;&#35273;&#34920;&#31034;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23545;&#21367;&#31215;&#29305;&#24449;&#26144;&#23556;&#20013;&#19981;&#21516;&#36890;&#36947;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#23384;&#22312;&#21478;&#19968;&#20010;&#36890;&#36947;&#19982;&#24863;&#20852;&#36259;&#30340;&#20004;&#20010;&#36890;&#36947;&#30456;&#20851;&#65292;&#21017;&#25104;&#23545;&#30456;&#20851;&#24615;&#23558;&#21464;&#24471;&#35823;&#23548;&#20154;&#65292;&#23548;&#33268;&#8220;&#28151;&#28102;&#8221;&#25928;&#24212;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#24212;&#35813;&#20272;&#35745;&#8220;&#20559;&#30456;&#20851;&#8221;&#65292;&#20197;&#28040;&#38500;&#28151;&#28102;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#21487;&#38752;&#22320;&#20272;&#35745;&#20559;&#30456;&#20851;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#31232;&#30095;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#65288;SICE&#65289;&#12290;&#22914;&#20309;&#23558;&#27492;&#36807;&#31243;&#34701;&#20837;CNN&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;SICE&#21046;&#23450;&#20026;CNN&#30340;&#19968;&#20010;&#26032;&#32467;&#26500;&#23618;&#12290;&#20026;&#30830;&#20445;&#31471;&#21040;&#31471;&#30340;&#21487;&#35757;&#32451;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#22312;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#25773;&#27493;&#39588;&#20013;&#35299;&#20915;&#19978;&#36848;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33719;&#24471;&#20102;&#22522;&#20110;&#20559;&#30456;&#20851;&#30340;&#28145;&#24230;&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual representation based on covariance matrix has demonstrates its efficacy for image classification by characterising the pairwise correlation of different channels in convolutional feature maps. However, pairwise correlation will become misleading once there is another channel correlating with both channels of interest, resulting in the ``confounding'' effect. For this case, ``partial correlation'' which removes the confounding effect shall be estimated instead. Nevertheless, reliably estimating partial correlation requires to solve a symmetric positive definite matrix optimisation, known as sparse inverse covariance estimation (SICE). How to incorporate this process into CNN remains an open issue. In this work, we formulate SICE as a novel structured layer of CNN. To ensure end-to-end trainability, we develop an iterative method to solve the above matrix optimisation during forward and backward propagation steps. Our work obtains a partial correlation based deep visual representa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11490</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#26234;&#29702;&#35770;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#38656;&#35201;&#29702;&#35299;&#20195;&#29702;&#20154;&#30340;&#20449;&#24565;&#12289;&#30446;&#26631;&#21644;&#24515;&#29702;&#29366;&#24577;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#30340;&#24120;&#35782;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#25552;&#39640;LLM&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#27979;&#37327;&#20102;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#65288;Davinci-2&#12289;Davinci-3&#12289;GPT-3.5-Turbo&#65289;&#30340;ToM&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#23427;&#20204;&#30340;ToM&#29702;&#35299;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21253;&#21547;&#20004;&#27493;&#24605;&#32500;&#25512;&#29702;&#21644;&#36880;&#27493;&#24605;&#32771;&#35828;&#26126;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#35757;&#32451;&#30340;LLMs&#65288;&#38500;Davinci-2&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;ToM&#20934;&#30830;&#24615;&#12290;GPT-4&#22312;&#38646;&#36718;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;&#36817;80%&#30340;ToM&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#19981;&#36275;&#27979;&#35797;&#38598;&#19978;87%&#30340;&#20154;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#25552;&#20379;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25552;&#31034;&#26102;&#65292;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#30340;ToM&#20934;&#30830;&#24615;&#26174;&#33879;&#39640;&#20110;&#26080;&#25552;&#31034;&#26102;&#65292;&#20854;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#65288;GPT-3.5-Turbo&#65289;&#36798;&#21040;&#20102;92%&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#21319;LLM&#22312;&#22797;&#26434;&#25512;&#29702;&#23588;&#20854;&#26159;ToM&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11127</link><description>&lt;p&gt;
&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65306;&#29702;&#35299;&#20854;&#31639;&#27861;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#22312;&#25552;&#39640;&#23454;&#35777;&#34920;&#29616;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance. (arXiv:2304.11127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#27714;&#26356;&#21152;&#22797;&#26434;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23454;&#39564;&#36890;&#24120;&#26377;&#35768;&#22810;&#21442;&#25968;&#65292;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#12290;Tree-structured Parzen estimator (TPE) &#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#26368;&#36817;&#30340;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25511;&#21046;&#21442;&#25968;&#30340;&#35282;&#33394;&#21644;&#31639;&#27861;&#30452;&#35273;&#23578;&#26410;&#24471;&#21040;&#35752;&#35770;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#30830;&#23450;&#27599;&#20010;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#20174;&#21078;&#26512;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#25512;&#33616;&#35774;&#32622;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#25512;&#33616;&#35774;&#32622;&#25552;&#39640;&#20102;TPE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;TPE&#23454;&#29616;&#21487;&#22312;https://github.com/nabenabe0928/tpe/tree/single-opt&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.10517</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#65306;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#27880;&#37322;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#33719;&#21462;&#25104;&#26412;&#65292;&#35757;&#32451;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;Segment Anything Model&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#22522;&#30784;&#27169;&#22411;&#65292;&#32463;&#36807;&#36229;&#36807;10&#20159;&#20010;&#27880;&#37322;&#30340;&#35757;&#32451;&#65292;&#20027;&#35201;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;&#65292;&#26088;&#22312;&#33021;&#22815;&#20197;&#20132;&#20114;&#26041;&#24335;&#20998;&#21106;&#29992;&#25143;&#23450;&#20041;&#30340;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;SAM&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#19981;&#28165;&#26970;&#35813;&#27169;&#22411;&#22312;&#36716;&#25442;&#21040;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#26102;&#20250;&#21463;&#21040;&#22810;&#22823;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;SAM&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#30340;11&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#29983;&#25104;&#28857;&#25552;&#31034;&#26469;&#27169;&#25311;&#20132;&#20114;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAM&#22522;&#20110;&#21333;&#28857;&#25552;&#31034;&#30340;&#34920;&#29616;&#22312;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26041;&#38754;&#39640;&#24230;&#21464;&#21270;&#65292;&#21363;&#20174;&#33034;&#26609;MRI&#25968;&#25454;&#38598;&#30340;0.1135&#21040;&#39627;&#20851;&#33410;X&#23556;&#32447;&#25968;&#25454;&#38598;&#30340;0.8650&#12290;
&lt;/p&gt;
&lt;p&gt;
Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#22330;&#26223;&#65292;&#25552;&#20986;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#22238;&#39038;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;&#20869;&#23384;&#25928;&#29575;&#21464;&#20307;&#21487;&#26377;&#25928;&#22320;&#20445;&#25345;&#19968;&#23450;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10335</link><description>&lt;p&gt;
&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#20013;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#32447;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A baseline on continual learning methods for video action recognition. (arXiv:2304.10335v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#22330;&#26223;&#65292;&#25552;&#20986;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#22238;&#39038;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;&#20869;&#23384;&#25928;&#29575;&#21464;&#20307;&#21487;&#26377;&#25928;&#22320;&#20445;&#25345;&#19968;&#23450;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36830;&#32493;&#23398;&#20064;&#21560;&#24341;&#20102;&#30740;&#31350;&#30028;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26088;&#22312;&#35299;&#20915;&#32463;&#20856;&#30417;&#30563;&#27169;&#22411;&#30340;&#38271;&#26399;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#37117;&#26159;&#38024;&#23545;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#36827;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#29366;&#24577;&#19979;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;&#38500;&#20102;&#30001;&#20110;&#26102;&#38388;&#32500;&#24230;&#32780;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#22806;&#65292;&#22312;&#35270;&#39057;&#29615;&#22659;&#20013;&#65292;&#20026;&#20102;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#30340;&#22238;&#39038;&#26041;&#27861;&#23545;&#35745;&#31639;&#36164;&#28304;&#35201;&#27714;&#26356;&#39640;&#12290;&#20026;&#20102;&#23545;&#25239;&#22686;&#21152;&#30340;&#20869;&#23384;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21435;&#26041;&#27861;&#37117;&#36890;&#29992;&#30340;&#22238;&#39038;&#26041;&#27861;&#21464;&#20307;&#65292;&#21033;&#29992;&#27169;&#22411;&#32622;&#20449;&#24230;&#25110;&#25968;&#25454;&#20449;&#24687;&#30340;&#25351;&#26631;&#26469;&#36873;&#25321;&#21487;&#35760;&#24518;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#25991;&#29486;&#20013;&#39044;&#26399;&#30340;&#19968;&#26679;&#65292;&#22238;&#39038;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65307;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#20869;&#23384;&#25928;&#29575;&#21464;&#20307;&#34987;&#35777;&#26126;&#22312;&#20445;&#25345;&#19968;&#23450;&#27700;&#24179;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning has recently attracted attention from the research community, as it aims to solve long-standing limitations of classic supervisedly-trained models. However, most research on this subject has tackled continual learning in simple image classification scenarios. In this paper, we present a benchmark of state-of-the-art continual learning methods on video action recognition. Besides the increased complexity due to the temporal dimension, the video setting imposes stronger requirements on computing resources for top-performing rehearsal methods. To counteract the increased memory requirements, we present two method-agnostic variants for rehearsal methods, exploiting measures of either model confidence or data information to select memorable samples. Our experiments show that, as expected from the literature, rehearsal methods outperform other approaches; moreover, the proposed memory-efficient variants are shown to be effective at retaining a certain level of performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.09349</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#22823;&#33041;&#65306;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#24863;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#22791;&#29289;&#29702;&#25110;&#34394;&#25311;&#23454;&#20307;&#65288;&#21363;&#26426;&#22120;&#20154;&#65289;&#24182;&#33021;&#22815;&#19982;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#35760;&#24518;&#21644;&#25511;&#21046;&#26159;&#20307;&#24863;&#31995;&#32479;&#30340;&#20004;&#20010;&#22522;&#26412;&#37096;&#20998;&#65292;&#36890;&#24120;&#38656;&#35201;&#20998;&#21035;&#20351;&#29992;&#26694;&#26550;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LLM-Brain&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#65292;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#12290;LLM-Brain&#26694;&#26550;&#38598;&#25104;&#20102;&#22810;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21033;&#29992;&#38646;-shot&#23398;&#20064;&#26041;&#27861;&#12290;LLM-Brain&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#12290;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20855;&#22791;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#23454;&#20307;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#65306;&#20027;&#21160;&#25506;&#32034;&#21644;&#23454;&#20307;&#38382;&#31572;&#26469;&#28436;&#31034;LLM-Brain&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;LiDAR-&#24815;&#24615;&#34701;&#21512;&#37324;&#31243;&#35745;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#23637;&#31034;&#20102;&#19981;&#21516;&#34701;&#21512;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;KITTI&#21644;EuRoC&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07728</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;LiDAR-&#24815;&#24615;&#34701;&#21512;&#37324;&#31243;&#35745;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TransFusionOdom: Interpretable Transformer-based LiDAR-Inertial Fusion Odometry Estimation. (arXiv:2304.07728v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;LiDAR-&#24815;&#24615;&#34701;&#21512;&#37324;&#31243;&#35745;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#23637;&#31034;&#20102;&#19981;&#21516;&#34701;&#21512;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;KITTI&#21644;EuRoC&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#37324;&#31243;&#35745;&#20272;&#35745;&#24615;&#33021;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#34701;&#21512;&#65292;&#36825;&#20063;&#26159;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#22522;&#26412;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#22312;&#30417;&#30563;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#37324;&#31243;&#35745;&#20272;&#35745;&#20219;&#21153;&#20013;&#65292;&#22914;&#20309;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#25191;&#34892;&#34701;&#21512;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19968;&#20123;&#31616;&#21333;&#30340;&#25805;&#20316;&#65292;&#22914;&#36880;&#20803;&#32032;&#27714;&#21644;&#21644;&#36830;&#25509;&#65292;&#24182;&#19981;&#20855;&#22791;&#20998;&#37197;&#33258;&#36866;&#24212;&#20851;&#27880;&#26435;&#37325;&#20197;&#26377;&#25928;&#21512;&#24182;&#19981;&#21516;&#27169;&#24577;&#30340;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#33719;&#24471;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#37324;&#31243;&#35745;&#32467;&#26524;&#21464;&#24471;&#22256;&#38590;&#12290;&#26368;&#36817;&#65292;Transformer&#26550;&#26500;&#26174;&#31034;&#20986;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31471;&#21040;&#31471;&#30340;&#28608;&#20809;&#38647;&#36798;-&#24815;&#24615;&#34701;&#21512;&#26694;&#26550;(&#21363;TransFusionOdom)&#26469;&#36827;&#34892;&#37324;&#31243;&#35745;&#20272;&#35745;&#12290;&#22810;&#22836;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#23637;&#31034;&#20102;&#21516;&#26500;&#21644;&#24322;&#26500;&#27169;&#24577;&#30340;&#19981;&#21516;&#34701;&#21512;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;TransFusionOdom&#27169;&#22411;&#22312;KITTI&#21644;EuRoC&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal fusion of sensors is a commonly used approach to enhance the performance of odometry estimation, which is also a fundamental module for mobile robots. However, the question of \textit{how to perform fusion among different modalities in a supervised sensor fusion odometry estimation task?} is still one of challenging issues remains. Some simple operations, such as element-wise summation and concatenation, are not capable of assigning adaptive attentional weights to incorporate different modalities efficiently, which make it difficult to achieve competitive odometry results. Recently, the Transformer architecture has shown potential for multi-modal fusion tasks, particularly in the domains of vision with language. In this work, we propose an end-to-end supervised Transformer-based LiDAR-Inertial fusion framework (namely TransFusionOdom) for odometry estimation. The multi-attention fusion module demonstrates different fusion approaches for homogeneous and heterogeneous modalit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;&#30340;&#31532;&#20108;&#23626;&#27604;&#36187;&#32467;&#26524;&#65292;&#39640;&#36136;&#37327;&#30340;SYNS-Patches&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;&#27604;&#36187;&#38590;&#24230;&#65292;&#25152;&#26377;&#25552;&#20132;&#20316;&#21697;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.07051</link><description>&lt;p&gt;
&#31532;&#20108;&#23626;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
The Second Monocular Depth Estimation Challenge. (arXiv:2304.07051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;&#30340;&#31532;&#20108;&#23626;&#27604;&#36187;&#32467;&#26524;&#65292;&#39640;&#36136;&#37327;&#30340;SYNS-Patches&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;&#27604;&#36187;&#38590;&#24230;&#65292;&#25152;&#26377;&#25552;&#20132;&#20316;&#21697;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#25361;&#25112;&#36187;&#65288;MDEC&#65289;&#30340;&#31532;&#20108;&#23626;&#27604;&#36187;&#32467;&#26524;&#12290;&#26412;&#27425;&#27604;&#36187;&#25509;&#21463;&#20219;&#20309;&#24418;&#24335;&#26041;&#24335;&#30340;&#30417;&#30563;&#65292;&#21253;&#25324;&#20840;&#30417;&#30563;&#12289;&#33258;&#30417;&#30563;&#12289;&#22810;&#20219;&#21153;&#25110;&#20195;&#29702;&#28145;&#24230;&#12290;&#27604;&#36187;&#30340;&#25968;&#25454;&#38598;&#22522;&#20110;SYNS-Patches&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#36136;&#37327;&#30340;&#23494;&#38598;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#29615;&#22659;&#22810;&#26679;&#24615;&#12290;&#27604;&#36187;&#25910;&#21040;&#20102;8&#20010;&#29420;&#29305;&#30340;&#25552;&#20132;&#20316;&#21697;&#65292;&#25152;&#26377;&#22522;&#20110;&#28857;&#20113;&#25110;&#22522;&#20110;&#22270;&#20687;&#30340;&#25351;&#26631;&#34920;&#29616;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#27700;&#24179;&#12290;&#26368;&#20339;&#30417;&#30563;&#25552;&#20132;&#20316;&#21697;&#30340;&#30456;&#23545;F-&#20998;&#25968;&#25552;&#39640;&#20102;27.62&#65285;&#65292;&#32780;&#26368;&#20339;&#33258;&#30417;&#30563;&#25552;&#20132;&#20316;&#21697;&#25552;&#39640;&#20102;16.61&#65285;&#65292;&#36825;&#20123;&#32467;&#26524;&#20195;&#34920;&#20102;&#30495;&#27491;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the results for the second edition of the Monocular Depth Estimation Challenge (MDEC). This edition was open to methods using any form of supervision, including fully-supervised, self-supervised, multi-task or proxy depth. The challenge was based around the SYNS-Patches dataset, which features a wide diversity of environments with high-quality dense ground-truth. This includes complex natural environments, e.g. forests or fields, which are greatly underrepresented in current benchmarks.  The challenge received eight unique submissions that outperformed the provided SotA baseline on any of the pointcloud- or image-based metrics. The top supervised submission improved relative F-Score by 27.62%, while the top self-supervised improved it by 16.61%. Supervised submissions generally leveraged large collections of datasets to improve data diversity. Self-supervised submissions instead updated the network architecture and pretrained backbones. These results represent a si
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#65292;&#20197;&#20811;&#26381;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#21644;&#31435;&#27861;&#26426;&#26500;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#36880;&#27493;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#24688;&#24403;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2304.04914</link><description>&lt;p&gt;
&#30417;&#31649;&#24066;&#22330;&#65306;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Regulatory Markets: The Future of AI Governance. (arXiv:2304.04914v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#65292;&#20197;&#20811;&#26381;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#21644;&#31435;&#27861;&#26426;&#26500;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#36880;&#27493;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#24688;&#24403;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24688;&#24403;&#22320;&#30417;&#31649;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26085;&#30410;&#32039;&#36843;&#30340;&#25919;&#31574;&#25361;&#25112;&#12290;&#31435;&#27861;&#26426;&#26500;&#21644;&#30417;&#31649;&#26426;&#26500;&#32570;&#20047;&#32763;&#35793;&#20844;&#20247;&#38656;&#27714;&#20026;&#27861;&#24459;&#35201;&#27714;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#26410;&#33021;&#20351;AI&#31995;&#32479;&#30340;&#29983;&#20135;&#32773;&#21644;&#20351;&#29992;&#32773;&#23545;&#27665;&#20027;&#35201;&#27714;&#36127;&#36131;&#12290;&#25552;&#20986;&#20102;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20811;&#26381;&#21629;&#20196;&#21644;&#25511;&#21046;&#30417;&#31649;&#21644;&#33258;&#25105;&#30417;&#31649;&#30340;&#23616;&#38480;&#24615;&#12290;&#30417;&#31649;&#24066;&#22330;&#21487;&#20197;&#20351;&#25919;&#24220;&#20026;AI&#30417;&#31649;&#24314;&#31435;&#25919;&#31574;&#20248;&#20808;&#32423;&#65292;&#21516;&#26102;&#20381;&#38752;&#24066;&#22330;&#21147;&#37327;&#21644;&#34892;&#19994;&#30740;&#21457;&#21162;&#21147;&#26469;&#24320;&#21019;&#26368;&#33021;&#23454;&#29616;&#25919;&#31574;&#21046;&#23450;&#32773;&#22768;&#26126;&#30446;&#26631;&#30340;&#30417;&#31649;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Appropriately regulating artificial intelligence is an increasingly urgent policy challenge. Legislatures and regulators lack the specialized knowledge required to best translate public demands into legal requirements. Overreliance on industry self-regulation fails to hold producers and users of AI systems accountable to democratic demands. Regulatory markets, in which governments require the targets of regulation to purchase regulatory services from a private regulator, are proposed. This approach to AI regulation could overcome the limitations of both command-and-control regulation and self-regulation. Regulatory market could enable governments to establish policy priorities for the regulation of AI, whilst relying on market forces and industry R&amp;D efforts to pioneer the methods of regulation that best achieve policymakers' stated objectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DiffMimic&#65292;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#26377;&#26356;&#24555;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#21516;&#26102;&#36890;&#36807;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.03274</link><description>&lt;p&gt;
DiffMimic: &#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
DiffMimic: Efficient Motion Mimicking with Differentiable Physics. (arXiv:2304.03274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiffMimic&#65292;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#39640;&#25928;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#26377;&#26356;&#24555;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#21516;&#26102;&#36890;&#36807;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27169;&#20223;&#26159;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#21160;&#30011;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#37117;&#24314;&#31435;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20043;&#19978;&#65292;&#23384;&#22312;&#37325;&#24230;&#22870;&#21169;&#24037;&#31243;&#12289;&#39640;&#26041;&#24046;&#21644;&#38590;&#20197;&#25506;&#32034;&#30340;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#27169;&#25311;&#22120;&#65288;DPS&#65289;&#30340;&#36816;&#21160;&#27169;&#20223;&#26041;&#27861;&#65292;&#21517;&#20026;DiffMimic&#65292;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#21644;&#22522;&#20110;&#30495;&#23454;&#29289;&#29702;&#20808;&#39564;&#23398;&#20064;&#31283;&#23450;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#26356;&#24555;&#21644;&#26356;&#31283;&#23450;&#30340;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#28436;&#31034;&#37325;&#25773;&#26426;&#21046;&#65292;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#20869;&#23454;&#29616;&#31283;&#23450;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion mimicking is a foundational task in physics-based character animation. However, most existing motion mimicking methods are built upon reinforcement learning (RL) and suffer from heavy reward engineering, high variance, and slow convergence with hard explorations. Specifically, they usually take tens of hours or even days of training to mimic a simple motion sequence, resulting in poor scalability. In this work, we leverage differentiable physics simulators (DPS) and propose an efficient motion mimicking method dubbed DiffMimic. Our key insight is that DPS casts a complex policy learning task to a much simpler state matching problem. In particular, DPS learns a stable policy by analytical gradients with ground-truth physical priors hence leading to significantly faster and stabler convergence than RL-based methods. Moreover, to escape from local optima, we utilize a Demonstration Replay mechanism to enable stable gradient backpropagation in a long horizon. Extensive experiments o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#32536;Transformer&#21644;&#39044;&#35757;&#32451;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#28216;&#25103;&#20013;&#36827;&#34892;&#22909;&#21451;&#25490;&#21517;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.10043</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#36793;&#32536;Transformer&#22312;&#22312;&#32447;&#28216;&#25103;&#20013;&#36827;&#34892;&#22909;&#21451;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Friend Ranking in Online Games via Pre-training Edge Transformers. (arXiv:2302.10043v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#32536;Transformer&#21644;&#39044;&#35757;&#32451;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#28216;&#25103;&#20013;&#36827;&#34892;&#22909;&#21451;&#25490;&#21517;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#28216;&#25103;&#20013;&#65292;&#22909;&#21451;&#22238;&#24518;&#26159;&#25552;&#39640;&#27599;&#26085;&#27963;&#36291;&#29992;&#25143;&#25968;&#37327;&#30340;&#37325;&#35201;&#36884;&#24452;&#12290;&#26412;&#25991;&#23558;&#22909;&#21451;&#22238;&#24518;&#38382;&#39064;&#35270;&#20026;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#20197;&#20351;&#29992;&#65288;&#27963;&#36291;&#30340;&#21644;&#22833;&#33853;&#30340;&#65289;&#29609;&#23478;&#29305;&#24449;&#20197;&#21450;&#21382;&#21490;&#20107;&#20214;&#30340;&#20960;&#31181;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36793;&#32536;Transformer&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#27454;&#33150;&#35759;&#28216;&#25103;&#30340;&#31163;&#32447;&#23454;&#39564;&#21644;&#22312;&#32447;A/B&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Friend recall is an important way to improve Daily Active Users (DAU) in online games. The problem is to generate a proper lost friend ranking list essentially. Traditional friend recall methods focus on rules like friend intimacy or training a classifier for predicting lost players' return probability, but ignore feature information of (active) players and historical friend recall events. In this work, we treat friend recall as a link prediction problem and explore several link prediction methods which can use features of both active and lost players, as well as historical events. Furthermore, we propose a novel Edge Transformer model and pre-train the model via masked auto-encoders. Our method achieves state-of-the-art results in the offline experiments and online A/B Tests of three Tencent games.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#29983;&#25104;&#30340;&#22495;&#32034;&#24341;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#27934;&#23519;&#35270;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02561</link><description>&lt;p&gt;
&#22495;&#32034;&#24341;&#21464;&#20998;&#36125;&#21494;&#26031;&#65306;&#21487;&#35299;&#37322;&#30340;&#22495;&#32034;&#24341;&#29992;&#20110;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation. (arXiv:2302.02561v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#29983;&#25104;&#30340;&#22495;&#32034;&#24341;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#27934;&#23519;&#35270;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22495;&#32034;&#24341;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#24635;&#26159;&#26377;&#36825;&#26679;&#30340;&#22495;&#32034;&#24341;&#21487;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#27010;&#29575;&#35282;&#24230;&#25552;&#20379;&#20102;&#22495;&#32034;&#24341;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#22495;&#32034;&#24341;&#65292;&#20174;&#32780;&#25552;&#20379;&#39069;&#22806;&#30340;&#22495;&#20851;&#31995;&#27934;&#23519;&#65292;&#24182;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23545;&#25239;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#22312;&#24179;&#34913;&#28857;&#22788;&#25214;&#21040;&#20102;&#26368;&#20248;&#30340;&#22495;&#32034;&#24341;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#22495;&#32034;&#24341;&#65292;&#20351;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;&#29616;&#26377;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Wang-ML-Lab/VDI&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have shown that leveraging domain index can significantly boost domain adaptation performance (arXiv:2007.01807, arXiv:2202.03628). However, such domain indices are not always available. To address this challenge, we first provide a formal definition of domain index from the probabilistic perspective, and then propose an adversarial variational Bayesian framework that infers domain indices from multi-domain data, thereby providing additional insight on domain relations and improving domain adaptation performance. Our theoretical analysis shows that our adversarial variational Bayesian framework finds the optimal domain index at equilibrium. Empirical results on both synthetic and real data verify that our model can produce interpretable domain indices which enable us to achieve superior performance compared to state-of-the-art domain adaptation methods. Code is available at https://github.com/Wang-ML-Lab/VDI.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11719</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#32435;&#20837;&#25991;&#26723;&#25688;&#35201;&#29983;&#25104;&#20013;&#65306;&#22522;&#20110;GPT-2&#30340;&#21069;&#32512;&#35843;&#25972;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#22312;&#25991;&#26723;&#25688;&#35201;&#25216;&#26415;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#20294;&#26159;&#29983;&#25104;&#30340;&#25688;&#35201;&#21644;&#21407;&#22987;&#25991;&#26412;&#20043;&#38388;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#20173;&#28982;&#26102;&#26377;&#21457;&#29983;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#37319;&#29992;&#25552;&#31034;&#26469;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20855;&#20307;&#30740;&#31350;&#20102;&#21069;&#32512;&#35843;&#25972;&#65292;&#23427;&#20351;&#29992;&#19968;&#32452;&#21487;&#35757;&#32451;&#30340;&#36830;&#32493;&#21069;&#32512;&#25552;&#31034;&#21644;&#31163;&#25955;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#24110;&#21161;&#25688;&#35201;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#35757;&#32451;&#30340;&#21069;&#32512;&#21487;&#20197;&#24110;&#21161;&#25688;&#35201;&#27169;&#22411;&#20934;&#30830;&#22320;&#20174;&#31163;&#25955;&#25552;&#31034;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#20174;&#32780;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#36825;&#20123;&#25688;&#35201;&#22312;&#20107;&#23454;&#19978;&#19982;&#31163;&#25955;&#25552;&#31034;&#19968;&#33268;&#12290;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;ROUGE&#25913;&#36827;&#34920;&#26126;&#65292;&#23558;&#20107;&#23454;&#30693;&#35782;&#26126;&#30830;&#22320;&#28155;&#21152;&#21040;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24212;&#29992;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10802</link><description>&lt;p&gt;
BTS&#65306;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#23460;&#20869;&#20004;&#25151;&#38388;&#23384;&#22312;&#26816;&#27979;&#20013;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI. (arXiv:2212.10802v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#23460;&#20869;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;CSI&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#22914;&#29289;&#20307;&#31227;&#21160;&#12289;&#22823;&#27668;&#22240;&#32032;&#21644;&#26426;&#22120;&#37325;&#21551;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#32791;&#26102;&#30340;&#26631;&#27880;&#26469;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#35774;&#35745;&#19968;&#20010;&#36830;&#32493;&#30417;&#25511;&#30340;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24605;&#20102;&#19968;&#31181;&#21452;&#25240;&#21472;&#24072;&#29983;&#65288;BTS&#65289;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#23384;&#22312;&#20110;&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#21407;&#22987;&#23545;&#20598;&#24072;&#29983;&#32593;&#32476;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;CSI&#20013;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#22686;&#24378;&#30340;&#24809;&#32602;&#25439;&#22833;&#20989;&#25968;&#21033;&#29992;&#29109;&#21644;&#36317;&#31163;&#27979;&#37327;&#26469;&#21306;&#20998;&#28145;&#23618;&#29305;&#24449;&#65292;&#38477;&#20302;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, indoor human presence detection based on supervised learning (SL) and channel state information (CSI) has attracted much attention. However, the existing studies that rely on spatial information of CSI are susceptible to environmental changes, such as object movement, atmospheric factors, and machine rebooting, which degrade prediction accuracy. Moreover, SL-based methods require time-consuming labeling for retraining models. Therefore, it is imperative to design a continuously monitored model life-cycle using a semi-supervised learning (SSL) based scheme. In this paper, we conceive a bifold teacher-student (BTS) learning approach for presence detection systems that combines SSL by utilizing partially labeled and unlabeled datasets. The proposed primal-dual teacher-student network intelligently learns spatial and temporal features from labeled and unlabeled CSI. Additionally, the enhanced penalized loss function leverages entropy and distance measures to distinguish dr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02842</link><description>&lt;p&gt;
VISEM-Tracking&#65292;&#19968;&#20221;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VISEM-Tracking, a human spermatozoa tracking dataset. (arXiv:2212.02842v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#23376;&#36816;&#21160;&#30340;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#26174;&#24494;&#38236;&#35266;&#23519;&#65292;&#30001;&#20110;&#25152;&#35266;&#23519;&#30340;&#31934;&#23376;&#22312;&#35270;&#37326;&#20013;&#30340;&#24555;&#36895;&#31227;&#21160;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#27491;&#30830;&#30340;&#32467;&#26524;&#65292;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#22521;&#35757;&#12290;&#22240;&#27492;&#65292;&#22312;&#35786;&#25152;&#20013;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#31934;&#23376;&#20998;&#26512;&#65288;CASA&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#29992;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#26469;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;VISEM-Tracking&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20&#20010;30&#31186;&#30340;&#35270;&#39057;&#35760;&#24405;&#65288;&#21253;&#25324;29,196&#24103;&#65289;&#30340;&#28287;&#24615;&#31934;&#23376;&#21046;&#22791;&#29289;&#65292;&#20855;&#22791;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#35813;&#39046;&#22495;&#30340;&#19987;&#23478;&#20998;&#26512;&#30340;&#19968;&#32452;&#31934;&#23376;&#29305;&#24449;&#12290;&#38500;&#20102;&#24050;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21098;&#36753;&#65292;&#20197;&#20415;&#36890;&#36807;&#33258;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#36731;&#26494;&#35775;&#38382;&#21644;&#20998;&#26512;&#25968;&#25454;&#12290;&#20316;&#20026;&#26412;&#25991;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#32447;&#31934;&#23376;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as selfor unsupervised learning. As part of this paper, we present baseline sperm detection performan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#32452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#22312;&#31354;&#38388;&#19978;&#21448;&#22312;&#39057;&#29575;&#19978;&#20998;&#35299;&#20449;&#21495;&#65292;&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#32534;&#30721;&#29305;&#23450;&#39057;&#29575;&#26469;&#23384;&#20648;&#27599;&#20010;&#32593;&#26684;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;2D&#22270;&#20687;&#25311;&#21512;&#12289;3D&#24418;&#29366;&#37325;&#24314;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#32039;&#20945;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.01735</link><description>&lt;p&gt;
&#31070;&#32463;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#32452;
&lt;/p&gt;
&lt;p&gt;
Neural Fourier Filter Bank. (arXiv:2212.01735v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01735
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#32452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#22312;&#31354;&#38388;&#19978;&#21448;&#22312;&#39057;&#29575;&#19978;&#20998;&#35299;&#20449;&#21495;&#65292;&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#32534;&#30721;&#29305;&#23450;&#39057;&#29575;&#26469;&#23384;&#20648;&#27599;&#20010;&#32593;&#26684;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;2D&#22270;&#20687;&#25311;&#21512;&#12289;3D&#24418;&#29366;&#37325;&#24314;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#32039;&#20945;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#39640;&#25928;&#19988;&#39640;&#24230;&#35814;&#32454;&#30340;&#37325;&#26500;&#12290;&#21463;&#23567;&#27874;&#21551;&#21457;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#31070;&#32463;&#22330;&#65292;&#26082;&#22312;&#31354;&#38388;&#19978;&#21448;&#22312;&#39057;&#29575;&#19978;&#20998;&#35299;&#20449;&#21495;&#12290;&#25105;&#20204;&#36981;&#24490;&#26368;&#36817;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#31354;&#38388;&#20998;&#35299;&#33539;&#20363;&#65292;&#20294;&#19982;&#29616;&#26377;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36890;&#36807;&#20613;&#37324;&#21494;&#29305;&#24449;&#32534;&#30721;&#40723;&#21169;&#22312;&#27599;&#20010;&#32593;&#26684;&#20013;&#23384;&#20648;&#29305;&#23450;&#30340;&#39057;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24102;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#20197;&#22312;&#36866;&#24403;&#30340;&#23618;&#27425;&#19978;&#25509;&#21463;&#36825;&#20123;&#20613;&#37324;&#21494;&#32534;&#30721;&#30340;&#29305;&#24449;&#65292;&#20197;&#20351;&#39640;&#39057;&#32452;&#20214;&#20381;&#27425;&#32047;&#31215;&#22312;&#20302;&#39057;&#32452;&#20214;&#20043;&#19978;&#65292;&#26368;&#21518;&#23558;&#23427;&#20204;&#30456;&#21152;&#20197;&#24418;&#25104;&#26368;&#32456;&#36755;&#20986;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#65288;2D&#22270;&#20687;&#25311;&#21512;&#65292;3D&#24418;&#29366;&#37325;&#24314;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21253;&#25324;&#27169;&#22411;&#32039;&#20945;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/ubc-vision/NFFB&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method to provide efficient and highly detailed reconstructions. Inspired by wavelets, we learn a neural field that decompose the signal both spatially and frequency-wise. We follow the recent grid-based paradigm for spatial decomposition, but unlike existing work, encourage specific frequencies to be stored in each grid via Fourier features encodings. We then apply a multi-layer perceptron with sine activations, taking these Fourier encoded features in at appropriate layers so that higher-frequency components are accumulated on top of lower-frequency components sequentially, which we sum up to form the final output. We demonstrate that our method outperforms the state of the art regarding model compactness and convergence speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CrossSplit&#30340;&#26032;&#35757;&#32451;&#31243;&#24207;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#20998;&#21106;&#30340;&#26631;&#31614;&#20462;&#27491;&#21644;&#21322;&#30417;&#30563;&#35757;&#32451;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#32531;&#35299;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#26631;&#31614;&#22122;&#22768;&#35760;&#24518;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.01674</link><description>&lt;p&gt;
CrossSplit: &#36890;&#36807;&#25968;&#25454;&#20998;&#21106;&#32531;&#35299;&#26631;&#31614;&#22122;&#22768;&#35760;&#24518;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CrossSplit: Mitigating Label Noise Memorization through Data Splitting. (arXiv:2212.01674v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CrossSplit&#30340;&#26032;&#35757;&#32451;&#31243;&#24207;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#20998;&#21106;&#30340;&#26631;&#31614;&#20462;&#27491;&#21644;&#21322;&#30417;&#30563;&#35757;&#32451;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#32531;&#35299;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#26631;&#31614;&#22122;&#22768;&#35760;&#24518;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#24773;&#20917;&#19979;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#40065;&#26834;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#29616;&#26377;&#30340;&#26631;&#31614;&#20462;&#27491;&#21644;&#20849;&#21516;&#25945;&#23398;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31243;&#24207;&#8212;&#8212;CrossSplit&#65292;&#20197;&#32531;&#35299;&#22122;&#22768;&#26631;&#31614;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;CrossSplit&#20351;&#29992;&#22312;&#20004;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#19981;&#30456;&#20132;&#37096;&#20998;&#19978;&#35757;&#32451;&#30340;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#32452;&#21512;&#20102;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;(i)&#20132;&#21449;&#20998;&#21106;&#26631;&#31614;&#20462;&#27491;&#65306;&#30001;&#20110;&#22312;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#19981;&#33021;&#35760;&#24518;&#26469;&#33258;&#20854;&#20182;&#37096;&#20998;&#30340;&#31034;&#20363;-&#26631;&#31614;&#23545;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#23545;&#31561;&#32593;&#32476;&#30340;&#39044;&#27979;&#24179;&#28369;&#35843;&#25972;&#27599;&#20010;&#32593;&#32476;&#21576;&#29616;&#30340;&#35757;&#32451;&#26631;&#31614;&#65307;(ii)&#20132;&#21449;&#20998;&#21106;&#21322;&#30417;&#30563;&#35757;&#32451;&#65306;&#22312;&#19968;&#20010;&#37096;&#20998;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#20063;&#20351;&#29992;&#21478;&#19968;&#20010;&#37096;&#20998;&#30340;&#26410;&#26631;&#35760;&#36755;&#20837;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#12289;Tiny-ImageNet&#21644;mini-WebVision&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#22122;&#22768;&#21644;&#24178;&#25200;&#19979;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We approach the problem of improving robustness of deep learning algorithms in the presence of label noise. Building upon existing label correction and co-teaching methods, we propose a novel training procedure to mitigate the memorization of noisy labels, called CrossSplit, which uses a pair of neural networks trained on two disjoint parts of the labelled dataset. CrossSplit combines two main ingredients: (i) Cross-split label correction. The idea is that, since the model trained on one part of the data cannot memorize example-label pairs from the other part, the training labels presented to each network can be smoothly adjusted by using the predictions of its peer network; (ii) Cross-split semi-supervised training. A network trained on one part of the data also uses the unlabeled inputs of the other part. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#27169;&#22411;&#35780;&#20272;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#22312; ImageNet 256x256 &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID &#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.17091</link><description>&lt;p&gt;
&#21033;&#29992;&#37492;&#21035;&#22120;&#24341;&#23548;&#22312;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#23436;&#21892;&#29983;&#25104;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models. (arXiv:2211.17091v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#27169;&#22411;&#35780;&#20272;&#26356;&#21152;&#20934;&#30830;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#22312; ImageNet 256x256 &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID &#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#8220;&#37492;&#21035;&#22120;&#24341;&#23548;&#8221;&#26041;&#27861;&#26088;&#22312;&#25913;&#21892;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#37492;&#21035;&#22120;&#65292;&#26126;&#30830;&#22320;&#30417;&#30563;&#21435;&#22122;&#26679;&#26412;&#36335;&#24452;&#26159;&#21542;&#30495;&#23454;&#12290;&#19982; GAN &#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#32852;&#21512;&#35757;&#32451;&#35780;&#20998;&#21644;&#37492;&#21035;&#22120;&#32593;&#32476;&#12290;&#30456;&#21453;&#65292;&#22312;&#35780;&#20998;&#35757;&#32451;&#20043;&#21518;&#35757;&#32451;&#37492;&#21035;&#22120;&#65292;&#20351;&#37492;&#21035;&#22120;&#35757;&#32451;&#31283;&#23450;&#19988;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;&#26679;&#26412;&#29983;&#25104;&#20013;&#65292;&#25105;&#20204;&#21521;&#39044;&#35757;&#32451;&#30340;&#35780;&#20998;&#28155;&#21152;&#19968;&#20010;&#36741;&#21161;&#39033;&#20197;&#27450;&#39575;&#37492;&#21035;&#22120;&#12290;&#35813;&#39033;&#23558;&#27169;&#22411;&#35780;&#20998;&#30699;&#27491;&#20026;&#26368;&#20248;&#37492;&#21035;&#22120;&#22788;&#30340;&#25968;&#25454;&#35780;&#20998;&#65292;&#36825;&#24847;&#21619;&#30528;&#37492;&#21035;&#22120;&#20197;&#34917;&#20805;&#30340;&#26041;&#24335;&#24110;&#21161;&#26356;&#22909;&#22320;&#35780;&#20272;&#20998;&#25968;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#22312; ImageNet 256x256 &#19978;&#23454;&#29616;&#20102; FID 1.83 &#21644;&#21484;&#22238;&#29575; 0.64 &#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#31867;&#20284;&#20110;&#39564;&#35777;&#25968;&#25454;&#30340; FID&#65288;1.68&#65289;&#21644;&#21484;&#22238;&#29575;&#65288;0.66&#65289;&#12290;&#25105;&#20204;&#22312; https://github.com/alsdudrla10/DG &#19978;&#20844;&#24320;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#19981;&#24403;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.05105</link><description>&lt;p&gt;
&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#65306;&#32531;&#35299;&#25193;&#25955;&#27169;&#22411;&#20013;&#19981;&#24403;&#36864;&#21270;
&lt;/p&gt;
&lt;p&gt;
Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. (arXiv:2211.05105v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#19981;&#24403;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#65292;&#24182;&#22240;&#27492;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#30001;&#20110;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#38543;&#26426;&#20174;&#20114;&#32852;&#32593;&#19978;&#25235;&#21462;&#30340;&#25968;&#21313;&#20159;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23427;&#20204;&#20063;&#38754;&#20020;&#26469;&#33258;&#36864;&#21270;&#21644;&#20559;&#35265;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#21453;&#36807;&#26469;&#65292;&#23427;&#20204;&#29978;&#33267;&#21487;&#33021;&#24378;&#21270;&#36825;&#20123;&#20559;&#35265;&#12290;&#20026;&#20102;&#24110;&#21161;&#24212;&#23545;&#36825;&#20123;&#19981;&#33391;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#65288;SLD&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#34913;&#37327;&#30001;&#20110;&#26410;&#36807;&#28388;&#21644;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#38598;&#32780;&#24341;&#36215;&#30340;&#19981;&#24403;&#36864;&#21270;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#20687;&#29983;&#25104;&#27979;&#35797;&#24179;&#21488;&#8212;&#8212;&#21253;&#21547;&#19987;&#38376;&#30340;&#12289;&#35206;&#30422;&#35064;&#38706;&#21644;&#26292;&#21147;&#31561;&#27010;&#24565;&#30340;&#23454;&#38469;&#22270;&#20687;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#19981;&#24403;&#22270;&#20687;&#25552;&#31034;&#65288;I2P&#65289;&#12290;&#27491;&#22914;&#25105;&#20204;&#35814;&#23613;&#30340;&#23454;&#35777;&#35780;&#20272;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;&#24341;&#20837;&#30340;SLD&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#20102;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#23545;&#22270;&#20687;&#36136;&#37327;&#27809;&#26377;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.07675</link><description>&lt;p&gt;
&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#20197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65306;&#22312;&#33647;&#29289;&#24320;&#21457;&#20013;&#21457;&#29616;&#32452;&#32455;&#23398;&#25913;&#21464;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#30340;&#31995;&#32479;&#12290;&#22312;&#32452;&#32455;&#23398;&#20013;&#65292;&#27491;&#24120;&#26679;&#26412;&#36890;&#24120;&#26159;&#22823;&#37327;&#23384;&#22312;&#30340;&#65292;&#32780;&#24322;&#24120;&#65288;&#30149;&#29702;&#65289;&#24773;&#20917;&#36890;&#24120;&#24456;&#23569;&#25110;&#19981;&#21487;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22312;&#20581;&#24247;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21333;&#31867;&#20998;&#31867;&#22120;&#21487;&#20197;&#26816;&#27979;&#21040;&#20998;&#24067;&#22806;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#19982;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#21069;&#24050;&#32463;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#12290;&#20294;&#26159;&#65292;&#39044;&#35757;&#32451;&#30340;&#29616;&#25104;CNN&#34920;&#31034;&#21487;&#33021;&#23545;&#32452;&#32455;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#19981;&#25935;&#24863;&#65292;&#32780;&#20581;&#24247;&#32452;&#32455;&#30340;&#33258;&#28982;&#21464;&#24322;&#21487;&#33021;&#23548;&#33268;&#36828;&#31163;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#20351;&#34920;&#31034;&#36866;&#24212;&#20581;&#24247;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#36741;&#21161;&#20219;&#21153;&#19978;&#35757;&#32451;CNN&#65292;&#35813;&#20219;&#21153;&#21306;&#20998;&#19981;&#21516;&#29289;&#31181;&#12289;&#22120;&#23448;&#21644;&#26579;&#33394;&#35797;&#21058;&#30340;&#20581;&#24247;&#32452;&#32455;&#12290;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#27880;&#24037;&#20316;&#37327;&#65292;&#22240;&#20026;&#20581;&#24247;&#26679;&#26412;&#21487;&#20197;&#33258;&#21160;&#33719;&#24471;&#19978;&#36848;&#26631;&#31614;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#24378;&#21046;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
We present a system for anomaly detection in histopathological images. In histology, normal samples are usually abundant, whereas anomalous (pathological) cases are scarce or not available. Under such settings, one-class classifiers trained on healthy data can detect out-of-distribution anomalous samples. Such approaches combined with pre-trained Convolutional Neural Network (CNN) representations of images were previously employed for anomaly detection (AD). However, pre-trained off-the-shelf CNN representations may not be sensitive to abnormal conditions in tissues, while natural variations of healthy tissue may result in distant representations. To adapt representations to relevant details in healthy tissue we propose training a CNN on an auxiliary task that discriminates healthy tissue of different species, organs, and staining reagents. Almost no additional labeling workload is required, since healthy samples come automatically with aforementioned labels. During training we enforce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#22411;&#39550;&#39542;&#21592;&#34892;&#20026;&#27169;&#22411;&#30340;&#28151;&#21512;&#36816;&#36755;&#31995;&#32479;&#20223;&#30495;&#24179;&#21488;&#65292;&#27169;&#25311;&#39550;&#39542;&#21592;&#19982;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20132;&#20114;&#65292;&#22312;&#39640;&#20445;&#30495;&#30340;&#20223;&#30495;&#30740;&#31350;&#20013;&#21487;&#29992;&#20110;&#35780;&#20272;&#30896;&#25758;&#26816;&#27979;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.05540</link><description>&lt;p&gt;
&#22522;&#20110;&#22686;&#24378;&#22411;&#39550;&#39542;&#21592;&#34892;&#20026;&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#30896;&#25758;&#26816;&#27979;&#31639;&#27861;&#20223;&#30495;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Augmented Driver Behavior Models for High-Fidelity Simulation Study of Crash Detection Algorithms. (arXiv:2208.05540v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#22411;&#39550;&#39542;&#21592;&#34892;&#20026;&#27169;&#22411;&#30340;&#28151;&#21512;&#36816;&#36755;&#31995;&#32479;&#20223;&#30495;&#24179;&#21488;&#65292;&#27169;&#25311;&#39550;&#39542;&#21592;&#19982;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20132;&#20114;&#65292;&#22312;&#39640;&#20445;&#30495;&#30340;&#20223;&#30495;&#30740;&#31350;&#20013;&#21487;&#29992;&#20110;&#35780;&#20272;&#30896;&#25758;&#26816;&#27979;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#36830;&#25509;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;CAVs&#65289;&#30340;&#23433;&#20840;&#24615;&#21644;&#26377;&#25928;&#24615;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#22823;&#37327;&#30340;&#27979;&#35797;&#21644;&#35780;&#20272;&#12290;&#22312;&#20851;&#38190;&#21644;&#21361;&#38505;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#36825;&#20123;&#31995;&#32479;&#30340;&#38656;&#27714;&#20351;&#24471;&#35780;&#20272;&#25104;&#26412;&#38750;&#24120;&#26114;&#36149;&#12289;&#21487;&#33021;&#23384;&#22312;&#21361;&#38505;&#65292;&#24182;&#19988;&#32791;&#26102;&#36739;&#38271;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#20351;&#29992;&#20223;&#30495;&#24179;&#21488;&#26469;&#30740;&#31350;&#21644;&#35780;&#20272;&#20854;&#31639;&#27861;&#21644;&#35774;&#35745;&#12290;&#23545;&#20110;&#19982;CAVs&#25110;&#20854;&#20182;&#36710;&#36742;&#20114;&#21160;&#30340;&#39550;&#39542;&#21592;&#25110;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#34892;&#20026;&#24314;&#27169;&#26159;&#36825;&#31181;&#27169;&#25311;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#34429;&#28982;&#20026;&#20154;&#31867;&#34892;&#20026;&#24320;&#21457;&#23436;&#32654;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#21644;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#22312;&#29616;&#26377;&#27169;&#25311;&#22120;&#29992;&#20110;&#39550;&#39542;&#21592;&#34892;&#20026;&#30340;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#26174;&#33879;&#30340;&#25193;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#36816;&#36755;&#31995;&#32479;&#30340;&#20223;&#30495;&#24179;&#21488;&#65292;&#21253;&#25324;&#20154;&#39550;&#39542;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#39550;&#39542;&#20219;&#21153;&#36827;&#34892;&#20998;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#20223;&#30495;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing safety and efficiency applications for Connected and Automated Vehicles (CAVs) require a great deal of testing and evaluation. The need for the operation of these systems in critical and dangerous situations makes the burden of their evaluation very costly, possibly dangerous, and time-consuming. As an alternative, researchers attempt to study and evaluate their algorithms and designs using simulation platforms. Modeling the behavior of drivers or human operators in CAVs or other vehicles interacting with them is one of the main challenges of such simulations. While developing a perfect model for human behavior is a challenging task and an open problem, we present a significant augmentation of the current models used in simulators for driver behavior. In this paper, we present a simulation platform for a hybrid transportation system that includes both human-driven and automated vehicles. In addition, we decompose the human driving task and offer a modular approach to simulat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SOVR&#30340;&#23545;&#25239;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#32858;&#28966;&#37325;&#35201;&#26679;&#26412;&#65292;&#22686;&#21152;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#65292;&#20174;&#32780;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.10283</link><description>&lt;p&gt;
&#19968;&#23545;&#20854;&#20313;&#25439;&#22833;&#20989;&#25968;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#32858;&#28966;&#37325;&#35201;&#26679;&#26412;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-vs-the-Rest Loss to Focus on Important Samples in Adversarial Training. (arXiv:2207.10283v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SOVR&#30340;&#23545;&#25239;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#32858;&#28966;&#37325;&#35201;&#26679;&#26412;&#65292;&#22686;&#21152;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#65292;&#20174;&#32780;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#12290;&#30001;&#20110;&#23545;&#25239;&#35757;&#32451;&#23384;&#22312;&#22256;&#38590;&#65292;&#22914;&#38656;&#35201;&#39640;&#27169;&#22411;&#23481;&#37327;&#65292;&#36890;&#36807;&#21152;&#26435;&#20132;&#21449;&#29109;&#25439;&#22833;&#20851;&#27880;&#37325;&#35201;&#25968;&#25454;&#28857;&#24050;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#22797;&#26434;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;Auto-Attack&#12290;&#26412;&#25991;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#20204;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#21407;&#22240;&#26159;&#30495;&#23454;&#26631;&#31614;&#21644;&#20854;&#20182;&#26631;&#31614;&#20043;&#38388;&#30340;&#23545;&#25968;&#20960;&#29575;&#20043;&#38388;&#30340;&#36739;&#23567;&#38388;&#38548;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#26159;&#26681;&#25454;&#23545;&#25968;&#20960;&#29575;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#20998;&#31867;&#30340;&#65292;&#25152;&#20197;&#23545;&#25968;&#20960;&#29575;&#30340;&#38388;&#38548;&#24212;&#35813;&#36275;&#22815;&#22823;&#65292;&#20197;&#36991;&#20813;&#25915;&#20987;&#32763;&#36716;&#26368;&#22823;&#30340;&#23545;&#25968;&#20960;&#29575;&#12290;&#37325;&#35201;&#24615;&#24863;&#30693;&#26041;&#27861;&#19981;&#20250;&#22686;&#21152;&#37325;&#35201;&#26679;&#26412;&#30340;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#65292;&#20294;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#27604;&#20250;&#20943;&#23569;&#36739;&#19981;&#37325;&#35201;&#26679;&#26412;&#30340;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#12290;&#20026;&#20102;&#22686;&#21152;&#37325;&#35201;&#26679;&#26412;&#30340;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20999;&#25442;&#19968;&#23545;&#20854;&#20313;&#65288;SOVR&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#22312;&#20855;&#26377;&#36739;&#23567;&#23545;&#25968;&#20960;&#29575;&#38388;&#38548;&#30340;&#37325;&#35201;&#26679;&#26412;&#20013;&#20174;&#20132;&#21449;&#29109;&#20999;&#25442;&#21040;&#19968;&#23545;&#20854;&#20313;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#20998;&#26512;&#12289;&#28040;&#34701;&#30740;&#31350;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;SOVR&#23545;&#25239;&#25239;&#20987;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new loss function for adversarial training. Since adversarial training has difficulties, e.g., necessity of high model capacity, focusing on important data points by weighting cross-entropy loss has attracted much attention. However, they are vulnerable to sophisticated attacks, e.g., Auto-Attack. This paper experimentally reveals that the cause of their vulnerability is their small margins between logits for the true label and the other labels. Since neural networks classify the data points based on the logits, logit margins should be large enough to avoid flipping the largest logit by the attacks. Importance-aware methods do not increase logit margins of important samples but decrease those of less-important samples compared with cross-entropy loss. To increase logit margins of important samples, we propose switching one-vs-the-rest loss (SOVR), which switches from cross-entropy to one-vs-the-rest loss for important samples that have small logit margins. We prov
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#35299;&#37322;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#36807;&#24230;&#36924;&#36817;&#25968;&#20540;Fixpoint&#36845;&#20195;&#22120;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#22495;&#65292;CH-Zonotope&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20256;&#25773;&#21644;&#21253;&#21547;&#26816;&#26597;&#12290;&#35813;&#26694;&#26550;&#22312;&#30740;&#31350;monDEQ&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#39564;&#35777;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2110.08260</link><description>&lt;p&gt;
&#25277;&#35937;&#35299;&#37322;&#19979;&#30340;Fixpoint&#36845;&#20195;&#22120;&#21450;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Abstract Interpretation of Fixpoint Iterators with Applications to Neural Networks. (arXiv:2110.08260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#35299;&#37322;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#36807;&#24230;&#36924;&#36817;&#25968;&#20540;Fixpoint&#36845;&#20195;&#22120;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#22495;&#65292;CH-Zonotope&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20256;&#25773;&#21644;&#21253;&#21547;&#26816;&#26597;&#12290;&#35813;&#26694;&#26550;&#22312;&#30740;&#31350;monDEQ&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#39564;&#35777;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25277;&#35937;&#35299;&#37322;&#26694;&#26550;&#65292;&#20197;&#31934;&#30830;&#22320;&#36807;&#24230;&#36924;&#36817;&#25968;&#20540;Fixpoint&#36845;&#20195;&#22120;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19982;&#36890;&#24120;&#29992;&#20110;&#36924;&#36817;&#25152;&#26377;&#21487;&#36798;&#31243;&#24207;&#29366;&#24577;&#30340;&#26631;&#20934;&#25277;&#35937;&#35299;&#37322;&#65288;AI&#65289;&#19981;&#21516;&#65292;&#20154;&#20204;&#21482;&#38656;&#35201;&#25277;&#35937;&#20855;&#20307;&#30340;Fixpoints&#65292;&#21363;&#26368;&#32456;&#30340;&#31243;&#24207;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38024;&#23545;&#20855;&#20307;&#25910;&#25947;&#24615;&#21644;&#21807;&#19968;&#24615;&#20445;&#35777;&#30340;&#25968;&#20540;Fixpoint&#36845;&#20195;&#22120;&#65292;&#22522;&#20110;&#20004;&#20010;&#20027;&#35201;&#30340;&#25216;&#26415;&#36129;&#29486;&#65306;&#65288;i&#65289;&#29702;&#35770;&#19978;&#30340;&#27934;&#23519;&#21147;&#65292;&#20801;&#35768;&#25105;&#20204;&#35745;&#31639;&#20986;&#19981;&#20351;&#29992;&#20132;&#27719;&#28857;&#30340;&#22768;&#38899;&#21644;&#31934;&#30830;&#30340;Fixpoint&#25277;&#35937;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#22495;&#65292;CH- Zonotope&#65292;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#20801;&#35768;&#26377;&#25928;&#30340;&#20256;&#25773;&#21644;&#21253;&#21547;&#26816;&#26597;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;CRAFT&#30340;&#24037;&#20855;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20110;Fixpoint&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;monDEQ&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;CRAFT&#36229;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new abstract interpretation framework for the precise over-approximation of numerical fixpoint iterators. Our key observation is that unlike in standard abstract interpretation (AI), typically used to over-approximate all reachable program states, in this setting, one only needs to abstract the concrete fixpoints, i.e., the final program states. Our framework targets numerical fixpoint iterators with convergence and uniqueness guarantees in the concrete and is based on two major technical contributions: (i) theoretical insights which allow us to compute sound and precise fixpoint abstractions without using joins, and (ii) a new abstract domain, CH-Zonotope, which admits efficient propagation and inclusion checks while retaining high precision. We implement our framework in a tool called CRAFT and evaluate it on a novel fixpoint-based neural network architecture (monDEQ) that is particularly challenging to verify. Our extensive evaluation demonstrates that CRAFT exceeds the
&lt;/p&gt;</description></item></channel></rss>